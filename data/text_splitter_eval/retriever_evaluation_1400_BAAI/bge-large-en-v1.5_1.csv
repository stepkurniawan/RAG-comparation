|question|ground_truths|contexts|context_precision|context_recall
0|What is the advantage of A/B testing?|The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.|"[""In the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A/B Testing==\n'''Advantages'''\nA/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. “Online Controlled Experiments and A/B Testing.” Encyclopedia of Machine Learning and Data Mining, 2017, 922–29. https://doi.org/10.1007/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. “Experimentation and Start-up Performance: Evidence from A/B Testing.” Management Science 68, no. 9 (September 2022): 6434–53. https://doi.org/10.1287/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A / B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart""]"|0.125|0.0
1|What is the ANOVA a powerful for?|Reducing variance in field experiments or complex laboratory experiments|"[""====Analysis of Variance====\n'''The [https://www.investopedia.com/terms/a/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment. \n\nThe general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.\n\n====Preconditions====\nRegarding the preconditions of the [https://www.youtube.com/watch?v=oOuu8IBd-yo ANOVA], it is important to realize that the data should ideally be '''normally distributed''' on all levels, which however is often violated due to small sample sizes. Since a non-normal distribution may influence the outcome of the test, boxplots are a helpful visual aid, as these allow for a simple detection tool of non-normal distribution levels. \n\nEqually should ideally the variance be comparable across all levels, which is called '''[https://blog.minitab.com/blog/statistics-and-quality-data-analysis/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia homoscedastic]'''. What is also important is the criteria of '''independence''', meaning that samples of factor levels should not influence each other. For this reason are for instance in ecological experiments plants typically planted in individual pots. In addition does the classical ANOVA assume a '''balanced design''', which means that all factor levels have an equal sample size. If some factor levels have less samples than others, this might pose interactions in terms of normals distribution and variance, but there is another effect at play. Larger sample sizes on one factor level may create a disbalance, where factor levels with larger samples pose a larger influence on the overall model result. \n\n====One way and two way ANOVA====\nSingle factor analysis that are also called '[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a '''[https://www.statisticshowto.com/post-hoc/ posthoc test]'''. A prominent example is the [https://sciencing.com/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https://www.statisticshowto.com/post-hoc/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.\n\n====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong.""]"|0.34146341463414637|0.0
2|What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?|Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.|"['[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Bayesian Inference until 2020.\'\'\' Search terms: \'Bayesian\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.\'\'\' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes\' name only became associated with the method in the 1900s (3).\n\n\'\'\'The family of methods based on the concept of Bayesian analysis has risen the last 50 years\'\'\' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n\'\'\'Bayesian analysis relies on using probability figures as an expression of our beliefs about events.\'\'\' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is ""prior"", and the probability figure assigned to the prior is called ""prior probability"". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called ""posterior probability"", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|\'\'\'The probability distribution for prior, evidence, and posterior.\'\'\']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes\' Theorem.\n\n==== Bayes\' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes\' theorem.\n\n\'\'\'Conditional Probability\'\'\' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n\'\'\'Joint Probability\'\'\', also called ""conjoint probability"", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n\'\'\'Marginal Probability\'\'\' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for ""Bayes theorem"" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n\'\'Caution:\'\' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n\'\'Note:\'\' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let\'s see how this formulation of Bayes\' Theorem can be applied in a simple coin toss example:\n\n\n=== \'\'\'Example I: Classic coin toss\'\'\' ===\n\'\'\'Imagine, you are flipping 2 fair coins.\'\'\' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)']"|0.15492957746478872|0.0
3|Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?|Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.|"['====4. Build on the available literature.====\n[[File:Caspar David Friedrich - Der Wanderer über dem Nebelmeer.jpg|thumb|right|Although the ""Wander über dem Nebelmeer"" does not literally stand on the shoulders of giants, the painting of Caspar David Friedrich is a good visualisation of the metaphor.]]\nNo case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new. The more you stand on the shoulders of giants, the further you can see. We need to realize that science is build on failure and iteration. Only by knowing about previous failures but also successes, and only by committing to the canon of knowledge, can we contribute and create new knowledge.\n\n\n==Natural experiments and statistics - a rough guide==\nSeveral layers of information are to be considered when linking statistics to natural experiments. Contextualising natural experiments can be structured into at least 4 steps.\n\n1) \'\'\'How is the case connected and embedded in the global context?\'\'\'\nWhat are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems. We hence acknowledge that this contextualisation is place based, and places have boundaries and borders. While it is way beyond this text to discuss this issue, the question how a place is bound can be a source of great confusion. A good rule of thumb from a statistical standpoint can be the idea that a place does only exists if its within variance, regarding central variables, is lower than the variance between the place and its neighbouring places.\n\n2) \'\'\'what are imbalances and dispersions within the case, both spatially and temporally?\'\'\'\nIn other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social context of the case\'s dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly. \n\n3) \'\'\'How are the livelihoods within the case?\'\'\'\nWhile many would connect this to the last point, it may be worthwhile to consider this on its own. Livelihoods are what defines groups of people and their possibility to thrive. While this is equally bound to [[Glossary|culture]] and justice, it repeatedly proves that it is much more than the mere sum of the two. Livelihoods are often characterised by a deeply qualitative manifestation, which is where statistics need be to aware of its limitations. While statistics can compile descriptive information on a community and tackle some of the interactions within a community, it has to fail considering other points. This limitation is important to realise.\n\n4) \'\'\'what is the intervention of the natural experiment within the case?\'\'\'\nHere, a whole world of interventions could be described. Instead, let us just settle on what was the active part of the researcher: systematically compare the state before the intervention with the state after the intervention.\n\n==Experiments in sustainability science==\n[[File:World 3 model.jpg|600px|thumb|right|\'\'\'The world 3 model was part of the ""Limits to Growth"" report of the Club of Rome.\'\'\' The picture creates a sense of the multitude and complexity of variables that influence a single case study in the real world.]]\nExperiments in sustainability science are characterised by an intervention and the production of empirical evidence. These two key criteria are essential for experiments in sustainability science, which can be in addition also differentiated into a problem focused and a solution orientated focus. Nevertheless, problem orientated studies with full control over all variables (except for the one(s) being investigated) are still clearly timely in sustainability science, as many experiments conducted in ecology prove. Less control over variables is equally important, and again ecology, agricultural science but also experiments in psychology come to mind. Problem orientated focus with no control over variables is at the heart of a joined problem framing of [[Glossary|transdisciplinary]] research in sustainability science, and such an approach also characterised the first Club of Rome report. Solution orientated focus with total or some control over variables is insofar a remarkable step since it marks a shift from the still dominating [[Glossary|paradigm]] of research where facts are only approximated. Since solutions may be compromises but can be achieved, a solution orientated research indicates a shift from more classical lines of thinking. The solution orientated approach with no control over variables marks the frontier of research, since it is clearly long term thinking. Prominent efforts are the IPBES, the sustainable development goal measures, and the current efforts of the rising number of transdisciplinary projects. While these are starting points, I consider these efforts to be a clear sign of hope, not only for out planet, but likewise for the development of science out of its dogmatic slumber.\n\n==Meta-analysis: Integrating cases towards holistic understanding==\nOut of the increasing number of scientific studies with a comparable design, an international school of thinking emerged: [http://meta-evidence.co.uk/difference-systematic-review-meta-analysis/ Meta-analysis]! Within meta-analytical frameworks, great care is taken to combine several studies to investigate their overall outcome. Rooted deeply in medicine and psychology, meta-analysis used the statistical power of studies that show a similar design and setting. Since some studies are larger while others build on smaller samples, meta-analysis are able to take some of these differences into account.\n\nIt is however important to understand the thinking behind meta-analysis, as it helps to create supra-knowledge about certain questions that were enough in the focus so that a number of integrable studies is available. Graphical overviews show summaries of effects, and so called random factors allow for the correction of different sample sizes and other factors. Beyond the disciplines where meta analysis were initially established, this line of thinking found its way into ecology, conservation, agriculture and many other arenas of science that investigate building on deductive designs. To this end, challenges often arise out of field experiments where less variables are being controlled for or are being understood, and hence comparisons may not be valid or at least less plausible.']"|0.0|0.0
4|What is the recommended course of action for datasets with only categorical data?|For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.|"['<syntaxhighlight lang=""Python"" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n</syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=""Python"" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n</syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=""Python"" line>\ndata.info()\n</syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (""non-null"") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the ""object"" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For ""sex"" and ""passed"", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=""Python"" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=""Python"" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term ‘residual’ refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=""Python"" line>\nmodel_1 = smf.ols(formula=\'points ~ ID + quanti\', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).']"|0.012048192771084338|0.0
5|What is a Generalised Linear Model (GLM)?|A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.|"['\'\'\'How do I know?\'\'\'\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n\'\'\'You have arrived at a Generalised Linear Model (GLM).\'\'\' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?']"|0.10126582278481013|1.0
6|What is Cluster Analysis?|Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).|"['[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n\'\'\'In short:\'\'\' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|\'\'\'SCOPUS hits for Clustering until 2019.\'\'\' Search terms: \'Clustering\', \'Cluster Analysis\' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as \'\'similarity measure\'\' or \'\'distance measure\'\', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named ""unsupervised learning"", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term ""clustering"" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=""R"" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(""x1"", ""x2"", ""cluster"")\n    return(df)\n}\n</syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns \'\'\'n\'\'\' examples to one of \'\'\'k\'\'\' clusters, where \'\'\'n\'\'\' is the sample size and  \'\'\'k\'\'\', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:\n\n# The number of cluster \'\'\'k\'\'\' is chosen by the data analyst\n# The algorithm randomly picks \'\'\'k\'\'\' centroids and assigns each point to the closest centroid to get \'\'\'k\'\'\' initial clusters\n# The algorithm recalculates the centroid by taking average of all points in each cluster and updates the centroids and re-assigns the points to the closest centroid.\n# The algorithm repeats Step 3 until all points stop changing clusters.\n\nTo get an intuitive sense of how this k-means clustering algorithm works, visit: [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====\n\nTo implement k-Means Clustering, the data table needs to only contain numeric data type. With a data frame or matrix with numeric value where the rows signify individual data example and the columns signify the \'\'features\'\' (number of features = the number of dimensions of the data set), k-Means clustering can be performed with the code below:\n\n<syntaxhighlight lang=""R"" line>\n# Generate data and perform the clustering\ndf <- create_cluster_data(150, 1.25, 3)\ndata_cluster = kmeans(df, centers=3) # perform the clustering']"|0.014084507042253521|0.0
7|What is the purpose of Network Analysis?|Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.|"[""== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) '''First of all, is the statistics the right approach to begin with?''' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem. \n\n2) '''What are the efforts you need to put into the initial data gathering?''' If you decided that statistics would be valuable to be applied, the question then would be, how? To rephrase this statement: How exactly? Your sampling with all its constructs, sample sizes and replicates decides about the fate of everything you going to do later. A flawed dataset or a small or biased sample will lead to failure, or even worse, wrong results. Play it safe in the beginning, do not try to overplay your hand. Slowly edge your way into the application of statistics, and always reflect with others about your sampling strategy. \n\n3) '''The analysis then demands hand-on skills, as implementing tests within a software is something that you learn best through repetition and practice.''' I suggest you to team up with other peers who decide to go deeper into statistical analysis. If you however decide against that, try to find geeks that may help you with your data analysis. Modern research works in teams of complementary people, thus start to think in these dimensions. If you chip in the topical expertise of the effort to do the sampling, other people may be glad about the chance to analyse the data.\n\n4) '''This is also true for the interpretation, which most of all builds on experience.''' This is the point were a supervisor or a PhD student may be able to glance at a result and tell you which points are relevant, and which are negotiable. Empirical research typically produces results where in my experience about 80 % are next to negliable. It takes time to learn the difference between a trivial and an innovative result. Building on knowledge of the literature helps again to this end, but be patient as the interpretation of statistics is a skill that needs to ripen, since context matters. It is not so much about the result itself, but more about the whole context it is embedded in.\n\n5) The last and most important point explores this thought further. '''What are the limitations of your results?''' Where can you see flaws, and how does the multiverse of biases influence your results and interpretation? What are steps to be taken in future research? And what would we change if we could start over and do the whole thing again? All these questions are like ghosts that repeatedly come to haunt a researcher, which is why we need to remember we look at pieces of the puzzle. Acknowledging this is I think very important, as much of the negative connotation statistics often attracts is rooted in a lack of understanding. If people would have the privilege to learn about statistics, they could learn about the power of statistics, as wells its limitations. \n\n'''Never before did more people in the world have the chance to study statistics.''' While of course statistics can only offer a part of the puzzle, I would still dare to say that this is reason for hope. If more people can learn to unlock this knowledge, we might be able to move out of ignorance and more towards knowledge. I think it would be very helpful if in a controversial debate everybody could dig deep into the available information, and make up their own mind, without other people telling them what to believe. Learning about statistics is like learning about anything else, it is lifelong learning. I believe that true masters never achieve mastership, instead they never stop to thrive for it.\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden (concept, text) and Christopher Franz (implementation, linkages).""]"|0.05263157894736842|1.0
8|What is the purpose of ANCOVA in statistical analysis?|ANCOVA is used to compare group means while controlling for the effect of a covariate.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>']"|0.07766990291262135|1.0
9|What are the key principles and assumptions of ANCOVA?|ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>']"|0.0970873786407767|0.8
10|What are the assumptions associated with ANCOVA?|ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>']"|0.05825242718446602|0.2
11|What are the strengths and challenges of Content Analysis?|Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.|"[""== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.\n* Validity of the inferred results - i.e. the fitting of the results to the intended kind of knowledge - may be reached a) through the researchers' knowledge of contextual factors of the data and b) through validation with other sources, e.g. by the means of triangulation. However, the latter can be difficult to do due to the uniqueness of the data (1-3). Any content analysis is a reduction of the data, which should be acknowledged critically, and which is why the coding scheme should be precise and include the aforementioned explanation, examples and exclusion criteria.\n\n==== Connectedness ====\n* Since verbal discourse is an important data source for Content Analysis, the latter is often used as an analysis method of transcripts of standardized, [[Open Interview|open]], or [[Semi-structured Interview|semi-structured interviews]].\n* Content Analysis is one form of textual analysis. The latter also includes other approaches such as discourse analysis, rhetorical analysis, or [[Ethnography|ethnographic]] analysis (2). However, Content Analysis differs from these methods in terms of methodological elements and the kinds of questions it addresses (2).\n\n\n== Outlook ==\n* The usage of automated coding with the use of computers may be seen as one important future direction of the method (1, 5). To date, the human interpretation of ambiguous language imposes a high validity of the results which cannot (yet) be provided by a computer. Alas, the development of an appropriate algorithm and text recognition software pose a challenge. The meaning of words changes in different contexts and several expressions may mean the same. Especially in terms of qualitative analyses, this currently makes human coders indispensable. Yet, the emergence of big data, Artificial Intelligence and [[Machine Learning]] might make it possible in the foreseeable future to use automated coding more regularly and with a high validity.\n* Another relevant direction is the shift from text to visual and audio data as a primary form of data (5). With ever-increasing amounts of pictures, video and audio files on the internet, future studies may refer to these kinds of sources more often.\n\n== Key publications ==\nKuckartz, U. (2016). ''Qualitative Inhaltsanalyse: Methoden, Praxis, Computerunterstützung (3., überarbeitete Auflage). Grundlagentexte Methoden.'' Weinheim, Basel: Beltz Juventa.\n* A very exhaustive (German language) work in which the author explains different types of qualitative content analysis by also giving tangible examples.\n\nKrippendorff, K. 2004. ''Content Analysis - An Introduction to Its Methodology. Second Edition''. SAGE Publications.\n* A much-quoted, extensive description of the method's history, conceptual foundations, uses and application. \n\nBerelson, B. 1952. ''Content analysis in communication research.'' Glencoe, Ill.: Free Press.\n* An early review of concurrent forms of (quantitative) content analysis.\n\nSchreier, M. 2014. ''Varianten qualitativer Inhaltsanalyse: Ein Wegweiser im Dickicht der Begrifflichkeiten.'' Forum Qualitative Sozialforschung 15(1). Artikel 18.\n* A (German language) differentiation between the variations of the qualitative content analysis.\n\nErlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n* A very helpful guide to content analysis, using the examples shown above.\n\n\n== References ==\n(1) Krippendorff, K. 1989. ''Content Analysis.'' In: Barnouw et al. (Eds.). ''International encyclopedia of communication.'' Vol. 1. 403-407. New York, NY: Oxford University Press.\n\n(2) White, M.D. Marsh, E.E. 2006. ''Content Analysis: A Flexible Methodology.'' Library Trends 55(1). 22-45. \n\n(3) Stemler, S. 2000. ''An overview of content analysis.'' Practical Assessment, Research, and Evaluation 7. Article 17.\n\n(4) Mayring, P. 2000. ''Qualitative Content Analysis''. Forum Qualitative Social Research 1(2). Article 20.\n\n(5) Stemler, S. 2015. ''Content Analysis. Emerging Trends in the Social and Behavioral Sciences: An Interdisciplinary, Searchable, and Linkable Resource.'' 1-14.\n\n(6) Erlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.""]"|0.14102564102564102|0.5
12|What are the three main methods to calculate the correlation coefficient and how do they differ?|The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n\'\'\'In short:\'\'\' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, \'\'\'correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!\'\'\'\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n\'\'\'A note on calculating the correlation coefficient:\'\'\'\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson\'s correlation coefficient, Spearman\'s rank correlation coefficient and Kendall\'s rank coefficient. \n\'\'\'Pearson\'s correlation coefficient\'\'\' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson\'s correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. \'\'\'Spearman\'s rank correlation\'\'\' coefficient calculates the rank order of the variables\' values using a monotonic function whereas \'\'\'Kendall\'s rank correlation coefficient\'\'\' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson\'s correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\n#Fig.1\ndata(""mtcars"")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = ""Correlation between Miles per Gallon and Horsepower"",\n     xlab = ""Miles per Gallon"",\n     ylab = ""Horsepower"",\n     pch = 16,\n     col = ""red"",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n</syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables “mpg” and “hp” from the “mtcars” dataset. The Pearson\'s correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=""R"" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n</syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=""R"" inline>plot()</syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* \'\'\'x:\'\'\' variable, that will be displayed on the x-axis.\n* \'\'\'y:\'\'\' variable, that will be displayed on the y-axis.\n* \'\'\'xlab:\'\'\' title for the x-axis.\n* \'\'\'ylab:\'\'\' title for the y-axis.\n* \'\'\'pch:\'\'\' shape and size of the plotted observations, in this case, filled circles. [http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* \'\'\'col:\'\'\' plotting color. You can either write the name of the color or use the [https://www.r-graph-gallery.com/41-value-of-the-col-function.html color number].\n* \'\'\'las:\'\'\' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* \'\'\'xlim:\'\'\' set the limit of the x-axis.\n* \'\'\'ylim:\'\'\' set the limit of the y-axis.\n* \'\'\'abline:\'\'\' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=""R"" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n</syntaxhighlight>']"|0.05555555555555555|1.0
13|What is the purpose of a correlogram and how is it created?|A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.|"['=== R Code ===\n<syntaxhighlight lang=""R"" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n</syntaxhighlight>\n\nClear and meaningful coding and plots are important. In order to achieve this, we have to change the names of the variables from the “mtcars” dataset into something meaningful. One way to do this, is to change the names of the columns and rows of the correlation variable.\n<syntaxhighlight lang=""R"" line>\ncorrelations <- cor(mtcars)[1:11, 1:11]\ncolnames(correlations) <- c(""Miles per Gallon"", ""Cylinders"", \n                            ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                            ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                            ""Gears"", ""Carburetors"")\nrownames(correlations) <- c(""Miles per Gallon"", ""Cylinders"", \n                            ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                            ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                            ""Gears"", ""Carburetors"")\n</syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=""R"" line>\n# Fig.5\ncorrplot(correlations,\n         method = ""circle"",\n         type = ""upper"",\n         order = ""hclust"",\n         tl.col = ""black"",\n         tl.srt = 45,\n         tl.cex = 0.6)\n</syntaxhighlight>\n\nThe parameters are different from the previous scatter plots. Obviously, here you need the corrplot() function and define your parameters, regarding to your preferred taste, in this function. Some of the parameters will be explained briefly.\n\n* \'\'\'method\'\'\': which method should be used to visualize your correlation matrix. There are seven different methods (“circle”, “square”, “ellipse”, “number”, “shade”, “color”, “pie”), “circle” is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* \'\'\'type\'\'\': how the correlation matrix will be displayed. It can either be “upper”, “lower” or “full”. Full is called by default.\n* \'\'\'order\'\'\': order method for the correlation coefficients. The “hclust” method orders them in hierarchical order, but it also possible to order them alphabetical (“alphabetical”) or with a [[Principal_Component_Analysis|principal component analysis]] (“PCA”).\n* \'\'\'tl.col\'\'\': color of the labels.\n* \'\'\'tl.srt:\'\'\' rotation of the labels in degrees.\n* \'\'\'tl.cex:\'\'\' size of the labels.\n\n== Visualisation with ggplot ==\n=== Overview ===\n=== R code ===\nCOMING SOON\n\nAs you can see, there are many different ways to visualize correlations between variables. The right correlation plot depends on your data and on the number of variables you want to analyze. But never forget, correlation plots show you only the relationship between the variables and nothing about the causality.\n\n\n== References ==\n* https://sustainabilitymethods.org/index.php/Causality_and_correlation\n* https://en.wikipedia.org/wiki/Correlation_and_dependence\n* https://codingwithmax.com/correlation-vs-causation-examples/\n* https://towardsdatascience.com/what-it-takes-to-be-correlated-ce41ad0d8d7f\n* http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient\n\nA nice example that shows how easy it is to create a spurious correlation:\n\n* https://rstudio-pubs-static.s3.amazonaws.com/4192_1180a799cd6c4d2ba6e4ed2702860efb.html\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden and ?.']"|0.203125|0.6666666666666666
14|What is telemetry?|Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.|"['THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=""Python"" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n</syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=""Python"" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir(\'/path/to/your/directory\')\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndf = pd.read_csv(\'D202.csv\')\ndf.head()\n</syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let\'s choose the most relevant columns for our research:\n\n<syntaxhighlight lang=""Python"" line>\n## Let\'s choose the most relevant columns for our research:\ndf[\'start_date\'] = pd.to_datetime(df[\'DATE\'] + \' \' + df[\'START TIME\'])\ndf[\'cost_dollars\'] = df[\'COST\'].apply(lambda x: float(x[1:]))\ndf.rename(columns={\'USAGE\': \'usage_kwh\'}, inplace=True)\ndf = df.drop(columns=[\'TYPE\', \'UNITS\', \'DATE\', \'START TIME\', \'END TIME\', \'NOTES\', \'COST\']).set_index(\'start_date\')\n</syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe “cost_dollars” by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don\'t specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=""Python"" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n</syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https://lg4ml.org/grounddetection/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=""Python"" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y=\'usage_kwh\',\n        title=\'Usage of Electricity over 2 Weeks\',\n        labels={\'start_date\': \'Date\', \'usage_kwh\': \'Usage (KWh)\'}) ## uses the data from ""start_date"" called ""Date"", and the data of ""usage_kwh"" called ""usage (KwH)""\n</syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time</small>\n\n<syntaxhighlight lang=""Python"" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df[\'usage_kwh\'])) ## transforms into frequencies\nfrequencies = rfftfreq(df[\'usage_kwh\'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)']"|0.0963855421686747|0.0
15|What is a common reason for deviation from the normal distribution?|A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.|"['[[File:Qqplot notnomral.jpg|thumb|left|5. A gamma distribution, where the variances increases with the square of the mean.]]\n[[File:Qqplot negbinom.jpg|thumb|center|6. A negative binomial distribution that is clearly not following a normal distribution. In other words here the points are not on the line, the visual inspection of this qqplot concludes that your residuals are not normally distributed.]]\n\n===Non-normal distributions===\n\'\'\'Sometimes the world is [https://www.statisticshowto.com/probability-and-statistics/non-normal-distributions/ not normally distributed].\'\'\' At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which [https://www.isixsigma.com/tools-templates/normality/dealing-non-normal-data-strategies-and-tools/ reasons] exists why data is not normally distributed. \n\n==== The Poisson distribution ====\n[[File:Bildschirmfoto 2020-04-08 um 12.05.28.png|thumb|500px|\'\'\'This picture shows you several possible poisson distributions.\'\'\' They differ according to the lambda, the rate parameter.]]\n\n[https://www.youtube.com/watch?v=BbLfV0wOeyc Things that can be counted] are often [https://www.britannica.com/topic/Poisson-distribution not normally distributed], but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. \'\'\'How many people do you think drink one or two cups of coffee per day? Quite many, I guess.\'\'\' How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes. \n\nNow here comes the funny surprise. Transform the data following a [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 Poisson distribution], and it will typically follow the normal distribution if you use the decadic logarithm (log). Hence skewed data can be often transformed to match the normal distribution. While many people refrain from this, it actually may make sense in such examples as [https://sustainabilitymethods.org/index.php/Is_the_world_linear%3F island biogeography]. Discovered by MacArtur & Wilson, it is a prominent example of how the log of the numbers of species and the log of island size are closely related. While this is one of the fundamental basic of ecology, a statistician would have preferred the use of the Poisson distribution.\n\n===== Example for a log transformation of a Poisson distribution =====\n[[File:Poisson Education small.png|thumb|400px|left]]\n[[File:Poisson Education log small.png|thumb|400px|left]]\nOne example for skewed data can be found in the R data set “swiss”, it contains data about socio-economic indicators of about 50 provinces in Switzerland in 1888. The variable we would like to look at is “Education”, which shows how many men in the army (in %) have an education level beyond primary school. \nAs you can see when you look at the first diagram, in 30 provinces only 10 percent of the people received education beyond the primary school.\n\nTo obtain a normal distribution (which is useful for many statistical tests), we can use the natural logarithm.\n\nIf you would like to know, how to conduct an analysis like on the left-hand side, we uploaded the code right below:\n\n<syntaxhighlight lang=""R"" line>\n\n# we will work with the swiss() dataset.\n# to obtain a histogram of the variable Education, you type\n\nhist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n</syntaxhighlight>\n\n====The Pareto distribution====\n[[File:Bildschirmfoto 2020-04-08 um 12.28.46.png|thumb|300px|\'\'\'The Pareto distribution can also be apllied when we are looking at how wealth is spread across the world.\'\'\']]\n\n\'\'\'Did you know that most people wear 20 % of their clothes 80 % of their time?\'\'\' This observation can be described by the [https://www.youtube.com/watch?v=EAynHZE-lK4 Pareto distribution]. For many phenomena that describe proportion within a given population, you often find that few make a lot, and many make few things. Unfortunately this is often the case for workloads, and we shall hope to change this. For such proportions the [https://www.statisticshowto.com/pareto-distribution/ Pareto distribution] is quite relevant. Consequently, it is rooted in [https://www.pragcap.com/the-pareto-principle-and-wealth-inequality/ income statistics]. Many people have a small to average income, and few people have a large income. This makes this distribution so important for economics, and also for sustainability science.\n\n\n=== Visualizing data: Boxplots ===\nA nice way to visualize a data set is to draw a [[Barplots,_Histograms_and_Boxplots#Boxplots|boxplot]]. You get a rough overview how the data is distributed and moreover you can say at a glance if it’s normally distributed. The same is true for [[Barplots,_Histograms_and_Boxplots#Histograms|histograms]], but we will focus on the boxplot for now. For more information on both these forms of data visualisation, please refer to the entry on [[Barplots, Histograms and Boxplots]].']"|0.0|0.0
16|How can the Shapiro-Wilk test be used in data distribution?|The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.|"[""'''In short:''' The (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''ν = n−1'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value μ, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i</sub>'' deviates significantly from a specified value ''μ'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''μ'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''μ.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''σ'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.\n\n==Two-sample t-test==\nWhen wanting to find out whether the means of two samples of a distribution are deviating significantly. If the two samples are independent from each other, an independent two-sample t-test has to be used. If the samples are dependent, which means that the values being tested stem from the same samples or that the two samples are paired, a paired t-test can be used.\n===Independent Samples===\nFor independent samples with similar variances (a maximum ratio of 2), the t-statistic is calculated in the following way:\n[[File:prbdst7.png|700px|frameless|center]]\nwith the estimated pooled standard deviation\n[[File:prbdst8.png|700px|frameless|center]]\nIn accordance with the One-sample t-test, the sample sizes, means and standard deviations of the samples 1 and 2 are denoted by ''n<sub>1/2</sub>'', \u2002''x<sub>1/2</sub>'' and ''σ<sub>x<sub>1/2</sub></sub>'' respectively.\nThe degrees of freedom which are required for conducting the hypothesis testing is given as ''ν = n<sub>1</sub> + n<sub>2</sub> − 2''.\nFor samples with unequal variances, meaning that one sample variance is more than twice as big as the other, Welch’s t-test has to be used, leading to a different t-statistic t and different degrees of freedom ''ν'':\n[[File:prbdst9.png|700px|frameless|center]]\nAn approximation for the degrees of freedom can be calculated using the Welch-Satterthwaite equation:\n[[File:prbdst10.png|700px|frameless|center]]\nIt can be easily shown, that the t-statistic simplifies for equal sample sizes:\n[[File:prbdst11.png|700px|frameless|center]]\n\n===Paired Samples===\nWhen testing whether the means of two paired samples are differing significantly, the t-statistic consists of variables that differ from the ones used in previous tests:\n[[File:prbdst12.png|700px|frameless|center]]\nInstead of the independent means and standard deviations of the samples, new variables are used, that depend on the differences between the variable pairs. ''x<sub>d</sub>'' is given as the average of the differences of the sample pairs and ''σ<sub>D</sub>'' denotes the corresponding standard deviation. The value of ''μ<sub>0</sub>'' is set to zero to test whether the mean of the differences takes on a significant value.\n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n[[Category:Quantitative]]\n\nThe [[Table_of_Contributors| authors]] of this entry is Moritz Wohlstein.""]"|0.09523809523809523|0.0
17|Why is the Delphi method chosen over traditional forecasting methods?|The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.|"['3. After the first round, the participants\' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. \'Consensus\' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).\n\nNext, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group\'s positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be ""(...) a highly motivating experience for participants"" (Rayens & Hahn 2000, p.309) due to the feedback on the group\'s opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which ""(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one\'s mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others."" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)\n\nHowever, several potential pitfalls and challenges may arise during the Delphi process:\n* Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue (4, 5, 6).\n* A specific Delphi format that was useful in one study must not work as well in another context. Instead, the process must be adapted to the research design and underlying problem (4).\n* The proper selection of participating experts constitutes a major challenge for Delphi processes (3, 4, 5, 6). In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors (4).\n* The monitor (= researcher) must not impose their own preconceptions upon the respondents when developing the questionnaire but be open for contributions from the participants. The questions should be concise and understandable and should not incentivise the participant to ""get the job over with"" (Linstone & Turoff 1975, p.568; 5).\n* Diverse forms of [[Glossary|bias]] might occur on the part of the participants that need to be anticipated by the researcher. These include discount of the future, over-optimism / over-pessimism, misinterpretations with regard to the complexity and uncertainty involved in forecasting the future as well as other forms of bias that may be imposed through the feedback process (4, 6).\n* The responses must be adequately summarized, analyzed and presented to the participants (see the variety of measures for \'consensus\' in What the method does)). ""Agreement about a recommendation, future event, or potential decision does not disclose whether the individuals agreeing did so for the same underlying reasons. Failure to pursue these reasons can lead to dangerously false results."" (Linstone & Turoff 1975, p.568).\n* Disagreements between participants should be explored instead of being ignored so that the final consensus is not artificial (4).\n* The participants should be recompensated for their demanding task (4)\n\n\n== Normativity ==\n==== Connectedness / nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).\n\n==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the \'real-world\'.']"|0.18518518518518517|1.0
18|What is the main goal of Sustainability Science and what are the challenges it faces?|The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.|"[""[[File:SustainabilityCompetencies.png|750px|thumb|center|'''Key Competencies for Sustainability.''' Source: Wiek et al. 2011, p.206]]\nThe criteria from Wiek et al are outstanding in the capacity to serve as boundary object, since I do not see these categories as mutually exclusive, but instead strongly interwoven with each other. I can whole-heartedly recommend to return to these criteria then and again, and to reflect on yourself through the lense of these criteria.\n\n== Knowledge for action-oriented sustainability science ==\nAt the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and context-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science:\n[[File:Types of knowledge for action-oriented sustainability science.png|900px|thumb|center|'''Types of knowledge for action-oriented sustainability science.''' Source: Caniglia et al. 2020, p.4]]\n\nThis showcases that this knowledge - and more importantly - the perspective from a philosophy-of-science viewpoint is only starting to emerge, and much more work will be needed until our methodological canon and the knowledge we want to produce enable us to solve the problems we are facing, but also to create these solution in ways that are closer to a mode how we want to create these solutions. We may well be able to solve certain things, and to produce knowledge that can be seen as solutions. I would however argue, that it also matters how we create these solutions and how we create knowledge. Only if people are empowered and society and science work seamlessly together - with ethical restrictions and guidelines in place, of course - will we not only produce the knowledge needed, but we also produce it in a way how we should as scientists. '''Science is often disconnected and even arrogant, and building an educational system that is reflexive and interconnected will be maybe the largest challenge we face.''' This is why we give you these criteria here, because I think that we need to consider what further design criteria can be in order to enhance and diversify our conceptual thinking about the scientific methodological canon. Plurality on scientific methods will necessarily mean to evolve, and in this age of interconnectedness, our journey is only beginning.\n\n== Interaction with stakeholders == \nScientific methods can engage with non-scientific actors on diverse levels, depending on the extent of their involvement in the process of scientific inquiry. Interaction with stakeholder may be especially relevant in [[Transdisciplinarity|transdisciplinary]] research. \n<br/>'''Here, we refer to four levels of interaction:'''\n* ''Information'': Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation.\n* ''Consultation'': A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process.\n* ''Collaboration'': Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue.\n* ''Empowerment'': The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.\n\nYou can find more on these four categories in Brandt et al 2013, where a general introduction to the research landscape of transdisciplinary research in sustainability science is given. More will follow later on such approaches, and so much more still has to follow in science overall, since the declared distinction of science being in an ivory tower is only slowly crumbling. We need to question this [[Levels of Theory|paradigm]], and [[Questioning the status quo in methods|be critical of the status quo of normal science]]. More knowledge is needed, and especially, different knowledge.\n\n== References ==\n* Wiek et al. 2011. ''Key competencies in sustainability: a reference framework for academic program development''. Sustainability Science 6. 203-218.\n* Caniglia, G., Luederitz, C., von Wirth, T., Fazey, I., Martín-López, B., Hondrila, K., König, A., von Wehrden, H., Schäpke, N.A., Laubichler, M.D. and Lang, D.J., 2020. ''A pluralistic and integrated approach to action-oriented knowledge for sustainability.'' Nature Sustainability, pp.1-8.\n* Brandt, P., Ernst, A., Gralla, F., Luederitz, C., Lang, D.J., Newig, J., Reinert, F., Abson, D.J. and Von Wehrden, H., 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological economics, 92, pp.1-15.\n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz.""]"|0.09523809523809523|1.0
19|Why are critical theory and ethics important in modern science?|Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.|"[""The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n'''Forming hypotheses'''\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n'''Limitations of hypothesis'''\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - ‘causal’ form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]""]"|0.1323529411764706|0.5
20|What is system thinking?|System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.|"['Every system is ""(...) defined by its boundaries"" (Haraldsson 2004, p.13). The borders we draw for our system analysis influence which level of detail we apply to our view on the system, and which elements we investigate. System elements can be animate (animals, humans) or inanimate (rocks, rain), conceptual (motivation) or real (harvest), quantifiable (money) or rather qualitative (well-being) (2). For example, a system could be a tree, with the leaves, the stem and such elements interacting with each other, but also the forest in which our tree interacts with the soil, the weather, other plants, animals and inanimate objects. The system could also be the globe, where this forest interacts with other ecosystems, or the system in which Planet Earth interacts with the rest of the universe - our solar system. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n\'\'\'The system is at the basis of System Thinking.\'\'\' System Thinking is a form of scientific approach to organizing and understanding \'systems\' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of \'holistic\' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that ""(...) all system dynamics are in principle non-linear"" and that ""(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions"" (Haraldsson 2004, p.6).\n\n\'\'\'Peter Checkland introduced the notion that there are two main types of System Thinking:\'\'\' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a \'system\' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a \'system\' an ""(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world"" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a \'system\'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|\'\'\'Hard System Thinking and Soft System Thinking\'\'\' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: \'\'System Analysis\'\' and \'\'System Dynamics\'\' (1).\n\n\'\'System Analysis\'\' ""(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships."" (Haraldsson 2004, p.5). \'\'System Analysis\'\', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].\n\n\'\'System Dynamics\'\', then, focuses on the interaction part of the system. It ""(...) refers to the re-creation of the understanding of a system and its feedbacks. It aims at exploring dynamic responses to changes within or from outside the system. (...) System Dynamics deals with mathematical representation of our mental models and is a secondary step after we have developed our mental model."" (Haraldsson 2004, p.5). System Dynamics, as the name suggests, enables the researcher to observe and measure the behavior of the system. The interactions between the individual elements are not just recreated, but the consequences of these interactions are quantified and assessed.\n\nSystem Thinking allows for a shift in the perception of [[Causality|causality]]. Instead of assuming linear causality (A causes B, B causes C) it allows for the integration of further influencing factors, as well as a more neutral depiction of the system at hand. C may now be seen as a property that [[Agency, Complexity and Emergence|emerges]] from the relation between A and B, instead of perceiving it as a direct consequence of B. Haraldsson (2004, p.21) provides an illustrative example here: ""We start by asking the initial question: ""I want to understand how water flows into the glass and what I do to fill it up."" Instead of looking at the action from an individual point of view, where the ""I am"" is the active part and at the centre of focus, we shift our perception to the structure of the action. The ""I am"" simply becomes a part of the feedback process, not standing apart from it. Suddenly we have shifted out attention to the structure of the behavior and we can observe that the structure is causing the behavior. (...) We have now transformed the traditional linear thinking into a circular argument.""']"|0.25|0.0
21|What is the main principle of the Feynman Method?|The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n\'\'Teaching is the best way to learn.\'\'\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# \'\'\'Select the topic you want to learn more about.\'\'\' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don\'t go to broad - focus on a specific topic. You will not be able to explain ""Economics"" or ""Physics"" in one go.\n# \'\'\'Find someone to talk to\'\'\'. Ideally, this person does not know anything about this topic. If you don\'t have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# \'\'\'Make notes.\'\'\' You will quickly realize yourself which parts of the topic you are not able to explain, and/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# \'\'\'Have a look at your notes and try to find more information.\'\'\' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# \'\'\'Now explain the topic again.\'\'\' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https://karrierebibel.de/feynman-methode/ Karrierebibel]\n* [https://blog.doist.com/feynman-technique/ ToDo-ist]\n* [https://www.goodwall.io/blog/feynman-technique/ Goodwall]\n* [https://www.youtube.com/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.']"|0.13333333333333333|1.0
22|What is the difference between fixed and random factors in ANOVA designs?|Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.|"[""[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ Difference between crossed & nested factors]: A short article\n\n[https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/ Model reduction]: A helpful article\n\n[https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https://www.simplypsychology.org/milgram.html Field Experiments in Psychology]: A famous example\n\n[https://www.nature.com/articles/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https://revisesociology.com/2016/01/17/field-experiments-sociology/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https://www.youtube.com/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction\n\n[https://www.youtube.com/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.0|0.3333333333333333
23|What is the replication crisis and how does it affect modern research?|The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.|"[""This radical development coincides with yet another revolution that shattered science, namely the digital age. '''Computers allowed for faster calculation and novel methodological approaches.''' The internet fueled new sources and forms of knowledge, and the associated new forms of communication triggered an exchange between researchers at an unprecedented pace. All means of electronic communication, online [[Glossary|journals]] and the fact that many researchers today have their own computer led to an exponential increase in scientific collaboration. While this sometimes also breads opportunism and a shift to quantity instead of quality in research, it is undeniable that today much of scientific information is not further away from us than the click of a mouse. Technology cannot be an end in itself, but as a means to an end it enables today an exponential pace of research, which manifested itself most illustratively in the Corona crisis. The global community of researchers united in their utmost strength, and the speed and diversity of knowledge creation is unprecentented in the history of our civilisation. Never before was more interaction between the latest scientific inquiry or results and the society.\n\n== Additional Information ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html More information] on Kuhn's theory of scientific paradigm shifts.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.35714285714285715|0.0
24|What is the purpose and process of the flashlight method in group discussions?|The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || \'\'\'[[:Category:Team Size 30+|30+]]\'\'\'\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> \'\'Is everyone on the same page?\'\'\n<br> \'\'Are there important issues that have been neglected so far?\'\'\n<br> \'\'Is there unspoken dissonance?\'\'\n<br> \'\'Is there an elephant in the room?\'\'\n<br> \'\'What are we actually talking about?\'\'\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== \'\'Please note further\'\' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> \'\'What song would you choose to characterize the current state of discussion, and why?\'\' <br> ...\n\n== Links ==\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/blitzlicht/\n<br> https://www.bpb.de/lernen/formate/methoden/62269/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar Mölleken.']"|0.15217391304347827|0.3333333333333333
25|What types of data can Generalized Linear Models handle and calculate?|Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n\'\'\'Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.\'\'\' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution.']"|0.1568627450980392|0.8181818181818182
26|What is a heatmap and why is it useful?|A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function and \'\'\'mtcars\'\'\' dataset.\nIt is important to note that the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=""R"" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=""R"" line>\ndata(""mtcars"")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=""R"" line>\nfullcolnames <- c(""Miles per Gallon"", ""Number of Cylinders"",\n                  ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                  ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                  ""Number of Gears"", ""Number of Carburetors"")\n</syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = ""column"", labCol = fullcolnames, \n        margins = c(11,5))\n</syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let’s look at the feature <syntaxhighlight lang=""R"" inline>“Number of Carburetors”</syntaxhighlight>. We can see that \'\'\'Maserati Bora\'\'\' has the darkest color, hence it has the largest number of carburetors, followed by \'\'\'Ferrari Dino\'\'\', which has the second-largest number of carburetors. While other models such as \'\'\'Fiat X1-9\'\'\' or \'\'\'Toyota\'\'\' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.\n\n==Explanation of used arguments==\n* <syntaxhighlight lang=""R"" inline>Colv = NA</syntaxhighlight> and <syntaxhighlight lang=""R"" inline>Rowv = NA</syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.\n* <syntaxhighlight lang=""R"" inline>scale = “column”</syntaxhighlight> is used to normalize the columns of the matrix (to absorb the variation between columns). As it was stated previously, normalization is needed due to the algorithm by which the colors are set. Here in our dataset, the values of features “Gross horsepower” and “Displacement” are much larger than the rest. Therefore, without normalization, these two columns will be all marked approximately equally high and all the other columns equally low. Normalizing means that we keep the relative values in each column but not the real numbers. In the interpretation sense it means that, for example, the same color of features “Miles per Gallon” and “Number of Cylinders” of Mazda RX4 does not mean that the actual values are the same or approximately the same (placed in the same bin). It only means that the relative values of each of these cells in corresponding columns are the same or are in the same bin.\n* <syntaxhighlight lang=""R"" inline>margins</syntaxhighlight> is used to fit the columns and rows names into the graph. The reason we used it here is because of the renaming of the columns, which is resulted in longer names that did not fit well by themselves.']"|0.015151515151515152|1.0
27|How did Alhazen contribute to the development of scientific methods?|Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.|"[""'''Note:''' The German version of this entry can be found here: [[Scientific methods and societal paradigms (German)]].\n\n'''In short:''' This entry discusses how [[Glossary|scientific methods]] have influenced society - and vice versa.\n__NOTOC__\n== The role of scientific paradigms for society ==\nFrom early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar. However, science was for the most part of the last two millennia in a critical relation to the metaphysical, as there was often a quest for ontological truths between religions and science. While the East was more open to allow science to thrive and made active use of its merits; in Europe, many developments were seen as critical, with Galileo Galileo being a prominent example. Since this changed with the [[History of Methods|Enlightenment]], science paved the way for the rise of the European empires, and with it the associated paradigms.\n\n== Three examples for an active interaction ==\nWhile the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples. \n\n==== Medicine ====\nA prominent example of a strong interaction is medicine, which has at its heart the care for the patient. However, this naive assumption cannot hold the diverse paradigms that influenced and build medicine over time. Today, ananmesis - the information gained by a physician by asking specific questions of a patient - gained in importance, and the interdisciplinary conferences of modern treatments combine different expertise with the goal of a more holistic recognition of the diseases or challenges of the individual patient. \n\n==== Engineering ====\nEngineering is another branch of science which builds on a long tradition, and has at its early stages quite literally paved the road for many developments of modernity. While factories and production processes are today also seen more critically, it has become clear already since Marx that the working condition of modern production are not independent of questions of inequality. In addition, production processes are shifting in order to enable more sustainable production processes, indicating another paradigm shift in engineering. \n\n==== Agricultural science ====\nThe last example, agricultural science, is also widely built around positivistic methodology of modern science, allowing of an optimisation of agricultural production in order to maximise agricultural yield, often with dire consequences. The so-called [https://www.thoughtco.com/green-revolution-overview-1434948 'Green Revolution'] wreaked havoc on the environment, destroyed local livelihoods across the globe, and untangled traditional social-ecological systems into abusive forms that led ultimately to their demise in many parts of the world. \n\nThese three examples showcase how the development of modern science led to abusive, unbalanced, and often unsustainable developments that would in the long run trigger new paradigms such as the post-modernity, degrowths and other often controversially discussed alternatives to existing paradigms. Science was clearly an accomplice in driving many negative developments, and willingly developed the basis for many methodological foundations and paradigms that were seen in a different light after they were utilised over a longer time.\n\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently, science often acted morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational [[Glossary|change]]. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n\n==Science looking at parts of reality==\nSince the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world. The second mistake is that scientific progress can be seen as continuous (see Laudan: Progress and its Problems), while societal paradigms are often utilising snapshots of scientific theories and tend to ignore further development in the respective branch of science. This makes science in turn vulnerable, as it has to claim responsibility for mistakes society made in interpreting scientific theories, and translating them into societal paradigms. In the following message I will illustrate these capital mistakes of science based on several examples. \n\n==== Social Darwinism ====\nThe evolutionary theory of Charles Darwin can be seen as a first example that illustrates how a scientific theory had catastrophic consequences when it was adapted as a societal paradigm. Ideas that the poor in late Victorian England were unworthy of state intervention, and that social welfare was hence a mistake were build on a misunderstanding of Darwins theory, and Darwin opposed the application of his theory for societal debates. Furthermore, he was horrified that his ideas was also taken as a basis to claim superiority of some races over other races, a crude and scientifically wrong claim that paved the road for some of the worst atrocities of the 20th century.""]"|0.0|0.0
28|How can multivariate data be graphically represented?|Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.|"[""'''In short:''' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically '''barplots'''. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.\n\n'''Correlation plots''' ('xyplots') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a [[Regression Analysis|regression]] line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.\n\n'''Boxplots''' are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant.\n\n[[File:Xyplot.png|250px|thumb|left|'''A Correlation plot.''' The line shows the regression, the dots are the data points.]]\n[[File:Boxplot3.png|250px|thumb|right|'''Boxplots.''']]\n[[File:2Barplots.png|420px|thumb|center|'''Barplots.''' The left diagram shows absolute, the right one relative Barplots.]]\n\n\n[[File:Histogram structure.png|300px|thumb|right|'''A Histogram.''']]\nA '''histogram''' is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n\nSimple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|'''A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.''' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|'''A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.''' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|'''A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic's importance in the research, and the connections show if topics were covered alongside each other.''' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n'''Descriptive Infographics''' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.""]"|0.17073170731707318|1.0
29|What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?|Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.|"['Please refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231.\n\n(5) [https://www.vodafone-institut.de/aiandi/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305–306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP’06 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1–12).']"|0.15151515151515152|0.5
30|What are some of the challenges faced by machine learning techniques?|Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.|"['Please refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231.\n\n(5) [https://www.vodafone-institut.de/aiandi/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305–306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP’06 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1–12).']"|0.09090909090909091|1.0
31|What are the characteristics of scientific methods?|Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.|"['\'\'\'This sub-wiki deals with scientific methods.\'\'\' <br/>\n\n=== What are scientific methods? ===\nWe define \'\'Scientific Methods\'\' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either \'\'\'reproducible\'\'\' and \'\'\'learnable\'\'\'; can be \'\'\'documented\'\'\' and are \'\'\'learnable\'\'\'; or are \'\'\'reproducible\'\'\', can be \'\'\'documented\'\'\', and are \'\'\'learnable\'\'\'. \n* From a systematic perspective, methods are approaches that help us \'\'\'gather\'\'\' data, \'\'\'analyse\'\'\' data, and/or \'\'\'interpret\'\'\' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n\'\'\'This Wiki describes each presented method in terms of\'\'\' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki\'s underlying [[Design Criteria of Methods]].<br/>\n\'\'\'This means that each method fulfills one or more categories of each of the following criteria:\'\'\'\n<br/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=""pages"" hideroot=""on"">Methods</categorytree>\n<br>\nWe also have what we call \'\'\'Level 2\'\'\' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* \'\'\'[[Statistics]]\'\'\': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* \'\'\'[[Interviews]]\'\'\': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read.']"|0.11627906976744186|0.3333333333333333
32|What is the main goal of practicing mindfulness?|The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || [[:Category:Productivity Tools|Productivity Tools]] || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Noble Eightfold Path - Mindfulness.png|350px|thumb|right|\'\'\'The Noble Eightfold Path, with Mindfulness being the seventh practice.\'\'\' Source: [https://en.wikipedia.org/wiki/Noble_Eightfold_Path Wikipedia], Ian Alexander, CC BY-SA 4.0]]\nMindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the context of many diverse but often religious schools of thinking.\n\n== Goals ==\nSince the goal of mindfulness is basically having ""no mind"", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free. \n\n== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen – sitting meditation – is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|\'\'\'[https://www.headspace.com/de Headspace] is an app that can help you meditate\'\'\', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around – often referred to as the monkey mind –, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions – just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!\n\n== Links & Further Reading ==\n* [https://www.headspace.com Headspace] \n* [https://www.youtube.com/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https://thichnhathanhfoundation.org/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https://en.wikipedia.org/wiki/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on \'\'Zen Mind, Beginner\'s Mind\'\' is classic introduction to Zen.\n* [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn.']"|0.16666666666666666|1.0
33|How is information arranged in a Mindmap?|In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\'\'\'Mindmapping is a tool for the visual organisation of information\'\'\', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|\'\'\'MindMaps can take the form of trees, with the words on the branches, or clusters/bubbles, as in this example.\'\'\' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https://www.thetutorteam.com/wp-content/uploads/2019/07/shutterstock_712786150.jpg thetutorteam.com]]]\n\n\'\'\'A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.\'\'\' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). \'\'\'This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.\'\'\' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. \'\'\'Elements may be subordinate to other elements, indicating nestedness of the information.\'\'\' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n\'\'Sources:\'\'\n* [https://www.mindmapping.com/de/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http://methodenpool.uni-koeln.de/download/mindmapping.pdf Uni Köln Methodenpool - Mind-Mapping]]\n* [https://kreativitätstechniken.info/problem-verstehen/mindmapping/ Kreativitätstechniken.info - Mindmapping]]\n* [https://www.lifehack.org/articles/work/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https://www.thetutorteam.com/blog/mind-maps-how-they-can-help-your-child-achieve/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.']"|0.13725490196078433|1.0
34|Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?|Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.|"['[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""|\'\'\' [[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\'\'\'In short:\'\'\' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Mixed Effects Models until 2020.\'\'\' Search terms: \'Mixed Effects Model\' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher\'s introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher\'s experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: \'\'\'Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables\'\'\'. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. \'\'\'The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.\'\'\' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n\'\'\'The biggest strength of Mixed Effect Models is how versatile they are.\'\'\' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done \'\'better\'\' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. \'\'\'All positive factors\' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.\'\'\' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.']"|0.0392156862745098|0.0
35|How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?|Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.|"[""Another very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.0|0.0
36|Why should stepwise procedures in model reduction be avoided?|Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.|"['== \'\'\'Starting to engage with model reduction - an initial approach\'\'\' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam\'s razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam\'s razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant.']"|0.058823529411764705|1.0
37|What are the methods to identify redundancies in data for model reduction?|The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.|['The last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find.']|0.34615384615384615|0.0
38|How are 'narratives' used in Narrative Research?|'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.|"['Narrative Research is ""(...) the study of stories"" (Polkinghorne 2007, p.471) and thus ""(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience."" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the \'narratology\', i.e. the structure and grammar of a story; the \'narrative content\', i.e. the themes and meanings conveyed through the story; and/or the \'narrative context\', which revolves around the effects of the story (Squire et al. 2014).\n\n\'\'\'One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.\'\'\' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, \'narrative\' is the underlying ""frame of reference"" (Moen 2006, p.57) for the research. An example for the latter understanding is the \'Narrative Interview\'.\n\n==== Narrative Interviews ====\n[[File:Narrative Research Phases.png|400px|thumb|right|\'\'\'Basic phases of the narrative Interview.\'\'\' Source: Jovchelovitch & Bauer 2000, p.5.]]\n\nThe Narrative Interview is an Interview format that ""(...) encourages and stimulates an interviewee (...) to tell a story about some significant event in their life and social context."" (Jovchelovitch & Bauer 2000, p.3). \'Narrative Interviewing\' ""is considered a form of unstructured, in-depth interview with specific features."" (Jovchelovitch & Bauer 2000, p.4) To this end, it is a form of the [[Open Interview]], which - compared to [[Semi-structured Interview|Semi-structured]] and [[Survey|Structured Interviews]] - is relatively free from deductively pre-developed question schemata: ""To elicit a less imposed and therefore more \'valid\' rendering of the informant\'s perspective, the influence of the interviewer should be minimal (...) \'\'\'The [Narrative Interview] goes further than any other interview method in avoiding pre-structuring the interview.\'\'\' (...) It uses a specific type of everyday communication, namely story-telling and listening, to reach this objective."" (Jovchelovitch & Bauer 2000, p.4) Here, it is central that the interviewer appreciate the interviewee\'s perspective, by using the subject\'s language and by posing ""as someone who knows nothing or very little about the story being told, and who has no particular interests related to it"" (ibid, p.5). The interview is then transcribed and analyzed using different forms of coding (see Content Analysis), with a focus on the narrative elements. For more information on the methodological foundations of conducting and analyzing narrative Interviews, please refer to Jovchelovitch & Bauer 2000.\n\n==== Narrative Inquiry as collaborative story-creation ====\nIn a third understanding of Narrative Research, which is most commonly referred to as \'Narrative Inquiry\', \'narratives\' are more than the underlying phenomenon - they are a central methodological element. \'\'\'\'This approach dissolves the barrier between researcher and subject further, and the collaboration between both is central to the methodological design\'\'\' (Clandinin 2006, Moen 2006). This type of narrative research does not apply an \'outsider\'s perspective\', but instead is ""(...) collaboration between researcher and participants, over time, in a place or series of places, and in social interaction with milieus. An inquirer enters this matrix in the midst and progresses in the same spirit, concluding the inquiry still in the midst of living and telling, reliving and retelling, the stories of the experiences that made up people\'s lives, both individual and social."" (Clandinin 2006, p.20, citing Clandinin & Connelly 2000). To this end, Clandinin (2006) distinguishes between the re-telling of stories that Interview participants tell the researchers, and the telling of stories that the researchers experience themselves, e.g. in ethnographic studies. The difference is not so much of methodological nature as it is in the purpose and perspective of the research (Barrett & Stauffer 2009). In this understanding, \'Narrative Inquiry\' is a reflexive and iterative process, with the researchers entering into a field of experiences, telling their own stories, telling the stories of other participants, and dialogically co-creating joined stories with them (Moen 2006). The created narratives serve as a way of presenting the research experiences, but also as forms of data for the analysis of the joint and conveyed experiences.']"|0.391304347826087|1.0
39|What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?|Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n\'\'\'Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.\'\'\' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution.']"|0.19607843137254902|0.0
40|What are the three conditions under which Poisson Distribution can be used?|Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.|"['[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (λ) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to λ. The Lamda, λ, also represents the mean and variance of the distribution.\n\n==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, λ, its average value is λ.\n\n2. Variance of the distribution is also λ, implying when λ increases the width of the distribution goes as square root lambda √λ.\n\n3. A converse in Raikov’s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.\n\nConsider the example of radioactive decay for long-lived isotopes, in a radioactive sample containing a large number of nuclei, each of which has a tiny probability of decaying during some time interval, T. Let’s say the rate of decay is 0.31 decay/second and is monitored for 10 seconds. That gives us the λ1 0.31 * 10 = 3.1 which means the probability equals,\n[[File:3 equation.PNG|center]]\nConsider another example where λ2 is 2.7, the probability is\n[[File:4 equation.PNG|center]]\n\nTherefore,\n\'\'\'If we look at the total number k = k1 + k2 of a radioactive decay in a time T, the result is also a Poisson distribution with λ = λ1 + λ2 -> λ = 3.1 + 2.7 = 5.8 : P(k, λ1 + λ2 )\'\'\'\n[[File:5 equation.PNG|center]]\n\n\'\'If we have two independent Poisson-distributed variables, their sum is Poisson distributed too.\'\'\n\n4. The skewness is measured by 1/√λ\n\n5. Excess kurtosis is measured by 1/λ. See the difference between excess kurtosis and kurtosis [https://www.investopedia.com/terms/e/excesskurtosis.asp here]. In a nutshell, excess kurtosis compares the kurtosis of the distribution with the kurtosis of a normal distribution and can therefore tell you if an (extreme) event is more or less likely in this case than if the distribution followed a normal distribution.\n\n6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> ∞ and λ = np, rate parameter, λ is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean λ > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = λ = μ =45\n\nStandard deviation = ∂ = √λ = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https://builtin.com/data-science/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=""Python"" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n</syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=""Python"" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n</syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1']"|0.024691358024691357|1.0
41|How does the Pomodoro technique work?|The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don\'t want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n\'\'\'Pomodoro is very simple. All you need is work to be done and a timer.\'\'\'  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to \'\'25 minutes = 1 ""Pomodoro""\'\').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15–30 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https://play.google.com/store/apps/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https://apps.apple.com/us/app/focus-keeper-time-management/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.']"|0.21428571428571427|1.0
42|What is the 'curse of dimensionality'?|The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.|"['The question of model reduction will preoccupy statistics for the next decades, and this development will be interacting with a further rise of Bayes theorem and other questions related to information processing. Time will tell how regressions will emerge on the other side, yet it is undeniable that there is a use case for this specific type of statistical model. Whether science will become better in terms of the theoretical foundations of regressions, in recognising and communicating the restrictions and flaws of regressions, and not overplaying their hand when it comes to the creation of knowledge, is an altogether different story. \n\n\n== Key Publications ==\n\n== References ==\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden and Quentin Lehrer.']"|0.0|0.0
43|Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?|Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.|"['<syntaxhighlight lang=""Python"" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n</syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=""Python"" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n</syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=""Python"" line>\ndata.info()\n</syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (""non-null"") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the ""object"" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For ""sex"" and ""passed"", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=""Python"" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=""Python"" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term ‘residual’ refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=""Python"" line>\nmodel_1 = smf.ols(formula=\'points ~ ID + quanti\', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).']"|0.08433734939759036|0.4782608695652174
44|How did Shell contribute to the advancement of Scenario Planning?|"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."|"[""Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n* A detailed description of how to conduct Scenario Planning, explained through case studies in the advertisement industry.\n\nAmer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n* A (rather complex) overview on different types of Scenario Planning across the literature.\n\nSwart, R.J., Raskin, P., Robinson, J. 2004. ''The problem of the future: sustainability science and scenario analysis.'' Global Environmental Change 14(2). 137-146.\n* A conceptual paper that elaborates on the potential of scenarios in and for sustainability science.\n\n\n== References ==\n(1) Wack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n\n(2) Schoemaker, P.J.H. 1993. ''Multiple Scenario Development: Its Conceptual and Behavioral Foundation.'' Strategic Management Journal 14(3). 193-213.\n\n(3) Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n\n(4) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design Journal 20(1). 1916-1926.\n\n(5) Amer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n\n(6) Wiek et al. 2006. ''Functions of scenarios in transition processes.'' Futures 38(7). 740-766.\n\n(7)  Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n\n== Further Information ==\n* Shell works with Scenarios still today. [[On this websitehttps://www.shell.com/energy-and-innovation/the-energy-future/scenarios.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL3NjZW5hcmlvcy5odG1s|On this website]], you find a lot of information about their scenario approach.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.""]"|0.0|0.0
45|Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?|Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.|"['[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n|\'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n\'\'\'In short:\'\'\' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Social Network Analysis until 2019.\'\'\' Search terms: \'Social Network Analysis\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel\'\'\'. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued ""against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions."" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that ""(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. \'\'\'Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.""\'\'\' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as ""entitites"", ""actors"", ""vertices"" or ""agents"" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are ""(...) tied to one another through socially meaningful relations"" (Prell et al. 2009, p.503), which can be ""(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection"" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.\n\n[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|\'\'\'Types of Ties in a Social Network.\'\'\' Source: Borgatti et al. 2009, p.894]]']"|0.14634146341463414|1.0
46|What are the limitations of Stacked Area Plots?|Stacked Area Plots are not suitable for studying the evolution of individual data series.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nThis entry aims to introduce Stacked Area Plot and its visualization using R’s <syntaxhighlight lang=""R"" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> has the following syntax:\n\n\'\'\'Syntax\'\'\': <syntaxhighlight lang=""R"" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=""R"" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n</syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=""R"" inline>""uspopage""</syntaxhighlight> using the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> from the <syntaxhighlight lang=""R"" inline>ggplot2 package</syntaxhighlight>:\n\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n</syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of “Blues”, and add labels.\n\n<syntaxhighlight lang=""R"" line>\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = ""black"", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = ""Blues"")+\n  labs(title = ""US Population by Age"", \n       subtitle = ""Between 1900 and 2000"",\n       x = ""Year"",\n       y = ""Population (Thousands)"")\n</syntaxhighlight>\n\n==References==\n\n* R Graphics Cookbook, 2nd edition by Winston Chang\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Maria Jose Machuca.']"|0.03125|0.0
47|What is the purpose of Thought Experiments?|"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."|"['[[File:Thought Experiment Concept Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Thought Experiments]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.\n\n== Background ==\n[[File:Scopus hits Thought Experiment.png|450px|thumb|right|\'\'\'SCOPUS hits per year for Thought Experiments until 2019.\'\'\' Search term: \'thought experiment\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'The Thought Experiment may well be the oldest scientific method.\'\'\' The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the \'\'What if\'\' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early [[History of Methods|enlightenment]]. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the \'\'Origin of Species\'\' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term ""Thought Experiment"", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.\n\n== What the method does ==\nThought Experiments are the philosophical method that asks the ""What if"" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word ""experiment"" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.  \n\nThe easiest way to set up a Thought Experiment is to ask yourself a ""What if"" question. Many Thought Experiments resolve around decisions, choices or potential states of the future. A famous example is the Trolley experiment, where a train rolls towards five train track workers, who would all be killed be the oncoming train, unaware of their potential demise. You can now change the direction of the train, and lead it to another track. There, one worker would be killed. Uncountable numbers of variations of this experiment exist (see Further Information), as it can teach much about choice, ethics, and responsibility. For instance, many people would change the train direction, but hardly anyone would push a person onto the track to derail the train. This would save the other five, and the outcome would be the same. However the difference between pushing a lever or pushing a person has deep psychological ramifications that resolve around guilt. This exemplifies that the Thought Experiment does not necessarily have a \'\'best\'\' outcome, as the outcome depends - in this example - on your moral choices. Some might argue that you should hurl yourself onto the track to stop the train, thereby not changing the countable outcome, but performing a deeply altruistic act that saves everybody else. Most people would probably be unable to do this. \n\n\'\'\'Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world.\'\'\' Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schrödinger\'s Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences.']"|0.23809523809523808|1.0
48|What is temporal autocorrelation?|Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.|"['The autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df[\'usage_kwh\'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel(\'Lag (hours)\') ## title x axis\nplt.ylabel(\'Autocorrelation of electricity usage with confidence interval\') ## title y axis\nplt.title(\'\') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n</syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days</small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered “close” to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object\'s neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object\'s neighborhood is significantly lower than the densities of its neighbors\' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don\'t know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df[\'usage_kwh\'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (""to_numpy()"") and reshapes it to a single column (""reshape (-1, 1)""), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf[\'outlier\'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df[\'outlier\']] ## creates  a column where only outliers are selected\n\nplt.plot(df[\'usage_kwh\']) ## plots the dataframe\nplt.scatter(x=outliers.index, y=outliers[\'usage_kwh\'], color=\'tab:red\', marker=\'x\') ## creates a scatter plot with the outliers, marked with a red x\nplt.title(\'Outliers in the dataset (marked in red)\') ## title of the plot\nplt.xlabel(\'Date\') ## titlex axis\nplt.ylabel(\'Usage (KWh)\')## title y axis\nplt.show()\n</syntaxhighlight>\n[[File:outlier plot.png|700px|center|]]\n<small>Figure 7: Outlier in the chosen part of the dataset (marked with red cross)</small>\n\nIn this case, these are probably false-positive outliers. The dataset is already pretty clean and does likely not contain many outliers.\n\nThe imputation of missing values is the next challenge after detecting outliers. The naive solution to this problem is to replace a missing point with the mean of all points. While this approach will not skew the data distribution significantly, the imputed values might be far off from the actual value and make messy graphics. An alternative is applying regression models that use predictive models to estimate missing points.\n\nA combined approach to both detects and replaces outliers is the Kalman Filter, which predicts new points based on the previous points and estimates a noise measure for new points. Thus, when an anomaly is detected, it is replaced by the prediction from the model using historical data (Hoogendoorn & Funk 2018).\n\n===Forecasting Time Series Data===\n\nForecasting time series data can be of tremendous value in information gain. Since this is a large topic, this article will only touch on one method for forecasting: AutoRegressive Integrated Moving Average models, or ARIMA for short. It consists of three parts:']"|0.0|0.0
49|What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?|The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models.|"['To illustrate the choice of methods in transdisciplinary research, how the selected methods can be combined, and how they relate to agency, see one of the following examples.\n\n\n== Examples ==\n[[File:Besatzfisch.png|450px|thumb|center|The research project ""Besatzfisch"". [http://besatz-fisch.de/content/view/90/86/lang,german/  Source]]]\n* The research project [http://besatz-fisch.de/content/view/34/57/lang,german/ ""Besatzfisch""] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to \'\'\'understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.\'\'\' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (\'\'Cooperation\'\'). Second, anglers were questioned about fish population sizes and their economic implications (\'\'Consultation\'\') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (\'\'Consultation\'\'). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (\'\'Empowerment\'\'). Fifth, social-ecological models were developed based on the previous empirical results. (\'\'Consultation\'\') Sixth, the project results are published in different forms of media for different target groups (\'\'Information\'\').\n\n* Another interesting example is the article [https://www.ecologyandsociety.org/vol23/iss2/art9/ ""Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau""]. In order to \'\'\'assess potential future social-ecological developments in Mongolia\'\'\', researchers first held a workshop with stakeholders, mostly from academia but with diverse disciplinary backgrounds, and a few non-scientific stakeholders. In this workshop, first, key elements that influence sustainable development in the region were identified. Step by step, these were then ranked to identify critical uncertainties, which led to the development of potential future scenarios (\'\'Consultation\'\'). Also, the stakeholders\' opinions on the workshop were later assessed through interviews, indicating positive impacts on their work and perspectives (\'\'Empowerment\'\'). The insights from the workshops were translated into system dynamics models by the researchers that were iteratively feedbacked by the stakeholders (\'\'Consultation\'\'), which led to a final model (\'\'Information\'\'). This approach was not purely transdisciplinary but illustrates a transdisciplinary workflow.\n\n* In the article ""[https://www.researchgate.net/publication/329789267_Developing_an_optimized_breeding_goal_for_Austrian_maternal_pig_breeds_using_a_participatory_approach Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach]"" the authors describe a \'\'\'participatory approach conducted in order to develop new indicators (traits) for animal health in pig breeding in Austria\'\'\'. First, propositions for new indicators to revise current policy were collected by the directors of the coalition of pig breeders, as well as in a science-led workshop with breeders and staff of the coalition (\'\'Consultation\'\'). The results were assessed based on a literature study and academic consultation. Then, the results were communicated back for feedback, tested on farms, and refined in further transdisciplinary workshops (\'\'Consultation\'\'). Next, pig breeders were asked and trained to record the new traits for one year (\'\'Collaboration\'\') in a [[Citizen Science]]-like approach. This process showed to improve the quality of research data while generating new knowledge and skills for the breeders (\'\'Empowerment\'\'). The gathered data was feedbacked by the scientists and was set to lead to policy recommendations (\'\'Information\'\').\n\n\n== Open questions and future directions ==\n[[File:Ivory Tower.jpg|300px|thumb|right|\'\'\'The proverbial Ivory Tower is challenged through transdisciplinary research.\'\'\' Source: [http://sciblogs.co.nz/app/uploads/2017/03/academias-ivory-tower.jpg Sciblogs].]]\nThe future of transdisciplinary research is promising. However, major hurdles still need to be overcome. These include\n* the question how willing academia is to leave the proverbial \'ivory tower\' and get in contact with political, economic and societal actors\n* the need for more experiences and practical guidance on how the aforementioned challenges in the conduction of transdisciplinary research can be solved (integration of knowledge, balancing of interests and methods, communication and mutual learning, joint development of solutions)\n* a lack of educational approaches that support transdisciplinary thinking and working\n\n""As the world’s natural resources dwindle and the climate changes, the need for sustainability research will increase—as will the need for both integrated research and clear frameworks for conducting such research. Research on sustainability is vital to our collective interests and will require more and more collaboration across various boundaries. The more clearly we can articulate the bridges between those boundaries, the easier those novel collaborations will be."" (Stock & Burton 2011, p.1106)\n\n\n== Key Publications ==\nLang et al. 2012. \'\'Transdisciplinary research in sustainability science: practice, principles, and challenges.\'\'\n\nDefila, R. Di Giulio, A. (eds). 2018. \'\'Transdisziplinär und transformativ forschen. Eine Methodensammlung.\'\' Springer VS.\n\nBrandt et al. 2013. \'\'A review of transdisciplinary research in sustainability science.\'\' Ecological Economics 92. 1-15.\n\nGAIA Special Episode \'\'Labs in the Real World - Advancing Transdisciplinarity and Transformations\'\'. \n\nGibbons, M. (ed.) 1994. \'\'The new production of knowledge: The dynamics of science and research in contemporary societies.\'\' SAGE.\n\nWiek, A. and Lang D.J., 2016.Transformational Sustainability Research Methodology” in Heinrichs, H. et al. (eds.), 2016. Sustainability Science, Dordrecht: Springer Netherlands. Available at: http://link.springer.com/10.1007/978-‐94-‐017-‐7242-‐62.   \n\n\n== References ==\n* Lang et al. 2012. \'\'Transdisciplinary research in sustainability science: practice, principles, and challenges\'\'.\n\n* Kates et al. 2015. \'\'Sustainability Science\'\'.']"|0.017241379310344827|1.0
