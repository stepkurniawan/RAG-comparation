{"question":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA a powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is the purpose of ANCOVA in statistical analysis?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram and how is it created?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"Reducing variance in field experiments or complex laboratory experiments","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA is used to compare group means while controlling for the effect of a covariate.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"contexts":{"0":["In the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https:\/\/data.library.virginia.edu\/the-wilcoxon-rank-sum-test\/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A\/B Testing==\n'''Advantages'''\nA\/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A\/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A\/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A\/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. \u201cOnline Controlled Experiments and A\/B Testing.\u201d Encyclopedia of Machine Learning and Data Mining, 2017, 922\u201329. https:\/\/doi.org\/10.1007\/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. \u201cExperimentation and Start-up Performance: Evidence from A\/B Testing.\u201d Management Science 68, no. 9 (September 2022): 6434\u201353. https:\/\/doi.org\/10.1287\/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A \/ B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart","'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==A\/B Testing in a nutshell==\nA\/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A\/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A\/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n'''For example:'''\n\n\"To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.\"\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\n===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A\/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.\n\nThe sample size is determined by the percentage of users included in the test variants (control and treatment) and the duration of the experiment. As the experiment runs for a longer period of time, more visitors are exposed to the variants, resulting in an increase in the sample size. Because many external factors vary over time, it is important to randomize over time by running the control and treatment variants simultaneously at a fixed percentage throughout the experiment. Thereby the goal is to obtain adequate statistical power, where the statistical power of an experiment is the probability of detecting a particular effect if it exists. In practice, one can assign any percentages to the control and treatment, but 50% gives the experiment maximum statistical power.\n\nFurthermore, it is important to analyze only the subset of the population\/users that were potentially affected. For example, in an A\/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.\n\n==Execution Phase==\nThe execution phase involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan. During this phase, users are randomly assigned to the control or treatment group ensuring that the study is conducted in a controlled and unbiased manner.\n\n==Evaluation Phase==\nThe evaluation phase involves analyzing the data collected during the study and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about whether there was a statistical significant difference between the treatment group and the control group. One commonly used method is calculating the [[Designing_studies#P-value|p-value]] of the statistical test, or by using [https:\/\/www.youtube.com\/watch?v=9TDjifpGj-k Bayes' theorem] calculating the probability that the treatment had a positive effect based on the observed data and the prior beliefs about the treatment.\n\nDepending on the type of data being collected different [[Simple Statistical Tests|statistical tests]] should be considered. For example, when dealing with discrete metrics such as click-through rate, the [https:\/\/mathworld.wolfram.com\/FishersExactTest.html Fisher exact test] can be used to calculate the exact p-value, while the [https:\/\/www.youtube.com\/watch?v=7_cs1YlZoug chi-squared test] may be more appropriate for larger sample sizes.\n\nIn the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https:\/\/data.library.virginia.edu\/the-wilcoxon-rank-sum-test\/ Wilcoxon rank sum test] may be more appropriate.","'''In short:''' T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on two-sample T-tests and covers the concept and purpose of the t-test, underlying assumptions, its implementation in R, as well as multiple variants for different conditions. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]].\n\n==General Information==\nLet us start by looking at the basic idea behind a two-tailed two-sample t-test. Conceptually speaking we have two hypotheses. \n\nH<sub>0<\/sub>: Means between the two samples do not differ significantly.<br\/>\nH<sub>1<\/sub>: Means between the two samples do differ significantly. <br\/>\n\nIn mathematical terms (\u03bc<sub>1<\/sub> and \u03bc<sub>2<\/sub> denote the mean values of the two samples): \n\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> = \u03bc<sub>2<\/sub> <br>\nH<sub>1<\/sub>: \u03bc<sub>1<\/sub> \u2260 \u03bc<sub>2<\/sub>. \n\n[[File:Kurt_olaf.jpg|500px|frameless|right]]\n'''Here is an example to illustrate this.''' The farmers Kurt and Olaf grow sunflowers and wonder who has the bigger ones. So they each measure a total of 100 flowers and put the values into a data frame.\n\n<syntaxhighlight lang=\"R\" line>\n# Create a dataset\nset.seed(320)\n\nkurt <- rnorm(100, 60.5, 22)\nolaf <- rnorm(100, 63, 23)\n<\/syntaxhighlight>\n\nOur task is now to find out whether means values differ significantly between two groups.\n<syntaxhighlight lang=\"R\" line>\n# perform t-test\nt.test(kurt, olaf)\n\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.1275\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -11.97670   1.50973\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n<\/syntaxhighlight>\n\nSo now, we performed a t-test and got a result. '''But how can we interpret the output?''' <br\/>\nThe criterion to consult is the p-value. This value represents the probability of the data given that H0 is actually true. Hence, a low p-value indicates that the data is very unlikely if H0 applies. Therefore, one might reject this hypothesis (in favor of the alternative hypothesis H1) if the p-value turns out to be below a certain threshold (\u03b1, usually set prior testing), which is often set to 0.05 and usually not larger than 0.1. <br\/>\n\nIn this case, the p-value is greater than 0.1. Therefore, the probability of H0 is considered to be \u201ctoo large\u201d to reject this hypothesis. Hence, we conclude that means do not differ significantly, even though we can say that descriptively the sample mean of Olaf\u2019s flowers is higher.\n<br\/>\nThere are multiple options to fine-tune the t-test if one already has a concrete hypothesis in mind concerning the direction and\/or magnitude of the difference. In the first case, one might apply a one-tailed t-test. The hypotheses pairs would change accordingly to either of these: <br>\n\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> \u2265 \u03bc<sub>2<\/sub><br>\nH<sub>1<\/sub>: \u03bc<sub>1<\/sub> < \u03bc<sub>2<\/sub>\n<br>\nor\n<br>\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> \u2264 \u03bc<sub>2<\/sub>\n<br>H<sub>1<\/sub>: \u03bc<sub>1<\/sub> > \u03bc<sub>2<\/sub>\n\nNote that the hypotheses need to be mutually exclusive and H0 always contains some form of equality sign. In R, one-tailed testing is possible by setting '''alternative = \"greater\"''' or '''alternative = \"less\"'''. Maybe Olaf is the more experienced farmer so we have already have \u00e0 priori the hypothesis that his flowers are on average larger. This would refer to our alternative hypothesis. The code would change only slightly:\n\n<syntaxhighlight lang=\"R\" line>\nt.test(kurt, olaf, alternative = \"less\")\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.06373\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##       -Inf 0.4172054\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n<\/syntaxhighlight>\n\nAs one can see, the p-value gets smaller, but it is still not below the \u201cmagic threshold\u201d of 0.05. The question of how to interpret this result might be answered differently depending on whom you ask. Some people would consider the result \u201cmarginally significant\u201d or would say that there is \u201ca trend towards significance\u201d while others would just label it as being non-significant. In our case, let us set \u03b1\n = 0.05 for the following examples and call a result \u201csignificant\u201d only if the p-value is below that threshold."],"1":["====Analysis of Variance====\n'''The [https:\/\/www.investopedia.com\/terms\/a\/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment. \n\nThe general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.\n\n====Preconditions====\nRegarding the preconditions of the [https:\/\/www.youtube.com\/watch?v=oOuu8IBd-yo ANOVA], it is important to realize that the data should ideally be '''normally distributed''' on all levels, which however is often violated due to small sample sizes. Since a non-normal distribution may influence the outcome of the test, boxplots are a helpful visual aid, as these allow for a simple detection tool of non-normal distribution levels. \n\nEqually should ideally the variance be comparable across all levels, which is called '''[https:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia homoscedastic]'''. What is also important is the criteria of '''independence''', meaning that samples of factor levels should not influence each other. For this reason are for instance in ecological experiments plants typically planted in individual pots. In addition does the classical ANOVA assume a '''balanced design''', which means that all factor levels have an equal sample size. If some factor levels have less samples than others, this might pose interactions in terms of normals distribution and variance, but there is another effect at play. Larger sample sizes on one factor level may create a disbalance, where factor levels with larger samples pose a larger influence on the overall model result. \n\n====One way and two way ANOVA====\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https:\/\/www.thoughtco.com\/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a '''[https:\/\/www.statisticshowto.com\/post-hoc\/ posthoc test]'''. A prominent example is the [https:\/\/sciencing.com\/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https:\/\/www.statisticshowto.com\/post-hoc\/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.\n\n====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong.","====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong. \n\nFurthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the deductive world become intertwined, and this poses a risk for the validity of scientific results.\n\nFor more on the Analysis of Variance, please refer to the [[ANOVA]] entry.\n\n==Examples==\n[[File:Guinea pig computer.jpg|thumb|right|The tooth growth of guinea pigs is a good R data set to illustrate how the ANOVA works]]\n===Toothgrowth of guinea pigs===\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the ToothGrowth data set is about: ?ToothGrowth\n\n#The code is partly from boxplot help (?boxplot). If you like to know the meaning of the code below, you can look it up there\ndata(ToothGrowth)\n\n# to create a boxplot \nboxplot(len ~ dose, data = ToothGrowth,\n                           boxwex = 0.25, \n                           at = 1:3 - 0.2,\n                           subset = supp == \"VC\", col = \"yellow\",\n                           main = \"Guinea Pigs\u2019 Tooth Growth\",\n                           xlab = \"Vitamin C dose mg\",\n                           ylab = \"tooth length\",\n                           xlim = c(0.5, 3.5), ylim = c(0, 35), yaxs = \"i\")\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n                           boxwex = 0.25,\n                           at = 1:3 + 0.2,\n                           subset = supp == \"OJ\", col = \"orange\")\nlegend(2, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c (\"yellow\", \"orange\"))\n\n#to apply an ANOVA\nmodel1<-aov(len ~ dose*supp, data = ToothGrowth)\nsummary(model1) \n#Interaction is significant\n<\/syntaxhighlight>\n\n====Insect sprays====\n[[File:A man using RIPA insecticide to kill bedbugs Wellcome L0032188.jpg|thumb|right|To find out, which insectide works effictively, you can approach an ANOVA]]\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the InsectSprays data set is about: ?InsectSprays\ndata(InsectSprays)\nattach(InsectSprays)\ntapply(count, spray, length)\nboxplot(count~spray) \n# can you guess which sprays are effective by looking at the boxplot?\n# to find out which sprays differ significantly without applying many t-tests, you can use a postdoc test\nmodel2<-aov(count~spray)\nTukeyHSD(model2)\n# compare the results to the boxplot if you like\n<\/syntaxhighlight>\n\n==Balanced vs. unbalanced designs==\nThere is such a thing as a perfect statistical design, and then there is reality.\n\nStatistician often think in so called balanced designs, which indicate that the samples across several levels were sampled with the same intensity. Take three soil types, which were sampled for their agricultural yield in a ''mono crop''. Ideally, all soil types should be investigated with the same amount of samples. If we would have three soil types -clay, loam, and sand- we should not sample sand 100 times, and clay only 10 times. If we did, our knowledge about sandy soils would be much higher compared to clay soil. This does not only represent a problem when it comes to the general knowledge, but also creates statistical problems. \n\nFirst of all, the sandy soil would be represented much more in an analysis that does not or cannot take such an unbalanced sampling into account. \n\nSecond, many analysis have assumptions about a certain statistical distribution, most notably the normal distribution, and a smaller sample may not show a normal distribution, which in turn may create a [[Bias in statistics|bias]] within the analysis. In order to keep this type of bias at least constant across all levels, we either need a balanced design, or use an analysis that compensates for such unbalanced designs. This analysis was realised with so called Type III ANOVA, which can take different sampling intensities into account. Type III ANOVA corrects for the error that is potentially inferred due to differing sample density, that means number of samples per level.\n\n'''Why is this relevant, you ask?''' \n\nBecause the world is messy. Plants die. So do animals. Even people die. It is sad. For a statistician particularly because it makes one miss out on samples, and dis-balances your whole design. And while this may not seem like a bad problem for laypeople, for statistician it is a real mess. Therefore the ways to deal with unbalanced designs were such a breakthrough, because they finally allowed the so neatly thinking statisticians to not only deal with the nitty-gritty mess of unbalanced designs, but with real world data.","'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions."],"2":["[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|'''SCOPUS hits per year for Bayesian Inference until 2020.''' Search terms: 'Bayesian' in Title, Abstract, Keywords. Source: own.]]\n'''The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.''' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes' name only became associated with the method in the 1900s (3).\n\n'''The family of methods based on the concept of Bayesian analysis has risen the last 50 years''' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n'''Bayesian analysis relies on using probability figures as an expression of our beliefs about events.''' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is \"prior\", and the probability figure assigned to the prior is called \"prior probability\". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called \"posterior probability\", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|'''The probability distribution for prior, evidence, and posterior.''']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes' Theorem.\n\n==== Bayes' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes' theorem.\n\n'''Conditional Probability''' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n'''Joint Probability''', also called \"conjoint probability\", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n'''Marginal Probability''' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for \"Bayes theorem\" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n''Caution:'' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n''Note:'' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let's see how this formulation of Bayes' Theorem can be applied in a simple coin toss example:\n\n\n=== '''Example I: Classic coin toss''' ===\n'''Imagine, you are flipping 2 fair coins.''' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)","[[File:Cube-1655118 1280.jpg|thumb|The most common example explaining probability is rolling the dice.]]\n'''[https:\/\/www.youtube.com\/watch?v=uzkc-qNVoOk Probability] indicates the likelihood whether something will occur or not.''' Typically, probabilities are represented by a number between zero and one, where one indicates the hundred percent probability that an event may occur, while zero indicates an impossibility of this event to occur. \n__NOTOC__\n[https:\/\/www.britannica.com\/science\/probability\/Risks-expectations-and-fair-contracts The concept of probability goes way back] to Arabian mathematicians and was initially strongly associated with cryptography. With rising recognition of preconditions that need to be met in order to discuss probability, [[Glossary|concepts]] such as evidence, validity, and transferability were associated with probabilistic thinking. Probability plays also a role when it came to games, most importantly rolling dice. With the rise of the Enlightenment many mathematical underpinnings of probability were explored, most notably by the mathematician Jacob Bernoulli.\n\n'''Gauss presented a real breakthrough, due to the discovery of the normal distribution.''' It allowed the feasible approach to link sample size of observations with an understanding of the likelihood how plausible these observations were. Again building on Sir Francis Bacon, the theory of probability reached its final breakthrough once it was applied in statistical hypothesis testing. It is important to notice that this would throw modern statistics into an understanding through the lens of so-called [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ frequentist statistics]. This line of thinking dominates up until today, and is widely built on repeated samples to understand the distribution of probabilities across a phenomenon. \n\n[[File:Bildschirmfoto 2020-04-15 um 17.18.08.png|thumb|Another simple example for calculating probability which you have probably also discussed in school is flipping a coin. Here there are only two options: head or tail.]]\n\n'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while [https:\/\/365datascience.com\/bayesian-vs-frequentist-approach\/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this theory explored deeply, and builds the foundation of branches in data science and machine learning. The two approaches are also often coined as objectivists for frequentist probability fellows, and subjectivists for folllowers of [https:\/\/www.youtube.com\/watch?v=9TDjifpGj-k Bayes theorem]. \n\nAnother perspective on the two approaches can be built around the question whether we design studies - or whether we base our analysis on the data we just have. This debate is the basis for the deeply entrenched conflicts you have in statistics up until today, and was already the basis for the conflicts between Pearson and Fisher. From an epistemological perspective, this can be associated with the question of inductive or [[Glossary|deductive reasoning]], although not many statisticians might not be too keen to explore this relation deeply, since they are often stuck in either deductive or inductive thinking, but not both. \n\n'''While probability today can be seen as one of the core foundations of statistical testing, probability as such is increasingly criticised.''' It would exceed this chapter to discuss this in depth, but let me just highlight that without understanding probability, much of the scientific literature building on quantitative methods is hard to understand. What is important to notice, is that probability has trouble considering [https:\/\/sustainabilitymethods.org\/index.php\/Why_statistics_matters#Occam.27s_razor Occam's razor]. This is related to the fact that probability can deal well with the chance of an event to a occur, but it widely ignores the complexity that can influence such a likeliness. Modern statistics explore this thought further but let us just realise here: without learning probability we would have trouble reading the contemporary scientific literature.\n\n[[File:Bildschirmfoto 2020-04-17 um 15.40.02.png|500px|thumb|The GINI coefficient is a good example for a measure which compares the income distribution of different countries.]]\n\nThe probability can be best explained with the [https:\/\/www.stat.colostate.edu\/~vollmer\/stat307pdfs\/LN4_2017.pdf normal distribution]. The normal distribution basically tells us through probability how a certain value will add to an array of values. Take the example of the height of people, or more specifically people who define themselves as males. Within a given population or country, these have an average height. This means in other words, that you have the highest chance to have this height when you are part of this population. You have a slightly lower chance to have a slightly smaller or larger height compared to the average height. And you have a very small chance to be much smaller or much taller compared to the average. In other words, your probability is small to be very tall or very small. Hence the distribution of height follows a normal distribution, and this normal distribution can be broken down into probabilities. In addition, such a distribution can have a variance, and these variances can be compared to other variances by using a so called [https:\/\/www.youtube.com\/watch?v=FlIiYdHHpwU f test]. Take the example of height of people who define themselves as males. Now take the people who define themselves as females from the same population and compare just these two groups. You may realise that in most larger populations these two are comparable. This is quite relevant when you want to compare the income distribution between different countries. Many countries have [http:\/\/www.oecd.org\/statistics\/compare-your-income.htm different average incomes], but the distribution across the average as well as the very poor and the filthy rich can still be compared. In order to do this, the [http:\/\/www.sthda.com\/english\/wiki\/f-test-compare-two-variances-in-r f-test] is quite helpful.","== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future. \n\n\n== Key Publications ==\n* Bayes, T. 1997. LII. ''An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S''. Phil. Trans. R. Soc. 53 (1763). 370\u2013418.\n* Box, G. E. P., & Tiao, G. C. 1992. ''Bayesian Inference in Statistical Analysis.'' John Wiley & Sons, Inc.\n* de Finetti, B. 2017. ''Theory of Probability''. In A. Mach\u00ed & A. Smith (Eds.). ''Wiley Series in Probability and Statistics.'' John Wiley & Sons, Ltd.\n* Kruschke, J.K., Liddell, T.M. 2018. ''Bayesian data analysis for newcomers.'' Psychon Bull Rev 25. 155\u2013177.\n\n\n== References ==\n(1) Jeffreys, H. 1973. ''Scientific Inference''. Cambridge University Press.<br\/>\n(2) Spiegelhalter, D. 2019. ''The Art of Statistics: learning from data''. Penguin UK.<br\/>\n(3) Downey, A.B. 2013. ''Think Bayes: Bayesian statistics in Python''. O'Reilly Media, Inc.<br\/>\n(4) Donovan, T.M. & Mickey, R.M. 2019. ''Bayesian statistics for beginners: A step-by-step approach.'' Oxford University Press.<br\/>\n(5) Kurt, W. 2019. ''Bayesian Statistics the Fun Way: Understanding Statistics and Probability with Star Wars, LEGO, and Rubber Ducks.'' No Starch Press.\n\n\n== Further Information ==\n* [https:\/\/library.wur.nl\/frontis\/bayes\/03_o_hagan.pdfBayesian statistics: principles and benefits]\n* [https:\/\/www.youtube.com\/watch?v=HZGCoVF3YvM 3Blue1Brown: Bayes' Theorem]\n* [https:\/\/www.youtube.com\/watch?v=SrEmzdOT65s Basic Probability: Joint, Marginal, and Conditional Probability]\n* [https:\/\/www.youtube.com\/watch?v=9TDjifpGj-k Crash Course Statistics: Examples of Bayes' Theorem being applied.]\n* [https:\/\/www.youtube.com\/watch?v=TVq2ivVpZgQ Monty Hall Problem: D!NG]\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Prabesh Dhakal and Henrik von Wehrden."],"3":["====4. Build on the available literature.====\n[[File:Caspar David Friedrich - Der Wanderer \u00fcber dem Nebelmeer.jpg|thumb|right|Although the \"Wander \u00fcber dem Nebelmeer\" does not literally stand on the shoulders of giants, the painting of Caspar David Friedrich is a good visualisation of the metaphor.]]\nNo case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new. The more you stand on the shoulders of giants, the further you can see. We need to realize that science is build on failure and iteration. Only by knowing about previous failures but also successes, and only by committing to the canon of knowledge, can we contribute and create new knowledge.\n\n\n==Natural experiments and statistics - a rough guide==\nSeveral layers of information are to be considered when linking statistics to natural experiments. Contextualising natural experiments can be structured into at least 4 steps.\n\n1) '''How is the case connected and embedded in the global context?'''\nWhat are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems. We hence acknowledge that this contextualisation is place based, and places have boundaries and borders. While it is way beyond this text to discuss this issue, the question how a place is bound can be a source of great confusion. A good rule of thumb from a statistical standpoint can be the idea that a place does only exists if its within variance, regarding central variables, is lower than the variance between the place and its neighbouring places.\n\n2) '''what are imbalances and dispersions within the case, both spatially and temporally?'''\nIn other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social context of the case's dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly. \n\n3) '''How are the livelihoods within the case?'''\nWhile many would connect this to the last point, it may be worthwhile to consider this on its own. Livelihoods are what defines groups of people and their possibility to thrive. While this is equally bound to [[Glossary|culture]] and justice, it repeatedly proves that it is much more than the mere sum of the two. Livelihoods are often characterised by a deeply qualitative manifestation, which is where statistics need be to aware of its limitations. While statistics can compile descriptive information on a community and tackle some of the interactions within a community, it has to fail considering other points. This limitation is important to realise.\n\n4) '''what is the intervention of the natural experiment within the case?'''\nHere, a whole world of interventions could be described. Instead, let us just settle on what was the active part of the researcher: systematically compare the state before the intervention with the state after the intervention.\n\n==Experiments in sustainability science==\n[[File:World 3 model.jpg|600px|thumb|right|'''The world 3 model was part of the \"Limits to Growth\" report of the Club of Rome.''' The picture creates a sense of the multitude and complexity of variables that influence a single case study in the real world.]]\nExperiments in sustainability science are characterised by an intervention and the production of empirical evidence. These two key criteria are essential for experiments in sustainability science, which can be in addition also differentiated into a problem focused and a solution orientated focus. Nevertheless, problem orientated studies with full control over all variables (except for the one(s) being investigated) are still clearly timely in sustainability science, as many experiments conducted in ecology prove. Less control over variables is equally important, and again ecology, agricultural science but also experiments in psychology come to mind. Problem orientated focus with no control over variables is at the heart of a joined problem framing of [[Glossary|transdisciplinary]] research in sustainability science, and such an approach also characterised the first Club of Rome report. Solution orientated focus with total or some control over variables is insofar a remarkable step since it marks a shift from the still dominating [[Glossary|paradigm]] of research where facts are only approximated. Since solutions may be compromises but can be achieved, a solution orientated research indicates a shift from more classical lines of thinking. The solution orientated approach with no control over variables marks the frontier of research, since it is clearly long term thinking. Prominent efforts are the IPBES, the sustainable development goal measures, and the current efforts of the rising number of transdisciplinary projects. While these are starting points, I consider these efforts to be a clear sign of hope, not only for out planet, but likewise for the development of science out of its dogmatic slumber.\n\n==Meta-analysis: Integrating cases towards holistic understanding==\nOut of the increasing number of scientific studies with a comparable design, an international school of thinking emerged: [http:\/\/meta-evidence.co.uk\/difference-systematic-review-meta-analysis\/ Meta-analysis]! Within meta-analytical frameworks, great care is taken to combine several studies to investigate their overall outcome. Rooted deeply in medicine and psychology, meta-analysis used the statistical power of studies that show a similar design and setting. Since some studies are larger while others build on smaller samples, meta-analysis are able to take some of these differences into account.\n\nIt is however important to understand the thinking behind meta-analysis, as it helps to create supra-knowledge about certain questions that were enough in the focus so that a number of integrable studies is available. Graphical overviews show summaries of effects, and so called random factors allow for the correction of different sample sizes and other factors. Beyond the disciplines where meta analysis were initially established, this line of thinking found its way into ecology, conservation, agriculture and many other arenas of science that investigate building on deductive designs. To this end, challenges often arise out of field experiments where less variables are being controlled for or are being understood, and hence comparisons may not be valid or at least less plausible.","Klarer ist, dass sich Agency weitgehend um komplexe Versuchsaufbauten dreht, die verschiedene Wissenschaftsbereiche wie Neurowissenschaften und Psychologie, Psychologie und Mikro\u00f6konomie oder Verhaltenswissenschaften und Zoologie verbinden. Abgesehen von unseren unterschiedlichen Theorien zur Handlungsm\u00e4chtigkeit erkennen wir erst jetzt die Herausforderungen im Zusammenhang mit ihrer Messung sowie die M\u00f6glichkeiten zur \u00dcberbr\u00fcckung verschiedener Wissenschaftsbereiche vollst\u00e4ndig an. Vom empirischen Standpunkt aus betrachtet, wurde bisher kein systematischer Zugang zu Wirkungsmechanismen gefunden, weshalb einigen wenigen Studien gro\u00dfe Bedeutung beigemessen wird und bestehende Denkschulen oft auf unterschiedlichen Pr\u00e4missen aufbauen. Ein gutes Beispiel f\u00fcr den Zugang zu Agency ist zum Beispiel die Kombination psychologischer oder verhaltensorientierter Experimente mit neurologischen Untersuchungen, obwohl selbst solche komplizierten Versuchsanordnungen nicht frei von mehrdeutigen Ergebnissen sind. Ein anderer methodischer Ansatz f\u00fcr den Zugang zu Agency k\u00f6nnte in der Gamifizierung liegen, da solche Ans\u00e4tze in der Lage w\u00e4ren, das Verhalten von Spielern innerhalb einer komplexen Umgebung zu testen, einschlie\u00dflich Reaktionen und Motivationen. Das Konzept des Altruismus macht jedoch deutlich, dass die Erkl\u00e4rung solcher Verhaltensweisen vorerst unerreichbar sein k\u00f6nnte. Genau wie die Agency kann Altruismus grob in evolution\u00e4re oder metaphysische Erkl\u00e4rungen unterteilt werden. Die Zeit wird zeigen, ob diese Denkschulen \u00fcberbr\u00fcckt werden k\u00f6nnen. \n\n''Komplexe Systemtheorie'' steht zunehmend im Fokus vieler Forschungsbereiche, was nicht \u00fcberrascht. Die Dynamik komplexer Systeme ist ein Eldorado f\u00fcr viele empirische Forschenden und viele Disziplinen sind auf diesem Gebiet t\u00e4tig. Aus methodischer Sicht sind Methoden wie die Netzwerkanalyse oder Strukturgleichungsmodelle, aber auch theoretische Arbeiten wie das [https:\/\/en.wikipedia.org\/wiki\/Elinor_Ostrom Ostrom-Framework] Beispiele f\u00fcr die zunehmende Anerkennung komplexer Systeme. Das Ostrom-Framework schl\u00e4gt hier eine Br\u00fccke, da es einen konzeptionellen Fokus auf Ressourcen mit einer methodologischen Denkweise verbindet, um eine strukturierte Erkennung der Systemdynamik zu erm\u00f6glichen. Mehrere Ans\u00e4tze versuchen, die Dynamik komplexer Systeme innerhalb eines analytischen Rahmens zu implementieren, doch leiden diese Ans\u00e4tze oft unter einer L\u00fccke zwischen empirischen Daten und hinreichend komplexen theoriegeleiteten Modellen, da wir erst beginnen, uns der L\u00fccke zwischen einzelnen Fallstudien und einer breiteren Sichtweise der empirischen Ergebnisse zu n\u00e4hern. Hier wird das Ostrom-Framework als wegweisender Ansatz anerkannt, obwohl wir verstehen m\u00fcssen, dass selbst dieses weit verbreitete Framework in \u00fcberraschend wenigen empirischen Studien umgesetzt wurde (Partelow 2019).\n[[File:Ostrom framework.jpg|400px|thumb|left|'''Das Ostrom-Framework erm\u00f6glicht eine Reflexion \u00fcber sozio-\u00f6kologische Systeme.''' Quelle: [https:\/\/science.sciencemag.org\/content\/325\/5939\/419.abstract Ostrom 2009.]]]\n\nEs ist wichtig, sich bewusst zu machen, dass die Probleme, die komplexe Systeme umfassen k\u00f6nnen, oft diagnostiziert werden k\u00f6nnen - was als Systemwissen bezeichnet wird - aber das transformative Wissen, das zur L\u00f6sung dieser Probleme notwendig ist, ist letztlich das, was komplex ist. Zum Beispiel wissen viele Menschen, dass wir bereits verstanden haben, dass der Klimawandel real ist und was wir dagegen tun k\u00f6nnen. Was wir wirklich tun k\u00f6nnten, um die tieferen Probleme zu l\u00f6sen - d.h. das Verhalten und die Konsummuster der Menschen zu \u00e4ndern - ist jedoch viel schwieriger zu erreichen, und genau dar\u00fcber denken komplexe Systeme oft nach. '''Problemdiagnose ist oft einfacher, als L\u00f6sungen zu schaffen.''' Eine weitere Herausforderung in komplexen Systemen ist die zeitliche Dynamik. Beispielsweise bauen meteorologische Modelle auf komplexen Theorien auf, werden aber auch st\u00e4ndig auf der Grundlage einer Vielzahl von Daten verfeinert. Diese Modelle zeigen, dass wir in der Lage sind, das Wetter f\u00fcr einige Tage und grobe Muster sogar f\u00fcr Wochen vorherzusagen, aber es gibt immer noch eine klare Einschr\u00e4nkung unseres Verst\u00e4ndnisses des Wetters, wenn wir weiter in die Zukunft gehen. Ein aktuelles Beispiel f\u00fcr komplexe Systemdynamik ist die Corona-Pandemie. W\u00e4hrend in der Fr\u00fchphase der Pandemie unser Wissen \u00fcber die Ausbreitung des Virus und die damit verbundene Dynamik schnell wuchs, sind wirksame L\u00f6sungen ein langfristiges Ziel. Es besteht ein klarer Zusammenhang, wie bestimmte Aktionen zu bestimmten Entwicklungen der Pandemie f\u00fchren, aber der lokale und regionale Kontext kann sich zu schwerwiegenden Dynamiken und Unterschieden ausweiten. Dar\u00fcber hinaus ist die Entwicklung potenzieller L\u00f6sungen - wie z.B. eines Impfstoffs - sehr komplex und schwer zu erreichen. W\u00e4hrend also viele Entwicklungen w\u00e4hrend dieser Pandemie verstanden werden k\u00f6nnen, ist es sicherlich komplexer, eine L\u00f6sung zu finden.","'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Agency, Complexity and Emergence]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag diskutiert drei zentrale Elemente von Systemen, und wie wir sie methodisch untersuchen k\u00f6nnen.\n\nAgency, Komplexit\u00e4t und Emergenz k\u00f6nnen als Linsen verstanden werden, durch die wir sowohl Ergebnisse sehen k\u00f6nnen, die aus theoretischen \u00dcberlegungen stammen, als auch Informationen, auf die wir durch empirische Forschung zugreifen. Da solche Konzepte die beiden Bereiche der wissenschaftlichen Untersuchung - Theorie und Praxis - miteinander verbinden, nenne ich die drei Konzepte Agency, Komplexit\u00e4t und Emergenz \"Grenzobjekte\". Sie sind gute Beispiele f\u00fcr Ideen, die in der Wissenschaftsphilosophie am relevantesten sind. Dar\u00fcber hinaus bestimmen sie in hohem Ma\u00dfe unser grundlegendes und normatives Weltbild sowie die Konsequenzen des Wissens, das wir aus der Welt gewinnen. Wenn ich z.B. die Weltsicht habe, dass es keinen freien Willen gibt, dann wird dies auch einen grundlegenden Einfluss darauf haben, wie ich Ergebnisse aus der empirischen Untersuchung der Handlungen von Menschen interpretiere. '''Ich pers\u00f6nlich glaube, dass die Menschen das Potential haben, einen freien Willen zu haben, so wie alle Milch in Butter verwandelt werden kann'''. Es gibt andere Konzepte als agency, complexity und emergence, die es aus der Perspektive der Wissenschaftstheorie zu verfolgen lohnt, aber der Einfachheit und Priorit\u00e4t halber werden wir uns hier nur mit diesen drei Grenzobjekten befassen. Beginnen wir mit dem Versuch, die Vielfalt der Definitionen dieser Begriffe aufzuzeigen.\n\n\n== Agency ==\nDie einfachste Definition von Agency, die weithin als die methodisch relevanteste angesehen wird, ist \"die F\u00e4higkeit eines Individuums, vors\u00e4tzlich zu handeln, unter der Annahme eines kausalen Ergebnisses, das auf dieser Handlung beruht\". Innerhalb der verschiedenen Diskurse und Disziplinen gibt es jedoch eine Vielzahl von Ans\u00e4tzen, die breitere oder engere Definitionen von Agency in Betracht ziehen. Es gibt eine erste Meinungsverschiedenheit dar\u00fcber, ob eine handlungsbeteiligte Personmeinen Geisteszustand haben sollte, der in der Lage ist, das Ergebnis der Handlung zu antizipieren, und in diesem Punkt sind sich so unterschiedliche Bereiche wie Neurowissenschaft und Philosophie bereits uneinig. Zum Beispiel sind Reflexe Instinkte, und es ist sehr schwierig, herauszufinden, ob solche Handlungen beabsichtigt sind und Annahmen \u00fcber das Ergebnis enthalten. Daher schlie\u00dfen breitere Definitionen von Agency die handlungsbasierte Anpassung eine*r Agent*in an seine*ihre Umgebung ein, was auch einen Gro\u00dfteil des Tierreichs einschlie\u00dft. Diese weit gefasste Definition des Handelns wird hier ignoriert, obwohl festgestellt werden sollte, dass die Handlungen einiger h\u00f6herer Tiere auf Absicht und mentale Zust\u00e4nde hindeuten, die in der Lage sein k\u00f6nnten, vorauszusehen. Diese sind jedoch vom methodischen Standpunkt aus schwer zu untersuchen, obwohl von dieser Richtung in Zukunft viel erwartet werden kann. \n[[File:Tiger.jpg.jpg|350px|thumb|right|'''Tiere reagieren auf ihre Umwelt, aber handeln sie mit einer Absicht?''' Quelle: pixabay]]\n'''Handlungen von Agent*innen m\u00fcssen vors\u00e4tzlich sein, d.h. eine blo\u00dfe Handlung, die als Serendipit\u00e4t angesehen werden kann, ist nicht Teil des Handelns.''' Ebenso sind nicht vorhergesehene Folgen von Handlungen, die auf Kausalketten beruhen, ein Problem hinsichtlich Agency. Agency steht vor einer Herausforderung, wenn es darum geht, entweder Serendipit\u00e4t oder Murphys Gesetz anzuerkennen. Solche gl\u00fccklichen oder ungl\u00fccklichen Handlungen wurden von dem*der Agent*in nicht vorhergesehen und sind daher nicht wirklich in der Definition von Agency enthalten. Es gibt also ein metaphysisches Problem, wenn wir versuchen, den*die Agent*in, seine*ihre Handlungen und die Folgen der Handlungen zu unterscheiden. Man k\u00f6nnte behaupten, dass dieses Problem gel\u00f6st werden kann, wenn man sich allein auf die Folgen der Handlungen der Agent*innen konzentriert. Diese folgerichtige Sichtweise ist jedoch zum Teil eine theoretische \u00dcberlegung, da diese Sichtweise zwar viele interessante Experimente hervorbringen kann, uns aber nicht wirklich hilft, das Problem der unbeabsichtigten Handlungen an sich zu l\u00f6sen. Dennoch ist die Konzentration auf blo\u00dfe Handlungen auch deshalb relevant, weil sie es erlaubt, sich von eine*r einzelne*n Akteur*in zu entfernen und sich stattdessen auf die Interaktionen innerhalb der Agency mehrerer Akteur*innen zu konzentrieren, was ein guter Anfang f\u00fcr das n\u00e4chste Konzept ist."],"4":["<syntaxhighlight lang=\"Python\" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n<\/syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=\"Python\" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n<\/syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.info()\n<\/syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (\"non-null\") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the \"object\" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For \"sex\" and \"passed\", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n<\/syntaxhighlight> \n\n<syntaxhighlight lang=\"Python\" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n<\/syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"sex\") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= \"sex\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"group\", diag_kind=\"hist\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=\"Python\" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap='PuBu');\n<\/syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=\"Python\" line>\nmodel_1 = smf.ols(formula='points ~ ID + quanti', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).","'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Missing values: types and distributions==\n\nMissing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55).\n\nIn this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.\n\n==Types of missing values==\n\nThere are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149).\n\nUnit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).\n\nNext, we will look at how missing values can be distributed and what the implications of such distributions are. Generally, both types of missing values can occur in any distribution.\n\n==Distributions of missing values==\n\nThe distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).\n\n===Missing completely at random===\nMissing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).\n\nThe implications of MCAR for data analysis are relatively straightforward. Because the missing values are completely random, they do not introduce any bias into the analysis. Therefore, it is generally safe to impute the missing values using statistical methods, such as mean imputation or multiple imputations. However, even if the missing values are MCAR, there may still be other factors that can affect the analysis. It is important to consider the number and proportion of missing values (Scheffer 2002, 156). The larger the proportion of missing values in your overall dataset the less reliable is the use of the data. Imagine you had many unit nonresponse missing values across many different individuals, which results in having no variable without any missing value. This might affect the quality of your dataset. If and how this is the case, needs to be decided case by case.\n\n===Missing at random===\nMissing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55). For example, imagine you conduct a survey to analyze the relationship between education and income and there are missing values concerning income. If the missing values depend on education, then these missing values are missing at random. If they would depend on their actual income, they would not.\n\nThe implications of MAR for data analysis are more complex than those for MCAR. Because the missing values are not completely random, they may introduce bias into the analysis if they are not properly accounted for. Therefore, it is important to carefully consider the underlying reasons for the missing data and take these into account when imputing the missing values. One common approach to dealing with MAR missing values is to use regression or other statistical methods to model the relationship between the missing values and the other variables in the dataset (Tsikriktsis 2005, 56). Once the relationship is clear, other methods can be used to approximate to correct the variables for the bias due to the missing values missing at random.\n\n===Missing not at random===\nMissing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).\n\nThe implications of MNAR for data analysis are more complex than those for MCAR or MAR. Because the missing values are systematically related to the true values of the missing data, they can introduce bias into the analysis if they are not properly accounted for. In some cases, this bias may be difficult or impossible to correct, even with advanced statistical methods (Tsikriktsis 2005, 55).\n\n==Determining the randomness of missing data==\n\nThere are two common methods to determine the randomness of missing data. The first method involves forming two groups: one with missing data for a single variable and one with valid values for that variable. If significant differences are found between the two groups regarding their relationship to other variables of interest, it may indicate a non-random missing data process. The second method involves assessing the correlation of missing data for any pair of variables. If low correlations are found between pairs of variables, it may indicate complete randomness in the missing data (MCAR). However, if significant correlations are found between some pairs of variables, it may be necessary to assume that the data are only missing at random (MAR) (Tsikriktsis 2005, 55 - 56).\n\nOverall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. \"Missing data: our view of the state of the art.\" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. \"Dealing with missing data.\" (2002).\n\nTsikriktsis, Nikos. \"A review of techniques for treating missing data in OM survey research.\" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]","#visualising the dataframe\ntext_tbl <- data.frame(\n  c1 = colnames(df_panel)[1:11], \n  c2 = c(colnames(df_panel)[c(12:21)], \"\")\n)\n\nkable(text_tbl, col.names = NULL) %>%\n  kable_styling() %>%\n  column_spec(1) %>%\n  column_spec(2) \n<\/syntaxhighlight>\n\nThe resulting data frame can be seen on Figure 5. A lot of the variables in the data are categorical (nominal). Many machine learning algorithms cannot operate on nominal data directly. They require all input variables and output variables to be numeric. \n\nTo achieve that, the nominal variables have to be one-hot encoded. This means that each distinct value of the variable is transformed into a new binary variable ([https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/ source]).\n\nFirst, all categorical variables will be determined.\n\n[[File:catvariables.png|500px|thumb|right|Fig. 6]]\n<syntaxhighlight lang = \"R\" line>\n#Fig. 6\ncat_vars = names(which(sapply(df_panel, class) == \"factor\"))\ndf_cat = df_panel[, which(colnames(df_panel) %in% cat_vars)]\nhead(df_cat[, c(1:4)]) %>% \n  kable() %>% \n  kable_styling()\n\ncat_vars = names(which(sapply(df_panel, class) == \"factor\"))\ndf_cat = df_panel[, which(colnames(df_panel) %in% cat_vars)]\nhead(df_cat[, c(4:dim(df_cat)[2])]) %>% \n  kable() %>% \n  kable_styling()\n<\/syntaxhighlight>\n\nNot for all these variables it makes sense to create a new binary variable for each of the distinct values, as they just contain to many different values or even contain only values that are unique, like the <syntaxhighlight lang = \"R\" inline>names<\/syntaxhighlight> variable. These variables need to be transformed in a more logical way. For example the <syntaxhighlight lang = \"R\" inline>host_since<\/syntaxhighlight> variable. This can be transformed into an integer variable by calculating the difference in days between the date the host joined Airbnb and the date of the listing being offered.\n\n<syntaxhighlight lang = \"R\" line>\ndf_panel$host_since = abs(as.integer(difftime(df_panel$date, df_panel$host_since)))\n\ncat_vars = cat_vars[cat_vars %in% \"host_since\" == FALSE] \n<\/syntaxhighlight>\n\nMoreover, the <syntaxhighlight lang = \"R\" inline>date<\/syntaxhighlight> variable would not provide a lot of information if each date would be transformed into a new binary variable. However, as has been shown in the exploratory data analysis, there are a lot of seasonal influences on the price that can be captured by the <syntaxhighlight lang = \"R\" inline>date<\/syntaxhighlight>. Therefore, the date variable will be transformed into two new columns. One column that specifies the month an accommodation is being offered and another column specifing the day of the week in which the accommodation can be rented. Monthly and daily seasonalities (especially weekends) seemed to be the most influencial. \n\n<syntaxhighlight lang = \"R\" line>\ndf_panel = df_panel %>% \n  mutate(\"month_of_year\" = month(date)) %>% \n  mutate(\"dayofweek\" = wday(date))\n\ncat_vars = cat_vars[cat_vars %in% \"date\" == FALSE] \ncat_vars = c(cat_vars, \"month_of_year\", \"dayofweek\")\n\n<\/syntaxhighlight>\n\nLastly, the <syntaxhighlight lang = \"R\" inline>name<\/syntaxhighlight> variable does not contain any information as well without further processing. As has been shown earlier, there are certain words, which are used to promote expensive as well as rather cheap accommodations. The words that have been identified there will be used now. For each title of an Airbnb listing, the number of words that are used from the set of words used to promote expensive flats is determined (as well as the number of words used to promote cheaper accommodations). These features will be saved as <syntaxhighlight lang = \"R\" inline>num_words_exp<\/syntaxhighlight> and <syntaxhighlight lang = \"R\" inline>num_words_cheap<\/syntaxhighlight> respectively.\n\n<syntaxhighlight lang = \"R\" line>\nwords_cheap = c(\"spacious\", \"charming\", \"cozy\", \"columbia\", \"convenient\", \"affordable\")\nwords_exp = c(\"luxury\", \"amazing\", \"garden\", \"gorgeous\", \"best\", \n              \"stunning\", \"terrace\", \"luxurious\", \"times square\")\n# function that returns the number of words_b that appear in words_a\ncount_word_matches = function(words_a, words_b){\n  if(sapply(strsplit(words_a, \" \"), length) == 0){\n    return(0)\n  }\n  else{\n    return(sum(sapply(strsplit(tolower(words_a), \" \"), function(x) x %in% words_b)))\n  }\n}\nnum_words_cheap = sapply(df_panel$name, \n                         function(x) count_word_matches(as.character(x), words_cheap))\nnum_words_exp = sapply(df_panel$name, \n                         function(x) count_word_matches(as.character(x), words_exp))\ndf_panel = cbind(df_panel, num_words_cheap, num_words_exp)\n\ncat_vars = cat_vars[cat_vars %in% \"name\" == FALSE] \n<\/syntaxhighlight> \n\nThe other categorical variables can now be transformed using one-hot encoding. After that, all categorical variables will be removed from the data frame.\n\n<syntaxhighlight lang = \"R\" line>\ndmy = dummyVars(\" ~ .\", data = df_panel[, cat_vars])\ndmy = predict(dmy, newdata = df_panel[,cat_vars])\ndf_panel = cbind(df_panel, dmy)\n\ndf_panel = df_panel[, -which(colnames(df_panel) %in% c(cat_vars, \"name\", \"date\", \"host_since\"))]\n\n# delete those entries where the price is zero\ndf_panel = df_panel[df_panel$price != 0, ]\n<\/syntaxhighlight>"],"5":["'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes\/no (1\/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n<\/imagemap>\n\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to \"rule out\", the answer to this question is 'yes'.\n\n\n= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n<\/imagemap>\n\n'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n<\/imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as 'discrete' data.\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. Abundance data is referred to as 'integer' in R, i.e. it exists in full numbers, and continuous data is 'numeric' - it has a comma.\n\n\n== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n<\/imagemap>\n\n'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?","[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution.","Regarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."],"6":["[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>\n<br\/>\n<br\/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Clustering until 2019.''' Search terms: 'Clustering', 'Cluster Analysis' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as ''similarity measure'' or ''distance measure'', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named \"unsupervised learning\", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term \"clustering\" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=\"R\" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(\"x1\", \"x2\", \"cluster\")\n    return(df)\n}\n<\/syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns '''n''' examples to one of '''k''' clusters, where '''n''' is the sample size and  '''k''', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:\n\n# The number of cluster '''k''' is chosen by the data analyst\n# The algorithm randomly picks '''k''' centroids and assigns each point to the closest centroid to get '''k''' initial clusters\n# The algorithm recalculates the centroid by taking average of all points in each cluster and updates the centroids and re-assigns the points to the closest centroid.\n# The algorithm repeats Step 3 until all points stop changing clusters.\n\nTo get an intuitive sense of how this k-means clustering algorithm works, visit: [https:\/\/www.naftaliharris.com\/blog\/visualizing-k-means-clustering\/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====\n\nTo implement k-Means Clustering, the data table needs to only contain numeric data type. With a data frame or matrix with numeric value where the rows signify individual data example and the columns signify the ''features'' (number of features = the number of dimensions of the data set), k-Means clustering can be performed with the code below:\n\n<syntaxhighlight lang=\"R\" line>\n# Generate data and perform the clustering\ndf <- create_cluster_data(150, 1.25, 3)\ndata_cluster = kmeans(df, centers=3) # perform the clustering","'''Hierarchical Clustering'''\n* Johnson, S.C. Hierarchical clustering schemes. Psychometrika 32, 241\u2013254 (1967). [https:\/\/doi.org\/10.1007\/BF02289588](https:\/\/doi.org\/10.1007\/BF02289588)\n* Murtagh, F., & Legendre, P. (2014). Ward\u2019s hierarchical agglomerative clustering method: which algorithms implement Ward\u2019s criterion?. Journal of classification, 31(3), 274-295.\n\n'''Other Clustering Methods'''\n* A. K. Jain, M. N. Murty, and P. J. Flynn. 1999. Data clustering: a review. ACM Comput. Surv. 31, 3 (September 1999), 264\u2013323. DOI:[https:\/\/doi.org\/10.1145\/331499.331504](https:\/\/doi.org\/10.1145\/331499.331504)\n* Lanza, S. T., & Rhoades, B. L. (2013). Latent class analysis: an alternative perspective on subgroup analysis in prevention and treatment. Prevention science : the official journal of the Society for Prevention Research, 14(2), 157\u2013168. [https:\/\/doi.org\/10.1007\/s11121-011-0201-1](https:\/\/doi.org\/10.1007\/s11121-011-0201-1)\n* Erich Schubert, J\u00f6rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. 2017. DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN. ACM Trans. Database Syst. 42, 3, Article 19 (July 2017), 21 pages. DOI:[https:\/\/doi.org\/10.1145\/3068335](https:\/\/doi.org\/10.1145\/3068335)\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Prabesh Dhakal.","Further, the researchers developed an analysis scheme to analyze the extracted categories and themes quantitatively:\n<br>\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Analysis scheme.png|750px|thumb|center|'''Analysis scheme for the systematic review on sustainable agriculture.''' Source: Velten et al. 2015, p.7837]]\n<br>\nIn a third step, the authors conducted a [[Clustering Methods|Cluster Analysis]] on the academic literature based on the quantitative data in order to identify groups of literature that bring up specific aspects of the concept.\n\nBased on their qualitative results, the researchers highlight that 'sustainable agriculture' can be separated into three distinct groups (Goals, Strategies, Fields of Action) with specific themes and categories, each. Based on their quantitative analysis, they show how the most frequent topics revolve around anthropocentric rather than ecocentric values when it comes to defining what is 'sustainable agriculture', and that the realization of sustainable agriculture mostly focuses on technological solutions on the farm level. However, they also found a strong alternative discourse that focuses on ecocentric and alternative approaches. They could further identify categories that changed in importance over time, and how the distribution of themes differed between academic and non-academic literature and between scientific disciplines. Lastly, their subsequent cluster analysis led to five groups of conceptual understandings of 'sustainable agriculture'.\n\nThis study exemplifies how a Systematic Literature Review with both qualitative and quantitative elements provides a rich conceptual overview of a topic which can be used to further structure work in this field, and raises questions for academia as well as practitioners. In their conclusion, the authors \"(...) recommend embracing the complexity of sustainable agriculture with its varied and seemingly contradictory aspects\" since they found \"(...) the different conceptions of sustainable agriculture to be not as contradicting and mutually exclusive as they\nhave often been portrayed\". (p.7857). '''This shows how a systematic review can help grasp the diversity of aspects on a given topic, and draw the respective conclusions.'''\n\n== Key publications ==\nHart, C. 2018. ''Doing a Literature Review''. SAGE Publications.\n* An extensive overview on all relevant steps of the literature review process targeted at young scholars at the Master's and Doctorate level.\n\nBooth, A. Sutton, A. Papaioannou, D. 2016. ''Systematic approaches to a successful literature review.'' Second Edition. SAGE Publications.\n* A step-by-step illustration of the literature review process and what makes it 'systematic'.\n\n\n== References ==\n(1) Rowley, J. Slack, F. 2004. ''Conducting a Literature Review.'' Management Research News 27(6). 31-39.\n\n(2) Hart, C. 2018. ''Doing a Literature Review.'' SAGE Publications.\n\n(3) Booth, A. Sutton, A. Papaioannou, D. 2016. ''Systematic approaches to a successful literature review.'' Second Edition. SAGE Publications.\n\n(4) Randolph, J. 2009. ''A Guide to Writing the Dissertation Literature Review.'' Practical Assessment, Research, and Evaluation 14(14).\n\n(5) Wikipedia. ''Systematic Review.'' Available at [https:\/\/en.wikipedia.org\/wiki\/Systematic_review#Research_fields](https:\/\/en.wikipedia.org\/wiki\/Systematic_review#Research_fields) (last accessed 09.07.2020)\n\n(6) Chalmers, I. Hedges, L.V. Cooper, H. 2002. ''A Brief History of Research Synthesis.'' Evaluation & The Health Professions 25(1). 12-37.\n\n(7) Newig, J. Fritsch, O. 2009. ''THE CASE SURVEY METHOD AND APPLICATIONS IN POLITICAL SCIENCE.'' APSA 2009 Conference Paper.\n\n(8) Velten, S. Leventon, J. Jager, N. Newig, J. 2015. ''What is Sustainable Agriculture? A Systematic Review''. Sustainability 7. 7833-7865.\n\n\n== Further Information ==\n* [https:\/\/www.youtube.com\/watch?v=WUErib-fXV0 A short YouTube Video on Systematic Literature Reviews] by 'Research Shorts' that summarizes the method in 3 minutes.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."],"7":["== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n<\/imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) '''First of all, is the statistics the right approach to begin with?''' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem. \n\n2) '''What are the efforts you need to put into the initial data gathering?''' If you decided that statistics would be valuable to be applied, the question then would be, how? To rephrase this statement: How exactly? Your sampling with all its constructs, sample sizes and replicates decides about the fate of everything you going to do later. A flawed dataset or a small or biased sample will lead to failure, or even worse, wrong results. Play it safe in the beginning, do not try to overplay your hand. Slowly edge your way into the application of statistics, and always reflect with others about your sampling strategy. \n\n3) '''The analysis then demands hand-on skills, as implementing tests within a software is something that you learn best through repetition and practice.''' I suggest you to team up with other peers who decide to go deeper into statistical analysis. If you however decide against that, try to find geeks that may help you with your data analysis. Modern research works in teams of complementary people, thus start to think in these dimensions. If you chip in the topical expertise of the effort to do the sampling, other people may be glad about the chance to analyse the data.\n\n4) '''This is also true for the interpretation, which most of all builds on experience.''' This is the point were a supervisor or a PhD student may be able to glance at a result and tell you which points are relevant, and which are negotiable. Empirical research typically produces results where in my experience about 80 % are next to negliable. It takes time to learn the difference between a trivial and an innovative result. Building on knowledge of the literature helps again to this end, but be patient as the interpretation of statistics is a skill that needs to ripen, since context matters. It is not so much about the result itself, but more about the whole context it is embedded in.\n\n5) The last and most important point explores this thought further. '''What are the limitations of your results?''' Where can you see flaws, and how does the multiverse of biases influence your results and interpretation? What are steps to be taken in future research? And what would we change if we could start over and do the whole thing again? All these questions are like ghosts that repeatedly come to haunt a researcher, which is why we need to remember we look at pieces of the puzzle. Acknowledging this is I think very important, as much of the negative connotation statistics often attracts is rooted in a lack of understanding. If people would have the privilege to learn about statistics, they could learn about the power of statistics, as wells its limitations. \n\n'''Never before did more people in the world have the chance to study statistics.''' While of course statistics can only offer a part of the puzzle, I would still dare to say that this is reason for hope. If more people can learn to unlock this knowledge, we might be able to move out of ignorance and more towards knowledge. I think it would be very helpful if in a controversial debate everybody could dig deep into the available information, and make up their own mind, without other people telling them what to believe. Learning about statistics is like learning about anything else, it is lifelong learning. I believe that true masters never achieve mastership, instead they never stop to thrive for it.\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden (concept, text) and Christopher Franz (implementation, linkages).","[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations \"(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns.\" (Prell et al. 2009, p.503). '''Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as \"dyads\", but rather the study of patterns within a network.''' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors' behavior emerges from the interaction between them: \"Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions.\" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent's actions (Borgatti et al. 2009)","==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* '''Network boundaries:''' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** ''position-based approach'': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** ''event-based'' approach: those who had participated in key events are believed to define the population\n** ''relation-based approach'': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the ''exogenously defined boundaries'', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* '''Type of ties:''' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: ''directed'' ties, which go from one node to another, and ''undirected ties'', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, \"(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact.\" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* '''Data Collection''': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a ''prompted recall'' approach, they are asked which people they would think of with regards to a specific topic (e.g. \"To whom would you go for advice at work?\") while they are shown a pre-determined list of potentially relevant individuals. In the ''free list'' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the '''visual representation''' of the network. For this, the researcher \"(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections.\" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the \"number of lines which it is necessary to traverse in order to get from one point to another.\" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|'''Different network structures.''' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|'''An exemplary network structure.''' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]\n\n== Strengths & Challenges ==\n* There is a range of challenges in the gathering of network data through [[Semi-structured Interview|Interviews]] and [[Survey|Surveys]], which can become long and cumbersome, and in which the interviewees may differently understand and recall their relations with other actors, or misinterpret the connections between other actors. (Marin & Wellman 2010, p.15)\n* The definition of network boundaries is crucial, since \"(...) the inappropriate inclusion or exclusion of a small number of entities can have ramifications which extend well beyond those entities themselves\". Apart from the excluded entities and their relations, all relations between these entities and the rest of the network, and thus the network's structural properties, are affected. (Butts 2008). For more insights on the topic of System Boundaries, please refer to [[System Boundaries|the respective article]]."],"8":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")\n\nlegend('topleft',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == \"grp1\"]~ t1[group == \"grp1\"],\n              data = data))\nabline(lm (t3[group == \"grp2\"]~ t1[group == \"grp2\"],\n              data = data))\nabline(lm (t3[group == \"grp3\"]~ t1[group == \"grp3\"],\n              data = data))\n<\/syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https:\/\/sustainabilitymethods.org\/index.php\/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n<\/syntaxhighlight>","'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.","====Analysis of Variance====\n'''The [https:\/\/www.investopedia.com\/terms\/a\/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment. \n\nThe general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.\n\n====Preconditions====\nRegarding the preconditions of the [https:\/\/www.youtube.com\/watch?v=oOuu8IBd-yo ANOVA], it is important to realize that the data should ideally be '''normally distributed''' on all levels, which however is often violated due to small sample sizes. Since a non-normal distribution may influence the outcome of the test, boxplots are a helpful visual aid, as these allow for a simple detection tool of non-normal distribution levels. \n\nEqually should ideally the variance be comparable across all levels, which is called '''[https:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia homoscedastic]'''. What is also important is the criteria of '''independence''', meaning that samples of factor levels should not influence each other. For this reason are for instance in ecological experiments plants typically planted in individual pots. In addition does the classical ANOVA assume a '''balanced design''', which means that all factor levels have an equal sample size. If some factor levels have less samples than others, this might pose interactions in terms of normals distribution and variance, but there is another effect at play. Larger sample sizes on one factor level may create a disbalance, where factor levels with larger samples pose a larger influence on the overall model result. \n\n====One way and two way ANOVA====\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https:\/\/www.thoughtco.com\/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a '''[https:\/\/www.statisticshowto.com\/post-hoc\/ posthoc test]'''. A prominent example is the [https:\/\/sciencing.com\/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https:\/\/www.statisticshowto.com\/post-hoc\/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.\n\n====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong."],"9":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")\n\nlegend('topleft',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == \"grp1\"]~ t1[group == \"grp1\"],\n              data = data))\nabline(lm (t3[group == \"grp2\"]~ t1[group == \"grp2\"],\n              data = data))\nabline(lm (t3[group == \"grp3\"]~ t1[group == \"grp3\"],\n              data = data))\n<\/syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https:\/\/sustainabilitymethods.org\/index.php\/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n<\/syntaxhighlight>","plot(fitted(model_1),\n     residuals(model_1))\n<\/syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=\"R\" line>\nhist(residuals(model_1),\n     col=\"yellow\")\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"R\" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n<\/syntaxhighlight>\n<br>\nHistogram of residual values is \"bell shaped\" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=\"R\" line>\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=\"R\" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the \"noise\" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question \"What treatment type has the most effect on anxiety level?\"\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n<\/syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]","'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions."],"10":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")\n\nlegend('topleft',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == \"grp1\"]~ t1[group == \"grp1\"],\n              data = data))\nabline(lm (t3[group == \"grp2\"]~ t1[group == \"grp2\"],\n              data = data))\nabline(lm (t3[group == \"grp3\"]~ t1[group == \"grp3\"],\n              data = data))\n<\/syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https:\/\/sustainabilitymethods.org\/index.php\/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n<\/syntaxhighlight>","plot(fitted(model_1),\n     residuals(model_1))\n<\/syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=\"R\" line>\nhist(residuals(model_1),\n     col=\"yellow\")\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"R\" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n<\/syntaxhighlight>\n<br>\nHistogram of residual values is \"bell shaped\" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=\"R\" line>\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=\"R\" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = \"II\")\n<\/syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the \"noise\" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question \"What treatment type has the most effect on anxiety level?\"\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n<\/syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]","====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong. \n\nFurthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the deductive world become intertwined, and this poses a risk for the validity of scientific results.\n\nFor more on the Analysis of Variance, please refer to the [[ANOVA]] entry.\n\n==Examples==\n[[File:Guinea pig computer.jpg|thumb|right|The tooth growth of guinea pigs is a good R data set to illustrate how the ANOVA works]]\n===Toothgrowth of guinea pigs===\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the ToothGrowth data set is about: ?ToothGrowth\n\n#The code is partly from boxplot help (?boxplot). If you like to know the meaning of the code below, you can look it up there\ndata(ToothGrowth)\n\n# to create a boxplot \nboxplot(len ~ dose, data = ToothGrowth,\n                           boxwex = 0.25, \n                           at = 1:3 - 0.2,\n                           subset = supp == \"VC\", col = \"yellow\",\n                           main = \"Guinea Pigs\u2019 Tooth Growth\",\n                           xlab = \"Vitamin C dose mg\",\n                           ylab = \"tooth length\",\n                           xlim = c(0.5, 3.5), ylim = c(0, 35), yaxs = \"i\")\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n                           boxwex = 0.25,\n                           at = 1:3 + 0.2,\n                           subset = supp == \"OJ\", col = \"orange\")\nlegend(2, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c (\"yellow\", \"orange\"))\n\n#to apply an ANOVA\nmodel1<-aov(len ~ dose*supp, data = ToothGrowth)\nsummary(model1) \n#Interaction is significant\n<\/syntaxhighlight>\n\n====Insect sprays====\n[[File:A man using RIPA insecticide to kill bedbugs Wellcome L0032188.jpg|thumb|right|To find out, which insectide works effictively, you can approach an ANOVA]]\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the InsectSprays data set is about: ?InsectSprays\ndata(InsectSprays)\nattach(InsectSprays)\ntapply(count, spray, length)\nboxplot(count~spray) \n# can you guess which sprays are effective by looking at the boxplot?\n# to find out which sprays differ significantly without applying many t-tests, you can use a postdoc test\nmodel2<-aov(count~spray)\nTukeyHSD(model2)\n# compare the results to the boxplot if you like\n<\/syntaxhighlight>\n\n==Balanced vs. unbalanced designs==\nThere is such a thing as a perfect statistical design, and then there is reality.\n\nStatistician often think in so called balanced designs, which indicate that the samples across several levels were sampled with the same intensity. Take three soil types, which were sampled for their agricultural yield in a ''mono crop''. Ideally, all soil types should be investigated with the same amount of samples. If we would have three soil types -clay, loam, and sand- we should not sample sand 100 times, and clay only 10 times. If we did, our knowledge about sandy soils would be much higher compared to clay soil. This does not only represent a problem when it comes to the general knowledge, but also creates statistical problems. \n\nFirst of all, the sandy soil would be represented much more in an analysis that does not or cannot take such an unbalanced sampling into account. \n\nSecond, many analysis have assumptions about a certain statistical distribution, most notably the normal distribution, and a smaller sample may not show a normal distribution, which in turn may create a [[Bias in statistics|bias]] within the analysis. In order to keep this type of bias at least constant across all levels, we either need a balanced design, or use an analysis that compensates for such unbalanced designs. This analysis was realised with so called Type III ANOVA, which can take different sampling intensities into account. Type III ANOVA corrects for the error that is potentially inferred due to differing sample density, that means number of samples per level.\n\n'''Why is this relevant, you ask?''' \n\nBecause the world is messy. Plants die. So do animals. Even people die. It is sad. For a statistician particularly because it makes one miss out on samples, and dis-balances your whole design. And while this may not seem like a bad problem for laypeople, for statistician it is a real mess. Therefore the ways to deal with unbalanced designs were such a breakthrough, because they finally allowed the so neatly thinking statisticians to not only deal with the nitty-gritty mess of unbalanced designs, but with real world data."],"11":["== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.\n* Validity of the inferred results - i.e. the fitting of the results to the intended kind of knowledge - may be reached a) through the researchers' knowledge of contextual factors of the data and b) through validation with other sources, e.g. by the means of triangulation. However, the latter can be difficult to do due to the uniqueness of the data (1-3). Any content analysis is a reduction of the data, which should be acknowledged critically, and which is why the coding scheme should be precise and include the aforementioned explanation, examples and exclusion criteria.\n\n==== Connectedness ====\n* Since verbal discourse is an important data source for Content Analysis, the latter is often used as an analysis method of transcripts of standardized, [[Open Interview|open]], or [[Semi-structured Interview|semi-structured interviews]].\n* Content Analysis is one form of textual analysis. The latter also includes other approaches such as discourse analysis, rhetorical analysis, or [[Ethnography|ethnographic]] analysis (2). However, Content Analysis differs from these methods in terms of methodological elements and the kinds of questions it addresses (2).\n\n\n== Outlook ==\n* The usage of automated coding with the use of computers may be seen as one important future direction of the method (1, 5). To date, the human interpretation of ambiguous language imposes a high validity of the results which cannot (yet) be provided by a computer. Alas, the development of an appropriate algorithm and text recognition software pose a challenge. The meaning of words changes in different contexts and several expressions may mean the same. Especially in terms of qualitative analyses, this currently makes human coders indispensable. Yet, the emergence of big data, Artificial Intelligence and [[Machine Learning]] might make it possible in the foreseeable future to use automated coding more regularly and with a high validity.\n* Another relevant direction is the shift from text to visual and audio data as a primary form of data (5). With ever-increasing amounts of pictures, video and audio files on the internet, future studies may refer to these kinds of sources more often.\n\n== Key publications ==\nKuckartz, U. (2016). ''Qualitative Inhaltsanalyse: Methoden, Praxis, Computerunterst\u00fctzung (3., \u00fcberarbeitete Auflage). Grundlagentexte Methoden.'' Weinheim, Basel: Beltz Juventa.\n* A very exhaustive (German language) work in which the author explains different types of qualitative content analysis by also giving tangible examples.\n\nKrippendorff, K. 2004. ''Content Analysis - An Introduction to Its Methodology. Second Edition''. SAGE Publications.\n* A much-quoted, extensive description of the method's history, conceptual foundations, uses and application. \n\nBerelson, B. 1952. ''Content analysis in communication research.'' Glencoe, Ill.: Free Press.\n* An early review of concurrent forms of (quantitative) content analysis.\n\nSchreier, M. 2014. ''Varianten qualitativer Inhaltsanalyse: Ein Wegweiser im Dickicht der Begrifflichkeiten.'' Forum Qualitative Sozialforschung 15(1). Artikel 18.\n* A (German language) differentiation between the variations of the qualitative content analysis.\n\nErlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n* A very helpful guide to content analysis, using the examples shown above.\n\n\n== References ==\n(1) Krippendorff, K. 1989. ''Content Analysis.'' In: Barnouw et al. (Eds.). ''International encyclopedia of communication.'' Vol. 1. 403-407. New York, NY: Oxford University Press.\n\n(2) White, M.D. Marsh, E.E. 2006. ''Content Analysis: A Flexible Methodology.'' Library Trends 55(1). 22-45. \n\n(3) Stemler, S. 2000. ''An overview of content analysis.'' Practical Assessment, Research, and Evaluation 7. Article 17.\n\n(4) Mayring, P. 2000. ''Qualitative Content Analysis''. Forum Qualitative Social Research 1(2). Article 20.\n\n(5) Stemler, S. 2015. ''Content Analysis. Emerging Trends in the Social and Behavioral Sciences: An Interdisciplinary, Searchable, and Linkable Resource.'' 1-14.\n\n(6) Erlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.","While there is a wide range of qualitative Content Analysis approaches, this entry will focus on joint characteristics of these. For more information on the different variations, please refer to Schreier (2014) in the Key Publications.\n\n==== General Methodological Process ====\n* Any Content Analysis starts with the ''Design Phase'' in which the research questions are defined, the potential sources are gathered, and the analytical constructs are established that connect the prospective data to the general target of the analysis. These constructs can be based on existing theories or practices, the experience and knowledge of experts, or previous research (2).\n* Next, the ''Unitizing'' is done, i.e. the definition of analysis units. It may be distinguished between sampling units (= sources of data, e.g. newspaper articles, interviews) and analysis units (= units of data that are coded and analyzed, e.g. single words or broader messages), with different approaches to identifying both (see Krippendorff 2004).\n* Also, the ''sampling'' method is determined and the sample is drawn.\n* Then, the ''Coding Scheme'' - or 'Codebook' - is developed and the data are coded. 'Coding' generally describes the transfer of the available data into more abstract groups. A code is a label that represents a group of words that share a similar meaning (3). Codes may also be grouped into categories if they thematically belong together. There are two typical approaches to developing the coding scheme: a rather theory-driven and a rather data-driven approach. In a theory-driven approach, codes are developed based on theoretical constructs, research questions and elements such as an interview guide, which leads to a rather deductive coding procedure. In a data-driven approach, the coding system is developed as the researcher openly scans subsamples of the available material and develops codes based on these initial insights, which is a rather inductive process. Often, both approaches are combined, with an initial set of codes being derived from theory, and then iteratively adapted through a first analysis of the available material.\n* With the coding scheme at hand, the researcher reads (repeatedly) through the data material and assigns each analysis unit to one of the codes. The ''coding process'' may be conducted by humans, or - if sufficiently explicit coding instructions are possible - by a computer. In order to provide reliability, the codes should be intersubjective, i.e. every researcher should be able to code similarly, which is why the codes must be exhaustive in terms of the overarching construct, mutually exclusive, clearly defined, have unambiguous examples as well as exclusion criteria.\n* Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural context of the message, or on the message's effect. The learnings may be validated in the face of other information sources, which is often difficult when Content Analysis is used in cases where there is no other possibility to access this knowledge. Finally, new hypotheses may also be formulated (all from 1, 2).\n\n[[File:Example Content Analysis (1).jpg|500px|thumb|right|'''An exemplary interview transcript from a person that was admitted to an emergency center.''' Source: Erlingsson & Brysiewicz 2017.]]\n\n[[File:Example Content Analysis (2).png|500px|thumb|right|'''The coding results for statements from the interview transcript above, grouped into one of several themes.''' Source: Erlingsson & Brysiewicz 2017.]]\n\nQualitative Content Analysis is a rather inductive process. The process is guided by the research questions - hypotheses may be tested, but this is not the main goal. As with much of qualitative research, this method attempts to understand the (subjective) meaning behind the analyzed data and to draw conclusions from there. Its purpose is to get hold of the 'bigger picture', including the communicative context surrounding the genesis of the data. The qualitative Content Analysis is a very iterative process. The coding scheme is often not determined prior to the coding process. Instead, its development is guided by the research questions and done in close contact to the data, e.g. by reading through all data first and identifying relevant themes. The codes are then re-defined iteratively as the researcher applies them to the data. As an example, the comparative analysis of two texts can be mentioned, where codes are developed based on the first text, re-iterated by reading through the second text, then jointly applied to the first text again. Also, new research questions may be added during the analysis if the first insights into the data raise them. This approach is closely related to the methodological concept of [[Grounded Theory]] approaches.\n\nWith regards to the coding process, the analysis units are assigned to the codes but not statistically analysed. There is an exemption to this - the evaluative content analysis, where statements are evaluated on a nominal or ordinal scale, which makes simple quantitative (statistical) analyses possible. However, for most forms of qualitative Content Analysis, the assigned items per code are interpreted qualitatively and conclusions are inferred from these interpretations. Examples and quotes may be used to illustrate the findings. Overall, in this approach, a deep understanding of the data and their meaning is relevant, as is a proper description of specific cases, and possibly a triangulation with other data sources on the same subject. \n\n\n== Strenghts & Challenges ==\n* Content Analysis counteracts biases because it \"(...) assures not only that all units of analysis receive equal treatment, whether they are entered at the beginning or at the end of an analysis but also that the process is objective in that it does not matter who performs the analysis or where and when.\" (see Normativity) (Krippendorff 1989, p.404). In this case, 'objective' refers to 'intersubjective'. Yet, biases cannot be entirely prevented, e.g. in the sampling process, the development of the coding scheme, or the interpretation and evaluation of the coded data.\n* Content Analysis allows for researchers to apply their own social-scientific constructs \"by which texts may become meaningful in ways that a culture may not be aware of.\" (Krippendorff 1989, p.404)\n* However, especially in case of qualitative analysis of smaller data sample sizes, the theory and hypotheses derived from data cannot be generalized beyond this data (1). Triangulation, i.e. the comparison of the findings to other knowledge on the same topic, may provide more validity to the conclusions (see Normativity).","The scope of the research may range from qualitative, unstructured, small-scale observations to more quantitative, large-scale, structured studies, depending on the research questions and theoretical approach (2). Quantitative approaches allow for the identification of trends and variations, while more qualitative approaches strengthen the understanding of prevalent phenomena (1). The possibility of combining both approaches is special about video-based research and enables researchers to quantitatively validate comprehensive qualitative findings (1, 4).\n\n== Strengths & Challenges ==\n* The biggest strength of the method stems from the nature of the data. '''Video material can be slowed down, stopped, rewound, re-watched and stored for a long time.''' This makes video-based research \"(...) a tool for social scientists to observe phenomena that are too complex to be noticed by the naked eye.\" (Jan\u00edk et al. 2009, p.7). The researchers are not limited to what they were able to note down during the event itself, but can assess everything that happened as often as they like (1, 4, 6). At the same time, this complexity can be reduced to a specific aspect of interest for a first analysis, after which the researchers can come back to the data at any later point for further inquiries (2). Long-time comparisons are also possible if similarly taped videos are produced over a span of time. Several researchers - potentially from different disciplinary backgrounds - can code and analyze the material at once, and regularly discuss their insights and exchange perspectives which can lead to the [[Glossary|emergence]] of new ideas and analytic categories (1, 3, 4, 6). This increases inter-coder reliability without having to determine a narrow focus of the research prior to the data collection (3, 4). In addition, the data format facilitates the [[Glossary|communication]] of results since exemplary scenes or images can be taken from the video (4).\n* '''The complexity of the respective situation as captured by the video material allows for both qualitative and quantitative analysis and the application of a range of different research questions and perspectives.''' Also, as mentioned before, qualitative and quantitative results can be integrated because of the rewatchability of the data.\n\nSome of the beneficial methodological elements also pose challenges. \n* Compared to - for example - the act of taking notes, the taping of videos is more complicated, needs more (expensive) video, audio, computer and data storage equipment as well as more technological knowledge and training (3).\n* '''The video data suggests objectivity but may not be objective due to the focus of the camera during the recording.''' If one individual or one perspective is favored, the subsequent analysis has an inherent focus on this element (1, 3, 4). Also, without additional data on the analyzed situation to contextualize the video material that may be gathered, e.g. through [[Ethnography|observations]] or [[Open Interview|Interviews]], researchers may develop a false feeling of understanding - especially foreign-country - (teaching) practices (1).\n* This supplementary data, which may be collected in addition to the videotaping, is also rather essential for a postponed analysis of the video material. If neglected, it may be impossible to properly analyze the videotaped situation some time after the initial research (1, 3).\n* With a camera being present, the subjects \/ participants may act differently than they usually would (3, 4).\n* Lastly, the act of videotaping individuals brings along moral implications of privacy and confidentiality that need to be addressed beforehand, e.g. by establishing an open relationship with the subjects of research and ensuring agreement on the usage of the video material (1, 3; see Normativity).\n\n== Normativity ==\n* The gathering of video material is very closely related to methods of [[Ethnography]], while the analysis of the gathered data is basically a special form of [[Content Analysis]].\n* As mentioned above, the video data may - or even should - be supplemented with additional qualitative and\/or quantitative data, gathered e.g. by the use of a questionnaire, participant observation or through the analysis of documents such as worksheets or work samples in classrooms (1, 4).\n* \"Collecting videotape data requires a careful consideration of ethical and legal obligations regarding the protection of the confidentiality and privacy rights of those individuals who are filmed. If conducting cross-national studies, researchers should be aware that some countries have more fully developed laws and regulations than others, often with specific considerations for minors\" (Jacobs et al. 2007, p.290)\n\n== Key publications ==\nStigler et al. 1999. ''THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.'' U.S. Department of Education. National Center for Education Statistics Washington.\n* The TIMSS study was an empirical milestone for the integration of (comparative) video-based research, comparing the teaching practices in 231 classrooms in three countries.\n\n== References ==\n(1) Ulewicz, M. Beatty, A. (eds.) 2001. ''The Power of Video Technology in International Comparative Research in Education.'' National Academy Press Washington.\n\n(2) Jan\u00edk, T. Seidel, T. Najvar, P. ''Introduction: On the Power of Video Studies in Investigating Teaching and Learning.'' In: Jan\u00edk, T. Seidel, T. (eds). 2009. ''The Power of Video Studies in Investigating Teaching and Learning in the Classroom.'' Waxmann Verlag GmbH M\u00fcnster. 7-19.\n\n(3) Jacobs, J.K. Hollingsworth, H. Givvin, K.B. 2007. ''Video-Based Research Made \"Easy\": Methodological Lessons Learned from the TIMSS Video Studies.'' Field Methods 19. 284-299.\n\n(4) Stigler, J. Gallimore, R.G. 2000. ''Using Video Surveys to Compare Classrooms and Teaching Across Cultures: Examples and Lessons from the TIMSS Video Studies''. Educational Psychologist 35(2). 87-100.\n\n(5) Stigler et al. 1999. ''THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.'' U.S. Department of Education. Natoinal Center for Education Statistics Washington."],"12":["'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br\/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https:\/\/sustainabilitymethods.org\/index.php\/Partial_Correlation this entry].\n__TOC__\n<br\/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, '''correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!'''\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\ndata(\"mtcars\")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = \"Correlation between Miles per Gallon and Horsepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     pch = 16,\n     col = \"red\",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n<\/syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=\"R\" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n<\/syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=\"R\" inline>plot()<\/syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* '''x:''' variable, that will be displayed on the x-axis.\n* '''y:''' variable, that will be displayed on the y-axis.\n* '''xlab:''' title for the x-axis.\n* '''ylab:''' title for the y-axis.\n* '''pch:''' shape and size of the plotted observations, in this case, filled circles. [http:\/\/www.sthda.com\/english\/wiki\/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* '''col:''' plotting color. You can either write the name of the color or use the [https:\/\/www.r-graph-gallery.com\/41-value-of-the-col-function.html color number].\n* '''las:''' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* '''xlim:''' set the limit of the x-axis.\n* '''ylim:''' set the limit of the y-axis.\n* '''abline:''' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n<\/syntaxhighlight>","[[File:ConceptCorrelation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Correlations]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n\n<br\/><br\/>\n'''In short:''' Correlation analysis examines the statistical relationship between two continuous variables. For R examples on Correlations, please refer to [[Correlation Plots]].\n\n== Background ==\n[[File:Correlation.png|400px|thumb|right|'''SCOPUS hits per year for Correlations until 2020.''' Search terms: 'Correlation' in Title, Abstract, Keywords. Source: own.]]\n\nKarl Pearson is considered to be the founding father of mathematical statistics; hence it is no surprise that one of the central methods in statistics - to test the relationship between two continuous variables - was invented by him at the brink of the 20th century (see Karl Pearson's \"Notes on regression and inheritance in the case of two parents\" from 1895). His contribution was based on work from Francis Galton and Auguste Bravais. With more data becoming available and the need for an \u201cexact science\u201d as part of the industrialization and the rise of modern science, the Pearson correlation paved the road to modern statistics at the beginning of the 20th century. While other approaches such as the t-test or the Analysis of Variance ([[ANOVA]]) by Pearson's arch-enemy Fisher demanded an experimental approach, the correlation simply required data with a continuous measurement level. Hence it appealed to the demand for an analysis that could be conducted based solely on measurements done in engineering, or on counting as in economics, without being preoccupied too deeply with the reasoning on why variables correlated. '''Pearson recognized the predictive power of his discovery, and the correlation analysis became one of the most abundantly used statistical approaches in diverse disciplines such as economics, ecology, psychology and social sciences.''' Later came the \u200bregression analysis, which implies a causal link between two continuous variables. This makes it different from a correlation, where two variables are related, but not necessarily causally linked. This article focuses on correlation analysis and only touches upon regressions. For more, please refer to the entry on [[Regression Analysis]].)\n\n\n== What the method does ==\nCorrelation analysis examines the relationship between two [[Data formats|continuous variables]], and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called ''correlation coefficient'' indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. \n\nCorrelations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including [[Survey]]s and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date.\n\n'''There are different forms of correlation analysis.''' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https:\/\/365datascience.com\/students-t-distribution\/ Student's t-distribution]. Alternative correlation measures like [https:\/\/www.statisticssolutions.com\/kendalls-tau-and-spearmans-rank-correlation-coefficient\/ Kendall's tau and Spearman's rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data.\n\n==== Calculating Pearson's correlation coefficient r ====\nThe formula to calculate [https:\/\/www.youtube.com\/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br\/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br\/>\n\n'''1) Is the relationship between two variables positive or negative?''' If one variable increases, and the other one increases, too, we have  a positive (\"+\") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\u00a0[https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3534609\/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (\"-\"): for example, the relationship between 'pizza eaten' and 'pizza left' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship.","=== Core elements of correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br\/>\n\n'''1) Is the relationship between two variables positive or negative?''' If one variable increases, and the other one increases, too, we have  a positive (\"+\") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\u00a0[https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3534609\/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (\"-\"): for example, the relationship between 'pizza eaten' and 'pizza left' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship. \n\n'''2) Is the correlation coefficient small or large?''' It can range from -1 to +1, and is an important measure when we evaluate the strength of a statistical relationship. Data points may scatter widely in a [[Correlation_Plots#Scatter_Plot|scatter plot,]] or there may be a rather linear relationship - and everything in between. An example for a perfect positive correlation (with a correlation coefficient ''r'' of +1) is the relationship between temperature in [[To_Rule_And_To_Measure#Celsius_vs_Fahrenheit_vs_Kelvin|Celsius and Fahrenheit]]. This should not be surprising, since Fahrenheit is defined as 32 + 1.8\u00b0 C. Therefore, their relationship is perfectly linear, which results in such a strong correlation coefficient. We can thus say that 100% of the variation in temperature in Fahrenheit is explained by the temperature in Celsius.\n\nOn the other hand, you might encounter data of two variables that is scattered all the way in a scatter plot and you cannot find a significant relationship. The correlation coefficient ''r'' might be around 0.1, or 0.2. Here, you can assume that there is no strong relationship between these two variables, and that one variable does not explain the other one. \n\nThe stronger the correlation coefficient of a relation is, the more may these relations matter, some may argue. If the points are distributed like stars in the sky, then the relationship is probably not significant and interesting. Of course this is not entirely generalisable, but it is definitely true that a neutral relation only tells you, that the relation does not matter. At the same time, even weaker relations may give important initial insights into the data, and if two variables show any kind of relation, it is good to know the strength. By practising to quickly grasp the strength of a correlation, you become really fast in understanding relationships in data. Having this kind of skill is essential for anyone interested in approximating facts through quantitative data.  \n\n'''3) What does the relationship between two variables explain?''' \nThis is already an advanced skill, and is rather related to regression analysis. So if you have looked at the strength of a correlation, and its direction, you are good to go generally. But sometimes, these measures change in different parts of the data. \n\nTo illustrate this, let us have a look at the example of the percentage of people working in\u00a0[https:\/\/ourworldindata.org\/employment-in-agriculture?source=post_page--------------------------- Agriculture]\u00a0within individual countries. Across the world, people at a low income (<5000 Dollar\/year) have a high variability in terms of agricultural employment:  half of the population of the Chad work in agriculture, while in Zimbabwe it is only 10\u00a0%. However, at an income above 15000 Dollar\/year, there is hardly any variance in the percentage of people that work in agriculture: it is always very low. If you plotted this, you would see that the data points are rather broadly spread in the lower x-values (with x as the income), but are more linearly spread in the higher income areas (= x values). This has reasons, and there are probably one or several variables that explain this variability. Maybe there are other factors that have a stronger influence on the percentage of farmers in lower income groups than for higher incomes, where the income is a good predictor. \n\nA correlation analysis helps us identify such variances in the data relationship, and we should look at correlation coefficients and the direction of the relationship for different parts of the data. We often have a stronger relation across parts of the dataset, and a weaker relation across other parts of the dataset. These differences are important, as they hint at underlying influencing variables or factors that we did not understand yet.\n\n[[File:Correlation coefficient examples.png|600px|thumb|center|'''Examples for the correlation coefficient.''' Source: Wikipedia, Kiatdd, CC BY-SA 3.0]]\n<br>\n\n== Causality ==\nWhere to start, how to end? \n\n'''Causality is one of the most misused and misunderstood concepts in statistics.''' All the while, it is at the heart of the fact that all statistics are normative. While many things can be causally linked, many are not. The problem is that we dearly want certain things to be causally linked, while we want other things not to be causally linked. This confusion has many roots, and spans across such untameable arenas such as faith, psychology, culture, social constructs and so on. Causality can be everything that is good about statistics, and it can be equally everything that is wrong about statistics. To put it in other words: it can be everything that is great about us humans, but it can be equally the root cause of everything that is wrong with us.\n\nWhat is attractive about causality? People search for explanations, and this constant search is probably one of the building blocks of our civilisation. Humans look for reasons to explain phenomena and patterns, often with the goal of prediction. If I understood a causal relation, I may be able to know more about the future, cashing in on being either prepared for this future, or at least being able to react properly. \n\nThe problem with causality is that different branches of science as well as different streams of philosophy have different explanations of causality, and there exists an exciting diversity of theories about causality. Let us approach the topic systematically."],"13":["=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n<\/syntaxhighlight>\n\nClear and meaningful coding and plots are important. In order to achieve this, we have to change the names of the variables from the \u201cmtcars\u201d dataset into something meaningful. One way to do this, is to change the names of the columns and rows of the correlation variable.\n<syntaxhighlight lang=\"R\" line>\ncorrelations <- cor(mtcars)[1:11, 1:11]\ncolnames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\nrownames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\n<\/syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=\"R\" line>\n# Fig.5\ncorrplot(correlations,\n         method = \"circle\",\n         type = \"upper\",\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         tl.cex = 0.6)\n<\/syntaxhighlight>\n\nThe parameters are different from the previous scatter plots. Obviously, here you need the corrplot() function and define your parameters, regarding to your preferred taste, in this function. Some of the parameters will be explained briefly.\n\n* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (\u201ccircle\u201d, \u201csquare\u201d, \u201cellipse\u201d, \u201cnumber\u201d, \u201cshade\u201d, \u201ccolor\u201d, \u201cpie\u201d), \u201ccircle\u201d is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* '''type''': how the correlation matrix will be displayed. It can either be \u201cupper\u201d, \u201clower\u201d or \u201cfull\u201d. Full is called by default.\n* '''order''': order method for the correlation coefficients. The \u201chclust\u201d method orders them in hierarchical order, but it also possible to order them alphabetical (\u201calphabetical\u201d) or with a [[Principal_Component_Analysis|principal component analysis]] (\u201cPCA\u201d).\n* '''tl.col''': color of the labels.\n* '''tl.srt:''' rotation of the labels in degrees.\n* '''tl.cex:''' size of the labels.\n\n== Visualisation with ggplot ==\n=== Overview ===\n=== R code ===\nCOMING SOON\n\nAs you can see, there are many different ways to visualize correlations between variables. The right correlation plot depends on your data and on the number of variables you want to analyze. But never forget, correlation plots show you only the relationship between the variables and nothing about the causality.\n\n\n== References ==\n* https:\/\/sustainabilitymethods.org\/index.php\/Causality_and_correlation\n* https:\/\/en.wikipedia.org\/wiki\/Correlation_and_dependence\n* https:\/\/codingwithmax.com\/correlation-vs-causation-examples\/\n* https:\/\/towardsdatascience.com\/what-it-takes-to-be-correlated-ce41ad0d8d7f\n* http:\/\/www.r-tutor.com\/elementary-statistics\/numerical-measures\/correlation-coefficient\n\nA nice example that shows how easy it is to create a spurious correlation:\n\n* https:\/\/rstudio-pubs-static.s3.amazonaws.com\/4192_1180a799cd6c4d2ba6e4ed2702860efb.html\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden and ?.","'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br\/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https:\/\/sustainabilitymethods.org\/index.php\/Partial_Correlation this entry].\n__TOC__\n<br\/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, '''correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!'''\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\ndata(\"mtcars\")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = \"Correlation between Miles per Gallon and Horsepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     pch = 16,\n     col = \"red\",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n<\/syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=\"R\" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n<\/syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=\"R\" inline>plot()<\/syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* '''x:''' variable, that will be displayed on the x-axis.\n* '''y:''' variable, that will be displayed on the y-axis.\n* '''xlab:''' title for the x-axis.\n* '''ylab:''' title for the y-axis.\n* '''pch:''' shape and size of the plotted observations, in this case, filled circles. [http:\/\/www.sthda.com\/english\/wiki\/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* '''col:''' plotting color. You can either write the name of the color or use the [https:\/\/www.r-graph-gallery.com\/41-value-of-the-col-function.html color number].\n* '''las:''' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* '''xlim:''' set the limit of the x-axis.\n* '''ylim:''' set the limit of the y-axis.\n* '''abline:''' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n<\/syntaxhighlight>","=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n<\/syntaxhighlight>\n\n\nThe scatter plot matrix from this package is already very nice by default. It splits the plot into an upper, lower and diagonal part. The upper part consists of the correlation coefficients for the different variables. The red stars show you the results of the implemented correlation test. There is a range from zero to three stars and the higher the number of stars, the higher is the significance of the results for the test. In the diagonal part of the plot are histograms for every variable and show you the distribution of the variable. The bivariate scatter plots can be found on the lower part of the plot and contain a fitted line by default.\n\n\n== Line chart ==\n=== Definition ===\nA line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value.  \n\nMultiple categories can be displayed in the same view, each represented by a unique line. Sometimes a point (circle\/dot) is also used to substantiate the visibility of individual values. The lines used in a line chart will generally be straight. However, sometimes curved line interpolation may be used as a method of estimating values between known data points. This approach can be useful to help emphasise a general trend. While this might slightly compromise the visual accuracy of discrete values if you already have approximations, this will have less impact.\n\n=== R Code ===\nWe will first plot a basic line chart based on a built-in dataset called <syntaxhighlight lang=\"R\" inline>EuStockMarkets<\/syntaxhighlight>. The data set contains data on the closing stock prices of different European stock indices over the years 1991 to 1998.\n\nTo make things easier, we will first transform the built-in dataset into a data frame object. Then, we will use that data frame to create the plot.\n\nThe table that contains information about the different market indices looks like this:\n\n{| class=\"wikitable\"\n|-\n! DAX !! SMI !! CAC !! FTSE\n|-\n| 1628.75|| 1678.1 || 1772.8 || 2443.6\n|-\n| 1613.63|| 1688.5 || 1750.5 || 2460.2\n|-\n| 1606.51|| 1678.6 || 1718.0 || 2448.2\n|-\n| ... || ... || ... || ...\n|}\n[[File:Simple line chart.png|350px|thumb|right|Fig.3]]\nHere, the data for all the columns are numeric.\n\nThe following line chart shows how the <syntaxhighlight lang=\"R\" inline>DAX<\/syntaxhighlight> index from the table from previous section.\n\n<syntaxhighlight lang=\"R\" line>\n# Fig.3\n#read the data as a data frame\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX,  # simply select a stock index\n     type='l')       # choose 'l' for line chart\n<\/syntaxhighlight>\n\n[[File:Line chart.png|350px|thumb|right|Fig.4]]\nAs you can see, the plot is very simple. We can enhance the way this plot looks by making a few tweaks, making it more informative and aesthetically pleasing.\n\n<syntaxhighlight lang=\"R\">\n# Fig.4\n# get the data\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX, # select the data\n     type='l',      # choose 'l' for line chart\n     col='blue',    # choose the color of the line\n     lwd = 2,       # choose the line width \n     main = 'Line Chart of DAX Index (1991-1998)',         # title of the plot\n     xlab = 'Time (1991 to 1998)', ylab = 'Prices in EUR') # x- and y-axis labels\n<\/syntaxhighlight>\n\nYou can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot<\/syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n<\/syntaxhighlight>"],"14":["THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https:\/\/www.kaggle.com\/datasets\/jaganadhg\/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=\"Python\" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n<\/syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=\"Python\" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir('\/path\/to\/your\/directory')\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndf = pd.read_csv('D202.csv')\ndf.head()\n<\/syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let's choose the most relevant columns for our research:\n\n<syntaxhighlight lang=\"Python\" line>\n## Let's choose the most relevant columns for our research:\ndf['start_date'] = pd.to_datetime(df['DATE'] + ' ' + df['START TIME'])\ndf['cost_dollars'] = df['COST'].apply(lambda x: float(x[1:]))\ndf.rename(columns={'USAGE': 'usage_kwh'}, inplace=True)\ndf = df.drop(columns=['TYPE', 'UNITS', 'DATE', 'START TIME', 'END TIME', 'NOTES', 'COST']).set_index('start_date')\n<\/syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe \u201ccost_dollars\u201d by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don't specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=\"Python\" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n<\/syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https:\/\/lg4ml.org\/grounddetection\/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=\"Python\" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y='usage_kwh',\n        title='Usage of Electricity over 2 Weeks',\n        labels={'start_date': 'Date', 'usage_kwh': 'Usage (KWh)'}) ## uses the data from \"start_date\" called \"Date\", and the data of \"usage_kwh\" called \"usage (KwH)\"\n<\/syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time<\/small>\n\n<syntaxhighlight lang=\"Python\" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df['usage_kwh'])) ## transforms into frequencies\nfrequencies = rfftfreq(df['usage_kwh'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nLoom is a '''video recording software which lets you easily record your screen and yourself at the same time''' right in your browser. This way, you can show your work or ideas to colleagues or others, and not waste time and harddrive space on recording videos right on your computer.\n\n== Goals ==\n* Communicate your work to colleagues, collaborators, students in a way that is precise and engaging.\n* Better enable others to prepare for meetings.\n\n== Getting started ==\n[[File:Loom Logo.png|400px|thumb|right|'''Loom logo.''' Source: [https:\/\/www.loom.com\/ Loom]]]\nFor some situations, it might be practical to record your screen and talk over what is shown. Loom lets you record your browser, Excel file, presentation or anything you open on your computer, and record your face and voice while clicking through the applications. For browser recordings, you can simply enable a Loom extension; for anything else, you download the software; and there is also a mobile app. You just record the video and share the link and others can watch it online. There are voice transcripts, and you can share the video alongside a call-to-action, if you like.\n\n'''We found Loom to be helpful to convey a point to colleagues so that they can prepare for a meeting, or to quickly summarize our work for colleagues and others''', so that they can engage with the video whenever they find the time. Loom is easier than recording the video with external software, and sharing or re-uploading the created .mp4-file. As opposed to video-call screen-sharing, Loom can be used asynchronously.\n\n[https:\/\/www.loom.com\/ Loom] is free for anyone and allows for 5-minute videos, up to 100 videos in total. There are paid options that allow for recordings of up to 45 minutes, and these are [https:\/\/www.loom.com\/education free for educational accounts].\n\n== Links & Further reading ==\n* [https:\/\/www.youtube.com\/watch?v=3PY6v9s1MU8 An introductory video to using Loom] \n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","__NOTITLE__\n== Welcome to the ''Sustainability Methods Wiki!'' ==\nThe aim of this Wiki is to present and explain fundamental methods, terms and tools relevant to (Sustainability) Science and discuss underlying questions. The Wiki is composed of several sub-wikis:\n\n{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = [[Courses]] | class = center |\n|content =\n<nowiki><\/nowiki>\nThis section revolves around '''curated selections of Wiki entries (and more)''' as introductions to specific topics.\n}}\n\n{{InfoCard\n|heading = [[Methods]] | class = center |\n|content =\n<nowiki><\/nowiki>\nMethods are at the heart of scientific research. '''Learn about the most important methods in Sustainability Science - and beyond.'''\n}}\n\n\n}}\n\n{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = [[Skills & Tools]] | class = center |\n|content =\n<nowiki><\/nowiki>\nEvery type of work can be facilitated through appropriate Skills & Tools. '''Dive in and learn something new!'''\n}}\n\n{{InfoCard\n|heading = [[Normativity of Methods]] | class = center |\n|content =\n<nowiki><\/nowiki>\nThe choice of methods influences the knowledge we produce. '''Here you can learn more about this relation.'''\n}}\n\n}}\n\n== First time visitor? ==\n'''Please watch this short introduction to the Wiki.'''\n{{#evu:https:\/\/www.youtube.com\/watch?v=MjlJTjzLg6M\n|alignment=center\n}}\n<br\/>\n* Have a look at the '''[[Sustainability_Methods:About|About]] page.''' Here, you will find more information on what the Wiki is all about. It also contains a FAQ section with answers to some general questions concerning the Wiki, and a contact mail in case you would like to provide feedback or ask further questions.\n\n* Now you can get started! '''Pick a section above''' and choose one or more entries to learn about. <br\/>\n\n* If you do not know which methods to start with, have a look at the '''[https:\/\/sustainabilitymethods.org\/method_recommendation_tool Method Recommendation Tool]'''!\n\n* You can also generate a random entry on top of the page or read one of the 5 newest Wiki entries:\n{{Special:NewestPages\/-\/5}}\n\n__NOTOC__"],"15":["[[File:Qqplot notnomral.jpg|thumb|left|5. A gamma distribution, where the variances increases with the square of the mean.]]\n[[File:Qqplot negbinom.jpg|thumb|center|6. A negative binomial distribution that is clearly not following a normal distribution. In other words here the points are not on the line, the visual inspection of this qqplot concludes that your residuals are not normally distributed.]]\n\n===Non-normal distributions===\n'''Sometimes the world is [https:\/\/www.statisticshowto.com\/probability-and-statistics\/non-normal-distributions\/ not normally distributed].''' At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which [https:\/\/www.isixsigma.com\/tools-templates\/normality\/dealing-non-normal-data-strategies-and-tools\/ reasons] exists why data is not normally distributed. \n\n==== The Poisson distribution ====\n[[File:Bildschirmfoto 2020-04-08 um 12.05.28.png|thumb|500px|'''This picture shows you several possible poisson distributions.''' They differ according to the lambda, the rate parameter.]]\n\n[https:\/\/www.youtube.com\/watch?v=BbLfV0wOeyc Things that can be counted] are often [https:\/\/www.britannica.com\/topic\/Poisson-distribution not normally distributed], but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. '''How many people do you think drink one or two cups of coffee per day? Quite many, I guess.''' How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes. \n\nNow here comes the funny surprise. Transform the data following a [https:\/\/towardsdatascience.com\/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 Poisson distribution], and it will typically follow the normal distribution if you use the decadic logarithm (log). Hence skewed data can be often transformed to match the normal distribution. While many people refrain from this, it actually may make sense in such examples as [https:\/\/sustainabilitymethods.org\/index.php\/Is_the_world_linear%3F island biogeography]. Discovered by MacArtur & Wilson, it is a prominent example of how the log of the numbers of species and the log of island size are closely related. While this is one of the fundamental basic of ecology, a statistician would have preferred the use of the Poisson distribution.\n\n===== Example for a log transformation of a Poisson distribution =====\n[[File:Poisson Education small.png|thumb|400px|left]]\n[[File:Poisson Education log small.png|thumb|400px|left]]\nOne example for skewed data can be found in the R data set \u201cswiss\u201d, it contains data about socio-economic indicators of about 50 provinces in Switzerland in 1888. The variable we would like to look at is \u201cEducation\u201d, which shows how many men in the army (in %) have an education level beyond primary school. \nAs you can see when you look at the first diagram, in 30 provinces only 10 percent of the people received education beyond the primary school.\n\nTo obtain a normal distribution (which is useful for many statistical tests), we can use the natural logarithm.\n\nIf you would like to know, how to conduct an analysis like on the left-hand side, we uploaded the code right below:\n\n<syntaxhighlight lang=\"R\" line>\n\n# we will work with the swiss() dataset.\n# to obtain a histogram of the variable Education, you type\n\nhist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n<\/syntaxhighlight>\n\n====The Pareto distribution====\n[[File:Bildschirmfoto 2020-04-08 um 12.28.46.png|thumb|300px|'''The Pareto distribution can also be apllied when we are looking at how wealth is spread across the world.''']]\n\n'''Did you know that most people wear 20 % of their clothes 80 % of their time?''' This observation can be described by the [https:\/\/www.youtube.com\/watch?v=EAynHZE-lK4 Pareto distribution]. For many phenomena that describe proportion within a given population, you often find that few make a lot, and many make few things. Unfortunately this is often the case for workloads, and we shall hope to change this. For such proportions the [https:\/\/www.statisticshowto.com\/pareto-distribution\/ Pareto distribution] is quite relevant. Consequently, it is rooted in [https:\/\/www.pragcap.com\/the-pareto-principle-and-wealth-inequality\/ income statistics]. Many people have a small to average income, and few people have a large income. This makes this distribution so important for economics, and also for sustainability science.\n\n\n=== Visualizing data: Boxplots ===\nA nice way to visualize a data set is to draw a [[Barplots,_Histograms_and_Boxplots#Boxplots|boxplot]]. You get a rough overview how the data is distributed and moreover you can say at a glance if it\u2019s normally distributed. The same is true for [[Barplots,_Histograms_and_Boxplots#Histograms|histograms]], but we will focus on the boxplot for now. For more information on both these forms of data visualisation, please refer to the entry on [[Barplots, Histograms and Boxplots]].","#Now we can for example test, if the difference in sepal length between setosa and virginica is significant:\n#H0 hypothesis: The medians (distributions) of setosa and virginica are equal\n#H1 hypothesis: The medians (distributions) of setosa and virginica differ\n\n#test for normality\nshapiro.test(setosa$Sepal.Length)\nshapiro.test(virginica$Sepal.Length)\n#both are normally distributed\n\n#wilcoxon sum of ranks test\nwilcox.test(setosa$Sepal.Length,virginica$Sepal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it's almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis.\n#Based on this result we may conclude the medians of these two distributions differ. \n#The alternative hypothesis is stated as the \u201ctrue location shift is not equal to 0\u201d. \n#That\u2019s another way of saying \u201cthe distribution of one population is shifted to the left or \n#right of the other,\u201d which implies different medians. \n\n<\/syntaxhighlight>\n|}\n\n\n====f-test====\n'''The f-test allows you to compare the ''variance'' of two samples.''' Variance is calculated by taking the average of squared deviations from the mean and tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean. If the p-value of the f-test is lower than 0,05, the variances differ significantly. Important: for the f-Test, the data of the samples has to be normally distributed. \n\n'''Example''': If you examine players in a basketball and a hockey team, you would expect their heights to be different on average. But maybe the variance is not. Consider Figure 1 where the mean is different, but the variance the same - this could be the case for your hockey and basketball team. In contrast, the height could be distributed as shown in Figure 2. The f-test then would probably yield a p-value below 0,05.\n\n[[File:Normal distribution.jpg|400px|thumb|left|Figure 1 shows '''two datasets which are normally distributed, but shifted.''' Source: [https:\/\/snappygoat.com\/s\/?q=bestof%3ALda-gauss-variance-small.svg+en+Plot+of+two+normal+distributed+variables+with+small+variance+de+Plot+zweier+Normalverteilter+Variablen+mit+kleiner+Varianz#7c28e0e4295882f103325762899f736091eab855,0,3 snappy goat]]]\n[[File:NormalDistribution2.png|400px|thumb|right|Figure 2 '''shows two datasets that are normally distributed, but have different variances'''. Source: [https:\/\/www.notion.so\/sustainabilitymethods\/Simple-Statistical-Tests-bcc0055304d44564bc41661453423134#7d57d8c251f94da8974eeb8a658aaa29 Wikimedia]]]\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the f-Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an f-Test.\n#We will compare the variances of height of two fictive populations. First, we create two vectors with the command 'rnorm'. Using rnorm, you can decide how many values your vector should contain, besides the mean and the standard deviation of the vector. To learn, what else you can do with rnorm, type:\n?rnorm\n\n#Creating two vectors\nPopA=rnorm(40, mean=175, sd=1)\nPopB=rnorm(40, mean=182, sd=2)\n\n#Comparing them visually by creating histograms\nhist(PopA)\nhist(PopB)\n\n#Conducting a f-test to compare the variances\nvar.test(PopA, PopB) \n\n#And this is the result, telliing you that the two variances differ significantly\nF test to compare two variances\n\ndata:  PopA and PopB\nF = 0.38584, num df = 39, denom df = 39, p-value = 0.00371\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2040711 0.7295171\nsample estimates:\nratio of variances \n         0.3858411 \n<\/syntaxhighlight>\n|}\n\n\n== Normativity & Future of Simple Tests ==\n'''Simple tests are not abundantly applied these days in scientific research, and often seem outdated.''' Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion.\n\nHowever, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics. \n\nSimple tests are often the endpoint of many introductionary teachings on statistics, which is unfortunate. Overall, their lack in most of recent publications as well as wooden design frames of these approaches make these tests an undesirable starting point for many students, yet they are a vital stepping stone to more advanced models.\n\nHopefully, one day school children will learn simple test, because they could, and the world would be all the better for it. If more people early on would learn about probability, and simple tests are a stepping stone on this long road, there would be an education deeper rooted in data and analysis, allowing for better choices and understanding of citizens.\n\n\n== Key Publications ==\n* Student\" William Sealy Gosset. 1908. ''The probable error of a mean.'' Biometrika 6 (1). 1\u201325.\n* Cochran, William G. 1952. ''The Chi-square Test of Goodness of Fit''. The Annals of Mathematical Statistics 23 (3). 315\u2013345. \n* Box, G. E. P. 1953. ''Non-Normality and Tests on Variances.'' Biometrika 40 (3\/4). 318\u2013335.","'''Distributions can have different [https:\/\/www.youtube.com\/watch?v=XSSRrVMOqlQ skews].''' There is the symmetrical skew which is basically a normal distributions or bell curve that you can see on the picture. But normal distributions can also be skewed to the left or to the right depending on how mode, median and mean differ. For the symmetrical normal distribution they are of course all the same but for the right skewed distribution (mode < median < mean) it's different.\n\n\n==== Detecting the normal distribution ====\n[[File:Car Accidents Barplot 2.jpg|thumb|400px|left|'''This is a time series visualized through barplots.''']]\n[[File:Car Accidents Histogram 2.jpg|thumb|400px|left|'''This is the same data as a histogram.''']]\n[[File:Car Accidents Boxplot 2.jpg|thumb|400px|left|'''And this the data as a boxplot.''' You can see that the data is normally distributed because the whiskers and the quarters have nearly the same length.]]\n'''But when is data normally distributed?''' And how can you recognize it when you have a [[Barplots, Histograms and Boxplots|boxplot]] in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot's whiskers are largely uneven.\n\nYou can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n'''If you would like to know how one can create the diagrams which you see here, this is the R code:'''\n\n<syntaxhighlight lang=\"R\" line>\n\n# If you want some general information about the \"Seatbelt\" dataset, at which we will have look, you can use the ?-function.\n# As \"Seatbelts\" is a dataset in R, you can receive a lot of information here. You can see all datasets available in R by typing data().\n\n?Seatbelts\n     \n# to have a look a the dataset \"Seatbelts\" you can use several commands\n  \n## str() to know what data type \"Seatbelts\" is (e.g. a Time-Series, a matrix, a dataframe...)\nstr(Seatbelts)\n        \n## use show() or just type the name of the dataset (\"Seatbelts\") to see the table and all data it's containing\nshow(Seatbelts)\n# or\nSeatbelts\n      \n## summary() to have the most crucial information for each variable: minimum\/maximum value, median, mean...\nsummary(Seatbelts)\n\n     \n# As you saw when you used the str() function, \"Seatbelts\" is a Time-Series, which makes it hard to work with it. We should change it into a dataframe (as.data.frame()). We will also name the new dataframe \"seat\", which is more handy to work with.\n  \nseat<-as.data.frame(Seatbelts)\n     \n# To choose a single variable of the dataset, we use the '$' operator. If we want a barplot with all front drivers,\n# who were killed or seriously injured:\n     \nbarplot(seat$front)\n     \n# For a histogram:\n     \nhist(seat$front)\n  \n## To change the resolution of the histogram, you can use the \"breaks\"-argument of the hist-command, which states\n## in how many increments the plot should be divided\n     \nhist(seat$front, breaks = 30)\nhist(seat$front, breaks = 100)\n\n# For a boxplot:\n     \nboxplot(seat$front)\n\n<\/syntaxhighlight>\n\n==== The QQ-Plot ====\n[[File:Data caterpillar.png|thumb|right|1. Growth of caterpillars in relation to tannin content in food]]\nThe command <syntaxhighlight land = \"R\" inline>qqplot<\/syntaxhighlight> will return a Quantile-Quantile plot. This plot allows for a visual inspection on how your model residuals behave in relation to a normal distribution. On the y-axis there are the standardised residuals and on the x-axis the theoretical quantiles. The simple answer is, if your data points are on this line you are fine, you have normal errors, and you can stop reading here. If you want to know more about the theory behind this please continue. \nResiduals is the difference of your response variable and the fitted values. \n<br>\n<br>\n'''Residuals = response variable - fitted values'''\n<br>\n<br>\nFor a regression analysis this would be the difference of your data points to the regression line. \nThe standardised residuals depend on the model function you are applying.\n\nIn the following example, the standardised residuals are the residuals divided by the standard deviation. Let's take the caterpillar data set as an example. On the right you can see the table with the data: growth of caterpillars in relation to tannin content of their diet. Below, we will discuss some correlation plots between these two factors.\n\n[[File:Plot caterpillar.png|thumb|left|2. Plotting the data in an x-y plot already gives you an idea that growth probably depends on the tannin content.]]\n[[File:Qqplot2.png|thumb|right|4. The qqplot for this model looks good. Here the points are mostly on the line with point 4 and point 7 being slightly above and below the line. Still you would consider the residuals in this case to behave normally.]]\n[[File:Plot regression.png|thumb|center|3. Plotted regression line of the regression model \n<syntaxhighlight land = \"R\" inline>lm(growth~tannin)<\/syntaxhighlight> for testing the relation between two factors]]"],"16":["'''In short:''' The (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''\u03bd = n\u22121'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value \u03bc, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i<\/sub>'' deviates significantly from a specified value ''\u03bc'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''\u03bc'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''\u03bc.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''\u03c3'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.\n\n==Two-sample t-test==\nWhen wanting to find out whether the means of two samples of a distribution are deviating significantly. If the two samples are independent from each other, an independent two-sample t-test has to be used. If the samples are dependent, which means that the values being tested stem from the same samples or that the two samples are paired, a paired t-test can be used.\n===Independent Samples===\nFor independent samples with similar variances (a maximum ratio of 2), the t-statistic is calculated in the following way:\n[[File:prbdst7.png|700px|frameless|center]]\nwith the estimated pooled standard deviation\n[[File:prbdst8.png|700px|frameless|center]]\nIn accordance with the One-sample t-test, the sample sizes, means and standard deviations of the samples 1 and 2 are denoted by ''n<sub>1\/2<\/sub>'', \u2002''x<sub>1\/2<\/sub>'' and ''\u03c3<sub>x<sub>1\/2<\/sub><\/sub>'' respectively.\nThe degrees of freedom which are required for conducting the hypothesis testing is given as ''\u03bd = n<sub>1<\/sub> + n<sub>2<\/sub> \u2212 2''.\nFor samples with unequal variances, meaning that one sample variance is more than twice as big as the other, Welch\u2019s t-test has to be used, leading to a different t-statistic t and different degrees of freedom ''\u03bd'':\n[[File:prbdst9.png|700px|frameless|center]]\nAn approximation for the degrees of freedom can be calculated using the Welch-Satterthwaite equation:\n[[File:prbdst10.png|700px|frameless|center]]\nIt can be easily shown, that the t-statistic simplifies for equal sample sizes:\n[[File:prbdst11.png|700px|frameless|center]]\n\n===Paired Samples===\nWhen testing whether the means of two paired samples are differing significantly, the t-statistic consists of variables that differ from the ones used in previous tests:\n[[File:prbdst12.png|700px|frameless|center]]\nInstead of the independent means and standard deviations of the samples, new variables are used, that depend on the differences between the variable pairs. ''x<sub>d<\/sub>'' is given as the average of the differences of the sample pairs and ''\u03c3<sub>D<\/sub>'' denotes the corresponding standard deviation. The value of ''\u03bc<sub>0<\/sub>'' is set to zero to test whether the mean of the differences takes on a significant value.\n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n[[Category:Quantitative]]\n\nThe [[Table_of_Contributors| authors]] of this entry is Moritz Wohlstein.","#Now we can for example test, if the difference in sepal length between setosa and virginica is significant:\n#H0 hypothesis: The medians (distributions) of setosa and virginica are equal\n#H1 hypothesis: The medians (distributions) of setosa and virginica differ\n\n#test for normality\nshapiro.test(setosa$Sepal.Length)\nshapiro.test(virginica$Sepal.Length)\n#both are normally distributed\n\n#wilcoxon sum of ranks test\nwilcox.test(setosa$Sepal.Length,virginica$Sepal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it's almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis.\n#Based on this result we may conclude the medians of these two distributions differ. \n#The alternative hypothesis is stated as the \u201ctrue location shift is not equal to 0\u201d. \n#That\u2019s another way of saying \u201cthe distribution of one population is shifted to the left or \n#right of the other,\u201d which implies different medians. \n\n<\/syntaxhighlight>\n|}\n\n\n====f-test====\n'''The f-test allows you to compare the ''variance'' of two samples.''' Variance is calculated by taking the average of squared deviations from the mean and tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean. If the p-value of the f-test is lower than 0,05, the variances differ significantly. Important: for the f-Test, the data of the samples has to be normally distributed. \n\n'''Example''': If you examine players in a basketball and a hockey team, you would expect their heights to be different on average. But maybe the variance is not. Consider Figure 1 where the mean is different, but the variance the same - this could be the case for your hockey and basketball team. In contrast, the height could be distributed as shown in Figure 2. The f-test then would probably yield a p-value below 0,05.\n\n[[File:Normal distribution.jpg|400px|thumb|left|Figure 1 shows '''two datasets which are normally distributed, but shifted.''' Source: [https:\/\/snappygoat.com\/s\/?q=bestof%3ALda-gauss-variance-small.svg+en+Plot+of+two+normal+distributed+variables+with+small+variance+de+Plot+zweier+Normalverteilter+Variablen+mit+kleiner+Varianz#7c28e0e4295882f103325762899f736091eab855,0,3 snappy goat]]]\n[[File:NormalDistribution2.png|400px|thumb|right|Figure 2 '''shows two datasets that are normally distributed, but have different variances'''. Source: [https:\/\/www.notion.so\/sustainabilitymethods\/Simple-Statistical-Tests-bcc0055304d44564bc41661453423134#7d57d8c251f94da8974eeb8a658aaa29 Wikimedia]]]\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the f-Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an f-Test.\n#We will compare the variances of height of two fictive populations. First, we create two vectors with the command 'rnorm'. Using rnorm, you can decide how many values your vector should contain, besides the mean and the standard deviation of the vector. To learn, what else you can do with rnorm, type:\n?rnorm\n\n#Creating two vectors\nPopA=rnorm(40, mean=175, sd=1)\nPopB=rnorm(40, mean=182, sd=2)\n\n#Comparing them visually by creating histograms\nhist(PopA)\nhist(PopB)\n\n#Conducting a f-test to compare the variances\nvar.test(PopA, PopB) \n\n#And this is the result, telliing you that the two variances differ significantly\nF test to compare two variances\n\ndata:  PopA and PopB\nF = 0.38584, num df = 39, denom df = 39, p-value = 0.00371\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2040711 0.7295171\nsample estimates:\nratio of variances \n         0.3858411 \n<\/syntaxhighlight>\n|}\n\n\n== Normativity & Future of Simple Tests ==\n'''Simple tests are not abundantly applied these days in scientific research, and often seem outdated.''' Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion.\n\nHowever, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics. \n\nSimple tests are often the endpoint of many introductionary teachings on statistics, which is unfortunate. Overall, their lack in most of recent publications as well as wooden design frames of these approaches make these tests an undesirable starting point for many students, yet they are a vital stepping stone to more advanced models.\n\nHopefully, one day school children will learn simple test, because they could, and the world would be all the better for it. If more people early on would learn about probability, and simple tests are a stepping stone on this long road, there would be an education deeper rooted in data and analysis, allowing for better choices and understanding of citizens.\n\n\n== Key Publications ==\n* Student\" William Sealy Gosset. 1908. ''The probable error of a mean.'' Biometrika 6 (1). 1\u201325.\n* Cochran, William G. 1952. ''The Chi-square Test of Goodness of Fit''. The Annals of Mathematical Statistics 23 (3). 315\u2013345. \n* Box, G. E. P. 1953. ''Non-Normality and Tests on Variances.'' Biometrika 40 (3\/4). 318\u2013335.","In the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https:\/\/data.library.virginia.edu\/the-wilcoxon-rank-sum-test\/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A\/B Testing==\n'''Advantages'''\nA\/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A\/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A\/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A\/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. \u201cOnline Controlled Experiments and A\/B Testing.\u201d Encyclopedia of Machine Learning and Data Mining, 2017, 922\u201329. https:\/\/doi.org\/10.1007\/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. \u201cExperimentation and Start-up Performance: Evidence from A\/B Testing.\u201d Management Science 68, no. 9 (September 2022): 6434\u201353. https:\/\/doi.org\/10.1287\/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A \/ B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart"],"17":["3. After the first round, the participants' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. 'Consensus' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).\n\nNext, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group's positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be \"(...) a highly motivating experience for participants\" (Rayens & Hahn 2000, p.309) due to the feedback on the group's opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which \"(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one's mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others.\" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)\n\nHowever, several potential pitfalls and challenges may arise during the Delphi process:\n* Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue (4, 5, 6).\n* A specific Delphi format that was useful in one study must not work as well in another context. Instead, the process must be adapted to the research design and underlying problem (4).\n* The proper selection of participating experts constitutes a major challenge for Delphi processes (3, 4, 5, 6). In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors (4).\n* The monitor (= researcher) must not impose their own preconceptions upon the respondents when developing the questionnaire but be open for contributions from the participants. The questions should be concise and understandable and should not incentivise the participant to \"get the job over with\" (Linstone & Turoff 1975, p.568; 5).\n* Diverse forms of [[Glossary|bias]] might occur on the part of the participants that need to be anticipated by the researcher. These include discount of the future, over-optimism \/ over-pessimism, misinterpretations with regard to the complexity and uncertainty involved in forecasting the future as well as other forms of bias that may be imposed through the feedback process (4, 6).\n* The responses must be adequately summarized, analyzed and presented to the participants (see the variety of measures for 'consensus' in What the method does)). \"Agreement about a recommendation, future event, or potential decision does not disclose whether the individuals agreeing did so for the same underlying reasons. Failure to pursue these reasons can lead to dangerously false results.\" (Linstone & Turoff 1975, p.568).\n* Disagreements between participants should be explored instead of being ignored so that the final consensus is not artificial (4).\n* The participants should be recompensated for their demanding task (4)\n\n\n== Normativity ==\n==== Connectedness \/ nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).\n\n==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the 'real-world'.","[[File:ConceptDelphi.png|450px|left|frameless|[[Sustainability Methods:About|Method categorization for Delphi]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/>\n<br\/>\n'''In short:''' The Delphi Method is an interactive form of data gathering in which expert opinions are summarized and consensus is facilitated.\n\n== Background ==\nThe Delphi method originates from work at RAND Corporation, a US think-tank that advises the US military, in the late 1940s and 1950s (2, 3, 5). RAND developed \"Project Delphi\" as a mean of obtaining \"(...) the most reliable consensus of opinion of a group of experts.\" (Dalkey & Helmer 1963, p.1). At the time, the alternative - extensive gathering and analysis of quantitative data as a basis for forecasting and deliberating on future issues - was not technologically feasible (4, 5). Instead, experts were invited and asked for their opinions - and Delphi was born (see (1)).\n\n[[File:Delphi Method SCOPUS.png|400px|thumb|right|'''SCOPUS hits for the Delphi method until 2019.''' Search terms: 'Delphi Panel', 'Delphi Method', 'Delphi Methodology', 'Delphi Study', 'Delphi Survey' in Title, Abstract, Keywords. Source: own.]]\n\nIn 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).\n\n\n== What the method does ==\nThe Delphi method is \"(...) a systematic and interactive research technique for obtaining the judgment of a panel of independent experts on a specific topics\" (Hallowell & Gambatese 2010, p.99). It is used \"(...) to obtain, exchange, and develop informed opinion on a particular topic\" and shall provide \"(...) a constructive forum in which consensus may occur\" (Rayens & Hahn 2000, p.309). Put simply, experts on a topic are gathered and asked in a systematic process what they think about the future, until consensus is found. \n\n==== The Delphi procedure ====\n[[File:ResultVisualisationDelphi.png|400px|thumb|right|Questionnaire results for the original RAND study, asking for an estimate of bomb requirements. The estimated numbers per participant converge over the course of the Delphi procedure. Source: Dalkey & Helmer 1963, p.15]]\n\n'''A Delphi process typically undergoes four phases''' (see (4), (6)):\n\n1. A group of experts \/ stakeholders on a specific issue is identified and invited as participants for the Delphi. These participants represent different backgrounds: academics, government and non-government officials as wel, as practitioners. They should have a diverse set of perspectives and profound knowledge on the discussed issues. They may be grouped based on their organizations, skills, disciplines or qualifications (3). Their number typically ranges from 10 up to 30, depending on the complexity of the issue (2, 3, 5, 6). \n\nThe researchers then develop a questionnaire. It is informed by previous research as well as input from external experts (not the participants) who are asked to contribute knowledge and potential questions on the pertinent issue (2, 5). The amount of [[Glossary|consultation]] depends on the expertise of the researchers on the respective issue (2).\n\n2. The questionnaire is used to ask for the participants' opinions and positions related to the underlying issue. The questions often take a 'ranking-type' (3): they ask about the likelihood of potential future situations, the desirability of certain goals, the importance of specific issues and the feasibility of potential policy options. Participants may be asked to rank the answer options, e.g. from least to most desirable, least to most feasible etc. (2). Participants may also be asked yes\/no questions, or to provide an estimate as a number. They can provide further information on their answers in written form. (8)\n\nThe questioning is most commonly conducted in form of a questionnaire but has more recently also been realized as individual, group, phone or digital interview sessions (2, 5). Digital questioning allows for real-time assessments of the answers and thus a quicker process. However, a step-by-step procedure provides more time for the researchers to analyze the responses (4).\n\n3. After the first round, the participants' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. 'Consensus' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).","==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the 'real-world'.\n\n\n== Outlook ==\n==== Open questions ====\n* The diverse fields in which the Delphi method was applied has diversified and thus potentially confounded its methodological homogeneity, raising the need for a more comparable application and reporting of the method (6, 7)\n\n\n== An exemplary study ==\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 title.png|600px|frameless|center|The title of the exemplary study for Delphi method. Source: kauko & Palmroos 2014]]\nIn their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. '''The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets.''' Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks' foreign net assets etc.) with simple point estimates. These variables were chosen to \"fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible.\" (p.316). The participants were provided with information on the past developments of each of these variables.\n\nThe researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. '''The responses from the respective previous round were re-distributed to the participants''' (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again. \n\nAfter the third round, the participants were additionally asked to fill out survey questions on a 1-5 [[Likert Scale]] about how reliable their considered their own forecasts, and how much attention they had paid to the others' forecasts and comments when these were re-distributed, both regarding each variable individually. This was done to better understand each individual's thought process.\n<br>\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 results.png|800px|thumb|center|'''The results for the Delphi process.''' It shows that the mean estimates of the group became better over time, and were most often quite close to the actual realisation. Source: Kauko & Palmroos 2014, p.326.]]\n<br>\nThe forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. '''They found that the individual responses did indeed converge over the Delphi process, and that the \"Delphi group improved between rounds 1 and 3 in 13 of the questions.\"''' (p.320). They also found that \"[d]isagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement\" (p.322) and that the Delphi process \"clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future\", which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that \"paying attention to each others' answers made the forecasts more accurate\" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.\n\n\n== Key Publications ==\n* Linstone, H. Turoff, M. 1975. ''The Delphi Method: Techniques and Applications''. Addison-Wesley, Boston.\nAn extensive description of the characteristics, history, pitfalls and philosophy behind the Delphi method.\n* Dalkey, N. Helmer, O. 1963. An experimental application of the Delphi method to the use of experts. Management Science 9(3). 458-467.\nThe original document illustrating the first usage of the ''Delphi'' method at RAND.\n* Gordon, T.J. Helmer, O. 1964. Report on a long-range forecasting study. RAND document P-2982.\nThe report that popularized ''Delphi'' outside of the military defense field."],"18":["[[File:SustainabilityCompetencies.png|750px|thumb|center|'''Key Competencies for Sustainability.''' Source: Wiek et al. 2011, p.206]]\nThe criteria from Wiek et al are outstanding in the capacity to serve as boundary object, since I do not see these categories as mutually exclusive, but instead strongly interwoven with each other. I can whole-heartedly recommend to return to these criteria then and again, and to reflect on yourself through the lense of these criteria.\n\n== Knowledge for action-oriented sustainability science ==\nAt the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and context-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science:\n[[File:Types of knowledge for action-oriented sustainability science.png|900px|thumb|center|'''Types of knowledge for action-oriented sustainability science.''' Source: Caniglia et al. 2020, p.4]]\n\nThis showcases that this knowledge - and more importantly - the perspective from a philosophy-of-science viewpoint is only starting to emerge, and much more work will be needed until our methodological canon and the knowledge we want to produce enable us to solve the problems we are facing, but also to create these solution in ways that are closer to a mode how we want to create these solutions. We may well be able to solve certain things, and to produce knowledge that can be seen as solutions. I would however argue, that it also matters how we create these solutions and how we create knowledge. Only if people are empowered and society and science work seamlessly together - with ethical restrictions and guidelines in place, of course - will we not only produce the knowledge needed, but we also produce it in a way how we should as scientists. '''Science is often disconnected and even arrogant, and building an educational system that is reflexive and interconnected will be maybe the largest challenge we face.''' This is why we give you these criteria here, because I think that we need to consider what further design criteria can be in order to enhance and diversify our conceptual thinking about the scientific methodological canon. Plurality on scientific methods will necessarily mean to evolve, and in this age of interconnectedness, our journey is only beginning.\n\n== Interaction with stakeholders == \nScientific methods can engage with non-scientific actors on diverse levels, depending on the extent of their involvement in the process of scientific inquiry. Interaction with stakeholder may be especially relevant in [[Transdisciplinarity|transdisciplinary]] research. \n<br\/>'''Here, we refer to four levels of interaction:'''\n* ''Information'': Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation.\n* ''Consultation'': A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process.\n* ''Collaboration'': Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue.\n* ''Empowerment'': The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and\/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.\n\nYou can find more on these four categories in Brandt et al 2013, where a general introduction to the research landscape of transdisciplinary research in sustainability science is given. More will follow later on such approaches, and so much more still has to follow in science overall, since the declared distinction of science being in an ivory tower is only slowly crumbling. We need to question this [[Levels of Theory|paradigm]], and [[Questioning the status quo in methods|be critical of the status quo of normal science]]. More knowledge is needed, and especially, different knowledge.\n\n== References ==\n* Wiek et al. 2011. ''Key competencies in sustainability: a reference framework for academic program development''. Sustainability Science 6. 203-218.\n* Caniglia, G., Luederitz, C., von Wirth, T., Fazey, I., Mart\u00edn-L\u00f3pez, B., Hondrila, K., K\u00f6nig, A., von Wehrden, H., Sch\u00e4pke, N.A., Laubichler, M.D. and Lang, D.J., 2020. ''A pluralistic and integrated approach to action-oriented knowledge for sustainability.'' Nature Sustainability, pp.1-8.\n* Brandt, P., Ernst, A., Gralla, F., Luederitz, C., Lang, D.J., Newig, J., Reinert, F., Abson, D.J. and Von Wehrden, H., 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological economics, 92, pp.1-15.\n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz.","Wiek, A. and Lang D.J., 2016.Transformational Sustainability Research Methodology\u201d in Heinrichs, H. et al. (eds.), 2016. Sustainability Science, Dordrecht: Springer Netherlands. Available at: http:\/\/link.springer.com\/10.1007\/978-\u201094-\u2010017-\u20107242-\u201062.   \n\n\n== References ==\n* Lang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges''.\n\n* Kates et al. 2015. ''Sustainability Science''.\n\n* Stock, P. Burton, R.J.F. 2011. ''Defining Terms for Integrated (Multi-Inter-Trans-Disciplinary Sustainability Research)''. Sustainability 3. 1090-1113.\n\n* Arnold, A. Piontek, F. ''Zentrale Begriffe im Kontext der Reallaborforschung.'' in: Defila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\n* Gibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\n* Mauser et al. 2013. ''Transdisciplinary global change research: the co-creation of knowledge for sustainability.'' Current Opinion in Environmental Sustainability 5. 420-431.\n\n* Ruppert-Winkel et al. 2015. ''Characteristics, emerging needs, and challenges of transdisciplinary sustainability science: experiences from the German Social-Ecological Research Program.'' Ecology and Society 20(3). 13-30.\n\n* Hall, T. E. O'Rourke, M. 2014. ''Responding to communication challenges in transdisciplinary sustainability science. Heuristics for transdisciplinary sustainability studies: Solution-oriented approaches to complex problems.'' 119-139.\n\n* Allington, G. R. H., M. E. Fernandez-Gimenez, J. Chen, and D. G. Brown. 2018. ''Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau.'' Ecology and Society 23(2):9. \n\n* Pfeiffer, C. Schodl, K. Fuerst-Waltl, B. Willam, A. Leeb, C. Winckler, C. 2018. ''Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach.'' Journal fo Central European Agriculture 19(4). 858-864.\n\n\n== Further Information ==\n* The [http:\/\/intrepid-cost.ics.ulisboa.pt\/about-intrepid\/ INTREPID] network revolves around transdisciplinary and interdisciplinary research and provides useful actors and learnings in this field.\n* [http:\/\/www.transdisciplinarity.ch\/td-net\/Aktuell\/td-net-News.html TD-NET] is a resourceful Swiss platform that organizes and presents activities in the field. The same can be said about the [https:\/\/complexitycontrol.org\/methods-of-transdisciplinary-research\/ Complexity or Control blog], hosted at Leuphana University.\n* The [https:\/\/www.reallabor-netzwerk.de\/ Reallabor-Netzwerk] also hosts useful information about real world laboratories and transdisciplinary research.\n* You can find a lot of considerations concerning transdisciplinarity on [https:\/\/i2insights.org\/tag\/transdisciplinarity-general-relevance\/ this part of the 'Integration and Insights' blog]. \n* Mauser et al. (2013, p.420) present the ''[https:\/\/futureearth.org\/initiatives\/ Future Earth Initiative]'', which emerged from Rio+20 and \"will provide a new platform and paradigm for integrated global environmental change research that will be designed and conducted in partnership with society to produce the knowledge necessary for societal transformations towards sustainability\".\n\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz.","The course '''Methods of Environmental Sciences''' covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective. \n\n__TOC__\n<br\/>\n==== Definition & History of Methods ====\nWithin this lecture we deeply focus on the formation of a new arena in science that is including not only system knowledge, but also normative knowledge as well as transformative knowledge. In order to create solution for the problems we currently face, a solution orientated agenda is necessary.  This may demand the creation of novel methodological pathways to knowledge creation. Here, we give a tentative overview on the developments up until now.\n* [[History of Methods in Sustainability Science]]\n\n==== Design Criteria of Methods in Sustainability Science ====\nThere are several design criteria that allow you to systematise methods. Many of these criteria are part of the \u201cusual suspects\u201d in normal science ''sensu strictu'' Kuhn. Here, we discuss further design criteria and knowledge types that can be relevant to systematise knowledge production through methods for sustainability science.\n* [[Design Criteria of Methods in Sustainability Science]]\n\n==== [[Thought Experiments]] & [[Legal Research]] ====\n\n==== Quantitative Methods in the Humanities ====\n* [[Causality and correlation]]\n\n==== [[Geographical Information Systems]] ====\n\n==== [[Grounded Theory]] ====\n\n==== Interviews ====\n* [[Semi-structured Interview]]<br\/>\n* [[Open Interview]]\n\n==== The ecological experiment ====\n* [[Experiments and Hypothesis Testing]]\n\n==== Causal Loop Diagrams ====\n* [[System Thinking & Causal Loop Diagrams]]\n\n==== Questioning the status quo in method-driven research ====\n* [[Questioning the status quo in methods]]\n\n==== [[Social Network Analysis]] ====\n\n==== Meta-Analysis ====\n* [[Meta-Analysis]]\n* [[Systematic Literature Review]]\n\n==== Mixed Methods in transdisciplinary research ====\n* [[Transdisciplinarity]]\n* [[Visioning & Backcasting]]\n* [[Scenario Planning]]\n* [[Living Labs & Real World Laboratories]]\n\n----\n[[Category: Courses]]"],"19":["The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br\/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n'''Forming hypotheses'''\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n'''Limitations of hypothesis'''\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - \u2018causal\u2019 form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]","[[File:Bildschirmfoto 2020-06-22 um 08.25.14.png|400px|thumb|right|'''Derek parfit is one of the most famous philosophers of the 20th and early 21st century.''' His thesis on normative ethics can be seen as his most important work.]]\n\nFollowing Sidgwicks \"Methods of ethics\", ethics can be defined as ''the world how it ought to be''. Derek Parfit argued that if philosophy be a mountain, Western philosophy climbs it from three sides: \n* The first side is Utilitarianism, which is widely preoccupied with the question how we can evaluate the outcome of an action. The most ethical choice would be the action that creates the most good for the largest amount of people.\n* The second side is reason, which can be understood as the human capability to reflect what one is ought to do. Kant said much to this end, and Parfit associated it to the individual, or better, the reasonable individual.\n* The third side of the mountain is the social contract, which states that a range of moral obligations are agreed upon in societies, a thought that was strongly developed by Locke. Parfit associated this even wider, referring to the whole society in his triple theory. \n\n\n== Do ethics matter for statistics? ==\nPersonally, I think ethics matters deeply for statistics. Let me try to convince you. \nLooking at the epistemology of statistics, we learned that much of modern civilisation is built on statistical approaches, such as the design and analysis of [[Experiments]] or the [[Correlations|correlation]] of two continuous variables. Statistics propelled much of the exponential growth in our [[Scientific methods and societal paradigms|society]], and Statistics is responsible for many of the problems we currently face through our unsustainable behaviour. After all, Statistics was willing to utilize means and accomplish goals that led us more into the direction of a more unsustainable path. Many would argue now that if statistics were a weapon, itself would not kill. Instead, it would be the human hand that uses it. This is insofar true as statistics would not exist without us, just as weapons were forged by us. However, I would argue that statistics are still deeply [[Normativity of Methods|normative]], as they are associated with our culture, society, social strata, economies and so much more. This is why we should embrace a critical perspective on statistics. Much in our society depends on statistics, and many decisions are taken because of statistics. As we have learned, some of these statistics might be problematic or even wrong, and consequently, this can render the decisions wrong as well. More strangely, our statistics can be correct, but as we have learned, statistics can even then contribute to our downfall, for instance when they contribute to a process that leads to unsustainable production processes. '''We may calculate something correctly, but the result can be morally wrong.''' Ideally, our statistics would always be correct, and the moral implications that follow out of our actions that are informed by statistics are also right.\n \n[[File:Bildschirmfoto 2020-06-22 um 08.35.42.png|600px|left|thumb|'''GDP is a good example of a construct used in statistics which influences our daily life and especially politics to a huge extent.''']]\n'''However, statistics is more often than not seen as something that is not normative, and some people consider statistics to create objective knowledge.''' This is rooted in the deep traditions and norms of the disciplines where statistics are an established methodological approach, and in the history and theory of science that governs our research. Many scientists are regrettably still [[Bias and Critical Thinking|positivists]], and often consider the knowledge they create to be objective. More often than not, this is not a conscious choice, but the combination of unreflected teachers in some education system in general.  Today, obvious moral dilemmas and ambiguities are generally part of complex ethical pre-checks in many study designs, for which medicine provides the gold standard. Here, preventing blunders was established early on, and is now part of the canon of many disciplines, with medicine leading the way. Such problems often deal with questions of sample size, randomisation and the question when a successful treatment should be given to all participants. These are indirect reflections on validity and plausibility within the study design, acknowledging that failures or flaws in these elements may lead to biased or even plain wrong results of the study. \n\nWhat is however widely lacking within the broader debates in science is how the utilisation of statistics can have wider normative consequences, both during the application of statistics, but also due to the consequences that arise out of results that were propelled by statistics. In her book \"Making comparisons count\", Ruth Chang explores one example of such relations, but the gap between ethics and statistics is so large, we might define it altogether as widely undiscovered country. More will need to be done to overcome the gap between ethics and statistics, and a connection is hampered by the fact that both fields are widely unclear in terms of the overall accepted norms. While many statistical textbooks exist, these are more often than not disciplinary, and consolidating the field of ethics this diversity is certainly no small endeavour. Creating connections between statistics and ethics is a challenge,  because there is a long history of flaws in this connection that triggered blurred, if not wrong decisions. We will therefore look at specific examples from both directions, starting with a view on ethics from the perspective of statistics.\n\n== Looking at ethics from statistics ==\n[[File:Food-supply-kcal.png|600px|right|thumb|[https:\/\/slides.ourworldindata.org\/hunger-and-food-provision\/#\/kcalcapitaday-by-world-regions-mg-png '''Our World in Data] provides plenty information''', for example on how the worldwide food provision changed over the last decades.]]","==The future of statistics within science==\n===Solve theory of science===\nUntangling the crisis of Western science for good is like the moonshot on this list. The perceived [[Bias in statistics|ghost of positivism]] that is still haunting us to this day, and the counter-revolutionary movements that emerged out of it triggered a division that left much of current research still being stuck in an illusion of objectivism, while some are lost in their maze of universal rejection or critical reflection. '''Critical realism with its subjective view of scientific knowledge, and its possibility for ontological truths still being out there, may have solved the current dilemmas of theory of science.''' Unfortunately, most researcher are not aware of this, or reject or ignore it. The errors of the past, and the [[Bias in statistics|biases]] these errors create inside of us as individuals as well as within the scientific community deserve a critical perspective on science. Equally, we need to create knowledge to continue the path of this human civilization, since we unleashed many wicked problems that need to be solved. Otherwise all may be in vain, and science needs to acknowledge that. Statistics is probably one of the branches of science that is furthest away from critical realism, yet if we change our education systems to enable a reflexive humanism as a baseline for our education, I cannot see why critical realism should not spread, and ultimately prevail. From a current viewpoint, it looks like our best ticket to the moon, and beyond.\n\n===Establish postdisciplinary freedom===\nScientific disciplines are a testimony of the oppressive evolving of science out of the [[History of Methods|Enlightenment]], leading to silos of knowledge that we call scientific discipline. '''While it is clear that this minimises the chances of a more holistic knowledge production, scientific disciplines are still necessary from a perspective of depth of knowledge.''' Medicine is a good example where most researchers are highly specialised, because there is hardly any other way to contribute to the continuous evolution of knowledge. We may thus conclude that focus in itself is necessary, and often helpful. There are however also other factors about the existence of scientific disciplines that are important to raise. First of all, scientific disciplines are in a fight about priorities of knowledge and interpretation. Many disciplines claim that their knowledge is indeed of a higher value than the knowledge of the other discipline. It is clear that this notion needs to be rejected once we take a step back and look at the whole picture, since such claims of superiority do not make any sense. Yet from a perspective of [[Bias and Critical Thinking|critical realism]], one could claim that ethics and maybe even philosophy are on a different level, because the can transcend epistemological perspective, and may even create ontological truths. While other disciplines thus vanish in the future, philosophy, and more importantly [[Ethics and Statistics|ethics]], are about our responsibility as researchers, and may thus play a pivotal role. I would propose that statistics could contribute to this end, because statistics is at its heart not disciplinary. Instead, statistics could provide a reflexive link between different domains of knowledge, despite it being almost in an opposite position today, since statistics is often the methodological dogma of many scientific disciplines.\n\n===Clarify the role of theory===\nStatistics today is stuck between a rock and a hard place. Statistics can help to test hypotheses, leading to a accepting or rejection of our questions that are rooted in our theories, making [[:Category:Deductive|deductive]] research often rigid and incremental. At the extreme opposite end, there is the [[:Category:Inductive|inductive]] line of thinking, which claims an open mind independent of theory, yet often still looks at the world through the lens of a theoretical foundation. '''Science builds on theory, yet the same theories can also lock us into a partial view of the world.''' This is not necessarily bad, yet the divide between inductive and deductive approaches has been haunting statistics just as many other branches of science. Some approaches in statistics almost entirely depend on deductive thinking, such as the [[ANOVA]]. Other approaches such as [[Clustering Methods|cluster analysis]] are widely inductive. However, all these different analyses can be used both in inductive and deductive fashion, and indeed they are. No wonder that statistics created great confusion. The ANOVA for example was a breakthrough in psychological research, yet the failure to reproduce many experiments highlights the limitations of the theories that are being pursued. Equal challenges can be raised for ecology, economy, and many other empirical branches of science. Only when we understand that our diverse theories offer mere partial explanations, shall these theories be settled in their proper places.\n\n===Reduce and reflect bias===\n'''[[Bias and Critical Thinking|Bias]] has been haunting research from the very beginning, because all humans are biased.''' Statistics has learned to quantify or even overcome some biases, for instance the one related to sampling or analysis are increasingly tamed. However, there are many more biases out there, and to this day most branches of science only had a rather singular focus on biases. In the future we may pool our knowledge and build on wider experience, and may learn to better reflect our biases, and through transparency and open communication, we may thus reduce them. It seems more than unclear how we will do this, but much is to be gained.\n\n===Allow for comparability===\n'''How can we compare different dimensions of knowledge?''' To give an example, how much worth in coin is courage? Or my future happiness? Can such things be compared, and evaluated? Derek Parfit wrote that we are irrational in the way how we value the distant future less as compared to the presence, even if we take the likelihood of this distant future becoming a reality into account. This phenomenon is called temporal discounting. Humans are strangely incapable of such comparisons, yet statistics have opened a door into a comparability that allows to unravel a new understanding of the comparisons in our head with other comparisons, or in other words, to contextualise our perspectives. Temporal discounting is already today playing less of a role because of teleconnections and global market chains. What would however be more important, is if people gained - through statistics - a deeper insight into their existence compared to everybody else. Such a radical contextualisation of ourselves would surely change our perspective on our role in the world."],"20":["Every system is \"(...) defined by its boundaries\" (Haraldsson 2004, p.13). The borders we draw for our system analysis influence which level of detail we apply to our view on the system, and which elements we investigate. System elements can be animate (animals, humans) or inanimate (rocks, rain), conceptual (motivation) or real (harvest), quantifiable (money) or rather qualitative (well-being) (2). For example, a system could be a tree, with the leaves, the stem and such elements interacting with each other, but also the forest in which our tree interacts with the soil, the weather, other plants, animals and inanimate objects. The system could also be the globe, where this forest interacts with other ecosystems, or the system in which Planet Earth interacts with the rest of the universe - our solar system. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n'''The system is at the basis of System Thinking.''' System Thinking is a form of scientific approach to organizing and understanding 'systems' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of 'holistic' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that \"(...) all system dynamics are in principle non-linear\" and that \"(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions\" (Haraldsson 2004, p.6).\n\n'''Peter Checkland introduced the notion that there are two main types of System Thinking:''' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a 'system' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a 'system' an \"(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world\" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a 'system'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|'''Hard System Thinking and Soft System Thinking''' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: ''System Analysis'' and ''System Dynamics'' (1).\n\n''System Analysis'' \"(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships.\" (Haraldsson 2004, p.5). ''System Analysis'', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].\n\n''System Dynamics'', then, focuses on the interaction part of the system. It \"(...) refers to the re-creation of the understanding of a system and its feedbacks. It aims at exploring dynamic responses to changes within or from outside the system. (...) System Dynamics deals with mathematical representation of our mental models and is a secondary step after we have developed our mental model.\" (Haraldsson 2004, p.5). System Dynamics, as the name suggests, enables the researcher to observe and measure the behavior of the system. The interactions between the individual elements are not just recreated, but the consequences of these interactions are quantified and assessed.\n\nSystem Thinking allows for a shift in the perception of [[Causality|causality]]. Instead of assuming linear causality (A causes B, B causes C) it allows for the integration of further influencing factors, as well as a more neutral depiction of the system at hand. C may now be seen as a property that [[Agency, Complexity and Emergence|emerges]] from the relation between A and B, instead of perceiving it as a direct consequence of B. Haraldsson (2004, p.21) provides an illustrative example here: \"We start by asking the initial question: \"I want to understand how water flows into the glass and what I do to fill it up.\" Instead of looking at the action from an individual point of view, where the \"I am\" is the active part and at the centre of focus, we shift our perception to the structure of the action. The \"I am\" simply becomes a part of the feedback process, not standing apart from it. Suddenly we have shifted out attention to the structure of the behavior and we can observe that the structure is causing the behavior. (...) We have now transformed the traditional linear thinking into a circular argument.\"","'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries. \n\n== The problem with System Boundaries ==\nWhile boundaries of some isolated systems as well as simple theoretical models can be rather clearly defined, much attention of the recent literature has actually focussed on the effects that systemic interactions create across distances, which have historically been considered separate (see for example [['https:\/\/www.ncdc.noaa.gov\/teleconnections\/'|'teleconnections']] in atmospheric research). It is now evident that inland watersheds are linked to oceangraphic processes through freshwater and nutrient inputs, although these have historically and disciplinarily been examined separately. Furthermore, the nestedness of systems is another challenge - the idea that smaller systems are contained within, and are the constituent components of, larger systems such as organs within the larger integrated functionality of the human body. Some organs can be removed or replaced, but only in relation to the overall functions they provide and how they fit or match the context of the overall body (i.e, blood type or immune response). Government jurisdictions give other clear examples, where countries may contain several provinces as part of a federal system, which in turn can contain smaller administrative units, creating a multi-level administrative whole. \n\nThe debate often resonates around a void between the global and the local levels of social and ecological [[Glossary|scales]] such as space, jurisdiction, administration and even time (e.g., slow to fast). While the global level of most scales is trivial to define, the local level of a scale is less clear. What becomes clear in the definition process between global and local, is the ambiguity and often arbitrary nature of defining what the reality of the system being examined is. Normative choices have to be made, especially for defining research questions and hypotheses, i.e., what is a part of the specific system, what is not considered to be a part of it, and what is an important element that you want to know about? This has implications for the relationships and outcomes you want to investigate. '''Standardization of system definitions, or system defining [[Glossary|processes]], would also be a useful consensus activity to increase comparability and exchange within system science.''' Entire fields of research have thus emerged around normative understandings of what the optimal system level unit to examine might be, such as rural sociology, landscape ecology, coastal governance or micro-economics. What is evident - and becomes unclear - is the spectrum of definitions and choices between the two ends of a scale, and the degree to which the categorical differences in system boundary definitions are meaningful for how we analyse and interpret their functionality.\n\n[[File:System Boundaries - Farm.png|600px|thumb|center|'''Defining the system boundaries influences the scope and content of analysis'''. Source: [https:\/\/www.researchgate.net\/publication\/323959122_D11_Report_on_resilience_framework_for_EU_agriculture Meuwissen et al. 2018.]]]\n\n== Defining System Boundaries ==\nOut of these challenges to define system boundaries we recognize a clear gap on how to approach a globally coherent definition, which can recognize the wide array of contextual differences in how systems are defined and measured. We need to consider that some systems are either divided from larger or parallel systems, or that smaller systems are nested into other systems. \n\n'''System boundaries can be defined based on a variety of parameters.''' For the simplicity of the argument, we focus on one parameter out of several, yet prefer not to discuss in depth the interaction of different parameters. Many people would define such interactions as [[Agency, Complexity and Emergence|complexity]] within systems thinking, but the examination of system complexity is premised on a coherent understanding of what is in and out of the system being examined. This is an inherent precondition for analysis, which is often discussed or taken as an assumption that is often not clearly defined. For example, spatially explicit parameters are an intuitive aspect of many defining processes, and therefore shape our perceptions of what an appropriate system boundary might be. To use a spatial example: '''a larger city can be divided into smaller neighbourhoods, while neighbourhoods can be defined based on different parameters, such as function, ethnic groups or cultural settings.''' While the definition of 'neighbourhoods' can be also informal, it is also well established within urban planning. There is an evident awareness that system boundaries exist, yet many would not be able to define why two systems differ. It is overall easier to define system boundaries based on spatial parameters, such as structural elements, ecosystems, or the design of a city, while informal parameters such as those based on normative dimensions are less clearly definable. Being initially clear within a research project about the boundaries of a system may help to clarify a ranking of what relevant parameters of the system are in terms of its boundaries, but also considering the variance of these parameters within a system. Basically, some parameters can be dramatically different within a system than outside, have no relevance within a system, or can have only relevance within the system. All [[Glossary|assumption]]s are plausible parameters to serve as system boundaries.\n\n'''Independent of the given parameter, we propose that both the state and the variance can be clear delimiters in defining boundaries between systems.''' When looking at a larger system, it can have an overall average value as well as an overall variance regarding a system parameter. If we would now divide the larger system into two smaller parts, it might be that the two systems have a different average value regarding this parameter. However, two smaller systems could also be different regarding their variance, where one smaller system has an overall large heterogeneity, and the other one a small heterogeneity.\n\nA good example of this could be two neighbourhoods within one city. One could be very heterogeneous in terms of green infrastructure, having many smaller parks, trees, and green rooftops. The other one could be highly homogenous, being widely dominated by houses without any green infrastructure. Clearly, both systems vary both in terms of the average value, as well as the variance. Thus, where boundaries are drawn to define those neighbourhoods in that city will dictate the analysis and conclusions about its variance and values.","As this study shows, systematically assessing causalities in a system and taking into account the interconnectedness between elements provides profound insights into system dynamics. Research can thus find further aspects to investigate, and policy and local actors may act according to these insights.\n\n== Key Publications ==\nForrester, J. W. 1961. ''Industrial dynamics.'' Pegasus Communications, Waltham, MA.\n* The publication that widely introduced system thinking to the world.\n\nHaraldsson, H.V. 2004. ''Introduction to System Thinking and Causal Loop Diagrams.'' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n* A brief explanation of System Thinking and a dailed description of how to develop a Causal Loop Diagram.\n\nMeadows, D. 2008. ''Thinking in Systems. A Primer.'' Chelsea Green Publishing, Vermont.\n* An good introduction to the topic in the field of sustainability science.\n\nCheckland, P. Systems Thinking. In: Curry, W.L. Galliers, B. 1999. ''Rethinking Managagement Information Systems.'' Oxford University Press. 45-56.\n* Representative for the various contributions Checkland made since the 1970s.\n\n\n== References ==\n\n(1) Haraldsson, H.V. 2004. ''Introduction to System Thinking and Causal Loop Diagrams.'' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n\n(2) Team TIP. 2011. ''Guidelines for drawing causal loop diagrams.'' Systems Thinker 22(1). 5-7.\n\n(3) Toole, M.T. 2005. ''A Project Management Causal Loop Diagram.'' ARCOM Conference, London, UK.\n\n(4) McGlashan et al. 2016. ''Quantifying a Systems Map: Network Analysis of a Childhood Obesity Causal Loop Diagram.'' PLoS ONE 11(10):\n\n(5) Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n(6) Arnold, R.D. Wade, J.P. 2015. ''A Definition of Systems Thinking: A Systems Approach.'' 2015 Conference on Systems Engineering Research.\n\n(7) Chu, D. 2011. ''Complexity: Against Systems.'' Theory in Biosciences. 182-196.\n\n(8) Mitchell, S. 2004. ''Why integrative pluralism?'' Emergence: Complexity and Organization 1. 1-14.\n\n(9) Zexian, Y. Xuhui, Y. 2010. ''A Revolution in the Field of Systems Thinking - A Review of Checkland's System Thinking.'' Systems Research and Behavioral Science 27. 140-155.\n\n(10) Checkland, P. 2000. ''Soft Systems Methodology: A Thirty Year Retrospective''. Systems Research and Behavioral Science 17.11\u201358.\n\n\n== Further Information ==\n* [https:\/\/ncase.me\/loopy\/ LOOPY] is a simple web-based tool for creating your own feedback loops. It's easy to apply and a good start to get to know the process.\n* [https:\/\/www.youtube.com\/watch?v=UgZTXf5PDis This video by Climate Interactive] on YouTube showcases how Causal Loop Diagrams can be used to model actions against Climate Change.\n* [https:\/\/ocw.mit.edu\/courses\/sloan-school-of-management\/15-871-introduction-to-system-dynamics-fall-2013\/index.htm An open MIT class] for those who want to dive deeper into System Dynamics. There is a short introduction lecture by MIT professor John Sterman available [https:\/\/www.youtube.com\/watch?t=163&v=AnTwZVviXyY&feature=youtu.be here.]\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."],"21":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# '''Select the topic you want to learn more about.''' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don't go to broad - focus on a specific topic. You will not be able to explain \"Economics\" or \"Physics\" in one go.\n# '''Find someone to talk to'''. Ideally, this person does not know anything about this topic. If you don't have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# '''Make notes.''' You will quickly realize yourself which parts of the topic you are not able to explain, and\/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# '''Have a look at your notes and try to find more information.''' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https:\/\/karrierebibel.de\/feynman-methode\/ Karrierebibel]\n* [https:\/\/blog.doist.com\/feynman-technique\/ ToDo-ist]\n* [https:\/\/www.goodwall.io\/blog\/feynman-technique\/ Goodwall]\n* [https:\/\/www.youtube.com\/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.","[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|'''SCOPUS hits per year for Bayesian Inference until 2020.''' Search terms: 'Bayesian' in Title, Abstract, Keywords. Source: own.]]\n'''The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.''' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes' name only became associated with the method in the 1900s (3).\n\n'''The family of methods based on the concept of Bayesian analysis has risen the last 50 years''' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n'''Bayesian analysis relies on using probability figures as an expression of our beliefs about events.''' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is \"prior\", and the probability figure assigned to the prior is called \"prior probability\". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called \"posterior probability\", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|'''The probability distribution for prior, evidence, and posterior.''']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes' Theorem.\n\n==== Bayes' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes' theorem.\n\n'''Conditional Probability''' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n'''Joint Probability''', also called \"conjoint probability\", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n'''Marginal Probability''' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for \"Bayes theorem\" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n''Caution:'' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n''Note:'' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let's see how this formulation of Bayes' Theorem can be applied in a simple coin toss example:\n\n\n=== '''Example I: Classic coin toss''' ===\n'''Imagine, you are flipping 2 fair coins.''' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)","The second grave challenge of statistics is numbers per se. Most people are not fond of numbers. Some are even afraid. Fear is the worst advisor. Numbers are nothing to be afraid of, and likewise, statistics are nothing to be afraid of either. Yet learning statistics is no short task, and this is the third obstacle I see. \n\nLearning a scientific method, any scientific method, takes time. It is like learning an instrument or learning martial arts. Learning a method is not just about a mere internalisation of knowledge, but also about gaining experience. Be patient. Take the time to build this experience. Within this Wiki, I give you a basic introduction of how statistics can help you to create knowledge, and how you can build this experience best.\n\n====Occam's razor====\n[[File:William of Ockham.jpg|thumb|left|William of Occam]]\n\"Entities should not be multiplied without necessity.\"\n\nThe Franciscan friar [https:\/\/en.wikipedia.org\/wiki\/William_of_Ockham William of Occam] almost single-handedly came up with one of the most fundamental principles to date in science. He basically concluded, that \"everything should be as simple as possible, but as complex as necessary.\" Being a principle, it is suggested that this thought extends to all. While in his time it was rooted in philosophy or more specifically in logic,  [https:\/\/science.howstuffworks.com\/innovation\/scientific-experiments\/occams-razor.htm Occam's razor] turned out to propel many scientific fields later on, such as physics, biology, theology, mathematics and many more. It is remarkable how this principle purely rooted in theoretical consideration generated the foundation for the scientific method, which would surface centuries later out of it. It also poses as one of the main building blocks of modern statistics as William of Occam came up with the principle of parsimony. While this is well known in science today, we are up until today busy discussing whether things are simple or complex. Much of the scientific debate up until today is basically a pendulum swing between these two extremes, with some people oversimplifying things, while others basically saying that everything is so complex we may never understand it. Occam's razor concludes that the truth is in between.\n\n====The scientific method====\n[[File:Francis Bacon.jpg|thumb|left|Francis Bacon]]\nThe [https:\/\/www.khanacademy.org\/science\/high-school-biology\/hs-biology-foundations\/hs-biology-and-the-scientific-method\/a\/the-science-of-biology scientific method] was a true revolution since it enabled science to inductive observations and thus indirectly paved the road towards the testing of hypotheses through observation. Before, science was vastly dominated by theorizing -that is developing theories- yet testing theories proved to be more difficult. While people tended to observe since the dawn of humans, making such observations in a systematic way opened a new world in science. Especially [https:\/\/www.britannica.com\/science\/Baconian-method Francis Bacon] influenced this [https:\/\/www.khanacademy.org\/humanities\/monarchy-enlightenment\/baroque-art1\/beginners-guide-baroque1\/a\/francis-bacon-and-the-scientific-revolution major shift], for which he laid the philosophical foundation in his \"Novum Organon\". \n\n'''All observations are normative, as they are made by people.''' This means that observations are constructs, where people tend to see things through a specific \"lens\". A good example of such a specific normative perspective is the number zero, which was kind of around for a long time, but only recognised as such in India and Arabia in the 8th-9th century ([https:\/\/en.wikipedia.org\/wiki\/History_of_the_Hindu\u2013Arabic_numeral_system 0]). Today, the 0 seems almost as if it was always there, but in the antique world, there was no certainty whether the 0 is an actual number or not. This illustrates how normative perspectives change and evolve, although not everybody may be aware of such radical developments as [https:\/\/www.sciencealert.com\/more-than-half-of-americans-could-be-confused-about-arabic-numbers Arabic numbers].\n\n==A very short history of statistics==\nBuilding on Occam's razor and the scientific method, a new mode of science emerged. Rigorous observation and the testing of hypotheses became one important building block of our civilisation. One important foundation of statistics was [https:\/\/www.youtube.com\/watch?v=XQoLVl31ZfQ probability theory], which kind of hit it off during the period known as ''[https:\/\/en.wikipedia.org\/wiki\/Age_of_Enlightenment Enlightenment]''. \n\nProbability was important as it enabled differentiation between things happening by chance, or following underlying principles that can be calculated by probability. Of course, probability does not imply that it can be understood why something is not happening by chance, but it is a starting point to get out of a world that is not well understood. Statistics, or more importantly probability, was however not only an important scholar development during the enlightenment, but they also became a necessity.\nOne of the first users of probability was [https:\/\/www.britannica.com\/science\/probability\/Risks-expectations-and-fair-contracts Jan de Wit], leader in the Netherlands from 1652 to 1672. He applied the probabilistic theory to determine proper rates of selling annuities. Annuities are payments which are made yearly but back in the days states often collected them during times of war. He stated that annuities and also life insurances should be connected to probability calculations and mortality records in order to determine the perfect charge of payment."],"22":["[https:\/\/www.sare.org\/Learning-Center\/Bulletins\/How-to-Conduct-Research-on-Your-Farm-or-Ranch\/Text-Version\/Basics-of-Experimental-Design\/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https:\/\/www.theanalysisfactor.com\/the-difference-between-crossed-and-nested-factors\/ Difference between crossed & nested factors]: A short article\n\n[https:\/\/www.ohio.edu\/plantbio\/staff\/mccarthy\/quantmet\/lectures\/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https:\/\/support.minitab.com\/en-us\/minitab\/18\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/regression-models\/model-reduction\/ Model reduction]: A helpful article\n\n[https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https:\/\/en.wikipedia.org\/wiki\/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https:\/\/www.simplypsychology.org\/milgram.html Field Experiments in Psychology]: A famous example\n\n[https:\/\/www.nature.com\/articles\/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https:\/\/revisesociology.com\/2016\/01\/17\/field-experiments-sociology\/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https:\/\/www.youtube.com\/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction\n\n[https:\/\/www.youtube.com\/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","====Blocking====\n[[File:Block Experiments.jpg|thumb|Designing an experiment using block effects.]]\nIn order to tame the variance of the real world, [https:\/\/www.youtube.com\/watch?v=10ikXret7Lk blocking] was a plausible approach. By basically introducing agricultural fields as [https:\/\/www.ndsu.edu\/faculty\/horsley\/RCBD.pdf blocks], the variance from individual blocks can be tamed. This was one of the breakthroughs, as the question of what we want to know i.e. hypothesis testing, was statistically uncoupled from the question what we do not want to know, i.e. the variance inferred from individual blocks. Consequently, the samples and treatment combinations need to be randomised within the different blocks, or can be alternatively replicated within these blocks. This has become established as a standard approach in the designing of experiments, often for rather pragmatic reasons. For instance are [https:\/\/www.sare.org\/Learning-Center\/Bulletins\/How-to-Conduct-Research-on-Your-Farm-or-Ranch\/Text-Version\/Basics-of-Experimental-Design\/Common-Research-Designs-for-Farmers agricultural fields] often showing local characteristics in terms of soil and microclimate, and these should be tamed by the clear designation of blocks and enough blocks in total within the experiment. The last point is central when thinking in terms of variance, since it would naturally be very hard to think in terms of variance regarding e.g. only two blocks. A higher number of blocks allow to better tame the block effect. This underlines the effort that often needs to go into designing experiments, since a sufficient number of blocks would basically mean that the effort can be multiplied by the number of blocks that are part of the design. Ten blocks means ten times as much work, and maybe with the result that there is no variance among the blocks overall.\n\n====Nested designs====\n[[File:Bildschirmfoto 2020-05-21 um 17.06.05.png|thumb|A nested design is used for experiments in which there is an interest in a set of treatments and the experimental units are sub-sampled.]]\nWithin field experiments, one factor is often nested within another factor. The [https:\/\/www.theanalysisfactor.com\/the-difference-between-crossed-and-nested-factors\/ principle of nestedness] works generally like the principle of Russian dolls: Smaller ones are encapsulated within larger ones. For instance can a block be seen as the largest Russian doll, and the treatments are then nested in the block, meaning each treatment is encapsulated within each block. This allows for a testing where the variance of the block effect can be minimised, and the variance of the treatment levels can be statistically compared. Quite often the variance across different levels of nestedness is a relevant information in itself, meaning for instance how much variance is explained by a different factor. Especially spatially [https:\/\/www.ohio.edu\/plantbio\/staff\/mccarthy\/quantmet\/lectures\/ANOVA-III.pdf nested designs] can have such a hierarchical structure, such as neighbourhoods within cities, streets within neighbourhoods and houses in streets. The nested structure would in this case be Cities\/neighbourhoods\/streets\/houses. Just as with blocks, a nested structure demands a clear designing of an experiment, and greatly increase the sample size. Hence such a design should be implemented after much reflection, based on experience, and ideally by [[Glossary|consultation]] with experts both in statistics as well as the given system.\n\n==Analysis==\n[[File:Farm-fields-crops-green.jpg|thumb|left|How to grow our plants best? If we simplify our model and eliminate nonsignificant treatments, we may find out.]]\nThe analysis of field experiments demands great care, since this is mostly a deductive approach where equal emphasis is put on what we understand, and what we do not understand. Alternatively, we could highlight that rejection of the hypothesis is the most vital step of any experiment. In statistical terms the question of explained vs. unexplained variance is essential. \nThe first step is however checking the p-value. Which treatments are significant, and which ones are not? When it comes to two-way [[ANOVA]]s, we may need to reduce the model to obtain a minimum adequate model. This basically equals a reduction of the full model into the most parsimonious version, following Occam's razor. While some researchers tend to report the full model, with all non-significant treatments and treatment combinations, I think this is wrong. If we reduce the model, the p-values change. This can make the difference between a treatment that is significant, and a non-significant model. Therefore, [https:\/\/support.minitab.com\/en-us\/minitab\/18\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/regression-models\/model-reduction\/ model reduction] is being advised for. For the sake of simplicity, this can be done by first reducing the highest level interactions that are not significant. However, the single effects always need to be included, as the single effects are demanded if interactions of a treatment are part of the model, even if these single effects are not significant. An example of such a procedure would be the NPK dataset in R - it contains information about the effect of nitrogen, phosphate and potassium on the growth of peas. There, a full model can be constructed, and then the non-significant treatments and treatment interactions are subsequently removed to arrive at the minimum adequate model, which is the most parsimonious model. This illustrates that Occam's razor is not only a theoretical principle, but has direct application in statistics. \n\n<syntaxhighlight lang=\"R\" line>\n\n#the dataset npk contains information about the effect of nitrogen, phosphate and potassium on the growth of peas\n#let us do a model with it\ndata(npk)\nstr(npk)\nsummary(npk)\nhist(npk$yield)\n\npar(mfrow=c(2,2))\npar(mar=c(2,2,1,1))\nboxplot(npk$yield~npk$N)\nboxplot(npk$yield~npk$P)\nboxplot(npk$yield~npk$K)\nboxplot(npk$yield~npk$block)\ngraphics.off()\n\nmp<-boxplot(npk$yield~npk$N*npk$P*npk$K)#tricky labels\n#we can construct a full model\nmodel<-aov(yield~N*P*K,data=npk)\nsummary(model)","Single factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https:\/\/www.thoughtco.com\/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. '''If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.''' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIn addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs. \n\n\n== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis.\n\n\n== Normativity ==\nDesigning an ANOVA-based design demands experience, and knowledge of the previous literature. The deductive approach of an ANOVA is thus typically embedded into an incremental development in the literature. ANOVA-based designs are therefore more often than not part of the continuous development in normal science. However, especially since the millennium, other more advanced approaches gained momentum, such as mixed effect models, information theoretical approaches, and structural equation models. The rigid root of the normal distribution and the basis of p-values is increasingly recognised as rigid if not outright flawed, and model reduction in more complex ANOVA designs is far from coherent between different branches of sciences. Some areas of science reject p-driven statistics altogether, while other branches of science are still publishing full models without any model reduction whatsoever. In addition, the ANOVA is today also often used to analyse inductive datasets, which is technically ok, but can infer several problems from a statistical standpoint, as well as based on a critical perspective rooted in a coherent theory of science."],"23":["This radical development coincides with yet another revolution that shattered science, namely the digital age. '''Computers allowed for faster calculation and novel methodological approaches.''' The internet fueled new sources and forms of knowledge, and the associated new forms of communication triggered an exchange between researchers at an unprecedented pace. All means of electronic communication, online [[Glossary|journals]] and the fact that many researchers today have their own computer led to an exponential increase in scientific collaboration. While this sometimes also breads opportunism and a shift to quantity instead of quality in research, it is undeniable that today much of scientific information is not further away from us than the click of a mouse. Technology cannot be an end in itself, but as a means to an end it enables today an exponential pace of research, which manifested itself most illustratively in the Corona crisis. The global community of researchers united in their utmost strength, and the speed and diversity of knowledge creation is unprecentented in the history of our civilisation. Never before was more interaction between the latest scientific inquiry or results and the society.\n\n== Additional Information ==\n* [https:\/\/www.simplypsychology.org\/Kuhn-Paradigm.html More information] on Kuhn's theory of scientific paradigm shifts.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.","Another development that emerged during the last decades is the conducting of so called real-world experiments, which are often singular case studies with interventions, yet typically less or no control of variables. These approaches are slowly being developed in diverse branches of research, and allow to open a [[Meta-Analysis|meta-analytical]] dimension, where a high number of case studies is averaged in terms of the research results. The combination of different studies enables a different perspective, yet currently such approaches are either restricted to rigid clinical trials or to meta-analyses with more variables than cases. \n\nReal-world experiments are thus slowly emerging to bridge experimental rigour with the often perceived messiness of the problems we face and how we engage with them as researchers, knowing that one key answer involving these is the joint learning together with stakeholders. This development may allow us to move one step further in current [[System Thinking & Causal Loop Diagrams|systems thinking]], where still many phenomena we cannot explained are simply labeled as complex. We will have to acknowledge in the future which phenomena we may begin to understand in the future, and which phenomena we may never be able to fully understand. [[Non-equilibrium dynamics|Non-equilibrium theory]] is an example where unpredictable dynamics can still be approaches by a scientific theory. Chaos theory is another example, where it is clear that we may not be able to grasp the dynamics we investigate in a statistical sense, yet we may be able to label dynamics as chaotic and allow a better understanding of our own limitations. Complexity is somewhat inbetween, leaning partly towards the explainable, yet also having stakes in the unexplainable dynamics we face. '''Statistics is thus at a crossroad, since we face the limitations of our approaches, and have to become better in taking these into account.''' \n\nWithin statistics, new approches are rapidly emerging, yet to date the dominion of scientific disciplines still haunts our ability to apply the most parsimonious model. Instead, the norms of our respective discipline still override our ability to acknowledge not only our limitations, but also the diverse biases we face as statisticians, scientists and as a people. Civil society is often still puzzled how to make sense of our contributions that originate in statistics, and we have to become better in contextualising statistical results, and translate the consequences of these to other people. '''To date, there is a huge gap between [[Ethics and Statistics|statistics and ethics,]] and the 20th century has proven that a perspective restricted to numbers will not suffice, but instead may contribute to our demise.''' We need to find ways to not only create statistical results, but also face the responsibility of the consequences of such analyses and interpretations. In the future, more people may be able to approximate knowlegde though statistics, and to be equally able to act based on this knowledge in a reasonable sense, bridging societal demands with our capacity for change. \n\n\n==What was missing==\nEverybody who actively participated in this module now has a glimpse of what statistics is all about. I like to joke that if statistics is like the iceberg that sank the Titanic, then you now have enough ice for a Gin-Tonic, and you should enjoy that. The colleagues I admire for their skills in terms of statistics spent several thousand hours of their life on statistics, some even tens of thousands of hours. By comparison, this module encapsulates about 150 hours, at least according to the overall plan. Therefore, this module focuses on knowledge. It does not include the advanced statistics that demand experience. Questions of models reductions, [[Mixed Effect Models|mixed effect models]], [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|multivariate statistics]] and many other approaches were never touched upon, because this would have been simply too much. \n\n'''In itself, this whole module is already daring endeavour, and you are very brave that you made it through.''' We never had such a course when we were students. We learned how to calculate a mean value, or how to make a t-test. That was basically it. Hence this course is designed to be a challenge, but it is also supposed to give you enough of an overview to go on. Deep insights and realisation happen in your head. We gave you a head start, and gave you the tools to go on. '''Now it is up to you to apply the knowledge you have, to deepen it, transfer it into other contexts and applications, and thus move from knowledge to experience.''' Repetition and reflection forge true masters. Today, there are still too few people willing to spend enough time on statistics to become truly versatile in this arena of science. If you want to go elsewhere now, fine. You now learned enough to talk to experts in statistics, given that they are willing to talk to you. You gained data literacy. You can build bridges, the problems we face demand that we work in teams, and who knows what the future has in stock for you. \n\nNevertheless, maybe some of you want to go on, moving from apprenticeship to master level. Statistics is still an exciting, emerging arena, and there is much to be learned. One colleague of mine once said about me that I could basically \"smell what a dataset is all about\". I dare you to do better. I am sure that the level of expertise, skill and experience I gained is nothing but a stepping stone to deeper knowledge and more understanding, especially between all of us, regarding the interconnectedness of us all. '''I hope that all of you find a way how you can contribute best, and maybe some of you want to give statistics a try.''' If so, then the next section is for you.","'''Note:''' This entry is a brief introduction to natural experiments. For more details on other types of experiments, please refer to the entries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n__TOC__\n==Natural experiments==\n[[File:Easter Island.jpg|thumb|right|Although this seems to be a little contradictory here, the impossibility of replication is a problem in the case of the Easter Island.]]\nOut of a diverse rooting in discussions about [[Glossary|complexity]], [https:\/\/learningforsustainability.net\/systems-thinking\/ system thinking] and the need to understand specific contexts more deeply, the classic experimental setting did at some point become more and more challenged. What emerged out of the development of [https:\/\/sustainabilitymethods.org\/index.php\/Interactions#The_field_experiment field experiments] was an almost exact opposite trend considering the reduction of complexity. What do we learn from singular cases? How do we deal with cases that are of pronounced importance, yet cannot be replicated? And what can be inferred from the design of such case studies? A famous example from ethnographic studies is the [http:\/\/www.eisp.org\/818\/ Easter Island]. Why did the people there channel much of their resources into building gigantic statues, thereby bringing their society to the brink of collapse? While this is a surely intriguing question, there are no replicates of the Easter Islands. This is at a first glance a very specific and singular problem, yet it is often considered to be an important example on how unsustainable behaviour led to a collapse of a while civilisation. Such settings are referred to as [https:\/\/www.britannica.com\/science\/natural-experiment Natural Experiments]. From a certain perspective, our whole planet is a Natural Experiment, and it is also from a statistical perspective a problem that we do not have any replicates, besides other ramifications and unclarity that derives such single case studies, which are however often increasingly relevant on a smaller scale as well. With a rise in qualitative methods both in diversity and abundance, and an urge for understanding even [[Glossary|complex systems]] and cases, there is clearly a demand for the integration of knowledge from Natural Experiments. '''From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant, plausible and valid.''' To this end, I proclaim the concept of the niche in order to illustrate and conceptualise how single cases can still contribute to the production and canon of knowledge.\n\nFor example the [https:\/\/academic.oup.com\/rcfs\/article\/4\/2\/155\/1555737#113865691 financial crisis from 2007], where many [[Glossary|patterns]] where comparable to previous crisis, but other factors were different. Hence this crisis is comparable to many previous factors and patterns regarding some layers of information, but also novel and not transferable regarding other dynamics. We did however understand based on the single case of this financial crisis that certain constructs in our financial systems are corrupt if not broken. The contribution to develop the financial world further is hence undeniable, even so far that many people agree that the changes that were being made are certainly not enough. \n\n'''Another prominent example of a single case or phenomena is the Covid pandemic that emerges further while I am writing these lines.''' While much was learned from previous pandemics, this pandemic is different, evolves different, and creates different ramifications. The impact of our societies and the opportunity to learn from this pandemic is however undeniable. While classical experiments evolve knowledge like pawns in a chess game, moving forward step by step, a crisis such as the Covid pandemic is more like the horse in a chess game, jumping over larger gaps, being less predictable, and certainly harder to master. The evolution of knowledge in an interconnected world often demands a rather singular approach as a starting point. This is especially important in normative sciences, where for instance in conservation biology many researchers approach solutions through singular case studies. Hence the solution orientated agenda of sustainability science emerged to take this into account, and further.\n\nTo this end, [https:\/\/journals.sagepub.com\/doi\/pdf\/10.1177\/0963662505050791 real world experiments] are the latest development in the diversification of the arena of experiments. These types of experiments are currently widely explored in the literature, yet I do not recognise a coherent understanding of what real-world experiments are to date in the available literature. These experiments can however be seen as a continuation of the trend of natural experiments, where a solution orientated agenda tries to generate one or several interventions, the effects of which are tested often within singular cases, but the evaluation criteria are clear before the study was conducted. Most studies to date have defined this with vigour; nevertheless, the development of real-world experiments is only starting to emerge. For more info, please refer to the entry on [[Living Labs & Real World Laboratories]].\n\n[[File:Lueneburg 2030.jpg|800px|thumb|center|'''A wonderful example for a bunch of real world experiments is the project L\u00fcneburg 2030+.''' This map provides an overview of the different experiments.]]"],"24":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> ''Is everyone on the same page?''\n<br> ''Are there important issues that have been neglected so far?''\n<br> ''Is there unspoken dissonance?''\n<br> ''Is there an elephant in the room?''\n<br> ''What are we actually talking about?''\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> ''What song would you choose to characterize the current state of discussion, and why?'' <br> ...\n\n== Links ==\nhttps:\/\/www.methodenkartei.uni-oldenburg.de\/uni_methode\/blitzlicht\/\n<br> https:\/\/www.bpb.de\/lernen\/formate\/methoden\/62269\/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar M\u00f6lleken.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique''' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe '''Dreamers'''...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: ''Which ideas come to mind? What would be an ideal solution to the problem?''\n\nThe '''Realists'''...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: ''How does the idea feel? How could it be implemented? Who should do it and at what cost?''\n\nThe '''Critics'''...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: ''What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?''\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. '''The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.''' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to 'neutralize' their minds) in order to understand the other roles' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He\/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n''Sources:''\n* Tools Hero - [https:\/\/www.toolshero.com\/creativity\/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https:\/\/arbeitdigital.de\/wirtschaftslexikon\/kreativitaetstechniken\/walt-disney-methode\/ Walt-Disney-Methode]\n* Karrierebibel - [https:\/\/karrierebibel.de\/disney-methode\/ Disney Methode: Denkblockaden \u00fcberwinden]\n* Impulse - [https:\/\/www.impulse.de\/management\/selbstmanagement-erfolg\/walt-disney-methode\/3831387.html Walt Disney Methode]\n\n[https:\/\/www.youtube.com\/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br\/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe World Caf\u00e9 is a method for facilitating discussions in big groups. With many participants, discussion rounds tend to be sprawling, slow and dominated by strong speakers. If you want to facilitate a discussion that is more effective, energetic, and inclusive, the World Caf\u00e9 is a helpful technique. It divides participants into moderated subgroups, who then wander together through a parcours of stations with different questions, all the while the atmosphere is relaxed and casual like in a caf\u00e9.\n\n== Goals ==\nSplitting big groups into subgroups fosters inclusive, energetic, effective and in-depth discussions: \n* reserved speakers can feel more comfortable speaking in a smaller group\n* the parcours format allows people to physically move through the room in between discussions\n* the moderator can steer the discussion towards unexplored issues with every new subgroup\n* every participant contributed to the collective results in the end\n\n== Getting started ==\nDepending on group size, room capacities and questions you want to discuss, different stations are set up (can be tables, boards, flipcharts etc.) with a moderator who will introduce the question and lead the discussion. The participants will be divided into as many subgroups as there are stations. Each subgroup will visit every station. The moderator welcomes the subgroup participants and introduces the question. Within a given time slot, the subgroups will discuss the question and write down their ideas and insights, before they then wander to the next station. The moderators remain with their station and welcome the next group. They present the question plus a broad overview of the insights of the former group and deepen the discussion with the new group. After the parcours has been completed by all subgroups, the moderators present the collective discussion results of each question to the full group. \n\nIt is helpful to have one moderator who is in charge of the clock and who manages the parcours direction.\n\n== Links & Further reading ==\nhttp:\/\/www.theworldcafe.com\/\n\nhttps:\/\/www.methodenkartei.uni-oldenburg.de\/uni_methode\/world-cafe\/\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Dagmar M\u00f6lleken."],"25":["[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution.","'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes\/no (1\/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n<\/imagemap>\n\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to \"rule out\", the answer to this question is 'yes'.\n\n\n= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n<\/imagemap>\n\n'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n<\/imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as 'discrete' data.\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. Abundance data is referred to as 'integer' in R, i.e. it exists in full numbers, and continuous data is 'numeric' - it has a comma.\n\n\n== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n<\/imagemap>\n\n'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?","Regarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."],"26":["'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and\/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row\/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=\"R\" inline>heatmap()<\/syntaxhighlight> function and '''mtcars''' dataset.\nIt is important to note that the <syntaxhighlight lang=\"R\" inline>heatmap()<\/syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=\"R\" inline>as.matrix()<\/syntaxhighlight> function.\n<syntaxhighlight lang=\"R\" line>\ndata(\"mtcars\")\nmatcars <- as.matrix(mtcars)\n<\/syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=\"R\" line>\nfullcolnames <- c(\"Miles per Gallon\", \"Number of Cylinders\",\n                  \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                  \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                  \"Number of Gears\", \"Number of Carburetors\")\n<\/syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = \"column\", labCol = fullcolnames, \n        margins = c(11,5))\n<\/syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let\u2019s look at the feature <syntaxhighlight lang=\"R\" inline>\u201cNumber of Carburetors\u201d<\/syntaxhighlight>. We can see that '''Maserati Bora''' has the darkest color, hence it has the largest number of carburetors, followed by '''Ferrari Dino''', which has the second-largest number of carburetors. While other models such as '''Fiat X1-9''' or '''Toyota''' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.\n\n==Explanation of used arguments==\n* <syntaxhighlight lang=\"R\" inline>Colv = NA<\/syntaxhighlight> and <syntaxhighlight lang=\"R\" inline>Rowv = NA<\/syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.\n* <syntaxhighlight lang=\"R\" inline>scale = \u201ccolumn\u201d<\/syntaxhighlight> is used to normalize the columns of the matrix (to absorb the variation between columns). As it was stated previously, normalization is needed due to the algorithm by which the colors are set. Here in our dataset, the values of features \u201cGross horsepower\u201d and \u201cDisplacement\u201d are much larger than the rest. Therefore, without normalization, these two columns will be all marked approximately equally high and all the other columns equally low. Normalizing means that we keep the relative values in each column but not the real numbers. In the interpretation sense it means that, for example, the same color of features \u201cMiles per Gallon\u201d and \u201cNumber of Cylinders\u201d of Mazda RX4 does not mean that the actual values are the same or approximately the same (placed in the same bin). It only means that the relative values of each of these cells in corresponding columns are the same or are in the same bin.\n* <syntaxhighlight lang=\"R\" inline>margins<\/syntaxhighlight> is used to fit the columns and rows names into the graph. The reason we used it here is because of the renaming of the columns, which is resulted in longer names that did not fit well by themselves.","'''Note:''' This entry revolves specifically around Treemaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br>\n<br>\n'''In short:''' A treemap is a rectangle-based visualization method for large, hierarchical data sets. Originally designed to visualize files on a hard drive and developed by Shneiderman and Johnson. They capture two types of information in the data: (1) the value of individual data points; (2) the structure of the hierarchy.\n__TOC__\n<br>\n\n== Definition ==\nTreemaps display hierarchical (tree-structured) [[Glossary|data]]. They are composed of a series of nested rectangles (tiles) whose areas are proportional to the data they represent. Each branch of the tree is given a rectangle, which is then subdivided into smaller rectangles representing sub-branches. The conceptual idea is to break down the data into its constituent parts and quickly identify its large and small components.\n<br>\n<br\/>\n[[File:Switzerlandtreemap.png|400px|thumb|right|Fig.1: Switzerland imports in 2017. Source: commons.wikimedia.org]]\n'''Treemaps are used:''' <br>\n1. To study data with respect to two quantitative values: <br>\n\u2013 positive quantitative value standing for the size of the rectangle (area cannot be negative) and<br>\n\u2013 second or categorical quantitative value standing for the color of the individual rectangles.<br>\n2. To display very large amount of hierarchial data in a limited space.<br> \n3. To make a quick, high-level summary of the similarities and differences within one category as well as between multiple categories (not precise comparisons).\n<br>\n<br>\nThe efficient use of physical space and the intelligent color management make treemaps powerful visualization technique applied to a wide variety of domains. They are used to display significant amounts of information in financial, commercial, governmental and similar fields. The treemap on Fig.1 shows Switzerland imports in 2017.\n[[File:Motorbikestreemap.png|300px|thumb|right|Fig.2: Category-wise sales figure for motorbikes. Source: www.fusioncharts.com]]\n'''Adding new Dimensions.''' With the intelligent use of colors, new dimensions can be added to the diagram. The usual practice is to use color in different rectangles to indicate a second categorical or quantitative value. If color is used to express a quantitative value, it\u2019s strongly encouraged to use only one color (if all the numbers are positive) or two colors (one for negative and one for positive), and vary the intensity of the color to express precise value.\n<br>\n<br>\nThe following treemap (Fig.2) illustrates the category-wise (Street, Cruiser and etc.) sales figure for motorbikes. The size of the rectangles within each category indicates the relative number of sales. Different colors and color intensities show growth and declines of the motorbike sales. \u201cStatic\u201d shows that sales neither grew nor declined. Very intense orange indicates a big shift downward, and very intense green indicates a big shift upwards.\n\nFrom Fig.2 it can be concluded that appropriate use of color enables us to use tree maps to represent losses, declines in sales or other non-positive values. The second quantitative value is not represented by the area of the rectangle.\n<br>\n<br>\nThe way the rectangle is divided and arranged into sub-rectangles depends on '''the tiling algorithm''' used.\n<br>\n<br>\nMany tiling algorithms have been developed and here are some of them:\n<br>\n<br>\n'''Squarified''' - keeps each rectangle as square as possible. It also tries to order the consecutive elements of the dataset (blocks, tiles) in descending order from the upper left corner to the lower right corner of the graph.\n<br>\n<br>\n'''Slice and Dice''' uses parallel lines to divide a root into branches (large rectangles). Then they are subdivided into smaller rectangles representing sub-branches again by using parallel lines. At each level of the hierarchy the orientation of the lines is switched (vertical vs. horizontal).\n\n== R Code ==\nImagine you have book A, consisting of 200 pages, which you use in your statistics course. This book contains of 2 main sections: B (80pages) and C (120pages). B section covers topics of Descriptive Statistics and C section covers topics of Inferential Statistics.\n<br>\n<br>\nTopics of B section are: D(30pages) and E(50pages). D is about sample mean and sample standard deviation while E is about Skewness and Kurtosis.\n<br>\n<br>\nTopics of C section are: F(20pages), G(40pages) and H(60pages). F is about Hypothesis Testing, G covers Confidence Intervals and H focuses on Regression Analysis.\n<br>\n<br>\nYou have tree-structured data and want to make a treemap for displaying the constituent sections of book and make comparisons of its\nsmall and large components.\n[[File:Customtreemap.png|300px|thumb|right|Fig.3]]\n<syntaxhighlight lang=\"R\">\n#Fig.3\nlibrary(treemap) \ngroup = c(rep(\"B\",2), rep(\"C\",3)) \nsubgroup = c(\"D\",\"E\",\"F\",\"G\",\"H\") \nvalue = c(30,50,20,40,60) \ndata= data.frame(group,subgroup,value) \ntreemap(data,index=c(\"group\",\"subgroup\"),\n        vSize = \"value\",\n        palette = \"Set2\",\n        title=\"A\",\n        type=\"index\",\n        bg.labels=c(\"white\"),\n        align.labels=list(c(\"center\", \"center\"), \n                          c(\"right\", \"bottom\")))\n<\/syntaxhighlight>\n\n==References and further reading material==\n# Ben Shneiderman (1992). \u201cTree visualization with tree-maps: 2-d space-filling approach\u201d. ACM Transactions on Graphics. 11: 92\u201399.\n# Ben Shneiderman, April 11, 2006, Discovering Business Intelligence Using Treemap Visualizations, http:\/\/www.perceptualedge.com\/articles\/b-eye\/treemaps.pdf\n# https:\/\/towardsdatascience.com\/treemaps-why-and-how-cfb1e1c863e8\n# https:\/\/www.nngroup.com\/articles\/treemaps\/\n# https:\/\/www.fusioncharts.com\/resources\/chart-primers\/treemap-chart\/\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Shahlo Hasanova.","Coloring options for the heatmap\nThe choice of color for the heatmap is one of the most important aspects of creating an understandable and nice-looking representation of the data. If you do not specify the color (as in the example above) then the default color palette will be applied. However, you can use the argument <syntaxhighlight lang=\"R\" inline>col<\/syntaxhighlight> and choose from a wide variety of palettes for coloring your heatmap.\n\nThere are two options of setting a color palette for the heatmap:\n* First option is to use the palettes from R: <syntaxhighlight lang=\"R\" inline>cm.colors()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>heat.colors()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>rainbow()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>terrain.color()<\/syntaxhighlight>  or <syntaxhighlight lang=\"R\" inline>topo.colors()<\/syntaxhighlight> \n* The second option is to install color palettes packages such as <syntaxhighlight lang=\"R\" inline>RColorBrewer<\/syntaxhighlight> \n\n==Additional materials==\n* [https:\/\/www.r-graph-gallery.com\/heatmap Other functions for building a heatmap]\n* [https:\/\/www.datanovia.com\/en\/blog\/how-to-normalize-and-standardize-data-in-r-for-great-heatmap-visualization\/ How and why we should normalize data for a heatmap]\n* [https:\/\/vwo.com\/blog\/heatmap-colors\/ How to choose the color palette for a heatmap]\n* [https:\/\/blog.bioturing.com\/2018\/09\/24\/heatmap-color-scale\/ Do's and Dont's in choosing a color palette for a heatmap]\n* [https:\/\/www.displayr.com\/what-is-dendrogram\/ What is a dendrogram]\n* [https:\/\/sustainabilitymethods.org\/index.php\/Clustering_Methods More about clustering methods and how to build a dendrogram in R]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Evgeniya Chetneva."],"27":["'''Note:''' The German version of this entry can be found here: [[Scientific methods and societal paradigms (German)]].\n\n'''In short:''' This entry discusses how [[Glossary|scientific methods]] have influenced society - and vice versa.\n__NOTOC__\n== The role of scientific paradigms for society ==\nFrom early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar. However, science was for the most part of the last two millennia in a critical relation to the metaphysical, as there was often a quest for ontological truths between religions and science. While the East was more open to allow science to thrive and made active use of its merits; in Europe, many developments were seen as critical, with Galileo Galileo being a prominent example. Since this changed with the [[History of Methods|Enlightenment]], science paved the way for the rise of the European empires, and with it the associated paradigms.\n\n== Three examples for an active interaction ==\nWhile the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples. \n\n==== Medicine ====\nA prominent example of a strong interaction is medicine, which has at its heart the care for the patient. However, this naive assumption cannot hold the diverse paradigms that influenced and build medicine over time. Today, ananmesis - the information gained by a physician by asking specific questions of a patient - gained in importance, and the interdisciplinary conferences of modern treatments combine different expertise with the goal of a more holistic recognition of the diseases or challenges of the individual patient. \n\n==== Engineering ====\nEngineering is another branch of science which builds on a long tradition, and has at its early stages quite literally paved the road for many developments of modernity. While factories and production processes are today also seen more critically, it has become clear already since Marx that the working condition of modern production are not independent of questions of inequality. In addition, production processes are shifting in order to enable more sustainable production processes, indicating another paradigm shift in engineering. \n\n==== Agricultural science ====\nThe last example, agricultural science, is also widely built around positivistic methodology of modern science, allowing of an optimisation of agricultural production in order to maximise agricultural yield, often with dire consequences. The so-called [https:\/\/www.thoughtco.com\/green-revolution-overview-1434948 'Green Revolution'] wreaked havoc on the environment, destroyed local livelihoods across the globe, and untangled traditional social-ecological systems into abusive forms that led ultimately to their demise in many parts of the world. \n\nThese three examples showcase how the development of modern science led to abusive, unbalanced, and often unsustainable developments that would in the long run trigger new paradigms such as the post-modernity, degrowths and other often controversially discussed alternatives to existing paradigms. Science was clearly an accomplice in driving many negative developments, and willingly developed the basis for many methodological foundations and paradigms that were seen in a different light after they were utilised over a longer time.\n\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently, science often acted morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational [[Glossary|change]]. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n\n==Science looking at parts of reality==\nSince the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world. The second mistake is that scientific progress can be seen as continuous (see Laudan: Progress and its Problems), while societal paradigms are often utilising snapshots of scientific theories and tend to ignore further development in the respective branch of science. This makes science in turn vulnerable, as it has to claim responsibility for mistakes society made in interpreting scientific theories, and translating them into societal paradigms. In the following message I will illustrate these capital mistakes of science based on several examples. \n\n==== Social Darwinism ====\nThe evolutionary theory of Charles Darwin can be seen as a first example that illustrates how a scientific theory had catastrophic consequences when it was adapted as a societal paradigm. Ideas that the poor in late Victorian England were unworthy of state intervention, and that social welfare was hence a mistake were build on a misunderstanding of Darwins theory, and Darwin opposed the application of his theory for societal debates. Furthermore, he was horrified that his ideas was also taken as a basis to claim superiority of some races over other races, a crude and scientifically wrong claim that paved the road for some of the worst atrocities of the 20th century.","'''Note:''' The German version of this entry can be found here: [[History of Methods (German)]]\n\n'''In short:''' This entry provides an overview on the history of scientific methodology through the ages. Where did science come from, and which influences and changes occurred until now?\n__TOC__\n<br\/>\n==== Antiquity - ''Observe And Understand'' ====\n'''With the rise of language, and much later with the invention of writing, we witnessed the dawn of human civilisation.''' Communicating experience and knowledge is considered to be one of the most pivotal steps that led to the formation of societies. Once we moved from being hunters and gatherers into larger and more complex [[Glossary|cultures]] that lived in cities, a surplus in food production allowed for some privileged people to preoccupy themselves with other goals than the mere safeguarding of their daily survival. This led to the blossoming of early cultures across the globe, many of which with tremendous developments in terms of agriculture, engineering and architecture. The rise of urban cultures East and West led to a new line of inquiry and ultimately also a new line of thinking, most notably in civilizations such as the Vedic, the Zhou period, early Persian culture, and - often most recognised in the West - Ancient Greece. \n\n[[File:1200px-Aristotle_Altemps_Inv8575.jpg|300px|thumb|'''Aristotle (384 - 322 BC).''' Source: [https:\/\/de.wikipedia.org\/wiki\/Aristoteles Wikipedia]]]\nGreece is often in the focus because we consider it the birthplace of modern democracy, despite only a small privileged elite actually being considered citizens. Though these privileged were only few, we owe [https:\/\/plato.stanford.edu\/entries\/aristotle-politics\/ Aristotle], [https:\/\/plato.stanford.edu\/entries\/socrates\/ Socrates] and [https:\/\/plato.stanford.edu\/entries\/plato\/ Plato] and many others the foundation of Western Philosophy. '''Empirical inquiry did not weigh down thinking yet, hence much of the thinking of Greek philosophy was lofty but free from the burden of real world inquiry.''' There were connections between philosophical thinking and the real world - you might remember [https:\/\/plato.stanford.edu\/entries\/pythagoras\/ Pythagoras] from school - yet much later philosophy and the rest of science would be vastly disconnected. Early accounts such as the scriputures of Herodot give testimony of the history of this age, and represent one of the earliest accounts of a systematic description of geography and culture. Early mathematics paved the way for more fundamental approaches, and [[Survey|surveys]] were a common part of the governance at the time. Hence many approaches that we would consider scientific methods were already being utilised, but not so much for scientific inquiry as for a direct purpose of benefit that was not necessarily associated with scientific knowledge production.\n\nEastern cultures often had a comparably early development of philosophies, with [https:\/\/plato.stanford.edu\/entries\/confucius\/ Confucius]' work as well as the Vedas and the Pali canon as testimony of the continuous cultural influence that is comparable to Greek philosophy in the West. Equally did many Eastern empires use methods such as census and survey as measures of governance, and many other notable approaches that would later contribute to the formation of scientific methods. '''Law was an early testimony of the necessity of rules and norms in human societies.''' Consequently, [[Legal Research]] can be seen as one of the earliest forms of inquiry that translated systematic inquiry and analysis directly to the real world.\n\nBetween the ancients and the medieval times there a somewhat blurry gap, with the fall of the Roman Empire in the West, the expansion of the Mongol empire in the East, and the occupation of much of India by Islam as notable elements of [[Glossary|change]]. All this triggered not only an enormous cultural change, but more importantly an increasing exchange, leading to schools of thoughts that became increasingly connected through writing, and also often in thinking. '''Logic is an example of a branch of philosophy that linked the ancients (Plato, Confucius, [https:\/\/plato.stanford.edu\/entries\/buddha\/ Buddha]) with the philosophy of the enlightenment.''' This was an important precondition in medieval times for a systematic line of thinking, triggering or further developing pragmatic, analytic and sceptic approaches, among others. Logic as a part of philosophy provided the basis for a clear set of terms and rhetoric within inquiry, and would later become even more important with regards to rationality. The early need for a clear wording was thus deeply rooted in philosophy, highlighting the importance of this domain until today. \n\nMany concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https:\/\/www.britannica.com\/biography\/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''","The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br\/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n'''Forming hypotheses'''\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n'''Limitations of hypothesis'''\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - \u2018causal\u2019 form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]"],"28":["'''In short:''' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically '''barplots'''. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.\n\n'''Correlation plots''' ('xyplots') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a [[Regression Analysis|regression]] line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.\n\n'''Boxplots''' are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant.\n\n[[File:Xyplot.png|250px|thumb|left|'''A Correlation plot.''' The line shows the regression, the dots are the data points.]]\n[[File:Boxplot3.png|250px|thumb|right|'''Boxplots.''']]\n[[File:2Barplots.png|420px|thumb|center|'''Barplots.''' The left diagram shows absolute, the right one relative Barplots.]]\n\n\n[[File:Histogram structure.png|300px|thumb|right|'''A Histogram.''']]\nA '''histogram''' is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n\nSimple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|'''A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.''' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|'''A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.''' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|'''A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic's importance in the research, and the connections show if topics were covered alongside each other.''' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n'''Descriptive Infographics''' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.","<imagemap>Image:Statistical Figures Overview 27.05.png|1050px|frameless|center|\ncircle 120 201 61 [[Big problems for later|Factor analysis]]\ncircle 312 201 61 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA, TREE SPECIES IN AMERICA as three colors, with joint species in the overlaps]]\ncircle 516 190 67 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA as two colors, with joint species in the overlaps]]\ncircle 718 178 67 [[Stacked Barplots|Stacked Barplot, e.g. count data of different species (colors) for the variable TREES]]\ncircle 891 179 67 [[Barplots, Histograms and Boxplots#Barplots|Barplot, e.g. different kinds of trees (x) as count data (y) for the variable TREES]]\ncircle 1318 184 67 [[Barplots, Histograms and Boxplots#Histograms|Histogram, e.g. the variable EXAM POINTS as count data (y) per interval (x)]]\ncircle 1510 187 67 [[Correlation_Plots#Line_chart|Line Chart, e.g. TIME (x) and BITCOIN VALUE (y)]]\ncircle 1689 222 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), POPULATION (bubble size)]]\ncircle 1896 238 67 [[Big problems for later|Ordination, e.g. numeric variables (AGE, INCOME, HEIGHT) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\ncircle 202 326 67 [[Treemap|Treemap, e.g. FORESTS (colors) and count data of the included species (rectangles)]]\ncircle 410 323 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. different species of trees (absolute count data per color) for the variables TREES in ASIA, TREES IN AMERICA, TREES IN AFRICA, TREES IN EUROPE (x)]]\ncircle 608 295 67 [[Stacked Barplots|Proportions Stacked Barplot, e.g. relative count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 812 277 67 [[Pie Charts|Pie Chart, e.g. different kinds of trees (relative count data per color) for the variable TREE SPECIES]]\ncircle 1015 308 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE HEIGHT (y) for beeches]]\ncircle 1228 287 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS per point (x)]]\ncircle 1422 294 67 [[Correlation_Plots#Scatter_Plot|Scatter Plot, e.g. RUNNER ENERGY LEVEL (y) per KILOMETERS (x)]]\ncircle 1574 379 67 [[Big problems for later|Heatmap with lines]]\ncircle 1788 401 67 [[Correlation_Plots#Correlogram|Correlogram, e.g. the CORRELATION COEFFICIENT (shade) for each pair of the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 297 441 67 [[Wordcloud|Wordcloud]]\ncircle 516 434 67 [[Big problems for later|Spider Plot, e.g. relative count data of different species (shape) for the variables TREES IN EUROPE (green), TREES IN ASIA (blue), TREES IN AMERICA (red)]]\ncircle 710 402 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 1323 410 67 [[Regression Analysis#Simple linear regression in R|Linear Regression Plot, e.g. INCOME (y) per AGE (x)]]\ncircle 392 558 67 [[Chord Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites]]\ncircle 621 521 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites (if connections are valued: 3 variables)]]\ncircle 853 496 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. different TREE SPECIES (x) and TREE HEIGHT (y)]]\ncircle 1014 521 67 [[Stacked Area Plot|Stacked Area Plot, e.g. INCOME (x) and count data (y) of BOUGHT ITEMS (colors) (if y is EXPENSES: three variables)]]\ncircle 1174 502 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS IN MATHS (blue) and EXAM POINTS in HISTORY (green) per point (x)]]\ncircle 1438 521 67 [[Big problems for later|Multiple Regression, e.g. INCOME (y) per AGE (x) in different COUNTRIES]]\ncircle 1657 554 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]","circle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]\ncircle 622 812 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER and categorical BRAND]]\ncircle 733 759 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color), POPULATION SIZE (bubble size)]]\ncircle 782 875 67 [[Big problems for later|Factor analysis]]\ncircle 1271 825 67 [[Big problems for later|Structural Equation Plot]]\ncircle 1394 771 67 [[Big problems for later|Ordination, e.g. numeric and categorical variables (AGE, INCOME, HEIGHT, PROFESSION) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\n<\/imagemap>"],"29":["Please refer to this [https:\/\/www.youtube.com\/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=\"text\" inline>f(x)<\/syntaxhighlight> that predicts <syntaxhighlight lang=\"text\" inline>y<\/syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, ''k-means clustering'', ''hierarchical clustering'', various approaches to performing ''regression'', ''principle component analysis'' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called ''Differential Privacy'' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. \"Support-vector networks.\" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. \"A theory of the learnable.\" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199\u2013231.\n\n(5) [https:\/\/www.vodafone-institut.de\/aiandi\/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305\u2013306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).","[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"|  '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>\n\n__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and\/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be ''noisy'' or the outputs are expected to be different in each case, you cannot hand-craft the \"[[Glossary|rules]]\" that account for ''every'' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: '''Machine Learning'''. The core idea behind Machine Learning is that instead of being required to hand-craft ''all'' the rules that take inputs and provide outputs in a fairly accurate manner, you can ''train'' the machine to ''learn'' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term \"Machine Learning\" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to \"learn\" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> pairs where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ..., y_n)<\/syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called ''true label''. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> and one of the many ''training algorithms'', you can use the model with new data <syntaxhighlight lang=\"text\" inline>(X_new)<\/syntaxhighlight> to make predictions <syntaxhighlight lang=\"text\" inline>(y_hat)<\/syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]\n\nIf you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification\/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* '''Performance measure (P)''' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the \"learned\" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book \"Learning from Data\" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:","(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).\n\n== Further Information ==\n* [https:\/\/www.datacamp.com\/community\/tutorials\/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-in-r Machine Learning in R for Beginners]\n* [https:\/\/www.youtube.com\/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https:\/\/www.youtube.com\/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]<\/nowiki>]\n* [https:\/\/medium.com\/s\/story\/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https:\/\/www.repricerexpress.com\/amazons-algorithm-a9\/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal."],"30":["Please refer to this [https:\/\/www.youtube.com\/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=\"text\" inline>f(x)<\/syntaxhighlight> that predicts <syntaxhighlight lang=\"text\" inline>y<\/syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, ''k-means clustering'', ''hierarchical clustering'', various approaches to performing ''regression'', ''principle component analysis'' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called ''Differential Privacy'' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. \"Support-vector networks.\" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. \"A theory of the learnable.\" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199\u2013231.\n\n(5) [https:\/\/www.vodafone-institut.de\/aiandi\/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305\u2013306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).","[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"|  '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>\n\n__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and\/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be ''noisy'' or the outputs are expected to be different in each case, you cannot hand-craft the \"[[Glossary|rules]]\" that account for ''every'' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: '''Machine Learning'''. The core idea behind Machine Learning is that instead of being required to hand-craft ''all'' the rules that take inputs and provide outputs in a fairly accurate manner, you can ''train'' the machine to ''learn'' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term \"Machine Learning\" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to \"learn\" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> pairs where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ..., y_n)<\/syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called ''true label''. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> and one of the many ''training algorithms'', you can use the model with new data <syntaxhighlight lang=\"text\" inline>(X_new)<\/syntaxhighlight> to make predictions <syntaxhighlight lang=\"text\" inline>(y_hat)<\/syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]\n\nIf you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification\/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* '''Performance measure (P)''' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the \"learned\" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book \"Learning from Data\" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:","(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).\n\n== Further Information ==\n* [https:\/\/www.datacamp.com\/community\/tutorials\/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-in-r Machine Learning in R for Beginners]\n* [https:\/\/www.youtube.com\/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https:\/\/www.youtube.com\/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]<\/nowiki>]\n* [https:\/\/medium.com\/s\/story\/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https:\/\/www.repricerexpress.com\/amazons-algorithm-a9\/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal."],"31":["'''This sub-wiki deals with scientific methods.''' <br\/>\n\n=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and\/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n'''This Wiki describes each presented method in terms of''' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki's underlying [[Design Criteria of Methods]].<br\/>\n'''This means that each method fulfills one or more categories of each of the following criteria:'''\n<br\/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br\/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=\"pages\" hideroot=\"on\">Methods<\/categorytree>\n<br>\nWe also have what we call '''Level 2''' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* '''[[Statistics]]''': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* '''[[Interviews]]''': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read.","'''We need to choose and apply methods depending on the type of knowledge we aim to create, regardless of the disciplinary background or tradition.''' We should aim to become more and more experienced and empowered to use the method that is most ideal for each research purpose and not rely solely on what our discipline has always been doing. In order to achieve this, design criteria of methods can help to create a conceptualization of the nature of methods. In other words: what are the underlying principles that guide the available scientific methods? First, we need to start with the most fundamental question:\n\n\n== What are scientific methods? ==\nGenerally speaking, ''scientific methods create knowledge''. This knowledge creation process follows certain principles and has a certain rigour. Knowledge that is created through scientific methods should be ideally [https:\/\/plato.stanford.edu\/entries\/scientific-reproducibility\/ ''reproducible''], which means that someone else under the given circumstances would come up with the same insights when using the same respective methods. This is insofar important, as other people would maybe create different data under a similar setting, but all the data should answer research questions or hypotheses in the same way. However, there are some methods that may create different knowledge patterns, which is why documentation is pivotal in the application of scientific methods. Some forms of knowledge, such as the perception of individuals, cannot be reproduced, because these are singular perspectives. '''Knowledge created through scientific methods hence either follows a systematic application of methods, or a systematic documentation of the application of methods'''. Reproducible approaches create the same knowledge, and other approaches should be equally well documented to safeguard that for all steps taken it can be understood what was being done precisely. \n\nAnother possibility to define methods concerns the different stages of research in which they are applied. '''Methods are about gathering data, analysing data, and interpreting data.''' Not all methods do all of these three steps, in fact most methods are even exclusive to one or two of these steps. For instance, one may analyse [[Semi-structured Interview|structured interviews]] - which are one way to gather data - with statistical tests, which are a form of analysis. The interpretation of the results is then built around the design of the interviews, and there are norms and much experience concerning the interpretation of statistical tests. Hence, gathering data, analysing it, and then interpreting the results are part of a process that we call ''design criteria of methods''. Established methods often follow certain more or less established norms, and the norms can be broken down into the main design criteria of methods. Let us have a look at these. \n\n==== Quantitative vs Qualitative ====\n'''One of the strongest discourses regarding the classification of methods revolves around the question whether a method is quantitative or qualitative'''. [[:Category:Quantitative|''Quantitative'']] methods focus on the measurement, counting and constructed generalisation, linking the statistical or mathematical analysis of data, as well as the interpretation of data that consists of numbers to extract [[Glossary|patterns]] or support theories. Simply spoken, quantitative methods are about numbers. [[:Category:Qualitative|''Qualitative'']] methods, on the other hand, focus on the human dimensions of the observable or conceptual reality, often linking observational data or interpretation of existing data directly to [[Glossary|theory]] or concepts, allowing for deep contextual understanding. Both quantitative and qualitative methods are ''[[Normativity of Methods|normative]]''. These two generally different lines of thinking are increasingly linked in recent decades, yet the majority of research - and more importantly, disciplines - are dominated by either qualitative or quantitative methods. While this is perfectly fine per se, there is a deep ideological trench between these two approaches, and much judgement is passed on which approach is more valid. This can be misleading if not wrong, and propose instead to choose the appropriate approach depending on the intended knowledge. However, there is one caveat: much of the scientific canon of the last decades was dominated by research building on quantitative approaches. However, new exciting methods emerge especially in qualitative research. Since novel solutions are necessary for the problems we currently face, it seems necessary that the amount as well as the proportion of qualitative research increases in the future.","The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br\/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n'''Forming hypotheses'''\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n'''Limitations of hypothesis'''\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - \u2018causal\u2019 form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]"],"32":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Noble Eightfold Path - Mindfulness.png|350px|thumb|right|'''The Noble Eightfold Path, with Mindfulness being the seventh practice.''' Source: [https:\/\/en.wikipedia.org\/wiki\/Noble_Eightfold_Path Wikipedia], Ian Alexander, CC BY-SA 4.0]]\nMindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the context of many diverse but often religious schools of thinking.\n\n== Goals ==\nSince the goal of mindfulness is basically having \"no mind\", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free. \n\n== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen \u2013 sitting meditation \u2013 is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|'''[https:\/\/www.headspace.com\/de Headspace] is an app that can help you meditate''', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around \u2013 often referred to as the monkey mind \u2013, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https:\/\/www.forbes.com\/sites\/alicegwalton\/2017\/02\/28\/8-science-based-tricks-for-quieting-the-monkey-mind\/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions \u2013 just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!\n\n== Links & Further Reading ==\n* [https:\/\/www.headspace.com Headspace] \n* [https:\/\/www.youtube.com\/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https:\/\/thichnhathanhfoundation.org\/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https:\/\/en.wikipedia.org\/wiki\/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on ''Zen Mind, Beginner's Mind'' is classic introduction to Zen.\n* [https:\/\/www.forbes.com\/sites\/alicegwalton\/2017\/02\/28\/8-science-based-tricks-for-quieting-the-monkey-mind\/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn.","Another approach that was beneficial to me is to write about things that keep preoccupying your mind. If you keep coming back to a certain thought, yet cannot really verbalise why you cannot let it be, why not write about it? Writing can be a surprisingly catalytic and clarifying approach to structure and analyse your thoughts. Writing 500-1000 words per day should be a no-brainer for any aspiring academic. Bear with me, this will grow on you if your are lucky. Start with a research diary, reflecting and verbalising what you learned on this specific day. This only works if you make it a habit. Remember that mails and other communication counts into the word count. I sometimes receive mails that are pieces of art. New years resolutions are worthless to start a new habit like writing. You need to minimise the friction instead, finding the right time, place and mood that makes you automatically start writing no matter what. Me, I sit in the chair where I write most texts, listening to the \"Tales from the Loop\"-soundtrack that propelled about 90% of all texts I wrote in the last year. If I put on this soundtrack, my fingers start twitching almost by instinct. Writing should be a reward, as I think it is a privilege. Writing cannot be pressed between two other time slots, it needs to be free and unbound, allowing your mind to do nothing else. From then on it is to me how Jazz is in music. Much of Jazz music is hard work and practice, almost to the point where your subconscious takes over and you are in autopilot mode. You need to practice enough so that your lack of skill does not stop you from writing. To me, this learning curve is surprisingly rewarding, it is almost like learning to be a rock climber. The first day is the horror. All muscles ache, you are basically destroyed. This will get worse for a few days. Suddenly, after two weeks of daily practice you will surprise yourself. After three months of daily practice you will lift yourself easily up the wall on previously impossible routes, and to others your path looks smooth and easy going. Writing is just like this.\n\n'''Studying teaches you to try things out'''<br>\nBeside the three staples of academics - reading, group work and writing - learning at a University is also about many other opportunities to learn and grow. This list is very specific and context depended for each and every single person. Still, the general consensus is that studying is about trying things out, how you can learn best, and find out what you are good at, and how you can contribute best. Here are some points that I consider to be relevant.\n\n'''Soft skills'''<br>\nAmong the diverse term of soft skills are personal traits and approaches that basically help us to interact. While this could be associated to group work (see above), I think it is good to make a mind map that you keep updating and exchange about with others. This is nothing that you need to obsess about, but more like a conscious and reflexive diary of your own personal development. Actually, a research diary can be a good first step. Also, if you witness others that excel at a certain soft skill, approach them and ask them how they learned their respective skills. It is also quite helpful -surprise- to practice. Presentations are something that are often not right the first time, and constructive feedback from critical people that you trust goes a long way. Much of the literature and other resources on soft skills are often over-enthusiastic, and promise the one and only best approach. Do not let yourself be fooled by such simple fixes, some of the soft skill literature is rather fringe. Still, new approaches to knowledge and interaction await, much can be gained, and only a bit of time may be lost. Why not giving another soft skill a go? The most important step is then to try it really out. Doing meditation once will tell you nothing about it, yet after some weeks you may perceive some changes. Your first World Caf\u00e9 was a failure? Well, try it again, several times, in different settings. For soft skills you need to stay open minded.\n\n'''Digital natives?'''<br>\nWe are awash with information to the brim, and continuously on the edge of drowning in it. Mastering all things digital may be one of the most important skills in this age and place. I think the most important rule to this end is: Less is more. Evidence how bad too much exposure to the digital world seems to be is mounting. Social media made many promises, yet I am not sure how many were kept. I can acknowledge that it can create meaningful linkages, build capacity, and even be a lifeline to your distant friend. Nevertheless, I would propose to be very reflexive which emotions are triggered by social media within you. This may lead to the conclusion to detox. The same holds true for all things extreme, namely binge watching, youtube or Spiegel Online. Instead you need to become versatile in a word processor, Powerpoint, maybe a graphical software, and get a hold of your direct digital communication. E-mail is still a thing, and writing a good e-mail is a skill that is equally admirable and often missed out on by students. I have been there. Again, practice goes a long way. Also, be conscious about data structure, backups, and online plans. You should be able to impress others with you digital skills. This will open many doors, and tilt many opinions towards your direction. Get at it!","'''Journalling''' might also generally be helpful to get thoughts out of your head and onto a piece of (digital) paper. Just sit down every evening for 10 minutes or so and write down whatever is going through your head. If you find it hard to start, just write down what happened on that day. \n\n'''Confide in people you trust.''' It might be hard at first, but our experience is that not only does it help to explicate your thoughts and feelings, but also to understand that you are not alone in how you feel.\n\n===  Lifestyle choices ===\n\nA friend once called these lifestyle choices the \u201cKleines 1x1 der Psychohygiene\u201d (Psychological Hygiene 101):\n\n# '''Sleep regularly and long enough.''' For most people, this means at least 7 and no more than 9 hours of sleep every day, at the same time. From personal experience I would say this makes all the difference in the world. \n# '''Eat & drink regularly.''' Try to eat good, healthy food, and drink at least 2 litres of water per day. \n# '''Do sports.''' Seriously, whatever it is. Walking for 30 mins, jogging, climbing, doing Yoga, going to the Gym, doing jumping jacks. Whatever it is, it\u2019s almost certainly helpful.  \n# '''Regularity.''' This is a big one. Structures help a lot. If you have to study or work, try to do it at a place outside of your home, every day at the same time. The place does not really matter as long as it is not your own room, be it the university or a caf\u00e9 or a friends place. \n\nThese might seem trivial, and they certainly do not solve any underlying causes of whatever problem you might have. They do however help to get out of the hole and into a state where you can actually do something about your situation, and that is incredibly important.\n\n=== The student perspective on lifestyle choices ===\n\nAs a student, this can be especially frustrating. People might be partying and you don\u2019t want to leave at 11 to go to bed. After all, this is supposed to be the time of your life. Everyone else might seem like they just live into their day and do whatever feels right at the moment, and having a structured everyday life might prevent you from doing the same. What might help you is to think about this as a transitional phase: you\u2019re not going to do this forever. It\u2019s a phase you need in order to get back up on your feet and enjoy life again. It might suck, but it won\u2019t suck forever. \n\nWe wish you all the best.\n\n=== Links & Further reading ===\n\n* '''Das Kind in dir muss Heimat finden''' von Stefanie Stahl: https:\/\/www.stefaniestahl.de\/buecher_daskind_page1\/ (Recommendation by the team)\n\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."],"33":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|'''MindMaps can take the form of trees, with the words on the branches, or clusters\/bubbles, as in this example.''' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https:\/\/www.thetutorteam.com\/wp-content\/uploads\/2019\/07\/shutterstock_712786150.jpg thetutorteam.com]]]\n\n'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). '''This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.''' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. '''Elements may be subordinate to other elements, indicating nestedness of the information.''' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n''Sources:''\n* [https:\/\/www.mindmapping.com\/de\/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http:\/\/methodenpool.uni-koeln.de\/download\/mindmapping.pdf Uni K\u00f6ln Methodenpool - Mind-Mapping]]\n* [https:\/\/kreativit\u00e4tstechniken.info\/problem-verstehen\/mindmapping\/ Kreativit\u00e4tstechniken.info - Mindmapping]]\n* [https:\/\/www.lifehack.org\/articles\/work\/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https:\/\/www.thetutorteam.com\/blog\/mind-maps-how-they-can-help-your-child-achieve\/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConcept Maps are a '''form of visually organizing information''' for groups and individuals. Conceptual terms or short statements are presented in boxes or bubbles and interlinked with other terms through commented arrows, resulting in a hierarchical network-like structure that provides an overview on a topic.\n\n== Goals ==\n* Organize '''conceptual knowledge''' in a visual form to identify learning processes and knowledge gaps.\n* Create new knowledge, ideas and solutions by '''arranging information in a structured manner''' around a focus question, topic, or problem.\n\n== Getting started ==\n[[File:Concept Maps - Example 1.png|500px|thumb|right|'''An example for a Concept Map on the knowledge structure required for understanding why we have seasons.''' . Source: [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php Novak & Ca\u00f1as 2008, Cmap]]]\nConcept Maps result from the work of '''Joseph Novak and colleagues at Cornell University in the 1970s''' in the field of education, and have since been applied in this area and beyond. The original underlying idea was to analyze how children learn, assuming that they do so by assimilating new concepts and positioning these in a cognitive conceptual framework. The idea of Concept Maps emerged from the demand for a form of assessing these conceptual understandings.\n\nImportantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of [[Group Concept Mapping]]. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a [[Mindmap]] in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.\n\nConcept Maps '''help identify a current state of knowledge''' and show gaps within this knowledge, e.g. a lack of understanding on which elements are of importance to a question or topic, and how concepts are interrelated. Identifying these gaps helps fill knowledge gaps. Therefore, they can be a helpful tool for students or anyone learning a new topic to monitor one's own progress and understanding of the topic, and how this changes as learning units continue.\n\nFurther, Concept Maps can be '''a way of approaching a specific question''' or problem and support a systematic solution-development process. The visual representation of all relevant elements nconcerning a specific topic can thus help create new knowledge. There are even more imaginable purposes of the approach, including management and planning, creative idea-generation and more.\n\n[[File:Concept Map Example 2.png|500px|thumb|center|'''This example from Novak (2016, p.178) shows how a student's conceptual understanding of a topic develops over time.''' It illustrates how Concept Maps can help identify knowledge gaps or flaws (which are existent still in the map below), how much (and what kind of) learning process was made over time, and what to focus on in future learning. Source: Novak 2016, p.178.]]\n\n\n== Step by step ==\n[[File:Concept Map - Step by Step 1.png|400px|thumb|right|'''A list of concepts (left) and a \"string map\" that focuses on cross-links between concepts, as a groundwork for the concept map.''' Source: [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php CMap]]]\n\n# When learning to work with Concept Maps, Novak & Canas (2008) recommend to '''start with a domain of knowledge that one is familiar with'''. Here, the learner should focus on a specific focus question and\/or text item, activity, or problem to structure the presented elements and hierarchies around, and to contextualize the map by. As the authors highlight, often, \"learners tend to deviate from the focus question and build a concept map that may be related to the domain, but which does not answer the question. It is often stated that the first step to learning about something is to ask the right questions\".\n# After defining the domain of knowledge and focus question or problem, 15-25 '''key concepts should be identified and ranked''' according to their specificity. In a concept map, more general, broad concepts are on the top of the map, while more specific concepts are found below, resulting in a hierarchy from top to bottom. So the concepts should be ordered accordingly in a list first.\n# Then, a preliminary concept map is created, either digitally (with the 'official' IHMC CmapTools software, see below) or by using Post-Its and a whiteboard. Concepts can and should be moved around iteratively. Propositions  - connections between concepts with text - are added to the map to highlight and explain relationships between concepts. When a first good draft exists, one identifies cross-links, which are relationships between separate areas of the map, to further highlight conceptual linkages. Lastly, '''constant revision is applied''' and concepts can be re-positioned to further improve the map.\n\n\n== Links & Further reading ==\n* This entry was created based on the extensive entry by Novak & Canas (2008), available [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php here]. The authors are responsible for the introduction of Concept Maps in the 1970s and the development of the CmapTools.\n\n* Novak, J.D. 2016. The origins of the concept mapping tool and the continuing evolution of the tool. Information Visualisation 5. 175-184.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]","[[File:Group Concept Mapping - Cluster Rating Map - Example.png|700px|thumb|center|'''The resulting (yet unlabeled) Cluster Rating Map for McCaffrey et al. 2019''' (p.89).]]\n\n\n==== 5 - Interpretation of Maps ====\nNow, '''the group is asked to assign names to the clusters'''. Each participants looks at each cluster and the statements included, and suggests a name (e.g. a phrase, or a word) to describe the cluster, and the group negotiates until consensus is reached for each cluster. If there are a lot of clusters, it may be sensible to further develop names for groups of clusters - \"regions\" - but this depends on the map at hand. In any case, the names of the clusters should represent the statements included as well as the conceptual relation to other clusters which are close. This labeled Cluster Map is the main outcome of the Group Concept Mapping process. It can be re-arranged by the group if necessary, since they should feel comfortable with the conceptual framework it represents.\n\n'''In the end of the process, the group has the following results:'''\n* a statement list\n* a point map\n* a point rating map\n* a cluster list (listing all labeled clusters including the respective statements)\n* a labeled cluster map, \n* and a labeled cluster rating map.\n\n'''Our example'''<br>\nMcCaffrey et al. named the clusters themselves, based on \"cluster names provided by participants whose\nsorting produced results similar to the final cluster content, and (2) by reviewing statements within each cluster\" (p.89).\n\n[[File:Group Concept Mapping - Cluster List (first half).png|600px|frameless|center]]\n[[File:Group Concept Mapping - Cluster List (second half).png|600px|frameless|center]]\n\nThe final list of clusters in McCaffrey et al. (2019, p.90f) The clusters (left) are presented in order of importance (right), with a description and exemplary statements for each cluster in the center.\n\n\n==== 6 - Utilization of Maps ====\nThe group is now done with the Group Concept Mapping process, and can use either of the maps (preferable the Cluster Map, or Cluster Rating Map) as a baseline for their further work in many diverse ways. '''The maps show the most important elements they need to pay attention to,''' which can be used to coordinate future actions, prioritize tasks, and structure the process. The clusters can serve as the organizational foundation, or as groups of topics to work on, either when implementing measures, or developing an evaluation scheme.\n\n'''Our example'''<br>\nFor example, the results of the study by McCaffrey et al. (2019) may be of value for health care providers to evaluate and improve their services, and for health researchers to identify relevant aspects for further investigation.\n\n\n== Strengths & Challenges ==\n* Group Concept Mapping is a systematic and highly structured process with clear procedural steps, that helps a diverse group of people gather and structure their thoughts into a coherent and consensual set of maps.\n* '''The process of Group Concept Mapping is empowering''': all content that is included in, and leads to the final maps is created by the group itself, in their own language and based on their own perspectives. The participants will feel more ownership for the conceptual framework that results from the process, and the framework is more likely to be actively used by all involved actors than a framework that is imposed without involvement.\n* The end result of the process is a visual representation which introduces all important ideas at a glance in a structured manner. '''The maps can be easily communicated''' and presented without any knowledge about the methodological process.\n* Group Concept Mapping is versatile in that it can work with all kinds of statements, gathered from workshop sessions as presented above, or from documents, organisational structures etc. As Jackson and Trochim (2002) present, it can also be a useful approach to analyze open-ended survey responses, which are transformed into single statements and sorted by the researchers. Then, they can be analyzed using the multidimensional scaling and cluster analysis steps as presented above.\n* A challenge lies in the organisation of the process. For example, the number and selection of participants will influence the outcome and must therefore be done carefully. Further, the number of statements that are gathered, the way they are reduced if necessary, as well as the number of clusters are in the hands of the researchers. These decisions shape the end results and require thoughtful consideration of the topic and data at hand.\n\n\n== Normativity ==\n* Group Concept Mapping may be of interest in application-oriented research and transdisciplinary research. It allows for non-scientific actors to contribute their perspectives, actively engage with an issue and (research) question, and it empowers them to approach the problem with the self-developed conceptual framework.\n* The method is an interesting mix of qualitative research, akin to workshop-based or interview-like research approaches; and quantitative multivariate statistical analysis. Therefore, '''this single methodological process is a mixed methods approach in itsel'''f, highlighting the power of a sensible combination of diverse methods.\n* To assess the reliability and consistency of the process, Trochim et al. (1994) suggest using the contingency coefficient for all pairs of sorts from the matrix (Step 3). This way, it can be analyzed how the individual participants' sorts are interrelated, and if they sorted consistently. They further propose to split the group and correlate the resulting matrices and the results of the multi-dimensional scaling.\n* As McCaffrey et al. (2019) highlight, the results of Group Concept Mapping are mostly based on self-reported perception, which might differ from external analyses of the issue at hand. In their case, it was insightful to learn about patients' perspective on good health care, but medical reports offer another valuable perspective that should be taken into account.\n\n\n== Outlook ==\nMethodologically, Group Concept Mapping has been established for decades, and although elements of the process have found re-iterations and diversification, the general process seems unlikely to change drastically. However, as more transdisciplinary and practice-oriented research, as well as mixed methods approaches, take place, one can assume that Group Concept Mapping may find more diverse and frequent application in different fields of research in the future.\n\n\n== Key Publications ==\nTrochim, W.M.K. 1989. ''AN INTRODUCTION TO CONCEPT MAPPING FOR PLANNING AND EVALUATION.'' Evaluation and Program Planning 12. 1-16.\n\nKane, M., & Trochim, W. M. K. (2007). ''Concept mapping for planning and evaluation.'' Sage Publications, Inc."],"34":["[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.","W\u00e4hrend Psychologie, Medizin, Agrarwissenschaft, Biologie und sp\u00e4ter auch die \u00d6kologie auf diese Weise in der Anwendung von Versuchspl\u00e4nen und Studien gediehen, gab es auch eine zunehmende Anerkennung von Informationen, die [[Bias and Critical Thinking|Bias]] erzeugten oder die Ergebnisse anderweitig verzerrten. '''Die ANOVA wurde daher durch zus\u00e4tzliche Modifikationen erg\u00e4nzt, was schlie\u00dflich zu fortgeschritteneren Statistiken f\u00fchrte, die in der Lage waren, sich auf verschiedene statistische Effekte zu konzentrieren'', und den Einfluss von Bias zu reduzieren, z.B. in Stichproben, statistischem Bias oder anderen Fehlern. Somit wurden [[Mixed-Effect Models]] zu einem fortgeschrittenen n\u00e4chsten Schritt in der Geschichte der statistischen Modelle, der zu komplexeren statistischen Designs und Experimenten f\u00fchrte, bei denen immer mehr Informationen ber\u00fccksichtigt wurden. Dar\u00fcber hinaus f\u00fchrten meta-analytische Ans\u00e4tze dazu, mehrere Fallstudien zu einem systematischen \u00dcberblick zusammenzufassen und zusammenzufassen. Dies war der Beginn eines integrativeren Verst\u00e4ndnisses verschiedener Studien, die zu einer [[Meta-Analysis|Meta-Analyse]] zusammengefasst wurden, wobei auch die unterschiedlichen Kontexte der zahlreichen Studien ber\u00fccksichtigt wurden. Dar\u00fcber hinaus konzentrierte sich die Forschung mehr und mehr auf ein tieferes Verst\u00e4ndnis einzelner Fallstudien, wobei der spezifische Kontext des jeweiligen Falles st\u00e4rker betont wurde. Solche Einzelfallstudien sind in der medizinischen Forschung seit Jahrzehnten von Wert, wo trotz des offensichtlichen Mangels an einem breiteren Beitrag h\u00e4ufig neue Herausforderungen oder L\u00f6sungen ver\u00f6ffentlicht werden. Solche medizinischen Fallstudien berichten \u00fcber neue Erkenntnisse, auftauchende Probleme oder andere bisher unbekannte Falldynamiken und dienen oft als Ausgangspunkt f\u00fcr weitere Forschung. Aus so unterschiedlichen Urspr\u00fcngen wie [[System Thinking & Causal Loop Diagrams|Systemdenken]], Stadtforschung, [[Ethnography|Ethnographie]] und anderen Forschungsfeldern entstanden [[Living Labs & Real World Laboratories|Realwelt-Experimente]], die im allt\u00e4glichen sozialen oder kulturellen Umfeld stattfinden. Die starren Entw\u00fcrfe von Labor- oder Feldexperimenten werden gegen ein tieferes Verst\u00e4ndnis des spezifischen Kontexts und Falls eingetauscht. W\u00e4hrend Experimente aus der realen Welt bereits vor einigen Jahrzehnten entstanden sind, beginnen sie erst jetzt, breitere Anerkennung zu finden. Gleichzeitig stellt die Reproduzierbarkeitskrise die klassischen Labor- und Feldexperimente in Frage, da man sich dar\u00fcber im Klaren ist, dass viele Ergebnisse - zum Beispiel aus psychologischen Studien - nicht reproduziert werden k\u00f6nnen. All dies deutet darauf hin, dass zwar ein gro\u00dfer Teil unseres wissenschaftlichen Wissens aus Experimenten stammt, dass aber auch \u00fcber die Durchf\u00fchrung der Experimente selbst noch viel zu lernen verbleibt.\n----\n[[Category: Normativity_of_Methods]]\n[[Category: Methods]]\n[[Category: Statistics]]\n[[Category: Qualitative]]\n[[Category: Deductive]]\n[[Category: Individual]]\n[[Category: System]]\n[[Category: Global]]\n[[Category: Present]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution."],"35":["Another very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","The greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.\n\n'''Mixed Effect Models are built on a litany of preconditions, most of which most researchers choose to conveniently ignore.''' In my experience, this is more often than not ok, because it does not matter. Mixed Effect Models are - bless the maximum likelihood estimation - very sturdy. It is hard to find a model that does not have some predictive or explanatory value, even if hardly any pre-conditions are met. Still, this does not mean that we should ignore these. In order to sleep safe and sound at night, I am almost obsessed with model inspection, checking variance across levels, looking at the residuals, and looking for gaps and flaws in the model's fit. We should be really conservative to this end, because by focusing on fixed and random variance, we potentiate things that could go wrong. As I said, more often than not, this is not the case, but I propose to be super conservative when it comes to your model outcome. In order to get there, we need yet another thing: Model simplification.\n\nMixed Effect Models lead the forefront of statistics, and this might be the reason why the implementation of AIC (Akaike Information Criterion) as a parsimony-based evaluation criterion is more abundant here when compared to other statistical approaches. P-values fell widely out of fashion in many branches of science, as did the reporting of full models. Instead, model reduction based on information criteria approaches is on the rise, reporting parsimonious models that honour [[Why_statistics_matters#Occam.27s_razor | Occam's razor]]. Starting with the maximum model, these approaches reduce the model until it is the minimum adequate model - in other words, the model that is as simple as possible, but as complex as necessary. The AIC is about the equivalent of a p-value of 0.12, depending on the sample size, hence beware that the main question may be the difference from the null model. In other words, a model that is better from the Null model, but only just so based on the AIC, may not be significant because the p-value would be around 0.12. This links to the next point: Explanatory power.\n\nThere has been some sort of a revival of r<sup>2<\/sup> values lately, mainly based on the suggestion of r<sup>2<\/sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2<\/sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2<\/sup>  value, and even have their model selection not independent of the r<sup>2<\/sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2<\/sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2<\/sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2<\/sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.","[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions."],"36":["== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant.","The question of model reduction will preoccupy statistics for the next decades, and this development will be interacting with a further rise of Bayes theorem and other questions related to information processing. Time will tell how regressions will emerge on the other side, yet it is undeniable that there is a use case for this specific type of statistical model. Whether science will become better in terms of the theoretical foundations of regressions, in recognising and communicating the restrictions and flaws of regressions, and not overplaying their hand when it comes to the creation of knowledge, is an altogether different story. \n\n\n== Key Publications ==\n\n== References ==\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden and Quentin Lehrer.","The last problem of regression analysis is the diversity of disciplinary norms and conventions when it comes to the reduction of complex models. '''Many regressions are multiple regressions, where the dependent variable is explained by many predictors (= independent variables).''' The interplay and single value of several predictors merits a model reduction approach, or alternatively a clear procedure in terms of model constructions. Different disciplines, but also smaller branches of sciences, differ vastly when it comes to these diversities, making the identification of the most parsimonious approach currently a challenge. \n\n== Simple linear regression in R ==\n\nAs mentioned before, R can be a powerful tool for visualising and analysing regressions. In this section we will look at a simple linear regression using \"forbes\" dataset.\n\nFirst of all we need to load all the packages. In this example, we will use the forbes dataset from the library MASS.\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"MASS\", repos = \"ttp:\/\/cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(MASS)\n\n#Let's look closer at the dataset \n?forbes\n<\/syntaxhighlight>\n\nWe can see that it is a dataframe with 17 observations corresponding to observed boiling point and corrected barometric pressure in the Alps. Let's arrange the dataset and convert it into a tibble in order to make it easier to analyze. It allows us to manipulate the dataset quickly (because the variables type is directly displayed).\n\n<syntaxhighlight lang=\"R\" line>\nforbes_df <- forbes   # We just rename the data\nforbes_tib <- as_tibble(forbes_df) # Convert the dataframe into a tibble\nhead(forbes_tib)     # Shows first 6 rows of dataset\n\n#Output:\n## # A tibble: 6 x 2\n##      bp  pres\n##   <dbl> <dbl>\n## 1  194.  20.8\n## 2  194.  20.8\n## 3  198.  22.4\n## 4  198.  22.7\n## 5  199.  23.2\n## 6  200.  23.4\n\nstr(forbes_tib)     # Structure of Prestige dataset\n\n#Output:\n## Classes 'tbl_df', 'tbl' and 'data.frame':    17 obs. of  2 variables:\n##  $ bp  : num  194 194 198 198 199 ...\n##  $ pres: num  20.8 20.8 22.4 22.7 23.1 ...\n<\/syntaxhighlight>\n\nIt is important to be sure that there is no missing value (\"NA\") to apply the linear regression. In case of the forbes dataset, which is basically small, we can see that there is no NA.\n\n<syntaxhighlight lang=\"R\" line>\nsummary(forbes_tib)  # Summarize the data of forbes\n\n#Output:\n##        bp             pres      \n##  Min.   :194.3   Min.   :20.79  \n##  1st Qu.:199.4   1st Qu.:23.15  \n##  Median :201.3   Median :24.01  \n##  Mean   :203.0   Mean   :25.06  \n##  3rd Qu.:208.6   3rd Qu.:27.76  \n##  Max.   :212.2   Max.   :30.06\n<\/syntaxhighlight>\n\nIn order to make it easier to understand for us, we are going to convert the two variables:\n1. the boiling temperature of the water (From Farenheit to Celcius). We will use the formula to convert a temperature from Farenheit to Celcius: C = 5\/9 x (F-32)\n\n<syntaxhighlight lang=\"R\" line>\nrequire(MASS)\nrequire(dplyr)\nFA <- forbes_tib %>%  # We define a table F that stands for the F in the above formula \n  dplyr::select(bp)  # and containing all the information concerning Temperatures\n\nTempCel <- ((5\/9) * (FA-32))\nTempCel\n\n#Output:\n##           bp\n## 1   90.27778\n## 2   90.16667\n## 3   92.16667\n## 4   92.44444\n## 5   93.00000\n## 6   93.27778\n## 7   93.83333\n## 8   93.94444\n## 9   94.11111\n## 10  94.05556\n## 11  95.33333\n## 12  95.88889\n## 13  98.61111\n## 14  98.11111\n## 15  99.27778\n## 16  99.94444\n## 17 100.11111\n<\/syntaxhighlight>\n\n2. the barometric pressure (From inches of mercury to hPa). We will use the following formula to convert inches of mercury: hPa = Pressure (inHg) x 33.86389\n\n<syntaxhighlight lang=\"R\" line>\nrequire(MASS)\nrequire(dplyr)\nPress1 <- forbes_tib %>%\n  dplyr::select(pres)\n\n\nPressureHpa <- Press1 * 33.86389\nPressureHpa\n\n## Output:\n##         pres\n## 1   704.0303\n## 2   704.0303\n## 3   758.5511\n## 4   767.6944\n## 5   783.9491\n## 6   790.7218\n## 7   809.0083\n## 8   812.3947\n## 9   813.4106\n## 10  813.0720\n## 11  851.3382\n## 12  899.7636\n## 13  964.7822\n## 14  940.0616\n## 15  983.4074\n## 16 1011.8530\n## 17 1017.9485\n\n<\/syntaxhighlight>"],"37":["The last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find.","== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant.","'''Statistical info about data:'''\nTo get a descriptive statistical overview of the data, we can use the described method from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\nprint(data.describe())\n<\/syntaxhighlight>\nThis will calculate basic statistics for numeric columns, such as the mean, standard deviation, minimum and maximum values, and other summary statistics. This can be a useful way to get a general idea of the data and identify potential issues or trends.\nThe output of the described method will be a table with the following attributes:\n* count: the number of non-null entries\n* mean: the mean value\n* std: the standard deviation\n* min: the minimum value\n* 25%, 505, 75%: the lower, median, and upper quartiles\n* max: the maximum value\n\n'''Check for missing values'''\nTo check if the data has any missing values, we can use the isnull and any methods from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().any())\n<\/syntaxhighlight>\n\nThis will return a table with a True value for each column that has missing values, and a False value for each column that does not have missing values.\nChecking for missing values is important because most modeling techniques cannot handle missing data. If your data contains missing values, you will need to\neither impute the missing values (i.e. replace them with estimated values) or remove the rows with missing values before fitting a model.\nIf there are missing values in the data, you can use the sum method to check the number of missing values per column, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().sum())\n<\/syntaxhighlight>\n\nAlternatively, you can use the shape attribute to calculate the percentage of missing values per column, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().sum()\/data.shape[0]*100)\n<\/syntaxhighlight>\n\nThis can help you understand the extent of the missing values in your data and decide how to handle them.\n\n===Check for duplicate entries===\nTo check if there are duplicate entries in the data, we can use the duplicated method from Pandas, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.duplicated())\n<\/syntaxhighlight>\n\nThis will return a True value for each row that is a duplicate of another row, and a False value for each unique row.\nIf there are any duplicate entries in the data, we can remove them using the drop_duplicates method, like this:\n\n<syntaxhighlight lang=\"Python\" line>\ndata = data.drop_duplicates()\n<\/syntaxhighlight>\n\nThis will return a new dataframe that contains only unique rows, with the duplicate rows removed. This can be useful for ensuring that the data is clean and ready for analysis.\n\n'''Short introduction to data visualization'''\nData visualization can be a powerful tool for inspecting data and identifying patterns, trends, and anomalies. It allows you to quickly and easily explore the data, and get insights that might not be immediately obvious when looking at the raw data.\nFirst, we start by getting all columns with numerical data by using the select_dtypes method and filtering for in64 and float64:\n\n<syntaxhighlight lang=\"Python\" line>\n# get all numerical data columns\nnumeric_columns = data.select_dtypes(include=['int64', 'float64'])\nprint(numeric_columns.shape)\n# Print the numerical columns\nprint(numeric_columns)\n\n===Boxplot===\nThey are particularly useful for understanding the range and interquartile range of the data, as well as identifying outliers and comparing data between groups.\nIn the following, we want to plot multiple boxplots using the subplots method and the boxplot method:\n\n<syntaxhighlight lang=\"Python\" line>\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a boxplot for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i \/\/ 3, i % 3]\nax.boxplot(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n<\/syntaxhighlight>\n[[File:Boxplot inspection.PNG|centre right]]\n\n===Histogram===\nHistograms are a type of graphical representation that shows the distribution of a dataset. They are particularly useful for understanding the shape of a distribution and identifying patterns, trends, and anomalies in the data.\nIn the following, we want to plot multiple histograms using the subplots method and the hist method:\n\n<syntaxhighlight lang=\"Python\" line>\n12\/23\/22, 4:25 PM Data_Inspection_ASDA_Wiki - Jupyter Notebook\nlocalhost:8888\/notebooks\/1000\/Data_Inspection_ASDA_Wiki.ipynb 9\/9\nIn [15]:\nAt this point, we have most of the information needed to start the Data Cleaning process.\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a histogram for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i \/\/ 3, i % 3]\nax.hist(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n<\/syntaxhighlight>\n\n[[File:Hist inspection.PNG|centre right]]\n\nAt this point, we have most of the information needed to start the data-cleaning process.\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is Sian-Tang Teng. Edited by Milan Maushart"],"38":["Narrative Research is \"(...) the study of stories\" (Polkinghorne 2007, p.471) and thus \"(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience.\" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the 'narratology', i.e. the structure and grammar of a story; the 'narrative content', i.e. the themes and meanings conveyed through the story; and\/or the 'narrative context', which revolves around the effects of the story (Squire et al. 2014).\n\n'''One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.''' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, 'narrative' is the underlying \"frame of reference\" (Moen 2006, p.57) for the research. An example for the latter understanding is the 'Narrative Interview'.\n\n==== Narrative Interviews ====\n[[File:Narrative Research Phases.png|400px|thumb|right|'''Basic phases of the narrative Interview.''' Source: Jovchelovitch & Bauer 2000, p.5.]]\n\nThe Narrative Interview is an Interview format that \"(...) encourages and stimulates an interviewee (...) to tell a story about some significant event in their life and social context.\" (Jovchelovitch & Bauer 2000, p.3). 'Narrative Interviewing' \"is considered a form of unstructured, in-depth interview with specific features.\" (Jovchelovitch & Bauer 2000, p.4) To this end, it is a form of the [[Open Interview]], which - compared to [[Semi-structured Interview|Semi-structured]] and [[Survey|Structured Interviews]] - is relatively free from deductively pre-developed question schemata: \"To elicit a less imposed and therefore more 'valid' rendering of the informant's perspective, the influence of the interviewer should be minimal (...) '''The [Narrative Interview] goes further than any other interview method in avoiding pre-structuring the interview.''' (...) It uses a specific type of everyday communication, namely story-telling and listening, to reach this objective.\" (Jovchelovitch & Bauer 2000, p.4) Here, it is central that the interviewer appreciate the interviewee's perspective, by using the subject's language and by posing \"as someone who knows nothing or very little about the story being told, and who has no particular interests related to it\" (ibid, p.5). The interview is then transcribed and analyzed using different forms of coding (see Content Analysis), with a focus on the narrative elements. For more information on the methodological foundations of conducting and analyzing narrative Interviews, please refer to Jovchelovitch & Bauer 2000.\n\n==== Narrative Inquiry as collaborative story-creation ====\nIn a third understanding of Narrative Research, which is most commonly referred to as 'Narrative Inquiry', 'narratives' are more than the underlying phenomenon - they are a central methodological element. ''''This approach dissolves the barrier between researcher and subject further, and the collaboration between both is central to the methodological design''' (Clandinin 2006, Moen 2006). This type of narrative research does not apply an 'outsider's perspective', but instead is \"(...) collaboration between researcher and participants, over time, in a place or series of places, and in social interaction with milieus. An inquirer enters this matrix in the midst and progresses in the same spirit, concluding the inquiry still in the midst of living and telling, reliving and retelling, the stories of the experiences that made up people's lives, both individual and social.\" (Clandinin 2006, p.20, citing Clandinin & Connelly 2000). To this end, Clandinin (2006) distinguishes between the re-telling of stories that Interview participants tell the researchers, and the telling of stories that the researchers experience themselves, e.g. in ethnographic studies. The difference is not so much of methodological nature as it is in the purpose and perspective of the research (Barrett & Stauffer 2009). In this understanding, 'Narrative Inquiry' is a reflexive and iterative process, with the researchers entering into a field of experiences, telling their own stories, telling the stories of other participants, and dialogically co-creating joined stories with them (Moen 2006). The created narratives serve as a way of presenting the research experiences, but also as forms of data for the analysis of the joint and conveyed experiences.","[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Narrative Research describes qualitative field research based on narrative formats which are analyzed and\/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|'''SCOPUS hits per year for Narrative Research until 2020.''' Search terms: 'Narrative Research', 'narrative inquiry', 'narrative analysis' in Title, Abstract, Keywords. Source: own.]]\n'''[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years''' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). 'Storytelling' is defined as the distinct tonality, format and presentation in which a story is told. The term 'narrative' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term 'narrative' has been used used predominantly as a synonym for 'story' in academia for decades (Barrett & Stauffer 2009).\n\n'''Psychologist Jerome Bruner introduced the notion of 'narrative' as being one of two forms of distinct modes of thinking in 1984''' - the other being the 'logico-scientific' mode (Barrett & Stauffer 2009). While the latter is \"(...) more concerned with establishing universal truth conditions\" (Barrett & Stauffer 2009, p.9), the 'narrative' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that 'narratives' are a central form of human learning about - and [[Glossary|sense-making]] of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.\n\n'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).\n\n== What the method does ==\n'''First, there is a distinction to be made:''' 'Narrative' can refer to a form of Science Communication, in which research findings are presented in a story format (as opposed to classical representation of data) but not extended through new insights. 'Narrative' can also be understood as a form of scientific inquiry, generating new knowledge during its application. This entry will focus on the latter understanding.\n\n'''Next, it should be noted that Narrative Research entails different approaches''', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.\n\nNarrative Research -  or 'Narrative Inquiry' - is shaped by and focussing on a conceptual understanding of 'narratives' (Barrett & Stauffer 2009, p.15). Here, 'narratives' are seen as a format of [[Glossary|communication]] that people apply to make sense of their life experiences. \"Communities, social groups, and subcultures tell stories with words and meanings that are specific to their experience and way of life. The lexicon of a social group constitutes its perspective on the world, and it is assumed that narrations preserve particular perspectives in a more genuine form\" (Jovchelovitch & Bauer 2000, p.2). '''Narratives are therefore not merely forms of representing a chain of events, but a way of making sense of what is happening.''' Through the telling of a story, people link events in meaning. The elements that are conveyed in the story, and the way these are conveyed, indicates how the story-teller - and\/or their social surrounding - sees the world. They are a form of putting reality into cultural and individual perspective. Also, narratives are never final but change over time as new events arise and perspectives develop (Jovchelovitch & Bauer 2000, Webster & Mertova 2007, Squire et al. 2014, Moen 2006).","In terms of practical methodology, this form of narrative inquiry is very closely related to methods of [[Ethnography]], which are based on the active [[Glossary|participation]] in the field, i.e. the social situation of interest, and the creation of [[:Category:Qualitative|qualitative]] data in form of field notes. The distinctive component of Narrative Inquiry is the focus on narratives. Clandinin (2006, p.47f) explains the methodological approach as following: \"As we enter into narrative inquiry relationships, we (...) negotiate relationships, research purposes, transitions, as well as how we are going to be useful in those relationships. These negotiations occur moment by moment, within each encounter, sometimes in ways that we are not awake to\" or \"in intentional, wide awake ways as we work with our participants throughout the inquiry. As we live in the field with our participants, whether the field is a classroom, a hospital room or a meeting place where stories are told, we begin to compose (...) a range of kinds of field texts from photographs, field notes, and conversation transcripts to Interview transcripts. As narrative inquirers work with participants we need to be open to the myriad of imaginative possibilities for composing field texts. (...)  As we continue to negotiate our relationships with participants, at some points, we do leave the field to begin to compose research texts. This leaving of the field and a return to the field may occur and reoccur as there is a fluidity and recursiveness as inquirers compose research texts, negotiate them with participants, compose further field texts and recompose research texts.\" Data may be gathered in form of \"field notes; journal records; interview transcripts; one's own and other's observations; storytelling; letter writing; autobiographical writing; documents (...); and pictures\" (Moen 2006, p.61) and analyzed by forms of Content Analysis.\n\n== Strengths & Challenges ==\n* Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000)\n* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research \"(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates\". Further, \"Narrative inquiry attempts to capture the 'whole story', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important 'intervening' stages\" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).\n* '''The insights gained through narratives are subjective to the narrator, which implies advantages and challenges.''' Compared to an 'objective' description of, e.g. a chain of events, the narration provides insights about the individual's interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: \"Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived.\" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the 'real' world. Squire et al. (2014) refer to this distinction as different forms of 'truth' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical 'truth'. They should try \"(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather.\" Moen (2006, p.63), by comparison, explains that \"(...) there is no static and everlasting truth\", anyway.\n* This [[Bias and Critical Thinking|conflict between different 'truths']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, 'usefulness' and 'persuasiveness' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: \"Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable' (...) Narrative research does not claim to represent the exact 'truth', but rather aims for 'verisimilitude'\". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a 'trustworthy' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one's research results (see Moen 2006, p.64). Also, researchers \"(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts\" (Polkinghorne 2007, p.484).\n* Further challenges may arise during the active collaboration of the researcher in the field. For example, \"(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher\" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]]."],"39":["[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution.","'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes\/no (1\/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n<\/imagemap>\n\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to \"rule out\", the answer to this question is 'yes'.\n\n\n= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n<\/imagemap>\n\n'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n<\/imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as 'discrete' data.\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. Abundance data is referred to as 'integer' in R, i.e. it exists in full numbers, and continuous data is 'numeric' - it has a comma.\n\n\n== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n<\/imagemap>\n\n'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?","[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions."],"40":["[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (\u03bb) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to \u03bb. The Lamda, \u03bb, also represents the mean and variance of the distribution.\n\n==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, \u03bb, its average value is \u03bb.\n\n2. Variance of the distribution is also \u03bb, implying when \u03bb increases the width of the distribution goes as square root lambda \u221a\u03bb.\n\n3. A converse in Raikov\u2019s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.\n\nConsider the example of radioactive decay for long-lived isotopes, in a radioactive sample containing a large number of nuclei, each of which has a tiny probability of decaying during some time interval, T. Let\u2019s say the rate of decay is 0.31 decay\/second and is monitored for 10 seconds. That gives us the \u03bb1 0.31 * 10 = 3.1 which means the probability equals,\n[[File:3 equation.PNG|center]]\nConsider another example where \u03bb2 is 2.7, the probability is\n[[File:4 equation.PNG|center]]\n\nTherefore,\n'''If we look at the total number k = k1 + k2 of a radioactive decay in a time T, the result is also a Poisson distribution with \u03bb = \u03bb1 + \u03bb2 -> \u03bb = 3.1 + 2.7 = 5.8 : P(k, \u03bb1 + \u03bb2 )'''\n[[File:5 equation.PNG|center]]\n\n''If we have two independent Poisson-distributed variables, their sum is Poisson distributed too.''\n\n4. The skewness is measured by 1\/\u221a\u03bb\n\n5. Excess kurtosis is measured by 1\/\u03bb. See the difference between excess kurtosis and kurtosis [https:\/\/www.investopedia.com\/terms\/e\/excesskurtosis.asp here]. In a nutshell, excess kurtosis compares the kurtosis of the distribution with the kurtosis of a normal distribution and can therefore tell you if an (extreme) event is more or less likely in this case than if the distribution followed a normal distribution.\n\n6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> \u221e and \u03bb = np, rate parameter, \u03bb is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean \u03bb > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = \u03bb = \u03bc =45\n\nStandard deviation = \u2202 = \u221a\u03bb = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https:\/\/builtin.com\/data-science\/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=\"Python\" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n<\/syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=\"Python\" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n<\/syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1","<syntaxhighlight lang=\"Python\" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align='center')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n<\/syntaxhighlight>\n[[File: plot 1.png|center|700px]]\n\nThe resulting pmf confirms that the closest integer value to lamba i.e., 3 has the most number of years out of 10,000 meaning most years will have 3 duststorms. You can also see that the data is slightly skewed to the right since there is a larger variance to the right than to the left. Looking at the distribution, it looks fairly normally distributed. However, the low lambda does not allow to use the Poisson distribution as an approximation for a normal distribution. Most probably, the large dataset allows us to see it as a normal distribution, since most distributions converge to a normal distribution with increasing sample size.\n==6. References==\n* Brownlee, Jason: \"A Gentle Introduction to Probability Distributions\", 14.11.2019. Retrieved from: https:\/\/machinelearningmastery.com\/what-are-probability-distributions\/#:~:text=A%20probability%20distribution%20is%20a,properties%20that%20can%20be%20measured, last checked: 21.05.2023\n* Koehrsen, Will: \" The Poisson Process and Poisson Distribution Explained (With Meteors!), 28.10.2022. Retrieved from: https:\/\/builtin.com\/data-science\/poisson-process, last checked: 21.05.2023\n* Papoulis, Athanasios; Pillai, S. Unnikrishna: \"Probability, Random Variables, and Stochastic Processes\" (4th ed.).\n* Raikov, Dmitry (1937): \"On the decomposition of Poisson laws\". Comptes Rendus de l'Acad\u00e9mie des Sciences de l'URSS. 14: 9\u201311.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart","THIS ARTICLE IS STILL IN EDITING MODE\n==1. Introduction to Probability Distribution==\nA probability distribution summarizes the probabilities for the values of a random variable. The general properties\/features that define a distribution include the four mathematical moments: \n\n1.\t'''Expected value E(X)''': This is the outcome with the highest probability or likelihood or an average or mean value for the variable X.\n \n2.\t'''Variance Var(X)''': Variance denotes the spread of the values from the expected value. The standard deviation is the normalized value of variance obtained by taking a square root of the variance. The covariance summarizes the linear relationship for how the two variables change with respect to each other. \n\n3.\t'''Skewness''': Skewness is the measure of asymmetry of a random variable X about the mean E(X) of a probability distribution. It can be positive (tail on the right side), negative (tail on left side), zero (balanced tail on both sides) or undefined. Zero skewness does not always mean symmetric distribution as one tail can be long and thin and the other can be short and fat. \na\n4.\t'''Kurtosis''': Kurtosis is the measure of \u2018 *peaked-ness* \u2019 of distribution. It can be differentiating tool between distributions that have the same mean and variance. Higher kurtosis means a peak and fatter\/extended tails. \n\nEach random variable has its own probability distribution that may have a similar shape to other variables, however, the structure will differ based on whether the variable is discrete or continuous, since probability distributions of continuous variables have an infinite number of values and probability functions of discrete variables assign a probability to each concrete data point. \n\n==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=\"wikitable\"\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success\/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}\n\nPoisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https:\/\/studiousguy.com\/poisson-distribution-examples\/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===\n\n''The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time.''\n\n[[File:1 equation.PNG|center|250px]]\nT = Time interval \ne = natural logarithmic base \nk = number of events \nThe K! means K factorial. This means that we multiply all integers from K down to 1. Say K is 4 then K!= 4* 3* 2* 1= 24\n\nIntroducing lambda, \u03bb, as a rate parameter (events\/time)*T into the equation gives us \n[[File:2 equation.PNG|center]]\n\nSo, \u03bb is basically the expected number of event occurrences in an interval, a function of '''number of events''' and the '''time interval'''. Changing \u03bb means changing the probability of event occurrence in one interval.\n\nFor example, Ladislaus Bortkiewicz calculated the deaths of soldiers by horse kicks in the Prussian Army using Poisson Distribution in the late 1800s. He analyzed two decades of data from up to 10 army corps, equivalent to two centuries of observations for one army corps.\n[[File:Horse.png|center|500px]]\nFigure 1: Poisson Distribution for deaths by horse kick by Ladislaus Bortkiewicz. Source: [https:\/\/www.scribbr.com\/statistics\/poisson-distribution\/#:~:text=You%20can%20use%20a%20Poisson,days%20or%205%20square%20inches. Scribbr]\n\nAccording to his observation, an average of 0.61 soldiers died every year. However, the deaths were random, for example, in one year four soldiers died and for most years no deaths were caused by horses. Figure 1 shows the probability mass function graph.\nIn Poisson Distribution terms,\n* Death caused by a horse kick is an \u2018event\u2019\n* Mean per time interval, represented by \u03bb, is 0.61\n* The number of deaths by horse kick in a specific year is k.\n\n[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (\u03bb) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to \u03bb. The Lamda, \u03bb, also represents the mean and variance of the distribution."],"41":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to ''25 minutes = 1 \"Pomodoro\"'').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15\u201330 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Pomodoro_Technique Pomodoro Technique]\n* [https:\/\/lifehacker.com\/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https:\/\/www.youtube.com\/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https:\/\/play.google.com\/store\/apps\/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https:\/\/apps.apple.com\/us\/app\/focus-keeper-time-management\/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique''' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe '''Dreamers'''...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: ''Which ideas come to mind? What would be an ideal solution to the problem?''\n\nThe '''Realists'''...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: ''How does the idea feel? How could it be implemented? Who should do it and at what cost?''\n\nThe '''Critics'''...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: ''What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?''\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. '''The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.''' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to 'neutralize' their minds) in order to understand the other roles' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He\/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n''Sources:''\n* Tools Hero - [https:\/\/www.toolshero.com\/creativity\/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https:\/\/arbeitdigital.de\/wirtschaftslexikon\/kreativitaetstechniken\/walt-disney-methode\/ Walt-Disney-Methode]\n* Karrierebibel - [https:\/\/karrierebibel.de\/disney-methode\/ Disney Methode: Denkblockaden \u00fcberwinden]\n* Impulse - [https:\/\/www.impulse.de\/management\/selbstmanagement-erfolg\/walt-disney-methode\/3831387.html Walt Disney Methode]\n\n[https:\/\/www.youtube.com\/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br\/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nMiro is an online collaboration tool which allows you to hold virtual workshops, develop common ideas and designs, have a digital workspace or design presentations in a different way.\n\n== Goals ==\n* Work better together online\n* Integrate many different functions (project management, online collaboration, presentation) into one\n\n== Getting started ==\nMiro is an online-tools which allows you to collaborate with others (but also work by yourself). '''It is basically an infinite canvas onto which you can put all kinds of elements:''' shapes, text, videos, documents, interactive elements such as a [[Kanban]] Board or sticky notes. As collaboration happens in real-time, it can very much substitute the classical whiteboard or brown-paper and allow you to collaboratively brainstorm ideas, design a presentation, or manage your team.\n\nWe recommend using it for digital workshops, organizing your teamwork or as an alternative to traditional presentation formats such as Prezi, PowerPoint or Keynote.\n\nIt can be a bit overwhelming at first, but once you get the hang of it, it becomes really natural to use and hopefully will make your teamwork more productive and fun.\n\nTo get started, create an account on [https:\/\/miro.com the Miro website]. If you are eligible (e.g. when you're a member of Leuphana University), you can apply for a free Miro Education account which comes with all the premium features [https:\/\/miro.com\/education-whiteboard\/ here].\n\n== Links & Further Reading ==\nHere's a video that covers (almost) everything you need to know to get started working with Miro:\n{{#evu:https:\/\/www.youtube.com\/watch?v=pULLAEmhSho\n|alignment=center\n}}\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."],"42":["The question of model reduction will preoccupy statistics for the next decades, and this development will be interacting with a further rise of Bayes theorem and other questions related to information processing. Time will tell how regressions will emerge on the other side, yet it is undeniable that there is a use case for this specific type of statistical model. Whether science will become better in terms of the theoretical foundations of regressions, in recognising and communicating the restrictions and flaws of regressions, and not overplaying their hand when it comes to the creation of knowledge, is an altogether different story. \n\n\n== Key Publications ==\n\n== References ==\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden and Quentin Lehrer.","== Ordinations ==\n\nOrdination techniques evolved already more than a century ago in mathematics, and allowed fro a reduction of information that makes these analysis approaches timely up until today. Ordination techniques rely strongly on a profound knowledge of the underlying data format of the respective dataset that is being analysed. Since ordination allow for both inductive and deductive analysis, they often pose a risk for beginners, who typically get confused by the diversity of approaches and the answers these analysis may provide. This conduction is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques. What is more, ordinations are often deeply entangled in disciplinary cultures, with some approaches such as factor analysis being almost exclusive to some disciplines, and other approaches such as principal component analysis being utilised in quite diverse ways within different disciplines. This makes norms and rules when and how to apply different techniques widely scattered and intertwined with disciplinary norms, while the same disciplines are widely ignorant about other approach from different disciplines. Here, we try to diver a diverse and reflected overview of the different techniques, and what their respective strengths and weaknesses are. This will necessary demand a certain simplification, and will in addition trigger controversy within certain branches of science, and these controversies are either rooted in partial knowledge or in experiential  identity. Unboxing the whole landscape of ordinations is also a struggle because these analysis are neither discrete nor always conclusive. Instead they pose starting points that often serve initial analysis, or alternatively enable analysis widely uncoupled from the vast landscape of univariate statistics. We need to acknowledge to this end that there is a vast difference between the diverse approaches not only in the underlying mathematics, yet also how these may be partly ignored. This is probably the hardest struggle that you can fin neither in textbooks nor in articles. The empirical reality is that many applications of ordinations violate much of the mathematical assumptions or rules, yet the patterns derived from these analyses are still helpful if not even valid. Mathematicians can choose to live in a world where much of ordination techniques is perfect in every way, yet the datasets the world gives to ordinations are simply not. Instead, we have to acknowledge that multivariate data is almost always messy, contains a high amount of noise, many redundancies, and even data errors. Safety comes in numbers. Ordinations are so powerful exactly because they can channel all these problems through the safety of the size of the data, and thus derive either initial analysis or even results that serve as endpoints. However, there is a difference between initial or final results, and this will be our first starting point here.\n\nOrdination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through experience can you pick what serves your dataset best. This is associated to the reality that many analysis made with ordinations are indeed compromises. Ecology and psychology are two examples of disciplines why imagined ordinations deeply enough into the statistical designs to derive datasets where more often than not assumptions for statistical analysis of a respective ordination are met. However, many analyses based on ordinations are indeed compromises, and from a mathematical standpoint are real world analysis based on ordinations a graveyard of mathematical assumptions, and violation of analytical foundations that borderline ethical misconduct. In other words, much of ordinations are messy. This is especially true because ordinations are indeed revealing mostly continuous results in the form of location on ordination axes. While multivariate analyis based on cluster analysis are hence more discrete through the results being presented as groups, ordinations are typically nice to graphically inspect, but harder to analytical embedded into a wider framework. More on this point later. Let us now begin with a presentation of the diverse ordination types and their respective origins. \n\n=== Correspondence analysis ===\n\nThis ordination is one of the most original ordination techniques, and builds form its underlying mechanics on the principal component analysis. However, since it is based on the chi square test, it is mainly applied for categorical data, although it can also be applied to count data, given that the dataset contains enough statistical power for this. In a nutshell, the correspondence analysis creates orthogonal axis that represent a dimension reduction of the input data, thereby effectively reducing the multivariate categorical data into artificial exes, out of which the first contains the most explanatory power. Typically, the second and third axis contain still meaningful information, yet for most datasets the first two axis may suffice. The correspondence analysis is today mostly negotiable in terms of its direct application, yet serves as an important basis for other approaches, such as the Detrended Correspondence analysis or the Canonical Correspondence analysis. This is also partly related to the largest flaw in the Correspondence analysis, namely the so called Arch-effect, where information on the first two axis is skewed due to mathematical representation of the data. Still, the underlying calculation, mainly the reciprocal averaging approach make it stand out as a powerful tool to sort large multivariate datasets based on categorical or count data. Consequently, the basic reciprocal averaging was initially very relevant for scientific disciplines such as ecology and psychology. \n\n=== Detrended Correspondence analysis ===","Like I mentioned before, the number of PC created is equal to the number of input variables (in this cases, seven). Looking at the plot, the first two PCs combined can explain more than 90% of the dataset, an amazing number. This means this 7-dimensional dataset can be presented on a 2-dimensional space, and we still only lose less than 10% of the information. In other words, the first two PCs are the most important, and we can discard the other ones.\n\nNext, we look at the contribution of the original variables to the building of the two PCs, respectively.\n\n<syntaxhighlight lang=\"R\" line>\nfviz_contrib(data.pca, choice = \"var\", axes = 1)\nfviz_contrib(data.pca, choice = \"var\", axes = 2)\n<\/syntaxhighlight>\n\n[[File: PCA_ContribPlot.png|center|500px]]\n\nThe red, dashed line refers to the case where all of the variable contribute equally. In the left plot, ash, fat, sodium and carbohydrates contribute substantially to the forming of the first PC. On the other hand, moisture and calories influence the second PC heavily, as seen in the right plot.\n\nFinally, we look at the biplot of the analysis. This reveals the underlying patterns of the dataset:\n\n<syntaxhighlight lang=\"R\" line>\nfviz_pca_biplot(data.pca, label = \"var\", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n<\/syntaxhighlight>\n\n[[File: PCA_BiPlot.png|center|500px]]\n\nThe two axes of this plot are the newly created PCs. There are two main part of information presented on the plot:\n\n* The arrows show how the original variables correlate with the two PCs, and in turn, with each others. For example, from the way the moisture arrow presents itself we can infer a strong negative correlation of the variable and the second PC. Fat and sodium have a very strong positive correlation, and the more carbohydrates a pizza contains, the less protein it has. Adding argument <code>label = \"var\"<\/code> in the function allows for the variable names to be printed.\n* The points show how each individual pizza is plotted on this new coordinate system. Here, we go a step further and grouping those pizza under different brands (a categorical variable) using the arguement <code>habillage = data$brand<\/code>. By doing this, we unearth additional information about those brands. For example:\n** Brand A typically produce pizzas with a high level of fat and sodium.\n** Pizzas from brand B, C, and D are rich in proteins and ash, as opposed to pizza from brand E, F, G, H which are high in carbohydrates.\n** If you favorite pizza brand F goes out of business (for whatever reason), a pizza from brand E, G or H would be a good substitute in terms of nutritional value.\n\n=== R Example: Is standardization that important? ===\nTo answer this question, let us try an alternative scenario, where we conduct PCA without centering and scaling the variables.\n\n<syntaxhighlight lang=\"R\" line>\ndata.pca <- data %>% select(mois:cal) %>% prcomp(scale = FALSE)\nfviz_pca_biplot(data.pca, label = \"var\", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n<\/syntaxhighlight>\n\n[[File: PCA_BiPlotNoScale.png|center|500px]]\n\n[[File: PCA_ContribPlotNoScale.png|center|500px]]\n\nSuddenly all that matters are only the carbohydrate, moisture and fat level of the pizzas. Why is that the case?\n\nBy plotting the distribution of the original data using boxplots (on the left), we can see that the value range of data for the variables are vastly different. For example, the variable carbohydrates has much higher mean and variance compared to calories. By nature, PCA tries to form PCs where there is a widest spread in the data, so it will always prefer those variables with high \"absolute\" variance. It's like comparing 1000 milliseconds and 5 kilometers and putting more weight on the 1000 milliseconds because 1000 is bigger than 5.\n\n[[File: PCA_BoxPlot.png|center|700px]]\n\nThis is why standardization, or sometimes called feature scaling (scale data to mean 0 and standard deviation 1) is a crucial pre-processing step in many data analysis procedures and machine learning algorithms, including PCA. This allows the analysis to pay attention to all features equally, so that no variable dominates the others (equal importance).\n== Strengths & Weaknesses ==\n'''Strengths'''\n* Reduce complexity of data\n* Allows for concise visualization of main patterns in data\n* Remove correlated features\n* Enhance performance of algorithms\n* Reduce overfitting\n'''Weaknesses'''\n* Principle components are created based on linear assumptions\n* The created principle components are hard to interpret\n* Information loss through reduction of dimensionality (oftentimes acceptable)\n== Key Publications ==\n* Wold, S., Esbensen, K., & Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3), 37-52.\n\n* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150202.\n== See Also ==\n\n* [https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ&t A simple visual explanation of PCA] from StatQuest with Josh Starmer\n\n* [https:\/\/www.youtube.com\/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&index=4 An in-depth walkthrough of PCA] and its mathematical root with Victor Lavrenko\n----\n\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Ch\u00e2n L\u00ea."],"43":["<syntaxhighlight lang=\"Python\" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n<\/syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=\"Python\" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n<\/syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.info()\n<\/syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (\"non-null\") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the \"object\" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For \"sex\" and \"passed\", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n<\/syntaxhighlight> \n\n<syntaxhighlight lang=\"Python\" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n<\/syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"sex\") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= \"sex\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"group\", diag_kind=\"hist\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=\"Python\" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap='PuBu');\n<\/syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=\"Python\" line>\nmodel_1 = smf.ols(formula='points ~ ID + quanti', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).","#Now we can for example test, if the difference in sepal length between setosa and virginica is significant:\n#H0 hypothesis: The medians (distributions) of setosa and virginica are equal\n#H1 hypothesis: The medians (distributions) of setosa and virginica differ\n\n#test for normality\nshapiro.test(setosa$Sepal.Length)\nshapiro.test(virginica$Sepal.Length)\n#both are normally distributed\n\n#wilcoxon sum of ranks test\nwilcox.test(setosa$Sepal.Length,virginica$Sepal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it's almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis.\n#Based on this result we may conclude the medians of these two distributions differ. \n#The alternative hypothesis is stated as the \u201ctrue location shift is not equal to 0\u201d. \n#That\u2019s another way of saying \u201cthe distribution of one population is shifted to the left or \n#right of the other,\u201d which implies different medians. \n\n<\/syntaxhighlight>\n|}\n\n\n====f-test====\n'''The f-test allows you to compare the ''variance'' of two samples.''' Variance is calculated by taking the average of squared deviations from the mean and tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean. If the p-value of the f-test is lower than 0,05, the variances differ significantly. Important: for the f-Test, the data of the samples has to be normally distributed. \n\n'''Example''': If you examine players in a basketball and a hockey team, you would expect their heights to be different on average. But maybe the variance is not. Consider Figure 1 where the mean is different, but the variance the same - this could be the case for your hockey and basketball team. In contrast, the height could be distributed as shown in Figure 2. The f-test then would probably yield a p-value below 0,05.\n\n[[File:Normal distribution.jpg|400px|thumb|left|Figure 1 shows '''two datasets which are normally distributed, but shifted.''' Source: [https:\/\/snappygoat.com\/s\/?q=bestof%3ALda-gauss-variance-small.svg+en+Plot+of+two+normal+distributed+variables+with+small+variance+de+Plot+zweier+Normalverteilter+Variablen+mit+kleiner+Varianz#7c28e0e4295882f103325762899f736091eab855,0,3 snappy goat]]]\n[[File:NormalDistribution2.png|400px|thumb|right|Figure 2 '''shows two datasets that are normally distributed, but have different variances'''. Source: [https:\/\/www.notion.so\/sustainabilitymethods\/Simple-Statistical-Tests-bcc0055304d44564bc41661453423134#7d57d8c251f94da8974eeb8a658aaa29 Wikimedia]]]\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the f-Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an f-Test.\n#We will compare the variances of height of two fictive populations. First, we create two vectors with the command 'rnorm'. Using rnorm, you can decide how many values your vector should contain, besides the mean and the standard deviation of the vector. To learn, what else you can do with rnorm, type:\n?rnorm\n\n#Creating two vectors\nPopA=rnorm(40, mean=175, sd=1)\nPopB=rnorm(40, mean=182, sd=2)\n\n#Comparing them visually by creating histograms\nhist(PopA)\nhist(PopB)\n\n#Conducting a f-test to compare the variances\nvar.test(PopA, PopB) \n\n#And this is the result, telliing you that the two variances differ significantly\nF test to compare two variances\n\ndata:  PopA and PopB\nF = 0.38584, num df = 39, denom df = 39, p-value = 0.00371\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2040711 0.7295171\nsample estimates:\nratio of variances \n         0.3858411 \n<\/syntaxhighlight>\n|}\n\n\n== Normativity & Future of Simple Tests ==\n'''Simple tests are not abundantly applied these days in scientific research, and often seem outdated.''' Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion.\n\nHowever, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics. \n\nSimple tests are often the endpoint of many introductionary teachings on statistics, which is unfortunate. Overall, their lack in most of recent publications as well as wooden design frames of these approaches make these tests an undesirable starting point for many students, yet they are a vital stepping stone to more advanced models.\n\nHopefully, one day school children will learn simple test, because they could, and the world would be all the better for it. If more people early on would learn about probability, and simple tests are a stepping stone on this long road, there would be an education deeper rooted in data and analysis, allowing for better choices and understanding of citizens.\n\n\n== Key Publications ==\n* Student\" William Sealy Gosset. 1908. ''The probable error of a mean.'' Biometrika 6 (1). 1\u201325.\n* Cochran, William G. 1952. ''The Chi-square Test of Goodness of Fit''. The Annals of Mathematical Statistics 23 (3). 315\u2013345. \n* Box, G. E. P. 1953. ''Non-Normality and Tests on Variances.'' Biometrika 40 (3\/4). 318\u2013335.","Lastly, we need to make some checks to see if our model is statistically sound. We will check if the residuals are normally distributed, for heteroscedasticity, and serial correlation. \nTo check for normal distribution of the residuals, we take a look at the QQ-Plot to look at the distribution of the residuals. It should follow the shape of the red line roughly, to assume normal distribution of residuals. The QQ Plot compares the theoretical quantiles of the normal distribution with the residual quantiles. If the distributions are perfectly equal, meaning the residuals are perfectly normally distributed, the points will be perfectly on the line. You can find out more about QQ-Plots here: https:\/\/sustainabilitymethods.org\/index.php\/Data_distribution#The_QQ-Plot\n<syntaxhighlight lang=\"Python\" line>\nresiduals = model.resid\nfig = sm.qqplot(residuals, scipy.stats.t, fit=True, line=\"45\")\nplt.show()\n<\/syntaxhighlight>\n\nThis can also be tested using the Jarque-Bera test. A Jarque-Bera test compares the kurtosis und skewness of the distribution of your variable with the properties a normal distribution has.\nThe lower the value of the Jarque-Bera test, the more likely the residuals are normally distributed. If the p-value is above your chosen significance level (e.g., 0.05), you can assume that your residuals are normally distributed.\n<syntaxhighlight lang=\"Python\" line>\njarque_bera(df[\"CGPA\"])\n<\/syntaxhighlight>\n\nAs you can see, the value of the Jarque-Bera test is quite small and the p-value is even above 0.1. It can therefore not be said that the residuals do not follow a normal distribution.\nInstead, we assume that your variable CGPA follows a normal distribution. Note that you cannot do the test for binary variables since the Jarque-Bera test assumes that the distribution of residuals is continuous.\n\nNext, we should test for heteroscedasticity. In our regression model, we have assumed homoscedasticity which means that the variance of the residuals is equally distributed. The residuals are the difference between your observations from your predictions.\nIf this is not the case, you have heteroscedasticity. This is often the case because of outliers or skewness in the distribution of a variable. You can assess this visually by plotting the variance of your residuals against an independent variable. Again here, this only makes sense for continuous variables, which is why we look at GCPA.\n\n<syntaxhighlight lang=\"Python\" line>\n# Calculate the residuals\nresiduals = model.resid\n\n# Calculate the squared residuals ( to only have positive values)\nsquared_resid = residuals ** 2\n\n# Group the squared residuals by the values of each independent variable\ngrouped_resid = squared_resid.groupby(x['CGPA'])\n\n# Calculate the variance of the squared residuals for each group\nvar_resid = grouped_resid.var()\n\n# Plot the variance of the squared residuals against the values of each independent variable\nplt.scatter(var_resid.index, var_resid.values)\nplt.xlabel('CGPA')\nplt.ylabel('Variance of Squared Residuals')\nplt.show()\n<\/syntaxhighlight>\n\nWe can see that there are some outliers that might cause heteroscedasticity. We can also check this with the Breusch-Pagan test. \nIf the p-value is lower than your chosen significance level (e.g., 0.05), we need to reject the null hypothesis that we have homoscedasticity.\nWe would then treat the model to be heteroscedastic.\n\n<syntaxhighlight lang=\"Python\" line>\n##create the multiple regression model\nx = df[[\"CGPA\"]]\nx = sm.add_constant(x) \ny = df[\"Chance of Admit \"]\n\nmodel = sm.OLS(y, x).fit()\n\n# perform the Breusch-Pagan test\nbp_test = het_breuschpagan(model.resid, model.model.exog)\n\n# print the results\nprint(\"Breusch-Pagan test p-value:\", bp_test[1])\n<\/syntaxhighlight>\n\nThe test shows that we need to reject the assumption of homoscedasticity. We assume heteroscedasticity. \nBefore we adapt the model, we should also check for serial correlation.\nSerial correlation is the case in which error terms across time or observations are correlated (in our case across observations).\nIf we look at our regression output again, we can check for this using the Durbin-Watson test statistic. Generally speaking, if the \nvalue is around 2, there is no serial correlation, if it is 0, there is a perfect positive serial correlation, if it is 4, there is a perfect\nnegative correlation. The exact values for the number of independent variables and observations you can find [https:\/\/real-statistics.com\/statistics-tables\/durbin-watson-table\/ here]. Our Durbin-Watson\ntest value is 0.839 and we therefore have reason to assume a positive serial correlation. We therefore need to correct for this telling \nthat there is serial correlation (covariance type of H3).\n\n## Now, we need to correct for heteroscedasticity and serial correlation\n<syntaxhighlight lang=\"Python\" line>\nx = df[[\"CGPA\", \"Research\"]]\nx = sm.add_constant(x) \ny = df[\"Chance of Admit \"]\n\nmodel = sm.OLS(y, x)\n\nnw_model = model.fit(cov_type='HAC', cov_kwds={'maxlags': 20})\n\n# print the results\nprint(nw_model.summary())\n<\/syntaxhighlight>\n\nHAC corrects for heteroscedastic and the additional information \"cov_kwds={'maxlags': 20)\" corrects for serial correlation. \nAs a rule of thumb for our data, you can set the lags to half of your dataset size or the square root. In our case, this makes \nno difference. You can check it out. If you have the case of serial correlation in another dataset (especially if you\nhave time series data), you might have to perform other analytical tasks to get the correction right. [https:\/\/towardsdatascience.com\/advanced-time-series-analysis-in-python-decomposition-autocorrelation-115aa64f475e Here] is a good start for that.\nAs you can see in the results, nothing has really changed. Even though this seems odd and it seemed like a lot of unnecessary work,\nit is important to do these diagnostic checks and correct your model if needed. In another regression model, you might\nget completely different results after e.g., correcting for heteroscedasticity."],"44":["Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n* A detailed description of how to conduct Scenario Planning, explained through case studies in the advertisement industry.\n\nAmer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n* A (rather complex) overview on different types of Scenario Planning across the literature.\n\nSwart, R.J., Raskin, P., Robinson, J. 2004. ''The problem of the future: sustainability science and scenario analysis.'' Global Environmental Change 14(2). 137-146.\n* A conceptual paper that elaborates on the potential of scenarios in and for sustainability science.\n\n\n== References ==\n(1) Wack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n\n(2) Schoemaker, P.J.H. 1993. ''Multiple Scenario Development: Its Conceptual and Behavioral Foundation.'' Strategic Management Journal 14(3). 193-213.\n\n(3) Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n\n(4) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design Journal 20(1). 1916-1926.\n\n(5) Amer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n\n(6) Wiek et al. 2006. ''Functions of scenarios in transition processes.'' Futures 38(7). 740-766.\n\n(7)  Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n\n== Further Information ==\n* Shell works with Scenarios still today. [[On this websitehttps:\/\/www.shell.com\/energy-and-innovation\/the-energy-future\/scenarios.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL3NjZW5hcmlvcy5odG1s|On this website]], you find a lot of information about their scenario approach.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.","[[File:ConceptVisualisationScenarioPlanning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Scenario Planning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Scenario Planning is a systematic designation of potential futures to enable long term strategic planning.\n\n==Background==\n[[File:Scenario planning.png|400px|thumb|right|'''SCOPUS hits per year for Scenario Planning until 2019.''' Search terms: 'scenario planning', 'scenario construction', 'scenario-based', 'scenario study' in Title, Abstract, Keywords. Source: own.]]\n\n'''The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s'''. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were \"(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]\" in the US. (Amer et al. 2013, p.24).\n\n'''Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.''' At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.\n\n\n== What the method does ==\nScenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called \"scenarios\". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them \"(...) in coherent script-like or narrative fashion.\" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all \"fundamentally different\" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios \"(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties.\" (Schoemaker 1993, p.196).\n\n'''There is no ''one'' procedure for Scenario Planning''' (5). A commonly cited approach by Schoemaker (2, 3) includes the following steps:<br>\n1) Definition of time frame, scope, decision variables and major actors of the issue in question.\n\n2) Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. \n\n3) Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes.\n\n4) Elimination of implausible or impossible futures and, based on the themes that emerged from these, the creation of new scenarios. This process is repeatedly done until internally consistent scenarios are found. The number of scenarios developed depends on the scope and purpose of the planning process (1, 5).\n\n[[File:Scenario Planning Process.png|800px|thumb|center|'''Evaluation of the number of scenarios developed.''' Source: Amer et al. 2013. p.33]]\n\n5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.","5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.\n\n[[File:Scenario Planning Example.png|800px|thumb|center|'''Exemplary scenarios for life in Australian cities.''' Source: Gaziulusoy & Ryan 2017, p.1920]]\n\n\n== Strengths & Challenges ==\n* Scenario Planning allows for any organization that deploys it to be more innovative, flexible and thus better prepared for unforeseen disruptions and changes. In a corporate context, this can reduce costs, provide market benefits and improve internal communication (3, 5).\n* Scenario Planning broadens the structural perspective of an actor to think about the future (5). For example, an oil company may well be able to assess risks in their technical processes of oil exploration and extraction, but only through a more detailed scenario analysis they may be enabled to include economic, political and societal trends into their planning (2).\n* Scenarios are psychologically attractive. They are a way of transforming seemingly disparate data into relatable, coherent narratives. They present uncertainty across scenarios instead of providing probabilistic information for all elements within each individual one. In addition, they reduce the complexity and uncertainty of the future into graspable states (2).\n* Scenario Planning differs from adjacent methodological approaches. While a ''scenario'' illustrates a possible state of the future, a ''vision'' (see [[Visioning & Backcasting]] revolves around a desirable state of the future without taking its likelihood into consideration. Compared to Visioning, Scenario Planning might therefore be more useful for actual [[Glossary|decision-making]] but might as well be too narrow to envision holistic systemic changes (6). Additionally, a ''prediction'' as the classical method of economic ''forecasting'' describes likely states of the future as an extension of current developments, without the openness for [[Glossary|change]] that is inherent to Scenario Planning. \n\n'''Good scenarios should fulfill a range of characteristics''' (3, 5):\n* they need to be plausible and internally consistent, i.e. capable of happening.\n* they should be relevant, i.e. of help for decision making and connected to the issue that is to be solved.\n* they should be archetypal, i.e. not represent variations on the same theme but describe distinct futures.\n* they should describe a future that is in a state in which \"the system might exist for some length of time, as opposed to being highly transient.\" (Schoemaker 1995, p.30)\n* they should challenge the existent way of thinking about the future.\n* while Scenario Planning generally permits actors to broaden their perspective, this only works if the construction of scenarios is not biased, which easily happens (2, 3). One may unconsciously look for confirming evidence for personal presuppositions when identifying trends and uncertainties. Also, overconfidence that certain trends will (not) prevail may distort one's assessment. This should be paid attention to during the process (3). As Schoemaker (1995, p.38) puts it: \"When contemplating the future, it is useful to consider three classes of knowledge: 1. Things we know we know. 2. Things we know we don't know. 3. Things we don't know we don't know. Various biases (...) plague all three, but the greatest havoc is caused by the third.\" Ignorance in terms of the future should be acknowledged and addressed in order to challenge biases. \"And this is where scenario planning excels, since it is essentially a study of our collective ignorance.\" (ibid).\n\n[[File:Scenario Planning They Believed It.png|450px|thumb|right|'''Throughout history, smart minds have underestimated technological, economic and political developments.''' Scenario Planning can be a mean of addressing the overconfidence in thinking that things will not change. Source: Schoemaker 1995, p.26]]\n\n\n== Normativity ==\n==== Connectedness ====\n* Scenario Planning is connected to various other methodological approaches. First, it is based on a [[System Thinking and Causal Loop Diagrams|System Thinking]] approach, recognizing the interconnectedness and causal interference of elements within a system.\n* To gather a good understanding of the relevant stakeholders and trends, useful approaches are [[Social Network Analysis]] or [[Stakeholder Analysis]]. To support the analysis of how systemic variables interact with each other, [[Agent Based Modelling]] may be applied.\n* Scenario Planning may be done after a [[Visioning & Backcasting|Visioning]] process: after envisioning what is desirable, stakeholders may think about what is actually realistic (4).\n* Scenario Planning is to some extent comparable to the [[Delphi]], where experts share their opinions on the future of an issue and come to a joint prediction. However, Scenario Planning differs in that it attempts to develop several complex scenarios instead of reaching one single (often quantitative) result to a question."],"45":["[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n|'''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/>\n\n'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]\n\n'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions.\" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|'''Moreno's original work on Social Networks.''' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno's and Jennings' work was subsequently taken up and furthered as the field of ''''group dynamics', which was highly relevant in the US in the 1950s and 1960s.''' Simultaneously, sociologists and anthropologists further developed the approach in Britain. \"The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties.\" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n\"Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm.\" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that \"(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. '''Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.\"''' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as \"entitites\", \"actors\", \"vertices\" or \"agents\" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are \"(...) tied to one another through socially meaningful relations\" (Prell et al. 2009, p.503), which can be \"(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection\" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.\n\n[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]","[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations \"(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns.\" (Prell et al. 2009, p.503). '''Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as \"dyads\", but rather the study of patterns within a network.''' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors' behavior emerges from the interaction between them: \"Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions.\" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent's actions (Borgatti et al. 2009)","==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* '''Network boundaries:''' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** ''position-based approach'': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** ''event-based'' approach: those who had participated in key events are believed to define the population\n** ''relation-based approach'': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the ''exogenously defined boundaries'', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* '''Type of ties:''' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: ''directed'' ties, which go from one node to another, and ''undirected ties'', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, \"(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact.\" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* '''Data Collection''': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a ''prompted recall'' approach, they are asked which people they would think of with regards to a specific topic (e.g. \"To whom would you go for advice at work?\") while they are shown a pre-determined list of potentially relevant individuals. In the ''free list'' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the '''visual representation''' of the network. For this, the researcher \"(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections.\" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the \"number of lines which it is necessary to traverse in order to get from one point to another.\" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|'''Different network structures.''' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|'''An exemplary network structure.''' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]\n\n== Strengths & Challenges ==\n* There is a range of challenges in the gathering of network data through [[Semi-structured Interview|Interviews]] and [[Survey|Surveys]], which can become long and cumbersome, and in which the interviewees may differently understand and recall their relations with other actors, or misinterpret the connections between other actors. (Marin & Wellman 2010, p.15)\n* The definition of network boundaries is crucial, since \"(...) the inappropriate inclusion or exclusion of a small number of entities can have ramifications which extend well beyond those entities themselves\". Apart from the excluded entities and their relations, all relations between these entities and the rest of the network, and thus the network's structural properties, are affected. (Butts 2008). For more insights on the topic of System Boundaries, please refer to [[System Boundaries|the respective article]]."],"46":["'''Note:''' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nThis entry aims to introduce Stacked Area Plot and its visualization using R\u2019s <syntaxhighlight lang=\"R\" inline>ggplot2<\/syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> has the following syntax:\n\n'''Syntax''': <syntaxhighlight lang=\"R\" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()<\/syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=\"R\" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n<\/syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=\"R\" inline>\"uspopage\"<\/syntaxhighlight> using the function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> from the <syntaxhighlight lang=\"R\" inline>ggplot2 package<\/syntaxhighlight>:\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n<\/syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of \u201cBlues\u201d, and add labels.\n\n<syntaxhighlight lang=\"R\" line>\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")+\n  labs(title = \"US Population by Age\", \n       subtitle = \"Between 1900 and 2000\",\n       x = \"Year\",\n       y = \"Population (Thousands)\")\n<\/syntaxhighlight>\n\n==References==\n\n* R Graphics Cookbook, 2nd edition by Winston Chang\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Maria Jose Machuca.","'''In short:''' Stacked bar plots show the quantitative relationship that exists between a main category and its subcategories. This entry helps visualise two different types of stacked bar plots, ''Simple Stacked Plots'' and ''Proportions Stacked Plots'', and explains the difference between them. For more on the basics of barplots, please refer to the [[Barplots, Histograms and Boxplots]] entry.\n\n\n==Stacked Barplots: Proportions vs. Absolute Values==\nStacked bar plots show the quantitative relationship that exists between a main category and its subcategories. Each bar represents a principal category and it is divided into segments representing subcategories of a second categorical variable. The chart shows not only the quantitative relationship between the different subcategories with each other but also with the main category as a whole. They are also used to show how the composition of the subcategories changes over time.\n\nStacked bar plots should be used for Comparisons and Proportions but with emphasis on Composition. This composition analysis can be static for a certain moment in time, or dynamic for a determined period of time.\n\nStacked bar Plots are two-dimensional with two axes: one axis shows categories, the other axis shows numerical values. The axis where the categories are indicated does not have a scale (*) to highlight that it refers to discrete (mutually exclusive) groups. The axis with numerical values must have a scale with its corresponding measurements units.\n\n\n===When you should use a stacked bar plot===\nThe main objective of a standard bar chart is to compare numeric values between levels of a categorical variable. One bar is plotted for each level of the categorical variable, each bar\u2019s length indicating numeric value. A stacked bar chart also achieves this objective, but also targets a second goal.\n\nWe want to move to a stacked bar chart when we care about the relative decomposition of each primary bar based on the levels of a second categorical variable. Each bar is now comprised of a number of sub-bars, each one corresponding with a level of a secondary categorical variable. The total length of each stacked bar is the same as before, but now we can see how the secondary groups contributed to that total.\n\n\n==Two types of Stacked Barplots==\n1. '''Simple Stacked Plots'''\u00a0place the\u00a0'''absolute value'''\u00a0of each subcategory after or over the previous one. The numerical axis has a scale of numerical values. The graph shows the absolute value of each subcategory and the sum of these values indicates the total for the category. Usually, the principal bars have different final heights or lengths.\n   \nWe use simple stacked plots when relative and absolute differences matter. Ideal for comparing the total amounts across each group\/segmented bar.\n\n[[File:Simple_stacked_barplot.png|400px|frameless|right]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecie <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(specie,condition,value)\n \n# Stacked\nggplot(data, aes(fill=condition, y=value, x=specie)) + \n    geom_bar(position=\"stack\", stat=\"identity\")\n<\/syntaxhighlight>\n\n2. '''Proportions Stacked Plots''' place the '''percentage''' of each subcategory after or over the previous one. The numerical axis has a scale of percentage figures. The graph shows the percentage of each segment referred to the total of the category. All the principal bars have the same height.\n\nIn proportions stacked plots the emphasis is on the percentage composition of each subcategory since the totals by category are not shown; in other words, they are used when the key message is the percentage of composition and not the total within the categories. We use proportions stacked plots only when relative differences matter.\n\n[[File:Proportions_stacked_barplot.png|400px|frameless|left]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecies <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(species,condition,value)\n \n# Stacked + percent\nggplot(data, aes(fill=condition, y=value, x=species)) + \n    geom_bar(position=\"fill\", stat=\"identity\")\n<\/syntaxhighlight>\n\n----\n[[Category:Statistics]] [[Category:R examples]]","circle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]\ncircle 622 812 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER and categorical BRAND]]\ncircle 733 759 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color), POPULATION SIZE (bubble size)]]\ncircle 782 875 67 [[Big problems for later|Factor analysis]]\ncircle 1271 825 67 [[Big problems for later|Structural Equation Plot]]\ncircle 1394 771 67 [[Big problems for later|Ordination, e.g. numeric and categorical variables (AGE, INCOME, HEIGHT, PROFESSION) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\n<\/imagemap>"],"47":["[[File:Thought Experiment Concept Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Thought Experiments]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.\n\n== Background ==\n[[File:Scopus hits Thought Experiment.png|450px|thumb|right|'''SCOPUS hits per year for Thought Experiments until 2019.''' Search term: 'thought experiment' in Title, Abstract, Keywords. Source: own.]]\n\n'''The Thought Experiment may well be the oldest scientific method.''' The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the ''What if'' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early [[History of Methods|enlightenment]]. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the ''Origin of Species'' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term \"Thought Experiment\", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.\n\n== What the method does ==\nThought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.  \n\nThe easiest way to set up a Thought Experiment is to ask yourself a \"What if\" question. Many Thought Experiments resolve around decisions, choices or potential states of the future. A famous example is the Trolley experiment, where a train rolls towards five train track workers, who would all be killed be the oncoming train, unaware of their potential demise. You can now change the direction of the train, and lead it to another track. There, one worker would be killed. Uncountable numbers of variations of this experiment exist (see Further Information), as it can teach much about choice, ethics, and responsibility. For instance, many people would change the train direction, but hardly anyone would push a person onto the track to derail the train. This would save the other five, and the outcome would be the same. However the difference between pushing a lever or pushing a person has deep psychological ramifications that resolve around guilt. This exemplifies that the Thought Experiment does not necessarily have a ''best'' outcome, as the outcome depends - in this example - on your moral choices. Some might argue that you should hurl yourself onto the track to stop the train, thereby not changing the countable outcome, but performing a deeply altruistic act that saves everybody else. Most people would probably be unable to do this. \n\n'''Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world.''' Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schr\u00f6dinger's Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences.","[[File:Schr\u00f6dingers Cat.png|400px|thumb|left|'''Schr\u00f6dinger's Cat.''' Source: [https:\/\/ad4group.com\/schrodingers-cat-and-the-marketing-strategy\/ AD4Group].]]\n\nSo far, we see that there are Thought Experiments that resolve exclusively about a - subjective - human decision, and other types of Thought Experiments that are designed around setting in the physical world. The difference between these two is in the design of the experiment itself, as the first are always focused on the normative decisions or people, while the second focuses on our normative interpretation of anticipation of a design that is without a human influence. This distinction is already helpful, yet another dimension is about time. Many Thought Experiments are independent of time, while others try to reinterpret the past to make assumptions about a future about which we have no experience. Thought Experiments that focus on re-interpretation of the past (\"What if the assassination of Franz Ferdinands failed? Would the first World War still have happened?\") look for alternative pathways of history, often to understand the historical context and - more impotantly - the consequences of this context better. Most Thought Experiments are independent of a longer time scale. These experiments - such as the Trolley experiment - look at a very close future, and are often either very constructed or lack a connection to a specific context. Thought Experiments that focus on the future are often build around utopian or at least currently unthinkable examples that question the status quo, either form an ethical, societal, cultural or any other perspective. Such desirable futures are deeply normative yet can build an important bridge to our current reality through backcasting (\"What if there is no more poverty, and how can we achieve this?\"). \n\nAll this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https:\/\/en.wikipedia.org\/wiki\/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. '''Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.'''\n\n== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people's actions or decision, and the ''What if'' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. '''Schr\u00f6dinger's Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.''' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment \"What if I have an unlimited amount of ice cream?\" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the \"unlimited\", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.\n\n== Normativity ==\nThought experiments are as we have already learned as normative as are our assumptions about the world. Hume argued that Thought Experiments are based on the laws of nature, yet here I would disagree. While many famous Thought Experiments are about the physical world, others are only made up in our minds. Many Thought Experiments are downright impossible to be matched with our reality, and are even explicitly designed to do this, such as Thomas Nagels Encyclopaedia of the Universe. It is therefore important to realise that basically all Thought Experiments are in essence normative, and one could say even downright subjective. Building on Derek Parfit, I would however propose a different interpretation, and propose that we should not measure the normativity of Thought Experiments through the design and setting, but instead by their outcome. Many people might come to the same conclusions within a given Thought Experiment, and some conclusion drawn from Thought Experiments may matter more than others. Consequently the penultimate question - also for Derek Parfit - is whether there are some Thought Experiments that are not normative. \n\n== Outlook ==\nMuch of [[Glossary|art]] and the media can be seen as a Thought Experiment, and there are ample examples that Thought Experiments in the media and the arts triggered or supported severe societal transformations. Thought Experiments are of equal importance in ethics and physics, and the bridge-building of the methodological approach should not be overestimated. Examples from the past prove that Thought Experiments can enable a great epistemological flexibility and diversity. This flexibility is even so large that Thought Experiments serve as a bridge between the epistemological and the ontological, or in other words between everything we know - and how we know it - and everything we believe. By enabling the transformation of our own most individual thoughts, Thought Experiments may provide a boat or a bridge to link the metaphysical with the world of knowledge.\n\n== Key Publications ==\nParfit, D. (2011). ''On what matters.'' Oxford University Press.\n\nParfit, D. (1984). ''Reasons and persons''. OUP Oxford.\n\nKamm, F. M. (2015). The trolley problem mysteries. Oxford University Press.","'''Note:''' The German version of this entry can be found here: [[Experiments and Hypothesis Testing (German)]]<br\/>\n\n'''Note:''' This entry is a brief introduction to experiments. For more details, please refer to the entries on [[Experiments]], [[Case studies and Natural experiments]] as well as [[Field experiments]].\n\n'''In short:''' This entry introduces you to the concept of hypotheses and how to test them systematically.\n\n[[File:Concept visualisation - Experiments.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Experiments]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/>\n\n== Hypothesis building ==\n[[File:James Lind, A Treatise on the Scurvy, 1757 Wellcome M0013130.jpg|thumb|James Lind's \"A Treatise on Scurvy\", in which he systematically assessed what's most useful healing scurvy.]]\n\n'''Life is full of hypotheses.''' We are all puzzled by an [https:\/\/www.youtube.com\/watch?v=HZ9xZHWY0mw endless flow of questions]. We constantly test our surrounding for potential ''what ifs'', cluttering our mind, yet more often is our cluttered head unable to produce any generalizable knowledge. Still, our brain helps us filter our environment, also by constantly testing questions - or hypotheses - to enable us to better cope with the challenges we face. It is not the really the big questions that keep us pre-occupied every day, but these small questions keep us going.\n \nDid you ever pick the wrong food in a restaurant? Then your brain writes a little note to itself, and next time you pick something else. This enables us to find our niche in life. However, at some point, [https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0092867408009537 people started observing patterns in their environment] and communicated these to other people. Collaboration and communication were our means to become communities and much later, societies. This dates back to before the dawn of humans and can be observed for other animals as well. Within our development, it became the backbone of our modern civilization \u2013 for better or worse - once we tested our questions systematically. \nJames Lind, for example, conducted one of these [https:\/\/journals.sagepub.com\/doi\/pdf\/10.1177\/014107680309601201 first systematic approaches] at sea and managed to find a cure for the disease scurvy, which caused many problems to sailors in the 18th century.\n \n==== Hypothesis Testing ====\n[[File:Bildschirmfoto 2020-01-17 um 11.18.08.png|thumb|left|Here you can see one example for both the H0 and the H1.]]\n\nSpecific questions rooted in an initial idea, a theory, were thus tested repeatedly. Or perhaps it's better to say these hypotheses were tested with replicates and the influences disturbing our observation were kept constant. This is called <strong><i>testing of hypothesis<\/i><\/strong>, the repeated systematic investigation of a preconceived theory, where during the testing ideally everything is kept constant except for what is being investigated. While some call this the \"[https:\/\/www.youtube.com\/watch?v=ptADSmJCVwQ scientific method],\" Many researchers are not particularly fond of this label, as it implies that it is THE [https:\/\/www.khanacademy.org\/science\/high-school-biology\/hs-biology-foundations\/hs-biology-and-the-scientific-method\/a\/the-science-of-biology scientific method], which is certainly not true. Still, the systematic [https:\/\/www.youtube.com\/watch?v=ZzeXCKd5a18 testing of the hypothesis] and the knowledge process that resulted from it was certainly a scientific revolution (for more information, [[History_of_Methods|click here]]). \n\nWhat is important regarding this knowledge is the notion that a confirmed hypothesis indicates that something is true, but other previously unknown factors may falsify it later. This is why hypothesis testing allows us to approximate knowledge, many would argue, with a certain confidence, but [https:\/\/www.brgdomath.com\/philosophie\/erkenntnistheorie-tk10\/falsifikation-popper\/ we will never truly know something]. While this seems a bit confusing, a prominent example may help illustrate this. Before discovering Australia, all swans known in Europe were white. With the discovery of Australia, and to the surprise of Natural History researchers at the time, black swans were discovered. Hence before that, everybody would have hypothesized: [https:\/\/www.youtube.com\/watch?v=wf-sGqBsWv4 all swans are white]. Then Australia was discovered, and the world changed, or at least, the knowledge of Western cultures about swans."],"48":["The autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df['usage_kwh'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15\/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel('Lag (hours)') ## title x axis\nplt.ylabel('Autocorrelation of electricity usage with confidence interval') ## title y axis\nplt.title('') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n<\/syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days<\/small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered \u201cclose\u201d to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object's neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object's neighborhood is significantly lower than the densities of its neighbors' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don't know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df['usage_kwh'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (\"to_numpy()\") and reshapes it to a single column (\"reshape (-1, 1)\"), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf['outlier'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df['outlier']] ## creates  a column where only outliers are selected\n\nplt.plot(df['usage_kwh']) ## plots the dataframe\nplt.scatter(x=outliers.index, y=outliers['usage_kwh'], color='tab:red', marker='x') ## creates a scatter plot with the outliers, marked with a red x\nplt.title('Outliers in the dataset (marked in red)') ## title of the plot\nplt.xlabel('Date') ## titlex axis\nplt.ylabel('Usage (KWh)')## title y axis\nplt.show()\n<\/syntaxhighlight>\n[[File:outlier plot.png|700px|center|]]\n<small>Figure 7: Outlier in the chosen part of the dataset (marked with red cross)<\/small>\n\nIn this case, these are probably false-positive outliers. The dataset is already pretty clean and does likely not contain many outliers.\n\nThe imputation of missing values is the next challenge after detecting outliers. The naive solution to this problem is to replace a missing point with the mean of all points. While this approach will not skew the data distribution significantly, the imputed values might be far off from the actual value and make messy graphics. An alternative is applying regression models that use predictive models to estimate missing points.\n\nA combined approach to both detects and replaces outliers is the Kalman Filter, which predicts new points based on the previous points and estimates a noise measure for new points. Thus, when an anomaly is detected, it is replaced by the prediction from the model using historical data (Hoogendoorn & Funk 2018).\n\n===Forecasting Time Series Data===\n\nForecasting time series data can be of tremendous value in information gain. Since this is a large topic, this article will only touch on one method for forecasting: AutoRegressive Integrated Moving Average models, or ARIMA for short. It consists of three parts:","=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n<\/syntaxhighlight>\n\n\nThe scatter plot matrix from this package is already very nice by default. It splits the plot into an upper, lower and diagonal part. The upper part consists of the correlation coefficients for the different variables. The red stars show you the results of the implemented correlation test. There is a range from zero to three stars and the higher the number of stars, the higher is the significance of the results for the test. In the diagonal part of the plot are histograms for every variable and show you the distribution of the variable. The bivariate scatter plots can be found on the lower part of the plot and contain a fitted line by default.\n\n\n== Line chart ==\n=== Definition ===\nA line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value.  \n\nMultiple categories can be displayed in the same view, each represented by a unique line. Sometimes a point (circle\/dot) is also used to substantiate the visibility of individual values. The lines used in a line chart will generally be straight. However, sometimes curved line interpolation may be used as a method of estimating values between known data points. This approach can be useful to help emphasise a general trend. While this might slightly compromise the visual accuracy of discrete values if you already have approximations, this will have less impact.\n\n=== R Code ===\nWe will first plot a basic line chart based on a built-in dataset called <syntaxhighlight lang=\"R\" inline>EuStockMarkets<\/syntaxhighlight>. The data set contains data on the closing stock prices of different European stock indices over the years 1991 to 1998.\n\nTo make things easier, we will first transform the built-in dataset into a data frame object. Then, we will use that data frame to create the plot.\n\nThe table that contains information about the different market indices looks like this:\n\n{| class=\"wikitable\"\n|-\n! DAX !! SMI !! CAC !! FTSE\n|-\n| 1628.75|| 1678.1 || 1772.8 || 2443.6\n|-\n| 1613.63|| 1688.5 || 1750.5 || 2460.2\n|-\n| 1606.51|| 1678.6 || 1718.0 || 2448.2\n|-\n| ... || ... || ... || ...\n|}\n[[File:Simple line chart.png|350px|thumb|right|Fig.3]]\nHere, the data for all the columns are numeric.\n\nThe following line chart shows how the <syntaxhighlight lang=\"R\" inline>DAX<\/syntaxhighlight> index from the table from previous section.\n\n<syntaxhighlight lang=\"R\" line>\n# Fig.3\n#read the data as a data frame\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX,  # simply select a stock index\n     type='l')       # choose 'l' for line chart\n<\/syntaxhighlight>\n\n[[File:Line chart.png|350px|thumb|right|Fig.4]]\nAs you can see, the plot is very simple. We can enhance the way this plot looks by making a few tweaks, making it more informative and aesthetically pleasing.\n\n<syntaxhighlight lang=\"R\">\n# Fig.4\n# get the data\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX, # select the data\n     type='l',      # choose 'l' for line chart\n     col='blue',    # choose the color of the line\n     lwd = 2,       # choose the line width \n     main = 'Line Chart of DAX Index (1991-1998)',         # title of the plot\n     xlab = 'Time (1991 to 1998)', ylab = 'Prices in EUR') # x- and y-axis labels\n<\/syntaxhighlight>\n\nYou can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot<\/syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n<\/syntaxhighlight>","THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https:\/\/www.kaggle.com\/datasets\/jaganadhg\/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=\"Python\" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n<\/syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=\"Python\" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir('\/path\/to\/your\/directory')\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndf = pd.read_csv('D202.csv')\ndf.head()\n<\/syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let's choose the most relevant columns for our research:\n\n<syntaxhighlight lang=\"Python\" line>\n## Let's choose the most relevant columns for our research:\ndf['start_date'] = pd.to_datetime(df['DATE'] + ' ' + df['START TIME'])\ndf['cost_dollars'] = df['COST'].apply(lambda x: float(x[1:]))\ndf.rename(columns={'USAGE': 'usage_kwh'}, inplace=True)\ndf = df.drop(columns=['TYPE', 'UNITS', 'DATE', 'START TIME', 'END TIME', 'NOTES', 'COST']).set_index('start_date')\n<\/syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe \u201ccost_dollars\u201d by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don't specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=\"Python\" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n<\/syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https:\/\/lg4ml.org\/grounddetection\/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=\"Python\" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y='usage_kwh',\n        title='Usage of Electricity over 2 Weeks',\n        labels={'start_date': 'Date', 'usage_kwh': 'Usage (KWh)'}) ## uses the data from \"start_date\" called \"Date\", and the data of \"usage_kwh\" called \"usage (KwH)\"\n<\/syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time<\/small>\n\n<syntaxhighlight lang=\"Python\" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df['usage_kwh'])) ## transforms into frequencies\nfrequencies = rfftfreq(df['usage_kwh'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)"],"49":["To illustrate the choice of methods in transdisciplinary research, how the selected methods can be combined, and how they relate to agency, see one of the following examples.\n\n\n== Examples ==\n[[File:Besatzfisch.png|450px|thumb|center|The research project \"Besatzfisch\". [http:\/\/besatz-fisch.de\/content\/view\/90\/86\/lang,german\/  Source]]]\n* The research project [http:\/\/besatz-fisch.de\/content\/view\/34\/57\/lang,german\/ \"Besatzfisch\"] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to '''understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.''' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (''Cooperation''). Second, anglers were questioned about fish population sizes and their economic implications (''Consultation'') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (''Consultation''). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (''Empowerment''). Fifth, social-ecological models were developed based on the previous empirical results. (''Consultation'') Sixth, the project results are published in different forms of media for different target groups (''Information'').\n\n* Another interesting example is the article [https:\/\/www.ecologyandsociety.org\/vol23\/iss2\/art9\/ \"Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau\"]. In order to '''assess potential future social-ecological developments in Mongolia''', researchers first held a workshop with stakeholders, mostly from academia but with diverse disciplinary backgrounds, and a few non-scientific stakeholders. In this workshop, first, key elements that influence sustainable development in the region were identified. Step by step, these were then ranked to identify critical uncertainties, which led to the development of potential future scenarios (''Consultation''). Also, the stakeholders' opinions on the workshop were later assessed through interviews, indicating positive impacts on their work and perspectives (''Empowerment''). The insights from the workshops were translated into system dynamics models by the researchers that were iteratively feedbacked by the stakeholders (''Consultation''), which led to a final model (''Information''). This approach was not purely transdisciplinary but illustrates a transdisciplinary workflow.\n\n* In the article \"[https:\/\/www.researchgate.net\/publication\/329789267_Developing_an_optimized_breeding_goal_for_Austrian_maternal_pig_breeds_using_a_participatory_approach Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach]\" the authors describe a '''participatory approach conducted in order to develop new indicators (traits) for animal health in pig breeding in Austria'''. First, propositions for new indicators to revise current policy were collected by the directors of the coalition of pig breeders, as well as in a science-led workshop with breeders and staff of the coalition (''Consultation''). The results were assessed based on a literature study and academic consultation. Then, the results were communicated back for feedback, tested on farms, and refined in further transdisciplinary workshops (''Consultation''). Next, pig breeders were asked and trained to record the new traits for one year (''Collaboration'') in a [[Citizen Science]]-like approach. This process showed to improve the quality of research data while generating new knowledge and skills for the breeders (''Empowerment''). The gathered data was feedbacked by the scientists and was set to lead to policy recommendations (''Information'').\n\n\n== Open questions and future directions ==\n[[File:Ivory Tower.jpg|300px|thumb|right|'''The proverbial Ivory Tower is challenged through transdisciplinary research.''' Source: [http:\/\/sciblogs.co.nz\/app\/uploads\/2017\/03\/academias-ivory-tower.jpg Sciblogs].]]\nThe future of transdisciplinary research is promising. However, major hurdles still need to be overcome. These include\n* the question how willing academia is to leave the proverbial 'ivory tower' and get in contact with political, economic and societal actors\n* the need for more experiences and practical guidance on how the aforementioned challenges in the conduction of transdisciplinary research can be solved (integration of knowledge, balancing of interests and methods, communication and mutual learning, joint development of solutions)\n* a lack of educational approaches that support transdisciplinary thinking and working\n\n\"As the world\u2019s natural resources dwindle and the climate changes, the need for sustainability research will increase\u2014as will the need for both integrated research and clear frameworks for conducting such research. Research on sustainability is vital to our collective interests and will require more and more collaboration across various boundaries. The more clearly we can articulate the bridges between those boundaries, the easier those novel collaborations will be.\" (Stock & Burton 2011, p.1106)\n\n\n== Key Publications ==\nLang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges.''\n\nDefila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\nBrandt et al. 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological Economics 92. 1-15.\n\nGAIA Special Episode ''Labs in the Real World - Advancing Transdisciplinarity and Transformations''. \n\nGibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\nWiek, A. and Lang D.J., 2016.Transformational Sustainability Research Methodology\u201d in Heinrichs, H. et al. (eds.), 2016. Sustainability Science, Dordrecht: Springer Netherlands. Available at: http:\/\/link.springer.com\/10.1007\/978-\u201094-\u2010017-\u20107242-\u201062.   \n\n\n== References ==\n* Lang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges''.\n\n* Kates et al. 2015. ''Sustainability Science''.","'''Integration, Reflexion und Normativit\u00e4t sind Komponenten der Methodologie, die sich langsam durchsetzen.''' W\u00e4hrend der Tiefenfokus wahrscheinlich die Hauptstrategie der meisten Forscher bleiben wird, verlangen die \"wicked problems\", mit denen wir derzeit konfrontiert sind, nach neuen Ans\u00e4tzen, um eine Transformation zu erm\u00f6glichen und die damit verbundenen Mechanismen und Strukturen zu untersuchen. Vieles ist noch unbekannt, und wir m\u00fcssen die Fokussierung auf Ressourcen, die Anspr\u00fcche von Wissenschaftlern, das Wissen anderer Wissenschaftler zu bewerten und zu beurteilen, und nicht zuletzt die tiefe Verankerung der wissenschaftlichen Disziplinen, wenn es um ihre unterschiedlichen Methoden geht, \u00fcberwinden. \n\nUm Nietzsche zu zitieren: \"Nie gab es eine so neue Morgenr\u00f6te und einen so klaren Horizont, und ein so offenes Meer.\"\n\n\n== Additional Information ==\n* Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to southern Transylvania''. Ecology and Society 19(4): 32.\nZeigt eine Kombination verschiedener Methoden in empirischer Forschung auf, darunter Scenario Planning, GIS, Causal-Loop Diagrams).\n\n* Schreier, M. & Odag, \u00d6. ''Mixed Methods.'' In: G. Mey K. Mruck (Hrsg.). ''Handbuch Qualitative Forschung in der Psychologie.'' VS Verlag f\u00fcr Sozialwissenschaften | Springer Fachmedien Wiesbaden GmbH 2010. 263-277.\nEine gute Einf\u00fchrung in das Mixed Methods-Konzept.\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.\n[[Category:Normativity_of_Methods]]","====4. Build on the available literature.====\n[[File:Caspar David Friedrich - Der Wanderer \u00fcber dem Nebelmeer.jpg|thumb|right|Although the \"Wander \u00fcber dem Nebelmeer\" does not literally stand on the shoulders of giants, the painting of Caspar David Friedrich is a good visualisation of the metaphor.]]\nNo case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new. The more you stand on the shoulders of giants, the further you can see. We need to realize that science is build on failure and iteration. Only by knowing about previous failures but also successes, and only by committing to the canon of knowledge, can we contribute and create new knowledge.\n\n\n==Natural experiments and statistics - a rough guide==\nSeveral layers of information are to be considered when linking statistics to natural experiments. Contextualising natural experiments can be structured into at least 4 steps.\n\n1) '''How is the case connected and embedded in the global context?'''\nWhat are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems. We hence acknowledge that this contextualisation is place based, and places have boundaries and borders. While it is way beyond this text to discuss this issue, the question how a place is bound can be a source of great confusion. A good rule of thumb from a statistical standpoint can be the idea that a place does only exists if its within variance, regarding central variables, is lower than the variance between the place and its neighbouring places.\n\n2) '''what are imbalances and dispersions within the case, both spatially and temporally?'''\nIn other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social context of the case's dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly. \n\n3) '''How are the livelihoods within the case?'''\nWhile many would connect this to the last point, it may be worthwhile to consider this on its own. Livelihoods are what defines groups of people and their possibility to thrive. While this is equally bound to [[Glossary|culture]] and justice, it repeatedly proves that it is much more than the mere sum of the two. Livelihoods are often characterised by a deeply qualitative manifestation, which is where statistics need be to aware of its limitations. While statistics can compile descriptive information on a community and tackle some of the interactions within a community, it has to fail considering other points. This limitation is important to realise.\n\n4) '''what is the intervention of the natural experiment within the case?'''\nHere, a whole world of interventions could be described. Instead, let us just settle on what was the active part of the researcher: systematically compare the state before the intervention with the state after the intervention.\n\n==Experiments in sustainability science==\n[[File:World 3 model.jpg|600px|thumb|right|'''The world 3 model was part of the \"Limits to Growth\" report of the Club of Rome.''' The picture creates a sense of the multitude and complexity of variables that influence a single case study in the real world.]]\nExperiments in sustainability science are characterised by an intervention and the production of empirical evidence. These two key criteria are essential for experiments in sustainability science, which can be in addition also differentiated into a problem focused and a solution orientated focus. Nevertheless, problem orientated studies with full control over all variables (except for the one(s) being investigated) are still clearly timely in sustainability science, as many experiments conducted in ecology prove. Less control over variables is equally important, and again ecology, agricultural science but also experiments in psychology come to mind. Problem orientated focus with no control over variables is at the heart of a joined problem framing of [[Glossary|transdisciplinary]] research in sustainability science, and such an approach also characterised the first Club of Rome report. Solution orientated focus with total or some control over variables is insofar a remarkable step since it marks a shift from the still dominating [[Glossary|paradigm]] of research where facts are only approximated. Since solutions may be compromises but can be achieved, a solution orientated research indicates a shift from more classical lines of thinking. The solution orientated approach with no control over variables marks the frontier of research, since it is clearly long term thinking. Prominent efforts are the IPBES, the sustainable development goal measures, and the current efforts of the rising number of transdisciplinary projects. While these are starting points, I consider these efforts to be a clear sign of hope, not only for out planet, but likewise for the development of science out of its dogmatic slumber.\n\n==Meta-analysis: Integrating cases towards holistic understanding==\nOut of the increasing number of scientific studies with a comparable design, an international school of thinking emerged: [http:\/\/meta-evidence.co.uk\/difference-systematic-review-meta-analysis\/ Meta-analysis]! Within meta-analytical frameworks, great care is taken to combine several studies to investigate their overall outcome. Rooted deeply in medicine and psychology, meta-analysis used the statistical power of studies that show a similar design and setting. Since some studies are larger while others build on smaller samples, meta-analysis are able to take some of these differences into account.\n\nIt is however important to understand the thinking behind meta-analysis, as it helps to create supra-knowledge about certain questions that were enough in the focus so that a number of integrable studies is available. Graphical overviews show summaries of effects, and so called random factors allow for the correction of different sample sizes and other factors. Beyond the disciplines where meta analysis were initially established, this line of thinking found its way into ecology, conservation, agriculture and many other arenas of science that investigate building on deductive designs. To this end, challenges often arise out of field experiments where less variables are being controlled for or are being understood, and hence comparisons may not be valid or at least less plausible."]},"context_precision":{"0":0.0059171598,"1":0.0988372093,"2":0.0555555556,"3":0.0,"4":0.0042194093,"5":0.063583815,"6":0.0411764706,"7":0.0480769231,"8":0.0051546392,"9":0.0550458716,"10":0.0040160643,"11":0.0446927374,"12":0.014354067,"13":0.0040816327,"14":0.0585106383,"15":0.0044247788,"16":0.0055248619,"17":0.0691823899,"18":0.0081967213,"19":0.1052631579,"20":0.0872483221,"21":0.0224719101,"22":0.0084745763,"23":0.1274509804,"24":0.0408163265,"25":0.0346820809,"26":0.0994764398,"27":0.0066666667,"28":0.0078740157,"29":0.0972222222,"30":0.0347222222,"31":0.1216216216,"32":0.018404908,"33":0.0304878049,"34":0.0793650794,"35":0.0384615385,"36":0.0051020408,"37":0.0227272727,"38":0.0957446809,"39":0.0055248619,"40":0.0107526882,"41":0.0647482014,"42":0.0,"43":0.0037593985,"44":0.0340136054,"45":0.0454545455,"46":0.0135135135,"47":0.0532544379,"48":0.0041841004,"49":0.0075757576},"context_recall":{"0":1.0,"1":0.8333333333,"2":1.0,"3":0.0,"4":0.5,"5":1.0,"6":1.0,"7":1.0,"8":1.0,"9":1.0,"10":0.5,"11":1.0,"12":1.0,"13":1.0,"14":0.0,"15":0.0,"16":0.0,"17":1.0,"18":0.5555555556,"19":1.0,"20":0.0,"21":1.0,"22":0.3333333333,"23":0.5,"24":0.3333333333,"25":0.0,"26":1.0,"27":0.0,"28":1.0,"29":0.0,"30":0.6666666667,"31":1.0,"32":1.0,"33":1.0,"34":0.0,"35":0.6666666667,"36":1.0,"37":1.0,"38":0.6,"39":0.0,"40":1.0,"41":0.8,"42":0.0,"43":0.8,"44":0.5555555556,"45":1.0,"46":0.75,"47":1.0,"48":0.0,"49":1.0}}