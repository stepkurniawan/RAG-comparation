{"question":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA a powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is the purpose of ANCOVA in statistical analysis?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram and how is it created?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"Reducing variance in field experiments or complex laboratory experiments","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA is used to compare group means while controlling for the effect of a covariate.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"contexts":{"0":["'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==A\/B Testing in a nutshell==\nA\/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A\/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A\/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n'''For example:'''\n\n\"To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.\"\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\n===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A\/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.\n\nThe sample size is determined by the percentage of users included in the test variants (control and treatment) and the duration of the experiment. As the experiment runs for a longer period of time, more visitors are exposed to the variants, resulting in an increase in the sample size. Because many external factors vary over time, it is important to randomize over time by running the control and treatment variants simultaneously at a fixed percentage throughout the experiment. Thereby the goal is to obtain adequate statistical power, where the statistical power of an experiment is the probability of detecting a particular effect if it exists. In practice, one can assign any percentages to the control and treatment, but 50% gives the experiment maximum statistical power.\n\nFurthermore, it is important to analyze only the subset of the population\/users that were potentially affected. For example, in an A\/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.\n\n==Execution Phase==\nThe execution phase involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan. During this phase, users are randomly assigned to the control or treatment group ensuring that the study is conducted in a controlled and unbiased manner.\n\n==Evaluation Phase==\nThe evaluation phase involves analyzing the data collected during the study and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about whether there was a statistical significant difference between the treatment group and the control group. One commonly used method is calculating the [[Designing_studies#P-value|p-value]] of the statistical test, or by using [https:\/\/www.youtube.com\/watch?v=9TDjifpGj-k Bayes' theorem] calculating the probability that the treatment had a positive effect based on the observed data and the prior beliefs about the treatment.","Depending on the type of data being collected different [[Simple Statistical Tests|statistical tests]] should be considered. For example, when dealing with discrete metrics such as click-through rate, the [https:\/\/mathworld.wolfram.com\/FishersExactTest.html Fisher exact test] can be used to calculate the exact p-value, while the [https:\/\/www.youtube.com\/watch?v=7_cs1YlZoug chi-squared test] may be more appropriate for larger sample sizes.\n\nIn the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https:\/\/data.library.virginia.edu\/the-wilcoxon-rank-sum-test\/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A\/B Testing==\n'''Advantages'''\nA\/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A\/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A\/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A\/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. \u201cOnline Controlled Experiments and A\/B Testing.\u201d Encyclopedia of Machine Learning and Data Mining, 2017, 922\u201329. https:\/\/doi.org\/10.1007\/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. \u201cExperimentation and Start-up Performance: Evidence from A\/B Testing.\u201d Management Science 68, no. 9 (September 2022): 6434\u201353. https:\/\/doi.org\/10.1287\/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A \/ B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart","'''In short:''' T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on two-sample T-tests and covers the concept and purpose of the t-test, underlying assumptions, its implementation in R, as well as multiple variants for different conditions. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]].\n\n==General Information==\nLet us start by looking at the basic idea behind a two-tailed two-sample t-test. Conceptually speaking we have two hypotheses. \n\nH<sub>0<\/sub>: Means between the two samples do not differ significantly.<br\/>\nH<sub>1<\/sub>: Means between the two samples do differ significantly. <br\/>\n\nIn mathematical terms (\u03bc<sub>1<\/sub> and \u03bc<sub>2<\/sub> denote the mean values of the two samples): \n\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> = \u03bc<sub>2<\/sub> <br>\nH<sub>1<\/sub>: \u03bc<sub>1<\/sub> \u2260 \u03bc<sub>2<\/sub>. \n\n[[File:Kurt_olaf.jpg|500px|frameless|right]]\n'''Here is an example to illustrate this.''' The farmers Kurt and Olaf grow sunflowers and wonder who has the bigger ones. So they each measure a total of 100 flowers and put the values into a data frame.\n\n<syntaxhighlight lang=\"R\" line>\n# Create a dataset\nset.seed(320)\n\nkurt <- rnorm(100, 60.5, 22)\nolaf <- rnorm(100, 63, 23)\n<\/syntaxhighlight>\n\nOur task is now to find out whether means values differ significantly between two groups.\n<syntaxhighlight lang=\"R\" line>\n# perform t-test\nt.test(kurt, olaf)\n\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.1275\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -11.97670   1.50973\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n<\/syntaxhighlight>\n\nSo now, we performed a t-test and got a result. '''But how can we interpret the output?''' <br\/>\nThe criterion to consult is the p-value. This value represents the probability of the data given that H0 is actually true. Hence, a low p-value indicates that the data is very unlikely if H0 applies. Therefore, one might reject this hypothesis (in favor of the alternative hypothesis H1) if the p-value turns out to be below a certain threshold (\u03b1, usually set prior testing), which is often set to 0.05 and usually not larger than 0.1. <br\/>\n\nIn this case, the p-value is greater than 0.1. Therefore, the probability of H0 is considered to be \u201ctoo large\u201d to reject this hypothesis. Hence, we conclude that means do not differ significantly, even though we can say that descriptively the sample mean of Olaf\u2019s flowers is higher.\n<br\/>\nThere are multiple options to fine-tune the t-test if one already has a concrete hypothesis in mind concerning the direction and\/or magnitude of the difference. In the first case, one might apply a one-tailed t-test. The hypotheses pairs would change accordingly to either of these: <br>\n\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> \u2265 \u03bc<sub>2<\/sub><br>\nH<sub>1<\/sub>: \u03bc<sub>1<\/sub> < \u03bc<sub>2<\/sub>\n<br>\nor\n<br>\nH<sub>0<\/sub>: \u03bc<sub>1<\/sub> \u2264 \u03bc<sub>2<\/sub>\n<br>H<sub>1<\/sub>: \u03bc<sub>1<\/sub> > \u03bc<sub>2<\/sub>\n\nNote that the hypotheses need to be mutually exclusive and H0 always contains some form of equality sign. In R, one-tailed testing is possible by setting '''alternative = \"greater\"''' or '''alternative = \"less\"'''. Maybe Olaf is the more experienced farmer so we have already have \u00e0 priori the hypothesis that his flowers are on average larger. This would refer to our alternative hypothesis. The code would change only slightly:\n\n<syntaxhighlight lang=\"R\" line>\nt.test(kurt, olaf, alternative = \"less\")\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.06373\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##       -Inf 0.4172054\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n<\/syntaxhighlight>"],"1":["====Analysis of Variance====\n'''The [https:\/\/www.investopedia.com\/terms\/a\/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment. \n\nThe general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.\n\n====Preconditions====\nRegarding the preconditions of the [https:\/\/www.youtube.com\/watch?v=oOuu8IBd-yo ANOVA], it is important to realize that the data should ideally be '''normally distributed''' on all levels, which however is often violated due to small sample sizes. Since a non-normal distribution may influence the outcome of the test, boxplots are a helpful visual aid, as these allow for a simple detection tool of non-normal distribution levels. \n\nEqually should ideally the variance be comparable across all levels, which is called '''[https:\/\/blog.minitab.com\/blog\/statistics-and-quality-data-analysis\/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia homoscedastic]'''. What is also important is the criteria of '''independence''', meaning that samples of factor levels should not influence each other. For this reason are for instance in ecological experiments plants typically planted in individual pots. In addition does the classical ANOVA assume a '''balanced design''', which means that all factor levels have an equal sample size. If some factor levels have less samples than others, this might pose interactions in terms of normals distribution and variance, but there is another effect at play. Larger sample sizes on one factor level may create a disbalance, where factor levels with larger samples pose a larger influence on the overall model result. \n\n====One way and two way ANOVA====\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https:\/\/www.thoughtco.com\/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a '''[https:\/\/www.statisticshowto.com\/post-hoc\/ posthoc test]'''. A prominent example is the [https:\/\/sciencing.com\/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https:\/\/www.statisticshowto.com\/post-hoc\/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.","====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong. \n\nFurthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the deductive world become intertwined, and this poses a risk for the validity of scientific results.\n\nFor more on the Analysis of Variance, please refer to the [[ANOVA]] entry.\n\n==Examples==\n[[File:Guinea pig computer.jpg|thumb|right|The tooth growth of guinea pigs is a good R data set to illustrate how the ANOVA works]]\n===Toothgrowth of guinea pigs===\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the ToothGrowth data set is about: ?ToothGrowth\n\n#The code is partly from boxplot help (?boxplot). If you like to know the meaning of the code below, you can look it up there\ndata(ToothGrowth)\n\n# to create a boxplot \nboxplot(len ~ dose, data = ToothGrowth,\n                           boxwex = 0.25, \n                           at = 1:3 - 0.2,\n                           subset = supp == \"VC\", col = \"yellow\",\n                           main = \"Guinea Pigs\u2019 Tooth Growth\",\n                           xlab = \"Vitamin C dose mg\",\n                           ylab = \"tooth length\",\n                           xlim = c(0.5, 3.5), ylim = c(0, 35), yaxs = \"i\")\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n                           boxwex = 0.25,\n                           at = 1:3 + 0.2,\n                           subset = supp == \"OJ\", col = \"orange\")\nlegend(2, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c (\"yellow\", \"orange\"))\n\n#to apply an ANOVA\nmodel1<-aov(len ~ dose*supp, data = ToothGrowth)\nsummary(model1) \n#Interaction is significant\n<\/syntaxhighlight>\n\n====Insect sprays====\n[[File:A man using RIPA insecticide to kill bedbugs Wellcome L0032188.jpg|thumb|right|To find out, which insectide works effictively, you can approach an ANOVA]]\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the InsectSprays data set is about: ?InsectSprays\ndata(InsectSprays)\nattach(InsectSprays)\ntapply(count, spray, length)\nboxplot(count~spray) \n# can you guess which sprays are effective by looking at the boxplot?\n# to find out which sprays differ significantly without applying many t-tests, you can use a postdoc test\nmodel2<-aov(count~spray)\nTukeyHSD(model2)\n# compare the results to the boxplot if you like\n<\/syntaxhighlight>\n\n==Balanced vs. unbalanced designs==\nThere is such a thing as a perfect statistical design, and then there is reality.\n\nStatistician often think in so called balanced designs, which indicate that the samples across several levels were sampled with the same intensity. Take three soil types, which were sampled for their agricultural yield in a ''mono crop''. Ideally, all soil types should be investigated with the same amount of samples. If we would have three soil types -clay, loam, and sand- we should not sample sand 100 times, and clay only 10 times. If we did, our knowledge about sandy soils would be much higher compared to clay soil. This does not only represent a problem when it comes to the general knowledge, but also creates statistical problems. \n\nFirst of all, the sandy soil would be represented much more in an analysis that does not or cannot take such an unbalanced sampling into account.","'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]"],"2":["== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin.\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future. \n\n\n== Key Publications ==\n* Bayes, T. 1997. LII. ''An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S''. Phil. Trans. R. Soc. 53 (1763). 370\u2013418.\n* Box, G. E. P., & Tiao, G. C. 1992. ''Bayesian Inference in Statistical Analysis.'' John Wiley & Sons, Inc.\n* de Finetti, B. 2017. ''Theory of Probability''. In A. Mach\u00ed & A. Smith (Eds.). ''Wiley Series in Probability and Statistics.'' John Wiley & Sons, Ltd.\n* Kruschke, J.K., Liddell, T.M. 2018. ''Bayesian data analysis for newcomers.'' Psychon Bull Rev 25. 155\u2013177.\n\n\n== References ==\n(1) Jeffreys, H. 1973. ''Scientific Inference''. Cambridge University Press.<br\/>\n(2) Spiegelhalter, D. 2019. ''The Art of Statistics: learning from data''. Penguin UK.<br\/>\n(3) Downey, A.B. 2013. ''Think Bayes: Bayesian statistics in Python''. O'Reilly Media, Inc.<br\/>\n(4) Donovan, T.M. & Mickey, R.M. 2019. ''Bayesian statistics for beginners: A step-by-step approach.'' Oxford University Press.<br\/>\n(5) Kurt, W. 2019. ''Bayesian Statistics the Fun Way: Understanding Statistics and Probability with Star Wars, LEGO, and Rubber Ducks.'' No Starch Press.\n\n\n== Further Information ==\n* [https:\/\/library.wur.nl\/frontis\/bayes\/03_o_hagan.pdfBayesian statistics: principles and benefits]\n* [https:\/\/www.youtube.com\/watch?v=HZGCoVF3YvM 3Blue1Brown: Bayes' Theorem]\n* [https:\/\/www.youtube.com\/watch?v=SrEmzdOT65s Basic Probability: Joint, Marginal, and Conditional Probability]\n* [https:\/\/www.youtube.com\/watch?v=9TDjifpGj-k Crash Course Statistics: Examples of Bayes' Theorem being applied.]\n* [https:\/\/www.youtube.com\/watch?v=TVq2ivVpZgQ Monty Hall Problem: D!NG]\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Prabesh Dhakal and Henrik von Wehrden.","This does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Key Publications ==\n* Gleick, J. (2011). Chaos: Making a new science. Open Road Media.\n* Rohde, Klaus. Nonequilibrium ecology. Cambridge University Press, 2006.\n* Kruschke, John. \"Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.\" (2014).\n* Hastie, T. & Tibshirani, R. 1986. ''Generalized Additive Models''. Statistical Science 1(3). 297-318.\n\n==External Links==\n====Articles====\n[https:\/\/science.sciencemag.org\/content\/sci\/199\/4335\/1302.full.pdf?casa_token=SJbEKSHs2gwAAAAA:iWho1AqZsznpL8Tt5vvaPHX2OVggZkP2NlUjEZ8I0avaMKTs6BlCMA7LGG0405x6l5LBY9hTAhag One of the classical papers] on non-equilibrium ecology\n[https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF00334469.pdf Non-equilibrium theory in ecology]<br>\n[https:\/\/www.jstor.org\/stable\/pdf\/1942636.pdf?casa_token=gYme29pwLTsAAAAA:eZBniNyPMJyFe5F7hkmy51EkBk3h0Bm6ap6nG2WWs8-n6EjuhJ16sDt5mJFXipvIIUBu9mzjI16EkLwCgMG70s-YayWTrlzAm63iX3iBk0zk-Mgk4g Classical account on equilibrium and non-equilibrium] dynamics in ecology<br>\n[https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-46709-2_6 A balanced view on rangelands and non equilibrium dynamics]<br>\n[https:\/\/esajournals.onlinelibrary.wiley.com\/doi\/pdfdirect\/10.1890\/11-0802.1 Non-equilibrium dynamics in rangelands]<br>\n[https:\/\/www.mdpi.com\/2079-8954\/7\/1\/4\/htm A view on complexity]<br>\n[https:\/\/plato.stanford.edu\/entries\/chaos\/ A deeper dive into chaos]<br>\n[https:\/\/theconversation.com\/explainer-what-is-chaos-theory-10620 A nice take on Chaos]<br>\n[https:\/\/en.wikipedia.org\/wiki\/Chaos:_Making_a_New_Science The most definite guide to chaos] Note the synergies to the emergence of sustainability science<br>\n[https:\/\/www.r-bloggers.com\/2019\/05\/bayesian-models-in-r-2\/ Some intro into Bayesian statistics] in R<br>\n[https:\/\/www.wnycstudios.org\/podcasts\/radiolab\/episodes\/91684-stochasticity Stochasticity] just for kicks.\n\n\n====Videos====\n[https:\/\/www.youtube.com\/watch?v=fDek6cYijxI Chaos]: The Veritasium explanation<br>\n[https:\/\/www.youtube.com\/watch?v=5zI9sG3pjVU Laminar flow] is of course more awesome<br>\n[https:\/\/www.youtube.com\/watch?v=ovJcsL7vyrk Random] is not random, as this equation proves<br>\n[https:\/\/www.youtube.com\/watch?v=HZGCoVF3YvM One more take on Bayes]<br>\n\n\n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|'''SCOPUS hits per year for Bayesian Inference until 2020.''' Search terms: 'Bayesian' in Title, Abstract, Keywords. Source: own.]]\n'''The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.''' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes' name only became associated with the method in the 1900s (3).\n\n'''The family of methods based on the concept of Bayesian analysis has risen the last 50 years''' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n'''Bayesian analysis relies on using probability figures as an expression of our beliefs about events.''' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is \"prior\", and the probability figure assigned to the prior is called \"prior probability\". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called \"posterior probability\", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|'''The probability distribution for prior, evidence, and posterior.''']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes' Theorem.\n\n==== Bayes' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes' theorem.\n\n'''Conditional Probability''' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n'''Joint Probability''', also called \"conjoint probability\", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n'''Marginal Probability''' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for \"Bayes theorem\" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n''Caution:'' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)"],"3":["What is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https:\/\/www.britannica.com\/science\/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.\n\n\n== The way forward ==\nIf we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodologial ramifications. '''I can interview individuals, yet this will hardly allow me to prove agency.''' Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy. \n\nThe investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored [[Bayesian inference|Bayesian]] statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging. \n\nThe whole globe - although not a closed system \u2013 can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. '''Emergence is a truly cross-scalar phenomenon.''' Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.\n[[File:Climate model.png|400px|thumb|right|'''Climate models are increasingly getting more accurate, but the complexity and emergence of the global climate system may never be fully understood.''' Source: [https:\/\/blogs.egu.eu\/geolog\/2018\/09\/19\/how-to-forecast-the-future-with-climate-models\/ European Geosciences Union]]]","However, the urge for novel methodologies is so strong right now that experience in existing methodologies is rejected altogether. I consider this to be a dangerous new dogma, and kindly ask future researchers to consider experience in empirical methodologies as an important basis for innovation. As I have clearly shown above, there can be more than just theoretical considerations why methodological innovation can build on experience in existing methodologies. Such experience should always include a recognition of its limitations, and in the long run there might be a greater recognition of new design approaches that shall drive methodological innovation.\n\nThe interconnectedness of different forms of knowledge; reflexivity of researchers and other actors; the recognition of [[Agency, Complexity and Emergence|complexity]] in systems; and the necessity of power relations in knowledge production are prominent examples of current challenges that methodologies as of yet have not solved. We are probably still far away from the creation of a solidified canon of knowledge to solve these riddles. One could argue that we will never solve these methodological problems altogether, but that the joined action and interaction while working on these problems will be the actual solution, and that a real solution as with past methods of normal science shall never be reached. This is unclear to date, yet we need to become aware that when we question the ''status quo'' of knowledge production and methodologies, we may not be looking for goals, but more for a path. There are several examples in sustainability science that follow this path, such as [[Transdisciplinarity|transdisciplinarity]] and [[System Thinking & Causal Loop Diagrams|system thinking]]. However, according to critical realism, it may be good to solidify innovative and novel methodologies to a point that enables as to take meaningful actions based on the knowledge we create.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.","What is clearer is that ''agency'' widely revolves around complex experimental designs that bridge different realms of science, such as neuroscience and psychology, psychology and microeconomics or behavioural science and zoology. The concept of agency links different fields of inquiry. Besides our diverse theories when it comes to agency, we only start to fully acknowledge the challenges related to its measurement as well as the opportunities to bridge different domains of sciences. From an empirical standpoint, agency has not been systematically accessed, which is why much emphasis is given to some few studies, and existing schools of thought often build on diverse premises. A good example to access agency is for instance in the combination of psychological or behaviour focused experiments with neurological examinations, although even such complicated experimental settings are not free of ambiguous results. Another methodological approach to agency could be in gamification, since such approaches would be able to test behavior of players within a complex environment, including reactions and motivations. The concept of altruism does however illustrate that, for the time being, the explanation of such behaviors may be unattainable. Just like agency, altruism can be divided into evolutionary or metaphysical explanations, roughly speaking. Time will tell if these schools of thought can be bridged. \n\n''Complex system theory'' is increasingly in the focus of many fields of research, which is no surprise. The dynamics of [[Glossary|complex systems]] are an Eldorado for many empirical researchers, and many disciplines are engaged in this arena. From a methodological standpoint, methods such as network analysis or structural equation models and also theoretical work such as the [https:\/\/en.wikipedia.org\/wiki\/Elinor_Ostrom Ostrom framework] are examples of the increasing recognition of complex systems. The Ostrom framework builds a bridge to this end, as it links a conceptual focus on resources with a methodological line of thinking to allow for a structured recognition of system dynamics. Several approaches try to implement the dynamics of complex systems within an analytical [[Glossary|framework]], yet these approaches often suffer from a gap between empirical data and sufficiently complex theory-driven models, since we only start to approach the gap between individual case studies and a broader view of empirical results. Here, the Ostrom framework is recognized as a landmark approach, although we must understand that even this widely popular framework was implemented in surprisingly few empirical studies (Partelow 2019).\n[[File:Ostrom framework.jpg|400px|thumb|left|'''The Ostrom framework allows us to reflect on socio-ecological systems.''' Source: [https:\/\/science.sciencemag.org\/content\/325\/5939\/419.abstract Ostrom 2009.]]]\n\nTo this end, it is important to realise that the problems that complex systems may encompass can be often diagnosed - which is called system knowledge - however the transformative knowledge necessary to solve these problems is ultimately what is complex. For instance do many people know that we already understood that climate change is real, and what we could do against it. What we could really do to solve the deeper problems - i.e. changing people's behaviour and consumption patterns - is however much more difficult to achieve, and this is what complex system thinking often about. '''Problem diagnosis is often simpler than creating solutions.''' Another challenge in complex systems are temporal dynamics. For instance, meteorological models are built on complex theories yet are also constantly refined based on a diversity of data. These models showcase that we are able to predict the weather for some days, and coarse patterns even for weeks, but there is still a clear limitation of our understanding of the weather when we go further into the future. A recent example of complex system dynamics is the Corona pandemic. While in the early stage of the pandemic our knowledge about the spread of the virus and the associated dynamics grew quickly, effective solutions are a long-term goal. There are clear relations how certain actions lead to certain developments in the pandemic, but the local and regional context can cascade into severe dynamics and differences. In addition, the development of potential solutions - such as a vaccine - is very complex and difficult to achieve. While thus many developments during this pandemic can be understood, it is certainly more complex to create a solution. \n\nTracking down ''emergence'' has become a key focus in many areas of science, but organic chemistry can serve as an example of how much has been done, yet how much is still on the horizon. Many negative effects of chemicals were not anticipated, with prominent examples being FCKW, pesticides and antibiotics. The effect of different medical drugs on people is yet another example, since interactions between different medications are hardly understood at all, as the field only slowly unravels the negative side-effects of interacting medications or treatments. We are far away from understanding the impact that the concepts ''agency'', ''complex systems'' and ''emergence'' have on our knowledge. Yet, we need to diversify our canon of methods in order to approach these concepts from an empirical standpoint. Otherwise we will not be able to unlock new strata of knowledge. This includes the combination of different methods, the utilization of specific methods in a different context, as well as the development of novel methods."],"4":["<syntaxhighlight lang=\"Python\" line>\nlen(data[bool_series])#applying the bool series by checking the null value\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n<\/syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=\"Python\" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n<\/syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.info()\n<\/syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (\"non-null\") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the \"object\" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For \"sex\" and \"passed\", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n<\/syntaxhighlight> \n\n<syntaxhighlight lang=\"Python\" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n<\/syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"sex\") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= \"sex\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"group\", diag_kind=\"hist\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=\"Python\" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap='PuBu');\n<\/syntaxhighlight>\n\n[[File:Pic 3.png]]","=== Only categorical data: Chi Square Test ===\n'''You should do a Chi Square Test'''.<br\/>\nA Chi Square test can be used to test if one variable influenced the other one, or if they occur independently from each other. The key R command here is: <code>chisq.test()<\/code>. Check the entry on [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi Square Tests]] to learn more.\n\n\n=== Categorical and continuous data ===\n'''Your dataset consists of continuous and categorical data.''' How many levels does your categorical variable have?\n<imagemap>Image:Statistics flowchart - Categorical factor levels.png|650px|center|\npoly 321 3 175 149 325 304 473 152 473 15 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data]]\npoly 149 172 3 318 153 473 301 321 301 321  [[An_initial_path_towards_statistical_analysis#One_or_two_factor_levels: t-test|One or two factor levels: t-test]]\npoly 489 172 343 318 493 473 641 321 641 321 [[An_initial_path_towards_statistical_analysis#More_than_two_factor_levels: ANOVA|More than two factor levels: ANOVA]]\n<\/imagemap>\n\n'''How do I know?'''\n* A 'factor level' is a category in a categorical variable. For example, when your variable is 'car brands', and you have 'AUDI' and 'TESLA', you have two unique factor levels. \n* Investigate your data using 'levels(categoricaldata)' and count the number of levels it returns. How many different categories does your categorical variable have? If your data is not in the 'factor' format, you can either convert it or use 'unique(yourCategoricalData)' to get a similar result.\n\n\n==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br\/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()<\/code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n\n'''Depending on the variances of your variables, the type of t-test differs.'''\n\n<imagemap>Image:Statistics Flowchart - Equal variances.png|850px|center|\npoly 146 5 0 150 145 290 289 148 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 557 2 408 147 556 286 700 144 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 392 165 243 310 391 450 535 308 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\npoly 716 160 567 305 715 444 859 302 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\n<\/imagemap>\n\n'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()<\/code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()<\/code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE<\/code>.\n\n\n==== More than two factor levels: ANOVA ====\n'''Your categorical variable has more than two factor levels: you are heading towards an ANOVA.'''<br\/>\n\n'''However, you need to answer one more question''': what about the distribution of your dependent variable?\n<imagemap>Image:Statistics Flowchart - 2+ Factor Levels - normal distribution.png|650px|center|\npoly 291 5 150 140 291 270 423 136 [[Data distribution]]\npoly 141 152 0 287 141 417 273 283 261 270  [[ANOVA]]\npoly 442 152 301 287 442 417 574 283 562 270 [[ANOVA]]\n<\/imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()<\/code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)<\/code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.","Now, let us have another look at your variables. '''Do you have continuous and categorical independent variables?'''\n\n'''How do I know?'''\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\nIf your answer is NO, you should stick to the ANOVA - more specifically, to the kind of ANOVA that you saw above (based on regression analysis, or based on a generalised linear model). An ANOVA compares the means of more than two groups by extending the restriction of the t-test. An ANOVA is typically visualised using [[Introduction_to_statistical_figures#Boxplot|Boxplots]].<\/br> The key R command is <code>aov()<\/code>. Check the entry on the [[ANOVA]] to learn more.\n\nIf your answer is YES, you are heading way below. Click [[An_initial_path_towards_statistical_analysis#Is_there_a_categorical_predictor?|HERE]].\n\n\n== Only continuous variables ==\nSo your data is only continuous.<br\/>\nNow, you need to check if there dependencies between the variables.\n<imagemap>Image:Statistics Flowchart - Continuous - Dependencies.png|650px|center|\npoly 383 5 203 181 380 359 559 182 [[Causality]]\npoly 182 205 2 381 179 559 358 382 [[An_initial_path_towards_statistical_analysis#No_dependencies:_Correlations|No dependencies]]\npoly 585 205 405 381 582 559 761 382 [[An_initial_path_towards_statistical_analysis#Clear_dependencies|Clear dependencies]]\n<\/imagemap>\n\n'''How do I know?'''  \n* Consider the data from a theoretical perspective. Is there a clear direction of the dependency? Does one variable cause the other? Check out the entry on [[Causality]].\n\n\n=== No dependencies: Correlations ===\n'''If there are no dependencies between your variables, you should do a Correlation.'''<br\/>\nA correlation test inspects if two variables are related to each other. The direction of the connection (if or which variable influences another) is not set. Correlations are typically visualised using [[Introduction_to_statistical_figures#Scatter_Plot|Scatter Plots]] or [[Introduction_to_statistical_figures#Line_chart|Line Charts]]. Key R commands are <code>plot()<\/code> to visualise your data, and <code>cor.test()<\/code> to check for correlations. Check the entry on [[Correlations]] to learn more.\n\n'''The type of correlation that you need to do depends on your data distribution.'''\n\n<imagemap>Image:Statistics Flowchart - Normal Distribution.png|650px|center|\npoly 288 3 154 137 289 269 421 136 [[Data distribution]]\npoly 135 151 1 285 136 417 268 284 268 284  [[Correlations|Pearson]]\npoly 438 152 304 286 439 418 571 285 [[Correlations|Spearman]]\n<\/imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()<\/code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)<\/code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\n\n=== Clear dependencies ===\n'''Your dependent variable is explained by one at least one independent variable.''' Is the dependent variable normally distributed?\n<imagemap>Image:Statistics Flowchart - Dependent - Normal Distribution.png|650px|center|\npoly 289 2 152 139 290 267 423 13 [[Data distribution]]\npoly 137 151 0 288 138 416 271 281 [[An_initial_path_towards_statistical_analysis#Normally_distributed_dependent_variable:_Linear_Regression|Linear Regression]]\npoly 441 149 304 286 442 414 575 279 [[An_initial_path_towards_statistical_analysis#Not_normally_distributed_dependent_variable|Non-linear distribution of dependent variable]]\n<\/imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()<\/code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)<\/code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed."],"5":["[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.","Since not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","# CREATING A LINEAR MODEL\n# In our multiple regression linear model we will try to predict the income based on \n# other variables given in our data set. \n# In the formula we drop the Gender_Code_Female and Region_Rural to avoid singularity error, \n# as they have an exact linear relationship with their counterparts.\nmodel <- lm(Income ~ Spending + Gender_Code_Male + Region_Urban, \n            data = data)\n\nsummary(model)\n\n#Call:\n#lm(formula = Income ~ Spending + Gender_Code_Male + Region_Urban, \n#    data = data)\n#\n#Residuals:\n#     Min       1Q   Median       3Q      Max \n#-13.1885  -6.1105  -0.1535   5.8825  12.9356 \n#\n#Coefficients:\n#                 Estimate Std. Error t value Pr(>|t|)    \n#(Intercept)      14.06584    0.63227  22.246   <2e-16 ***\n#Spending          0.02634    0.04577   0.575   0.5651    \n#Gender_Code_Male  0.83435    0.42116   1.981   0.0478 *  \n#Region_Urban     22.78781    0.42049  54.193   <2e-16 ***\n#---\n#Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n#\n#Residual standard error: 6.927 on 1086 degrees of freedom\n#Multiple R-squared:  0.7319,\tAdjusted R-squared:  0.7312 \n#F-statistic: 988.5 on 3 and 1086 DF,  p-value: < 2.2e-16\n<\/syntaxhighlight>\n\nThe fitted regression line turns out to be:\n\n''Income = 14.06584 + 0.02634*(Spending) + 0.83435*(Gender_Code_Male) + 22.78781*(Region_Urban)''\n\nWe can use this equation to find the estimated income for an individual based on their monthly spendings, gender and region. For example, an individual who is a female living in the rural area and spending 5 mln per month is estimated to have an income of 14.19754 mln per month:\n\n''Income = 14.06584 + 0.02634*5 + 0.83435*0 + 22.78781*0 = 14.19754''\n\n=== Python Script ===\n\n<syntaxhighlight lang=\"Python\" line>\n\nimport pandas as pd # for data manipulation\nimport statsmodels.api as sm # for statistical computations and models\n\ndf = pd.read_csv(\"YOUR_PATHNAME\")\n\n#Let's look at our data.\n#We have five variables and 1112 entries in total\ndf\n\n#        ID Gender_Code Region  Income  Spending\n#0        1      Female  Rural    20.0      15.0\n#1        2        Male  Rural     5.0      12.0\n#2        3      Female  Urban    28.0      18.0\n#3        4        Male  Urban    40.0      10.0\n#4        5        Male  Urban    42.0       9.0\n#    ...         ...    ...     ...       ...\n#1108  1109      Female  Urban    33.0      16.0\n#1109  1110        Male  Urban    48.0       7.0\n#1110  1111        Male  Urban    31.0      16.0\n#1111  1112        Male  Urban    50.0      14.0\n#1112  1113        Male  Urban    26.0      11.0\n#[1113 rows x 5 columns]\n\n\ndf.info()           # Two variables are categorical, Gender_Code and Region\n#<class 'pandas.core.frame.DataFrame'>\n#RangeIndex: 1113 entries, 0 to 1112\n#Data columns (total 5 columns):\n# #   Column       Non-Null Count  Dtype  \n#---  ------       --------------  -----  \n# 0   ID           1113 non-null   int64  \n# 1   Gender_Code  1107 non-null   object \n# 2   Region       1107 non-null   object \n# 3   Income       1107 non-null   float64\n# 4   Spending     1108 non-null   float64\n#dtypes: float64(2), int64(1), object(2)\n\ndf.isnull().sum()   # We see that our dataframe has some NAs in every variable except for ID\n#ID             0\n#Gender_Code    6\n#Region         6\n#Income         6\n#Spending       5\n#dtype: int64\n\ndf = df.dropna() # Apply na.omit function to delete the\ndf.info()        # No NAs\n\npd.unique(df.Gender_Code)\n# Output: array(['Female', 'Male'], dtype=object) -> The variable is binary"],"6":["[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>\n<br\/>\n<br\/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Clustering until 2019.''' Search terms: 'Clustering', 'Cluster Analysis' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as ''similarity measure'' or ''distance measure'', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named \"unsupervised learning\", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term \"clustering\" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=\"R\" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(\"x1\", \"x2\", \"cluster\")\n    return(df)\n}\n<\/syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns '''n''' examples to one of '''k''' clusters, where '''n''' is the sample size and  '''k''', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:\n\n# The number of cluster '''k''' is chosen by the data analyst\n# The algorithm randomly picks '''k''' centroids and assigns each point to the closest centroid to get '''k''' initial clusters\n# The algorithm recalculates the centroid by taking average of all points in each cluster and updates the centroids and re-assigns the points to the closest centroid.\n# The algorithm repeats Step 3 until all points stop changing clusters.\n\nTo get an intuitive sense of how this k-means clustering algorithm works, visit: [https:\/\/www.naftaliharris.com\/blog\/visualizing-k-means-clustering\/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====","[[File:Group Concept Mapping Cluster Map.png|400px|frameless|center|]]\n'''The Cluster Map, which puts the statements (data points) into groups by means of Cluster Analysis.''' The visual representation of these clusters is in shapes instead of points. The amount of clusters is adjustable based on what makes sense for the data. Each cluster will be given a sensible name by the group (Step 5). Source: adapted from Trochim 1989, p.12\n\n\n'''Rating Maps'''<br>\nAt the end of this step, the group has a (two-dimensional) map that includes all statements as data points, placed according to their relatedness (the \"Point Map\"); and one map that also groups these data points into clusters (the \"Cluster Map\"). Two more maps can be created based on these. The ratings that were assigned to each statement in the matrix in Step 3 are placed in the respective position on the map, resulting in the \"Point Rating Map\". The same is done for each cluster, with the Point Ratings being averaged within each cluster.\n\n[[File:Group Concept Mapping Point Rating Map.png|400px|frameless|center|]]\n'''The respective Point Rating Map.''' For each data point, the mean rating as assigned by the group in Phase 3 is indicated, based on a 1-5 Likert Scale in this case. Source: Trochim 1989, p.13\n\n[[File:Group Concept Mapping Cluster Rating Map.png|400px|frameless|center|]]\n'''And finally, the (still unlabeled) Cluster Rating Map.''' Here, the point ratings of all data points (on a 1-5 Likert Scale) within each cluster are averaged, leading to a rating for each cluster, indicated by the \"levels\". With this map at hand, it is easy to see which groups of ideas, problems, goals, or individuals are most \"important\", \"difficult\", or \"likely\" for the group, which is of great help for their planning or evaluation process. Source: adapted from Trochim 1989, p.14\n\n'''Our example'''<br>\nMcCaffrey et al. first considered the three participant groups' responses individually to see if there were diverging conceptual understandings of what constitutes good health care, but eventually combined their results into one conceptual model. They created a point maps, and evaluated different numbers of clusters before eventually deciding on a 10-cluster result, with which they developed a cluster map. Each cluster represented a different aspect of good health care. They calculated ratings for all data points and clusters and created point and cluster rating maps, where a higher rating equals a more important aspect of good health care. They found that there are only small differences in their highest and lowest rated clusters, which they stated highlights that the initial selection of statements already included very important aspects of good health care. They further investigated if there were significant differences between different participant groups' clustering results, which they did not find.\n\n[[File:Group Concept Mapping - Cluster Rating Map - Example.png|700px|thumb|center|'''The resulting (yet unlabeled) Cluster Rating Map for McCaffrey et al. 2019''' (p.89).]]\n\n\n==== 5 - Interpretation of Maps ====\nNow, '''the group is asked to assign names to the clusters'''. Each participants looks at each cluster and the statements included, and suggests a name (e.g. a phrase, or a word) to describe the cluster, and the group negotiates until consensus is reached for each cluster. If there are a lot of clusters, it may be sensible to further develop names for groups of clusters - \"regions\" - but this depends on the map at hand. In any case, the names of the clusters should represent the statements included as well as the conceptual relation to other clusters which are close. This labeled Cluster Map is the main outcome of the Group Concept Mapping process. It can be re-arranged by the group if necessary, since they should feel comfortable with the conceptual framework it represents.\n\n'''In the end of the process, the group has the following results:'''\n* a statement list\n* a point map\n* a point rating map\n* a cluster list (listing all labeled clusters including the respective statements)\n* a labeled cluster map, \n* and a labeled cluster rating map.\n\n'''Our example'''<br>\nMcCaffrey et al. named the clusters themselves, based on \"cluster names provided by participants whose\nsorting produced results similar to the final cluster content, and (2) by reviewing statements within each cluster\" (p.89).\n\n[[File:Group Concept Mapping - Cluster List (first half).png|600px|frameless|center]]\n[[File:Group Concept Mapping - Cluster List (second half).png|600px|frameless|center]]\n\nThe final list of clusters in McCaffrey et al. (2019, p.90f) The clusters (left) are presented in order of importance (right), with a description and exemplary statements for each cluster in the center.\n\n\n==== 6 - Utilization of Maps ====\nThe group is now done with the Group Concept Mapping process, and can use either of the maps (preferable the Cluster Map, or Cluster Rating Map) as a baseline for their further work in many diverse ways. '''The maps show the most important elements they need to pay attention to,''' which can be used to coordinate future actions, prioritize tasks, and structure the process. The clusters can serve as the organizational foundation, or as groups of topics to work on, either when implementing measures, or developing an evaluation scheme.","To get an intuitive sense of how this k-means clustering algorithm works, visit: [https:\/\/www.naftaliharris.com\/blog\/visualizing-k-means-clustering\/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====\n\nTo implement k-Means Clustering, the data table needs to only contain numeric data type. With a data frame or matrix with numeric value where the rows signify individual data example and the columns signify the ''features'' (number of features = the number of dimensions of the data set), k-Means clustering can be performed with the code below:\n\n<syntaxhighlight lang=\"R\" line>\n# Generate data and perform the clustering\ndf <- create_cluster_data(150, 1.25, 3)\ndata_cluster = kmeans(df, centers=3) # perform the clustering\n\n# plot for sd = 1.25\nplot(df$x1, df$x2, \n     pch=df$cluster, \n     col=data_cluster$cluster, cex=1.3, \n     xlab=\"x1\", ylab=\"x2\", \n     main=\"k-Means Clustering Example with Clearly Clustered Data\", \n     sub=\"(In-cluster variance when generating data = 1.25)\")\npoints(data_cluster$centers[1,1], data_cluster$centers[1,2], \n       pch=15, cex=2, col=\"black\")\npoints(data_cluster$centers[2,1], data_cluster$centers[2,2], \n       pch=15, cex=2, col=\"red\")\npoints(data_cluster$centers[3,1], data_cluster$centers[3,2], \n       pch=15, cex=2, col=\"green\")\nlegend(\"topleft\", legend=c(\"True Cluster 1\", \"True Cluster 2\", \"True Cluster 3\", \"Cluster Center\"), \n       col=c(\"black\",\"black\",\"black\", \"black\"), pch = c(1, 2, 3, 15), cex=0.8)\ntext(-3.15, 10, \n     \"Colors signify the clusters identified\\nby k-means clustering algorithm\", \n     adj = c(0,0), cex=0.8)\n<\/syntaxhighlight>\n\nHere is the result of the preceding code:\n\n[[File:Result of K-Means Clustering.png| |Result of K-Means Clustering]]\n\nWe can see that k-Means performs quite good at separating the data into three clusters. However, if we increase the variance in the dataset, k-Means does not perform as well. (See image below)\n\n[[File:Result of K-Means Clustering (High Variance).png| |Result of K-Means Clustering (High Variance)]]\n\n==== Strengths and Weaknesses of k-Means Clustering ====\n\n'''Strengths'''\n* It is intuitive to understand.\n* It is relatively easy to implement.\n* It adapts to new data points and guarantees convergence.\n\n'''Weaknesses'''\n* The number of clusters ''k'' has to be chosen manually.\n* The vanilla k-Means algorithm cannot accommodate cases with a high number of clusters (high ''k'').\n* The k-Means clustering algorithm clusters ''all'' the data points. As a result, the centroids are affected by outliers.\n* As the dimension of the dataset increases, the performance of the algorithm starts to deteriorate.\n\n=== Hierarchical Clustering ===\nAs the name suggests, hierarchical clustering is a clustering method that builds a ''hierarchy'' of clusters. \n\nUnlike k-means clustering algorithms - as discussed above -, this clustering method does not require us to specify the number of clusters beforehand. As a result, this method is sometimes used to identify the number of clusters that a dataset has before applying other clustering algorithms that require us to specify the number of clusters at the beginning.\n\nThere are two strategies when performing hierarchical clustering:\n\n==== Hierarchical Agglomerative Clustering ====\n\nThis is a \"bottom-up\" approach of building hierarchy which starts by treating each data point as a single cluster, and successively merging pairs of clusters, or agglomerating, until all clusters have been merged into a single cluster that contains all data points. Each observation belongs to its own cluster, then pairs of clusters are merged as one moves up the hierarchy.\n\n==== Implementation of Agglomerative Clustering ====\nThe first example of agglomerative clustering is performed using the hclust function built in to R.\n\n<syntaxhighlight lang=\"R\" line>\nlibrary(dendextend)\n\n# generate data\ndf <- create_cluster_data(50, 1, 3, random_state=7)\n# create the distance matrix\ndist_df = dist(df[, 2], method=\"euclidean\") \n# perform agglomerative hierarchical clustering\nhc_df = hclust(dist_df, method=\"complete\")\n# create a simple plot of the dendrogram\nplot(hc_df)"],"7":["[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations \"(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns.\" (Prell et al. 2009, p.503). '''Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as \"dyads\", but rather the study of patterns within a network.''' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors' behavior emerges from the interaction between them: \"Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions.\" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent's actions (Borgatti et al. 2009)","==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* '''Network boundaries:''' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** ''position-based approach'': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** ''event-based'' approach: those who had participated in key events are believed to define the population\n** ''relation-based approach'': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the ''exogenously defined boundaries'', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* '''Type of ties:''' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: ''directed'' ties, which go from one node to another, and ''undirected ties'', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, \"(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact.\" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* '''Data Collection''': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a ''prompted recall'' approach, they are asked which people they would think of with regards to a specific topic (e.g. \"To whom would you go for advice at work?\") while they are shown a pre-determined list of potentially relevant individuals. In the ''free list'' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the '''visual representation''' of the network. For this, the researcher \"(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections.\" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the \"number of lines which it is necessary to traverse in order to get from one point to another.\" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|'''Different network structures.''' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|'''An exemplary network structure.''' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]","[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n|'''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/>\n\n'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]\n\n'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions.\" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|'''Moreno's original work on Social Networks.''' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno's and Jennings' work was subsequently taken up and furthered as the field of ''''group dynamics', which was highly relevant in the US in the 1950s and 1960s.''' Simultaneously, sociologists and anthropologists further developed the approach in Britain. \"The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties.\" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n\"Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm.\" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that \"(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. '''Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.\"''' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as \"entitites\", \"actors\", \"vertices\" or \"agents\" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are \"(...) tied to one another through socially meaningful relations\" (Prell et al. 2009, p.503), which can be \"(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection\" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows."],"8":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")","'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]","<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n<\/syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"],"9":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")","<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n<\/syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]","Auf der Grundlage des aktuellen Methodenkanons k\u00f6nnen wir sagen: '''Vieles wird (noch?) nicht beobachtet, vieles ist nicht bekannt, und es wird entscheidend sein, zu verstehen, was wir zumindest vorl\u00e4ufig nicht wissen k\u00f6nnen.''' Diese drei unterschiedlichen Qualit\u00e4ten des Wissens machen deutlich, dass die Forschung zusammenarbeiten muss - Agency, viele komplexe Systeme und emergente Ph\u00e4nomene k\u00f6nnen von einer Disziplin allein nicht ausreichend untersucht werden. Auch wenn die Chemie Emergenz bei chemischen Reaktionen und die Medizin Wechselwirkungen zwischen verschiedenen Medikamenten entdecken kann, werden diese Ergebnisse normativ, sobald sie in der realen Welt anwendbar sind. Zu dieser Behauptung k\u00f6nnte man hier eine Ausnahme von der Ethik oder allgemein gesprochen von der Philosophie machen, die sich sinnvoll mit allen drei Wissensbereichen - unbeobachtet, nicht bekannt und nie bekannt - befassen kann, aber m\u00f6glicherweise nicht zu allen empirischen Problemen beitragen kann. Auf der anderen Seite kann alles au\u00dfer der Philosophie m\u00f6glicherweise nur ins Unbeobachtete und Unbekannte gehen. Wir m\u00fcssen unsere methodischen Ans\u00e4tze kombinieren, um das Wissen zu schaffen, das jetzt ben\u00f6tigt wird. Die Notwendigkeit der Zusammenarbeit ist eine Frage der Verantwortung, und nur wenn sich alle Disziplinen aufl\u00f6sen (oder zumindest ihr Hauptziel neu definieren, um zu vereinen und nicht zu differenzieren), kann Agency vollst\u00e4ndig erforscht und k\u00f6nnen komplexe Probleme gel\u00f6st werden. Viele der L\u00f6sungen, die wir jetzt schon in unseren H\u00e4nden halten, k\u00e4men unseren Vorfahren wie Zauberei vor. Es liegt in unserer Verantwortung, diesen Weg weiter zu beschreiten, nicht als Technokrat*innen oder Positivist*innen, die arroganten Stolz auf ihre Errungenschaften haben, sondern als blo\u00dfe Mitwirkende an einer breiteren Debatte, die letztlich die ganze Gesellschaft umfassen sollte. \n\nUm es mit Derek Parfit zu sagen:\n\n\"Einige Dinge (...) sind wichtig, und es gibt einen besseren und einen schlechteren Weg zu leben. Nachdem die Menschen viele tausend Jahre lang auf Ursachen in einer Weise reagiert haben, die ihnen half, zu \u00fcberleben und sich fortzupflanzen, k\u00f6nnen sie nun auf andere Ursachen reagieren. Wir sind Teil eines Universums, das beginnt, sich selbst zu verstehen. Und wir k\u00f6nnen teilweise verstehen, nicht nur, was tats\u00e4chlich wahr ist, sondern auch, was wahr sein sollte und was wir in der Lage sein k\u00f6nnten, wahr zu machen. Was jetzt am wichtigsten ist, ist, dass wir es vermeiden, die Menschheitsgeschichte zu beenden. Wenn es anderswo keine vern\u00fcnftigen Wesen gibt, kann es von uns und unseren Nachfolgern abh\u00e4ngen, ob sich das alles lohnt, denn die Existenz des Universums wird im Gro\u00dfen und Ganzen gut gewesen sein\".\n\n----\n[[Category: Normativity of Methods]]"],"10":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n<\/syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n<\/syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n<\/syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his\/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")","<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n<\/syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]","==== Normally distributed dependent variable: Linear Regression ====\n'''If your dependent variable(s) is\/are normally distributed, you should do a Linear Regression.'''<br\/>\nA linear regression is a linear approach to modelling the relationship between one (simple regression) or more (multiple regression) independent and a dependent variable. It is basically a correlation with causal connections between the correlated variables. Check the entry on [[Regression Analysis]] to learn more.\n\n'''There may be one exception to a plain linear regression:''' if you have several predictors (= independent variables), there is one more decision to make:\n\n\n===== Is there a categorical predictor? =====\n'''You have several predictors (= independent variables) in your dataset.''' But is (at least) one of them categorical?\n<imagemap>Image:Statistics Flowchart - Categorical predictor.png|650px|center|\npoly 387 1 208 184 385 359 563 183 [[Data formats]]\npoly 180 197 1 380 178 555 356 379 [[An_initial_path_towards_statistical_analysis#ANCOVA|ANCOVA]]\npoly 584 196 405 379 582 554 760 378 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Multiple Regression]]\n<\/imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric variables.\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. Pay attention to the data format of your independent variable(s). ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\n\n===== ANCOVA =====\n'''If you have at least one categorical predictor, you should do an ANCOVA'''. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more.\n\n\n==== Not normally distributed dependent variable ====\n'''The dependent variable(s) is\/are not normally distributed.''' Which kind of distribution does it show, then? For both Binomial and Poisson distributions, your next step is the Generalised Linear Model. However, it is important that you select the proper distribution type in the GLM.\n<imagemap>Image:Statistics Flowchart - Dependent - Distribution type.png|650px|center|\npoly 290 4 154 140 288 269 423 138 [[Data distribution]]\npoly 138 151 2 287 136 416 270 284 271 285 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\npoly 440 152 304 288 438 417 572 285 573 286 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\n<\/imagemap>\n\n'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes\/no (1\/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n<\/imagemap>\n\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to \"rule out\", the answer to this question is 'yes'.\n\n\n= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time."],"11":["== Strengths & Challenges ==\n==== Strengths ====\n* Focus Groups allow to explore people\u2019s views in a social context: \u201cWe learn about the 'meaning' of AIDS, (or sex, or health or food or cigarettes) through talking with and observing other people, through conversations at home or at work; and we act (or fail to act) on that knowledge in a social context. When researchers want to explore people's understandings, or to influence them, it makes sense to employ methods which actively encourage the examination of these social processes in action.\u201d (Kitzinger, p. 117). As S\u00e4yn\u00e4joki et al. (2014, p.6625) highlight: \"Focus groups are particularly useful in studies where the researcher seeks to uncover attitudes, perceptions and beliefs.\"\n* Furthermore, the group atmosphere makes many participants feel more at ease and encourages them to share their thoughts and perceptions.\n* Finally, Focus Groups are a way to gather data from several people at a time and are thus considered to be cost-effective and timesaving (4).\n\n==== Challenges ====\n* A challenge one needs to be aware of when conducting and analyzing Focus Groups is the censorship of certain \u2013 e.g. minority or marginalized \u2013 viewpoints, which can arise from the group composition (3). As Parker and Tritter (2006, p. 31) note: \u201cAt the collective level, what often emerges from a focus group discussion is a number of positions or views that capture the majority of the participants\u2019 standpoints. Focus group discussions rarely generate consensus but they do tend to create a number of views which different proportions of the group support.\"\n* Further, considering the effect group dynamics have on the viewpoints expressed by the participants is important, as the same people might answer differently in an individual interview. Depending on the focus of the study, either a Focus Group, an individual interview or a combination of both might be appropriate (3).\n* Last but not least - and as with other qualitative research methods - Focus Groups do not necessarily aim for representativeness but for an in-depth understanding of the perspectives studied (3). However, \u201cas the number of focus groups in the overall sample increases and their composition broadens, there is an extent to which the representativeness of their findings might be viewed as more acceptable and valid.\u201d (Parker, pp. 30).\n\n\n== Outlook ==\nWith regards to future research, Bloor et al. (2001) see the necessity to employ more resources and effort regarding \u201cgroup composition, recruitment, planning, conduct, transcription and analysis\u201d (p. 89) for Focus Groups to fulfill the requirements on methods used in the field of academic social research (in comparison to pure market research for example).\n\nAdditionally, future publications of studies which applied Focus Groups should place more focus on the impact the recruitment and the sampling process had on the collection, analysis, and interpretation of the data. Thus, more information on how Focus Group participants were sampled or on which basis a certain sample size (i.e. number of groups) was chosen should be provided, and their implications on the interaction among the participants as well as the data analysis and interpretation should be discussed (5, 2).\n\nGiven the complexity of Focus Group data, also the development of further methodologies for data analysis should be continued (5, 4).\n\n\n== An exemplary study ==\n[[File:Focus Groups exemplary study S\u00e4yn\u00e4joki et al. 2014 - Title.png|600px|frameless|center|The title of the exemplary Focus Group study (S\u00e4yn\u00e4joki et al. 2014).]]\n'''S\u00e4yn\u00e4joki et al (2014) used Focus Groups to investigate how Finnish urban planners perceive the potential positive impact of their field on environmental sustainability.''' Focus Groups were chosen since \"the focus of the study was not solely on what people think but on how they articulate, rationalise, challenge each other's views\" (p.6625). The researchers invited 32 urban planning specialists to a workshop, including individuals \"from fourteen Finnish cities, the Finnish environment ministry, two architectural firms, four consulting companies, one of Finland\u2019s largest energy companies, a market leading construction company, the Green Building Council Finland and the Finnish Association of Building Owners and Construction Clients (RAKLI)\" (p.6626). \n\n'''The researchers asked this group three questions as a prompt to the subsequent focus group sessions:'''\n* \"Why is environmental sustainability assessed in urban planning?\"\n* \"How does environmental assessment steer decision-making in urban planning?\"\n* \"What is the role of urban planning in environmental sustainability?\"\n\n'''Then, the 32 participants were divided into three groups of 11, 11 and 10 participants, respectively, with one moderating researcher each.''' All moderators were provided with the same guideline, and their role was \"to enhance interaction and to ensure that all participants had an equal chance to contribute.\" However, \"[w]ithin these limits, much of the discussion was left to the participants in order to learn what they found interesting and important. Nevertheless, occasionally the moderators attempted to develop the discourses by encouraging the participants to explain their views, or even through discreet provocation. The group discussions, each approximately an hour in length, were audio-recorded and manually transcribed and also video-recorded. In parallel with the recordings, the moderators made notes concerning mainly the atmosphere, the interaction and the participants\u2019 reactions.\" (p.6627). Importantly, in addition to the three questions that were posed prior to the focus groups, two further questions guided the moderators' actions:\n* \"How is the power of urban planners to promote environmental sustainability limited?\"\n* \"How is urban density considered in terms of environmentally sustainable land use?\"","== Strenghts & Challenges ==\n* Content Analysis counteracts biases because it \"(...) assures not only that all units of analysis receive equal treatment, whether they are entered at the beginning or at the end of an analysis but also that the process is objective in that it does not matter who performs the analysis or where and when.\" (see Normativity) (Krippendorff 1989, p.404). In this case, 'objective' refers to 'intersubjective'. Yet, biases cannot be entirely prevented, e.g. in the sampling process, the development of the coding scheme, or the interpretation and evaluation of the coded data.\n* Content Analysis allows for researchers to apply their own social-scientific constructs \"by which texts may become meaningful in ways that a culture may not be aware of.\" (Krippendorff 1989, p.404)\n* However, especially in case of qualitative analysis of smaller data sample sizes, the theory and hypotheses derived from data cannot be generalized beyond this data (1). Triangulation, i.e. the comparison of the findings to other knowledge on the same topic, may provide more validity to the conclusions (see Normativity).\n\n\n== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.\n* Validity of the inferred results - i.e. the fitting of the results to the intended kind of knowledge - may be reached a) through the researchers' knowledge of contextual factors of the data and b) through validation with other sources, e.g. by the means of triangulation. However, the latter can be difficult to do due to the uniqueness of the data (1-3). Any content analysis is a reduction of the data, which should be acknowledged critically, and which is why the coding scheme should be precise and include the aforementioned explanation, examples and exclusion criteria.\n\n==== Connectedness ====\n* Since verbal discourse is an important data source for Content Analysis, the latter is often used as an analysis method of transcripts of standardized, [[Open Interview|open]], or [[Semi-structured Interview|semi-structured interviews]].\n* Content Analysis is one form of textual analysis. The latter also includes other approaches such as discourse analysis, rhetorical analysis, or [[Ethnography|ethnographic]] analysis (2). However, Content Analysis differs from these methods in terms of methodological elements and the kinds of questions it addresses (2).\n\n\n== Outlook ==\n* The usage of automated coding with the use of computers may be seen as one important future direction of the method (1, 5). To date, the human interpretation of ambiguous language imposes a high validity of the results which cannot (yet) be provided by a computer. Alas, the development of an appropriate algorithm and text recognition software pose a challenge. The meaning of words changes in different contexts and several expressions may mean the same. Especially in terms of qualitative analyses, this currently makes human coders indispensable. Yet, the emergence of big data, Artificial Intelligence and [[Machine Learning]] might make it possible in the foreseeable future to use automated coding more regularly and with a high validity.\n* Another relevant direction is the shift from text to visual and audio data as a primary form of data (5). With ever-increasing amounts of pictures, video and audio files on the internet, future studies may refer to these kinds of sources more often.\n\n== Key publications ==\nKuckartz, U. (2016). ''Qualitative Inhaltsanalyse: Methoden, Praxis, Computerunterst\u00fctzung (3., \u00fcberarbeitete Auflage). Grundlagentexte Methoden.'' Weinheim, Basel: Beltz Juventa.\n* A very exhaustive (German language) work in which the author explains different types of qualitative content analysis by also giving tangible examples.\n\nKrippendorff, K. 2004. ''Content Analysis - An Introduction to Its Methodology. Second Edition''. SAGE Publications.\n* A much-quoted, extensive description of the method's history, conceptual foundations, uses and application. \n\nBerelson, B. 1952. ''Content analysis in communication research.'' Glencoe, Ill.: Free Press.\n* An early review of concurrent forms of (quantitative) content analysis.\n\nSchreier, M. 2014. ''Varianten qualitativer Inhaltsanalyse: Ein Wegweiser im Dickicht der Begrifflichkeiten.'' Forum Qualitative Sozialforschung 15(1). Artikel 18.\n* A (German language) differentiation between the variations of the qualitative content analysis.\n\nErlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n* A very helpful guide to content analysis, using the examples shown above.\n\n\n== References ==\n(1) Krippendorff, K. 1989. ''Content Analysis.'' In: Barnouw et al. (Eds.). ''International encyclopedia of communication.'' Vol. 1. 403-407. New York, NY: Oxford University Press.\n\n(2) White, M.D. Marsh, E.E. 2006. ''Content Analysis: A Flexible Methodology.'' Library Trends 55(1). 22-45.","While there is a wide range of qualitative Content Analysis approaches, this entry will focus on joint characteristics of these. For more information on the different variations, please refer to Schreier (2014) in the Key Publications.\n\n==== General Methodological Process ====\n* Any Content Analysis starts with the ''Design Phase'' in which the research questions are defined, the potential sources are gathered, and the analytical constructs are established that connect the prospective data to the general target of the analysis. These constructs can be based on existing theories or practices, the experience and knowledge of experts, or previous research (2).\n* Next, the ''Unitizing'' is done, i.e. the definition of analysis units. It may be distinguished between sampling units (= sources of data, e.g. newspaper articles, interviews) and analysis units (= units of data that are coded and analyzed, e.g. single words or broader messages), with different approaches to identifying both (see Krippendorff 2004).\n* Also, the ''sampling'' method is determined and the sample is drawn.\n* Then, the ''Coding Scheme'' - or 'Codebook' - is developed and the data are coded. 'Coding' generally describes the transfer of the available data into more abstract groups. A code is a label that represents a group of words that share a similar meaning (3). Codes may also be grouped into categories if they thematically belong together. There are two typical approaches to developing the coding scheme: a rather theory-driven and a rather data-driven approach. In a theory-driven approach, codes are developed based on theoretical constructs, research questions and elements such as an interview guide, which leads to a rather deductive coding procedure. In a data-driven approach, the coding system is developed as the researcher openly scans subsamples of the available material and develops codes based on these initial insights, which is a rather inductive process. Often, both approaches are combined, with an initial set of codes being derived from theory, and then iteratively adapted through a first analysis of the available material.\n* With the coding scheme at hand, the researcher reads (repeatedly) through the data material and assigns each analysis unit to one of the codes. The ''coding process'' may be conducted by humans, or - if sufficiently explicit coding instructions are possible - by a computer. In order to provide reliability, the codes should be intersubjective, i.e. every researcher should be able to code similarly, which is why the codes must be exhaustive in terms of the overarching construct, mutually exclusive, clearly defined, have unambiguous examples as well as exclusion criteria.\n* Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural context of the message, or on the message's effect. The learnings may be validated in the face of other information sources, which is often difficult when Content Analysis is used in cases where there is no other possibility to access this knowledge. Finally, new hypotheses may also be formulated (all from 1, 2).\n\n[[File:Example Content Analysis (1).jpg|500px|thumb|right|'''An exemplary interview transcript from a person that was admitted to an emergency center.''' Source: Erlingsson & Brysiewicz 2017.]]\n\n[[File:Example Content Analysis (2).png|500px|thumb|right|'''The coding results for statements from the interview transcript above, grouped into one of several themes.''' Source: Erlingsson & Brysiewicz 2017.]]\n\nQualitative Content Analysis is a rather inductive process. The process is guided by the research questions - hypotheses may be tested, but this is not the main goal. As with much of qualitative research, this method attempts to understand the (subjective) meaning behind the analyzed data and to draw conclusions from there. Its purpose is to get hold of the 'bigger picture', including the communicative context surrounding the genesis of the data. The qualitative Content Analysis is a very iterative process. The coding scheme is often not determined prior to the coding process. Instead, its development is guided by the research questions and done in close contact to the data, e.g. by reading through all data first and identifying relevant themes. The codes are then re-defined iteratively as the researcher applies them to the data. As an example, the comparative analysis of two texts can be mentioned, where codes are developed based on the first text, re-iterated by reading through the second text, then jointly applied to the first text again. Also, new research questions may be added during the analysis if the first insights into the data raise them. This approach is closely related to the methodological concept of [[Grounded Theory]] approaches.\n\nWith regards to the coding process, the analysis units are assigned to the codes but not statistically analysed. There is an exemption to this - the evaluative content analysis, where statements are evaluated on a nominal or ordinal scale, which makes simple quantitative (statistical) analyses possible. However, for most forms of qualitative Content Analysis, the assigned items per code are interpreted qualitatively and conclusions are inferred from these interpretations. Examples and quotes may be used to illustrate the findings. Overall, in this approach, a deep understanding of the data and their meaning is relevant, as is a proper description of specific cases, and possibly a triangulation with other data sources on the same subject."],"12":["'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br\/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https:\/\/sustainabilitymethods.org\/index.php\/Partial_Correlation this entry].\n__TOC__\n<br\/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, '''correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!'''\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\ndata(\"mtcars\")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = \"Correlation between Miles per Gallon and Horsepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     pch = 16,\n     col = \"red\",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n<\/syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=\"R\" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n<\/syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=\"R\" inline>plot()<\/syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* '''x:''' variable, that will be displayed on the x-axis.\n* '''y:''' variable, that will be displayed on the y-axis.\n* '''xlab:''' title for the x-axis.\n* '''ylab:''' title for the y-axis.\n* '''pch:''' shape and size of the plotted observations, in this case, filled circles. [http:\/\/www.sthda.com\/english\/wiki\/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* '''col:''' plotting color. You can either write the name of the color or use the [https:\/\/www.r-graph-gallery.com\/41-value-of-the-col-function.html color number].\n* '''las:''' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* '''xlim:''' set the limit of the x-axis.\n* '''ylim:''' set the limit of the y-axis.\n* '''abline:''' this function creates a regression line for the two variables.","[[File:ConceptCorrelation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Correlations]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n\n<br\/><br\/>\n'''In short:''' Correlation analysis examines the statistical relationship between two continuous variables. For R examples on Correlations, please refer to [[Correlation Plots]].\n\n== Background ==\n[[File:Correlation.png|400px|thumb|right|'''SCOPUS hits per year for Correlations until 2020.''' Search terms: 'Correlation' in Title, Abstract, Keywords. Source: own.]]\n\nKarl Pearson is considered to be the founding father of mathematical statistics; hence it is no surprise that one of the central methods in statistics - to test the relationship between two continuous variables - was invented by him at the brink of the 20th century (see Karl Pearson's \"Notes on regression and inheritance in the case of two parents\" from 1895). His contribution was based on work from Francis Galton and Auguste Bravais. With more data becoming available and the need for an \u201cexact science\u201d as part of the industrialization and the rise of modern science, the Pearson correlation paved the road to modern statistics at the beginning of the 20th century. While other approaches such as the t-test or the Analysis of Variance ([[ANOVA]]) by Pearson's arch-enemy Fisher demanded an experimental approach, the correlation simply required data with a continuous measurement level. Hence it appealed to the demand for an analysis that could be conducted based solely on measurements done in engineering, or on counting as in economics, without being preoccupied too deeply with the reasoning on why variables correlated. '''Pearson recognized the predictive power of his discovery, and the correlation analysis became one of the most abundantly used statistical approaches in diverse disciplines such as economics, ecology, psychology and social sciences.''' Later came the \u200bregression analysis, which implies a causal link between two continuous variables. This makes it different from a correlation, where two variables are related, but not necessarily causally linked. This article focuses on correlation analysis and only touches upon regressions. For more, please refer to the entry on [[Regression Analysis]].)\n\n\n== What the method does ==\nCorrelation analysis examines the relationship between two [[Data formats|continuous variables]], and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called ''correlation coefficient'' indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. \n\nCorrelations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including [[Survey]]s and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date.\n\n'''There are different forms of correlation analysis.''' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https:\/\/365datascience.com\/students-t-distribution\/ Student's t-distribution]. Alternative correlation measures like [https:\/\/www.statisticssolutions.com\/kendalls-tau-and-spearmans-rank-correlation-coefficient\/ Kendall's tau and Spearman's rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data.\n\n==== Calculating Pearson's correlation coefficient r ====\nThe formula to calculate [https:\/\/www.youtube.com\/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br\/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br\/>","[[File:Bildschirmfoto 2019-10-18 um 10.51.34.png|400px|thumb|center|'''By comparison, life expectancy and agricultural land have no correlation - which obviously makes sense.''' Source: Gapminder]]\n \n[[File:Bildschirmfoto 2019-10-18 um 10.30.35.png|400px|thumb|center|'''Income does have an impact on how much CO2 a country emits.''' Source: Gapminder]]\n\n\n=== A quick introduction to regression lines ===\nAs you can see, '''correlation analysis is first and foremost a matter of identifying ''if'' and ''how'' two variables are related.''' We do not necessarily assume that we can predict the value of one variable based on the value of the other variable - we only see how they are related. People often show a correlation in a scatter plot - the x-axis is one variable, the y-axis the other one. You can see this in the example below. Then, they put a line on the data. This line - the \"regression line\" - represents the correlation coefficient. It is the best approximation for all data points. This means that this line has the minimum distance to all data points. If all data points are exactly on the line, we have a correlation of +1 or -1 (depending on the direction of the line). However, the further the data points are from the line, the closer the correlation coefficient is to 0, and the less meaningful the correlation is.\n\n[[File:Correlation coefficient examples.png|600px|thumb|center|'''Examples for the correlation coefficient.''' Source: Wikipedia, Kiatdd, CC BY-SA 3.0]]\n<br>\n'''It is however important to know two things:'''<br>\n1) Do not confuse the slope of this line (the 'regression coefficient') - i.e. the number of y-values that the regression line steps per x-value - with the correlation coefficient. They are not the same, and this often leads to confusion. The regression coefficient of the line can easily be 5 or 10, but the correlation coefficient will always be between -1 and +1.\n\n2) Regressions only really make sense if there is some kind of causal explanation for the relationship. We can create a regression line for all correlations of all pairs of two variables, but we might end up suggesting a causal relationship when there really is none. As an example, have a look at the correlation below. There is no regression line here, but the visualisation implies that there is some connection, right? However, it does not really make sense that the divorce rate in Maine and the margarine consumption are related, even though their correlation coefficient is obviously quite high! So you should always question correlations, and ask yourself which kinds of variables are tested for their relationship, and if you can derive meaningful results from doing so.\n\n[[File:860-header-explainer-correlationchart.jpg|500px|thumb|center|'''Correlations can be deceitful'''. Source: [http:\/\/www.tylervigen.com\/spurious-correlations Spurious Correlations]]]\n<br>\n\n== Strengths & Challenges ==\n* Correlation analysis can be a powerful tool both for inductive reasoning, without a theoretical foundation; or deductive reasoning, which is based on theory. This makes it versatile and has enabled new discoveries as well as the support of existing theories.\n* The versatility of the method expands over all spatial and temporal scales, and basically any discipline that uses continuous data. This makes it clear why correlation analysis has become such a powerhorse for many researchers over time, and is so prevalent also in public debates.\n* Correlations are rather easy to apply, and most software allows to derive simple scatterplots that can then be analyzed using correlations. However, you need some minimal knowledge about data distribution, since for instance the Pearson correlation is based on data that is normally distributed."],"13":["You can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot<\/syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n<\/syntaxhighlight>\n\nClear and meaningful coding and plots are important. In order to achieve this, we have to change the names of the variables from the \u201cmtcars\u201d dataset into something meaningful. One way to do this, is to change the names of the columns and rows of the correlation variable.\n<syntaxhighlight lang=\"R\" line>\ncorrelations <- cor(mtcars)[1:11, 1:11]\ncolnames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\nrownames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\n<\/syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=\"R\" line>\n# Fig.5\ncorrplot(correlations,\n         method = \"circle\",\n         type = \"upper\",\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         tl.cex = 0.6)\n<\/syntaxhighlight>\n\nThe parameters are different from the previous scatter plots. Obviously, here you need the corrplot() function and define your parameters, regarding to your preferred taste, in this function. Some of the parameters will be explained briefly.\n\n* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (\u201ccircle\u201d, \u201csquare\u201d, \u201cellipse\u201d, \u201cnumber\u201d, \u201cshade\u201d, \u201ccolor\u201d, \u201cpie\u201d), \u201ccircle\u201d is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* '''type''': how the correlation matrix will be displayed. It can either be \u201cupper\u201d, \u201clower\u201d or \u201cfull\u201d. Full is called by default.\n* '''order''': order method for the correlation coefficients. The \u201chclust\u201d method orders them in hierarchical order, but it also possible to order them alphabetical (\u201calphabetical\u201d) or with a [[Principal_Component_Analysis|principal component analysis]] (\u201cPCA\u201d).\n* '''tl.col''': color of the labels.\n* '''tl.srt:''' rotation of the labels in degrees.\n* '''tl.cex:''' size of the labels.\n\n== Visualisation with ggplot ==\n=== Overview ===\n=== R code ===\nCOMING SOON\n\nAs you can see, there are many different ways to visualize correlations between variables. The right correlation plot depends on your data and on the number of variables you want to analyze. But never forget, correlation plots show you only the relationship between the variables and nothing about the causality.\n\n\n== References ==\n* https:\/\/sustainabilitymethods.org\/index.php\/Causality_and_correlation\n* https:\/\/en.wikipedia.org\/wiki\/Correlation_and_dependence\n* https:\/\/codingwithmax.com\/correlation-vs-causation-examples\/\n* https:\/\/towardsdatascience.com\/what-it-takes-to-be-correlated-ce41ad0d8d7f\n* http:\/\/www.r-tutor.com\/elementary-statistics\/numerical-measures\/correlation-coefficient\n\nA nice example that shows how easy it is to create a spurious correlation:\n\n* https:\/\/rstudio-pubs-static.s3.amazonaws.com\/4192_1180a799cd6c4d2ba6e4ed2702860efb.html\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden and ?.","'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br\/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https:\/\/sustainabilitymethods.org\/index.php\/Partial_Correlation this entry].\n__TOC__\n<br\/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, '''correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!'''\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\ndata(\"mtcars\")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = \"Correlation between Miles per Gallon and Horsepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     pch = 16,\n     col = \"red\",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n<\/syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=\"R\" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n<\/syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=\"R\" inline>plot()<\/syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* '''x:''' variable, that will be displayed on the x-axis.\n* '''y:''' variable, that will be displayed on the y-axis.\n* '''xlab:''' title for the x-axis.\n* '''ylab:''' title for the y-axis.\n* '''pch:''' shape and size of the plotted observations, in this case, filled circles. [http:\/\/www.sthda.com\/english\/wiki\/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* '''col:''' plotting color. You can either write the name of the color or use the [https:\/\/www.r-graph-gallery.com\/41-value-of-the-col-function.html color number].\n* '''las:''' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* '''xlim:''' set the limit of the x-axis.\n* '''ylim:''' set the limit of the y-axis.\n* '''abline:''' this function creates a regression line for the two variables.","==== Covariance matrix ====\n\nThe covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties:\n* The size of the matrix is equal to the number of features in the data\n* The main diagonal on the matrix contains the variances of each initial variables.\n* The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2)\n\nThe covariance matrix gives you a summary of the relationship among the initial variables.\n* A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa)\n* A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa)\n\n==== Eigenvectors \/ Principle Components & Eigenvalues ====\nNow we have the covariance matrix. This matrix can be used to transform one vector into another. Normally when this transformation happens, two things happen: the original is  rotated and get streched\/squished to form a new vector. When an abitrary vector is multipled by the covariance matrix, the result will be a new vector whose direction is nudged\/rotated towards the greatest spread in the data. In the figure below, we start with the arbitrary vector (-1, 0.5) in red. Multiplying the red vector with covariance matrix gives us the blue vector, and repeating this gives us the black vector. As you can see, the result rotation tends to converge towards the widest spread direction of the data.\n\n[[File: PCAEigenvector01.png|center|500px]]\n\nThis prompts the questions: Can we find directly find the vector which already lies on this \"widest spread direction\". The answer is yes, with the help of eigenvectors. Simply put, an eigenvectors of a certain matrix is a vector that, when transformed by the matrix, does not rotate. It remains on its own span, and the only thing that changes is its magnitude. This (constant) change ratio in magnitude corresponding to each eigenvector is called eigenvalue. It indicates how much of the data variability can be explained by its eigenvector.\n\nFor this toy dataset, since there are two dimensions, we get (at most) two egenvectors and two corresponding eigenvalues. Even if we only plot the eigenvectors scaled by their eigenvalues, we will basically have a summary data (and its spreading). At this point, the eigenpairs are be viewed as the principle components of the data.\n\n[[File: PCAEigenvector02.png|center|500px]]\n\n==== Ranking the principle components ====\nAs you may have noticed, the eigenvectors are perpendicular to each other. This is no coincidence. You can think of it this way: because we want to maximize the variance explained by each of the principle components, these components need to be independent from one another, therefore their orthogonality.  Thus, to define a set of principle components, you find the direction which can explain the variability in the data the most: that is your first principle component (the eigenvector with the highest eigenvalue). The second principle compent will be percepdicular to the first, and explain most of what is left of the variability. This continues until the d-th principle component is found.\n\nBy doing so, you are also sorting the \"importance\" of the principle components in terms of the information amount it contains what is used to explain the data. To be clear, the sum of all eigenvalues is the total variability in the data. From here, you can choose to discard any PCs whose percentage of explained variances are low. In many cases, if around 80% of the variance can be explained by the first k PCs, we can discard the other (d - k) PCs. Of course, this is only one of the heuristics method to determine k.  You can also use thr elbow method (the scree plot) like in k-means.\n\n==== Summary ====\n* PCA is a feature extraction technique widely used to reduce dimensionality of datasets.\n* PCA works by calculating the eigenvectors and the corresponding eigenvalues of the initial variables in the data. These are the principle components. Number of PCs = number of eigenvectors = number of features.\n* The PCs are ranked by the eigenvalues, and iteratively show the directions in which the data spreads the most (after accounting for the previous PCs).\n* We can choose to keep a few of the first PCs that cummulatively explains the data well enough, and these are the new reduced dimension of the data.\n* Standardization is a crucial step in data pre-processing to ensure the validity of the PCA results.\n\n\n=== R Example ===\nGoing back to the example in the introduction, the dataset can be found here: https:\/\/www.kaggle.com\/sdhilip\/nutrient-analysis-of-pizzas\n\n<syntaxhighlight lang=\"R\" line>\n# Loading library\nlibrary(tidyverse) # For pre-processing data\nlibrary(factoextra) # For visualization\ntheme_set(theme_bw()) # Set theme for plots\n\n# Load data\ndata <- read_csv(\"Pizza.csv\")\nhead(data)\n<\/syntaxhighlight>\n\nAs shown here, there are seven measurements of nutrients for each pizza. Our goal is to reduce these seven dimensions of information down to only two, so that we can present the main patterns in our data on a flat piece of paper."],"14":["Telemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is \u201cthe system of determining information about an animal through the use of radio signals from or to a device carried by the animal\u201d (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===\n\nFuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\nSutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\nRobertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work. British Trust for Ornithology, Tring.\n\n=== Empirical ===\n\nGibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tama\u00f1o de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n== References ==\n(1) Sutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\n\n(2) Robertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work.\n \n(3) Fuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\n\n(4) Bibby, C. J., Burgess, N. D., Hillis, D. M., Hill, D. A., & Mustoe, S. (1992).\u00a0Bird census techniques. Elsevier.\n\n(5) Buckland, S. T., Anderson, D. R., Burnham, K. P., & Laake, J. L. (1993). Distance sampling: estimating abundance of biological populations. Chapman & Hall, London.\n\n(6) Gibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tama\u00f1o de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n(7) Verner, J. (1985). Assessment of counting techniques. Current Ornithology: Volume 2, 247-302.\n\n(8) CBC (2021). How birding\u2019s pandemic popularity is expanding data collection for science.  https:\/\/www.cbc.ca\/news\/science\/science-birding-pandemic-data-wildlife-1.6113333 (accessed on 06.03.2023)\n\n(9) Boersch-Supan, P. H., Trask, A. E., & Baillie, S. R. (2019). Robustness of simple avian population trend models for semi-structured citizen science data is species-dependent. Biological Conservation, 240, 108286.\n\n(10) Science Daily (2020). Community science birding data does not yet capture global bird trends. https:\/\/www.sciencedaily.com\/releases\/2020\/07\/200707084012.html (accessed on 06.03.2023)\n\n(11) Gutema, T. M. (2015). Wildlife radio telemetry: use, effect and ethical consideration with emphasis on birds and mammals. Int J Sci Basic Appl Res, 24(2), 306-313.\n\n(12) Barron, D. G., Brawn, J. D., & Weatherhead, P. J. (2010). Meta\u2010analysis of transmitter effects on avian behaviour and ecology. Methods in Ecology and Evolution, 1(2), 180-187.\n\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau.\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]","THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https:\/\/www.kaggle.com\/datasets\/jaganadhg\/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=\"Python\" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n<\/syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=\"Python\" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir('\/path\/to\/your\/directory')\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndf = pd.read_csv('D202.csv')\ndf.head()\n<\/syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let's choose the most relevant columns for our research:\n\n<syntaxhighlight lang=\"Python\" line>\n## Let's choose the most relevant columns for our research:\ndf['start_date'] = pd.to_datetime(df['DATE'] + ' ' + df['START TIME'])\ndf['cost_dollars'] = df['COST'].apply(lambda x: float(x[1:]))\ndf.rename(columns={'USAGE': 'usage_kwh'}, inplace=True)\ndf = df.drop(columns=['TYPE', 'UNITS', 'DATE', 'START TIME', 'END TIME', 'NOTES', 'COST']).set_index('start_date')\n<\/syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe \u201ccost_dollars\u201d by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don't specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=\"Python\" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n<\/syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https:\/\/lg4ml.org\/grounddetection\/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).","==Further Readings==\n* [https:\/\/www.w3schools.com\/html\/html_xhtml.asp What is Extensible Hypertext Markup Language?]\n* [https:\/\/rss.com\/blog\/how-do-rss-feeds-work\/ RSS feed and their uses]\n* [https:\/\/www.cloudflare.com\/learning\/security\/api\/what-is-api-call\/#:~:text=Application%20programming%20interfaces%20(APIs)%20are,provide%20a%20service%20or%20information What is API?]\n* [https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/ BeautifulSoup Documentation:] For more on CSS Selectors and other ways of using BeautifulSoup\n* [https:\/\/www.projectpro.io\/article\/python-libraries-for-web-scraping\/625 BeautifulSoup Alternatives]\n* [https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Methods Types of HTTP Requests]\n* [https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Messages HTTP Messages (Requests and Responses)]\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"],"15":["In the following example, the standardised residuals are the residuals divided by the standard deviation. Let's take the caterpillar data set as an example. On the right you can see the table with the data: growth of caterpillars in relation to tannin content of their diet. Below, we will discuss some correlation plots between these two factors.\n\n[[File:Plot caterpillar.png|thumb|left|2. Plotting the data in an x-y plot already gives you an idea that growth probably depends on the tannin content.]]\n[[File:Qqplot2.png|thumb|right|4. The qqplot for this model looks good. Here the points are mostly on the line with point 4 and point 7 being slightly above and below the line. Still you would consider the residuals in this case to behave normally.]]\n[[File:Plot regression.png|thumb|center|3. Plotted regression line of the regression model \n<syntaxhighlight land = \"R\" inline>lm(growth~tannin)<\/syntaxhighlight> for testing the relation between two factors]]\n\n[[File:Qqplot notnomral.jpg|thumb|left|5. A gamma distribution, where the variances increases with the square of the mean.]]\n[[File:Qqplot negbinom.jpg|thumb|center|6. A negative binomial distribution that is clearly not following a normal distribution. In other words here the points are not on the line, the visual inspection of this qqplot concludes that your residuals are not normally distributed.]]\n\n===Non-normal distributions===\n'''Sometimes the world is [https:\/\/www.statisticshowto.com\/probability-and-statistics\/non-normal-distributions\/ not normally distributed].''' At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which [https:\/\/www.isixsigma.com\/tools-templates\/normality\/dealing-non-normal-data-strategies-and-tools\/ reasons] exists why data is not normally distributed. \n\n==== The Poisson distribution ====\n[[File:Bildschirmfoto 2020-04-08 um 12.05.28.png|thumb|500px|'''This picture shows you several possible poisson distributions.''' They differ according to the lambda, the rate parameter.]]\n\n[https:\/\/www.youtube.com\/watch?v=BbLfV0wOeyc Things that can be counted] are often [https:\/\/www.britannica.com\/topic\/Poisson-distribution not normally distributed], but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. '''How many people do you think drink one or two cups of coffee per day? Quite many, I guess.''' How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes. \n\nNow here comes the funny surprise. Transform the data following a [https:\/\/towardsdatascience.com\/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 Poisson distribution], and it will typically follow the normal distribution if you use the decadic logarithm (log). Hence skewed data can be often transformed to match the normal distribution. While many people refrain from this, it actually may make sense in such examples as [https:\/\/sustainabilitymethods.org\/index.php\/Is_the_world_linear%3F island biogeography]. Discovered by MacArtur & Wilson, it is a prominent example of how the log of the numbers of species and the log of island size are closely related. While this is one of the fundamental basic of ecology, a statistician would have preferred the use of the Poisson distribution.\n\n===== Example for a log transformation of a Poisson distribution =====\n[[File:Poisson Education small.png|thumb|400px|left]]\n[[File:Poisson Education log small.png|thumb|400px|left]]\nOne example for skewed data can be found in the R data set \u201cswiss\u201d, it contains data about socio-economic indicators of about 50 provinces in Switzerland in 1888. The variable we would like to look at is \u201cEducation\u201d, which shows how many men in the army (in %) have an education level beyond primary school. \nAs you can see when you look at the first diagram, in 30 provinces only 10 percent of the people received education beyond the primary school.\n\nTo obtain a normal distribution (which is useful for many statistical tests), we can use the natural logarithm.\n\nIf you would like to know, how to conduct an analysis like on the left-hand side, we uploaded the code right below:\n\n<syntaxhighlight lang=\"R\" line>\n\n# we will work with the swiss() dataset.\n# to obtain a histogram of the variable Education, you type\n\nhist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n<\/syntaxhighlight>","'''Example''': If you examine players in a basketball and a hockey team, you would expect their heights to be different on average. But maybe the variance is not. Consider Figure 1 where the mean is different, but the variance the same - this could be the case for your hockey and basketball team. In contrast, the height could be distributed as shown in Figure 2. The f-test then would probably yield a p-value below 0,05.\n\n[[File:Normal distribution.jpg|400px|thumb|left|Figure 1 shows '''two datasets which are normally distributed, but shifted.''' Source: [https:\/\/snappygoat.com\/s\/?q=bestof%3ALda-gauss-variance-small.svg+en+Plot+of+two+normal+distributed+variables+with+small+variance+de+Plot+zweier+Normalverteilter+Variablen+mit+kleiner+Varianz#7c28e0e4295882f103325762899f736091eab855,0,3 snappy goat]]]\n[[File:NormalDistribution2.png|400px|thumb|right|Figure 2 '''shows two datasets that are normally distributed, but have different variances'''. Source: [https:\/\/www.notion.so\/sustainabilitymethods\/Simple-Statistical-Tests-bcc0055304d44564bc41661453423134#7d57d8c251f94da8974eeb8a658aaa29 Wikimedia]]]\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the f-Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an f-Test.\n#We will compare the variances of height of two fictive populations. First, we create two vectors with the command 'rnorm'. Using rnorm, you can decide how many values your vector should contain, besides the mean and the standard deviation of the vector. To learn, what else you can do with rnorm, type:\n?rnorm\n\n#Creating two vectors\nPopA=rnorm(40, mean=175, sd=1)\nPopB=rnorm(40, mean=182, sd=2)\n\n#Comparing them visually by creating histograms\nhist(PopA)\nhist(PopB)\n\n#Conducting a f-test to compare the variances\nvar.test(PopA, PopB) \n\n#And this is the result, telliing you that the two variances differ significantly\nF test to compare two variances\n\ndata:  PopA and PopB\nF = 0.38584, num df = 39, denom df = 39, p-value = 0.00371\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2040711 0.7295171\nsample estimates:\nratio of variances \n         0.3858411 \n<\/syntaxhighlight>\n|}\n\n\n== Normativity & Future of Simple Tests ==\n'''Simple tests are not abundantly applied these days in scientific research, and often seem outdated.''' Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion.\n\nHowever, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics. \n\nSimple tests are often the endpoint of many introductionary teachings on statistics, which is unfortunate. Overall, their lack in most of recent publications as well as wooden design frames of these approaches make these tests an undesirable starting point for many students, yet they are a vital stepping stone to more advanced models.\n\nHopefully, one day school children will learn simple test, because they could, and the world would be all the better for it. If more people early on would learn about probability, and simple tests are a stepping stone on this long road, there would be an education deeper rooted in data and analysis, allowing for better choices and understanding of citizens.\n\n\n== Key Publications ==\n* Student\" William Sealy Gosset. 1908. ''The probable error of a mean.'' Biometrika 6 (1). 1\u201325.\n* Cochran, William G. 1952. ''The Chi-square Test of Goodness of Fit''. The Annals of Mathematical Statistics 23 (3). 315\u2013345. \n* Box, G. E. P. 1953. ''Non-Normality and Tests on Variances.'' Biometrika 40 (3\/4). 318\u2013335.\n\n\n== References ==\n(1) [https:\/\/en.wikipedia.org\/wiki\/Student's_t-test Article on the \"Student's t-test\" on Wikipedia]\n\n\n== Further Links ==\n* Videos\n[https:\/\/www.youtube.com\/watch?v=HZ9xZHWY0mw The Hypothesis Song]: A little musical introduction to the topic\n\n[https:\/\/www.youtube.com\/watch?v=ZzeXCKd5a18 Hypothesis Testing]: An introduction of the Null and Alternative Hypothesis\n\n[https:\/\/www.youtube.com\/watch?v=ptADSmJCVwQ The Scientific Method]: The musical way to remember it\n\n[https:\/\/www.youtube.com\/watch?v=wf-sGqBsWv4 Popper's Falsification]: The explanation why not all swans are white","<\/syntaxhighlight>\n[[File:Skewed residuals.png|thumb|left|These plots show you two examples of different distributions of residuals.]]\n\nA second case of a [https:\/\/statisticsbyjim.com\/regression\/check-residual-plots-regression-analysis\/ skewed distribution] is when the residuals are showing any sort of clustering. Residuals should be distributed like stars in the sky. If they are not, then your error is not normally distributed, which basically indicates that you are missing some important information. \n\nThe third and last problem you can bump into with your residuals are gaps. Quite often you have sections in your data about which you know nothing about, as you have no data for this section. An example would be if you have a lot of height measurement of people between 140-180 cm in height, and one person that is 195 cm tall. About the section between 180-195cm we know nothing, there is a gap.\n \nSuch flaws within residuals are called errors. These errors are the reason why all models are wrong. If a model would be perfect, then there would be probably a mistake, at least when considering models in statistics. Learning to read statistical models and their respective residuals is the daily bread and butter of statistical modelling. The ability to see flaws in residuals as well as the ability to just see a correlation plot in consider the strength of the relation, if any exists, is an important skill in data mining. Learning about models means to also learn about the flaws of models.\n\n==Is the world linear?==\n[[File:IslandTheory.png|thumb|right|This graph shows you the linear relationship between island size and number of species. As written in MacArthur's and Wilson's book \"The Theory in Island Biogeography\" the larger islands are the more species they host.]]\n'''The short answer: Mostly yes, otherwise not.'''\n\nThe long answer: The world is more linear than you think it is.\n\nI believe that many people are puzzled by the complexity of the world, while indeed many phenomena as well as the associated underlying laws are rather simple. There are many phenomena that are [https:\/\/www.youtube.com\/watch?v=wigmon6jlQE linear], and this is worth noticing. People today often think that the world is non-linear. Regime shifts, disasters and crisis are thought to be prominent examples. I would argue, that even these shifts follow linear patterns, though on a smaller temporal scale. Take the last financial crisis. A lot was building up towards this crisis, yet the collapse that happened in one day followed a linear pattern, if only a strongly exponential one. Many supposably non-linear shifts are indeed linear, they just happen very quickly.\n[[File:IslandTheoryBook.jpg|thumb|left|The book The Theory of Island Biogeography by MacArthur and Wilson tells you a lot about the relationship between island size and number of species.]]\nLinear patterns describe something like a natural law. Within a certain reasonable data section, you can often observe that one phenomena increases if another phenomena increases, and this relation follows a linear pattern. If you make the oven more hot, your veggies in there will be done quicker. However, this linearity only works within a certain reasonable section of the data. It would be possible to put the oven on 100 \u00b0C and the veggies would cook much slower than at 150 \u00b0C. However, this does not mean that a very long time at -30 \u00b0C in the freezer would boil your veggies in the long run as well.\n[[File:Linearity Ebola.png|thumb|right|This graph points out until which point the outbreak of the Ebola virus followed a linear pattern and at which point it increased exponentially.]]\n[[File:WaterPhaseHeatDiagram.png|thumb|right|On this picture you can see an example for non-linearity. The graph shows you the phase changes of water with increasing temperature.]]\nAnother prominent example is the [https:\/\/en.wikipedia.org\/wiki\/The_Theory_of_Island_Biogeography Island Theory] from MacArthur and Wilson. They counted species on islands, and found out that, the larger islands are, the more species they host. While this relation has a lot of complex underpinnings, it is -on a logarithmic scale- totally linear. Larger islands contain more species, with a clearly mathematical beauty. \n\nPhenomena that you can count are often linear on a log scale. Among coffee drinkers most coffee drinkers drink 1-2 cups per day, but few drink 10, which is probably good. Counting cup consumption per day in coffee drinker follows a log-linear distribution. The same hold for infections within an outbreak of a contagious disease. The Western African Ebola crisis was an incredibly complex and tragic event, but when we investigated the early increase in cases, the distribution is exponentially increasing -in this case-, and this increase follows an almost cruel linear pattern. \n\nNevertheless, there are some truly non-linear shifts, such as frost. Water is at -1 \u00b0C fundamentally different than at +1\u00b0C. This is a clear change governed by the laws of physics, yet such non-linear shifts are rare in my experience. If they occur, they are highly experimentally reproducible, and can be explained by some fundamental underlying law. Therefore, I would refrain from describing the complexity and wickedness of the modern world and its patterns and processes as non-linear. We are just not bright enough to understand the linearity in the data yet. I would advise you to seek linearity, if only logarithmic or exponential one.\n\n\n==References==\n\nNeuman, William Lawrence. ''Social Research Methods: Qualitative and Quantitative Approaches.'' 7. ed., Pearson new internat. ed. Pearson custom library. Harlow: Pearson, 2014."],"16":["'''In short:''' The (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''\u03bd = n\u22121'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value \u03bc, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i<\/sub>'' deviates significantly from a specified value ''\u03bc'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''\u03bc'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''\u03bc.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''\u03c3'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.\n\n==Two-sample t-test==\nWhen wanting to find out whether the means of two samples of a distribution are deviating significantly. If the two samples are independent from each other, an independent two-sample t-test has to be used. If the samples are dependent, which means that the values being tested stem from the same samples or that the two samples are paired, a paired t-test can be used.\n===Independent Samples===\nFor independent samples with similar variances (a maximum ratio of 2), the t-statistic is calculated in the following way:\n[[File:prbdst7.png|700px|frameless|center]]\nwith the estimated pooled standard deviation\n[[File:prbdst8.png|700px|frameless|center]]\nIn accordance with the One-sample t-test, the sample sizes, means and standard deviations of the samples 1 and 2 are denoted by ''n<sub>1\/2<\/sub>'', \u2002''x<sub>1\/2<\/sub>'' and ''\u03c3<sub>x<sub>1\/2<\/sub><\/sub>'' respectively.\nThe degrees of freedom which are required for conducting the hypothesis testing is given as ''\u03bd = n<sub>1<\/sub> + n<sub>2<\/sub> \u2212 2''.\nFor samples with unequal variances, meaning that one sample variance is more than twice as big as the other, Welch\u2019s t-test has to be used, leading to a different t-statistic t and different degrees of freedom ''\u03bd'':\n[[File:prbdst9.png|700px|frameless|center]]\nAn approximation for the degrees of freedom can be calculated using the Welch-Satterthwaite equation:\n[[File:prbdst10.png|700px|frameless|center]]\nIt can be easily shown, that the t-statistic simplifies for equal sample sizes:\n[[File:prbdst11.png|700px|frameless|center]]","hist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n<\/syntaxhighlight>\n\n====The Pareto distribution====\n[[File:Bildschirmfoto 2020-04-08 um 12.28.46.png|thumb|300px|'''The Pareto distribution can also be apllied when we are looking at how wealth is spread across the world.''']]\n\n'''Did you know that most people wear 20 % of their clothes 80 % of their time?''' This observation can be described by the [https:\/\/www.youtube.com\/watch?v=EAynHZE-lK4 Pareto distribution]. For many phenomena that describe proportion within a given population, you often find that few make a lot, and many make few things. Unfortunately this is often the case for workloads, and we shall hope to change this. For such proportions the [https:\/\/www.statisticshowto.com\/pareto-distribution\/ Pareto distribution] is quite relevant. Consequently, it is rooted in [https:\/\/www.pragcap.com\/the-pareto-principle-and-wealth-inequality\/ income statistics]. Many people have a small to average income, and few people have a large income. This makes this distribution so important for economics, and also for sustainability science.\n\n\n=== Visualizing data: Boxplots ===\nA nice way to visualize a data set is to draw a [[Barplots,_Histograms_and_Boxplots#Boxplots|boxplot]]. You get a rough overview how the data is distributed and moreover you can say at a glance if it\u2019s normally distributed. The same is true for [[Barplots,_Histograms_and_Boxplots#Histograms|histograms]], but we will focus on the boxplot for now. For more information on both these forms of data visualisation, please refer to the entry on [[Barplots, Histograms and Boxplots]].\n\n\n'''What are the components of a boxplot and what do they represent?'''\n[[File:Boxplot.png|frameless|500px|right]]\nThe '''median''' marks the exact middle of your data, which is something different than the mean. If you imagine a series of random numbers, e.g. 3, 5, 7, 12, 26, 34, 40, the median would be 12.\nBut what if your data series comprises an even number of numbers, like 1, 6, 19, 25, 26, 55? You take the mean of the numbers in the middle, which is 22 and hence 22 is your median.\n\nThe box of the boxplot is divided in the '''lower''' and the '''upper quartile'''. In each quarter there are, obviously, a quarter of the data points. To define them, you split the data set in two halves (outgoing from the median) and calculate again the median of each half. In a random series of numbers (6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95) your median is 43, your lower quartile is 14 and your upper quartile 87.\n\nThe space between the lower quartile line and the upper quartile line (the box) is called the interquartile range ('''IQR'''), which is important to define the length of the '''whiskers'''. The data points which are not in the range of the whiskers are called '''outliers''', which could e.g. be a hint that they are due to measuring errors. To define the end of the upper whisker, you take the value of the upper quartile and add the product of 1,5 * IQR.\n\n[[File:Boxplot Boxplot Text 2.jpg|thumb|400px|right|'''The boxplot for the series of data:''' 6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95]]\n\n\n'''Sticking to our previous example:'''\nThe IQR is the range between the lower (14) and the upper quartile (87), therefore 73.\nMultiply 73 by 1,5 and add it to the value of the upper quartile: 87 + 109,5 = 196,5\n\nFor the lower whisker, the procedure is nearly the same. Again, you use the product of 1,5*IQR, but this time you subtract this value from the lower quartile:\nHere is your lower whisker: 14 \u2013 109,5 = -95,5\n\nAnd as there are no values outside of the range of our whiskers, we have no outliers. Furthermore, the whiskers to not extend to their extremes, which we calculated above, but instead mark the most extreme data points.\n\n<syntaxhighlight lang=\"R\" line>\n\n#boxplot for our random series of numbers 6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95\n\nboxplot.example<-c(6,7,14,15,21,43,76,81,87,89,95)\nsummary(boxplot.example)","'''Annotation:''' This entry concludes the Introductory class on statistics for Bachelor students at Leuphana. It is not an autonomous entry on its own.\n\n==The great recap==\nWithin this module, we focused on learning simple statistics. '''Understanding basic [[Descriptive statistics|descriptive statistics]] allows us to calculate averages, sums and many other measures that help us grasp the essentials about certain data.''' [[Data formats]] are essential in order to understand the diverse forms that data can take. We learned that all data is constructed, which becomes most apparent when looking at [[To Rule And To Measure|indicators]] which can tell some story, yet without deeper knowledge about the construction - i.e., the context of an indicator - it is hard to grasp. \n\nOnce you get a hold of the diverse data formats, you can see then how data can represent different [[Data distribution|distributions]]. While much quantitative data is normally distributed, there are also exceptions, such as discrete data of phenomena that can be counted that is often showing a skewed distribution. Within frequentists statistics, statistical distributions are key, because these allow form a statistical standpoint to [[Experiments and Hypothesis Testing|test hypotheses]]. '''You assume that data follows a certain distribution, and that is often one important preconditions for the test or model you want to conduct.''' Whether your data then shows non-random patterns, but whether a hypothesis can actually be accepted or rejected, depends actually more often than not on the p-value. This value is the calculation whether your results are random and follow mere chance, of whether there is a significant pattern that you tested for. This is at its core what frequentist statistics are all about. \n\nThe most [[Simple Statistical Tests|simple tests]] can test for counts of groups within two variables (chi-square test), comparisons of two distributions (f-test) and the comparison of the mean values of two samples of a variables (t-test). Other tests avoid the question of statistical distribution by breaking the data into ranks, which are however less often applied (Wilcoxon test). '''A breakthrough in statistics was the development of the [[Correlations|correlation]], which allows to test whether two continuous datasets are meaningfully related.''' If one of the variables increases, the other variable increases as well, which would be a positive correlation. If one variable increases, and the other one decreases, we speak of a negative correlation. The strength of this relation is summarised by the correlation coefficient, which ranges from -1 to 1, and a values furthest from 0 indicates a strong relation, while 0 basically indicates complete randomness in the relation. This is tested again by p-values, where once more a values smaller than 0.05 indicates a non-random relation, which in statistics is called a significant relation. \n\nWhile correlation opened Pandoras box of statistics, it also raised a great confusion concerning the question whether a relation is [[Causality and correlation|causal]] or not. There are clear criteria that indicate causality, such as similarity in features of phenomena that have the same effect onto a variable. '''In order to statistically test for causal relations, [[Regression Analysis|regressions]] were developed.''' Regressions check for relations between variables, but revolve around a logical connection between these variables to allow for causal inferences. In addition, they allow to test not only the relation of one continuous variable in relation to another dependent variable. Instead, several independent variables can be tested, thus allowing to build more complex models and test more advanced hypotheses. Again, the relation is indicated to be significant by the p-value. However, the strength of the model is not measured in a coefficient, but instead in the r-square value, which is the sum of squares of the individual data points distance from the regression line. A regression line is hence the line that represents the regression model, which best explains the relation between the dependent and independent variable(s). \n\nWhile regressions were originally designed to test for clear hypotheses, these models are today utilised under diverse contexts, even in [[:Category:Inductive|inductive]] research, thereby creating tensions when it comes to the interpretation of the model results. A significant regression does not necessarily indicate a causal relation. This is a matter of the [[Normativity of Methods|normativity]] of the respective branch within science, and ultimately, also a question of philosophy of science. This is comparable to the [[ANOVA|analysis of variance]] (ANOVA), which unleashed the potential to conduct [[experiments]], starting in agricultural research, yet quickly finding its way into psychology, biology, medicine and many other areas in science. '''The ANOVA allows to compare several groups in terms of their mean values, and even to test for interaction between different independent variables.''' The strength of the model can be approximated by the amount of explained variance, and the p-value indicates whether the different groups within the independent variables differ overall. One can however also test whether one groups differs from another groups, thus comparing all groups individually by means of a posthoc test (e.g. Tukey)."],"17":["== Normativity ==\n==== Connectedness \/ nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).\n\n==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the 'real-world'.\n\n\n== Outlook ==\n==== Open questions ====\n* The diverse fields in which the Delphi method was applied has diversified and thus potentially confounded its methodological homogeneity, raising the need for a more comparable application and reporting of the method (6, 7)\n\n\n== An exemplary study ==\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 title.png|600px|frameless|center|The title of the exemplary study for Delphi method. Source: kauko & Palmroos 2014]]\nIn their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. '''The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets.''' Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks' foreign net assets etc.) with simple point estimates. These variables were chosen to \"fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible.\" (p.316). The participants were provided with information on the past developments of each of these variables.\n\nThe researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. '''The responses from the respective previous round were re-distributed to the participants''' (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again. \n\nAfter the third round, the participants were additionally asked to fill out survey questions on a 1-5 [[Likert Scale]] about how reliable their considered their own forecasts, and how much attention they had paid to the others' forecasts and comments when these were re-distributed, both regarding each variable individually. This was done to better understand each individual's thought process.\n<br>\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 results.png|800px|thumb|center|'''The results for the Delphi process.''' It shows that the mean estimates of the group became better over time, and were most often quite close to the actual realisation. Source: Kauko & Palmroos 2014, p.326.]]\n<br>\nThe forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. '''They found that the individual responses did indeed converge over the Delphi process, and that the \"Delphi group improved between rounds 1 and 3 in 13 of the questions.\"''' (p.320). They also found that \"[d]isagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement\" (p.322) and that the Delphi process \"clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future\", which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that \"paying attention to each others' answers made the forecasts more accurate\" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.","The questioning is most commonly conducted in form of a questionnaire but has more recently also been realized as individual, group, phone or digital interview sessions (2, 5). Digital questioning allows for real-time assessments of the answers and thus a quicker process. However, a step-by-step procedure provides more time for the researchers to analyze the responses (4).\n\n3. After the first round, the participants' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. 'Consensus' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).\n\nNext, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group's positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be \"(...) a highly motivating experience for participants\" (Rayens & Hahn 2000, p.309) due to the feedback on the group's opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which \"(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one's mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others.\" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)\n\nHowever, several potential pitfalls and challenges may arise during the Delphi process:\n* Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue (4, 5, 6).\n* A specific Delphi format that was useful in one study must not work as well in another context. Instead, the process must be adapted to the research design and underlying problem (4).\n* The proper selection of participating experts constitutes a major challenge for Delphi processes (3, 4, 5, 6). In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors (4).\n* The monitor (= researcher) must not impose their own preconceptions upon the respondents when developing the questionnaire but be open for contributions from the participants. The questions should be concise and understandable and should not incentivise the participant to \"get the job over with\" (Linstone & Turoff 1975, p.568; 5).\n* Diverse forms of [[Glossary|bias]] might occur on the part of the participants that need to be anticipated by the researcher. These include discount of the future, over-optimism \/ over-pessimism, misinterpretations with regard to the complexity and uncertainty involved in forecasting the future as well as other forms of bias that may be imposed through the feedback process (4, 6).\n* The responses must be adequately summarized, analyzed and presented to the participants (see the variety of measures for 'consensus' in What the method does)). \"Agreement about a recommendation, future event, or potential decision does not disclose whether the individuals agreeing did so for the same underlying reasons. Failure to pursue these reasons can lead to dangerously false results.\" (Linstone & Turoff 1975, p.568).\n* Disagreements between participants should be explored instead of being ignored so that the final consensus is not artificial (4).\n* The participants should be recompensated for their demanding task (4)","[[File:ConceptDelphi.png|450px|left|frameless|[[Sustainability Methods:About|Method categorization for Delphi]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/>\n<br\/>\n'''In short:''' The Delphi Method is an interactive form of data gathering in which expert opinions are summarized and consensus is facilitated.\n\n== Background ==\nThe Delphi method originates from work at RAND Corporation, a US think-tank that advises the US military, in the late 1940s and 1950s (2, 3, 5). RAND developed \"Project Delphi\" as a mean of obtaining \"(...) the most reliable consensus of opinion of a group of experts.\" (Dalkey & Helmer 1963, p.1). At the time, the alternative - extensive gathering and analysis of quantitative data as a basis for forecasting and deliberating on future issues - was not technologically feasible (4, 5). Instead, experts were invited and asked for their opinions - and Delphi was born (see (1)).\n\n[[File:Delphi Method SCOPUS.png|400px|thumb|right|'''SCOPUS hits for the Delphi method until 2019.''' Search terms: 'Delphi Panel', 'Delphi Method', 'Delphi Methodology', 'Delphi Study', 'Delphi Survey' in Title, Abstract, Keywords. Source: own.]]\n\nIn 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).\n\n\n== What the method does ==\nThe Delphi method is \"(...) a systematic and interactive research technique for obtaining the judgment of a panel of independent experts on a specific topics\" (Hallowell & Gambatese 2010, p.99). It is used \"(...) to obtain, exchange, and develop informed opinion on a particular topic\" and shall provide \"(...) a constructive forum in which consensus may occur\" (Rayens & Hahn 2000, p.309). Put simply, experts on a topic are gathered and asked in a systematic process what they think about the future, until consensus is found. \n\n==== The Delphi procedure ====\n[[File:ResultVisualisationDelphi.png|400px|thumb|right|Questionnaire results for the original RAND study, asking for an estimate of bomb requirements. The estimated numbers per participant converge over the course of the Delphi procedure. Source: Dalkey & Helmer 1963, p.15]]\n\n'''A Delphi process typically undergoes four phases''' (see (4), (6)):\n\n1. A group of experts \/ stakeholders on a specific issue is identified and invited as participants for the Delphi. These participants represent different backgrounds: academics, government and non-government officials as wel, as practitioners. They should have a diverse set of perspectives and profound knowledge on the discussed issues. They may be grouped based on their organizations, skills, disciplines or qualifications (3). Their number typically ranges from 10 up to 30, depending on the complexity of the issue (2, 3, 5, 6). \n\nThe researchers then develop a questionnaire. It is informed by previous research as well as input from external experts (not the participants) who are asked to contribute knowledge and potential questions on the pertinent issue (2, 5). The amount of [[Glossary|consultation]] depends on the expertise of the researchers on the respective issue (2).\n\n2. The questionnaire is used to ask for the participants' opinions and positions related to the underlying issue. The questions often take a 'ranking-type' (3): they ask about the likelihood of potential future situations, the desirability of certain goals, the importance of specific issues and the feasibility of potential policy options. Participants may be asked to rank the answer options, e.g. from least to most desirable, least to most feasible etc. (2). Participants may also be asked yes\/no questions, or to provide an estimate as a number. They can provide further information on their answers in written form. (8)"],"18":["'''Transdisciplinary research is of special importance to Sustainability Science and has received immense recognition in this field in the last years.''' This is because \"[s]ustainability is also inherently transdisciplinary\" (Stock & Burton 2011, p.1091), as it builds on the premise of solving real-world problems which are deeply nestled in ecological, political, economic and social processes and structures and therefore cannot be understood and solved without engaging with these spheres (Kates et al. 2015). Transdisciplinary research is a suitable approach for Sustainability Science: it allows to incorporate the knowledge of relevant stakeholders; it considers the normative dimensions involved in societal endeavors (that is, diverging norms, goals and visions of different societal spheres); and it increases [[Glossary|legitimacy]], ownership and accountability for the jointly developed solutions (Lang et al. 2012; Stock & Burton 2011). The integrative transdisciplinary approach highlights systemic interdependencies, enables a better understanding of complex issues and provides better knowledge to develop socially robust and applicable, effective solutions (Lang et al. 2012; Mauser et al. 2015).\n\n\n== How to do TD in Sustainability Science ==\nHere, we will refer to the ideal-typical model presented by Lang et al (2012). It introduces a conceptual guide, structured into three phases, on how transdisciplinary research for sustainability science should ideally be conducted. The relevant steps of each phase as well as general principles for the whole process will be listed below. The presented process is recursive: Problems emerging in scientific as well as societal practice are integrated in the transdisciplinary research approach. After its three phases have been realized, the newly generated knowledge and solutions are re-integrated into these two spheres, where they are applied and consolidated. Ultimately, new challenges emerge that demand renewed transdisciplinary collaboration.\n\n==== The ideal-typical model ====\n[[File:TDmodel.png|500px|thumb|left|'''The ideal-typical model for TD research in Sustainability Science.''' Source: Lang et al. 2012]]\n\n''Phase A''\n* Build a collaborative research team.\n* Create joint understandings and definitions of the problem.\n* Collaboratively define research objects and objectives, research questions, and success criteria.\n* Design a methodological [[Glossary|framework]].\n\n''Phase B''\n* Assign roles and responsibilities.\n* Apply research methods and settings to create the intended knowledge.\n\n''Phase C''\n* Realize two-dimensional integration.\n* Generate products for both parties (societal and scientific).\n\n''General principles''\n* Facilitate continuous evaluation\n* Mitigate conflict constellations\n* Enhance capabilities for and interest in participation\n<br>\n\n==== Challenges ====\nThere is a wide range of potential pitfalls challenging the achieval of the ideal-typical model.\n\n* Mauser et al. (2013) remark that the roles and responsibilities of all stakeholders involved need to be clarified from the beginning and existent inequalities need to be removed. Further, they comment that integrated research modes demand new methods and concepts, new institutional arrangements, organizational [[Glossary|competencies]] and tailored funding possibilities. The cooperation between science and society requires reflexive learning processes, appropriate communication tools and also needs to reflect on the role of science in the overall endeavour. Also, the authors highlight that there may be inertia to change in the given system which needs to be overcome.\n* Lang et al. (2012) name various issues in the framing of problems (lack of problem awareness, unbalanced problem ownership), processual issues (insufficient legitimacy of the actors involved, conflicting methodological standards, lack of integration (knowledge, structures, [[Glossary|communication)]], discontinuous [[Glossary|participation]]), as well as challenges concerning the development of solutions (ambiguity of results, fear to fail, limited solution options, lack of legitimacy of transdisciplinary outcomes, capitalization on distorted research results, issues when tracking impacts). Also, \"[a]rguing from a more conventional research perspective, scientists might be skeptical with respect to reliability, validity, and other epistemological and methodological aspects of collaborative research (\u2018\u2018credibility\u2019\u2019). Practitioners and stakeholders, on the other hand, might be skeptical regarding the practical relevance of the results (\u2018\u2018salience\u2019\u2019).\" (Lang et al. 2012)\n* Hall & Rourke (2014) expand on communicative challenges, claiming that \"[n]ot all of the challenges that threaten TDSS [transdisciplinary sustainability science] are communication challenges, but communication breakdown can exacerbate any of them. Because of its centrality, care must be taken by collaborators to cultivate a healthy communication dynamic; however, given the many perspectives involved in a typical TDSS project, this will not be easy. These projects meet complex problems with complex responses, entailing the need to remain flexible and responsive to participant requirements and the need to modify the approach if new information and values arise.\" They highlight communicative challenges especially in the framing of problems (exclusion of important perspectives, different views of the problem) and the conduction of the research (unwillingness to share personal perspectives, failure to recognize or articulate differences in individual assumptions, uncertainties and incomplete or incompatible knowledge, limited cognitive abilities to integrated individual partial knowledge).\n\n==== Methods of transdisciplinary research ====\nSeveral scientific methods are useful for transdisciplinary work. Many methods used in TD research represent single elements of a broader transdisciplinary inquiry, such as [[Interviews]] which may be used in a first step of [[Glossary|consultation]]. Other methods, however, originate from transdisciplinary endeavours or are strongly associated to them. Among these, noteworthy methods include:","The course '''Methods of Environmental Sciences''' covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective. \n\n__TOC__\n<br\/>\n==== Definition & History of Methods ====\nWithin this lecture we deeply focus on the formation of a new arena in science that is including not only system knowledge, but also normative knowledge as well as transformative knowledge. In order to create solution for the problems we currently face, a solution orientated agenda is necessary.  This may demand the creation of novel methodological pathways to knowledge creation. Here, we give a tentative overview on the developments up until now.\n* [[History of Methods in Sustainability Science]]\n\n==== Design Criteria of Methods in Sustainability Science ====\nThere are several design criteria that allow you to systematise methods. Many of these criteria are part of the \u201cusual suspects\u201d in normal science ''sensu strictu'' Kuhn. Here, we discuss further design criteria and knowledge types that can be relevant to systematise knowledge production through methods for sustainability science.\n* [[Design Criteria of Methods in Sustainability Science]]\n\n==== [[Thought Experiments]] & [[Legal Research]] ====\n\n==== Quantitative Methods in the Humanities ====\n* [[Causality and correlation]]\n\n==== [[Geographical Information Systems]] ====\n\n==== [[Grounded Theory]] ====\n\n==== Interviews ====\n* [[Semi-structured Interview]]<br\/>\n* [[Open Interview]]\n\n==== The ecological experiment ====\n* [[Experiments and Hypothesis Testing]]\n\n==== Causal Loop Diagrams ====\n* [[System Thinking & Causal Loop Diagrams]]\n\n==== Questioning the status quo in method-driven research ====\n* [[Questioning the status quo in methods]]\n\n==== [[Social Network Analysis]] ====\n\n==== Meta-Analysis ====\n* [[Meta-Analysis]]\n* [[Systematic Literature Review]]\n\n==== Mixed Methods in transdisciplinary research ====\n* [[Transdisciplinarity]]\n* [[Visioning & Backcasting]]\n* [[Scenario Planning]]\n* [[Living Labs & Real World Laboratories]]\n\n----\n[[Category: Courses]]","* Hall, T. E. O'Rourke, M. 2014. ''Responding to communication challenges in transdisciplinary sustainability science. Heuristics for transdisciplinary sustainability studies: Solution-oriented approaches to complex problems.'' 119-139.\n\n* Allington, G. R. H., M. E. Fernandez-Gimenez, J. Chen, and D. G. Brown. 2018. ''Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau.'' Ecology and Society 23(2):9. \n\n* Pfeiffer, C. Schodl, K. Fuerst-Waltl, B. Willam, A. Leeb, C. Winckler, C. 2018. ''Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach.'' Journal fo Central European Agriculture 19(4). 858-864.\n\n\n== Further Information ==\n* The [http:\/\/intrepid-cost.ics.ulisboa.pt\/about-intrepid\/ INTREPID] network revolves around transdisciplinary and interdisciplinary research and provides useful actors and learnings in this field.\n* [http:\/\/www.transdisciplinarity.ch\/td-net\/Aktuell\/td-net-News.html TD-NET] is a resourceful Swiss platform that organizes and presents activities in the field. The same can be said about the [https:\/\/complexitycontrol.org\/methods-of-transdisciplinary-research\/ Complexity or Control blog], hosted at Leuphana University.\n* The [https:\/\/www.reallabor-netzwerk.de\/ Reallabor-Netzwerk] also hosts useful information about real world laboratories and transdisciplinary research.\n* You can find a lot of considerations concerning transdisciplinarity on [https:\/\/i2insights.org\/tag\/transdisciplinarity-general-relevance\/ this part of the 'Integration and Insights' blog]. \n* Mauser et al. (2013, p.420) present the ''[https:\/\/futureearth.org\/initiatives\/ Future Earth Initiative]'', which emerged from Rio+20 and \"will provide a new platform and paradigm for integrated global environmental change research that will be designed and conducted in partnership with society to produce the knowledge necessary for societal transformations towards sustainability\".\n\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."],"19":["The disciplinary bias of modern science thus creates a deeply normative methodological bias, which some disciplines may try to take into account yet others clearly not. In other words, the dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research, and we need to be aware and reflexive about this. '''The largest bias concerning methods is the choice of methods per se.''' A critical perspective is thus not only of relevance from a perspective of societal responsibility, but equally from a view on the empirical. Clear documentation and reproducibility of research are important but limited stepping stones in a critique of the methodological. This cannot replace a critical perspective, but only amends it. Empirical knowledge will only look at parts - or strata according to Roy Bhaskar - of reality, yet philosophy can offer a generalisable perspective or theory, and Critical Theory, Critical Realism as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.\n\n== Further Information ==\n[https:\/\/www.thoughtco.com\/critical-theory-3026623 Some words on Critical Theory]<br>\n[https:\/\/www.newworldencyclopedia.org\/entry\/Critical_realism#Contemporary_critical_realism A short entry on critical realism]\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.","'''Critical Theory is thus a vital step towards a wider integration of diverse philosophies,''' but also from a methodological standpoint it is essential since it allowed for the emergence of a true and holistic critique of everything empirical. While this may be valued as an attack, is can also be interpreted as necessary step, since the arrogance and the claim of truth in empiricism can be interpreted not only as a deep danger to methods. Popper does not offer a true solution to positivism, and indeed he was very much hyped by many. His thought that the holy grail of knowledge can ultimately be never truly reached also generates certain problems. He can still be admired because he called for scientists to be radical, while acknowledging that most scientists are not radical. In addition, we could see it from a post-modernist perspective as a necessary step to prevent an influence of empiricism that might pose a threat to and by humankind itself, may it be through nuclear destruction, the unachievable and feeble goal of a growth economy (my wording), the naive and technocratic hoax of the eco modernists (also my wording) or any other [[Glossary|paradigm]] that is short-sighted or naive. In other words, we look at the postmodern. \n\nCritical Theory to this end is now developing to connect to other facets of the discourse, and some may argue that its focus onto the social science can be seen critical in itself, or at least as a normative choice that is clearly anthropocentric, has a problematic relationship with the empirical, and has mixed relations with its diverse offspring that includes gender research, critique of globalisation, and many other normative domains that are increasingly explored today. Building on the three worlds of Popper (the physical world, the mind world, human knowledge), we should note another possibility, that is Critical Realism. Roy Bhaskar proposed three ontological domains (''strata of knowledge''): the real (which is ''everything there is''), the actual (''everything we can grasp''), and the empirical (''everything we can observe''). During the last decade, humankind unlocked ever more strata of knowledge, hence much of the actual became empirical to us. We have to acknowledge that some strata of knowledge are hard to relate, or may even be unrelatable, which has consequences for our methodological understanding of the world. Some methods may unlock some strata of knowledge but not others. Some may be specific, some vague. And some may only unlock new strata based on a novel combinations. What is most relevant to this end is however, that we might look for causal links, but need to be critical that new strata of knowledge may make them obsolete. Consequently, there are no universal laws that we can thrive for, but instead endless strata to explore.\n\nComing back to bias, '''Critical Theory seems as an antidote to bias''', and some may argue Critical Realism even more so, as it combines the criticality with a certain humbleness necessary when exploring the empirical and causal. The explanatory characteristic allowed by Critical Realism might be good enough for the pragmatist, the practical may speak to the modern engagement of science with and for society, and the normative is aware of \u2013 well - all things normative, including the critical. Hence a door was opened to a new mode of science, focussing on the situation and locatedness of research within the world. This was surely a head start with Kant, who opened the globe to the world of methods. There is however a critical link in Habermas, who highlighted the duality of the rational individual on a small scale and the role of global societies as part of the economy (Habermas 1987). This underlines a crucial link to the original three foundational theories in philosophy, albeit in a dramatic and focused interpretation of modernity. Habermas himself was well aware of the tensions between these two approaches \u2013 the critical and the empirical -, yet we owe it to Critical Theory and its continuations that a practical and reflexive knowledge production can be conducted within deeply normative systems such as modern democracies. \n\nLinking to the historical development of methods, we can thus clearly claim that Critical Theory (and Critical Realism) opened a new domain or mode of thinking, and its impact can be widely felt way beyond the social science and philosophy that it affected directly. However, coming back to bias, the answer to an almost universal rejection of empiricism will [[Big problems for later|not be followed here]]. Instead, we need to come back to the three foundational theories of philosophy, and need to acknowledge that reason, social contract and utilitarianism are the foundation of the first empirical disciplines that are at their core normative (e.g. psychology, social and political science, and economics). Since bias can be partly related to these three theories, and consequentially to specific empirical disciplines, we need to recognise that there is an overarching methodological bias. This methodological bias has a signature rooted in specific design criteria, which are in turn related to specific disciplines. Consequently, this methodological bias is a disciplinary bias - even more so, since methods may be shared among scientific disciplines, but most disciplines claim either priority or superiority when it comes to the ownership of a method.","The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br\/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs."],"20":["== What the method does ==\nBefore explaining System Thinking, it should first be explained what is a 'system'.\n\n==== Systems, System Thinking, System Analysis & System Dynamics ====\nA system is a \"(...) network of multiple variables that are connected to each other through causal relationships\", based on which the network \"(...) expresses some sort of behaviour, which can only be characterized through observation as a whole\" (Haraldsson 2004, p.11). This behavior remains persistent in a variety of circumstances. More simply put, a system is a \"(...) collection of connected things (...) that influence one another.\" (Toole 2005, p.2) Both the collection of elements and their interactions are important elements of the system which is emphasized by saying that a system is 'more than a collection of its parts' (Arnold & Wade 2015, p.2).\n\nEvery system is \"(...) defined by its boundaries\" (Haraldsson 2004, p.13). The borders we draw for our system analysis influence which level of detail we apply to our view on the system, and which elements we investigate. System elements can be animate (animals, humans) or inanimate (rocks, rain), conceptual (motivation) or real (harvest), quantifiable (money) or rather qualitative (well-being) (2). For example, a system could be a tree, with the leaves, the stem and such elements interacting with each other, but also the forest in which our tree interacts with the soil, the weather, other plants, animals and inanimate objects. The system could also be the globe, where this forest interacts with other ecosystems, or the system in which Planet Earth interacts with the rest of the universe - our solar system. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n'''The system is at the basis of System Thinking.''' System Thinking is a form of scientific approach to organizing and understanding 'systems' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of 'holistic' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that \"(...) all system dynamics are in principle non-linear\" and that \"(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions\" (Haraldsson 2004, p.6).\n\n'''Peter Checkland introduced the notion that there are two main types of System Thinking:''' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a 'system' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a 'system' an \"(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world\" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a 'system'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|'''Hard System Thinking and Soft System Thinking''' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: ''System Analysis'' and ''System Dynamics'' (1).\n\n''System Analysis'' \"(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships.\" (Haraldsson 2004, p.5). ''System Analysis'', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].\n\n''System Dynamics'', then, focuses on the interaction part of the system. It \"(...) refers to the re-creation of the understanding of a system and its feedbacks. It aims at exploring dynamic responses to changes within or from outside the system. (...) System Dynamics deals with mathematical representation of our mental models and is a secondary step after we have developed our mental model.\" (Haraldsson 2004, p.5). System Dynamics, as the name suggests, enables the researcher to observe and measure the behavior of the system. The interactions between the individual elements are not just recreated, but the consequences of these interactions are quantified and assessed.","'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries. \n\n== The problem with System Boundaries ==\nWhile boundaries of some isolated systems as well as simple theoretical models can be rather clearly defined, much attention of the recent literature has actually focussed on the effects that systemic interactions create across distances, which have historically been considered separate (see for example [['https:\/\/www.ncdc.noaa.gov\/teleconnections\/'|'teleconnections']] in atmospheric research). It is now evident that inland watersheds are linked to oceangraphic processes through freshwater and nutrient inputs, although these have historically and disciplinarily been examined separately. Furthermore, the nestedness of systems is another challenge - the idea that smaller systems are contained within, and are the constituent components of, larger systems such as organs within the larger integrated functionality of the human body. Some organs can be removed or replaced, but only in relation to the overall functions they provide and how they fit or match the context of the overall body (i.e, blood type or immune response). Government jurisdictions give other clear examples, where countries may contain several provinces as part of a federal system, which in turn can contain smaller administrative units, creating a multi-level administrative whole. \n\nThe debate often resonates around a void between the global and the local levels of social and ecological [[Glossary|scales]] such as space, jurisdiction, administration and even time (e.g., slow to fast). While the global level of most scales is trivial to define, the local level of a scale is less clear. What becomes clear in the definition process between global and local, is the ambiguity and often arbitrary nature of defining what the reality of the system being examined is. Normative choices have to be made, especially for defining research questions and hypotheses, i.e., what is a part of the specific system, what is not considered to be a part of it, and what is an important element that you want to know about? This has implications for the relationships and outcomes you want to investigate. '''Standardization of system definitions, or system defining [[Glossary|processes]], would also be a useful consensus activity to increase comparability and exchange within system science.''' Entire fields of research have thus emerged around normative understandings of what the optimal system level unit to examine might be, such as rural sociology, landscape ecology, coastal governance or micro-economics. What is evident - and becomes unclear - is the spectrum of definitions and choices between the two ends of a scale, and the degree to which the categorical differences in system boundary definitions are meaningful for how we analyse and interpret their functionality.\n\n[[File:System Boundaries - Farm.png|600px|thumb|center|'''Defining the system boundaries influences the scope and content of analysis'''. Source: [https:\/\/www.researchgate.net\/publication\/323959122_D11_Report_on_resilience_framework_for_EU_agriculture Meuwissen et al. 2018.]]]\n\n== Defining System Boundaries ==\nOut of these challenges to define system boundaries we recognize a clear gap on how to approach a globally coherent definition, which can recognize the wide array of contextual differences in how systems are defined and measured. We need to consider that some systems are either divided from larger or parallel systems, or that smaller systems are nested into other systems. \n\n'''System boundaries can be defined based on a variety of parameters.''' For the simplicity of the argument, we focus on one parameter out of several, yet prefer not to discuss in depth the interaction of different parameters. Many people would define such interactions as [[Agency, Complexity and Emergence|complexity]] within systems thinking, but the examination of system complexity is premised on a coherent understanding of what is in and out of the system being examined. This is an inherent precondition for analysis, which is often discussed or taken as an assumption that is often not clearly defined. For example, spatially explicit parameters are an intuitive aspect of many defining processes, and therefore shape our perceptions of what an appropriate system boundary might be. To use a spatial example: '''a larger city can be divided into smaller neighbourhoods, while neighbourhoods can be defined based on different parameters, such as function, ethnic groups or cultural settings.''' While the definition of 'neighbourhoods' can be also informal, it is also well established within urban planning. There is an evident awareness that system boundaries exist, yet many would not be able to define why two systems differ. It is overall easier to define system boundaries based on spatial parameters, such as structural elements, ecosystems, or the design of a city, while informal parameters such as those based on normative dimensions are less clearly definable. Being initially clear within a research project about the boundaries of a system may help to clarify a ranking of what relevant parameters of the system are in terms of its boundaries, but also considering the variance of these parameters within a system. Basically, some parameters can be dramatically different within a system than outside, have no relevance within a system, or can have only relevance within the system. All [[Glossary|assumption]]s are plausible parameters to serve as system boundaries.","System Thinking allows for a shift in the perception of [[Causality|causality]]. Instead of assuming linear causality (A causes B, B causes C) it allows for the integration of further influencing factors, as well as a more neutral depiction of the system at hand. C may now be seen as a property that [[Agency, Complexity and Emergence|emerges]] from the relation between A and B, instead of perceiving it as a direct consequence of B. Haraldsson (2004, p.21) provides an illustrative example here: \"We start by asking the initial question: \"I want to understand how water flows into the glass and what I do to fill it up.\" Instead of looking at the action from an individual point of view, where the \"I am\" is the active part and at the centre of focus, we shift our perception to the structure of the action. The \"I am\" simply becomes a part of the feedback process, not standing apart from it. Suddenly we have shifted out attention to the structure of the behavior and we can observe that the structure is causing the behavior. (...) We have now transformed the traditional linear thinking into a circular argument.\"\n\n==== Causal Loop Diagrams ====\nCausal Loop Diagrams (CLDs) are a crucial method for System Analysis since they allow for the modelling of systems. They make it possible \"(...) to 'map' the complexity of a problem of interest\" (McGlashan et al. 2016, p.2). A CLD allows us to not suppose a linear relationship between system elements, but rather understand cause and effect (1). It makes it possible to \"(...) understand how a behavior has been manifesting itself in a system so we can develop strategies to work with, or counteract the behavior. We also want to know to what extent and how the problem is connected with other 'systems'\" (Haraldsson 2004, p.20). By developing a CLD, one can visualize and thus better understand the feedback mechanisms that happen in a system (1). Therefore, they enable both a look at the structure as well as the processes in a system. The CLD also helps transfer system understanding (1). CLDs \"(...) provide a language for articulating our understanding of the dynamic, interconnected nature of our world. (...) By stringing together several loops, we can create a coherent story about a particular problem or issue.\" (Team TIP 2011, p.1)\n\n'''A Causal Loop Diagram consists of rather few elements:''' Variables, Causal Relationships and Polarity.\n\n'''Variables''' represent the individual elements of a system, but are adapted to the research intent underlying the creation of the CLD. They are \"(...) dynamic causes or effects of the problem under study\" (McGlashan et al. 2016, p.2). For example, in a CLD that recreates a forest ecosystem, the system element may be the soil, whereas the variable that we use in the CLD would be, for example, the change in the amount of carbon in the soil - if this is of interest for the research.\n\nBetween these variables, the CLD depicts '''causal relationships'''. Causal relationships \"(...) are represented by arrows that represent a directed cause from one variable to another\" (McGlashan et al. 2016, p.2). When a number of causal relationships (i.e. arrows) connects two or more variables to a circular structure, a loop constitutes.\n\nEach of these causal relationships has one of two polarities (or 'feedbacks'): positive or negative. Note that these are not equal to growth or decline! If the polarity is positive (exhibited by a '+'), the causally related variables change in the same direction: if the first variable grows in size, the other does, too; and if the first variable decreases, so does the second one. Negative polarity ('-') works into opposite directions: if the first variable decreases, the second one increases - and if the first one increases, the second one decreases (1, 3). The polarities influence the outcome of the systemic processes. \n\nThis is especially true for ''loops'', which arise when an output of a system is circled back and used as one or more inputs, through direct or indirect causal links. '''When there is an even number of negative polarities in a loop, the loop is called 'amplifying' or 'reinforcing', indicated by an (R).''' This includes the case where there are only positive polarities, since this is an even amount of negative polarities, too (zero, to be precise). In a reinforcing loop, some variables may be ever-increasing and others ever-decreasing (this is the case when there are some negative polarities), or all variables are ever-increasing or decreasing (only positive polarities). \n\n'''When there is an odd number of negative polarities in a loop, it is a 'balancing' loop, indicated by a (B).''' You can check whether a loop is reinforcing or balancing by choosing one variable, assuming that it is increasing (or decreasing) and going through the loop. If the behavior of this element leads to the same behavior, again (e.g., an increase in A leads to an increase in A), the loop is reinforcing. If the behavior switches (e.g. an increase in A leads to a decrase in A), it is balancing."],"21":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# '''Select the topic you want to learn more about.''' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don't go to broad - focus on a specific topic. You will not be able to explain \"Economics\" or \"Physics\" in one go.\n# '''Find someone to talk to'''. Ideally, this person does not know anything about this topic. If you don't have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# '''Make notes.''' You will quickly realize yourself which parts of the topic you are not able to explain, and\/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# '''Have a look at your notes and try to find more information.''' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https:\/\/karrierebibel.de\/feynman-methode\/ Karrierebibel]\n* [https:\/\/blog.doist.com\/feynman-technique\/ ToDo-ist]\n* [https:\/\/www.goodwall.io\/blog\/feynman-technique\/ Goodwall]\n* [https:\/\/www.youtube.com\/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.","[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|'''SCOPUS hits per year for Bayesian Inference until 2020.''' Search terms: 'Bayesian' in Title, Abstract, Keywords. Source: own.]]\n'''The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.''' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes' name only became associated with the method in the 1900s (3).\n\n'''The family of methods based on the concept of Bayesian analysis has risen the last 50 years''' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n'''Bayesian analysis relies on using probability figures as an expression of our beliefs about events.''' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is \"prior\", and the probability figure assigned to the prior is called \"prior probability\". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called \"posterior probability\", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|'''The probability distribution for prior, evidence, and posterior.''']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes' Theorem.\n\n==== Bayes' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes' theorem.\n\n'''Conditional Probability''' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n'''Joint Probability''', also called \"conjoint probability\", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n'''Marginal Probability''' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for \"Bayes theorem\" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n''Caution:'' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)","What is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https:\/\/www.britannica.com\/science\/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.\n\n\n== The way forward ==\nIf we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodologial ramifications. '''I can interview individuals, yet this will hardly allow me to prove agency.''' Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy. \n\nThe investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored [[Bayesian inference|Bayesian]] statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging. \n\nThe whole globe - although not a closed system \u2013 can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. '''Emergence is a truly cross-scalar phenomenon.''' Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.\n[[File:Climate model.png|400px|thumb|right|'''Climate models are increasingly getting more accurate, but the complexity and emergence of the global climate system may never be fully understood.''' Source: [https:\/\/blogs.egu.eu\/geolog\/2018\/09\/19\/how-to-forecast-the-future-with-climate-models\/ European Geosciences Union]]]"],"22":["When designing an ANOVA study, great care needs to be taken to have sufficient samples to allow for a critical interpretation of the results. Subsequently, ANOVA experiments became more complex, combining several independent variables and also allowing to correct for so called random factors, which are elements for which the variance is calculated out of the ANOVA model. This allows for instance to increase the sample size to minimise the effects of the variance in an agricultural experiment which is being conducted on several agricultural fields. In this example, agricultural fields are then included as block factor, which allows to minimise the variance inferred by these replications. Hence, the variance of the agricultural fields is tamed by a higher number of replicates. This led to the ANOVA becoming one of the most relevant methods in statistics, yet recent developments such as the reproducibility crisis in psychology highlight that care needs to be taken to not overplay ones hand. Preregistering hypotheses and more recognition of the [[Limitations of Statistics|limitations]] of such designs currently pave a path towards a more critical future of statistical designs. \n\nAnother development that emerged during the last decades is the conducting of so called real-world experiments, which are often singular case studies with interventions, yet typically less or no control of variables. These approaches are slowly being developed in diverse branches of research, and allow to open a [[Meta-Analysis|meta-analytical]] dimension, where a high number of case studies is averaged in terms of the research results. The combination of different studies enables a different perspective, yet currently such approaches are either restricted to rigid clinical trials or to meta-analyses with more variables than cases. \n\nReal-world experiments are thus slowly emerging to bridge experimental rigour with the often perceived messiness of the problems we face and how we engage with them as researchers, knowing that one key answer involving these is the joint learning together with stakeholders. This development may allow us to move one step further in current [[System Thinking & Causal Loop Diagrams|systems thinking]], where still many phenomena we cannot explained are simply labeled as complex. We will have to acknowledge in the future which phenomena we may begin to understand in the future, and which phenomena we may never be able to fully understand. [[Non-equilibrium dynamics|Non-equilibrium theory]] is an example where unpredictable dynamics can still be approaches by a scientific theory. Chaos theory is another example, where it is clear that we may not be able to grasp the dynamics we investigate in a statistical sense, yet we may be able to label dynamics as chaotic and allow a better understanding of our own limitations. Complexity is somewhat inbetween, leaning partly towards the explainable, yet also having stakes in the unexplainable dynamics we face. '''Statistics is thus at a crossroad, since we face the limitations of our approaches, and have to become better in taking these into account.''' \n\nWithin statistics, new approches are rapidly emerging, yet to date the dominion of scientific disciplines still haunts our ability to apply the most parsimonious model. Instead, the norms of our respective discipline still override our ability to acknowledge not only our limitations, but also the diverse biases we face as statisticians, scientists and as a people. Civil society is often still puzzled how to make sense of our contributions that originate in statistics, and we have to become better in contextualising statistical results, and translate the consequences of these to other people. '''To date, there is a huge gap between [[Ethics and Statistics|statistics and ethics,]] and the 20th century has proven that a perspective restricted to numbers will not suffice, but instead may contribute to our demise.''' We need to find ways to not only create statistical results, but also face the responsibility of the consequences of such analyses and interpretations. In the future, more people may be able to approximate knowlegde though statistics, and to be equally able to act based on this knowledge in a reasonable sense, bridging societal demands with our capacity for change. \n\n\n==What was missing==\nEverybody who actively participated in this module now has a glimpse of what statistics is all about. I like to joke that if statistics is like the iceberg that sank the Titanic, then you now have enough ice for a Gin-Tonic, and you should enjoy that. The colleagues I admire for their skills in terms of statistics spent several thousand hours of their life on statistics, some even tens of thousands of hours. By comparison, this module encapsulates about 150 hours, at least according to the overall plan. Therefore, this module focuses on knowledge. It does not include the advanced statistics that demand experience. Questions of models reductions, [[Mixed Effect Models|mixed effect models]], [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|multivariate statistics]] and many other approaches were never touched upon, because this would have been simply too much.","By increasing the '''[https:\/\/sciencing.com\/meaning-sample-size-5988804.html sample size]''', it is possible to test the hypothesis according to a certain probability, and to generate a measure of reliability. The larger the sample is, the higher is the statistical power to be regarded. Within controlled experiments the so-called '''[http:\/\/www.stat.yale.edu\/Courses\/1997-98\/101\/expdes.htm treatments]''' are typically groups, where continuous gradients are converted into factors. An example would be the amount of fertilizer, which can be constructed into \u201clow\u201d, \u201cmiddle\u201d and \u201dhigh\u201d amount of fertilizer. This allows a systematic testing based on a smaller number of replicates. The number of treatments or '''factor levels''' defines the '''[https:\/\/www.youtube.com\/watch?v=Cm0vFoGVMB8 degrees of freedom]''' of an experiment. The more levels are tested, the higher does the number of samples need to be, which can be calculated based on the experimental design. Therefore, scientists design their experiments very clearly before conducting the study, and within many scientific fields are such experimental designs even submitted to a precheck and registration to highlight transparency and minimize potential flaws or manipulations. \n[[File:Fertilization-campaign-overview-ultrawide.jpg|thumb|right|What increased the yield, the fertilizer or the watering levels? Or both?]]\nSuch experimental designs can even become more complicated when '''[https:\/\/statisticsbyjim.com\/regression\/interaction-effects\/ interaction effects]''' are considered. In such experiments, two different factors are manipulated and the interactions between the different levels are investigated. A standard example would be quantification of plant growth of a specific plant species under different watering levels and amounts of fertilizer. Taken together, it is vital for researchers conducting experiments to be versatile in the diverse dimensions of the design of experiments. Sample size, replicates, factor levels, degrees of freedom and statistical power are all to be considered when conducting an experiment. Becoming versatile in designing such studies takes practice.\n\n==How do I compare more than two groups ?==\n[[File:Sunset-field-of-grain-5980.jpg|thumb|right|How to increase the yield systematically?]]\nPeople knew about the weather, soils, fertilizer and many other things, and this is how they could maximize their agricultural yield. Or did they? People had local experience but general [[Glossary|patterns]] of what contributes to a high yield of a certain crop were comparably anecdotal. Before Fisher arrived in agriculture, statistics was less applied. There were applied approaches, which is why the [[Simple_Statistical_Tests#One_sample_t-test|t-test]] is called student test. However, most statisticians back then did statistics, and the agricultural folks did agriculture.\n\n[https:\/\/www.britannica.com\/biography\/Ronald-Aylmer-Fisher Fisher] put these two things together, and they became one. His core question was how to increase yield. For this, there was quite an opportunity at that time. Industrialisation contributed to an exponential growth of many, many things (among them artificial fertilizer) which enabled people to grow more crops and increase yield. Fisher was the one who made the question of how to increase yield systematically. '''By developing the Analysis of Variance ([[ANOVA]]), he enabled comparison of more than two groups in terms of a continuous phenomenon.''' He did this in a way that you could, for example, compare which fertilizer would produce the highest yield. With Fisher's method, one could create an experimental design, fertilize some plants a little bit, some plants more, and others not at all. This enabled research to compare what is called different treatments, which are the different levels of fertilizer; in this case: none, a little, and more fertilizer. This became a massive breakthrough in [https:\/\/www.youtube.com\/watch?v=9JKY74fPNVM agriculture], and Fisher's basic textbook became one of the most revolutionary methodological textbooks of all time, changing agriculture and enabling exponential growth of the human population.\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...]]\nOther disciplines had a similar problem when it came to comparing groups, most prominently [https:\/\/qsutra.com\/anova-in-pharmaceutical-and-healthcare\/ medicine] and psychology. Both disciplines readily accepted the ANOVA as their main method in early systematic experiments, since comparing different treatments is quite relevant when it comes to their experimental setups. Antibiotics were one of the largest breakthroughs in modern medicine, but how should one know which antibiotics work best to cure a specific disease? The ANOVA enables a clinical trial, and with it a systematic investigation into which medicine works bests against which disease. This was quite important, since an explanation on how this actually works was often missing up until today. Much of medical research does not offer a causal understanding, but due to the statistical data, at least patterns can be found that enable knowledge. This knowledge is, however, mostly not causal, which is important to remember. The ANOVA was an astounding breakthrough, since it enabled pattern recognition without demanding a causal explanation.","[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called '[https:\/\/www.youtube.com\/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https:\/\/en.wikipedia.org\/wiki\/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https:\/\/www.thoughtco.com\/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. '''If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.''' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIn addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs. \n\n\n== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis."],"23":["==Replication of experiments==\n[[File:Bildschirmfoto 2020-05-21 um 17.10.27.png|thumb|One famous example from the discipline of psychology is the Milgram shock experiment carried out by Stanley Milgram a professor from the Yale University in 1963.]]\nField experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for [https:\/\/en.wikipedia.org\/wiki\/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 agriculture] and [https:\/\/revisesociology.com\/2016\/01\/17\/field-experiments-sociology\/ other fields] of production to thrive, but then also did medicine, [https:\/\/www.simplypsychology.org\/milgram.html psychology], ecology and even [https:\/\/www.nature.com\/articles\/s41599-019-0372-0 economics] use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased. Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam\u2019s razor also when designing a study.\n\n==External Links==\n===Articles===\n\n[https:\/\/explorable.com\/field-experiments Field Experiments]: A definition\n\n[https:\/\/www.tutor2u.net\/psychology\/reference\/field-experiments Field Experiments]: Strengths & Weaknesses\n\n[https:\/\/en.wikipedia.org\/wiki\/Field_experiment#Examples Examples of Field Experiments]: A look into different disciplines\n\n[https:\/\/conjointly.com\/kb\/experimental-design\/ Experimental Design]: Why it is important\n\n[https:\/\/isps.yale.edu\/node\/16697 Randomisation]: A detailed explanation\n\n[https:\/\/www.sare.org\/Learning-Center\/Bulletins\/How-to-Conduct-Research-on-Your-Farm-or-Ranch\/Text-Version\/Basics-of-Experimental-Design Experimental Design in Agricultural Experiments]: Some basics\n\n[https:\/\/www.ndsu.edu\/faculty\/horsley\/RCBD.pdf Block Design]: An introduction with some example calculations\n\n[https:\/\/www.sare.org\/Learning-Center\/Bulletins\/How-to-Conduct-Research-on-Your-Farm-or-Ranch\/Text-Version\/Basics-of-Experimental-Design\/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https:\/\/www.theanalysisfactor.com\/the-difference-between-crossed-and-nested-factors\/ Difference between crossed & nested factors]: A short article\n\n[https:\/\/www.ohio.edu\/plantbio\/staff\/mccarthy\/quantmet\/lectures\/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https:\/\/support.minitab.com\/en-us\/minitab\/18\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/regression-models\/model-reduction\/ Model reduction]: A helpful article\n\n[https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https:\/\/en.wikipedia.org\/wiki\/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https:\/\/www.simplypsychology.org\/milgram.html Field Experiments in Psychology]: A famous example\n\n[https:\/\/www.nature.com\/articles\/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https:\/\/revisesociology.com\/2016\/01\/17\/field-experiments-sociology\/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https:\/\/www.youtube.com\/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction","[[File:Bildschirmfoto 2020-06-23 um 16.42.31.png|400px|right|thumb|'''The maxim ''Publish or Perish'' is dominating most of the scientific world today.''' Especially universities and scientific journals play an important role in this.]]\n\nThere are many facets that could be highlighted under such a provocative heading. Since Larry Laudan, it has become clear that the assumption that developments of science which are initiated by scientists are reasonable, is a myth at best. Take the example of one dominating [[Glossary|paradigm]] in science right now: Publish or perish. This paradigm highlights the current culture dominating most parts of science. If you do not produce a large amount of peer-reviewed publications, your career will not continue. This created quite a demand on statistics as well, and the urge to arrive at significant results that probably created frequent violations of Occam's razor (that is, things should be as complex as necessary, but as simple as possible). The reproducibility crisis in psychology is one example of these developments, yet all other disciplines building on statistics struggle to this end, too. Another problem is the fact that with this [[Questioning the status quo in methods|ever-increasing demand for \"scientific innovation\"]], models evolve, and it is hard to catch up. '''Thus, instead of having robust and parsimonious models, there are more and more unsuitable and overcomplicated models.''' The level of statistics at an average certainly increased. There are counter-examples where this is not necessarily the case, and rightly so. In medicine, for instance, the canon of statistics is fairly established and solidified for large parts of the research landscape. Thus, while many innovative publications are exploring new ways of statistics, and a highly innovative to this end, there is always a well-defined set of statistical methods that research can default on. Within many other branches of science, however, there is a certain increase in errors or at least slippery slopes. Statistics are part of the publish or perish, and with this pressure still rising, unreasonable actions in the application of statistics may still increase. Many other examples exist to this end, but this should highlight that statistics is not only still developing further, but statistics should also keep evolving, otherwise we may not be able the diversity of epistemological knowledge it can offer.\n\n== The imperfection of measurements ==\nAs a next step, let us look at the three pillars of Western ethics and how these are intertwined with statistics. We all discuss how we act reasonable, how we can maximize our happiness and how we live together as people. Hence reason, utilitarianism and the social contract are all connected to statistics in one way or the other. While this complicated relation may in the future be explored in more than a short wiki entry, lets focus on some examples here.\n\nOne could say that utilitarianism created the single largest job boost for the profession of statisticians. From predicting economic development, to calculating engineering problems, to finding the right lever to tilt elections, statistics are dominating in almost all aspects of modern society. '''Calculating the maximisation of utility is one of the large drivers of change in our globalised economy.''' But it does not stop there. Followers on social media and reply time to messages are two of many factors how to measure success in life these day, often draining their direct human interactions in the process, and leading to distraction or even torment. Philosophy is deeply engaged in discussing these conditions and dynamics, yet statistics needs to embed these topics, which are strongly related to ethics, more into their curriculum. If we become mercenaries for people with questionable goals, then we follow a long string of people that maximised utility for better or worse. History teaches us of the horrors that were conducted in the process of utility maximisation, and we need to end this string of willing aids to illegitimate goals. Instead, statistics should not only be about numbers, but also about the fact that these numbers have meaning. Numbers are not abstract representations of the world, but can have different meanings for different people. Hence numbers can add information that is missing, and can serve as a starting point for an often necessary deeper reflection.","When designing an ANOVA study, great care needs to be taken to have sufficient samples to allow for a critical interpretation of the results. Subsequently, ANOVA experiments became more complex, combining several independent variables and also allowing to correct for so called random factors, which are elements for which the variance is calculated out of the ANOVA model. This allows for instance to increase the sample size to minimise the effects of the variance in an agricultural experiment which is being conducted on several agricultural fields. In this example, agricultural fields are then included as block factor, which allows to minimise the variance inferred by these replications. Hence, the variance of the agricultural fields is tamed by a higher number of replicates. This led to the ANOVA becoming one of the most relevant methods in statistics, yet recent developments such as the reproducibility crisis in psychology highlight that care needs to be taken to not overplay ones hand. Preregistering hypotheses and more recognition of the [[Limitations of Statistics|limitations]] of such designs currently pave a path towards a more critical future of statistical designs. \n\nAnother development that emerged during the last decades is the conducting of so called real-world experiments, which are often singular case studies with interventions, yet typically less or no control of variables. These approaches are slowly being developed in diverse branches of research, and allow to open a [[Meta-Analysis|meta-analytical]] dimension, where a high number of case studies is averaged in terms of the research results. The combination of different studies enables a different perspective, yet currently such approaches are either restricted to rigid clinical trials or to meta-analyses with more variables than cases. \n\nReal-world experiments are thus slowly emerging to bridge experimental rigour with the often perceived messiness of the problems we face and how we engage with them as researchers, knowing that one key answer involving these is the joint learning together with stakeholders. This development may allow us to move one step further in current [[System Thinking & Causal Loop Diagrams|systems thinking]], where still many phenomena we cannot explained are simply labeled as complex. We will have to acknowledge in the future which phenomena we may begin to understand in the future, and which phenomena we may never be able to fully understand. [[Non-equilibrium dynamics|Non-equilibrium theory]] is an example where unpredictable dynamics can still be approaches by a scientific theory. Chaos theory is another example, where it is clear that we may not be able to grasp the dynamics we investigate in a statistical sense, yet we may be able to label dynamics as chaotic and allow a better understanding of our own limitations. Complexity is somewhat inbetween, leaning partly towards the explainable, yet also having stakes in the unexplainable dynamics we face. '''Statistics is thus at a crossroad, since we face the limitations of our approaches, and have to become better in taking these into account.''' \n\nWithin statistics, new approches are rapidly emerging, yet to date the dominion of scientific disciplines still haunts our ability to apply the most parsimonious model. Instead, the norms of our respective discipline still override our ability to acknowledge not only our limitations, but also the diverse biases we face as statisticians, scientists and as a people. Civil society is often still puzzled how to make sense of our contributions that originate in statistics, and we have to become better in contextualising statistical results, and translate the consequences of these to other people. '''To date, there is a huge gap between [[Ethics and Statistics|statistics and ethics,]] and the 20th century has proven that a perspective restricted to numbers will not suffice, but instead may contribute to our demise.''' We need to find ways to not only create statistical results, but also face the responsibility of the consequences of such analyses and interpretations. In the future, more people may be able to approximate knowlegde though statistics, and to be equally able to act based on this knowledge in a reasonable sense, bridging societal demands with our capacity for change. \n\n\n==What was missing==\nEverybody who actively participated in this module now has a glimpse of what statistics is all about. I like to joke that if statistics is like the iceberg that sank the Titanic, then you now have enough ice for a Gin-Tonic, and you should enjoy that. The colleagues I admire for their skills in terms of statistics spent several thousand hours of their life on statistics, some even tens of thousands of hours. By comparison, this module encapsulates about 150 hours, at least according to the overall plan. Therefore, this module focuses on knowledge. It does not include the advanced statistics that demand experience. Questions of models reductions, [[Mixed Effect Models|mixed effect models]], [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|multivariate statistics]] and many other approaches were never touched upon, because this would have been simply too much."],"24":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> ''Is everyone on the same page?''\n<br> ''Are there important issues that have been neglected so far?''\n<br> ''Is there unspoken dissonance?''\n<br> ''Is there an elephant in the room?''\n<br> ''What are we actually talking about?''\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> ''What song would you choose to characterize the current state of discussion, and why?'' <br> ...\n\n== Links ==\nhttps:\/\/www.methodenkartei.uni-oldenburg.de\/uni_methode\/blitzlicht\/\n<br> https:\/\/www.bpb.de\/lernen\/formate\/methoden\/62269\/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar M\u00f6lleken.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique''' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe '''Dreamers'''...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: ''Which ideas come to mind? What would be an ideal solution to the problem?''\n\nThe '''Realists'''...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: ''How does the idea feel? How could it be implemented? Who should do it and at what cost?''\n\nThe '''Critics'''...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: ''What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?''\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. '''The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.''' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to 'neutralize' their minds) in order to understand the other roles' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He\/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n''Sources:''\n* Tools Hero - [https:\/\/www.toolshero.com\/creativity\/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https:\/\/arbeitdigital.de\/wirtschaftslexikon\/kreativitaetstechniken\/walt-disney-methode\/ Walt-Disney-Methode]\n* Karrierebibel - [https:\/\/karrierebibel.de\/disney-methode\/ Disney Methode: Denkblockaden \u00fcberwinden]\n* Impulse - [https:\/\/www.impulse.de\/management\/selbstmanagement-erfolg\/walt-disney-methode\/3831387.html Walt Disney Methode]\n\n[https:\/\/www.youtube.com\/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br\/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe World Caf\u00e9 is a method for facilitating discussions in big groups. With many participants, discussion rounds tend to be sprawling, slow and dominated by strong speakers. If you want to facilitate a discussion that is more effective, energetic, and inclusive, the World Caf\u00e9 is a helpful technique. It divides participants into moderated subgroups, who then wander together through a parcours of stations with different questions, all the while the atmosphere is relaxed and casual like in a caf\u00e9.\n\n== Goals ==\nSplitting big groups into subgroups fosters inclusive, energetic, effective and in-depth discussions: \n* reserved speakers can feel more comfortable speaking in a smaller group\n* the parcours format allows people to physically move through the room in between discussions\n* the moderator can steer the discussion towards unexplored issues with every new subgroup\n* every participant contributed to the collective results in the end\n\n== Getting started ==\nDepending on group size, room capacities and questions you want to discuss, different stations are set up (can be tables, boards, flipcharts etc.) with a moderator who will introduce the question and lead the discussion. The participants will be divided into as many subgroups as there are stations. Each subgroup will visit every station. The moderator welcomes the subgroup participants and introduces the question. Within a given time slot, the subgroups will discuss the question and write down their ideas and insights, before they then wander to the next station. The moderators remain with their station and welcome the next group. They present the question plus a broad overview of the insights of the former group and deepen the discussion with the new group. After the parcours has been completed by all subgroups, the moderators present the collective discussion results of each question to the full group. \n\nIt is helpful to have one moderator who is in charge of the clock and who manages the parcours direction.\n\n== Links & Further reading ==\nhttp:\/\/www.theworldcafe.com\/\n\nhttps:\/\/www.methodenkartei.uni-oldenburg.de\/uni_methode\/world-cafe\/\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Dagmar M\u00f6lleken."],"25":["[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.","Since not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","[[File:ConceptRegression.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Regressions]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Regression analysis tests whether a relation between two continuous variables is positive or negative, how strong the relation is, and whether the relation is significantly different from chance.\n<br\/>\n\n'''Note:''' This entry revolves around simple linear regressions and the fundamentals of regression analysis. For more info, please refer to the entries on [[Causality and correlation]] as well as [[Generalized Linear Models]].\n\n__TOC__\n\n== Background ==\n[[File:Regression Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Regression Analysis until 2020.''' Search terms: 'Regression' in Title, Abstract, Keywords. Source: own.]]\nThe question whether two continuous variables are linked initially emerged with the [[History of Methods|rise of data from astronomical observation]]. Initial theoretical foundations were laid by Gauss and Legendre, yet many relevant developments also happened much later. '''At their core, the basics of regressions revolved around the importance of the [[Data_distribution#The_normal_distributionnormal_distribution|normal distribution]].''' While Yule and Pearson were more rigid in recognising the foundational importance of the normal distribution of the data, Fisher argued that only the response variable needs to follow this distribution. This highlights yet another feud between the two early key innovators in statistics - Fisher and Pearson - who seem to be only able to agree to disagree on each other. The regression analysis was rooted famously in an observation by Galton called ''regression towards the mean'' - which proclaims that within most statistical samples, an outlier point is more likely than not followed by a data point that is closer to the mean. This proves to be a natural law for many dynamics that can be observed, underlining the foundational importance of the normal distribution, and how it translates into our understanding of [[Glossary|patterns]] in the world. \n\nRegressions rose to worldwide recognition through econometrics, which used the increasing wealth of data from nation states and other systems to find relations within market dynamics and others patterns associated to economics. Equally, the regression was increasingly applied in medicine, engineering and many other fields of science. The 20th century became a time ruled by numbers, and the regression was one of its most important methods. '''Today, it is commonplace in all branches of science that utilise quantitative data to analyze data through regressions''', including economics, social science, ecology, engineering, medicine, psychology, and many other branches of science. Almost all statistical software packages allow for the analysis of regressions, the most common software solutions are R, SPSS, Matlab and Python. Thanks to the computer revolutions, most regressions are easy and fast in their computation, and with the rising availability of more and more data, the regression became the most abundantly used simple statistical model that exists to date.\n\n\n== What the method does ==\n[[File:Linear Regression Example.png|450px|thumb|right|'''An exemplary linear regression. It shows the distribution of the data points alongside two axes, and the regression line.''' Source: [https:\/\/en.wikipedia.org\/wiki\/Regression_analysis#\/media\/File:Linear_regression.svg Sewaqu, Wikipedia]]]\nRegressions statistically test the dependence of one continuous variable with another continuous variable. Building on a calculation that resolves around ''least squares'', regression analysis can test whether a relation between to continuous variables is positive or negative, how strong the relation is, and whether the relation is significantly different from chance, i.e. following a non-random pattern. This is an important difference to [[Correlations]], which only revolve around the relation between variables without assuming - or testing for - a causal link. Thus, identifying regressions can help us infer predictions about future developments of a relation.\n\n'''Within a regression analysis, a dependent variable is explained by an independent variable, both of which are [[Data_formats#Continuous_data|continuous]].''' At the heart of any regression analysis is the optimisation of a regression line that minimises the distance of the line to all individual points. In other words, the ''least square'' calculation maximizes the way how the regression line can integrate the sum of the squares of all individual data points to the regression line. The line can thus indicate a negative or positive relation by a negative or positive estimate, which is the value that indicates how much the y value increases if the x value increases."],"26":["'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and\/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row\/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=\"R\" inline>heatmap()<\/syntaxhighlight> function and '''mtcars''' dataset.\nIt is important to note that the <syntaxhighlight lang=\"R\" inline>heatmap()<\/syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=\"R\" inline>as.matrix()<\/syntaxhighlight> function.\n<syntaxhighlight lang=\"R\" line>\ndata(\"mtcars\")\nmatcars <- as.matrix(mtcars)\n<\/syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=\"R\" line>\nfullcolnames <- c(\"Miles per Gallon\", \"Number of Cylinders\",\n                  \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                  \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                  \"Number of Gears\", \"Number of Carburetors\")\n<\/syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = \"column\", labCol = fullcolnames, \n        margins = c(11,5))\n<\/syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let\u2019s look at the feature <syntaxhighlight lang=\"R\" inline>\u201cNumber of Carburetors\u201d<\/syntaxhighlight>. We can see that '''Maserati Bora''' has the darkest color, hence it has the largest number of carburetors, followed by '''Ferrari Dino''', which has the second-largest number of carburetors. While other models such as '''Fiat X1-9''' or '''Toyota''' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.","==Explanation of used arguments==\n* <syntaxhighlight lang=\"R\" inline>Colv = NA<\/syntaxhighlight> and <syntaxhighlight lang=\"R\" inline>Rowv = NA<\/syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.\n* <syntaxhighlight lang=\"R\" inline>scale = \u201ccolumn\u201d<\/syntaxhighlight> is used to normalize the columns of the matrix (to absorb the variation between columns). As it was stated previously, normalization is needed due to the algorithm by which the colors are set. Here in our dataset, the values of features \u201cGross horsepower\u201d and \u201cDisplacement\u201d are much larger than the rest. Therefore, without normalization, these two columns will be all marked approximately equally high and all the other columns equally low. Normalizing means that we keep the relative values in each column but not the real numbers. In the interpretation sense it means that, for example, the same color of features \u201cMiles per Gallon\u201d and \u201cNumber of Cylinders\u201d of Mazda RX4 does not mean that the actual values are the same or approximately the same (placed in the same bin). It only means that the relative values of each of these cells in corresponding columns are the same or are in the same bin.\n* <syntaxhighlight lang=\"R\" inline>margins<\/syntaxhighlight> is used to fit the columns and rows names into the graph. The reason we used it here is because of the renaming of the columns, which is resulted in longer names that did not fit well by themselves.\n\nColoring options for the heatmap\nThe choice of color for the heatmap is one of the most important aspects of creating an understandable and nice-looking representation of the data. If you do not specify the color (as in the example above) then the default color palette will be applied. However, you can use the argument <syntaxhighlight lang=\"R\" inline>col<\/syntaxhighlight> and choose from a wide variety of palettes for coloring your heatmap.\n\nThere are two options of setting a color palette for the heatmap:\n* First option is to use the palettes from R: <syntaxhighlight lang=\"R\" inline>cm.colors()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>heat.colors()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>rainbow()<\/syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>terrain.color()<\/syntaxhighlight>  or <syntaxhighlight lang=\"R\" inline>topo.colors()<\/syntaxhighlight> \n* The second option is to install color palettes packages such as <syntaxhighlight lang=\"R\" inline>RColorBrewer<\/syntaxhighlight> \n\n==Additional materials==\n* [https:\/\/www.r-graph-gallery.com\/heatmap Other functions for building a heatmap]\n* [https:\/\/www.datanovia.com\/en\/blog\/how-to-normalize-and-standardize-data-in-r-for-great-heatmap-visualization\/ How and why we should normalize data for a heatmap]\n* [https:\/\/vwo.com\/blog\/heatmap-colors\/ How to choose the color palette for a heatmap]\n* [https:\/\/blog.bioturing.com\/2018\/09\/24\/heatmap-color-scale\/ Do's and Dont's in choosing a color palette for a heatmap]\n* [https:\/\/www.displayr.com\/what-is-dendrogram\/ What is a dendrogram]\n* [https:\/\/sustainabilitymethods.org\/index.php\/Clustering_Methods More about clustering methods and how to build a dendrogram in R]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Evgeniya Chetneva.","===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=\"Python\" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap='PuBu');\n<\/syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=\"Python\" line>\nmodel_1 = smf.ols(formula='points ~ ID + quanti', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).\n\nresult = model_1.fit() ## estimates the regression\n\nprint(result.summary()) ## print the result\n<\/syntaxhighlight>\n\n[[File:new_attempt.png]]\n\nLooking at the Breusch-Pagan test, we can see that we cannot reject the assumption of homoscedasticity. \nConsidering the correlation coefficients, no statistically significant relationship can be identified. The positive relationship between quanti and points can be found again, but it is not statistically significant.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_2 = smf.ols(formula='points ~ ID + sex', data=data)\nresult_bp2 = model_2.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp2.resid, result_bp2.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp2_test[1])\n\n# Set the value for maxlags\nmaxlags = 25  # Update this value as needed\n\nresult = model_2.fit(cov_type='HAC', cov_kwds={'maxlags': maxlags})\nprint(result.summary())\n\n# Assuming 'sex' is a column in the DataFrame named 'data'\nsex_counts = data['sex'].value_counts()\n\n# Print the frequency table\nprint(sex_counts)\n<\/syntaxhighlight>\n\n[[File:OLS 2.png]]\n\nBased on the Breusch-Pagan test, the assumption of homoscedasticity needs to be rejected to the 0.1% significance level. Therefore, we correct for heteroscedasticity with HAC (for more details see here)\nLooking at the results, being female (\"sex\") has a large negative effect on points and is highly statistically significant. However, looking at the number of females in the dataset, we need to be very cautious to draw any conclusions. Since there are only four females in the dataset (and 73 males), the sample size is considered too small to make any statements about gendered effects on total points achieved. The correlation between ID and points can be ignored, since the last number of the matricle numbers follow no pattern.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_3 = smf.ols(formula='points ~ ID + exam', data=data)\nresult_bp2 = model_3.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp3.resid, result_bp3.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp3_test[1])\n\nresult = model_3.fit()\nprint(result.summary())\n<\/syntaxhighlight>\n\n[[File:OLS 3.png]]\n\nBased on the Breusch-Pagan test, the asssumption of homoscedasticity cannot be rejected to the 0.1% significance level. \nLooking at the results, no statistically significant result can be found, albeit the slight neative relationship between exam and points can also be found in the OLS."],"27":["Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https:\/\/www.britannica.com\/biography\/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''\n\n==== Before Enlightenment - ''Measure And Solve'' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|'''Mercator world map.''' Source: [https:\/\/de.wikipedia.org\/wiki\/Mercator-Projektion Wikipedia]]] \n'''Another breakthrough that was also rooted in geometry was the compilation of early trade maps.''' While many European maps were detailed but surely not on scale (Ebstorfer Weltkarte), early maps still enabled a supra-regional trade. The real breakthrough was the [[Geographical Information Systems|Mercator map]], which was - restricted to the knowledge at the time - the first map that enabled a clear navigation across the oceans, enabling colonialism and Western dominions and domination of the world. This is insofar highly relevant for methods, because it can be argued that the surplus from the colonies and distant countries was one of the main drivers of the thriving of the European colonial rulers, their economies and consequently, their science. A direct link can be made between the [[Normativity of Methods|inequalities]] that were increased by colonialism and the thriving of Western science, including the development of scientific methods.\n\n[[File:printing-press-2.jpg|300px|thumb|left|'''Invention of the printing press'''. Source: [[https:\/\/www.ecosia.org\/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \nWith a rising number of texts being available through the [https:\/\/www.britannica.com\/biography\/Johannes-Gutenberg invention of printing], the long known method of [[Hermeneutics]], which enabled deep and systematic analysis of writing, was one of the first methods to grow. A link can be made to translations and interpretations of the Bible, and many other religious discourses are indeed not different from deep text analysis and the derivation of interpretations. Hermeneutics go clearly a step further, and hence are one of the earliest and to date still most important scientific methods. [[Thought Experiments]] were another line of thinking that started to emerge more rapidly. By focussing on ''What if'' questions, scientists considered all sorts of questions to derive patterns of even law of nature ([https:\/\/plato.stanford.edu\/entries\/galileo\/ Galileo]) and also directed their view into the future. Thought experiments were in an informal way known since antiquity, yet during medieval times these assumptions became a trigger to question much of what was supposedly known. Through experiments they became early staring points for subsequent more systematic experimentation. \n\n[[File:Somer_Francis_Bacon.jpg|thumb|300px|'''Francis Bacon.''' Source: [https:\/\/de.wikipedia.org\/wiki\/Francis_Bacon Wikipedia]]] \nIt was [https:\/\/plato.stanford.edu\/entries\/francis-bacon\/ Francis Bacon] who ultimately moved out of this dominance of thinking and called for the importance of empirical inquiry. '''By observing nature and its phenomena, we are able to derive conclusion based on the particular.''' Bacon is thus often coined to be the father of 'the scientific method', an altogether misleading term, as there are many scientific methods. However, empiricism, meaning the empirical investigation of phenomena and the derivation of results or even rules triggered a scientific revolution, and also a specialization of different scientific branches. While Leonardo da Vinci (1452-1519) had been a polymath less than a century ago, Bacon (1561-1626) triggered an increase in empirical inquiry that led to the deeper formation of scientific disciplines. Many others contributed to this development: [https:\/\/plato.stanford.edu\/entries\/locke\/ Locke] claimed that human knowledge builds on experience, and [https:\/\/plato.stanford.edu\/entries\/hume\/ Hume] propelled this into skepticism, casting the spell of doubt on anything that is rooted in belief or dogma. By questioning long standing knowledge, science kicked in - among other things - the door of god himself, a circumstance that is too large to be presented here in any sense. What is important however is that - starting from the previous age of diverse approaches to knowledge - the combination of empiricism on the one end, and the associated philosophical developments on the other end, the age of reason had dawned.\n\n[[File:Timeline of methods V2.jpg|600px|frame|center|'''A timeline of major breakthroughs in science.''' Source: own]]","'''Note:''' The German version of this entry can be found here: [[Scientific methods and societal paradigms (German)]].\n\n'''In short:''' This entry discusses how [[Glossary|scientific methods]] have influenced society - and vice versa.\n__NOTOC__\n== The role of scientific paradigms for society ==\nFrom early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar. However, science was for the most part of the last two millennia in a critical relation to the metaphysical, as there was often a quest for ontological truths between religions and science. While the East was more open to allow science to thrive and made active use of its merits; in Europe, many developments were seen as critical, with Galileo Galileo being a prominent example. Since this changed with the [[History of Methods|Enlightenment]], science paved the way for the rise of the European empires, and with it the associated paradigms.\n\n== Three examples for an active interaction ==\nWhile the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples. \n\n==== Medicine ====\nA prominent example of a strong interaction is medicine, which has at its heart the care for the patient. However, this naive assumption cannot hold the diverse paradigms that influenced and build medicine over time. Today, ananmesis - the information gained by a physician by asking specific questions of a patient - gained in importance, and the interdisciplinary conferences of modern treatments combine different expertise with the goal of a more holistic recognition of the diseases or challenges of the individual patient. \n\n==== Engineering ====\nEngineering is another branch of science which builds on a long tradition, and has at its early stages quite literally paved the road for many developments of modernity. While factories and production processes are today also seen more critically, it has become clear already since Marx that the working condition of modern production are not independent of questions of inequality. In addition, production processes are shifting in order to enable more sustainable production processes, indicating another paradigm shift in engineering. \n\n==== Agricultural science ====\nThe last example, agricultural science, is also widely built around positivistic methodology of modern science, allowing of an optimisation of agricultural production in order to maximise agricultural yield, often with dire consequences. The so-called [https:\/\/www.thoughtco.com\/green-revolution-overview-1434948 'Green Revolution'] wreaked havoc on the environment, destroyed local livelihoods across the globe, and untangled traditional social-ecological systems into abusive forms that led ultimately to their demise in many parts of the world. \n\nThese three examples showcase how the development of modern science led to abusive, unbalanced, and often unsustainable developments that would in the long run trigger new paradigms such as the post-modernity, degrowths and other often controversially discussed alternatives to existing paradigms. Science was clearly an accomplice in driving many negative developments, and willingly developed the basis for many methodological foundations and paradigms that were seen in a different light after they were utilised over a longer time.\n\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently, science often acted morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational [[Glossary|change]]. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n\n==Science looking at parts of reality==\nSince the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world. The second mistake is that scientific progress can be seen as continuous (see Laudan: Progress and its Problems), while societal paradigms are often utilising snapshots of scientific theories and tend to ignore further development in the respective branch of science. This makes science in turn vulnerable, as it has to claim responsibility for mistakes society made in interpreting scientific theories, and translating them into societal paradigms. In the following message I will illustrate these capital mistakes of science based on several examples.","== Methodological paradigms over time ==\nConcerning the tensions between new paradigms and the existing methodology, an adaptation of existing methods or a development of new methods altogether may be needed. While for instance in the 1720, following Locke and Bacon, the approach to Newtonian theory was widely inductive, the following decades saw a proclamation of heat, electricity and phenomena of chemistry that could hardly be claimed to be inductively derived, as they are not observable as such. In consequence, a methodology was derived which enabled the hypothesis-forming deduction that should prevail and even dominate for quite some time to come. Lakatos offered a modification to Kuhn's \"revolutions\", as he proclaimed that several alternative \"research programs\" can exists in parallel, and these are interacting, maybe even several theories that are built around a heuristic core. However, Lakatos - equally like Kuhn - still considers empirical evidence or methodology as pivotal, allowing only the measurable reality to be the measure of success of a new paradigm. Again, Laudan introduced a new argument to add to the measure of the success of a theory: instead of relying on how many significant problems a new theory can solve, he raises concern about the truth of theories, and instead suggests to compare how one theory is more effective or progressive than another theory. Kuhn and Lakatos both claimed that a paradigm, and with it the associated branch of science, reaches maturity if it gains enough standing to ignore anomalies, and becomes independent of outside criticism. Both Kuhn and Lakatos consider this to be positive, as it makes this part of science more progressive.\n\nLaudan criticised this notion deeply, and considered their view on history of science widely flawed and constructed. In addition, history of science looks at parts of reality, beyond the illusion that it is rational agents acting as scientists. This notion of parts of reality can be linked to Roy Bhaskars view that all science can only unlock parts of reality that are not necessarily connected or can be meaningfully connected, since some parts of reality cannot be observed. This is an important connection towards Laudan, who claims that we have not yet understood rational scientific choice, yet this understanding is a precondition to investigate the social background that the respective science is embedded in. What I call the ''grand abduction'' here is hence the seamless interaction between science and society, where we have to recognise that these two realms are not two different entities, but instead are often embedded, integrated, and at times cannot be differentiated at all. While much of positivism often claimed a deductive position, societal development surely operates at longer time scales. Society has questions that science may answer, and demands that science needs to fulfill. Science has willingly fulfilled many demands of society, and has also contributed to many developments in society, many of which are rightly criticised, while other developments also lead to positive merits. However, following Laudan we should not only question that scientists are objective, but following Bhaskar we also have to question their claim of trying to explain objective reality. Neither is science rational, nor can scientists be framed as rational actors, nor can society claim a complete disconnect from the proposed ivory towers of science.\n\n\n== A way out of the current dilemma of societies doubt towards science, and the arrogance of science towards society ==\nWhy is this relevant today? Following Bhaskar we could argue that many parts of reality will never be unlocked by us, despite being part of reality. These phenomena will never be part of our reality, and shall not concern us further. This is the first obstacle societal paradigms and society as such face today, since the overarching acceptance of the limitations of science does not exist. Instead, the pendulum swings between criticism of science - often as a means to an end as for instance climate change deniers attempt - or a naive surprise whenever scientific results change, calling the recognition of so-called 'facts' into question. The recognition of critical realism that permanent realities may not exist is especially quite alien from the current societal debate in the Western world, where the distance between a acceptance or rejection of scientific paradigm grew larger of the last years. Regarding the changeability of paradigms, we have to follow Bhaskar in recognising that it can be wise to act based on certain scientific results, yet we also need to be critical. Blindly following everything that scientists claim does not work, because as Laudan highlighted, we cannot assume that scientific actors are always rational. Post truths and fake news are thus facets of knowledge that can be equally driven by irrational scientists or by an ignorant thrive for knowledge by societal actors. Surely, many more complex phenomena are part of the reality of post truths, fake news and associated challenges we currently face. Yet by blaming either scientists or society we will fail to overcome these challenges. Instead of claiming what science ought to be, we should shift to the responsibility of the individual scientist, and building on a critical 'history of ideas' will allow for a better understanding of how scientific [[Glossary|innovation]] happened in the past. \n\nTo summarise, science and society were never as disconnected as it was framed in the past. Instead, such constructed realities were built with misleading purposes, and during the last decades, philosophy of science has increasingly tried to solve these problems. Considering the current state and validity of knowledge debates in science and society, we can clearly claim that the responsibility of the individual researcher is a good starting point in order to further overcome these challenges. Current scientific methodologies still widely resolve around dogmas and rigid traditions that participated and built the challenges we currently face. Recognising these flaws is a first step in order to overcome these problems. Seriously engaging with this will be one of the leading challenges of our generation."],"28":["'''In short:''' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically '''barplots'''. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.\n\n'''Correlation plots''' ('xyplots') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a [[Regression Analysis|regression]] line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.\n\n'''Boxplots''' are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant.\n\n[[File:Xyplot.png|250px|thumb|left|'''A Correlation plot.''' The line shows the regression, the dots are the data points.]]\n[[File:Boxplot3.png|250px|thumb|right|'''Boxplots.''']]\n[[File:2Barplots.png|420px|thumb|center|'''Barplots.''' The left diagram shows absolute, the right one relative Barplots.]]\n\n\n[[File:Histogram structure.png|300px|thumb|right|'''A Histogram.''']]\nA '''histogram''' is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n\nSimple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|'''A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.''' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|'''A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.''' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]","[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|'''A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic's importance in the research, and the connections show if topics were covered alongside each other.''' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n'''Descriptive Infographics''' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.\n\n\n'''Of course, there is more.''' While the figures introduced above represent a vast share of the visual representations of data that you will encounter, there are different forms that have not yet been touched. '''We have found the website [https:\/\/www.data-to-viz.com\/#connectedscatter \"From data to Viz\"] to be extremely helpful when choosing appropriate data visualisation.''' You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.\n\n\n== How to visualize data in R ==\n'''The following overview includes all forms of data visualisation that we consider important.''' <br>\nBased on your data, have a look which forms of visualisation might be relevant for you. Just hover over the individual visualisation type and it will show you its name. It will also show you a quick example which this kind of visualisation might be helpful for. '''By clicking, you will be redirected to a dedicated entry with exemplary R code.'''<br>\n\nTip: If you are unsure whether you have qualitative or quantitative data, have a look at the entry on [[Data formats]]. Keep in mind: categorical (qualitative) data that is counted in order to visualise each category's occurrence, is not quantitative (= numeric) data. It's still qualitative data that is just transformed into count data. So the visualisations on the left do indeed display some kind of quantitative information, but the underlying data was always qualitative.","= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n<\/imagemap>\n\n'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n<\/imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as 'discrete' data.\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. Abundance data is referred to as 'integer' in R, i.e. it exists in full numbers, and continuous data is 'numeric' - it has a comma.\n\n\n== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n<\/imagemap>\n\n'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n<\/imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics."],"29":["[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"|  '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>\n\n__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and\/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be ''noisy'' or the outputs are expected to be different in each case, you cannot hand-craft the \"[[Glossary|rules]]\" that account for ''every'' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: '''Machine Learning'''. The core idea behind Machine Learning is that instead of being required to hand-craft ''all'' the rules that take inputs and provide outputs in a fairly accurate manner, you can ''train'' the machine to ''learn'' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term \"Machine Learning\" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to \"learn\" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> pairs where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ..., y_n)<\/syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called ''true label''. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> and one of the many ''training algorithms'', you can use the model with new data <syntaxhighlight lang=\"text\" inline>(X_new)<\/syntaxhighlight> to make predictions <syntaxhighlight lang=\"text\" inline>(y_hat)<\/syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]","There is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. \"Support-vector networks.\" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. \"A theory of the learnable.\" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199\u2013231.\n\n(5) [https:\/\/www.vodafone-institut.de\/aiandi\/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305\u2013306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).\n\n== Further Information ==\n* [https:\/\/www.datacamp.com\/community\/tutorials\/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-in-r Machine Learning in R for Beginners]\n* [https:\/\/www.youtube.com\/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https:\/\/www.youtube.com\/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]<\/nowiki>]\n* [https:\/\/medium.com\/s\/story\/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https:\/\/www.repricerexpress.com\/amazons-algorithm-a9\/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal.","In the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]\n\nIf you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification\/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* '''Performance measure (P)''' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the \"learned\" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book \"Learning from Data\" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:\n\n[[File:Mostafa Supervised Learning.png|thumb|The Learning Diagram from \"Learning from Data\" by Abu-Mostafa, Magdon-Ismail, & Lin (2012)]]\n\nThe ''target function'' is assumed to be unknown and the training data is assumed to be based on the target function that is to be estimated. The models that are trained based on the training data are assessed on some error measures (called ''metrics'') which guide the strategy for improving upon the Machine Learning algorithm that was \"learned\" in the previous steps. If you have enough data, and adopted a good Machine Learning strategies, you can expect the learned algorithm to perform quite well with new input data in the future.\n\n'''Supervised learning tasks can be broadly sub-categorized into ''regression learning'' and ''classification learning''.''' \n\nIn ''regression learning'', the objective is to predict a particular value when certain input data is given to the algorithm. An example of a regression learning task is predicting the price of a house when certain features of the house (eg. PLZ\/ZIP code, no. of bedrooms, no. of bathrooms, garage size, energy raging, etc.) are given as an input to the Machine Learning algorithm that has been trained for this specific task.\n\nIn ''classification learning'', the objective is to predict which ''class'' (or ''category'') a given observation\/sample falls under given the characteristics of the observation. There are 2 specific classification learning tasks: ''binary classification'' and ''multiclass classification''. As their names suggest, in binary classification, a given observation falls under one of the two classes (eg. classifying whether an email is spam or not), and in multiclass classification, a given observation falls under one of many classes (eg. in ''digit recognition'' task, the class for a picture of a given hand-written digit can range from 0 to 9; there are 10 classes).\n\n==== Unsupervised Learning ====\nWhereas supervised learning is based on pairs of input and output data, unsupervised learning algorithms function based only on the inputs: <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> and are rather used for recognizing patterns than for predicting specific value or class.\n\nSome of the methods that are categorized under unsupervised learning are ''[https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis Principal Component Analysis]'', ''[[Clustering_Methods|Clustering Methods]]'', ''[https:\/\/en.wikipedia.org\/wiki\/Collaborative_filtering Collaborative Filtering]'', ''[https:\/\/en.wikipedia.org\/wiki\/Hidden_Markov_model Hidden Markov Models]'', ''[https:\/\/brilliant.org\/wiki\/gaussian-mixture-model\/ Gaussian Mixture Models]'', etc.\n\n==== Reinforcement Learning ====\nThe idea behind reinforcement learning is that the \"machine\" learns from experiences much like a human or an animal would [1,2]. As such, the input data \"does not contain the target output, but instead contains some possible output together with a measure of how good that output is\" [1]. As such, the data looks like: <syntaxhighlight lang=\"text\" inline>(X, y, c)<\/syntaxhighlight> where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> is the input, <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ... , y_n)<\/syntaxhighlight> is the list of corresponding labels, and <syntaxhighlight lang=\"text\" inline>c = (c_1, c_2, ..., c_n)<\/syntaxhighlight> is the list of corresponding scores for each input-label pair. The objective of the machine is to perform such that the overall score is maximized.\n\n=== Approaches to Training Machine Learning Algorithms ==="],"30":["[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"|  '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>\n\n__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and\/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be ''noisy'' or the outputs are expected to be different in each case, you cannot hand-craft the \"[[Glossary|rules]]\" that account for ''every'' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: '''Machine Learning'''. The core idea behind Machine Learning is that instead of being required to hand-craft ''all'' the rules that take inputs and provide outputs in a fairly accurate manner, you can ''train'' the machine to ''learn'' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term \"Machine Learning\" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to \"learn\" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> pairs where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)<\/syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ..., y_n)<\/syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called ''true label''. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=\"text\" inline>(X, y)<\/syntaxhighlight> and one of the many ''training algorithms'', you can use the model with new data <syntaxhighlight lang=\"text\" inline>(X_new)<\/syntaxhighlight> to make predictions <syntaxhighlight lang=\"text\" inline>(y_hat)<\/syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]","There is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. \"Support-vector networks.\" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. \"A theory of the learnable.\" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199\u2013231.\n\n(5) [https:\/\/www.vodafone-institut.de\/aiandi\/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305\u2013306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).\n\n== Further Information ==\n* [https:\/\/www.datacamp.com\/community\/tutorials\/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-in-r Machine Learning in R for Beginners]\n* [https:\/\/www.youtube.com\/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https:\/\/www.youtube.com\/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]<\/nowiki>]\n* [https:\/\/medium.com\/s\/story\/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https:\/\/www.repricerexpress.com\/amazons-algorithm-a9\/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal.","=== Approaches to Training Machine Learning Algorithms ===\n\n==== Batch Learning ====\nThis Machine Learning approach, also called ''offline learning'', is the most common approach to Machine Learning. In this approach, a Machine Learning model is built from the entire dataset in one go.\n\nThe main disadvantage of this approach is that depending on the computational infrastructure being used, the data might not fit into the memory and\/or the training process can take a long time. Additionally, models based on batch learning need to be retrained on a semi-regular basis with new training examples in order for them to keep performing well.\n\nSome examples of batch learning algorithms are ''[http:\/\/pages.cs.wisc.edu\/~jerryzhu\/cs540\/handouts\/dt.pdf Decision Trees]''(C4.5, ID3, CART), ''[https:\/\/www.datacamp.com\/community\/tutorials\/support-vector-machines-r Support Vector Machines]'', etc.*\n\n==== Online Learning ====\nIn this Machine Learning approach, data is ordered and fed into the training algorithm in a sequential order instead of training on the entire data set at once. This approach is adopted when the dataset is so large that batch learning is infeasible, or when the nature of the data makes it so that more data is available over time (eg. stock prices, sales data, etc.)\n\nSome examples of online learning algorithms are ''[https:\/\/en.wikipedia.org\/wiki\/Perceptron Perceptron Learning Algorithm]'', ''[https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent stochastic gradient descent] based classifiers and regressors'', etc.\n\n=== What about Neural Networks? ===\nNeural networks are a specific approach to Machine Learning that can be adapted to solve tasks many different settings. As a result, they have been used for detecting spams in emails, identifying different objects in images, beating humans at games like Chess and Go, grouping customers based on their preferences and so on. In addition, neural networks can be used in all three types of Machine Learning tasks mentioned above - supervised, unsupervised, and reinforcement learning.\n\nPlease refer to this [https:\/\/www.youtube.com\/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=\"text\" inline>f(x)<\/syntaxhighlight> that predicts <syntaxhighlight lang=\"text\" inline>y<\/syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, ''k-means clustering'', ''hierarchical clustering'', various approaches to performing ''regression'', ''principle component analysis'' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called ''Differential Privacy'' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century."],"31":["'''This sub-wiki deals with scientific methods.''' <br\/>\n\n=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and\/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n'''This Wiki describes each presented method in terms of''' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki's underlying [[Design Criteria of Methods]].<br\/>\n'''This means that each method fulfills one or more categories of each of the following criteria:'''\n<br\/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br\/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=\"pages\" hideroot=\"on\">Methods<\/categorytree>\n<br>\nWe also have what we call '''Level 2''' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* '''[[Statistics]]''': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* '''[[Interviews]]''': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read.","The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br\/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs.","'''Note:''' This entry focuses especially on Methods of Sustainability Science. For a more general conceptual view on Methods, please refer to the entry on the [[Design Criteria of Methods]].\n\n'''In short:''' In this entry, you will find out more about how we can distinguish and design our methodological approaches in Sustainability Science.\n\n== Design Criteria in Sustainability Science - ''Why?'' ==\nThe [[Design Criteria of Methods|design criteria of methods]] that I propose for all methods - quantitative vs. qualitative, inductive vs. deductive, spatial and temporal scales - are like the usual suspects of scientific methods. Within normal science, these design criteria are what most scientists may agree upon to be central for the current debate and development about methods. Consequently, '''it is important to know these ''normal science'' design criteria also when engaging with sustainability science.''' However, some arenas in science depart from the current paradigm of science - ''sensu strictu'' [https:\/\/plato.stanford.edu\/entries\/thomas-kuhn\/#DeveScie Kuhn] - and this means also that they depart potentially from the design criteria of the normal sciences. \n\nThe knowledge science currently produces is not enough to solve the problems we currently face. In order to arrive at new solutions, we need to re-consider, adapt and innovative the current raster of methods in science. Many methods that are long established are valuable, and all scientific methods have their [[History of Methods|history and origin]], and thus are important. Nevertheless, it becomes more and more clear that in order to arrive at solutions for the problems we currently face, we need to consider if we need methodological innovation. In the past,  methodological innovation worked in one of these ways:\n# new methods were invented,\n# new combinations of methods were attempted,\n# and methods were re-designed to be applied in a novel context where they had never been used before. \nWhile all this is fascinating in itself, here I present a different approach towards an amendment of the methodological canon, for one simple reason: Developing the methodological canon of science takes experience in the application of scientific methods, and ideally also contextual experience in diverse methodological approaches. While I am personally deeply critical about scientific disciplines, this is a good argument that in order to become versatile in scientific methods, you may need to channel your development towards being versatile in scientific methods based on textbook knowledge. Normal sciences have textbooks to teach their methodological canon, and while I think we need to be critical of these methodological approaches, they can have a lot of value. If you ask why, the I would say simply, because you cannot re-apply methods in a different context or attempt recombinations of methods if you are not experienced in the application of methods. To this end, it is important to highlight that these methods only look at parts of reality, which is the main reason for being critical. [[Bias and Critical Thinking|Bias]] and the fact that accompanying [[Levels of Theory|paradigms and theoretical viewpoints]] of specific methods restrict the validity of specific methods makes me even more critical. Nevertheless, there is no alternative to building experience than through applying methods within active research. You need to get your hands dirty to get experience. Here, I propose that we start with the knowledge we want to produce, and the goal we aim at to produce this knowledge. If we want to empower stakeholders, we need to be aware which methods out of the existing canon might help us, and how we may need to combine these methods in order to produce the knowledge we aim at. Therefore, we present three types of design criteria that serve as a basis for reflection of what knowledge we want to produce.\n\n== Key Competencies in Sustainability ==\nIn 2011, Wiek et al. analyzed the prevalent literature and presented five Key Competencies that students of Sustainability Science should strive for. This is an excellent scheme to be reflexive about the [[Glossary|competencies]] you want to gain, and to get a better understanding on which competencies can be aimed at through specific methods. These competencies are as follows:\n* ''Systems Thinking'': Systems Thinking competency is the ability to analyze and understand [[Glossary|complex systems]] including the dynamics in the interrelation of their parts. Systems thinking integrates different domains (society, environment, economy, etc.) as well as different scales (from local to global). \n\n* ''Anticipatory'': Anticipatory competency describes the ability to develop realistic scenarios of future trajectories within complex systems, including positive (e.g. a carbon-neutral city) and negative (e.g. flooding stemming from climate change) developments. This may include rigorous concepts as well as convincing narratives and visions.\n\n* ''Normative'': Normative competency refers to the ability to evaluate, discuss and apply (sustainability) values. It is based on the acquisition of normative knowledge such as concepts of justice or equality.\n\n* ''Strategic'': In simple terms, this competence is about being able to \"get things done\". Strategic competency is the capability to develop and implement comprehensive strategies (i.e. interventions, projects, measures) that lead to sustainable future states across different societal domains (social, economic, ecologic, ...) and scales (local to global). It requires an intimate understanding of strategic concepts such as path dependencies, barriers and alliances as well as knowledge about viability, feasibility, effectiveness and efficiency of systemic interventions as well as potential of unintended consequences.\n\n* ''Interpersonal'': Interpersonal competence is the ability to motivate, enable, and facilitate collaborative and participatory sustainability research and problem solving. This capacity includes advanced skills in communicating, deliberating and negotiating, collaborating, [[Glossary|leadership]], pluralistic and trans-cultural thinking and empathy."],"32":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Noble Eightfold Path - Mindfulness.png|350px|thumb|right|'''The Noble Eightfold Path, with Mindfulness being the seventh practice.''' Source: [https:\/\/en.wikipedia.org\/wiki\/Noble_Eightfold_Path Wikipedia], Ian Alexander, CC BY-SA 4.0]]\nMindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the context of many diverse but often religious schools of thinking.\n\n== Goals ==\nSince the goal of mindfulness is basically having \"no mind\", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free. \n\n== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen \u2013 sitting meditation \u2013 is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|'''[https:\/\/www.headspace.com\/de Headspace] is an app that can help you meditate''', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around \u2013 often referred to as the monkey mind \u2013, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https:\/\/www.forbes.com\/sites\/alicegwalton\/2017\/02\/28\/8-science-based-tricks-for-quieting-the-monkey-mind\/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions \u2013 just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!","== Links & Further Reading ==\n* [https:\/\/www.headspace.com Headspace] \n* [https:\/\/www.youtube.com\/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https:\/\/thichnhathanhfoundation.org\/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https:\/\/en.wikipedia.org\/wiki\/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on ''Zen Mind, Beginner's Mind'' is classic introduction to Zen.\n* [https:\/\/www.forbes.com\/sites\/alicegwalton\/2017\/02\/28\/8-science-based-tricks-for-quieting-the-monkey-mind\/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn.","Another approach that was beneficial to me is to write about things that keep preoccupying your mind. If you keep coming back to a certain thought, yet cannot really verbalise why you cannot let it be, why not write about it? Writing can be a surprisingly catalytic and clarifying approach to structure and analyse your thoughts. Writing 500-1000 words per day should be a no-brainer for any aspiring academic. Bear with me, this will grow on you if your are lucky. Start with a research diary, reflecting and verbalising what you learned on this specific day. This only works if you make it a habit. Remember that mails and other communication counts into the word count. I sometimes receive mails that are pieces of art. New years resolutions are worthless to start a new habit like writing. You need to minimise the friction instead, finding the right time, place and mood that makes you automatically start writing no matter what. Me, I sit in the chair where I write most texts, listening to the \"Tales from the Loop\"-soundtrack that propelled about 90% of all texts I wrote in the last year. If I put on this soundtrack, my fingers start twitching almost by instinct. Writing should be a reward, as I think it is a privilege. Writing cannot be pressed between two other time slots, it needs to be free and unbound, allowing your mind to do nothing else. From then on it is to me how Jazz is in music. Much of Jazz music is hard work and practice, almost to the point where your subconscious takes over and you are in autopilot mode. You need to practice enough so that your lack of skill does not stop you from writing. To me, this learning curve is surprisingly rewarding, it is almost like learning to be a rock climber. The first day is the horror. All muscles ache, you are basically destroyed. This will get worse for a few days. Suddenly, after two weeks of daily practice you will surprise yourself. After three months of daily practice you will lift yourself easily up the wall on previously impossible routes, and to others your path looks smooth and easy going. Writing is just like this.\n\n'''Studying teaches you to try things out'''<br>\nBeside the three staples of academics - reading, group work and writing - learning at a University is also about many other opportunities to learn and grow. This list is very specific and context depended for each and every single person. Still, the general consensus is that studying is about trying things out, how you can learn best, and find out what you are good at, and how you can contribute best. Here are some points that I consider to be relevant.\n\n'''Soft skills'''<br>\nAmong the diverse term of soft skills are personal traits and approaches that basically help us to interact. While this could be associated to group work (see above), I think it is good to make a mind map that you keep updating and exchange about with others. This is nothing that you need to obsess about, but more like a conscious and reflexive diary of your own personal development. Actually, a research diary can be a good first step. Also, if you witness others that excel at a certain soft skill, approach them and ask them how they learned their respective skills. It is also quite helpful -surprise- to practice. Presentations are something that are often not right the first time, and constructive feedback from critical people that you trust goes a long way. Much of the literature and other resources on soft skills are often over-enthusiastic, and promise the one and only best approach. Do not let yourself be fooled by such simple fixes, some of the soft skill literature is rather fringe. Still, new approaches to knowledge and interaction await, much can be gained, and only a bit of time may be lost. Why not giving another soft skill a go? The most important step is then to try it really out. Doing meditation once will tell you nothing about it, yet after some weeks you may perceive some changes. Your first World Caf\u00e9 was a failure? Well, try it again, several times, in different settings. For soft skills you need to stay open minded.\n\n'''Digital natives?'''<br>\nWe are awash with information to the brim, and continuously on the edge of drowning in it. Mastering all things digital may be one of the most important skills in this age and place. I think the most important rule to this end is: Less is more. Evidence how bad too much exposure to the digital world seems to be is mounting. Social media made many promises, yet I am not sure how many were kept. I can acknowledge that it can create meaningful linkages, build capacity, and even be a lifeline to your distant friend. Nevertheless, I would propose to be very reflexive which emotions are triggered by social media within you. This may lead to the conclusion to detox. The same holds true for all things extreme, namely binge watching, youtube or Spiegel Online. Instead you need to become versatile in a word processor, Powerpoint, maybe a graphical software, and get a hold of your direct digital communication. E-mail is still a thing, and writing a good e-mail is a skill that is equally admirable and often missed out on by students. I have been there. Again, practice goes a long way. Also, be conscious about data structure, backups, and online plans. You should be able to impress others with you digital skills. This will open many doors, and tilt many opinions towards your direction. Get at it!"],"33":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|'''MindMaps can take the form of trees, with the words on the branches, or clusters\/bubbles, as in this example.''' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https:\/\/www.thetutorteam.com\/wp-content\/uploads\/2019\/07\/shutterstock_712786150.jpg thetutorteam.com]]]\n\n'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). '''This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.''' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. '''Elements may be subordinate to other elements, indicating nestedness of the information.''' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n''Sources:''\n* [https:\/\/www.mindmapping.com\/de\/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http:\/\/methodenpool.uni-koeln.de\/download\/mindmapping.pdf Uni K\u00f6ln Methodenpool - Mind-Mapping]]\n* [https:\/\/kreativit\u00e4tstechniken.info\/problem-verstehen\/mindmapping\/ Kreativit\u00e4tstechniken.info - Mindmapping]]\n* [https:\/\/www.lifehack.org\/articles\/work\/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https:\/\/www.thetutorteam.com\/blog\/mind-maps-how-they-can-help-your-child-achieve\/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","== Step by step ==\n[[File:Concept Map - Step by Step 1.png|400px|thumb|right|'''A list of concepts (left) and a \"string map\" that focuses on cross-links between concepts, as a groundwork for the concept map.''' Source: [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php CMap]]]\n\n# When learning to work with Concept Maps, Novak & Canas (2008) recommend to '''start with a domain of knowledge that one is familiar with'''. Here, the learner should focus on a specific focus question and\/or text item, activity, or problem to structure the presented elements and hierarchies around, and to contextualize the map by. As the authors highlight, often, \"learners tend to deviate from the focus question and build a concept map that may be related to the domain, but which does not answer the question. It is often stated that the first step to learning about something is to ask the right questions\".\n# After defining the domain of knowledge and focus question or problem, 15-25 '''key concepts should be identified and ranked''' according to their specificity. In a concept map, more general, broad concepts are on the top of the map, while more specific concepts are found below, resulting in a hierarchy from top to bottom. So the concepts should be ordered accordingly in a list first.\n# Then, a preliminary concept map is created, either digitally (with the 'official' IHMC CmapTools software, see below) or by using Post-Its and a whiteboard. Concepts can and should be moved around iteratively. Propositions  - connections between concepts with text - are added to the map to highlight and explain relationships between concepts. When a first good draft exists, one identifies cross-links, which are relationships between separate areas of the map, to further highlight conceptual linkages. Lastly, '''constant revision is applied''' and concepts can be re-positioned to further improve the map.\n\n\n== Links & Further reading ==\n* This entry was created based on the extensive entry by Novak & Canas (2008), available [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php here]. The authors are responsible for the introduction of Concept Maps in the 1970s and the development of the CmapTools.\n\n* Novak, J.D. 2016. The origins of the concept mapping tool and the continuing evolution of the tool. Information Visualisation 5. 175-184.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConcept Maps are a '''form of visually organizing information''' for groups and individuals. Conceptual terms or short statements are presented in boxes or bubbles and interlinked with other terms through commented arrows, resulting in a hierarchical network-like structure that provides an overview on a topic.\n\n== Goals ==\n* Organize '''conceptual knowledge''' in a visual form to identify learning processes and knowledge gaps.\n* Create new knowledge, ideas and solutions by '''arranging information in a structured manner''' around a focus question, topic, or problem.\n\n== Getting started ==\n[[File:Concept Maps - Example 1.png|500px|thumb|right|'''An example for a Concept Map on the knowledge structure required for understanding why we have seasons.''' . Source: [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php Novak & Ca\u00f1as 2008, Cmap]]]\nConcept Maps result from the work of '''Joseph Novak and colleagues at Cornell University in the 1970s''' in the field of education, and have since been applied in this area and beyond. The original underlying idea was to analyze how children learn, assuming that they do so by assimilating new concepts and positioning these in a cognitive conceptual framework. The idea of Concept Maps emerged from the demand for a form of assessing these conceptual understandings.\n\nImportantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of [[Group Concept Mapping]]. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a [[Mindmap]] in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.\n\nConcept Maps '''help identify a current state of knowledge''' and show gaps within this knowledge, e.g. a lack of understanding on which elements are of importance to a question or topic, and how concepts are interrelated. Identifying these gaps helps fill knowledge gaps. Therefore, they can be a helpful tool for students or anyone learning a new topic to monitor one's own progress and understanding of the topic, and how this changes as learning units continue.\n\nFurther, Concept Maps can be '''a way of approaching a specific question''' or problem and support a systematic solution-development process. The visual representation of all relevant elements nconcerning a specific topic can thus help create new knowledge. There are even more imaginable purposes of the approach, including management and planning, creative idea-generation and more.\n\n[[File:Concept Map Example 2.png|500px|thumb|center|'''This example from Novak (2016, p.178) shows how a student's conceptual understanding of a topic develops over time.''' It illustrates how Concept Maps can help identify knowledge gaps or flaws (which are existent still in the map below), how much (and what kind of) learning process was made over time, and what to focus on in future learning. Source: Novak 2016, p.178.]]\n\n\n== Step by step ==\n[[File:Concept Map - Step by Step 1.png|400px|thumb|right|'''A list of concepts (left) and a \"string map\" that focuses on cross-links between concepts, as a groundwork for the concept map.''' Source: [http:\/\/cmap.ihmc.us\/docs\/theory-of-concept-maps.php CMap]]]"],"34":["[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know.","W\u00e4hrend Psychologie, Medizin, Agrarwissenschaft, Biologie und sp\u00e4ter auch die \u00d6kologie auf diese Weise in der Anwendung von Versuchspl\u00e4nen und Studien gediehen, gab es auch eine zunehmende Anerkennung von Informationen, die [[Bias and Critical Thinking|Bias]] erzeugten oder die Ergebnisse anderweitig verzerrten. '''Die ANOVA wurde daher durch zus\u00e4tzliche Modifikationen erg\u00e4nzt, was schlie\u00dflich zu fortgeschritteneren Statistiken f\u00fchrte, die in der Lage waren, sich auf verschiedene statistische Effekte zu konzentrieren'', und den Einfluss von Bias zu reduzieren, z.B. in Stichproben, statistischem Bias oder anderen Fehlern. Somit wurden [[Mixed-Effect Models]] zu einem fortgeschrittenen n\u00e4chsten Schritt in der Geschichte der statistischen Modelle, der zu komplexeren statistischen Designs und Experimenten f\u00fchrte, bei denen immer mehr Informationen ber\u00fccksichtigt wurden. Dar\u00fcber hinaus f\u00fchrten meta-analytische Ans\u00e4tze dazu, mehrere Fallstudien zu einem systematischen \u00dcberblick zusammenzufassen und zusammenzufassen. Dies war der Beginn eines integrativeren Verst\u00e4ndnisses verschiedener Studien, die zu einer [[Meta-Analysis|Meta-Analyse]] zusammengefasst wurden, wobei auch die unterschiedlichen Kontexte der zahlreichen Studien ber\u00fccksichtigt wurden. Dar\u00fcber hinaus konzentrierte sich die Forschung mehr und mehr auf ein tieferes Verst\u00e4ndnis einzelner Fallstudien, wobei der spezifische Kontext des jeweiligen Falles st\u00e4rker betont wurde. Solche Einzelfallstudien sind in der medizinischen Forschung seit Jahrzehnten von Wert, wo trotz des offensichtlichen Mangels an einem breiteren Beitrag h\u00e4ufig neue Herausforderungen oder L\u00f6sungen ver\u00f6ffentlicht werden. Solche medizinischen Fallstudien berichten \u00fcber neue Erkenntnisse, auftauchende Probleme oder andere bisher unbekannte Falldynamiken und dienen oft als Ausgangspunkt f\u00fcr weitere Forschung. Aus so unterschiedlichen Urspr\u00fcngen wie [[System Thinking & Causal Loop Diagrams|Systemdenken]], Stadtforschung, [[Ethnography|Ethnographie]] und anderen Forschungsfeldern entstanden [[Living Labs & Real World Laboratories|Realwelt-Experimente]], die im allt\u00e4glichen sozialen oder kulturellen Umfeld stattfinden. Die starren Entw\u00fcrfe von Labor- oder Feldexperimenten werden gegen ein tieferes Verst\u00e4ndnis des spezifischen Kontexts und Falls eingetauscht. W\u00e4hrend Experimente aus der realen Welt bereits vor einigen Jahrzehnten entstanden sind, beginnen sie erst jetzt, breitere Anerkennung zu finden. Gleichzeitig stellt die Reproduzierbarkeitskrise die klassischen Labor- und Feldexperimente in Frage, da man sich dar\u00fcber im Klaren ist, dass viele Ergebnisse - zum Beispiel aus psychologischen Studien - nicht reproduziert werden k\u00f6nnen. All dies deutet darauf hin, dass zwar ein gro\u00dfer Teil unseres wissenschaftlichen Wissens aus Experimenten stammt, dass aber auch \u00fcber die Durchf\u00fchrung der Experimente selbst noch viel zu lernen verbleibt.\n----\n[[Category: Normativity_of_Methods]]\n[[Category: Methods]]\n[[Category: Statistics]]\n[[Category: Qualitative]]\n[[Category: Deductive]]\n[[Category: Individual]]\n[[Category: System]]\n[[Category: Global]]\n[[Category: Present]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","There has been some sort of a revival of r<sup>2<\/sup> values lately, mainly based on the suggestion of r<sup>2<\/sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2<\/sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2<\/sup>  value, and even have their model selection not independent of the r<sup>2<\/sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2<\/sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2<\/sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2<\/sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.\n\nAnother very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."],"35":["There has been some sort of a revival of r<sup>2<\/sup> values lately, mainly based on the suggestion of r<sup>2<\/sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2<\/sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2<\/sup>  value, and even have their model selection not independent of the r<sup>2<\/sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2<\/sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2<\/sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2<\/sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.\n\nAnother very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","Take the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.\n\nThe greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.\n\n'''Mixed Effect Models are built on a litany of preconditions, most of which most researchers choose to conveniently ignore.''' In my experience, this is more often than not ok, because it does not matter. Mixed Effect Models are - bless the maximum likelihood estimation - very sturdy. It is hard to find a model that does not have some predictive or explanatory value, even if hardly any pre-conditions are met. Still, this does not mean that we should ignore these. In order to sleep safe and sound at night, I am almost obsessed with model inspection, checking variance across levels, looking at the residuals, and looking for gaps and flaws in the model's fit. We should be really conservative to this end, because by focusing on fixed and random variance, we potentiate things that could go wrong. As I said, more often than not, this is not the case, but I propose to be super conservative when it comes to your model outcome. In order to get there, we need yet another thing: Model simplification.\n\nMixed Effect Models lead the forefront of statistics, and this might be the reason why the implementation of AIC (Akaike Information Criterion) as a parsimony-based evaluation criterion is more abundant here when compared to other statistical approaches. P-values fell widely out of fashion in many branches of science, as did the reporting of full models. Instead, model reduction based on information criteria approaches is on the rise, reporting parsimonious models that honour [[Why_statistics_matters#Occam.27s_razor | Occam's razor]]. Starting with the maximum model, these approaches reduce the model until it is the minimum adequate model - in other words, the model that is as simple as possible, but as complex as necessary. The AIC is about the equivalent of a p-value of 0.12, depending on the sample size, hence beware that the main question may be the difference from the null model. In other words, a model that is better from the Null model, but only just so based on the AIC, may not be significant because the p-value would be around 0.12. This links to the next point: Explanatory power.","[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know."],"36":["== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented.","The last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find.","===Challenges===\n* Even if bootstrapping is asymptotically consistent, it does not provide general finite-sample guarantees.\n* The result may depend on the representative sample.\n* The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples).\n* Bootstrapping can be time-consuming.\n* There is a limitation of information that can be obtained through resampling even if number of bootstrap samples is large.\n\n===Normativity===\n\nBootstrap methods offer considerable potential for modelling in complex problems, not least because they enable the choice of estimator to be separated from the assumptions under which its properties are to be assessed.\n\nAlthough the bootstrap is sometimes treated as a replacement for \"traditional statistics\". But the bootstrap rests on \"traditional\" ideas, even if their implementation via simulation is not \"traditional\".\n\nThe computation can not replace thought about central issues such as the structure of a problem, the type of answer required, the sampling design and data quality.\n\nMoreover, as with any simulation experiment, it is essential to monitor the output to ensure that no unanticipated complications have arisen and to check that the results make sense. The aim of computing is insight, not numbers.[2]\n\n==Outlook==\nOver the years bootstrap method has seen a tremendous improvement in its accuracy level. Specifically improved computational powers have allowed for larger possible sample sizes used for estimation. As bootstrapping allows to have much better results with less amount of data, the interest for this method rises substantially in research field. Furthermore, bootstrapping is applied in machine learning to assess and improve models. In the age of computers and data driven solutions bootstrapping has good perspectives for spreading and development.\n\n==Key Publications==\n* Efron, B. (1982) The Jackknife, the Bootstrap, and Other Resampling Plans. Philadelphia: Society for Industrial and Applied Mathematics.\n* Efron, B. and Tibshirani, R. J. (1993) An Introduction to the Bootstrap. New York: Chapman and Hall.\n==References==\n# Dennis Boos and Leonard Stefanski. Significance Magazine, December 2010. Efron's Bootstrap.\n# A. C. Davison and Diego Kuonen. Statistical Computing & Statistical Graphics Newsletter. Vol.13 No.1. An Introduction to the Bootstrap with Applications in R\n# Michael Wood. Significance, December 2004. Statistical inference using bootstrap confidence interval.\n# Jeremy Orloff and Jonathan Bloom. Bootstrap confidence intervals. Class 24, 18.05.\n# CRAN documentation. Package \"bootstrap\", June 17, 2019.\n\n==Further Information==\n\nYoutube video. Statquest. [https:\/\/www.youtube.com\/watch?v=isEcgoCmlO0&ab_channel=StatQuestwithJoshStarmer Bootstrapping main ideas]\n\nYoutube video. MarinStatsLectures. [https:\/\/www.youtube.com\/watch?v=Om5TMGj9td4&ab_channel=MarinStatsLectures-RProgramming%26Statistics Bootstrap Confidence Interval with R]\n\nYoutube video. [https:\/\/www.youtube.com\/watch?v=9STZ7MxkNVg&ab_channel=MarinStatsLectures-RProgramming%26Statistics MarinStatsLectures. Bootstrap Hypothesis Testing in Statistics with Example]\n\nArticle. [https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-the-bootstrap-method\/#:~:text=The%20bootstrap%20method%20is%20a,the%20mean%20or%20standard%20deviation.&text=That%20when%20using%20the%20bootstrap,and%20the%20number%20of%20repeats. A Gentle Introduction to the Bootstrap Method]\n\nArticle. [https:\/\/statisticsbyjim.com\/hypothesis-testing\/bootstrapping\/ Jim Frost. Introduction to Bootstrapping in Statistics with an Example]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Andrei Perov."],"37":["The last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find.","== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented.","Overall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. \"Missing data: our view of the state of the art.\" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. \"Dealing with missing data.\" (2002).\n\nTsikriktsis, Nikos. \"A review of techniques for treating missing data in OM survey research.\" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]"],"38":["[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Narrative Research describes qualitative field research based on narrative formats which are analyzed and\/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|'''SCOPUS hits per year for Narrative Research until 2020.''' Search terms: 'Narrative Research', 'narrative inquiry', 'narrative analysis' in Title, Abstract, Keywords. Source: own.]]\n'''[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years''' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). 'Storytelling' is defined as the distinct tonality, format and presentation in which a story is told. The term 'narrative' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term 'narrative' has been used used predominantly as a synonym for 'story' in academia for decades (Barrett & Stauffer 2009).\n\n'''Psychologist Jerome Bruner introduced the notion of 'narrative' as being one of two forms of distinct modes of thinking in 1984''' - the other being the 'logico-scientific' mode (Barrett & Stauffer 2009). While the latter is \"(...) more concerned with establishing universal truth conditions\" (Barrett & Stauffer 2009, p.9), the 'narrative' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that 'narratives' are a central form of human learning about - and [[Glossary|sense-making]] of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.\n\n'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).\n\n== What the method does ==\n'''First, there is a distinction to be made:''' 'Narrative' can refer to a form of Science Communication, in which research findings are presented in a story format (as opposed to classical representation of data) but not extended through new insights. 'Narrative' can also be understood as a form of scientific inquiry, generating new knowledge during its application. This entry will focus on the latter understanding.\n\n'''Next, it should be noted that Narrative Research entails different approaches''', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.","'''Next, it should be noted that Narrative Research entails different approaches''', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.\n\nNarrative Research -  or 'Narrative Inquiry' - is shaped by and focussing on a conceptual understanding of 'narratives' (Barrett & Stauffer 2009, p.15). Here, 'narratives' are seen as a format of [[Glossary|communication]] that people apply to make sense of their life experiences. \"Communities, social groups, and subcultures tell stories with words and meanings that are specific to their experience and way of life. The lexicon of a social group constitutes its perspective on the world, and it is assumed that narrations preserve particular perspectives in a more genuine form\" (Jovchelovitch & Bauer 2000, p.2). '''Narratives are therefore not merely forms of representing a chain of events, but a way of making sense of what is happening.''' Through the telling of a story, people link events in meaning. The elements that are conveyed in the story, and the way these are conveyed, indicates how the story-teller - and\/or their social surrounding - sees the world. They are a form of putting reality into cultural and individual perspective. Also, narratives are never final but change over time as new events arise and perspectives develop (Jovchelovitch & Bauer 2000, Webster & Mertova 2007, Squire et al. 2014, Moen 2006). \n\nNarrative Research is \"(...) the study of stories\" (Polkinghorne 2007, p.471) and thus \"(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience.\" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the 'narratology', i.e. the structure and grammar of a story; the 'narrative content', i.e. the themes and meanings conveyed through the story; and\/or the 'narrative context', which revolves around the effects of the story (Squire et al. 2014).\n\n'''One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.''' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, 'narrative' is the underlying \"frame of reference\" (Moen 2006, p.57) for the research. An example for the latter understanding is the 'Narrative Interview'.\n\n==== Narrative Interviews ====\n[[File:Narrative Research Phases.png|400px|thumb|right|'''Basic phases of the narrative Interview.''' Source: Jovchelovitch & Bauer 2000, p.5.]]\n\nThe Narrative Interview is an Interview format that \"(...) encourages and stimulates an interviewee (...) to tell a story about some significant event in their life and social context.\" (Jovchelovitch & Bauer 2000, p.3). 'Narrative Interviewing' \"is considered a form of unstructured, in-depth interview with specific features.\" (Jovchelovitch & Bauer 2000, p.4) To this end, it is a form of the [[Open Interview]], which - compared to [[Semi-structured Interview|Semi-structured]] and [[Survey|Structured Interviews]] - is relatively free from deductively pre-developed question schemata: \"To elicit a less imposed and therefore more 'valid' rendering of the informant's perspective, the influence of the interviewer should be minimal (...) '''The [Narrative Interview] goes further than any other interview method in avoiding pre-structuring the interview.''' (...) It uses a specific type of everyday communication, namely story-telling and listening, to reach this objective.\" (Jovchelovitch & Bauer 2000, p.4) Here, it is central that the interviewer appreciate the interviewee's perspective, by using the subject's language and by posing \"as someone who knows nothing or very little about the story being told, and who has no particular interests related to it\" (ibid, p.5). The interview is then transcribed and analyzed using different forms of coding (see Content Analysis), with a focus on the narrative elements. For more information on the methodological foundations of conducting and analyzing narrative Interviews, please refer to Jovchelovitch & Bauer 2000.","== Strengths & Challenges ==\n* Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000)\n* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research \"(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates\". Further, \"Narrative inquiry attempts to capture the 'whole story', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important 'intervening' stages\" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).\n* '''The insights gained through narratives are subjective to the narrator, which implies advantages and challenges.''' Compared to an 'objective' description of, e.g. a chain of events, the narration provides insights about the individual's interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: \"Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived.\" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the 'real' world. Squire et al. (2014) refer to this distinction as different forms of 'truth' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical 'truth'. They should try \"(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather.\" Moen (2006, p.63), by comparison, explains that \"(...) there is no static and everlasting truth\", anyway.\n* This [[Bias and Critical Thinking|conflict between different 'truths']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, 'usefulness' and 'persuasiveness' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: \"Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable' (...) Narrative research does not claim to represent the exact 'truth', but rather aims for 'verisimilitude'\". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a 'trustworthy' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one's research results (see Moen 2006, p.64). Also, researchers \"(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts\" (Polkinghorne 2007, p.484).\n* Further challenges may arise during the active collaboration of the researcher in the field. For example, \"(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher\" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]]."],"39":["[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https:\/\/www.probabilisticworld.com\/frequentist-bayesian-approaches-inferential-statistics\/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.","Since not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence\/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and\/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know."],"40":["THIS ARTICLE IS STILL IN EDITING MODE\n==1. Introduction to Probability Distribution==\nA probability distribution summarizes the probabilities for the values of a random variable. The general properties\/features that define a distribution include the four mathematical moments: \n\n1.\t'''Expected value E(X)''': This is the outcome with the highest probability or likelihood or an average or mean value for the variable X.\n \n2.\t'''Variance Var(X)''': Variance denotes the spread of the values from the expected value. The standard deviation is the normalized value of variance obtained by taking a square root of the variance. The covariance summarizes the linear relationship for how the two variables change with respect to each other. \n\n3.\t'''Skewness''': Skewness is the measure of asymmetry of a random variable X about the mean E(X) of a probability distribution. It can be positive (tail on the right side), negative (tail on left side), zero (balanced tail on both sides) or undefined. Zero skewness does not always mean symmetric distribution as one tail can be long and thin and the other can be short and fat. \na\n4.\t'''Kurtosis''': Kurtosis is the measure of \u2018 *peaked-ness* \u2019 of distribution. It can be differentiating tool between distributions that have the same mean and variance. Higher kurtosis means a peak and fatter\/extended tails. \n\nEach random variable has its own probability distribution that may have a similar shape to other variables, however, the structure will differ based on whether the variable is discrete or continuous, since probability distributions of continuous variables have an infinite number of values and probability functions of discrete variables assign a probability to each concrete data point. \n\n==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=\"wikitable\"\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success\/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}\n\nPoisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https:\/\/studiousguy.com\/poisson-distribution-examples\/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===\n\n''The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time.''\n\n[[File:1 equation.PNG|center|250px]]\nT = Time interval \ne = natural logarithmic base \nk = number of events \nThe K! means K factorial. This means that we multiply all integers from K down to 1. Say K is 4 then K!= 4* 3* 2* 1= 24\n\nIntroducing lambda, \u03bb, as a rate parameter (events\/time)*T into the equation gives us \n[[File:2 equation.PNG|center]]\n\nSo, \u03bb is basically the expected number of event occurrences in an interval, a function of '''number of events''' and the '''time interval'''. Changing \u03bb means changing the probability of event occurrence in one interval.\n\nFor example, Ladislaus Bortkiewicz calculated the deaths of soldiers by horse kicks in the Prussian Army using Poisson Distribution in the late 1800s. He analyzed two decades of data from up to 10 army corps, equivalent to two centuries of observations for one army corps.\n[[File:Horse.png|center|500px]]\nFigure 1: Poisson Distribution for deaths by horse kick by Ladislaus Bortkiewicz. Source: [https:\/\/www.scribbr.com\/statistics\/poisson-distribution\/#:~:text=You%20can%20use%20a%20Poisson,days%20or%205%20square%20inches. Scribbr]","from scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n<\/syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=\"Python\" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n<\/syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1\n\n<syntaxhighlight lang=\"Python\" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align='center')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n<\/syntaxhighlight>\n[[File: plot 1.png|center|700px]]\n\nThe resulting pmf confirms that the closest integer value to lamba i.e., 3 has the most number of years out of 10,000 meaning most years will have 3 duststorms. You can also see that the data is slightly skewed to the right since there is a larger variance to the right than to the left. Looking at the distribution, it looks fairly normally distributed. However, the low lambda does not allow to use the Poisson distribution as an approximation for a normal distribution. Most probably, the large dataset allows us to see it as a normal distribution, since most distributions converge to a normal distribution with increasing sample size.\n==6. References==\n* Brownlee, Jason: \"A Gentle Introduction to Probability Distributions\", 14.11.2019. Retrieved from: https:\/\/machinelearningmastery.com\/what-are-probability-distributions\/#:~:text=A%20probability%20distribution%20is%20a,properties%20that%20can%20be%20measured, last checked: 21.05.2023\n* Koehrsen, Will: \" The Poisson Process and Poisson Distribution Explained (With Meteors!), 28.10.2022. Retrieved from: https:\/\/builtin.com\/data-science\/poisson-process, last checked: 21.05.2023\n* Papoulis, Athanasios; Pillai, S. Unnikrishna: \"Probability, Random Variables, and Stochastic Processes\" (4th ed.).\n* Raikov, Dmitry (1937): \"On the decomposition of Poisson laws\". Comptes Rendus de l'Acad\u00e9mie des Sciences de l'URSS. 14: 9\u201311.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart","According to his observation, an average of 0.61 soldiers died every year. However, the deaths were random, for example, in one year four soldiers died and for most years no deaths were caused by horses. Figure 1 shows the probability mass function graph.\nIn Poisson Distribution terms,\n* Death caused by a horse kick is an \u2018event\u2019\n* Mean per time interval, represented by \u03bb, is 0.61\n* The number of deaths by horse kick in a specific year is k.\n\n[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (\u03bb) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to \u03bb. The Lamda, \u03bb, also represents the mean and variance of the distribution.\n\n==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, \u03bb, its average value is \u03bb.\n\n2. Variance of the distribution is also \u03bb, implying when \u03bb increases the width of the distribution goes as square root lambda \u221a\u03bb.\n\n3. A converse in Raikov\u2019s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.\n\nConsider the example of radioactive decay for long-lived isotopes, in a radioactive sample containing a large number of nuclei, each of which has a tiny probability of decaying during some time interval, T. Let\u2019s say the rate of decay is 0.31 decay\/second and is monitored for 10 seconds. That gives us the \u03bb1 0.31 * 10 = 3.1 which means the probability equals,\n[[File:3 equation.PNG|center]]\nConsider another example where \u03bb2 is 2.7, the probability is\n[[File:4 equation.PNG|center]]\n\nTherefore,\n'''If we look at the total number k = k1 + k2 of a radioactive decay in a time T, the result is also a Poisson distribution with \u03bb = \u03bb1 + \u03bb2 -> \u03bb = 3.1 + 2.7 = 5.8 : P(k, \u03bb1 + \u03bb2 )'''\n[[File:5 equation.PNG|center]]\n\n''If we have two independent Poisson-distributed variables, their sum is Poisson distributed too.''\n\n4. The skewness is measured by 1\/\u221a\u03bb\n\n5. Excess kurtosis is measured by 1\/\u03bb. See the difference between excess kurtosis and kurtosis [https:\/\/www.investopedia.com\/terms\/e\/excesskurtosis.asp here]. In a nutshell, excess kurtosis compares the kurtosis of the distribution with the kurtosis of a normal distribution and can therefore tell you if an (extreme) event is more or less likely in this case than if the distribution followed a normal distribution.\n\n6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> \u221e and \u03bb = np, rate parameter, \u03bb is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean \u03bb > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = \u03bb = \u03bc =45\n\nStandard deviation = \u2202 = \u221a\u03bb = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https:\/\/builtin.com\/data-science\/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=\"Python\" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n<\/syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year"],"41":["{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to ''25 minutes = 1 \"Pomodoro\"'').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15\u201330 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https:\/\/en.wikipedia.org\/wiki\/Pomodoro_Technique Pomodoro Technique]\n* [https:\/\/lifehacker.com\/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https:\/\/www.youtube.com\/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https:\/\/play.google.com\/store\/apps\/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https:\/\/apps.apple.com\/us\/app\/focus-keeper-time-management\/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique''' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe '''Dreamers'''...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: ''Which ideas come to mind? What would be an ideal solution to the problem?''\n\nThe '''Realists'''...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: ''How does the idea feel? How could it be implemented? Who should do it and at what cost?''\n\nThe '''Critics'''...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: ''What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?''\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. '''The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.''' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to 'neutralize' their minds) in order to understand the other roles' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He\/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n''Sources:''\n* Tools Hero - [https:\/\/www.toolshero.com\/creativity\/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https:\/\/arbeitdigital.de\/wirtschaftslexikon\/kreativitaetstechniken\/walt-disney-methode\/ Walt-Disney-Methode]\n* Karrierebibel - [https:\/\/karrierebibel.de\/disney-methode\/ Disney Methode: Denkblockaden \u00fcberwinden]\n* Impulse - [https:\/\/www.impulse.de\/management\/selbstmanagement-erfolg\/walt-disney-methode\/3831387.html Walt Disney Methode]\n\n[https:\/\/www.youtube.com\/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br\/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.","{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nMiro is an online collaboration tool which allows you to hold virtual workshops, develop common ideas and designs, have a digital workspace or design presentations in a different way.\n\n== Goals ==\n* Work better together online\n* Integrate many different functions (project management, online collaboration, presentation) into one\n\n== Getting started ==\nMiro is an online-tools which allows you to collaborate with others (but also work by yourself). '''It is basically an infinite canvas onto which you can put all kinds of elements:''' shapes, text, videos, documents, interactive elements such as a [[Kanban]] Board or sticky notes. As collaboration happens in real-time, it can very much substitute the classical whiteboard or brown-paper and allow you to collaboratively brainstorm ideas, design a presentation, or manage your team.\n\nWe recommend using it for digital workshops, organizing your teamwork or as an alternative to traditional presentation formats such as Prezi, PowerPoint or Keynote.\n\nIt can be a bit overwhelming at first, but once you get the hang of it, it becomes really natural to use and hopefully will make your teamwork more productive and fun.\n\nTo get started, create an account on [https:\/\/miro.com the Miro website]. If you are eligible (e.g. when you're a member of Leuphana University), you can apply for a free Miro Education account which comes with all the premium features [https:\/\/miro.com\/education-whiteboard\/ here].\n\n== Links & Further Reading ==\nHere's a video that covers (almost) everything you need to know to get started working with Miro:\n{{#evu:https:\/\/www.youtube.com\/watch?v=pULLAEmhSho\n|alignment=center\n}}\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."],"42":["[[File: PCA_BiPlotNoScale.png|center|500px]]\n\n[[File: PCA_ContribPlotNoScale.png|center|500px]]\n\nSuddenly all that matters are only the carbohydrate, moisture and fat level of the pizzas. Why is that the case?\n\nBy plotting the distribution of the original data using boxplots (on the left), we can see that the value range of data for the variables are vastly different. For example, the variable carbohydrates has much higher mean and variance compared to calories. By nature, PCA tries to form PCs where there is a widest spread in the data, so it will always prefer those variables with high \"absolute\" variance. It's like comparing 1000 milliseconds and 5 kilometers and putting more weight on the 1000 milliseconds because 1000 is bigger than 5.\n\n[[File: PCA_BoxPlot.png|center|700px]]\n\nThis is why standardization, or sometimes called feature scaling (scale data to mean 0 and standard deviation 1) is a crucial pre-processing step in many data analysis procedures and machine learning algorithms, including PCA. This allows the analysis to pay attention to all features equally, so that no variable dominates the others (equal importance).\n== Strengths & Weaknesses ==\n'''Strengths'''\n* Reduce complexity of data\n* Allows for concise visualization of main patterns in data\n* Remove correlated features\n* Enhance performance of algorithms\n* Reduce overfitting\n'''Weaknesses'''\n* Principle components are created based on linear assumptions\n* The created principle components are hard to interpret\n* Information loss through reduction of dimensionality (oftentimes acceptable)\n== Key Publications ==\n* Wold, S., Esbensen, K., & Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3), 37-52.\n\n* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150202.\n== See Also ==\n\n* [https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ&t A simple visual explanation of PCA] from StatQuest with Josh Starmer\n\n* [https:\/\/www.youtube.com\/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&index=4 An in-depth walkthrough of PCA] and its mathematical root with Victor Lavrenko\n----\n\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Ch\u00e2n L\u00ea.","== Ordinations ==\n\nOrdination techniques evolved already more than a century ago in mathematics, and allowed fro a reduction of information that makes these analysis approaches timely up until today. Ordination techniques rely strongly on a profound knowledge of the underlying data format of the respective dataset that is being analysed. Since ordination allow for both inductive and deductive analysis, they often pose a risk for beginners, who typically get confused by the diversity of approaches and the answers these analysis may provide. This conduction is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques. What is more, ordinations are often deeply entangled in disciplinary cultures, with some approaches such as factor analysis being almost exclusive to some disciplines, and other approaches such as principal component analysis being utilised in quite diverse ways within different disciplines. This makes norms and rules when and how to apply different techniques widely scattered and intertwined with disciplinary norms, while the same disciplines are widely ignorant about other approach from different disciplines. Here, we try to diver a diverse and reflected overview of the different techniques, and what their respective strengths and weaknesses are. This will necessary demand a certain simplification, and will in addition trigger controversy within certain branches of science, and these controversies are either rooted in partial knowledge or in experiential  identity. Unboxing the whole landscape of ordinations is also a struggle because these analysis are neither discrete nor always conclusive. Instead they pose starting points that often serve initial analysis, or alternatively enable analysis widely uncoupled from the vast landscape of univariate statistics. We need to acknowledge to this end that there is a vast difference between the diverse approaches not only in the underlying mathematics, yet also how these may be partly ignored. This is probably the hardest struggle that you can fin neither in textbooks nor in articles. The empirical reality is that many applications of ordinations violate much of the mathematical assumptions or rules, yet the patterns derived from these analyses are still helpful if not even valid. Mathematicians can choose to live in a world where much of ordination techniques is perfect in every way, yet the datasets the world gives to ordinations are simply not. Instead, we have to acknowledge that multivariate data is almost always messy, contains a high amount of noise, many redundancies, and even data errors. Safety comes in numbers. Ordinations are so powerful exactly because they can channel all these problems through the safety of the size of the data, and thus derive either initial analysis or even results that serve as endpoints. However, there is a difference between initial or final results, and this will be our first starting point here.\n\nOrdination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through experience can you pick what serves your dataset best. This is associated to the reality that many analysis made with ordinations are indeed compromises. Ecology and psychology are two examples of disciplines why imagined ordinations deeply enough into the statistical designs to derive datasets where more often than not assumptions for statistical analysis of a respective ordination are met. However, many analyses based on ordinations are indeed compromises, and from a mathematical standpoint are real world analysis based on ordinations a graveyard of mathematical assumptions, and violation of analytical foundations that borderline ethical misconduct. In other words, much of ordinations are messy. This is especially true because ordinations are indeed revealing mostly continuous results in the form of location on ordination axes. While multivariate analyis based on cluster analysis are hence more discrete through the results being presented as groups, ordinations are typically nice to graphically inspect, but harder to analytical embedded into a wider framework. More on this point later. Let us now begin with a presentation of the diverse ordination types and their respective origins. \n\n=== Correspondence analysis ===","Equally, there are people who obsess about the explanatory power of models. They see the promise of causality in the smallest [[Glossary|patterns]] they find, and never stop iterating about how ''it all makes sense now''. Much has been explained in the past, but much may remain a mystery. Much that once was causal is lost, for no papers are available to remember it. The knowledge of people evolves, and with it the theories that our causality is often built upon. The predictive and the explanatory people even go to war against one another claiming victory after victory and their fight about superiority. However, many souls were lost in these endless quests, and to me it remains unclear if any real victory was ever won in this eternal iteration on whether patterns are causal or predictive. Both approaches can make sense, and the clever ones never went down the paths of priority, and avoided claiming what is better. They simply claimed their worldview, and were fine. They lived in their world of the two realms, and ignored the other realm completely. Many thus lived a happy life, half ignorant, but happy. These people are like the Ancient Ones living in their kingdom of old, with scientific disciplines and textbook knowledge, where kingdoms fought other kingdoms at times, but there was also peace and prosperity. \n\nWhat can be called a 'modern data scientists' will know nothing of these worlds of Old, but will become unattached to their data. One should be aware that all data is normative, but the patterns detection and analysis is ideally done with a beginner's mind, knowing everything and nothing at the same time. As the Ancients said, ''matters of great concern should be treated lightly'', to which someone replied, that matters of small concern should be treated seriously. The same is true for data analysis. '''We need to remember that the world is constructed, and that our data is only looking at parts of the picture.''' Despite all that, one should become detached from the analysis itself and versatile at it at the same time. Then coding becomes a form of art, and one can exceed one's own expectations. Your code becomes a statement, and even if people admire it, this does not matter to you. And thus you become the coder that you should be, and never will be. Then coding becomes a way.\n\n'''There are small mindfulness exercises in R-coding that have proven beneficial in the past:'''\n1) Chant the mantra \"library\". ''l-i-b-r-a-r-y''. Only by perfecting this mantra will you master the art to spell the word correctly, and load a library.\n2) Close the bracket. Through endless training, you can master the ancient art of closing every bracket you ever opened, understanding the inner circularity of life itself, and R-coding. Brackets in R are like breathing. Just as you breathe in and breathe out, you open and close brackets.\n3) Cleaning the workspace is like cleaning your inner self. After a long work session, the master will ritually close R as if it is second nature, and the knowledge of the ancients know how the very question \"do you want to save your workspace\" test the worthiness of the decibel always anew. \n4) Let go of your fear when coding. Simply breathe in, and breathe out. More is not expected of you to master the art of coding.\n\n----\n[[Category:Normativity of Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."],"43":["This does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Key Publications ==\n* Gleick, J. (2011). Chaos: Making a new science. Open Road Media.\n* Rohde, Klaus. Nonequilibrium ecology. Cambridge University Press, 2006.\n* Kruschke, John. \"Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.\" (2014).\n* Hastie, T. & Tibshirani, R. 1986. ''Generalized Additive Models''. Statistical Science 1(3). 297-318.\n\n==External Links==\n====Articles====\n[https:\/\/science.sciencemag.org\/content\/sci\/199\/4335\/1302.full.pdf?casa_token=SJbEKSHs2gwAAAAA:iWho1AqZsznpL8Tt5vvaPHX2OVggZkP2NlUjEZ8I0avaMKTs6BlCMA7LGG0405x6l5LBY9hTAhag One of the classical papers] on non-equilibrium ecology\n[https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF00334469.pdf Non-equilibrium theory in ecology]<br>\n[https:\/\/www.jstor.org\/stable\/pdf\/1942636.pdf?casa_token=gYme29pwLTsAAAAA:eZBniNyPMJyFe5F7hkmy51EkBk3h0Bm6ap6nG2WWs8-n6EjuhJ16sDt5mJFXipvIIUBu9mzjI16EkLwCgMG70s-YayWTrlzAm63iX3iBk0zk-Mgk4g Classical account on equilibrium and non-equilibrium] dynamics in ecology<br>\n[https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-46709-2_6 A balanced view on rangelands and non equilibrium dynamics]<br>\n[https:\/\/esajournals.onlinelibrary.wiley.com\/doi\/pdfdirect\/10.1890\/11-0802.1 Non-equilibrium dynamics in rangelands]<br>\n[https:\/\/www.mdpi.com\/2079-8954\/7\/1\/4\/htm A view on complexity]<br>\n[https:\/\/plato.stanford.edu\/entries\/chaos\/ A deeper dive into chaos]<br>\n[https:\/\/theconversation.com\/explainer-what-is-chaos-theory-10620 A nice take on Chaos]<br>\n[https:\/\/en.wikipedia.org\/wiki\/Chaos:_Making_a_New_Science The most definite guide to chaos] Note the synergies to the emergence of sustainability science<br>\n[https:\/\/www.r-bloggers.com\/2019\/05\/bayesian-models-in-r-2\/ Some intro into Bayesian statistics] in R<br>\n[https:\/\/www.wnycstudios.org\/podcasts\/radiolab\/episodes\/91684-stochasticity Stochasticity] just for kicks.\n\n\n====Videos====\n[https:\/\/www.youtube.com\/watch?v=fDek6cYijxI Chaos]: The Veritasium explanation<br>\n[https:\/\/www.youtube.com\/watch?v=5zI9sG3pjVU Laminar flow] is of course more awesome<br>\n[https:\/\/www.youtube.com\/watch?v=ovJcsL7vyrk Random] is not random, as this equation proves<br>\n[https:\/\/www.youtube.com\/watch?v=HZGCoVF3YvM One more take on Bayes]<br>\n\n\n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.","Overall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. \"Missing data: our view of the state of the art.\" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. \"Dealing with missing data.\" (2002).\n\nTsikriktsis, Nikos. \"A review of techniques for treating missing data in OM survey research.\" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]","'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Missing values: types and distributions==\n\nMissing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55).\n\nIn this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.\n\n==Types of missing values==\n\nThere are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149).\n\nUnit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).\n\nNext, we will look at how missing values can be distributed and what the implications of such distributions are. Generally, both types of missing values can occur in any distribution.\n\n==Distributions of missing values==\n\nThe distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).\n\n===Missing completely at random===\nMissing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).\n\nThe implications of MCAR for data analysis are relatively straightforward. Because the missing values are completely random, they do not introduce any bias into the analysis. Therefore, it is generally safe to impute the missing values using statistical methods, such as mean imputation or multiple imputations. However, even if the missing values are MCAR, there may still be other factors that can affect the analysis. It is important to consider the number and proportion of missing values (Scheffer 2002, 156). The larger the proportion of missing values in your overall dataset the less reliable is the use of the data. Imagine you had many unit nonresponse missing values across many different individuals, which results in having no variable without any missing value. This might affect the quality of your dataset. If and how this is the case, needs to be decided case by case.\n\n===Missing at random===\nMissing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55). For example, imagine you conduct a survey to analyze the relationship between education and income and there are missing values concerning income. If the missing values depend on education, then these missing values are missing at random. If they would depend on their actual income, they would not.\n\nThe implications of MAR for data analysis are more complex than those for MCAR. Because the missing values are not completely random, they may introduce bias into the analysis if they are not properly accounted for. Therefore, it is important to carefully consider the underlying reasons for the missing data and take these into account when imputing the missing values. One common approach to dealing with MAR missing values is to use regression or other statistical methods to model the relationship between the missing values and the other variables in the dataset (Tsikriktsis 2005, 56). Once the relationship is clear, other methods can be used to approximate to correct the variables for the bias due to the missing values missing at random.\n\n===Missing not at random===\nMissing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).\n\nThe implications of MNAR for data analysis are more complex than those for MCAR or MAR. Because the missing values are systematically related to the true values of the missing data, they can introduce bias into the analysis if they are not properly accounted for. In some cases, this bias may be difficult or impossible to correct, even with advanced statistical methods (Tsikriktsis 2005, 55).\n\n==Determining the randomness of missing data==\n\nThere are two common methods to determine the randomness of missing data. The first method involves forming two groups: one with missing data for a single variable and one with valid values for that variable. If significant differences are found between the two groups regarding their relationship to other variables of interest, it may indicate a non-random missing data process. The second method involves assessing the correlation of missing data for any pair of variables. If low correlations are found between pairs of variables, it may indicate complete randomness in the missing data (MCAR). However, if significant correlations are found between some pairs of variables, it may be necessary to assume that the data are only missing at random (MAR) (Tsikriktsis 2005, 55 - 56)."],"44":["[[File:ConceptVisualisationScenarioPlanning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Scenario Planning]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Scenario Planning is a systematic designation of potential futures to enable long term strategic planning.\n\n==Background==\n[[File:Scenario planning.png|400px|thumb|right|'''SCOPUS hits per year for Scenario Planning until 2019.''' Search terms: 'scenario planning', 'scenario construction', 'scenario-based', 'scenario study' in Title, Abstract, Keywords. Source: own.]]\n\n'''The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s'''. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were \"(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]\" in the US. (Amer et al. 2013, p.24).\n\n'''Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.''' At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.\n\n\n== What the method does ==\nScenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called \"scenarios\". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them \"(...) in coherent script-like or narrative fashion.\" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all \"fundamentally different\" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios \"(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties.\" (Schoemaker 1993, p.196).\n\n'''There is no ''one'' procedure for Scenario Planning''' (5). A commonly cited approach by Schoemaker (2, 3) includes the following steps:<br>\n1) Definition of time frame, scope, decision variables and major actors of the issue in question.\n\n2) Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. \n\n3) Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes.","They received feedback to these scenario drafts in a second round of workshops. Then, they used existent data on social-ecological trends in the region and rated how these trends might develop under each scenario, resulting in '''scenario maps,''' which provide a better idea of how the region would change under each scenario. \n<br>\n[[File:Scenario Planning - Exemplary study - Hanspach et al. 2014 - Scenario Maps.png|750px|thumb|center|'''Scenario maps created for Southern Transylvania.''' Each map shows how a specific social-ecological trend (three out of eight are shown) develop under one of the four scenarios. Red = likely, blue = unlikely. Source: Hanspach et al. 2014, p.38]]\n<br>\nFurther, '''visual representations (drawings) of the four scenarios''' were created to support imagination of what they would mean for the region.\n<br>\n[[File:Scenario Planning - Exemplary study - Hanspach et al. 2014 - Scenario Drawings.png|750px|thumb|center|'''These drawing visualize Southern Transylvania under each of the four developed scenarios.''' Source: Hanspach et al. 2014, p.37]]\n<br>\nOverall, the results of the scenario planning process and the analysis of the scenarios shows how this method can help identify the most important impact factors on future developments of a system and how distinct potential developments influence specific aspects of the system. It also becomes clear how this method can facilitate policy responses and how scenarios are a helpful form of communicating scientific data.\n\n\n== Key Publications ==\nWack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n* A first-hand report from inside of Shell's planning process in the 1960s.\n\nSchoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n* A detailed description of how to conduct Scenario Planning, explained through case studies in the advertisement industry.\n\nAmer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n* A (rather complex) overview on different types of Scenario Planning across the literature.\n\nSwart, R.J., Raskin, P., Robinson, J. 2004. ''The problem of the future: sustainability science and scenario analysis.'' Global Environmental Change 14(2). 137-146.\n* A conceptual paper that elaborates on the potential of scenarios in and for sustainability science.\n\n\n== References ==\n(1) Wack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n\n(2) Schoemaker, P.J.H. 1993. ''Multiple Scenario Development: Its Conceptual and Behavioral Foundation.'' Strategic Management Journal 14(3). 193-213.\n\n(3) Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n\n(4) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design Journal 20(1). 1916-1926.\n\n(5) Amer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n\n(6) Wiek et al. 2006. ''Functions of scenarios in transition processes.'' Futures 38(7). 740-766.\n\n(7)  Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n\n== Further Information ==\n* Shell works with Scenarios still today. [[On this websitehttps:\/\/www.shell.com\/energy-and-innovation\/the-energy-future\/scenarios.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL3NjZW5hcmlvcy5odG1s|On this website]], you find a lot of information about their scenario approach.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.","2) Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. \n\n3) Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes.\n\n4) Elimination of implausible or impossible futures and, based on the themes that emerged from these, the creation of new scenarios. This process is repeatedly done until internally consistent scenarios are found. The number of scenarios developed depends on the scope and purpose of the planning process (1, 5).\n\n[[File:Scenario Planning Process.png|800px|thumb|center|'''Evaluation of the number of scenarios developed.''' Source: Amer et al. 2013. p.33]]\n\n5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.\n\n[[File:Scenario Planning Example.png|800px|thumb|center|'''Exemplary scenarios for life in Australian cities.''' Source: Gaziulusoy & Ryan 2017, p.1920]]\n\n\n== Strengths & Challenges ==\n* Scenario Planning allows for any organization that deploys it to be more innovative, flexible and thus better prepared for unforeseen disruptions and changes. In a corporate context, this can reduce costs, provide market benefits and improve internal communication (3, 5).\n* Scenario Planning broadens the structural perspective of an actor to think about the future (5). For example, an oil company may well be able to assess risks in their technical processes of oil exploration and extraction, but only through a more detailed scenario analysis they may be enabled to include economic, political and societal trends into their planning (2).\n* Scenarios are psychologically attractive. They are a way of transforming seemingly disparate data into relatable, coherent narratives. They present uncertainty across scenarios instead of providing probabilistic information for all elements within each individual one. In addition, they reduce the complexity and uncertainty of the future into graspable states (2).\n* Scenario Planning differs from adjacent methodological approaches. While a ''scenario'' illustrates a possible state of the future, a ''vision'' (see [[Visioning & Backcasting]] revolves around a desirable state of the future without taking its likelihood into consideration. Compared to Visioning, Scenario Planning might therefore be more useful for actual [[Glossary|decision-making]] but might as well be too narrow to envision holistic systemic changes (6). Additionally, a ''prediction'' as the classical method of economic ''forecasting'' describes likely states of the future as an extension of current developments, without the openness for [[Glossary|change]] that is inherent to Scenario Planning. \n\n'''Good scenarios should fulfill a range of characteristics''' (3, 5):\n* they need to be plausible and internally consistent, i.e. capable of happening.\n* they should be relevant, i.e. of help for decision making and connected to the issue that is to be solved.\n* they should be archetypal, i.e. not represent variations on the same theme but describe distinct futures.\n* they should describe a future that is in a state in which \"the system might exist for some length of time, as opposed to being highly transient.\" (Schoemaker 1995, p.30)\n* they should challenge the existent way of thinking about the future.\n* while Scenario Planning generally permits actors to broaden their perspective, this only works if the construction of scenarios is not biased, which easily happens (2, 3). One may unconsciously look for confirming evidence for personal presuppositions when identifying trends and uncertainties. Also, overconfidence that certain trends will (not) prevail may distort one's assessment. This should be paid attention to during the process (3). As Schoemaker (1995, p.38) puts it: \"When contemplating the future, it is useful to consider three classes of knowledge: 1. Things we know we know. 2. Things we know we don't know. 3. Things we don't know we don't know. Various biases (...) plague all three, but the greatest havoc is caused by the third.\" Ignorance in terms of the future should be acknowledged and addressed in order to challenge biases. \"And this is where scenario planning excels, since it is essentially a study of our collective ignorance.\" (ibid).\n\n[[File:Scenario Planning They Believed It.png|450px|thumb|right|'''Throughout history, smart minds have underestimated technological, economic and political developments.''' Scenario Planning can be a mean of addressing the overconfidence in thinking that things will not change. Source: Schoemaker 1995, p.26]]"],"45":["[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n|'''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/>\n\n'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]\n\n'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions.\" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|'''Moreno's original work on Social Networks.''' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno's and Jennings' work was subsequently taken up and furthered as the field of ''''group dynamics', which was highly relevant in the US in the 1950s and 1960s.''' Simultaneously, sociologists and anthropologists further developed the approach in Britain. \"The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties.\" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n\"Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm.\" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that \"(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. '''Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.\"''' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as \"entitites\", \"actors\", \"vertices\" or \"agents\" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are \"(...) tied to one another through socially meaningful relations\" (Prell et al. 2009, p.503), which can be \"(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection\" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.","[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations \"(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns.\" (Prell et al. 2009, p.503). '''Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as \"dyads\", but rather the study of patterns within a network.''' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors' behavior emerges from the interaction between them: \"Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions.\" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent's actions (Borgatti et al. 2009)","==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* '''Network boundaries:''' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** ''position-based approach'': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** ''event-based'' approach: those who had participated in key events are believed to define the population\n** ''relation-based approach'': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the ''exogenously defined boundaries'', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* '''Type of ties:''' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: ''directed'' ties, which go from one node to another, and ''undirected ties'', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, \"(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact.\" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* '''Data Collection''': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a ''prompted recall'' approach, they are asked which people they would think of with regards to a specific topic (e.g. \"To whom would you go for advice at work?\") while they are shown a pre-determined list of potentially relevant individuals. In the ''free list'' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the '''visual representation''' of the network. For this, the researcher \"(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections.\" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the \"number of lines which it is necessary to traverse in order to get from one point to another.\" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|'''Different network structures.''' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|'''An exemplary network structure.''' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]"],"46":["'''Note:''' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nThis entry aims to introduce Stacked Area Plot and its visualization using R\u2019s <syntaxhighlight lang=\"R\" inline>ggplot2<\/syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> has the following syntax:\n\n'''Syntax''': <syntaxhighlight lang=\"R\" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()<\/syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=\"R\" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n<\/syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=\"R\" inline>\"uspopage\"<\/syntaxhighlight> using the function <syntaxhighlight lang=\"R\" inline>geom_area()<\/syntaxhighlight> from the <syntaxhighlight lang=\"R\" inline>ggplot2 package<\/syntaxhighlight>:\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n<\/syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of \u201cBlues\u201d, and add labels.\n\n<syntaxhighlight lang=\"R\" line>\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")+\n  labs(title = \"US Population by Age\", \n       subtitle = \"Between 1900 and 2000\",\n       x = \"Year\",\n       y = \"Population (Thousands)\")\n<\/syntaxhighlight>\n\n==References==\n\n* R Graphics Cookbook, 2nd edition by Winston Chang\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Maria Jose Machuca.","circle 1014 521 67 [[Stacked Area Plot|Stacked Area Plot, e.g. INCOME (x) and count data (y) of BOUGHT ITEMS (colors) (if y is EXPENSES: three variables)]]\ncircle 1174 502 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS IN MATHS (blue) and EXAM POINTS in HISTORY (green) per point (x)]]\ncircle 1438 521 67 [[Big problems for later|Multiple Regression, e.g. INCOME (y) per AGE (x) in different COUNTRIES]]\ncircle 1657 554 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]\ncircle 622 812 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER and categorical BRAND]]\ncircle 733 759 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color), POPULATION SIZE (bubble size)]]\ncircle 782 875 67 [[Big problems for later|Factor analysis]]\ncircle 1271 825 67 [[Big problems for later|Structural Equation Plot]]\ncircle 1394 771 67 [[Big problems for later|Ordination, e.g. numeric and categorical variables (AGE, INCOME, HEIGHT, PROFESSION) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\n<\/imagemap>","'''In short:''' Stacked bar plots show the quantitative relationship that exists between a main category and its subcategories. This entry helps visualise two different types of stacked bar plots, ''Simple Stacked Plots'' and ''Proportions Stacked Plots'', and explains the difference between them. For more on the basics of barplots, please refer to the [[Barplots, Histograms and Boxplots]] entry.\n\n\n==Stacked Barplots: Proportions vs. Absolute Values==\nStacked bar plots show the quantitative relationship that exists between a main category and its subcategories. Each bar represents a principal category and it is divided into segments representing subcategories of a second categorical variable. The chart shows not only the quantitative relationship between the different subcategories with each other but also with the main category as a whole. They are also used to show how the composition of the subcategories changes over time.\n\nStacked bar plots should be used for Comparisons and Proportions but with emphasis on Composition. This composition analysis can be static for a certain moment in time, or dynamic for a determined period of time.\n\nStacked bar Plots are two-dimensional with two axes: one axis shows categories, the other axis shows numerical values. The axis where the categories are indicated does not have a scale (*) to highlight that it refers to discrete (mutually exclusive) groups. The axis with numerical values must have a scale with its corresponding measurements units.\n\n\n===When you should use a stacked bar plot===\nThe main objective of a standard bar chart is to compare numeric values between levels of a categorical variable. One bar is plotted for each level of the categorical variable, each bar\u2019s length indicating numeric value. A stacked bar chart also achieves this objective, but also targets a second goal.\n\nWe want to move to a stacked bar chart when we care about the relative decomposition of each primary bar based on the levels of a second categorical variable. Each bar is now comprised of a number of sub-bars, each one corresponding with a level of a secondary categorical variable. The total length of each stacked bar is the same as before, but now we can see how the secondary groups contributed to that total.\n\n\n==Two types of Stacked Barplots==\n1. '''Simple Stacked Plots'''\u00a0place the\u00a0'''absolute value'''\u00a0of each subcategory after or over the previous one. The numerical axis has a scale of numerical values. The graph shows the absolute value of each subcategory and the sum of these values indicates the total for the category. Usually, the principal bars have different final heights or lengths.\n   \nWe use simple stacked plots when relative and absolute differences matter. Ideal for comparing the total amounts across each group\/segmented bar.\n\n[[File:Simple_stacked_barplot.png|400px|frameless|right]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecie <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(specie,condition,value)\n \n# Stacked\nggplot(data, aes(fill=condition, y=value, x=specie)) + \n    geom_bar(position=\"stack\", stat=\"identity\")\n<\/syntaxhighlight>\n\n2. '''Proportions Stacked Plots''' place the '''percentage''' of each subcategory after or over the previous one. The numerical axis has a scale of percentage figures. The graph shows the percentage of each segment referred to the total of the category. All the principal bars have the same height.\n\nIn proportions stacked plots the emphasis is on the percentage composition of each subcategory since the totals by category are not shown; in other words, they are used when the key message is the percentage of composition and not the total within the categories. We use proportions stacked plots only when relative differences matter.\n\n[[File:Proportions_stacked_barplot.png|400px|frameless|left]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecies <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(species,condition,value)\n \n# Stacked + percent\nggplot(data, aes(fill=condition, y=value, x=species)) + \n    geom_bar(position=\"fill\", stat=\"identity\")\n<\/syntaxhighlight>\n\n----\n[[Category:Statistics]] [[Category:R examples]]"],"47":["[[File:Thought Experiment Concept Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Thought Experiments]]]]\n\n<br\/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.\n\n== Background ==\n[[File:Scopus hits Thought Experiment.png|450px|thumb|right|'''SCOPUS hits per year for Thought Experiments until 2019.''' Search term: 'thought experiment' in Title, Abstract, Keywords. Source: own.]]\n\n'''The Thought Experiment may well be the oldest scientific method.''' The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the ''What if'' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early [[History of Methods|enlightenment]]. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the ''Origin of Species'' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term \"Thought Experiment\", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.\n\n== What the method does ==\nThought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.  \n\nThe easiest way to set up a Thought Experiment is to ask yourself a \"What if\" question. Many Thought Experiments resolve around decisions, choices or potential states of the future. A famous example is the Trolley experiment, where a train rolls towards five train track workers, who would all be killed be the oncoming train, unaware of their potential demise. You can now change the direction of the train, and lead it to another track. There, one worker would be killed. Uncountable numbers of variations of this experiment exist (see Further Information), as it can teach much about choice, ethics, and responsibility. For instance, many people would change the train direction, but hardly anyone would push a person onto the track to derail the train. This would save the other five, and the outcome would be the same. However the difference between pushing a lever or pushing a person has deep psychological ramifications that resolve around guilt. This exemplifies that the Thought Experiment does not necessarily have a ''best'' outcome, as the outcome depends - in this example - on your moral choices. Some might argue that you should hurl yourself onto the track to stop the train, thereby not changing the countable outcome, but performing a deeply altruistic act that saves everybody else. Most people would probably be unable to do this.","== Normativity ==\nThought experiments are as we have already learned as normative as are our assumptions about the world. Hume argued that Thought Experiments are based on the laws of nature, yet here I would disagree. While many famous Thought Experiments are about the physical world, others are only made up in our minds. Many Thought Experiments are downright impossible to be matched with our reality, and are even explicitly designed to do this, such as Thomas Nagels Encyclopaedia of the Universe. It is therefore important to realise that basically all Thought Experiments are in essence normative, and one could say even downright subjective. Building on Derek Parfit, I would however propose a different interpretation, and propose that we should not measure the normativity of Thought Experiments through the design and setting, but instead by their outcome. Many people might come to the same conclusions within a given Thought Experiment, and some conclusion drawn from Thought Experiments may matter more than others. Consequently the penultimate question - also for Derek Parfit - is whether there are some Thought Experiments that are not normative. \n\n== Outlook ==\nMuch of [[Glossary|art]] and the media can be seen as a Thought Experiment, and there are ample examples that Thought Experiments in the media and the arts triggered or supported severe societal transformations. Thought Experiments are of equal importance in ethics and physics, and the bridge-building of the methodological approach should not be overestimated. Examples from the past prove that Thought Experiments can enable a great epistemological flexibility and diversity. This flexibility is even so large that Thought Experiments serve as a bridge between the epistemological and the ontological, or in other words between everything we know - and how we know it - and everything we believe. By enabling the transformation of our own most individual thoughts, Thought Experiments may provide a boat or a bridge to link the metaphysical with the world of knowledge.\n\n== Key Publications ==\nParfit, D. (2011). ''On what matters.'' Oxford University Press.\n\nParfit, D. (1984). ''Reasons and persons''. OUP Oxford.\n\nKamm, F. M. (2015). The trolley problem mysteries. Oxford University Press.\n\nhttps:\/\/www.thoughtexperiments.net\/people-who-divide-like-an-amoeba\/\n\n== Further Information ==\nWhen you start thinking about '''variations of the famous Trolley experiment''', you can dive into normative questions of all sorts. Here are some versions that our students came up with (remember: in the classical version, a trolley threatens to kill five people on the tracks, and you have the choice to divert the train towards one other person instead).\n* The trolley can also be diverted towards five cows who are standing on the tracks. How do you value animals' lives compared to human lives?\n* The trolley threatens to kill one child, but can also be diverted towards three healthy, old people. Which role does age play for your decision?\n* The trolley might stop automatically. There is a 25% chance that it stops before it reaches three people that it is on track to kill, and a 75% chance of stopping when you divert it towards a single individual. How do probabilities influence the decision?\n* Instead of the option to divert the trolley to one individual instead of five, you have the option to push another person onto the tracks, which would stop the trolley. How does letting someone die differ from actively pushing them onto the tracks?\n* You are tied to the tracks, but can also divert the train down a cliff, killing the (unspecified number of) passengers. Would you?\n----\n[[Category:Qualitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden.","'''Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world.''' Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schr\u00f6dinger's Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences. \n\n[[File:Schr\u00f6dingers Cat.png|400px|thumb|left|'''Schr\u00f6dinger's Cat.''' Source: [https:\/\/ad4group.com\/schrodingers-cat-and-the-marketing-strategy\/ AD4Group].]]\n\nSo far, we see that there are Thought Experiments that resolve exclusively about a - subjective - human decision, and other types of Thought Experiments that are designed around setting in the physical world. The difference between these two is in the design of the experiment itself, as the first are always focused on the normative decisions or people, while the second focuses on our normative interpretation of anticipation of a design that is without a human influence. This distinction is already helpful, yet another dimension is about time. Many Thought Experiments are independent of time, while others try to reinterpret the past to make assumptions about a future about which we have no experience. Thought Experiments that focus on re-interpretation of the past (\"What if the assassination of Franz Ferdinands failed? Would the first World War still have happened?\") look for alternative pathways of history, often to understand the historical context and - more impotantly - the consequences of this context better. Most Thought Experiments are independent of a longer time scale. These experiments - such as the Trolley experiment - look at a very close future, and are often either very constructed or lack a connection to a specific context. Thought Experiments that focus on the future are often build around utopian or at least currently unthinkable examples that question the status quo, either form an ethical, societal, cultural or any other perspective. Such desirable futures are deeply normative yet can build an important bridge to our current reality through backcasting (\"What if there is no more poverty, and how can we achieve this?\"). \n\nAll this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https:\/\/en.wikipedia.org\/wiki\/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. '''Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.'''\n\n== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people's actions or decision, and the ''What if'' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. '''Schr\u00f6dinger's Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.''' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment \"What if I have an unlimited amount of ice cream?\" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the \"unlimited\", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments."],"48":["Values in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered \u201cclose\u201d to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object's neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object's neighborhood is significantly lower than the densities of its neighbors' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don't know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df['usage_kwh'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (\"to_numpy()\") and reshapes it to a single column (\"reshape (-1, 1)\"), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf['outlier'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df['outlier']] ## creates  a column where only outliers are selected\n\nplt.plot(df['usage_kwh']) ## plots the dataframe\nplt.scatter(x=outliers.index, y=outliers['usage_kwh'], color='tab:red', marker='x') ## creates a scatter plot with the outliers, marked with a red x\nplt.title('Outliers in the dataset (marked in red)') ## title of the plot\nplt.xlabel('Date') ## titlex axis\nplt.ylabel('Usage (KWh)')## title y axis\nplt.show()\n<\/syntaxhighlight>\n[[File:outlier plot.png|700px|center|]]\n<small>Figure 7: Outlier in the chosen part of the dataset (marked with red cross)<\/small>\n\nIn this case, these are probably false-positive outliers. The dataset is already pretty clean and does likely not contain many outliers.\n\nThe imputation of missing values is the next challenge after detecting outliers. The naive solution to this problem is to replace a missing point with the mean of all points. While this approach will not skew the data distribution significantly, the imputed values might be far off from the actual value and make messy graphics. An alternative is applying regression models that use predictive models to estimate missing points.\n\nA combined approach to both detects and replaces outliers is the Kalman Filter, which predicts new points based on the previous points and estimates a noise measure for new points. Thus, when an anomaly is detected, it is replaced by the prediction from the model using historical data (Hoogendoorn & Funk 2018).\n\n===Forecasting Time Series Data===\n\nForecasting time series data can be of tremendous value in information gain. Since this is a large topic, this article will only touch on one method for forecasting: AutoRegressive Integrated Moving Average models, or ARIMA for short. It consists of three parts:\n\n* The autoregressive (AR) component, which models the autocorrelation of the time series\n* The integrated (I) component, which models the non-stationary bits of the time series\n* The moving average (MA) component, which models the noise or error in the time series.\n\nEach component requires an appropriate selection of parameters. Choosing the right parameters can yield a powerful model. However, finding good values of the parameters can be difficult and requires further complicated techniques.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.api as sm ## needed to employ ARIMA","No long-term direction or seasonal patterns can be detected. But we can see that there is a large variance in the residuals, so quite a lot of noise.\n\n===Moving Average===\nWhen there is a lot of noise in the data, it can be useful to calculate the moving (or rolling) average. The moving average is a statistical measure that calculates the average of a window around a data point. It smooths out the data and helps identify patterns or trends that may not be immediately apparent.\n\n<syntaxhighlight lang=\"Python\" line>\ndf_full = pd.concat([\n    pd.DataFrame({'value': df['usage_kwh'], 'name': 'raw'}), ## creates a dataframe the actual residuals\n    pd.DataFrame({'value': df['usage_kwh'].rolling(window=24*4).mean(), 'name': 'day'}) ## creates a dataframe with the daily means (1-day time window, 24 hours times 4 measurements per hour (every 15 min)\n]) ##the concat command combines these two dataframes\npx.line(df_full, y='value', color='name',\n        title='Usage vs. a 1-day moving average of usage',\n        labels={'start_date': 'Date', 'value': 'Usage (KWh)'})\n<\/syntaxhighlight>\n\n[[File:MA plot.png|700px|center|]]\n<small>Figure 4: Electricity Usage with and without rolling average calculations<\/small>\n\nWe can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.api as sm ## needed to use the autocorrelation function\nautocorr = sm.tsa.acf(df['usage_kwh'], nlags=24*4*2)## determines #autocorrelation with a lag of 15 minutes over 2 days (24 hours * 4 (every 15 min) * 2 for two days) \nsteps = np. arange (0, len(autocorr) * 15, 15) \/ 60 ## produces an ##array of the length of the autocorrelation data times 15 (so per #minute) and indicates that each lag is 15 minutes apart. By dividing it by 60, the values are converted from minutes to hours.\npx.line(x=steps, y=autocorr, markers=True,\n        title='Autocorrelation of electricity usage',\n        labels={'x': 'Lag (hours)', 'y': 'Correlation'}) ## creates #plot of the autocorrelation function\n<\/syntaxhighlight>\n[[File:ACF plot.png|700px|center|]]\n<small>Figure 5: Autocorrelation of electricity usage over two days<\/small>\n\nThe autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df['usage_kwh'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15\/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel('Lag (hours)') ## title x axis\nplt.ylabel('Autocorrelation of electricity usage with confidence interval') ## title y axis\nplt.title('') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n<\/syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days<\/small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.","==== Covariance matrix ====\n\nThe covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties:\n* The size of the matrix is equal to the number of features in the data\n* The main diagonal on the matrix contains the variances of each initial variables.\n* The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2)\n\nThe covariance matrix gives you a summary of the relationship among the initial variables.\n* A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa)\n* A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa)\n\n==== Eigenvectors \/ Principle Components & Eigenvalues ====\nNow we have the covariance matrix. This matrix can be used to transform one vector into another. Normally when this transformation happens, two things happen: the original is  rotated and get streched\/squished to form a new vector. When an abitrary vector is multipled by the covariance matrix, the result will be a new vector whose direction is nudged\/rotated towards the greatest spread in the data. In the figure below, we start with the arbitrary vector (-1, 0.5) in red. Multiplying the red vector with covariance matrix gives us the blue vector, and repeating this gives us the black vector. As you can see, the result rotation tends to converge towards the widest spread direction of the data.\n\n[[File: PCAEigenvector01.png|center|500px]]\n\nThis prompts the questions: Can we find directly find the vector which already lies on this \"widest spread direction\". The answer is yes, with the help of eigenvectors. Simply put, an eigenvectors of a certain matrix is a vector that, when transformed by the matrix, does not rotate. It remains on its own span, and the only thing that changes is its magnitude. This (constant) change ratio in magnitude corresponding to each eigenvector is called eigenvalue. It indicates how much of the data variability can be explained by its eigenvector.\n\nFor this toy dataset, since there are two dimensions, we get (at most) two egenvectors and two corresponding eigenvalues. Even if we only plot the eigenvectors scaled by their eigenvalues, we will basically have a summary data (and its spreading). At this point, the eigenpairs are be viewed as the principle components of the data.\n\n[[File: PCAEigenvector02.png|center|500px]]\n\n==== Ranking the principle components ====\nAs you may have noticed, the eigenvectors are perpendicular to each other. This is no coincidence. You can think of it this way: because we want to maximize the variance explained by each of the principle components, these components need to be independent from one another, therefore their orthogonality.  Thus, to define a set of principle components, you find the direction which can explain the variability in the data the most: that is your first principle component (the eigenvector with the highest eigenvalue). The second principle compent will be percepdicular to the first, and explain most of what is left of the variability. This continues until the d-th principle component is found.\n\nBy doing so, you are also sorting the \"importance\" of the principle components in terms of the information amount it contains what is used to explain the data. To be clear, the sum of all eigenvalues is the total variability in the data. From here, you can choose to discard any PCs whose percentage of explained variances are low. In many cases, if around 80% of the variance can be explained by the first k PCs, we can discard the other (d - k) PCs. Of course, this is only one of the heuristics method to determine k.  You can also use thr elbow method (the scree plot) like in k-means.\n\n==== Summary ====\n* PCA is a feature extraction technique widely used to reduce dimensionality of datasets.\n* PCA works by calculating the eigenvectors and the corresponding eigenvalues of the initial variables in the data. These are the principle components. Number of PCs = number of eigenvectors = number of features.\n* The PCs are ranked by the eigenvalues, and iteratively show the directions in which the data spreads the most (after accounting for the previous PCs).\n* We can choose to keep a few of the first PCs that cummulatively explains the data well enough, and these are the new reduced dimension of the data.\n* Standardization is a crucial step in data pre-processing to ensure the validity of the PCA results.\n\n\n=== R Example ===\nGoing back to the example in the introduction, the dataset can be found here: https:\/\/www.kaggle.com\/sdhilip\/nutrient-analysis-of-pizzas\n\n<syntaxhighlight lang=\"R\" line>\n# Loading library\nlibrary(tidyverse) # For pre-processing data\nlibrary(factoextra) # For visualization\ntheme_set(theme_bw()) # Set theme for plots\n\n# Load data\ndata <- read_csv(\"Pizza.csv\")\nhead(data)\n<\/syntaxhighlight>\n\nAs shown here, there are seven measurements of nutrients for each pizza. Our goal is to reduce these seven dimensions of information down to only two, so that we can present the main patterns in our data on a flat piece of paper."],"49":["[[File:Lueneburg 2030.jpg|thumb|left|L\u00fcneburg 2030+ ist ein wunderbares Beispiel f\u00fcr eine Reihe von Realwelt-Eperimenten. Diese Karte bietet einen \u00dcberblick \u00fcber die verschiedenen Experimente.]]\n\nIn dieser Hinsicht sind [https:\/\/journals.sagepub.com\/doi\/pdf\/10.1177\/0963662505050791 Realwelt-Experimente] die neueste Entwicklung in der Diversifizierung des Experimentierfeldes. Diese Art von Experimenten wird derzeit in der Literatur umfassend untersucht, doch ich erkenne in der verf\u00fcgbaren Literatur bisher kein einheitliches Verst\u00e4ndnis dessen, was Experimente aus der realen Welt sind. Diese Experimente k\u00f6nnen jedoch als eine Fortsetzung des Trends der nat\u00fcrlichen Experimente angesehen werden, bei denen eine l\u00f6sungsorientierte Agenda versucht, eine oder mehrere Interventionen zu generieren, deren Wirkungen oft in Einzelf\u00e4llen getestet werden, wobei die Bewertungskriterien jedoch bereits vor der Durchf\u00fchrung der Studie klar sind. Die meisten Studien haben dies bisher mit Nachdruck definiert; dennoch zeichnet sich die Entwicklung von Experimenten aus der realen Welt erst langsam ab.\n\n\n== Experimente bis heute ==\nUnter dem Oberbegriff 'Experiment' werden somit unterschiedliche methodische Ans\u00e4tze zusammengefasst. W\u00e4hrend einfache Manipulationen wie medizinische Verfahren bereits in der Aufkl\u00e4rung als Experiment bekannt waren, gewann der Begriff 'Experiment' im Laufe des 20. Jahrhunderts an Bedeutung. Botanische Experimente wurden schon lange vorher durchgef\u00fchrt, aber es waren die Agrarwissenschaften, die die notwendigen methodischen Entw\u00fcrfe zusammen mit den geeigneten [[Bachelor Statistics Lecture|statistischen Analysen]] entwickelten und damit eine statistische Revolution ausl\u00f6sten, die in zahlreichen Wissenschaftsbereichen Wellen schlug. Die Varianzanalyse ([[ANOVA]]) wurde zum wichtigsten statistischen Ansatz zu diesem Zweck und erm\u00f6glichte die systematische Gestaltung von Versuchsanordnungen, sowohl im Labor als auch im landwirtschaftlichen Bereich.","'''Integration, Reflexion und Normativit\u00e4t sind Komponenten der Methodologie, die sich langsam durchsetzen.''' W\u00e4hrend der Tiefenfokus wahrscheinlich die Hauptstrategie der meisten Forscher bleiben wird, verlangen die \"wicked problems\", mit denen wir derzeit konfrontiert sind, nach neuen Ans\u00e4tzen, um eine Transformation zu erm\u00f6glichen und die damit verbundenen Mechanismen und Strukturen zu untersuchen. Vieles ist noch unbekannt, und wir m\u00fcssen die Fokussierung auf Ressourcen, die Anspr\u00fcche von Wissenschaftlern, das Wissen anderer Wissenschaftler zu bewerten und zu beurteilen, und nicht zuletzt die tiefe Verankerung der wissenschaftlichen Disziplinen, wenn es um ihre unterschiedlichen Methoden geht, \u00fcberwinden. \n\nUm Nietzsche zu zitieren: \"Nie gab es eine so neue Morgenr\u00f6te und einen so klaren Horizont, und ein so offenes Meer.\"\n\n\n== Additional Information ==\n* Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to southern Transylvania''. Ecology and Society 19(4): 32.\nZeigt eine Kombination verschiedener Methoden in empirischer Forschung auf, darunter Scenario Planning, GIS, Causal-Loop Diagrams).\n\n* Schreier, M. & Odag, \u00d6. ''Mixed Methods.'' In: G. Mey K. Mruck (Hrsg.). ''Handbuch Qualitative Forschung in der Psychologie.'' VS Verlag f\u00fcr Sozialwissenschaften | Springer Fachmedien Wiesbaden GmbH 2010. 263-277.\nEine gute Einf\u00fchrung in das Mixed Methods-Konzept.\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.\n[[Category:Normativity_of_Methods]]","'''1) Neue Methoden erfinden'''\nNeue Methoden zu erfinden klingt sicherlich aufregend. Es gibt eine ganze Welt des Wissens, die auf uns wartet, und neue Methoden k\u00f6nnten potenziell in der Lage sein, dieses neue Wissen zu erschlie\u00dfen. Trotz des Geistes, der viele Forschenden zu diesem Ziel motiviert, m\u00f6chte ich hervorheben, dass in der Vergangenheit die Erfindung neuer Methoden oft aus der klaren Erkenntnis eines Mangels an Methoden zur L\u00f6sung eines bestimmten Problems entstand. Die Vorschl\u00e4ge des Interviews als wissenschaftliche Methode oder der [[Grounded Theory]] wurzelten in der Erkenntnis eines Mangels an einem geeigneten methodischen Ansatz f\u00fcr ein spezifisches Problem. Als Beispiel seien hier [[Open Interview|offene Interviews]] genannt, die ein induktives Verst\u00e4ndnis der Wahrnehmungen von Individuen erm\u00f6glichten. Diese Art der Erkenntnis als solche gab es vorher nicht, und durch den Vorschlag dieser neuen Methoden konnte eine L\u00fccke geschlossen werden. Um diese L\u00fccke zu schlie\u00dfen, musste meiner Meinung nach diese L\u00fccke erkannt werden, was auf dem Erkennen von klarem Wissen und Erfahrungen mit Methoden, die bereits vorhanden waren, beruhte. \n\nNehmen wir zum Beispiel auch Fischers Varianzanalyse, die im Grunde genommen die Durchf\u00fchrung systematischer [[Experiments and Hypothesis Testing|Experimente]] auf eine v\u00f6llig neue Ebene brachte. Fischer sah die bestehende Forschung an dem landwirtschaftlichen Forschungszentrum, an dem er als Statistiker arbeitete, und er sah einen klaren Mangel an einer systematischen Methode, die es erlaubte, Wissen im Sinne von Mustererkennung basierend auf der Pr\u00fcfung einer vorherrschenden Hypothese zu generieren. Fischer kannte den Stand der Technik, und er erkannte eine deutliche L\u00fccke im Kanon der vorhandenen Methoden. Dar\u00fcber hinaus hatte er Erfahrung mit den bereits existierenden Methoden, was eine Voraussetzung f\u00fcr seine Formulierung der Varianzanalyse war. Die Erfindung neuer Methoden ist also durchaus ein allm\u00e4hlicher Prozess in einer kontinuierlichen Entwicklung, auch wenn Wissenschaftshistoriker*innen sie oft auf einen einzelnen Zeitpunkt reduzieren. Ich schlage daher vor, eher ein Kontinuum anzuerkennen, in dem solche neuen Methoden vorgeschlagen werden. Solche innovativen Einstellungen bauen typischerweise auf der Erfahrung mit bestehenden Methoden auf, und damit auf der Erkenntnis einer L\u00fccke innerhalb der bereits existierenden Methoden, oder besser noch, auf dem Wissen, das diese Methoden produzieren k\u00f6nnen. \n\n'''2) Relokation bestehender Methoden'''\nMethodische Innovationen bauen oft auf bestehenden Methoden auf, wenn diese in einem anderen Umfeld oder Kontext verwendet werden. So wurden z. B. [[Semi-structured Interview|Interviews]] in der 'World Value Survey' auf globaler Ebene angewandt und erm\u00f6glichten einen globalen Vergleich der Werte der Menschen auf der Grundlage von zehntausenden von Interviews. Die Methode der strukturierten Interviews gab es schon lange vorher, und man k\u00f6nnte argumentieren, dass solche gr\u00f6\u00dferen Umfragen sogar der 1960 von Strauss und Glaser verk\u00fcndeten Befragung vorausgingen. Ein globaler Vergleich war jedoch eindeutig einzigartig. Ein anderes Beispiel ist, wie die Varianzanalyse einst weitgehend f\u00fcr [[:Category:Deductive|deduktive]] Experimente reserviert war, bis der [[History of Methods|Aufstieg der Datenwissenschaft]] und die Verf\u00fcgbarkeit von mehr Daten durch das Internet und andere Quellen zur Anwendung der Varianzanalyse in rein [[:Category:Induktive|induktiven]] Settings f\u00fchrte. Ein weiteres Beispiel w\u00e4ren die vielf\u00e4ltigen Anwendungen der Textanalyse, die einst f\u00fcr ein sehr spezifisches Setting postuliert wurden, heute aber in allen m\u00f6glichen Wissenschaftszweigen Anwendung finden. Die Verlagerung einer Methode nutzt im Grunde eine bestehende Methode in einem anderen Setting oder Kontext, als sie urspr\u00fcnglich gedacht war. Auch hier bedeutet dies nicht nur, dass die Erfahrung mit der bestehenden Methode eine notwendige Voraussetzung f\u00fcr ihre Verlagerung ist, sondern das eigentliche Innovationspotenzial ergibt sich aus dem Erkennen einer L\u00fccke, und dass die Verlagerung der bestehenden Methode diese L\u00fccke durch die Schaffung neuen Wissens schlie\u00dfen kann."]},"context_precision":{"0":0.0062111801,"1":0.0066225166,"2":0.0054945055,"3":0.0,"4":0.0051020408,"5":0.0520231214,"6":0.0217391304,"7":0.0375,"8":0.0422535211,"9":0.0462962963,"10":0.0063291139,"11":0.0070921986,"12":0.0304878049,"13":0.0047619048,"14":0.0322580645,"15":0.0052356021,"16":0.0070921986,"17":0.0073529412,"18":0.0784313725,"19":0.0582524272,"20":0.009009009,"21":0.025974026,"22":0.0442477876,"23":0.1801801802,"24":0.0408163265,"25":0.0071942446,"26":0.0063291139,"27":0.0454545455,"28":0.0080645161,"29":0.0073529412,"30":0.0866666667,"31":0.0833333333,"32":0.0420168067,"33":0.0555555556,"34":0.009009009,"35":0.0384615385,"36":0.0229007634,"37":0.0804597701,"38":0.119047619,"39":0.0074626866,"40":0.0052631579,"41":0.0647482014,"42":0.0427350427,"43":0.0,"44":0.0451612903,"45":0.0125,"46":0.0126582278,"47":0.102739726,"48":0.0173410405,"49":0.0},"context_recall":{"0":1.0,"1":0.0,"2":1.0,"3":0.0,"4":0.0,"5":0.0,"6":1.0,"7":1.0,"8":1.0,"9":0.6,"10":1.0,"11":1.0,"12":1.0,"13":1.0,"14":1.0,"15":0.0,"16":0.0,"17":1.0,"18":1.0,"19":1.0,"20":0.0,"21":1.0,"22":0.0,"23":1.0,"24":0.3333333333,"25":0.0,"26":1.0,"27":1.0,"28":1.0,"29":1.0,"30":0.6,"31":1.0,"32":1.0,"33":0.5714285714,"34":0.0,"35":0.6666666667,"36":1.0,"37":1.0,"38":1.0,"39":0.0,"40":1.0,"41":0.8,"42":0.0,"43":0.0,"44":1.0,"45":0.5,"46":0.0,"47":1.0,"48":0.0,"49":0.0}}