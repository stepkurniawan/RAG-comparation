{"question":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA a powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is the purpose of ANCOVA in statistical analysis?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram and how is it created?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"Reducing variance in field experiments or complex laboratory experiments","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA is used to compare group means while controlling for the effect of a covariate.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"contexts":{"0":["'''Advantages'''\nA\/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.","==Advantages and Limitations of A\/B Testing==\n'''Advantages'''","Overall, A\/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented."],"1":["The ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of","'''The [https:\/\/www.investopedia.com\/terms\/a\/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of","The ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types"],"2":["but one could say that frequentist statistics are top-down thinking, while [https:\/\/365datascience.com\/bayesian-vs-frequentist-approach\/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago.","'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are","== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin."],"3":["'''What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency.''' Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy's law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of","Being happy is a serendipity. Those who can be optimistic can consider themselves lucky, and have a responsibility towards others. This responsibility is a truly societal responsibility, because only if we can all enable everybody to be at peace may we overcome our deeply rooted problems. Our first and foremost priority will be to learn ways to marginalise minorities less, and help humans less privileged than us. We should never forget that optimism can obviously be also part of a privilege. I was privileged enough to see","What is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never"],"4":["'''How do I know?'''\n* Investigate your data using <code>str<\/code> or <code>summary<\/code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.","#lets split the dataset into 3 datasets for each species\nversicolor <- subset(iris, Species == \"versicolor\")\nvirginica <- subset(iris, Species == \"virginica\")\nsetosa <- subset(iris, Species == \"setosa\")","#Lets split the dataset into 3 datasets for each species:\nversicolor <- subset(iris, Species == \"versicolor\")\nvirginica <- subset(iris, Species == \"virginica\")\nsetosa <- subset(iris, Species == \"setosa\")"],"5":["<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.","==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.","distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."],"6":["|}\n<br\/>\n<br\/>\n<br\/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.","Clustering is a method of grouping unlabeled data based on a certain metric, often referred to as ''similarity measure'' or ''distance measure'', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named \"unsupervised learning\", although clustering is also performed in many other fields such as data compression, pattern recognition, etc.","== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more."],"7":["== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.","'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]","* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on"],"8":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==","Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means","===== ANCOVA =====\n'''If you have at least one categorical predictor, you should do an ANCOVA'''. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more."],"9":["the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.","== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.","'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites =="],"10":["the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.","== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.","'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites =="],"11":["1989, p.403). Because of this, Content Analysis is a potent method to identify trends and patterns in (text) sources, to determine authorship, or to monitor (public) opinions on a specific topic (3).","==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.","Content Analysis is a \"(...) systematic, replicable technique for compressing many words of text into fewer content categories based on explicit rules of coding\" (Stemler 2000, p.1). However, the method entails more than mere word-counting. Instead, Content Analysis relies on the interpretation of the [[Glossary|data]] on behalf of the researcher. The mostly qualitative data material is assessed by creating a category system relevant to the material, and attributing parts of the"],"12":["'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships.","* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (\u201ccircle\u201d, \u201csquare\u201d, \u201cellipse\u201d, \u201cnumber\u201d, \u201cshade\u201d, \u201ccolor\u201d, \u201cpie\u201d), \u201ccircle\u201d is called by default and shows the correlation between the variables in different colors and sizes for the circles.","While Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs."],"13":["== Correlogram ==\n=== Definition ===","The correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot<\/syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients","correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear."],"14":["Telemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is \u201cthe system of determining information about an animal through the use of radio signals from or to a device carried by the animal\u201d (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can","(11) Gutema, T. M. (2015). Wildlife radio telemetry: use, effect and ethical consideration with emphasis on birds and mammals. Int J Sci Basic Appl Res, 24(2), 306-313.","THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data=="],"15":["The most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https:\/\/link.springer.com\/content\/pdf\/10.1186\/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is","Most things in their natural state follow a normal distribution. If somebody tells you that something is not normally","Reality is complex as people like to highlight today, but in a statistical sense that is often true. Most real datasets do not show a perfect model fit, but instead may individual data points deviate from the perfect fit the model assumes. What is now critical, is to assume that the error reflected by the residuals is normally distributed. Why, you ask? Because if it is not normally distributed, but instead shows a flawed pattern, then you missed an important variable that influences the distribution of the"],"16":["You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.","==== Data distribution ====","== Data distribution =="],"17":["* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).","== Normativity ==\n==== Connectedness \/ nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)","the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3)."],"18":["We can thus conclude that urgency, wickedness, normativity, context, scale integration and many other challenges are currently recognised in the community of sustainability science researchers, and deserve more focus in order to generate solutions. This non-exhaustive list of problems already showcases that this is more easily proposed than actually done. Within sustainability science and beyond, there is almost an obsession to question the ''status quo'', and to proclaim what methods should do. It sounds so appealing to \"dance with the system\"","The goal is to actively engage with sustainability problems in one\u2019s surroundings from the beginning on and thereby understand concepts, principles, methods of sustainability and think about solution options.\n\nEssential for this is the '''development of [https:\/\/www.youtube.com\/watch?v=1vHY2RJnr3A sustainability competencies]''', especially systems thinking, normative, and collaborative competencies as named by Wiek et al. (2011).","It can however be stated that Visioning and Backcasting may provide essential benefits for Sustainability research in the future. Sustainability Science has been framed to aim for System, Target and Transformation Knowledge and targets real-world problems and their solutions in integrated, participatory modes (see Brandt et al. 2013. ''A review of transdisciplinary research in sustainability science''). To this end, a Visioning process builds on system knowledge, and aims for target knowledge, while Backcasting provides transformation knowledge,"],"19":["'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.","as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.","important developments of the 20th century: Critical Theory."],"20":["'''The system is at the basis of System Thinking.''' System Thinking is a form of scientific approach to organizing and understanding 'systems' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of 'holistic' understanding, i.e. the idea that the components of","* System Thinking is an important foundation for any method that views interactions between different agents. This is of relevance to methods that support thinking about and planning for the future, most notably [[Scenario Planning]] as well as [[Visioning & Backcasting]].","* ''Systems Thinking'': Systems Thinking competency is the ability to analyze and understand [[Glossary|complex systems]] including the dynamics in the interrelation of their parts. Systems thinking integrates different domains (society, environment, economy, etc.) as well as different scales (from local to global)."],"21":["== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:","== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.","# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you"],"22":["Within [[ANOVA]] designs, the question whether a variable is a [https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within","design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are","[https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https:\/\/en.wikipedia.org\/wiki\/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https:\/\/www.simplypsychology.org\/milgram.html Field Experiments in Psychology]: A famous example"],"23":["a deeper understanding of the specific context and case. While real world experiments emerged some decades ago already, they are only starting to gain wider recognition. All the while, the reproducibility crisis challenges the classical laboratory and field experiments, as a wider recognition that many results - for instance from psychological studies - cannot be reproduced. All this indicates that while much of our scientific knowledge is derived from experiments, much remains to be known, also about the conduct of experiments themselves.","tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial","failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory"],"24":["== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment.","* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.","* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered."],"25":["consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.","<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.","distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."],"26":["==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.","'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.","== How to interpret a heatmap? =="],"27":["the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of","an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''","Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https:\/\/www.britannica.com\/biography\/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental"],"28":["How can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen\/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.","Multivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often","==Data Visualization=="],"29":["Machine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight","== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.","__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data."],"30":["== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.","* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.","There is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century."],"31":["=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''.","== What are scientific methods? ==","'''Based on these considerations, one needs to remember the following criteria when you approach a concrete scientific method:''' \n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]"],"32":["== Goals ==\nSince the goal of mindfulness is basically having \"no mind\", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free.","Mindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in","Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!"],"33":["== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]","'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus","== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan."],"34":["agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed","Mixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would","wurde daher durch zus\u00e4tzliche Modifikationen erg\u00e4nzt, was schlie\u00dflich zu fortgeschritteneren Statistiken f\u00fchrte, die in der Lage waren, sich auf verschiedene statistische Effekte zu konzentrieren'', und den Einfluss von Bias zu reduzieren, z.B. in Stichproben, statistischem Bias oder anderen Fehlern. Somit wurden [[Mixed-Effect Models]] zu einem fortgeschrittenen n\u00e4chsten Schritt in"],"35":["'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real","Typically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate","Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models."],"36":["== '''Starting to engage with model reduction - an initial approach''' ==","The most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of","reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics."],"37":["=== Redundancy analysis ===","== '''Starting to engage with model reduction - an initial approach''' ==","to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values."],"38":["Narrative Research is \"(...) the study of stories\" (Polkinghorne 2007, p.471) and thus \"(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience.\" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative","'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled","|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Narrative Research describes qualitative field research based on narrative formats which are analyzed and\/or created during the research process."],"39":["Over the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see","* Hastie, T. & Tibshirani, R. 1986. ''Generalized Additive Models''. Statistical Science 1(3). 297-318.","Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer"],"40":["Poisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=\"wikitable\"\n|-\n! Distribution !! Definition\n|-","For example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https:\/\/studiousguy.com\/poisson-distribution-examples\/ this page]."],"41":["== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:","== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details","# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to ''25 minutes = 1 \"Pomodoro\"'').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper)."],"42":["=== Curse of dimensionality ===","How can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen\/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.","This term was coined by Richard R. Bellman, an American applied mathematician. As the number of features \/ dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a"],"43":["Firstly, the aim for analyzing the dataset is to figure out the performance scored among the learning groups and gender for the solved questions, exercises and written exams.\n\nSecondly, we want to figure out the correlation between variables and most importantly to figure out heteroscedastic and homoscedastic dispersion, since the OLS is only apt when homoscedasticity is the case.\n\nTo analyse this data, some python libraries have to be imported:","==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.","The histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion."],"44":["As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success","'''Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.''' At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared","oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2)."],"45":["'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the","Romanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the","Moreno's and Jennings' work was subsequently taken up and furthered as the field of ''''group dynamics', which was highly relevant in the US in the 1950s and 1960s.''' Simultaneously, sociologists and anthropologists further developed the approach in Britain. \"The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties.\" (Scott"],"46":["==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==","'''Note:''' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].","A Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total."],"47":["|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.","Thought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other","us to understand something basically about ourselves. '''Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.'''"],"48":["===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.","Thus, let us focus on the main obstacle to this bright future that we have faced ever since we began: ''Temporal autocorrelation.'' This principle defines that we humans value everything that occurs in the close past or future to be more relevant than occurrences in the distant future or past. This is even independent of the likelihood whether future events will actually happen. As an example, imagine that you want to have a new computer every few years, and you can pay 5\u20ac to have a","== Temporal grain and measures of time =="],"49":["* The research project [http:\/\/besatz-fisch.de\/content\/view\/34\/57\/lang,german\/ \"Besatzfisch\"] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to '''understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.''' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of","* Das Forschungsprojekt [http:\/\/besatz-fisch.de\/content\/view\/34\/57\/lang,german\/ \"Besatzfisch\"] ist ein gutes Beispiel f\u00fcr ein langfristiges transdisziplin\u00e4res Forschungsprojekt, das sich mit unterschiedlichen methodischen Ans\u00e4tzen besch\u00e4ftigt. In diesem vierj\u00e4hrigen Projekt wurde versucht, '''die \u00f6kologische, soziale und wirtschaftliche Rolle und die","& quantitatively measured, much of this jointly with the cooperating anglers (''Cooperation''). Second, anglers were questioned about fish population sizes and their economic implications (''Consultation'') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (''Consultation''). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds"]},"context_precision":{"0":0.5555555556,"1":0.4545454545,"2":0.5454545455,"3":0.0,"4":0.0,"5":0.3846153846,"6":0.25,"7":0.3571428571,"8":0.1818181818,"9":0.0,"10":0.0,"11":0.6363636364,"12":0.3333333333,"13":0.3333333333,"14":0.0,"15":0.3333333333,"16":0.25,"17":0.375,"18":0.4444444444,"19":0.5714285714,"20":0.6666666667,"21":0.2,"22":0.25,"23":0.3125,"24":0.8,"25":0.4545454545,"26":0.0769230769,"27":0.125,"28":0.4444444444,"29":0.2222222222,"30":0.3636363636,"31":0.3,"32":0.5,"33":0.3333333333,"34":0.1111111111,"35":0.7,"36":0.4285714286,"37":0.6,"38":0.2727272727,"39":0.2727272727,"40":0.0555555556,"41":0.3684210526,"42":0.5555555556,"43":0.0909090909,"44":0.1666666667,"45":0.4545454545,"46":0.0909090909,"47":0.3333333333,"48":0.2857142857,"49":0.1},"context_recall":{"0":1.0,"1":0.0,"2":1.0,"3":0.0526315789,"4":0.0,"5":1.0,"6":1.0,"7":1.0,"8":1.0,"9":0.5,"10":1.0,"11":0.0,"12":1.0,"13":0.6666666667,"14":1.0,"15":0.1666666667,"16":1.0,"17":1.0,"18":0.5,"19":1.0,"20":1.0,"21":1.0,"22":1.0,"23":1.0,"24":1.0,"25":1.0,"26":1.0,"27":1.0,"28":1.0,"29":1.0,"30":1.0,"31":0.3333333333,"32":1.0,"33":1.0,"34":0.0,"35":1.0,"36":0.5,"37":0.0,"38":0.3333333333,"39":0.6666666667,"40":1.0,"41":0.3333333333,"42":1.0,"43":1.0,"44":1.0,"45":0.6666666667,"46":0.2857142857,"47":1.0,"48":1.0,"49":0.8}}