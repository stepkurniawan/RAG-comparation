|question|ground_truths|contexts|context_precision|context_recall
0|What is the advantage of A/B testing?|The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.|"[""==Advantages and Limitations of A/B Testing==\n'''Advantages'''\nA/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. “Online Controlled Experiments and A/B Testing.” Encyclopedia of Machine Learning and Data Mining, 2017, 922–29. https://doi.org/10.1007/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. “Experimentation and Start-up Performance: Evidence from A/B Testing.” Management Science 68, no. 9 (September 2022): 6434–53. https://doi.org/10.1287/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A / B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart"", '\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==A/B Testing in a nutshell==\nA/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n\'\'\'For example:\'\'\'\n\n""To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.""\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\n===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.\n\nThe sample size is determined by the percentage of users included in the test variants (control and treatment) and the duration of the experiment. As the experiment runs for a longer period of time, more visitors are exposed to the variants, resulting in an increase in the sample size. Because many external factors vary over time, it is important to randomize over time by running the control and treatment variants simultaneously at a fixed percentage throughout the experiment. Thereby the goal is to obtain adequate statistical power, where the statistical power of an experiment is the probability of detecting a particular effect if it exists. In practice, one can assign any percentages to the control and treatment, but 50% gives the experiment maximum statistical power.\n\nFurthermore, it is important to analyze only the subset of the population/users that were potentially affected. For example, in an A/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.\n\n==Execution Phase==\nThe execution phase involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan. During this phase, users are randomly assigned to the control or treatment group ensuring that the study is conducted in a controlled and unbiased manner.\n\n==Evaluation Phase==\nThe evaluation phase involves analyzing the data collected during the study and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about whether there was a statistical significant difference between the treatment group and the control group. One commonly used method is calculating the [[Designing_studies#P-value|p-value]] of the statistical test, or by using [https://www.youtube.com/watch?v=9TDjifpGj-k Bayes\' theorem] calculating the probability that the treatment had a positive effect based on the observed data and the prior beliefs about the treatment.\n\nDepending on the type of data being collected different [[Simple Statistical Tests|statistical tests]] should be considered. For example, when dealing with discrete metrics such as click-through rate, the [https://mathworld.wolfram.com/FishersExactTest.html Fisher exact test] can be used to calculate the exact p-value, while the [https://www.youtube.com/watch?v=7_cs1YlZoug chi-squared test] may be more appropriate for larger sample sizes.\n\nIn the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch\'s t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A/B Testing==\n\'\'\'Advantages\'\'\'\nA/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.', '\'\'\'In short:\'\'\' T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on two-sample T-tests and covers the concept and purpose of the t-test, underlying assumptions, its implementation in R, as well as multiple variants for different conditions. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]].\n\n==General Information==\nLet us start by looking at the basic idea behind a two-tailed two-sample t-test. Conceptually speaking we have two hypotheses. \n\nH<sub>0</sub>: Means between the two samples do not differ significantly.<br/>\nH<sub>1</sub>: Means between the two samples do differ significantly. <br/>\n\nIn mathematical terms (μ<sub>1</sub> and μ<sub>2</sub> denote the mean values of the two samples): \n\nH<sub>0</sub>: μ<sub>1</sub> = μ<sub>2</sub> <br>\nH<sub>1</sub>: μ<sub>1</sub> ≠ μ<sub>2</sub>. \n\n[[File:Kurt_olaf.jpg|500px|frameless|right]]\n\'\'\'Here is an example to illustrate this.\'\'\' The farmers Kurt and Olaf grow sunflowers and wonder who has the bigger ones. So they each measure a total of 100 flowers and put the values into a data frame.\n\n<syntaxhighlight lang=""R"" line>\n# Create a dataset\nset.seed(320)\n\nkurt <- rnorm(100, 60.5, 22)\nolaf <- rnorm(100, 63, 23)\n</syntaxhighlight>\n\nOur task is now to find out whether means values differ significantly between two groups.\n<syntaxhighlight lang=""R"" line>\n# perform t-test\nt.test(kurt, olaf)\n\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.1275\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -11.97670   1.50973\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n</syntaxhighlight>\n\nSo now, we performed a t-test and got a result. \'\'\'But how can we interpret the output?\'\'\' <br/>\nThe criterion to consult is the p-value. This value represents the probability of the data given that H0 is actually true. Hence, a low p-value indicates that the data is very unlikely if H0 applies. Therefore, one might reject this hypothesis (in favor of the alternative hypothesis H1) if the p-value turns out to be below a certain threshold (α, usually set prior testing), which is often set to 0.05 and usually not larger than 0.1. <br/>\n\nIn this case, the p-value is greater than 0.1. Therefore, the probability of H0 is considered to be “too large” to reject this hypothesis. Hence, we conclude that means do not differ significantly, even though we can say that descriptively the sample mean of Olaf’s flowers is higher.\n<br/>\nThere are multiple options to fine-tune the t-test if one already has a concrete hypothesis in mind concerning the direction and/or magnitude of the difference. In the first case, one might apply a one-tailed t-test. The hypotheses pairs would change accordingly to either of these: <br>\n\nH<sub>0</sub>: μ<sub>1</sub> ≥ μ<sub>2</sub><br>\nH<sub>1</sub>: μ<sub>1</sub> < μ<sub>2</sub>\n<br>\nor\n<br>\nH<sub>0</sub>: μ<sub>1</sub> ≤ μ<sub>2</sub>\n<br>H<sub>1</sub>: μ<sub>1</sub> > μ<sub>2</sub>\n\nNote that the hypotheses need to be mutually exclusive and H0 always contains some form of equality sign. In R, one-tailed testing is possible by setting \'\'\'alternative = ""greater""\'\'\' or \'\'\'alternative = ""less""\'\'\'. Maybe Olaf is the more experienced farmer so we have already have à priori the hypothesis that his flowers are on average larger. This would refer to our alternative hypothesis. The code would change only slightly:\n\n<syntaxhighlight lang=""R"" line>\nt.test(kurt, olaf, alternative = ""less"")\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.06373\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##       -Inf 0.4172054\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n</syntaxhighlight>\n\nAs one can see, the p-value gets smaller, but it is still not below the “magic threshold” of 0.05. The question of how to interpret this result might be answered differently depending on whom you ask. Some people would consider the result “marginally significant” or would say that there is “a trend towards significance” while others would just label it as being non-significant. In our case, let us set α\n = 0.05 for the following examples and call a result “significant” only if the p-value is below that threshold.\n\nIt is also possible to set a δ indicating how much the groups are assumed to differ.\n<br>\n<br>\nH<sub>0</sub>: μ<sub>1</sub> - μ<sub>2</sub> = δ\n<br>\nH<sub>1</sub>: μ<sub>1</sub> - μ<sub>2</sub> ≠ δ\n<br>\n<br>\nH<sub>0</sub>: μ<sub>1</sub> - μ<sub>2</sub> ≥ δ\n<br>\nH<sub>1</sub>: μ<sub>1</sub> - μ<sub>2</sub> < δ\n<br>\n<br>\nH<sub>0</sub>: μ<sub>1</sub> - μ<sub>2</sub> ≤ δ\n<br>\nH<sub>1</sub>: μ<sub>1</sub> - μ<sub>2</sub> > δ\n\nTo specify a δ, set \'\'\'mu = *delta of your choice*\'\'\'. If one sets a mu and only specifies one group, a one-sample t-test will be performed in which the group mean will be compared to the mu.\n\n\n==Assumptions==\nLike many statistical tests, the t-test builds on a number of assumptions that should be considered before applying it.']"|0.005208333333333333|0.0
1|What is the ANOVA a powerful for?|Reducing variance in field experiments or complex laboratory experiments|"['In addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs. \n\n\n== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n\'\'\'Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.\'\'\' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis.\n\n\n== Normativity ==\nDesigning an ANOVA-based design demands experience, and knowledge of the previous literature. The deductive approach of an ANOVA is thus typically embedded into an incremental development in the literature. ANOVA-based designs are therefore more often than not part of the continuous development in normal science. However, especially since the millennium, other more advanced approaches gained momentum, such as mixed effect models, information theoretical approaches, and structural equation models. The rigid root of the normal distribution and the basis of p-values is increasingly recognised as rigid if not outright flawed, and model reduction in more complex ANOVA designs is far from coherent between different branches of sciences. Some areas of science reject p-driven statistics altogether, while other branches of science are still publishing full models without any model reduction whatsoever. In addition, the ANOVA is today also often used to analyse inductive datasets, which is technically ok, but can infer several problems from a statistical standpoint, as well as based on a critical perspective rooted in a coherent theory of science. \n\nHence the ANOVA became a swiss army knive for group comparison for a continuous variable, and whenever different category levels need to be compared across a dependent variable, the ANOVA is being pursued. Whether there is an [[Causality|actual question of dependency]] is often ignored, let alone model assumptions and necessary preconditions. \'\'\'Science evolved, and with it, our questions became ever more complex, as are the problems that we face in the world, or that want to test.\'\'\' [[Agency, Complexity and Emergence#Complexity|Complexity]] reigns, and simple designs are more often than not questioned. The ANOVA remains as one of the two fundamental models of deductive statistics, with [[Regression Analysis|regression]] being the other important line of thinking. As soon as rigid questions of dependence were conveniently ignored, statisticians - or the researchers that applied statistics - basically dug the grave for these rigid yet robust approaches. There are still many cases where the ANOVA represents the most parsimonious and even adequate model. However, as long as positivist scientists share a room with a view in their ivory tower and fail to clearly indicate the limitations of their ANOVA-based designs, they undermine their [[Glossary|credibility]], and with it the [[Glossary|trust]] between science and society. The ANOVA is a testimony of how much statsicsi can serve society, for better or worse. The ANOVA may serve as a sound approximations of knowledge, yet at its worst it speaks of the arrogance of researchers who imply [[Causality|causality]] into mere patterns that can and will change once more knowledge becomes available.\n\n== Outlook ==\nThe Analysis of Variance was one of the most relevant contributions of statistics to the developments of the 20th century. By allowing for the systematic testing of hypotheses, not only did a whole line of thinking of the theory of science evolve, but whole disciplines were literally emerging. Lately, frequentist statistics was increasingly critizised for its reliance on p-values. Also, the reproducibility crisis highlights the limitations of ANOVA-based designs, which are often not reproducible. Psychological research faces this challenge for instance by pre-registering studies, indicating their statistical approach before approaching the data, and other branches of science are also attempting to do more justice to the limitations of the knowledge of experiments. In addition, new ways of experimentation of science evolve, introducing a systematic approach to case studies and solution oriented approaches. This may open a more systematic approach to inductive experiments, making documentation a key process in the creation of a canonised knowledge. Scientific experiments were at the forefront of developments that are seen more critically regarding their limitations. Taking more complexities into account, ANOVAs become a basis for more advanced statistics, and they can indeed serve as a robust basis if the limitations are clearly indicated, and the ANOVA designs add to parts of a larger picture of knowledge. \n\n\n== Key Publications ==\n\'\'\'Theoretical\'\'\'\n\nCrawley, M. J. (2007). The R book. John Wiley & Sons.\n\n== References ==\nCrawley, M. J. (2007). The R book. John Wiley & Sons.\nRucci, A. J., & Tweney, R. D. (1980). Analysis of variance and the"" second discipline"" of scientific psychology: A historical account. Psychological Bulletin, 87(1), 166.', '\'\'\'Note:\'\'\' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|\'\'\'SCOPUS hits per year for ANOVA until 2019.\'\'\' Search terms: \'ANOVA\' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). \'\'\'It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.\'\'\' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. \'\'\'The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.\'\'\' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. \'\'\'It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.\'\'\' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called \'[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]\' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or \'[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]\' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. \'\'\'If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.\'\'\' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels.', '\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=""R"" line>\nhist(residuals(model_1),\n     col=""yellow"")\n</syntaxhighlight>\n\n<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:']"|0.00411522633744856|1.0
2|What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?|Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.|"[""== Strengths & Challenges ==\n* Bayesian approaches incorporate prior information into its analysis. This means that any past information one has can be used in a fruitful way.\n* Bayesian approach provides a more intuitive and direct statement of the probability that the hypothesis is true, as opposed to the frequentist approach where the interpretation of p-value is convoluted.\n\n* Even though the concept is intuitive to understand, the mathematical formulation and definitions can be intimidating for beginners.\n* Identifying correct prior distribution can be very difficult in real life problems which are not based on careful experimental design.\n* Solving complex models with Bayesian approach is still computationally expensive.\n\n\n== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin.\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future. \n\n\n== Key Publications ==\n* Bayes, T. 1997. LII. ''An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S''. Phil. Trans. R. Soc. 53 (1763). 370–418.\n* Box, G. E. P., & Tiao, G. C. 1992. ''Bayesian Inference in Statistical Analysis.'' John Wiley & Sons, Inc.\n* de Finetti, B. 2017. ''Theory of Probability''. In A. Machí & A. Smith (Eds.). ''Wiley Series in Probability and Statistics.'' John Wiley & Sons, Ltd.\n* Kruschke, J.K., Liddell, T.M. 2018. ''Bayesian data analysis for newcomers.'' Psychon Bull Rev 25. 155–177.\n\n\n== References ==\n(1) Jeffreys, H. 1973. ''Scientific Inference''. Cambridge University Press.<br/>\n(2) Spiegelhalter, D. 2019. ''The Art of Statistics: learning from data''. Penguin UK.<br/>\n(3) Downey, A.B. 2013. ''Think Bayes: Bayesian statistics in Python''. O'Reilly Media, Inc.<br/>\n(4) Donovan, T.M. & Mickey, R.M. 2019. ''Bayesian statistics for beginners: A step-by-step approach.'' Oxford University Press.<br/>\n(5) Kurt, W. 2019. ''Bayesian Statistics the Fun Way: Understanding Statistics and Probability with Star Wars, LEGO, and Rubber Ducks.'' No Starch Press.\n\n\n== Further Information ==\n* [https://library.wur.nl/frontis/bayes/03_o_hagan.pdfBayesian statistics: principles and benefits]\n* [https://www.youtube.com/watch?v=HZGCoVF3YvM 3Blue1Brown: Bayes' Theorem]\n* [https://www.youtube.com/watch?v=SrEmzdOT65s Basic Probability: Joint, Marginal, and Conditional Probability]\n* [https://www.youtube.com/watch?v=9TDjifpGj-k Crash Course Statistics: Examples of Bayes' Theorem being applied.]\n* [https://www.youtube.com/watch?v=TVq2ivVpZgQ Monty Hall Problem: D!NG]\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Prabesh Dhakal and Henrik von Wehrden."", '== Bayes Theorem ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. However, this coin comes with a flip side\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. \'\'\'This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes\' theorem.\'\'\' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes\' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Key Publications ==\n* Gleick, J. (2011). Chaos: Making a new science. Open Road Media.\n* Rohde, Klaus. Nonequilibrium ecology. Cambridge University Press, 2006.\n* Kruschke, John. ""Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan."" (2014).\n* Hastie, T. & Tibshirani, R. 1986. \'\'Generalized Additive Models\'\'. Statistical Science 1(3). 297-318.\n\n==External Links==\n====Articles====\n[https://science.sciencemag.org/content/sci/199/4335/1302.full.pdf?casa_token=SJbEKSHs2gwAAAAA:iWho1AqZsznpL8Tt5vvaPHX2OVggZkP2NlUjEZ8I0avaMKTs6BlCMA7LGG0405x6l5LBY9hTAhag One of the classical papers] on non-equilibrium ecology\n[https://link.springer.com/content/pdf/10.1007/BF00334469.pdf Non-equilibrium theory in ecology]<br>\n[https://www.jstor.org/stable/pdf/1942636.pdf?casa_token=gYme29pwLTsAAAAA:eZBniNyPMJyFe5F7hkmy51EkBk3h0Bm6ap6nG2WWs8-n6EjuhJ16sDt5mJFXipvIIUBu9mzjI16EkLwCgMG70s-YayWTrlzAm63iX3iBk0zk-Mgk4g Classical account on equilibrium and non-equilibrium] dynamics in ecology<br>\n[https://link.springer.com/chapter/10.1007/978-3-319-46709-2_6 A balanced view on rangelands and non equilibrium dynamics]<br>\n[https://esajournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1890/11-0802.1 Non-equilibrium dynamics in rangelands]<br>\n[https://www.mdpi.com/2079-8954/7/1/4/htm A view on complexity]<br>\n[https://plato.stanford.edu/entries/chaos/ A deeper dive into chaos]<br>\n[https://theconversation.com/explainer-what-is-chaos-theory-10620 A nice take on Chaos]<br>\n[https://en.wikipedia.org/wiki/Chaos:_Making_a_New_Science The most definite guide to chaos] Note the synergies to the emergence of sustainability science<br>\n[https://www.r-bloggers.com/2019/05/bayesian-models-in-r-2/ Some intro into Bayesian statistics] in R<br>\n[https://www.wnycstudios.org/podcasts/radiolab/episodes/91684-stochasticity Stochasticity] just for kicks.\n\n\n====Videos====\n[https://www.youtube.com/watch?v=fDek6cYijxI Chaos]: The Veritasium explanation<br>\n[https://www.youtube.com/watch?v=5zI9sG3pjVU Laminar flow] is of course more awesome<br>\n[https://www.youtube.com/watch?v=ovJcsL7vyrk Random] is not random, as this equation proves<br>\n[https://www.youtube.com/watch?v=HZGCoVF3YvM One more take on Bayes]<br>\n\n\n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.', '[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Bayesian Inference until 2020.\'\'\' Search terms: \'Bayesian\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.\'\'\' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes\' name only became associated with the method in the 1900s (3).\n\n\'\'\'The family of methods based on the concept of Bayesian analysis has risen the last 50 years\'\'\' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n\'\'\'Bayesian analysis relies on using probability figures as an expression of our beliefs about events.\'\'\' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is ""prior"", and the probability figure assigned to the prior is called ""prior probability"". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called ""posterior probability"", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|\'\'\'The probability distribution for prior, evidence, and posterior.\'\'\']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes\' Theorem.\n\n==== Bayes\' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes\' theorem.\n\n\'\'\'Conditional Probability\'\'\' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n\'\'\'Joint Probability\'\'\', also called ""conjoint probability"", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n\'\'\'Marginal Probability\'\'\' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for ""Bayes theorem"" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n\'\'Caution:\'\' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n\'\'Note:\'\' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let\'s see how this formulation of Bayes\' Theorem can be applied in a simple coin toss example:\n\n\n=== \'\'\'Example I: Classic coin toss\'\'\' ===\n\'\'\'Imagine, you are flipping 2 fair coins.\'\'\' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)\n\n\'\'\'Solution:\'\'\'\nIf we represent the outcome of Heads by a H and the outcome of Tails by a T, then the possible outcomes of the two coins being tossed simultaneously can be written as: `HH`, `HT`, `TH`, `TT`\n\nThe two events A, and B are:\n\nA = the outcome is 2 heads (also called ""prior probability"")\n\nB = one of the outcomes was a head (also called ""marginal probability"")\n\nSo, essentially, the problem can be stated as: what is the probability of getting 2 heads, given that we\'ve already gotten 1 head? This is the posterior probability, which can be represented mathematically as:\n\n    P(2\\ heads|1\\ head)\n\nIn this case, the prior probability is the probability of getting 2 heads. We can see from our representation above that the prior probability is 1/4 as there is only one outcome where there are 2 Heads.']"|0.0045662100456621|1.0
3|Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?|Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.|"['Tracking down \'\'emergence\'\' has become a key focus in many areas of science, but organic chemistry can serve as an example of how much has been done, yet how much is still on the horizon. Many negative effects of chemicals were not anticipated, with prominent examples being FCKW, pesticides and antibiotics. The effect of different medical drugs on people is yet another example, since interactions between different medications are hardly understood at all, as the field only slowly unravels the negative side-effects of interacting medications or treatments. We are far away from understanding the impact that the concepts \'\'agency\'\', \'\'complex systems\'\' and \'\'emergence\'\' have on our knowledge. Yet, we need to diversify our canon of methods in order to approach these concepts from an empirical standpoint. Otherwise we will not be able to unlock new strata of knowledge. This includes the combination of different methods, the utilization of specific methods in a different context, as well as the development of novel methods.\n\nWhat is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https://www.britannica.com/science/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.\n\n\n== The way forward ==\nIf we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodologial ramifications. \'\'\'I can interview individuals, yet this will hardly allow me to prove agency.\'\'\' Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy. \n\nThe investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored [[Bayesian inference|Bayesian]] statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging. \n\nThe whole globe - although not a closed system – can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. \'\'\'Emergence is a truly cross-scalar phenomenon.\'\'\' Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.\n[[File:Climate model.png|400px|thumb|right|\'\'\'Climate models are increasingly getting more accurate, but the complexity and emergence of the global climate system may never be fully understood.\'\'\' Source: [https://blogs.egu.eu/geolog/2018/09/19/how-to-forecast-the-future-with-climate-models/ European Geosciences Union]]]\n\nBased on the current methodological canon we can say: \'\'\'Much is not (yet?) observed, much is not known, and it will be crucial to understand what we will not be able to know, at least for the time being.\'\'\' These three different qualities of knowledge clearly highlight that research will have to collaborate - agency, many complex systems and several phenomena that are emergent cannot be sufficiently investigated by one discipline alone. While chemistry may find emergence in chemical reactions, and medicine may find interactions between different drugs, these results become normative as soon as they are applicable in the real world. To this claim, one could make the exception of ethics or broadly speaking philosophy here, which can meaningfully engage with all three domains of knowledge - unobserved, not known and never known - yet may not be able to contribute to all empirical problems. On the other end, everything but philosophy may only be able to go into the un-observed and unknown. We need to combine our methodological approaches to create the knowledge that is needed now. The need for collaboration is a question of responsibility, and only if all disciplines dissolve (or at least redefine their main goal to unite and not differentiate), agency may be fully explored and complex problems may be solved. Many of the solutions we have in our hands right now already would seem like wizardry to our ancestors. It is our responsibility to continue on this path, not as technocrats or positivists that have arrogant pride of their achievement, but as mere contributors to a wider debate that should ultimately encompass all society. \n\nTo say it with Derek Parfit:\n\n""Some things (...) matter, and there are better and worse way to live. After many thousands of years of responding to reasons in ways that helped them to survive and reproduce, human beings can now respond to other reasons. We are a part of a Universe that is starting to understand itself. And we can partly understand, not only what is in fact true, but also what ought to be true, and what we might be able to make true. What now matters the most is that we avoid ending human history. If there are no rational beings elsewhere, it may depend on us and our successors whether it will all be worth it, because the existence of the Universe will have been on the whole good.""', '\'\'\'How do we keep learning to translate the everyday challenge into a something that does not lead to despair?\'\'\'\nBeing happy is a serendipity. Those who can be optimistic can consider themselves lucky, and have a responsibility towards others. This responsibility is a truly societal responsibility, because only if we can all enable everybody to be at peace may we overcome our deeply rooted problems. Our first and foremost priority will be to learn ways to marginalise minorities less, and help humans less privileged than us. We should never forget that optimism can obviously be also part of a privilege. I was privileged enough to see many different cultures, and remember that I saw optimistic and pessimistic world views in rich and poor. Yet this is easier said then lived, and only when all people can grow up with the capability to thrive in this world, and have their rights guarded will we surely and maybe finally be able to make the case for optimism. I for myself thrive towards this lack of inequality, and we have gotten closer over time. When my grandmother was born more than 100 years ago two thirds of all people lived in poverty. When my mother was born after the second world war it had hardly changed. When I was born in the mid-70s it was less than half of the people living on poverty. Today it is about 12%, depending on the threshold and how it is defined. The end of poverty will not mark the beginning of ultimate optimism, yet it is a path towards that. As long as I live I choose to throw all my might in contributing towards a world what is worthwhile our optimism, and this is what I call radical optimism. I cannot find an argument for me against it.\n|}\n\n{| class=""wikitable mw-collapsible mw-collapsed"" style=""width: 100%; background-color: white""\n|-\n! Some thoughts on practical ethics #4 - Cultural personal identity\n|-\n|by Henrik von Wehrden\nIt is well understood that we all are defined by our personal identity. The information that personal identity does not ultimately matter will not change that, and not knowing that our individual personal identity will end seems to be the easiest information we all learn to ignore early on in live. Very small children do not have a real personal identity. It seems as if personal identity is something that we discover, develop and/or learn. At a certain age, personal identity is being probed and framed, which is the age when most little children literally become a character. There are several reasons why this will surely not change even in the distant future, because it is part of our evolutionary development. In other words, personal identity is something we first have not, and then we have it. However, several people realise that their personal identity is indeed fleeting, which is not only the insight of the Buddha, but also Derek Parfit. \n\nThere are other people who could even flee into a world of non-identity. There is a prominent example Martin Pistorius. He was trapped for years in his own body, unable to move a muscle, and alter people in his surrounding that after years in a vegetative state, his consciousness had returned. In order to evade the agony of being trapped in his own body, he vanished into a place where ""nothing existed"". Yet while he described this to be a rather dark place, he was also able to vanish into a world of phantasy. Cultural identity is equally such a place of phantasy, because it is not about who a person is, but about who we are as a united group, interacting with each other. Cultural identity would not make sense if you are alone. Cultural identity can be thus seen as a construct that helps us to belong, and create some sort of unity among a group of people. In the past, this unity was often inherited, yet today in a globalised world, there are many cultural groups that are not inherited. The world grew more diverse, and there is a larger recognition of many different facets of cultural identity. \'\'\'Culture is what makes us diverse and enables societies to thrive.\'\'\' Yet culture cannot be defined as a homogeneous entity, but instead builds on diversity within nested groups. For example may certain traditional houses be built following a localised culture within the construction, but there are often deviances or diversities. This is why art is so central to our lives, because it is """"when our senses are at their fullest"" (Ken Robinson). Equally, in a cultural context, art can allow for a strong emotional unity. Many people find at the end of their own personal identity a great consolation that their culture goes on, and hence their contribution to this very culture will be preserved. \n\nCultural identity is therefore highly relevant, not only because of the emotional gratification to belong, but also because cultural identity can thus help people to make more sense of their personal identity, or the lack thereof, i.e. when we feel united. \'\'\'If we had no cultural identity, and because we have no personal identity, we would have practically no identity at all.\'\'\' This would be clearly a societal problem, because people are ,as was outlined above, not able to live within parts of their development without any form of identity within the foreseeable future. Identity is an important part of our development stages during adolescents, and without such steps people would be lost and confused at this age, and probably also later.\n\nHowever, we shall not forget that some of the worst atrocities in the history of people can be associated to cultural identity. It is however not the exclusion of people that do not belong to an identity group, that is the actual problem. Instead, it is the actions that may arise out of the exclusion of ""others"" from an identity group that is the true problem. \'\'\'Consequently, cultural identity should never enable members of a group to take negative actions against other people.\'\'\' This is in itself a very difficult assumption, not only because it would be hard to achieve. More importantly would it potentially elevate cultural dimensions onto the status of religion in a secular state. This is not my intention, yet I believe it is important to raise this issue as it would otherwise allow for critics to make the argument to raise concern. After all, culture is also about believes, and can be about values. Most would agree that culture should also not violate legal boundaries, and this matter is a description of many problems that rose in western democracies as part of the cancel culture and culture wars. Since culture builds on values and is set in the real world, it can also be about rights, and often is also about duties.', '\'\'\'Note:\'\'\' This is the German version of this entry. The original, English version can be found here: [[Agency, Complexity and Emergence]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n\'\'\'Kurz und knapp:\'\'\' Dieser Eintrag diskutiert drei zentrale Elemente von Systemen, und wie wir sie methodisch untersuchen können.\n\nAgency, Komplexität und Emergenz können als Linsen verstanden werden, durch die wir sowohl Ergebnisse sehen können, die aus theoretischen Überlegungen stammen, als auch Informationen, auf die wir durch empirische Forschung zugreifen. Da solche Konzepte die beiden Bereiche der wissenschaftlichen Untersuchung - Theorie und Praxis - miteinander verbinden, nenne ich die drei Konzepte Agency, Komplexität und Emergenz ""Grenzobjekte"". Sie sind gute Beispiele für Ideen, die in der Wissenschaftsphilosophie am relevantesten sind. Darüber hinaus bestimmen sie in hohem Maße unser grundlegendes und normatives Weltbild sowie die Konsequenzen des Wissens, das wir aus der Welt gewinnen. Wenn ich z.B. die Weltsicht habe, dass es keinen freien Willen gibt, dann wird dies auch einen grundlegenden Einfluss darauf haben, wie ich Ergebnisse aus der empirischen Untersuchung der Handlungen von Menschen interpretiere. \'\'\'Ich persönlich glaube, dass die Menschen das Potential haben, einen freien Willen zu haben, so wie alle Milch in Butter verwandelt werden kann\'\'\'. Es gibt andere Konzepte als agency, complexity und emergence, die es aus der Perspektive der Wissenschaftstheorie zu verfolgen lohnt, aber der Einfachheit und Priorität halber werden wir uns hier nur mit diesen drei Grenzobjekten befassen. Beginnen wir mit dem Versuch, die Vielfalt der Definitionen dieser Begriffe aufzuzeigen.\n\n\n== Agency ==\nDie einfachste Definition von Agency, die weithin als die methodisch relevanteste angesehen wird, ist ""die Fähigkeit eines Individuums, vorsätzlich zu handeln, unter der Annahme eines kausalen Ergebnisses, das auf dieser Handlung beruht"". Innerhalb der verschiedenen Diskurse und Disziplinen gibt es jedoch eine Vielzahl von Ansätzen, die breitere oder engere Definitionen von Agency in Betracht ziehen. Es gibt eine erste Meinungsverschiedenheit darüber, ob eine handlungsbeteiligte Personmeinen Geisteszustand haben sollte, der in der Lage ist, das Ergebnis der Handlung zu antizipieren, und in diesem Punkt sind sich so unterschiedliche Bereiche wie Neurowissenschaft und Philosophie bereits uneinig. Zum Beispiel sind Reflexe Instinkte, und es ist sehr schwierig, herauszufinden, ob solche Handlungen beabsichtigt sind und Annahmen über das Ergebnis enthalten. Daher schließen breitere Definitionen von Agency die handlungsbasierte Anpassung eine*r Agent*in an seine*ihre Umgebung ein, was auch einen Großteil des Tierreichs einschließt. Diese weit gefasste Definition des Handelns wird hier ignoriert, obwohl festgestellt werden sollte, dass die Handlungen einiger höherer Tiere auf Absicht und mentale Zustände hindeuten, die in der Lage sein könnten, vorauszusehen. Diese sind jedoch vom methodischen Standpunkt aus schwer zu untersuchen, obwohl von dieser Richtung in Zukunft viel erwartet werden kann. \n[[File:Tiger.jpg.jpg|350px|thumb|right|\'\'\'Tiere reagieren auf ihre Umwelt, aber handeln sie mit einer Absicht?\'\'\' Quelle: pixabay]]\n\'\'\'Handlungen von Agent*innen müssen vorsätzlich sein, d.h. eine bloße Handlung, die als Serendipität angesehen werden kann, ist nicht Teil des Handelns.\'\'\' Ebenso sind nicht vorhergesehene Folgen von Handlungen, die auf Kausalketten beruhen, ein Problem hinsichtlich Agency. Agency steht vor einer Herausforderung, wenn es darum geht, entweder Serendipität oder Murphys Gesetz anzuerkennen. Solche glücklichen oder unglücklichen Handlungen wurden von dem*der Agent*in nicht vorhergesehen und sind daher nicht wirklich in der Definition von Agency enthalten. Es gibt also ein metaphysisches Problem, wenn wir versuchen, den*die Agent*in, seine*ihre Handlungen und die Folgen der Handlungen zu unterscheiden. Man könnte behaupten, dass dieses Problem gelöst werden kann, wenn man sich allein auf die Folgen der Handlungen der Agent*innen konzentriert. Diese folgerichtige Sichtweise ist jedoch zum Teil eine theoretische Überlegung, da diese Sichtweise zwar viele interessante Experimente hervorbringen kann, uns aber nicht wirklich hilft, das Problem der unbeabsichtigten Handlungen an sich zu lösen. Dennoch ist die Konzentration auf bloße Handlungen auch deshalb relevant, weil sie es erlaubt, sich von eine*r einzelne*n Akteur*in zu entfernen und sich stattdessen auf die Interaktionen innerhalb der Agency mehrerer Akteur*innen zu konzentrieren, was ein guter Anfang für das nächste Konzept ist.\n\n\n== Komplexität ==\nViele Phänomene in der Natur sind einfach und folgen einfachen Regeln. Viele andere Phänomene sind nicht einfach, sondern können stattdessen als ""komplex"" definiert werden. Um zwischen den beiden zu verhandeln, gibt es das Grundgesetz des Ockhamschen Rasiermessers (Occam\'s Razor), das definiert, dass alle Dinge so einfach wie möglich und so komplex wie nötig sind. Die komplexe Systemtheorie hat sich in den letzten Jahrzehnten stark entwickelt und Wellen in die gesamte empirische Wissenschaft und darüber hinaus geschlagen. \'\'\'Problematisch ist jedoch, dass es keine einheitliche und allgemein akzeptierte Definition von Komplexität gibt, was ziemlich ironisch ist.\'\'\' Es scheint, dass die Komplexität an sich schon komplex ist. Daher werde ich mich hier auf einige der wichtigsten Argumente und Merkmale konzentrieren, die aus methodologischer Sicht relevant sind. Zunächst einmal werde ich alle Überlegungen auf komplexe Systeme beschränken. Zu diesem Zweck definiere ich [[System Thinking & Causal Loop Diagrams|""Systeme""]] einfach als eine beliebige Anzahl von Individuen oder Elementen, die interagieren. Vom methodischen Standpunkt aus sollten wir in der Lage sein, diese Interaktionen zu beobachten, während wir vom philosophischen Standpunkt aus in der Lage sein sollten, sie zu reflektieren.']"|0.0|1.0
4|What is the recommended course of action for datasets with only categorical data?|For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.|"['\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==Missing values: types and distributions==\n\nMissing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55).\n\nIn this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.\n\n==Types of missing values==\n\nThere are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149).\n\nUnit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).\n\nNext, we will look at how missing values can be distributed and what the implications of such distributions are. Generally, both types of missing values can occur in any distribution.\n\n==Distributions of missing values==\n\nThe distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).\n\n===Missing completely at random===\nMissing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).\n\nThe implications of MCAR for data analysis are relatively straightforward. Because the missing values are completely random, they do not introduce any bias into the analysis. Therefore, it is generally safe to impute the missing values using statistical methods, such as mean imputation or multiple imputations. However, even if the missing values are MCAR, there may still be other factors that can affect the analysis. It is important to consider the number and proportion of missing values (Scheffer 2002, 156). The larger the proportion of missing values in your overall dataset the less reliable is the use of the data. Imagine you had many unit nonresponse missing values across many different individuals, which results in having no variable without any missing value. This might affect the quality of your dataset. If and how this is the case, needs to be decided case by case.\n\n===Missing at random===\nMissing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55). For example, imagine you conduct a survey to analyze the relationship between education and income and there are missing values concerning income. If the missing values depend on education, then these missing values are missing at random. If they would depend on their actual income, they would not.\n\nThe implications of MAR for data analysis are more complex than those for MCAR. Because the missing values are not completely random, they may introduce bias into the analysis if they are not properly accounted for. Therefore, it is important to carefully consider the underlying reasons for the missing data and take these into account when imputing the missing values. One common approach to dealing with MAR missing values is to use regression or other statistical methods to model the relationship between the missing values and the other variables in the dataset (Tsikriktsis 2005, 56). Once the relationship is clear, other methods can be used to approximate to correct the variables for the bias due to the missing values missing at random.\n\n===Missing not at random===\nMissing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).\n\nThe implications of MNAR for data analysis are more complex than those for MCAR or MAR. Because the missing values are systematically related to the true values of the missing data, they can introduce bias into the analysis if they are not properly accounted for. In some cases, this bias may be difficult or impossible to correct, even with advanced statistical methods (Tsikriktsis 2005, 55).\n\n==Determining the randomness of missing data==\n\nThere are two common methods to determine the randomness of missing data. The first method involves forming two groups: one with missing data for a single variable and one with valid values for that variable. If significant differences are found between the two groups regarding their relationship to other variables of interest, it may indicate a non-random missing data process. The second method involves assessing the correlation of missing data for any pair of variables. If low correlations are found between pairs of variables, it may indicate complete randomness in the missing data (MCAR). However, if significant correlations are found between some pairs of variables, it may be necessary to assume that the data are only missing at random (MAR) (Tsikriktsis 2005, 55 - 56).\n\nOverall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. ""Missing data: our view of the state of the art."" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. ""Dealing with missing data."" (2002).\n\nTsikriktsis, Nikos. ""A review of techniques for treating missing data in OM survey research."" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]', ""Learning statistics takes time, as it is mostly experience that allows us to be able to approach the statistical analysis of any given dataset. '''While we cannot take off of you the burden to gather experience yourself, we developed this interactive page for you to find the best statistical method to analyze your given dataset.''' This can be a start for you to dive deeper into statistical analysis, and helps you better [[Designing studies|design studies]].<br/>\n\n<u>This page revolves around statistical analyses of data that has at least two variables.</u> If you only have one variable, e.g. height data of a dozen trees, or your ratings for five types of cake, you might be interested in simpler forms of data analysis and visualisation. Have a look at [[Descriptive statistics]] and [[Introduction to statistical figures]] to see which approaches might work for your data.\n\n<u>If you have more than one variable, you have come to the perfect place!</u> <br>\nGo through the images step by step, click on the answers that apply to your data, and let the page guide you. <br>\nIf you need help with data visualisation for any of these approaches, please refer to the entry on [[Introduction to statistical figures]].<br>If you are on mobile and/or just want a list of all entries, please refer to the [[Statistics]] overview page.\n----\n'''Start here with your data! This is your first question.'''\n\n<imagemap>Image:Statistics Flowchart - First Step.png|center|650px\npoly 289 5 151 140 289 270 423 137 [[An_initial_path_towards_statistical_analysis|This is where you start!]]\npoly 136 150 0 288 137 418 271 284 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|Multivariate Statistics]]\npoly 441 151 302 289 437 416 571 287 [[An_initial_path_towards_statistical_analysis#Univariate_statistics|Univariate Statistics]]\n</imagemap>\n\n'''How do I know?''' <br/>\n* What does the data show? Does the data logically suggest dependencies - a causal relation - between the variables? Have a look at the entry on [[Causality]] to learn more about causal relations and dependencies.\n\n\n= Univariate statistics =\n'''You are dealing with Univariate Statistics.''' Univariate statistics focuses on the analysis of one dependent variable and can contain multiple independent variables. But what kind of variables do you have?\n<imagemap>Image:Statistics Flowchart - Univariate Statistics.png|650px|center|\npoly 386 5 203 186 385 359 563 182 [[Data formats]]\npoly 180 200 3 380 181 555 359 378 [[An_initial_path_towards_statistical_analysis#At_least_one_categorical_variable|At least one categorical variable]]\npoly 584 202 407 378 584 556 762 379 [[An_initial_path_towards_statistical_analysis#Only_continuous_variables|Only continuous variables]]\n</imagemap>\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric (including continuous) variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is ''categorical''.\n\n\n== At least one categorical variable ==\n'''Your dataset does not only contain continuous data.''' Does it only consist of categorical data, or of categorical and continuous data?\n<imagemap>Image:Statistics Flowchart - Data Formats.png|650px|center|\npoly 288 2 151 139 289 271 424 138 [[Data formats]]\npoly 137 148 0 285 138 417 273 284  [[An_initial_path_towards_statistical_analysis#Only_categorical_data:_Chi_Square_Test|Only categorical data: Chi Square Test]]\npoly 436 151 299 288 437 420 572 287 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data|Categorical and continuous data]]\n</imagemap>\n\n'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\n\n=== Only categorical data: Chi Square Test ===\n'''You should do a Chi Square Test'''.<br/>\nA Chi Square test can be used to test if one variable influenced the other one, or if they occur independently from each other. The key R command here is: <code>chisq.test()</code>. Check the entry on [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi Square Tests]] to learn more.\n\n\n=== Categorical and continuous data ===\n'''Your dataset consists of continuous and categorical data.''' How many levels does your categorical variable have?\n<imagemap>Image:Statistics flowchart - Categorical factor levels.png|650px|center|\npoly 321 3 175 149 325 304 473 152 473 15 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data]]\npoly 149 172 3 318 153 473 301 321 301 321  [[An_initial_path_towards_statistical_analysis#One_or_two_factor_levels: t-test|One or two factor levels: t-test]]\npoly 489 172 343 318 493 473 641 321 641 321 [[An_initial_path_towards_statistical_analysis#More_than_two_factor_levels: ANOVA|More than two factor levels: ANOVA]]\n</imagemap>\n\n'''How do I know?'''\n* A 'factor level' is a category in a categorical variable. For example, when your variable is 'car brands', and you have 'AUDI' and 'TESLA', you have two unique factor levels. \n* Investigate your data using 'levels(categoricaldata)' and count the number of levels it returns. How many different categories does your categorical variable have? If your data is not in the 'factor' format, you can either convert it or use 'unique(yourCategoricalData)' to get a similar result.\n\n\n==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()</code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n\n'''Depending on the variances of your variables, the type of t-test differs.'''"", '[[File:ConceptOrdination.png|450px|frameless|left|[[Sustainability Methods:About|Method Categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method Categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n__NOTOC__\n<br/>\n<br/>\n<br/>\n\'\'\'In short:\'\'\' Principle Component Analysis is an unsupervised learning algorithm whose goals is to reduce the dimensionality of quantitative data down to a low number of dimensions.\n\n== Background ==\n[[File:PCA.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Principle Component Analysis until 2020.\'\'\' Search terms: \'PCA\', \'Principle Component Analysis\' in Title, Abstract, Keywords. Source: own.]]\n=== Motivation ===\n[[File: PCAPizza.jpg|right|300px]]\n\nPrincipal Component Analyses are helpful when you have a lot of different [[Glossary|data]] samples with a variety of variables. For example, the following dataset which contains different nutrient measurements in various pizzas from different pizza brands.\n\n* brand -- Pizza brand (class label)\n* id -- Sample analysed\n* mois -- Amount of water per 100 grams in the sample\n* prot -- Amount of protein per 100 grams in the sample\n* fat -- Amount of fat per 100 grams in the sample\n* ash -- Amount of ash per 100 grams in the sample\n* sodium -- Amount of sodium per 100 grams in the sample\n* carb -- Amount of carbohydrates per 100 grams in the sample\n* cal -- Amount of calories per 100 grams in the sample\n\nHow can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.\n\n=== Curse of dimensionality ===\nThis term was coined by Richard R. Bellman, an American applied mathematician. As the number of features / dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. \'\'\'Principle Component Analysis helps to alleviate this problem\'\'\'.\n[[File: PCA_BiPlot.png|center|500px]]\n\n== What the method does ==\nPrinciple Component Analysis is one of the foundational methods to combat the curse of dimensionality. It is an unsupervised learning algorithm whose goals is to reduce the dimensionality of the data, condensing its entirety down to a low number of dimensions (also called principle components, usually two or three). \n\nAlthough it comes with a cost of losing some information, it makes data visualization much easier, improves the space and time complexity required for machine learning algorithms tremendously, and allows for more intuitive intepretation of these models. PCA can also be categorized a feature extraction techniques, since it creates these principle components - new and more relevant features - from the original ones.\n\nThe essence of PCA lies in finding all directions in which the data ""spreads"", determining the extent in which the data spreads in those directions, keeping only few direction in which the data spreads the most. And voila, these are your new dimensions / features of the data.\n\n=== Road to PCA ===\n==== Standardization ====\nOftentimes the features in the data are measured on different scales. This step makes sure that all features contribute equally to the analysis. Otherwise, variables with large range will trump thoses with smaller range (for example: a time variable that ranges between 0ms and 1000ms with dominate over a distance variable that ranges between 0m and 10m). Each variable can be scaled by subtracting its mean and dividing by the standard deviation (this is the same as calculating the z-score, and in the end, all variables with have the same mean 0 and standard deviation of 1).\n\n==== Covariance matrix ====\n\nThe covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties:\n* The size of the matrix is equal to the number of features in the data\n* The main diagonal on the matrix contains the variances of each initial variables.\n* The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2)\n\nThe covariance matrix gives you a summary of the relationship among the initial variables.\n* A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa)\n* A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa)\n\n==== Eigenvectors / Principle Components & Eigenvalues ====\nNow we have the covariance matrix. This matrix can be used to transform one vector into another. Normally when this transformation happens, two things happen: the original is  rotated and get streched/squished to form a new vector. When an abitrary vector is multipled by the covariance matrix, the result will be a new vector whose direction is nudged/rotated towards the greatest spread in the data. In the figure below, we start with the arbitrary vector (-1, 0.5) in red. Multiplying the red vector with covariance matrix gives us the blue vector, and repeating this gives us the black vector. As you can see, the result rotation tends to converge towards the widest spread direction of the data.\n\n[[File: PCAEigenvector01.png|center|500px]]\n\nThis prompts the questions: Can we find directly find the vector which already lies on this ""widest spread direction"". The answer is yes, with the help of eigenvectors. Simply put, an eigenvectors of a certain matrix is a vector that, when transformed by the matrix, does not rotate. It remains on its own span, and the only thing that changes is its magnitude. This (constant) change ratio in magnitude corresponding to each eigenvector is called eigenvalue. It indicates how much of the data variability can be explained by its eigenvector.']"|0.004310344827586207|1.0
5|What is a Generalised Linear Model (GLM)?|A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n\'\'\'Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.\'\'\' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.', 'Depending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) \'\'\'First of all, is the statistics the right approach to begin with?\'\'\' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem.', 'In addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.']"|0.07602339181286549|1.0
6|What is Cluster Analysis?|Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).|"['[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n\'\'\'In short:\'\'\' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|\'\'\'SCOPUS hits for Clustering until 2019.\'\'\' Search terms: \'Clustering\', \'Cluster Analysis\' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as \'\'similarity measure\'\' or \'\'distance measure\'\', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named ""unsupervised learning"", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term ""clustering"" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=""R"" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(""x1"", ""x2"", ""cluster"")\n    return(df)\n}\n</syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns \'\'\'n\'\'\' examples to one of \'\'\'k\'\'\' clusters, where \'\'\'n\'\'\' is the sample size and  \'\'\'k\'\'\', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:\n\n# The number of cluster \'\'\'k\'\'\' is chosen by the data analyst\n# The algorithm randomly picks \'\'\'k\'\'\' centroids and assigns each point to the closest centroid to get \'\'\'k\'\'\' initial clusters\n# The algorithm recalculates the centroid by taking average of all points in each cluster and updates the centroids and re-assigns the points to the closest centroid.\n# The algorithm repeats Step 3 until all points stop changing clusters.\n\nTo get an intuitive sense of how this k-means clustering algorithm works, visit: [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====\n\nTo implement k-Means Clustering, the data table needs to only contain numeric data type. With a data frame or matrix with numeric value where the rows signify individual data example and the columns signify the \'\'features\'\' (number of features = the number of dimensions of the data set), k-Means clustering can be performed with the code below:\n\n<syntaxhighlight lang=""R"" line>\n# Generate data and perform the clustering\ndf <- create_cluster_data(150, 1.25, 3)\ndata_cluster = kmeans(df, centers=3) # perform the clustering\n\n# plot for sd = 1.25\nplot(df$x1, df$x2, \n     pch=df$cluster, \n     col=data_cluster$cluster, cex=1.3, \n     xlab=""x1"", ylab=""x2"", \n     main=""k-Means Clustering Example with Clearly Clustered Data"", \n     sub=""(In-cluster variance when generating data = 1.25)"")\npoints(data_cluster$centers[1,1], data_cluster$centers[1,2], \n       pch=15, cex=2, col=""black"")\npoints(data_cluster$centers[2,1], data_cluster$centers[2,2], \n       pch=15, cex=2, col=""red"")\npoints(data_cluster$centers[3,1], data_cluster$centers[3,2], \n       pch=15, cex=2, col=""green"")\nlegend(""topleft"", legend=c(""True Cluster 1"", ""True Cluster 2"", ""True Cluster 3"", ""Cluster Center""), \n       col=c(""black"",""black"",""black"", ""black""), pch = c(1, 2, 3, 15), cex=0.8)\ntext(-3.15, 10, \n     ""Colors signify the clusters identified\\nby k-means clustering algorithm"", \n     adj = c(0,0), cex=0.8)\n</syntaxhighlight>\n\nHere is the result of the preceding code:', 'Here is the result of the preceding code:\n\n[[File:Result of K-Means Clustering.png| |Result of K-Means Clustering]]\n\nWe can see that k-Means performs quite good at separating the data into three clusters. However, if we increase the variance in the dataset, k-Means does not perform as well. (See image below)\n\n[[File:Result of K-Means Clustering (High Variance).png| |Result of K-Means Clustering (High Variance)]]\n\n==== Strengths and Weaknesses of k-Means Clustering ====\n\n\'\'\'Strengths\'\'\'\n* It is intuitive to understand.\n* It is relatively easy to implement.\n* It adapts to new data points and guarantees convergence.\n\n\'\'\'Weaknesses\'\'\'\n* The number of clusters \'\'k\'\' has to be chosen manually.\n* The vanilla k-Means algorithm cannot accommodate cases with a high number of clusters (high \'\'k\'\').\n* The k-Means clustering algorithm clusters \'\'all\'\' the data points. As a result, the centroids are affected by outliers.\n* As the dimension of the dataset increases, the performance of the algorithm starts to deteriorate.\n\n=== Hierarchical Clustering ===\nAs the name suggests, hierarchical clustering is a clustering method that builds a \'\'hierarchy\'\' of clusters. \n\nUnlike k-means clustering algorithms - as discussed above -, this clustering method does not require us to specify the number of clusters beforehand. As a result, this method is sometimes used to identify the number of clusters that a dataset has before applying other clustering algorithms that require us to specify the number of clusters at the beginning.\n\nThere are two strategies when performing hierarchical clustering:\n\n==== Hierarchical Agglomerative Clustering ====\n\nThis is a ""bottom-up"" approach of building hierarchy which starts by treating each data point as a single cluster, and successively merging pairs of clusters, or agglomerating, until all clusters have been merged into a single cluster that contains all data points. Each observation belongs to its own cluster, then pairs of clusters are merged as one moves up the hierarchy.\n\n==== Implementation of Agglomerative Clustering ====\nThe first example of agglomerative clustering is performed using the hclust function built in to R.\n\n<syntaxhighlight lang=""R"" line>\nlibrary(dendextend)\n\n# generate data\ndf <- create_cluster_data(50, 1, 3, random_state=7)\n# create the distance matrix\ndist_df = dist(df[, 2], method=""euclidean"") \n# perform agglomerative hierarchical clustering\nhc_df = hclust(dist_df, method=""complete"")\n# create a simple plot of the dendrogram\nplot(hc_df)\n\n# Plot a more sophisticated version of the dendrogram \ndend_df <- as.dendrogram(hc_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\nlabels_colors(dend_df) <-\n    c(""black"", ""darkgreen"", ""red"")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\nlabels(dend_df) <- paste(as.character(\n    paste0(""cluster_"",df[,3],"" ""))[order.dendrogram(dend_df)],\n    ""("",labels(dend_df),"")"", \n    sep = """")\ndend_df <- hang.dendrogram(dend_df, hang_height=0.2)\ndend_df <- set(dend_df, ""labels_cex"", 0.8)\n\nplot(dend_df, \n     main = ""Clustered Data Set\n     (based on Agglomerative Clustering using hclust() function)"", \n     horiz =  FALSE,  nodePar = list(cex = 0.5), cex=1)\n\nlegend(""topleft"", \n       legend = c(""Cluster 1"", ""Cluster 2"", ""Cluster 3""), \n       fill = c(""darkgreen"", ""black"", ""red""))\n</syntaxhighlight>\n\nHere is the plot of the dendrogram from the code above:\n[[File:Hierarchical Clustering Algorithm Example (hclust).png| |An example for Hierarchical Clustering Algorithm created using hclust() function in R.]]\n\nThe following example relies on the agnes function from cluster package in R in order to preform agglomerative hierarchical clustering.\n\n<syntaxhighlight lang=""R"" line>\ndf <- create_cluster_data(50, 1, 3, random_state=7)\ndend_df <- agnes(df, metric=""euclidean"")\n\n# Plot the dendrogram\ndend_df <- as.dendrogram(dend_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\nlabels_colors(dend_df) <-\n    c(""black"", ""darkgreen"", ""red"")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\nlabels(dend_df) <- paste(as.character(\n    paste0(""cluster_"",df[,3],"" ""))[order.dendrogram(dend_df)],\n    ""("",labels(dend_df),"")"", \n    sep = """")\ndend_df <- hang.dendrogram(dend_df,hang_height=0.2)\ndend_df <- set(dend_df, ""labels_cex"", 0.8)\n\nplot(dend_df, \n     main = ""Clustered Data Set\n     (based on Agglomerative Clustering using agnes() function)"", \n     horiz =  FALSE,  nodePar = list(cex = 0.5), cex=1)\nlegend(""topleft"", \n       legend = c(""Cluster 1"", ""Cluster 2"", ""Cluster 3""), \n       fill = c(""darkgreen"", ""black"", ""red""))\n</syntaxhighlight>\nFollowing is the dendrogram created using the preceding code:\n[[File:Hierarchical Clustering Algorithm Example (agnes).png| |An example for Hierarchical Clustering Algorithm created using agnes() function in R.]]\n\n==== Hierarchical Divisive Clustering ====\nThis is a ""top-down"" approach of building hierarchy in which all data points start out as belonging to a single cluster. Then, the data points are split, or divided, into separate clusters recursively until each of them falls into its own separate individual clusters.\n\n* All observations start as one cluster, and splits are performed recursively as one moves down the hierarchy.\n\n==== Implementation of Divisive Clustering ====\n\n<syntaxhighlight lang=""R"" line>\n\n# Generate a dataset\ndf <- create_cluster_data(50, 1, 3, random_state=7)\n\n# Perform divisive clustering\ndend_df <- diana(df, metric=""euclidean"")\n\n# Plot the dendrogram\ndend_df <- as.dendrogram(dend_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\n\nlabels_colors(dend_df) <-\n    c(""black"", ""darkgreen"", ""red"")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\n\nlabels(dend_df) <- paste(as.character(\n    paste0(""cluster_"",df[,3],"" ""))[order.dendrogram(dend_df)],\n    ""("",labels(dend_df),"")"", \n    sep = """")', '\'\'\'Point Map\'\'\'<br>\nNow, \'\'\'each statement is placed on a blank, typically two-dimensional map\'\'\', with more closely related statements being placed more closely to each other. This is done using multidimensional scaling (MDS), a multivariate statistical approach, which accomplishes the complex task of placing all data points in the relative distance to all other data points, as defined by the values of relatedness indicated in the matrix. \n\n[[File:Group Concept Mapping Point Map.png|500px|frameless|center|]]\n\'\'\'An exemplary Point Map of 95 statements from Trochim 1989, p.11.\'\'\' Each point represents a statement that was gathered by the group, and is placed in a way that represents relatedness to the other statements.\n\n\n\'\'\'Cluster Map\'\'\'<br>\nIn a second step, \'\'\'all statements (= data points) are grouped into clusters\'\'\' ""which represent higher order conceptual groupings of the original set of statements."" (Trochim 1989). This is done - as proposed by Trochim (1989) - by using the X-Y coordinates of each statement after the multidimensional scaling as input for a Ward\'s hierarchical cluster analysis (HCA), which ""partitions the multidimensional scaling map into any number of clusters"" (Trochim 1989, 8). The analyst has to decide how many clusters the statements should be grouped into, based on what makes sense with regards to the topic and statements. You will find more on cluster analysis in [[Clustering Methods|this entry]].\n\nConcerning the software for the conduction of this clustering, Dare & Nowicki (2019, p.9) highlight that multidimensional scaling as well as hierarchical cluster analysis can be performed using statistical software such as R, SAS, or SPSS, or with the aforementioned groupwisdom© software.\n\n[[File:Group Concept Mapping Cluster Map.png|400px|frameless|center|]]\n\'\'\'The Cluster Map, which puts the statements (data points) into groups by means of Cluster Analysis.\'\'\' The visual representation of these clusters is in shapes instead of points. The amount of clusters is adjustable based on what makes sense for the data. Each cluster will be given a sensible name by the group (Step 5). Source: adapted from Trochim 1989, p.12\n\n\n\'\'\'Rating Maps\'\'\'<br>\nAt the end of this step, the group has a (two-dimensional) map that includes all statements as data points, placed according to their relatedness (the ""Point Map""); and one map that also groups these data points into clusters (the ""Cluster Map""). Two more maps can be created based on these. The ratings that were assigned to each statement in the matrix in Step 3 are placed in the respective position on the map, resulting in the ""Point Rating Map"". The same is done for each cluster, with the Point Ratings being averaged within each cluster.\n\n[[File:Group Concept Mapping Point Rating Map.png|400px|frameless|center|]]\n\'\'\'The respective Point Rating Map.\'\'\' For each data point, the mean rating as assigned by the group in Phase 3 is indicated, based on a 1-5 Likert Scale in this case. Source: Trochim 1989, p.13\n\n[[File:Group Concept Mapping Cluster Rating Map.png|400px|frameless|center|]]\n\'\'\'And finally, the (still unlabeled) Cluster Rating Map.\'\'\' Here, the point ratings of all data points (on a 1-5 Likert Scale) within each cluster are averaged, leading to a rating for each cluster, indicated by the ""levels"". With this map at hand, it is easy to see which groups of ideas, problems, goals, or individuals are most ""important"", ""difficult"", or ""likely"" for the group, which is of great help for their planning or evaluation process. Source: adapted from Trochim 1989, p.14\n\n\'\'\'Our example\'\'\'<br>\nMcCaffrey et al. first considered the three participant groups\' responses individually to see if there were diverging conceptual understandings of what constitutes good health care, but eventually combined their results into one conceptual model. They created a point maps, and evaluated different numbers of clusters before eventually deciding on a 10-cluster result, with which they developed a cluster map. Each cluster represented a different aspect of good health care. They calculated ratings for all data points and clusters and created point and cluster rating maps, where a higher rating equals a more important aspect of good health care. They found that there are only small differences in their highest and lowest rated clusters, which they stated highlights that the initial selection of statements already included very important aspects of good health care. They further investigated if there were significant differences between different participant groups\' clustering results, which they did not find.\n\n[[File:Group Concept Mapping - Cluster Rating Map - Example.png|700px|thumb|center|\'\'\'The resulting (yet unlabeled) Cluster Rating Map for McCaffrey et al. 2019\'\'\' (p.89).]]\n\n\n==== 5 - Interpretation of Maps ====\nNow, \'\'\'the group is asked to assign names to the clusters\'\'\'. Each participants looks at each cluster and the statements included, and suggests a name (e.g. a phrase, or a word) to describe the cluster, and the group negotiates until consensus is reached for each cluster. If there are a lot of clusters, it may be sensible to further develop names for groups of clusters - ""regions"" - but this depends on the map at hand. In any case, the names of the clusters should represent the statements included as well as the conceptual relation to other clusters which are close. This labeled Cluster Map is the main outcome of the Group Concept Mapping process. It can be re-arranged by the group if necessary, since they should feel comfortable with the conceptual framework it represents.\n\n\'\'\'In the end of the process, the group has the following results:\'\'\'\n* a statement list\n* a point map\n* a point rating map\n* a cluster list (listing all labeled clusters including the respective statements)\n* a labeled cluster map, \n* and a labeled cluster rating map.\n\n\'\'\'Our example\'\'\'<br>\nMcCaffrey et al. named the clusters themselves, based on ""cluster names provided by participants whose\nsorting produced results similar to the final cluster content, and (2) by reviewing statements within each cluster"" (p.89).\n\n[[File:Group Concept Mapping - Cluster List (first half).png|600px|frameless|center]]\n[[File:Group Concept Mapping - Cluster List (second half).png|600px|frameless|center]]\n\nThe final list of clusters in McCaffrey et al. (2019, p.90f) The clusters (left) are presented in order of importance (right), with a description and exemplary statements for each cluster in the center.\n\n\n==== 6 - Utilization of Maps ====\nThe group is now done with the Group Concept Mapping process, and can use either of the maps (preferable the Cluster Map, or Cluster Rating Map) as a baseline for their further work in many diverse ways. \'\'\'The maps show the most important elements they need to pay attention to,\'\'\' which can be used to coordinate future actions, prioritize tasks, and structure the process. The clusters can serve as the organizational foundation, or as groups of topics to work on, either when implementing measures, or developing an evaluation scheme.']"|0.04905660377358491|0.5
7|What is the purpose of Network Analysis?|Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.|"['==== Step by Step ====\n* \'\'\'Type of Network:\'\'\' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (\'\'whole networks\'\'), or focus on the network surrounding a specific node of interest (\'\'ego networks\'\'). They also decide for either \'\'one-mode networks\'\', focusing on one type of node that could be connected with any other; or \'\'two-mode networks\'\' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* \'\'\'Network boundaries:\'\'\' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** \'\'position-based approach\'\': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** \'\'event-based\'\' approach: those who had participated in key events are believed to define the population\n** \'\'relation-based approach\'\': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the \'\'exogenously defined boundaries\'\', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* \'\'\'Type of ties:\'\'\' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: \'\'directed\'\' ties, which go from one node to another, and \'\'undirected ties\'\', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, ""(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact."" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* \'\'\'Data Collection\'\'\': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a \'\'prompted recall\'\' approach, they are asked which people they would think of with regards to a specific topic (e.g. ""To whom would you go for advice at work?"") while they are shown a pre-determined list of potentially relevant individuals. In the \'\'free list\'\' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* \'\'\'Data Analysis\'\'\': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the \'\'\'visual representation\'\'\' of the network. For this, the researcher ""(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections."" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the ""number of lines which it is necessary to traverse in order to get from one point to another."" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|\'\'\'Different network structures.\'\'\' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|\'\'\'An exemplary network structure.\'\'\' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]\n\n== Strengths & Challenges ==\n* There is a range of challenges in the gathering of network data through [[Semi-structured Interview|Interviews]] and [[Survey|Surveys]], which can become long and cumbersome, and in which the interviewees may differently understand and recall their relations with other actors, or misinterpret the connections between other actors. (Marin & Wellman 2010, p.15)\n* The definition of network boundaries is crucial, since ""(...) the inappropriate inclusion or exclusion of a small number of entities can have ramifications which extend well beyond those entities themselves"". Apart from the excluded entities and their relations, all relations between these entities and the rest of the network, and thus the network\'s structural properties, are affected. (Butts 2008). For more insights on the topic of System Boundaries, please refer to [[System Boundaries|the respective article]].', '== Normativity ==\n* The structure of any network and thus the conclusions that can be drawn in the analysis very much depend on the relation that is observed. A corporation may be differently structured in terms of their informal compared to their official communication structures, and an individual may not be part of one network but central in another one that focuses on a different relational quality (Butts 2008)\n* Further, the choice of network boundaries as well as the underlying research intent can have normative implications. Also, actors within the network may be characterized using specific attributes, which may be a normative decision (such as for attributes of ethnicity, violence, or others).\n* The way in which a Social Network is visualized plays an important role. Researchers may choose from a variety of visualization forms in terms of symmetry, distribution and color of the represented network structures. It is important to highlight that these choices heavily influence how a Social Network is perceived, and that properly visualizing the available data is a matter of experience.\n* The method of social network analysis is connected to the methods Stakeholder Analysis as well as [[Clustering Methods|Clustering]]. Further, as mentioned above, the data necessary for Social Network Analysis can be gathered in [[Survey|Surveys]] or [[Semi-structured Interview|Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] or similar methods of data gathering. Last, the whole idea of analyzing systemic interactions between actors is the foundational idea of [[System Thinking & Causal Loop Diagrams|Systems Thinking.]]\n\n\n== An exemplary study ==\n[[File:Social Network Analysis - Lam et al. 2021 - Exemplary study title.png|600px|frameless|center|Title of Lam et al. 2021]]\n\'\'\'In their 2021 publication, researchers from Leuphana (Lam et al., see References) investigated the network ties between 32 NGOs driving sustainability initiatives in Southern Transylvania.\'\'\' Based on the [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226895/ [[Glossary|leverage points]] concept], they attempted to identify how these NGOs contribute to systemic change. For this, they focused on the positions of the NGOs in different networks, with each network representing relations that target different system characteristics (parameters, feedbacks, design, intent). As a basis for this, the authors identified types of ties between the NGOs that contribute to each of these four system characteristics.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Leverage Points.png|900px|frameless|center|Leverage Points and Social Network Analysis. From Lam et al. 2021]]<br>\n\nFurther, they investigated the amplification processes the NGOs applied to their initiatives to foster transformative change, grouped into four kinds of amplification processes: \'\'within\'\', \'\'out (dependent)\'\', \'\'out (independent)\'\' and \'\'beyond\'\'.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Amplification.png|750px|frameless|center|Survey questions on amplification processes by NGOs in Social Network Analysis on Leverage Points. Source: Lam et al. 2021]]<br>\n\nBased on these conceptual structures, \'\'\'the authors conducted an online survey\'\'\' in which 30 NGOs participated. In this survey, the NGOs were asked about their relations to other NGOs in terms of the kinds of questions shown above relating to the four leverage points. The NGOs were asked to rate the strength of the respective relations to another NGOs over the past five years with either ""not at all"" (0), low extent (1), moderate extent (2), high extent (3), and ""I don\'t know"". Further, the survey asked the NGOs four questions about the four kinds of amplification as shown above. \n\nWith the survey data, the authors created four networks using [https://gephi.org/ Gephi] and [https://nodexl.com/ NodeXL] software, one for each system characteristic. \'\'\'For each NGO (= node), they calculated three measures\'\'\' (Lam et al. 2021, p.816):\n* the \'\'weighted degree\'\', which measures the relations of the node to other nodes in the network, taking into consideration the weight of the relations. This measure provides insights on each node\'s individual interconnectedness.\n* the \'\'betweenness\'\', which highlights how often a node links other nodes that would otherwise be unconnected. The higher the betweenness of a node is, the more power it exerts on the network. (By the way, the node with the highest betweenness is often called a \'broker\').\n* the \'\'eigenvector centrality\'\', which measures the influence of a node in the network, weighted by the influence of its neighboring nodes. This highlights the future influence of a node.\n\nThey additionally tested for the influence of each NGO\'s amplification actions on these network measures by comparing those NGOs that applied a specific amplification action to those that did not, using a Mann-Whitney U test (also known as [[Simple_Statistical_Tests#Wilcoxon_Test|Wilcoxon rank-sum test]]).\n\nIn their results, they ""(...) found that while some NGOs had high centrality metrics across all four networks (...), other NGOs had high weighted degree, betweenness, or eigenvector in one particular network."" (p.817)\n[[File:Social Network Analysis - Lam et al. 2021 - Result visualisations.png|750px|frameless|center|Results from Lam et al. 2021, p.817]]\n\nWithout going into too many details at this point, we can see that the created networks tell us a lot about different aspects of the relation between actors. Based on the shown and further results, the authors concluded that\n# actors (NGOs) may have central roles either concerning all kinds of networks, or just in specific networks,\n# actors that amplify their own impact actively are potentially more central in networks, and\n# that Social Network Analysis with a leverage points perspective can help identify how and which actors play an important role for different kinds of sustainability transformations.\n\nFor more on this study, please refer to the References.\n\n\n== Key publications ==\n* Scott, J. 1988. \'\'Trend Report Social Network Analysis\'\'. Sociology 22(1). 109-127.\n* Borgatti, S.P. et al. 2009. \'\'Network Analysis in the Social Sciences.\'\' Science 323. 892-895.\n* Rowley, TJ. 1997. \'\'Moving beyond dyadic ties: A network theory of stakeholder influences.\'\' Academy of Management Review 2284). 887-910.\n* Bodin, Ö., Crona, B., Ernstson, H. 2006. \'\'Social networks in natural resource management: What is there to learn from a structural perspective?\'\' Ecology and Society 11(2).\n* Wasserman, S., Faust, K. 1994. \'\'Social network analysis: Methods and applications\'\' (Vol. 8). Cambridge university press.\n* Prell, C. (2012): \'\'Social network analysis: History, theory and methodology\'\', London.\n* Reed, M.S., Graves, A., Dandy, N., Posthumus, H., Hubacek, K., Morris, J., Prell, C., Quinn, C.H., Stringer, L.C. 2009. \'\'Who’s in and why? A typology of stakeholder analysis methods for natural resource management.\'\' Journal of Environmental Management 90, 1933-1949.', '[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n|\'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n\'\'\'In short:\'\'\' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Social Network Analysis until 2019.\'\'\' Search terms: \'Social Network Analysis\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel\'\'\'. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued ""against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions."" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that ""(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. \'\'\'Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.""\'\'\' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as ""entitites"", ""actors"", ""vertices"" or ""agents"" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are ""(...) tied to one another through socially meaningful relations"" (Prell et al. 2009, p.503), which can be ""(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection"" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.\n\n[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|\'\'\'Types of Ties in a Social Network.\'\'\' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations ""(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns."" (Prell et al. 2009, p.503). \'\'\'Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as ""dyads"", but rather the study of patterns within a network.\'\'\' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors\' behavior emerges from the interaction between them: ""Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions."" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent\'s actions (Borgatti et al. 2009)']"|0.033112582781456956|0.8
8|What is the purpose of ANCOVA in statistical analysis?|ANCOVA is used to compare group means while controlling for the effect of a covariate.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=""R"" line>\nhist(residuals(model_1),\n     col=""yellow"")\n</syntaxhighlight>\n\n<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:', '\'\'\'Note:\'\'\' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|\'\'\'SCOPUS hits per year for ANOVA until 2019.\'\'\' Search terms: \'ANOVA\' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). \'\'\'It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.\'\'\' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. \'\'\'The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.\'\'\' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. \'\'\'It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.\'\'\' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called \'[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]\' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or \'[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]\' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. \'\'\'If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.\'\'\' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels.', '<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=""R"" line>\noptions(contrasts = c(""contr.treatment"", ""contr.poly""))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=""R"" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the ""noise"" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question ""What treatment type has the most effect on anxiety level?""\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=""R"" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,\'data$group\') \n</syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]']"|0.03829787234042553|1.0
9|What are the key principles and assumptions of ANCOVA?|ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=""R"" line>\nhist(residuals(model_1),\n     col=""yellow"")\n</syntaxhighlight>\n\n<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:', '<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=""R"" line>\noptions(contrasts = c(""contr.treatment"", ""contr.poly""))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=""R"" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the ""noise"" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question ""What treatment type has the most effect on anxiety level?""\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=""R"" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,\'data$group\') \n</syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]', '\'\'\'Note:\'\'\' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|\'\'\'SCOPUS hits per year for ANOVA until 2019.\'\'\' Search terms: \'ANOVA\' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). \'\'\'It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.\'\'\' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. \'\'\'The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.\'\'\' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. \'\'\'It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.\'\'\' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called \'[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]\' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or \'[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]\' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. \'\'\'If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.\'\'\' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels.']"|0.03829787234042553|1.0
10|What are the assumptions associated with ANCOVA?|ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = ""Score by the treatment type"",\n     pch = 15,\n     xlab = ""anxiety score before the treatment"",\n     ylab = ""anxiety score after the treatment"")\n\nlegend(\'topleft\',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == ""grp1""]~ t1[group == ""grp1""],\n              data = data))\nabline(lm (t3[group == ""grp2""]~ t1[group == ""grp2""],\n              data = data))\nabline(lm (t3[group == ""grp3""]~ t1[group == ""grp3""],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=""R"" line>\nhist(residuals(model_1),\n     col=""yellow"")\n</syntaxhighlight>\n\n<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:', '<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=""R"" line>\noptions(contrasts = c(""contr.treatment"", ""contr.poly""))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=""R"" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the ""noise"" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question ""What treatment type has the most effect on anxiety level?""\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=""R"" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,\'data$group\') \n</syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]', ""Eine weitere allgemeine Unterscheidung lässt sich zwischen [[:Category:Inductive|induktiven]] und [[:Category:Deductive|deduktiven]] Ansätzen treffen. Viele deduktive Ansätze sind von einem Bias betroffen, die mit der Probenahme verbunden ist. Induktive Ansätze sind eher mit einem Bias bei der Interpretation verbunden. Deduktive Ansätze bauen oft um geplante Experimente herum, während die Stärke induktiver Ansätze darin liegt, dass sie weniger an methodische Designs gebunden sind, was auch dazu führen kann, dass Biases verborgener und damit schwerer zu entdecken sind. Aus diesem Grund liegt bei qualitativen Ansätzen jedoch häufig der Schwerpunkt auf einer prägnanten Dokumentation.\n\nDer Zusammenhang zwischen räumlichen Skalen und Bias ist recht einfach, da der Schwerpunkt [[:Category:Individual|auf dem Individuum]] mit kognitiven Biases zusammenhängt, während [[:Category:System|System-]]Skalen eher mit Vorurteilen, Biases im akademischen Bereich und statistischen Biases assoziiert werden. \nWährend die Auswirkungen zeitlicher Biases weniger erforscht sind, ist der prognostische Bias ein prominentes Beispiel, wenn es um [[:Categorie:Future|Zukunftsprognosen]] geht, und ein weiterer Fehler ist die Anwendung unserer kulturellen Ansichten und Werte auf Menschen [[:Category:Past|der Vergangenheit]], was noch nicht eindeutig als Bias benannt wurde.\nWas sowohl über räumliche als auch über zeitliche Skalen klar gesagt werden kann, ist, dass wir oft irrational gegenüber sehr weit entfernten Entitäten - in Raum oder Zeit - und sogar irrational mehr voreingenommen sind, als wir es sein sollten. '''Wir neigen zum Beispiel dazu, die Bedeutung eines fernen Zukunftsszenarios abzulehnen, obwohl es weitgehend den gleichen Chancen folgt, Realität zu werden, wie eine nahe Zukunft.''' Zum Beispiel möchte fast jede*r lieber morgen als in 20 Jahren im Lotto gewinnen, unabhängig von den Chancen, die Sie haben, zu leben und es geschehen zu sehen, oder von der längeren Zeit, die Sie mit Ihrem Lotteriegewinn für die (längere) kommende Zeit verbringen werden. Der Mensch ist ein höchst eigentümlich konstruiertes Wesen, und wir sind berüchtigt dafür, irrational zu handeln. Dies gilt auch für räumliche Distanz. Wir mögen uns irrational mehr um Menschen kümmern, die uns nahe stehen, als um Menschen, die sehr weit entfernt sind, sogar unabhängig von gemeinsamen Erfahrungen (z.B. mit Freunden) oder gemeinsamer Geschichte (z.B. mit der Familie). Auch hieraus lässt sich ein Bias ableiten, dessen wir uns bewusst sein können, der aber benannt werden muss. Zweifellos werden die aktuellen gesellschaftlichen Entwicklungen unsere Fähigkeit, unsere Biases zu erkennen, noch verstärken, da all diese Phänomene auch Wissenschaftler*innen betreffen. \n\n'''Die nachfolgende Tabelle kategorisiert verschiedene Typen von Bias''' gemäß des englischsprachigen [https://en.wikipedia.org/wiki/Bias Wikipedia-Eintrages über Bias], entsprechend zweier [[Design Criteria of Methods (German)|Designkriterien]].""]"|0.0051813471502590676|1.0
11|What are the strengths and challenges of Content Analysis?|Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.|"['== Strengths & Challenges ==\n* Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000)\n* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research ""(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates"". Further, ""Narrative inquiry attempts to capture the \'whole story\', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important \'intervening\' stages"" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).\n* \'\'\'The insights gained through narratives are subjective to the narrator, which implies advantages and challenges.\'\'\' Compared to an \'objective\' description of, e.g. a chain of events, the narration provides insights about the individual\'s interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: ""Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived."" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the \'real\' world. Squire et al. (2014) refer to this distinction as different forms of \'truth\' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical \'truth\'. They should try ""(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather."" Moen (2006, p.63), by comparison, explains that ""(...) there is no static and everlasting truth"", anyway.\n* This [[Bias and Critical Thinking|conflict between different \'truths\']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, \'usefulness\' and \'persuasiveness\' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: ""Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be \'well grounded\' and \'supportable\' (...) Narrative research does not claim to represent the exact \'truth\', but rather aims for \'verisimilitude\'"". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a \'trustworthy\' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one\'s research results (see Moen 2006, p.64). Also, researchers ""(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts"" (Polkinghorne 2007, p.484).\n* Further challenges may arise during the active collaboration of the researcher in the field. For example, ""(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher"" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]].\n\n\n== Normativity ==\n* As mentioned before, Narrative Inquiry methodology is in many regards similar to methods in [[Ethnography]], and shares elements with [[Hermeneutics]], [[Content Analysis|Qualitative Content Analysis]], and [[Open Interview|Open Interviews.]] Generally, it can be seen as a purely qualitative and inductive approach that focuses on limited temporal and spatial scales.\n* ""(...) [A] distinguishing feature of Narrative Research is a ""(...) move away from an objective conception of the researcher-researched relationship"" (...) to one in which the researcher is deeply involved in the research relationship. (...) In this process, narrative inquiry, becomes to varying degrees a study of self, or self alongside others, as well as of the inquiry participants and their experience of the world."" (Barrett & Stauffer 2009, p.11f, quote from Pinnegar & Daynes 2007, p.11). \'\'\'The autobiography of the researcher, as well as personal beliefs and practices and ethical positionality is brought into the research endeavor, which should be acknowledged in the process and results.\'\'\' ""Narrative inquirers cannot bracket themselves out of the inquiry but rather need to find ways to inquire into participants’ experiences, their own experiences as well as the co-constructed experiences developed through the relational inquiry process."" (Clandinin 2006, p.47)\n* Within this interaction in the field, the researcher must be aware of the consequences of creating new narratives in the studied subjects and situations, which is why Clandinin (2006, p.53) mentions ""(...) the importance of thinking in responsive and responsible ways about how narrative inquiry can shift the experiences of those with whom we engage"". In addition, Moen (2006, p.62) mentions that the narratives written down as a result of the research inquiry are themselves subject to interpretation: ""The story has been liberated from its origin and can enter into new interpretive frames, where it might assume meanings not intended by the persons involved in the original event. (...) [T]he narrative that is fixed in a text is thus considered an “open work” where the meaning is addressed to those who read and hear about it. Looking on narrative as an open text makes it possible to engage in a wide range of interpretations"".\n\n== Outlook ==\nBarrett & Stauffer (2009, p.16) claim that ""[n]arrative inquiry is still in its early stages of development (...). It will be subject to contestation over the years as the methodology develops, and other pathways are marked out."" This can be substantiated by the dispersed literature with its diverse understandings (see References), and the rather recent development of the discourse around narratives.\n\n\n== Key Publications ==\nVeroff et al. 1993. \'\'NEWLYWEDS TELL THEIHR STORIES: A NARRATIVE METHOD FOR ASSESSING MARITAL EXPERIENCES.\'\' Journal of Social and Personal Relationships 10. 437-457.\n* An example study that applied Narrative Interviews.', ""Schreier, M. 2014. ''Varianten qualitativer Inhaltsanalyse: Ein Wegweiser im Dickicht der Begrifflichkeiten.'' Forum Qualitative Sozialforschung 15(1). Artikel 18.\n* A (German language) differentiation between the variations of the qualitative content analysis.\n\nErlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n* A very helpful guide to content analysis, using the examples shown above.\n\n\n== References ==\n(1) Krippendorff, K. 1989. ''Content Analysis.'' In: Barnouw et al. (Eds.). ''International encyclopedia of communication.'' Vol. 1. 403-407. New York, NY: Oxford University Press.\n\n(2) White, M.D. Marsh, E.E. 2006. ''Content Analysis: A Flexible Methodology.'' Library Trends 55(1). 22-45. \n\n(3) Stemler, S. 2000. ''An overview of content analysis.'' Practical Assessment, Research, and Evaluation 7. Article 17.\n\n(4) Mayring, P. 2000. ''Qualitative Content Analysis''. Forum Qualitative Social Research 1(2). Article 20.\n\n(5) Stemler, S. 2015. ''Content Analysis. Emerging Trends in the Social and Behavioral Sciences: An Interdisciplinary, Searchable, and Linkable Resource.'' 1-14.\n\n(6) Erlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."", 'Some of the beneficial methodological elements also pose challenges. \n* Compared to - for example - the act of taking notes, the taping of videos is more complicated, needs more (expensive) video, audio, computer and data storage equipment as well as more technological knowledge and training (3).\n* \'\'\'The video data suggests objectivity but may not be objective due to the focus of the camera during the recording.\'\'\' If one individual or one perspective is favored, the subsequent analysis has an inherent focus on this element (1, 3, 4). Also, without additional data on the analyzed situation to contextualize the video material that may be gathered, e.g. through [[Ethnography|observations]] or [[Open Interview|Interviews]], researchers may develop a false feeling of understanding - especially foreign-country - (teaching) practices (1).\n* This supplementary data, which may be collected in addition to the videotaping, is also rather essential for a postponed analysis of the video material. If neglected, it may be impossible to properly analyze the videotaped situation some time after the initial research (1, 3).\n* With a camera being present, the subjects / participants may act differently than they usually would (3, 4).\n* Lastly, the act of videotaping individuals brings along moral implications of privacy and confidentiality that need to be addressed beforehand, e.g. by establishing an open relationship with the subjects of research and ensuring agreement on the usage of the video material (1, 3; see Normativity).\n\n== Normativity ==\n* The gathering of video material is very closely related to methods of [[Ethnography]], while the analysis of the gathered data is basically a special form of [[Content Analysis]].\n* As mentioned above, the video data may - or even should - be supplemented with additional qualitative and/or quantitative data, gathered e.g. by the use of a questionnaire, participant observation or through the analysis of documents such as worksheets or work samples in classrooms (1, 4).\n* ""Collecting videotape data requires a careful consideration of ethical and legal obligations regarding the protection of the confidentiality and privacy rights of those individuals who are filmed. If conducting cross-national studies, researchers should be aware that some countries have more fully developed laws and regulations than others, often with specific considerations for minors"" (Jacobs et al. 2007, p.290)\n\n== Key publications ==\nStigler et al. 1999. \'\'THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.\'\' U.S. Department of Education. National Center for Education Statistics Washington.\n* The TIMSS study was an empirical milestone for the integration of (comparative) video-based research, comparing the teaching practices in 231 classrooms in three countries.\n\n== References ==\n(1) Ulewicz, M. Beatty, A. (eds.) 2001. \'\'The Power of Video Technology in International Comparative Research in Education.\'\' National Academy Press Washington.\n\n(2) Janík, T. Seidel, T. Najvar, P. \'\'Introduction: On the Power of Video Studies in Investigating Teaching and Learning.\'\' In: Janík, T. Seidel, T. (eds). 2009. \'\'The Power of Video Studies in Investigating Teaching and Learning in the Classroom.\'\' Waxmann Verlag GmbH Münster. 7-19.\n\n(3) Jacobs, J.K. Hollingsworth, H. Givvin, K.B. 2007. \'\'Video-Based Research Made ""Easy"": Methodological Lessons Learned from the TIMSS Video Studies.\'\' Field Methods 19. 284-299.\n\n(4) Stigler, J. Gallimore, R.G. 2000. \'\'Using Video Surveys to Compare Classrooms and Teaching Across Cultures: Examples and Lessons from the TIMSS Video Studies\'\'. Educational Psychologist 35(2). 87-100.\n\n(5) Stigler et al. 1999. \'\'THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.\'\' U.S. Department of Education. Natoinal Center for Education Statistics Washington.\n\n(6) Brückmann, M. Knierim, B. \'\'Teaching and learning processes in physics instruction - chances of videotape classroom studies.\'\' In: Mikelskis-Seifert, S. Ringelband, U. Brückmann, M. (eds.) 2008. \'\'Four Decades of Research in Science Education - from Curriculum Development to Quality Improvement.\'\' Waxmann Verlag Münster.\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.']"|0.024390243902439025|0.0
12|What are the three main methods to calculate the correlation coefficient and how do they differ?|The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n\'\'\'In short:\'\'\' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, \'\'\'correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!\'\'\'\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n\'\'\'A note on calculating the correlation coefficient:\'\'\'\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson\'s correlation coefficient, Spearman\'s rank correlation coefficient and Kendall\'s rank coefficient. \n\'\'\'Pearson\'s correlation coefficient\'\'\' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson\'s correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. \'\'\'Spearman\'s rank correlation\'\'\' coefficient calculates the rank order of the variables\' values using a monotonic function whereas \'\'\'Kendall\'s rank correlation coefficient\'\'\' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson\'s correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\n#Fig.1\ndata(""mtcars"")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = ""Correlation between Miles per Gallon and Horsepower"",\n     xlab = ""Miles per Gallon"",\n     ylab = ""Horsepower"",\n     pch = 16,\n     col = ""red"",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n</syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables “mpg” and “hp” from the “mtcars” dataset. The Pearson\'s correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=""R"" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n</syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=""R"" inline>plot()</syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* \'\'\'x:\'\'\' variable, that will be displayed on the x-axis.\n* \'\'\'y:\'\'\' variable, that will be displayed on the y-axis.\n* \'\'\'xlab:\'\'\' title for the x-axis.\n* \'\'\'ylab:\'\'\' title for the y-axis.\n* \'\'\'pch:\'\'\' shape and size of the plotted observations, in this case, filled circles. [http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* \'\'\'col:\'\'\' plotting color. You can either write the name of the color or use the [https://www.r-graph-gallery.com/41-value-of-the-col-function.html color number].\n* \'\'\'las:\'\'\' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* \'\'\'xlim:\'\'\' set the limit of the x-axis.\n* \'\'\'ylim:\'\'\' set the limit of the y-axis.\n* \'\'\'abline:\'\'\' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=""R"" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n</syntaxhighlight>\n\n\nThe scatter plot matrix from this package is already very nice by default. It splits the plot into an upper, lower and diagonal part. The upper part consists of the correlation coefficients for the different variables. The red stars show you the results of the implemented correlation test. There is a range from zero to three stars and the higher the number of stars, the higher is the significance of the results for the test. In the diagonal part of the plot are histograms for every variable and show you the distribution of the variable. The bivariate scatter plots can be found on the lower part of the plot and contain a fitted line by default.\n\n\n== Line chart ==\n=== Definition ===\nA line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value.', '[[File:ConceptCorrelation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Correlations]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n\n<br/><br/>\n\'\'\'In short:\'\'\' Correlation analysis examines the statistical relationship between two continuous variables. For R examples on Correlations, please refer to [[Correlation Plots]].\n\n== Background ==\n[[File:Correlation.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Correlations until 2020.\'\'\' Search terms: \'Correlation\' in Title, Abstract, Keywords. Source: own.]]\n\nKarl Pearson is considered to be the founding father of mathematical statistics; hence it is no surprise that one of the central methods in statistics - to test the relationship between two continuous variables - was invented by him at the brink of the 20th century (see Karl Pearson\'s ""Notes on regression and inheritance in the case of two parents"" from 1895). His contribution was based on work from Francis Galton and Auguste Bravais. With more data becoming available and the need for an “exact science” as part of the industrialization and the rise of modern science, the Pearson correlation paved the road to modern statistics at the beginning of the 20th century. While other approaches such as the t-test or the Analysis of Variance ([[ANOVA]]) by Pearson\'s arch-enemy Fisher demanded an experimental approach, the correlation simply required data with a continuous measurement level. Hence it appealed to the demand for an analysis that could be conducted based solely on measurements done in engineering, or on counting as in economics, without being preoccupied too deeply with the reasoning on why variables correlated. \'\'\'Pearson recognized the predictive power of his discovery, and the correlation analysis became one of the most abundantly used statistical approaches in diverse disciplines such as economics, ecology, psychology and social sciences.\'\'\' Later came the \u200bregression analysis, which implies a causal link between two continuous variables. This makes it different from a correlation, where two variables are related, but not necessarily causally linked. This article focuses on correlation analysis and only touches upon regressions. For more, please refer to the entry on [[Regression Analysis]].)\n\n\n== What the method does ==\nCorrelation analysis examines the relationship between two [[Data formats|continuous variables]], and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called \'\'correlation coefficient\'\' indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. \n\nCorrelations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including [[Survey]]s and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date.\n\n\'\'\'There are different forms of correlation analysis.\'\'\' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https://365datascience.com/students-t-distribution/ Student\'s t-distribution]. Alternative correlation measures like [https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ Kendall\'s tau and Spearman\'s rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman\'s rho is the most robust correlation measure when it comes to non-normally distributed data.\n\n==== Calculating Pearson\'s correlation coefficient r ====\nThe formula to calculate [https://www.youtube.com/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br/>\n\n\'\'\'1) Is the relationship between two variables positive or negative?\'\'\' If one variable increases, and the other one increases, too, we have  a positive (""+"") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\xa0[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534609/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (""-""): for example, the relationship between \'pizza eaten\' and \'pizza left\' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship. \n\n\'\'\'2) Is the correlation coefficient small or large?\'\'\' It can range from -1 to +1, and is an important measure when we evaluate the strength of a statistical relationship. Data points may scatter widely in a [[Correlation_Plots#Scatter_Plot|scatter plot,]] or there may be a rather linear relationship - and everything in between. An example for a perfect positive correlation (with a correlation coefficient \'\'r\'\' of +1) is the relationship between temperature in [[To_Rule_And_To_Measure#Celsius_vs_Fahrenheit_vs_Kelvin|Celsius and Fahrenheit]]. This should not be surprising, since Fahrenheit is defined as 32 + 1.8° C. Therefore, their relationship is perfectly linear, which results in such a strong correlation coefficient. We can thus say that 100% of the variation in temperature in Fahrenheit is explained by the temperature in Celsius.\n\nOn the other hand, you might encounter data of two variables that is scattered all the way in a scatter plot and you cannot find a significant relationship. The correlation coefficient \'\'r\'\' might be around 0.1, or 0.2. Here, you can assume that there is no strong relationship between these two variables, and that one variable does not explain the other one.', 'To test the continuous variables for possible correlations, we use the seaborn heatmap function to visualize the correlation between multiple variables. With this tool, we can also check for possible multicollinearity of the variables, so correlation among the independent variables. The scale on the right shows the correlation coefficient ranging from +1 to -1.\n\n<syntaxhighlight lang=""Python"" line>\ndf_continous = df[[""CGPA"",""GRE Score"", ""TOEFL Score"", ""Chance of Admit ""]]\nsns.heatmap(df_continous.corr(), vmin=-1, vmax=1, annot=True, cmap = ""PiYG"")\nplt.show()\n</syntaxhighlight>\n\nWe can see in the heatmap that all the variables are highly correlated with each other. Another tool we can use to check for multicollinearity is the Variance Inflation Factor. To test for multicollinearity using the VIF score, we create a new table containing the variables and the VIF. To calculate the score, the variance_inflation_factor function from the statsmodels module is used.\n\n<syntaxhighlight lang=""Python"" line>\ndata = df[[""CGPA"", ""TOEFL Score"", ""GRE Score""]]\nvif_data = pd.DataFrame()\nvif_data[""feature""] = data.columns\n\nfor i in range(len(data.columns)):\n    vif_data[""VIF""] = variance_inflation_factor(data.values, i)\n\nprint(vif_data)\n</syntaxhighlight>\n\nOne recommendation for the VIF score is interpreting a score higher than 5 on a variable as a sign that this variable is correlated with the other given variables.\nAs expected based on the heatmap, all of the variables have a high VIF score, which we interpret as high multicollinearity. Therefore, we drop 2 of the 3 variables for the linear regression. In this case, the GRE Score and the TOEFL Score are dropped. \n\n==Regression Analysis==\nNow we can do the regression analysis. We use the variables CGPA and Research as predictor or independent variables for the model and the Chance of Admittance variable as our dependent variable. If we get a meaningful and significant model in the end, it allows us to make predictions on the chances of admittance of a student based on their CGPA and whether they already have some research experience or not. \n\n<syntaxhighlight lang=""Python"" line>\nx = df[[""CGPA"", ""Research""]]\nx = sm.add_constant(x) \ny = df[""Chance of Admit ""]\n\nmodel = sm.OLS(y, x).fit()\nprint(model.summary())\n</syntaxhighlight>\n\n==Interpretation of the Regression Results==\n* \'\'\'P>|t|\'\'\' is the p-value. We can see, that the p-value of our variables is very close to 0, which corresponds to our model being highly significant. Therefore, we can reject the null hypothesis (our coefficients are equal to 0). \n\n* \'\'\'coef\'\'\' are the regression coefficients of our model. We interpret the coefficient as the effect the independent variable has on the dependent variable. The higher the coefficient, the bigger the magnitude of the effect of the variable. We would also conclude that the variables have a positive effect on the dependent variable, meaning that if the CGPA is higher, the chance of admission also tends to be higher. Furthermore, we can see that the CGPA seems to make a larger impact on the chance of admission than the Research variable. \n\n* \'\'\'R-squared and Adjusted R-squared\'\'\' evaluates how well our regression model fits our data and is always represented with values between 0 and 1. Therefore, a higher R-squared value indicates smaller differences between our data and the fitted values. In our example, an adjusted R-squared value of .775 means that our model fits our data reasonably well.\n\n* \'\'\'AIC\'\'\' The Akaike information criterion cannot be interpreted by itself. Normally, we could use the AIC to compare our model against other different models. One common case is a comparison against the null model. The formula of the AIC weighs the number of parameters against the Likelihood function, consequently favoring models that use the least amount of parameters to model our data the best. Therefore, it corresponds to the principle of Occam\'s razor, which you can read more about here: https://sustainabilitymethods.org/index.php/Why_statistics_matters#Occam.27s_razor. For evaluation and selection of models, it is often used as an alternative to p-values.\nConsequently, the regression formula can be written down as: \n\'\'\'Chance of Admittance = CGPA * 0.1921 + Research * 0.0384 - 0.9486\'\'\', which can now be used to predict the chance of admittance of an undergraduate based on their research experience and their CGPA. \n\nLastly, we need to make some checks to see if our model is statistically sound. We will check if the residuals are normally distributed, for heteroscedasticity, and serial correlation. \nTo check for normal distribution of the residuals, we take a look at the QQ-Plot to look at the distribution of the residuals. It should follow the shape of the red line roughly, to assume normal distribution of residuals. The QQ Plot compares the theoretical quantiles of the normal distribution with the residual quantiles. If the distributions are perfectly equal, meaning the residuals are perfectly normally distributed, the points will be perfectly on the line. You can find out more about QQ-Plots here: https://sustainabilitymethods.org/index.php/Data_distribution#The_QQ-Plot\n<syntaxhighlight lang=""Python"" line>\nresiduals = model.resid\nfig = sm.qqplot(residuals, scipy.stats.t, fit=True, line=""45"")\nplt.show()\n</syntaxhighlight>\n\nThis can also be tested using the Jarque-Bera test. A Jarque-Bera test compares the kurtosis und skewness of the distribution of your variable with the properties a normal distribution has.\nThe lower the value of the Jarque-Bera test, the more likely the residuals are normally distributed. If the p-value is above your chosen significance level (e.g., 0.05), you can assume that your residuals are normally distributed.\n<syntaxhighlight lang=""Python"" line>\njarque_bera(df[""CGPA""])\n</syntaxhighlight>\n\nAs you can see, the value of the Jarque-Bera test is quite small and the p-value is even above 0.1. It can therefore not be said that the residuals do not follow a normal distribution.\nInstead, we assume that your variable CGPA follows a normal distribution. Note that you cannot do the test for binary variables since the Jarque-Bera test assumes that the distribution of residuals is continuous.\n\nNext, we should test for heteroscedasticity. In our regression model, we have assumed homoscedasticity which means that the variance of the residuals is equally distributed. The residuals are the difference between your observations from your predictions.\nIf this is not the case, you have heteroscedasticity. This is often the case because of outliers or skewness in the distribution of a variable. You can assess this visually by plotting the variance of your residuals against an independent variable. Again here, this only makes sense for continuous variables, which is why we look at GCPA.\n\n<syntaxhighlight lang=""Python"" line>\n# Calculate the residuals\nresiduals = model.resid\n\n# Calculate the squared residuals ( to only have positive values)\nsquared_resid = residuals ** 2\n\n# Group the squared residuals by the values of each independent variable\ngrouped_resid = squared_resid.groupby(x[\'CGPA\'])']"|0.0149812734082397|1.0
13|What is the purpose of a correlogram and how is it created?|A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n\'\'\'In short:\'\'\' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, \'\'\'correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!\'\'\'\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n\'\'\'A note on calculating the correlation coefficient:\'\'\'\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson\'s correlation coefficient, Spearman\'s rank correlation coefficient and Kendall\'s rank coefficient. \n\'\'\'Pearson\'s correlation coefficient\'\'\' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson\'s correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. \'\'\'Spearman\'s rank correlation\'\'\' coefficient calculates the rank order of the variables\' values using a monotonic function whereas \'\'\'Kendall\'s rank correlation coefficient\'\'\' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson\'s correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\n#Fig.1\ndata(""mtcars"")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = ""Correlation between Miles per Gallon and Horsepower"",\n     xlab = ""Miles per Gallon"",\n     ylab = ""Horsepower"",\n     pch = 16,\n     col = ""red"",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n</syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables “mpg” and “hp” from the “mtcars” dataset. The Pearson\'s correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=""R"" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n</syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=""R"" inline>plot()</syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* \'\'\'x:\'\'\' variable, that will be displayed on the x-axis.\n* \'\'\'y:\'\'\' variable, that will be displayed on the y-axis.\n* \'\'\'xlab:\'\'\' title for the x-axis.\n* \'\'\'ylab:\'\'\' title for the y-axis.\n* \'\'\'pch:\'\'\' shape and size of the plotted observations, in this case, filled circles. [http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* \'\'\'col:\'\'\' plotting color. You can either write the name of the color or use the [https://www.r-graph-gallery.com/41-value-of-the-col-function.html color number].\n* \'\'\'las:\'\'\' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* \'\'\'xlim:\'\'\' set the limit of the x-axis.\n* \'\'\'ylim:\'\'\' set the limit of the y-axis.\n* \'\'\'abline:\'\'\' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=""R"" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n</syntaxhighlight>\n\n\nThe scatter plot matrix from this package is already very nice by default. It splits the plot into an upper, lower and diagonal part. The upper part consists of the correlation coefficients for the different variables. The red stars show you the results of the implemented correlation test. There is a range from zero to three stars and the higher the number of stars, the higher is the significance of the results for the test. In the diagonal part of the plot are histograms for every variable and show you the distribution of the variable. The bivariate scatter plots can be found on the lower part of the plot and contain a fitted line by default.\n\n\n== Line chart ==\n=== Definition ===\nA line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value.', ""[[File:860-header-explainer-correlationchart.jpg|500px|left|thumb|'''Korrelationen können täuschen.''' Quelle: [http://www.tylervigen.com/spurious-correlations Spurious correlations]]] \nWahrscheinlichkeit war das zugrunde liegende Kernprinzip, oder mit anderen Worten, die statistische Antwort auf die Frage, ob etwas wirklich zufällig war oder ob es einen statistischen Zusammenhang gab. Unter den wachsenden Volkswirtschaften tauchten immer mehr Zahlen auf, und aufbauend auf der niederländischen Lösung der doppelten Buchführung stellte sich die Frage, ob in den Daten Muster zu finden waren. Konnte das Ernteergebnis der Baumwolle ihren Marktwert später im Jahr vorhersagen? Und würden Ernteausfälle eindeutig mit Hungersnöten in Zusammenhang stehen, oder könnte dies kompensiert werden? '''Statistische [[Correlations|Korrelationen]] waren in der Lage, sich auf Variablen zu beziehen und herauszufinden, ob die eine mit der anderen in Beziehung steht oder ob die beiden nicht miteinander verbunden sind.''' Dies löste einen heftigen Wechsel des Utilitarismus von der Philosophie in die Wirtschaft aus, die bis heute zu den Kerndisziplinen gehört. Über die Berechnung von Utilitarismus lässt sich viel sagen, [[Big problems for later|aber das werde ich sicher nicht hier tun]]. In der Wissenschaft wuchsen überall gelehrte Akademien, und die Universitäten blühten auf, auch wegen des wirtschaftlichen Paradigmas einer wachstums- und konsumorientierten Grundlinie, die sich herausbildete, und es entsteht immer mehr Wissen, um dieses Paradigma zu kritisieren und in Frage zu stellen. Dies löste das oft als Zeitalter der Reflexion bezeichnete Zeitalter aus, in dem die Empirie im Grunde ungezähmt wurde und die Tiefe der Untersuchung zu Disziplinen führte, die sich in noch tieferen Teildisziplinen zu verankern begannen. Die Auswirkungen auf die Gesellschaften waren schwerwiegend. Mechanisierung, neue Ansätze in der Landwirtschaft, der Aufstieg der modernen Medizin, eine immer raffiniertere Biologie und die Entwicklungen in der Chemie zeugen davon, dass die physische Welt immer mehr in den Fokus der Wissenschaft geriet. Die allgemeinen Linien des philosophischen Denkens - Vernunft, Gesellschaftsvertrag und Utilitarismus - lösten sich teilweise von der Philosophie ab und entwickelten sich auf Gedeih und Verderb zu eigenständigen Disziplinen wie Psychologie, Sozialwissenschaft, Politikwissenschaft, Kulturwissenschaften und Wirtschaftswissenschaften. Wir beobachten also eine Abweichung der empirischen Wissenschaften von der zuvor allumfassenden Philosophie. Der Empirismus war also auf dem Vormarsch, und mit ihm eine ausgeprägte Verschiebung in Wissenschaft und Gesellschaft.\n\n==== Nach den Kriegen - ''Der Aufstieg von [[Agency, Complexity and Emergence|Agency]]'' ====\nAus gesellschaftlichen, wirtschaftlichen, technologischen und anderen Entwicklungen entstand fast konsequenterweise ein Kontrapunkt. Dieser wurzelte bereits in [https://plato.stanford.edu/entries/kant-reason/ Kant] und seiner Annahme der Priorität der Vernunft, und [https://plato.stanford.edu/entries/marx/ Marx] machte einen entscheidenden Schritt hin zu einer kritischen Perspektive. Er entwickelte die Philosophie von einer Untersuchung der Weltbilder hin zu einer Agenda zur Schaffung von Veränderungen, die sich hauptsächlich auf das Wirtschaftssystem konzentrierte. '''All dies kaskadierte in die gewaltigen Veränderungen des 20. Jahrhunderts, die fast zu viel sind, um sie zu begreifen.''' Es könnte jedoch der Schluss gezogen werden, dass die Katastrophe der beiden Weltkriege, die Anerkennung der Ungleichheiten des Kolonialsystems und die Konzentration auf die Ungerechtigkeiten mit Emanzipation, Diskriminierung und Rassismus verbunden ist. Die Kritische Theorie kann als eine richtungsweisende Entwicklung angesehen werden, bei der die Strukturen und Bedingungen von Gesellschaften und Kulturen in den Mittelpunkt gerückt werden."", '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || [[:Category:Productivity Tools|Productivity Tools]] || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConceptual figures - or ""made-up figures"", as we like to call them - are [[Glossary|visualisations]] that do not represent data itself, but help illustrate a theoretical idea or a methodological design. They are often relevant for scientific publications, and can help improve presentations and pitches by making an idea or approach more understandable for the audience.\n\n== Goals ==\n* Create better figures to explain concepts or methodological designs\n* Expand on your text through visualisations\n\n== Getting started ==\n\'\'\'Here are some initial recommendable ideas on how to make this type of figure:\'\'\'\n\n1) \'\'\'Follow Ockham\'s razor\'\'\': just like for research designs - and much of life - Ockham\'s razor can guide your approach to developing a conceptual figure. It should include as much information as necessary, but as little as possible. Useless information that the figure does not benefit from should be avoided. Also, the information provided in the figure should be complementary to the text, not (entirely) identical, which would render the figure itself redundant. Good figures help us better understand the text that we read already, but do not take up space without providing anything new.\n\n2) \'\'\'Make your figures intuitive to understand.\'\'\' A figure with a lot of complex information usually overwhelms the reader. You want your figure to stimulate further engagement with what it shows. A good figure is graspable at first sight, at least in terms of the core elements. This also means that there should usually be no more than 3 - 5 five core elements within the figure, although there can be accompanying smaller elements around those. It should be clear right away what is shown, and how parts of the figure relate to each other. Then, the information that lies within the figure should be easily accessible, even if it takes some time to engage with everything that is shown. Be aware that people (at least in Western culture) usually scan figures from the top-left to the bottom right - build your figures accordingly.\n\n3) \'\'\'Avoid empty spaces or disbalances\'\'\'. A lot of blank space, or an unbalanced distribution of visual elements just feels off.\n\n4) \'\'\'Use clear fonts and a balanced color palette\'\'\'. Fonts are certainly a matter of taste, but some fonts are just more adequate for scientific publications or presentations than others. The text should be readable at first glance (see 2), so choose a font that is easy to decipher and ideally fits to the rest of the document. We recommend sans serif fonts like Ariel or Courier, but you should find your own preference or get inspired by other publications. Colors, then, should first make a figure more visually appealing. This can be done by using colors that fit together, for example by relying on a Wes-Anderson-Color-Palette, which is obviously the best set of colors. However, the use of certain signal colors can help guide the audience\'s attention to specific details. Colors can also hold information. For example, a color gradient can represent a decrease or increase of some value, or different categories of things. Also, colors can be used recurringly, for example when a certain color represents a specific actor, element, topic or place in one figure, and the same color represents the same element in another figure, which supports continuity in your design and guides the reader through the document.\n\n5) \'\'\'Create a narrative for the figure, and the figure as a narrative\'\'\'. A good figure can stand on its own, and represent something. It is alright if the full picture of the figure is only understandable with the help of the original text it expands upon. However, if you can show the figure to someone and (s)he\'ll understand what it is about, you are on the right track. For this, it is important to include all information necessary to understand what is shown (see 1). More importantly, a figure can give the reader information on how things relate to each other - focusing on temporal order, most commonly through the use of arrows or a timeline, or causal relations, often also done with arrows, although many more approaches are conceivable. What happens when, and what is a solution to, or consequence of, what? Also, proportions of information can contain information: if an element is smaller than others, it is maybe not as important.\n\n6) \'\'\'Learn from other figures, and develop your own style\'\'\'. It is okay to steal from others. There are lots of great conceptual figures in science, and studying them with a focus on what makes them great - or not great - can help find ideas to improve your own design. Of course, you shouldn\'t simply adapt other people\'s figures, but take parts or approaches that might fit your own needs. \n\n7) \'\'\'Some last tips:\'\'\'\n* Decide on one software solution to implement your figure in, and learn what it can do.\n* Acknowledge the expectations of your audience, or deliberately decide to break with their expectations when appropriate.\n* Always keep in mind how your figure relates to the character and content of the document it is placed in.\n* Follow your intuition: we all have our own ideas about what looks good, and we do our best work when we are convinced of what we are doing ourselves.\n* Last but not least, however, get feedback from peers when in doubt. They will be able to tell you if everything is understandable, and overcome the tunnel vision that you might develop when you are deep in a topic.\n<br/>\n[[File:Conceptual Figures.png|800px|frameless|center|\'\'\'A conceptual figure on conceptual figures.\'\'\' Source: own.]]\n<br/>\nEventually, however, our main recommendation remains to \'\'\'keep it simple\'\'\'. Don\'t overburden the reader with information, but focus on what is really important. For more helpful tips on visualisations, please refer to the entry on Statistical Figures and Poster Design.\n\n=== Examples ===\n\n* \'\'\'Doughnut Economics\'\'\'\n[[File:Conceptual Figures - Doughnut Economics.png|400px|thumb|left|\'\'\'The Doughnut Economics concept.\'\'\' Source: [https://doughnuteconomics.org/about-doughnut-economics Doughnut Economics Lab]]]']"|0.005076142131979695|0.3333333333333333
14|What is telemetry?|Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.|"['This trend in citizen participation is supported by technical development making bird identification easier through the usage of identification apps. Smartphone apps such as “eBirds” help to identify species and also upload photos and the location where the individual was observed. This leads to a growing global online-community of bird watchers who create new data points in observation databases that scientists and environmentalists can make use of for conservation research or planning. For instance, in Canada, the number of people submitting photos to eBirds increased by 30% between 2019 and 2020 (8).\n\nBecause many bird watchers are novices, bird identification is not always carried out correctly. This is a general problem of citizen science. In the case of eBirds, bird experts volunteer as reviewers who check photos for correct determination, especially in the case of rare species that are uncommon in a certain area. This increases the quality of the data (8). However, a study conducted in the UK has shown that citizen science data may be reliable only for widespread and common species, even with many data points and a good coverage of the area (9). \n\nScientists hope that [[Citizen_Science|citizen science]] data may fill data gaps, for instance in the tropics. However, comparisons to data from Bird Life International have shown that the bird abundances for rare species in the tropics are strongly over-estimated. A likely reason is that bird observers are very determined to find these rare species and thereby overlook or under-record more common species and focus on rare ones (10). \nNevertheless, as technology progresses, identification apps are becoming better and easier to use. Moreover, scientists are working on new approaches to include such data into their research in meaningful ways. This trend of increasing citizen participation is thus likely to continue.\n\nTelemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is “the system of determining information about an animal through the use of radio signals from or to a device carried by the animal” (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===\n\nFuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\nSutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\nRobertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work. British Trust for Ornithology, Tring.\n\n=== Empirical ===\n\nGibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tamaño de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n== References ==\n(1) Sutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\n\n(2) Robertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work.\n \n(3) Fuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\n\n(4) Bibby, C. J., Burgess, N. D., Hillis, D. M., Hill, D. A., & Mustoe, S. (1992).\xa0Bird census techniques. Elsevier.\n\n(5) Buckland, S. T., Anderson, D. R., Burnham, K. P., & Laake, J. L. (1993). Distance sampling: estimating abundance of biological populations. Chapman & Hall, London.\n\n(6) Gibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tamaño de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n(7) Verner, J. (1985). Assessment of counting techniques. Current Ornithology: Volume 2, 247-302.\n\n(8) CBC (2021). How birding’s pandemic popularity is expanding data collection for science.  https://www.cbc.ca/news/science/science-birding-pandemic-data-wildlife-1.6113333 (accessed on 06.03.2023)\n\n(9) Boersch-Supan, P. H., Trask, A. E., & Baillie, S. R. (2019). Robustness of simple avian population trend models for semi-structured citizen science data is species-dependent. Biological Conservation, 240, 108286.\n\n(10) Science Daily (2020). Community science birding data does not yet capture global bird trends. https://www.sciencedaily.com/releases/2020/07/200707084012.html (accessed on 06.03.2023)\n\n(11) Gutema, T. M. (2015). Wildlife radio telemetry: use, effect and ethical consideration with emphasis on birds and mammals. Int J Sci Basic Appl Res, 24(2), 306-313.\n\n(12) Barron, D. G., Brawn, J. D., & Weatherhead, P. J. (2010). Meta‐analysis of transmitter effects on avian behaviour and ecology. Methods in Ecology and Evolution, 1(2), 180-187.\n\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau.\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]', 'THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=""Python"" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n</syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=""Python"" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir(\'/path/to/your/directory\')\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndf = pd.read_csv(\'D202.csv\')\ndf.head()\n</syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let\'s choose the most relevant columns for our research:\n\n<syntaxhighlight lang=""Python"" line>\n## Let\'s choose the most relevant columns for our research:\ndf[\'start_date\'] = pd.to_datetime(df[\'DATE\'] + \' \' + df[\'START TIME\'])\ndf[\'cost_dollars\'] = df[\'COST\'].apply(lambda x: float(x[1:]))\ndf.rename(columns={\'USAGE\': \'usage_kwh\'}, inplace=True)\ndf = df.drop(columns=[\'TYPE\', \'UNITS\', \'DATE\', \'START TIME\', \'END TIME\', \'NOTES\', \'COST\']).set_index(\'start_date\')\n</syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe “cost_dollars” by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don\'t specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=""Python"" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n</syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https://lg4ml.org/grounddetection/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=""Python"" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y=\'usage_kwh\',\n        title=\'Usage of Electricity over 2 Weeks\',\n        labels={\'start_date\': \'Date\', \'usage_kwh\': \'Usage (KWh)\'}) ## uses the data from ""start_date"" called ""Date"", and the data of ""usage_kwh"" called ""usage (KwH)""\n</syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time</small>\n\n<syntaxhighlight lang=""Python"" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df[\'usage_kwh\'])) ## transforms into frequencies\nfrequencies = rfftfreq(df[\'usage_kwh\'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)\n\nn = 100 ##plots the first 100 frequencies\npx.line(x=frequencies[:n], y=transform[:n], markers=True,\n           title=\'Magnitude of Frequencies\',\n           labels={\'x\': \'Frequency\', \'y\': \'Magnitude\'}) ## creates plot with n=100 frequencies\n</syntaxhighlight>\n\n[[File:scatter plot.png|700px|center|]]\n<small>Figure 2: Scatter plot visualizing the number of times a certain frequency of electricity usage occurred</small>\n\n==Statistical Analysis==\nThere are a multitude of statistical analysis methods that can be used to analyze time series data. While simple statistical tests like the t-test, ANOVA, and regression analysis can be used with time series data to identify relationships and dependencies in the data, there are also more specific methods to analyze time series data.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || \'\'\'[[:Category:Software|Software]]\'\'\' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || \'\'\'[[:Category:Team Size 30+|30+]]\'\'\'\n|}\n\n== What, Why & When ==\nLoom is a \'\'\'video recording software which lets you easily record your screen and yourself at the same time\'\'\' right in your browser. This way, you can show your work or ideas to colleagues or others, and not waste time and harddrive space on recording videos right on your computer.\n\n== Goals ==\n* Communicate your work to colleagues, collaborators, students in a way that is precise and engaging.\n* Better enable others to prepare for meetings.\n\n== Getting started ==\n[[File:Loom Logo.png|400px|thumb|right|\'\'\'Loom logo.\'\'\' Source: [https://www.loom.com/ Loom]]]\nFor some situations, it might be practical to record your screen and talk over what is shown. Loom lets you record your browser, Excel file, presentation or anything you open on your computer, and record your face and voice while clicking through the applications. For browser recordings, you can simply enable a Loom extension; for anything else, you download the software; and there is also a mobile app. You just record the video and share the link and others can watch it online. There are voice transcripts, and you can share the video alongside a call-to-action, if you like.\n\n\'\'\'We found Loom to be helpful to convey a point to colleagues so that they can prepare for a meeting, or to quickly summarize our work for colleagues and others\'\'\', so that they can engage with the video whenever they find the time. Loom is easier than recording the video with external software, and sharing or re-uploading the created .mp4-file. As opposed to video-call screen-sharing, Loom can be used asynchronously.\n\n[https://www.loom.com/ Loom] is free for anyone and allows for 5-minute videos, up to 100 videos in total. There are paid options that allow for recordings of up to 45 minutes, and these are [https://www.loom.com/education free for educational accounts].\n\n== Links & Further reading ==\n* [https://www.youtube.com/watch?v=3PY6v9s1MU8 An introductory video to using Loom] \n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.']"|0.004608294930875576|1.0
15|What is a common reason for deviation from the normal distribution?|A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.|"['[[File:Outliers 2.png|900px]]\n\n\'\'Figure 2: Outliers detected with quantile-based method\'\'\n\n===Distribution Based===\nIn distribution-based outlier detection, the assumption is made that data from a measurement are normally distributed. Based on the mean and standard deviation of the data, a probability results for each point of the recording that belongs to this normal distribution. Using the so-called Chauvenet criterion, a lower limit can be defined below which a point is marked as an outlier. If the Chauvenet criterion is set to 0.5, all points that belong to the normal distribution of the data with a probability of less than 0.5% are recognized as outliers. Depending on how high the criterion is set, the number of detected outliers changes accordingly (this is illustrated in figure 3). How high the optimal limit is, i.e. to which value the Chauvenet criterion must be set, depends on the further goal and is often only determined over time. This approach is closely related to the Z-score, which is a measure of the deviation of a point from the mean of a standardized normal distribution.\nThe Chauvenet criterion can be applied to both univariate and multivariate data, offering the possibility to apply outlier detection either to single attributes or to a combination of attributes.\n\n<syntaxhighlight lang=""Python"" line>\n# Set the quantiles and get the respective values\nlower_q, upper_q = 0.02, 0.98## determine the qyartule thresholds.\nlower_value = data[\'z\'].quantile(lower_q)## apply the lower threshold to dataset.\nupper_value = data[\'z\'].quantile(upper_q)## apply the upper threshold to dataset.\n\n# Plot the results\ndef univariate_chauvenet(data_points: np.ndarray, criterion: float = None):\n    # Calculate mean and std of the data\n    mean = data_points.mean()\n    std = data_points.std()\n\n    # Calculate the criterion based on the number of points if not provided\n    if criterion is None:\n        criterion = 1.0/(2 * len(data_points)) ## criterion is set to 1 divided 2 times the length of the data set.\n\n    # Calculate the absolute deviation and scale by std\n    deviation = np.abs(data_points - mean) / std\n\n    # Calculate the probabilities using erfc and return the mask. Erfc is the error function calculating the probability of a data point (not) being an outlier.\n    # if the probability is lower than the criterion (so the likelihood that the data point is no outlier is smaller than the set criterion), it is an outlier.\n    probabilities = special.erfc(deviation)\n    return probabilities < criterion\n\n\nfig, axs = plt.subplots(ncols=3, figsize=(16, 5))##create plot with three sub-plots\nfig.suptitle(\'Univariate Chauvenet Criterion\', size=20)\nfor i, c in enumerate([0.01, 0.005, 0.001]): ## creating a for loop to check for outliers for different criteria.\n    mask = univariate_chauvenet(data_points=data[\'z\'].values, criterion=c)## create mask as above\n    sns.scatterplot(data=data, x=\'time\', y=\'z\', ax=axs[i],\n                    hue=np.where(mask, \'Outlier\', \'No Outlier\'),\n                    hue_order=[\'No Outlier\', \'Outlier\']) ##create scatter plots colored according to the mask.\n    axs[i].set(title=f\'Criterion = {c} with {sum(mask)} Outliers\')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n\n[[File:Outliers 3.png|900px]]\n\n\'\'Figure 3: Outliers detected with the Chauvenet Criterion (univariate)\'\'\n\nWe can also show the z value and the y value in one graph with multivariate data.\n\n\n<syntaxhighlight lang=""Python"" line>\ndef multivariate_chauvenet(data_points: np.ndarray, criterion: float = None) -> np.ndarray:\n    # Calculate the mean and covariance matrix of data points\n    mean = np.mean(data_points, axis=0)##determine mean for each column\n    covariance = np.cov(data_points.T)## determine covariance between y and the value z. For this, the data points arrays need to be transposed.\n\n    # Calculate criterion if not provided\n    if criterion is None:\n        criterion = 1 / (2 * len(data_points))\n\n    def calculate_probability(x: np.ndarray) -> float:\n        p = 1 / (2 * np.pi * np.linalg.det(covariance) ** 0.5) * \\\n            np.exp(-0.5 * np.matmul(np.matmul((x - mean), np.linalg.inv(covariance)), (x - mean)))\n        return p ## calculate the probability of a data point X being in a multivariate normal distribution with a given probability density function\n\n    # Calculate probabilities and return the mask\n    probabilities = np.array([calculate_probability(x) for x in data_points])\n    return probabilities < criterion\n\n\nmask = multivariate_chauvenet(data_points=data[[\'y\', \'z\']].values##create mask\nfig, ax = plt.subplots(figsize=(8, 6))##create plot\nfig.suptitle(\'Multivariate Chauvenet Criterion\', size=20)\nsns.scatterplot(data=data, x=\'y\', y=\'z\', hue=np.where(mask, \'No Outlier\', \'Outlier\'), ax=ax)##create plot with colors accroding to status of (no) outlier\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 4.png|900px]]\n\n\'\'Figure 4: Outliers detected with the Chauvenet Criterion (multivariate)\'\'\n\n===Neighbor Based===\nAnother approach to identifying outliers is the so-called Local-Outlier-Factor. It works by measuring the local deviation of a data point with respect to its neighbors, and it compares this deviation to the average deviation of the neighbors. A data point that has a significantly higher deviation than its neighbors is considered to be an outlier. By choosing the number of neighbors to consider, the number of outliers and their positioning within the data distribution is affected (see figure five).\n\n<syntaxhighlight lang=""Python"" line>\nfig, axs = plt.subplots(ncols=3, figsize=(16, 5))## create plot with three sub-plots\nfig.suptitle(\'Neighbor-Based Outlier Detection\', size=20)\nfor i, n in enumerate([5, 10, 20]):## the number determines how many neighbored data points are considered\n    lof = LocalOutlierFactor(n_neighbors=n)\n    mask = lof.fit_predict(data[[\'y\', \'z\']])\n    sns.scatterplot(data=data, x=\'y\', y=\'z\', hue=np.where(mask==1, \'No Outlier\', \'Outlier\'), ax=axs[i])\n    axs[i].set(title=f\'Neighbors = {n} with {sum(mask == -1)} Outliers\')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 5.png|900px]]\n\n\'\'Figure 5: Outliers detected with the neighbor-based method\'\' \n\n==What to do with Outliers==\nThe methods presented provide information about which data points are very likely to be outliers and show strong differences from the majority of the data set. The next step is to decide how to deal with the outliers. There are essentially two different variants for this:', ""== Data distribution ==\n[https://www.youtube.com/watch?v=bPFNxD3Yg6U Data distribution] is the most basic and also a fundamental step of analysis for any given data set. On the other hand, data distribution encompasses the most complex concepts in statistics, thereby including also a diversity of concepts that translates further into many different steps of analysis. Consequently, without [https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/ understanding the basics of data distribution], it is next to impossible to understand any statistics down the road. Data distribution can be seen as the [https://www.statisticshowto.datasciencecentral.com/probability-distribution/ fundamentals], and we shall often return to these when building statistics further.\n\n===The normal distribution===\n[[File:Bell curve deviation.jpg|thumb|500px|left|'''This is an ideal bell curve with the typical deviation in per cent.''' The σ sign (sigma) stands for standard deviation: within the range of -1 to +1 σ you have about 68,2% of your [[Glossary|data]]. Within -2 to +2 σ you have 95,4% of the data and so on.]] \nHow wonderful, it is truly a miracle how almost everything that can be measured seems to be following the normal distribution. Overall, the normal distribution is not only the most abundantly occurring, but also the [https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf earliest distribution] that was known. It follows the premise that most data in any given dataset has its majority around a mean value, and only small amounts of the data are found at the extremes. \n\n'''Most phenomena we can observe follow a normal distribution.''' The fact that many do not want this to be true is I think associated to the fact that it makes us assume that the world is not complex, which is counterintuitive to many. While I believe that the world can be complex, there are many natural laws that explain many phenomena we investigate. The Gaussian [https://www.youtube.com/watch?v=mtbJbDwqWLE normal distribution] is such an example. [https://studiousguy.com/real-life-examples-normal-distribution/ Most things] that can be measured in any sense (length, weight etc.) are normally distributed, meaning that if you measure many different items of the same thing, the data follows a normal distribution. \n\nThe easiest example is [https://statisticsbyjim.com/basics/normal-distribution/ tallness of people]. While there is a gender difference in terms of height, all people that would identify as e.g. females have a certain height. Most have a different height from each other, yet there are almost always many of a mean height, and few very small and few very tall females within a given population. There are of course exceptions, for instance due to selection biases. The members of a professional basketball team would for instance follow a selection [[Bias in statistics|bias]], as these would need to be ideally tall. Within the normal population, people’s height follow the normal distribution. The same holds true for weight, and many other things that can be measured. \n<br/>\n[[File:Gauss Normal Distribution.png|thumb|400px|center|'''Discovered by Gauss, it is only consecutive that you can find the normal distribution even at a 10DM bill.''']]\n\n\n==== Sample size matters ====\n[[File:NormalDistributionSampleSize.png|thumb|500px|right|'''Sample size matters.''' As these five plots show, bigger samples will more likely show a normal distribution.]]\n\nMost things in their natural state follow a normal distribution. If somebody tells you that something is not normally \ndistributed, this person is either very clever or not very clever. A [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/ small sample] can hamper you from finding a normal distribution. '''If you weigh five people you will hardly find a normal distribution, as the sample is obviously too small.''' While it may seem like a magic trick, it is actually true that many phenomena that can be measured will follow the normal distribution, at least when your sample is large enough. Consequently, much of the probabilistic statistics is built on the normal distribution.\n\n\n==== Why some distributions are skewed ====\n[[File:SkewedDistribution.png|thumb|500px|right|'''Data can be skewed.''' These graphs show you how distributions can differ according to mode, median and mean of the displayed data.]]\n\nThe most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.\n\n'''Distributions can have different [https://www.youtube.com/watch?v=XSSRrVMOqlQ skews].''' There is the symmetrical skew which is basically a normal distributions or bell curve that you can see on the picture. But normal distributions can also be skewed to the left or to the right depending on how mode, median and mean differ. For the symmetrical normal distribution they are of course all the same but for the right skewed distribution (mode < median < mean) it's different.\n\n\n==== Detecting the normal distribution ====\n[[File:Car Accidents Barplot 2.jpg|thumb|400px|left|'''This is a time series visualized through barplots.''']]\n[[File:Car Accidents Histogram 2.jpg|thumb|400px|left|'''This is the same data as a histogram.''']]\n[[File:Car Accidents Boxplot 2.jpg|thumb|400px|left|'''And this the data as a boxplot.''' You can see that the data is normally distributed because the whiskers and the quarters have nearly the same length.]]\n'''But when is data normally distributed?''' And how can you recognize it when you have a [[Barplots, Histograms and Boxplots|boxplot]] in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot's whiskers are largely uneven.\n\nYou can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left)."", '\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==Missing values: types and distributions==\n\nMissing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55).\n\nIn this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.\n\n==Types of missing values==\n\nThere are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149).\n\nUnit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).\n\nNext, we will look at how missing values can be distributed and what the implications of such distributions are. Generally, both types of missing values can occur in any distribution.\n\n==Distributions of missing values==\n\nThe distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).\n\n===Missing completely at random===\nMissing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).\n\nThe implications of MCAR for data analysis are relatively straightforward. Because the missing values are completely random, they do not introduce any bias into the analysis. Therefore, it is generally safe to impute the missing values using statistical methods, such as mean imputation or multiple imputations. However, even if the missing values are MCAR, there may still be other factors that can affect the analysis. It is important to consider the number and proportion of missing values (Scheffer 2002, 156). The larger the proportion of missing values in your overall dataset the less reliable is the use of the data. Imagine you had many unit nonresponse missing values across many different individuals, which results in having no variable without any missing value. This might affect the quality of your dataset. If and how this is the case, needs to be decided case by case.\n\n===Missing at random===\nMissing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55). For example, imagine you conduct a survey to analyze the relationship between education and income and there are missing values concerning income. If the missing values depend on education, then these missing values are missing at random. If they would depend on their actual income, they would not.\n\nThe implications of MAR for data analysis are more complex than those for MCAR. Because the missing values are not completely random, they may introduce bias into the analysis if they are not properly accounted for. Therefore, it is important to carefully consider the underlying reasons for the missing data and take these into account when imputing the missing values. One common approach to dealing with MAR missing values is to use regression or other statistical methods to model the relationship between the missing values and the other variables in the dataset (Tsikriktsis 2005, 56). Once the relationship is clear, other methods can be used to approximate to correct the variables for the bias due to the missing values missing at random.\n\n===Missing not at random===\nMissing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).\n\nThe implications of MNAR for data analysis are more complex than those for MCAR or MAR. Because the missing values are systematically related to the true values of the missing data, they can introduce bias into the analysis if they are not properly accounted for. In some cases, this bias may be difficult or impossible to correct, even with advanced statistical methods (Tsikriktsis 2005, 55).\n\n==Determining the randomness of missing data==\n\nThere are two common methods to determine the randomness of missing data. The first method involves forming two groups: one with missing data for a single variable and one with valid values for that variable. If significant differences are found between the two groups regarding their relationship to other variables of interest, it may indicate a non-random missing data process. The second method involves assessing the correlation of missing data for any pair of variables. If low correlations are found between pairs of variables, it may indicate complete randomness in the missing data (MCAR). However, if significant correlations are found between some pairs of variables, it may be necessary to assume that the data are only missing at random (MAR) (Tsikriktsis 2005, 55 - 56).\n\nOverall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. ""Missing data: our view of the state of the art."" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. ""Dealing with missing data."" (2002).\n\nTsikriktsis, Nikos. ""A review of techniques for treating missing data in OM survey research."" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]']"|0.021834061135371178|0.0
16|How can the Shapiro-Wilk test be used in data distribution?|The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.|"['You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n\'\'\'If you would like to know how one can create the diagrams which you see here, this is the R code:\'\'\'\n\n<syntaxhighlight lang=""R"" line>\n\n# If you want some general information about the ""Seatbelt"" dataset, at which we will have look, you can use the ?-function.\n# As ""Seatbelts"" is a dataset in R, you can receive a lot of information here. You can see all datasets available in R by typing data().\n\n?Seatbelts\n     \n# to have a look a the dataset ""Seatbelts"" you can use several commands\n  \n## str() to know what data type ""Seatbelts"" is (e.g. a Time-Series, a matrix, a dataframe...)\nstr(Seatbelts)\n        \n## use show() or just type the name of the dataset (""Seatbelts"") to see the table and all data it\'s containing\nshow(Seatbelts)\n# or\nSeatbelts\n      \n## summary() to have the most crucial information for each variable: minimum/maximum value, median, mean...\nsummary(Seatbelts)\n\n     \n# As you saw when you used the str() function, ""Seatbelts"" is a Time-Series, which makes it hard to work with it. We should change it into a dataframe (as.data.frame()). We will also name the new dataframe ""seat"", which is more handy to work with.\n  \nseat<-as.data.frame(Seatbelts)\n     \n# To choose a single variable of the dataset, we use the \'$\' operator. If we want a barplot with all front drivers,\n# who were killed or seriously injured:\n     \nbarplot(seat$front)\n     \n# For a histogram:\n     \nhist(seat$front)\n  \n## To change the resolution of the histogram, you can use the ""breaks""-argument of the hist-command, which states\n## in how many increments the plot should be divided\n     \nhist(seat$front, breaks = 30)\nhist(seat$front, breaks = 100)\n\n# For a boxplot:\n     \nboxplot(seat$front)\n\n</syntaxhighlight>\n\n==== The QQ-Plot ====\n[[File:Data caterpillar.png|thumb|right|1. Growth of caterpillars in relation to tannin content in food]]\nThe command <syntaxhighlight land = ""R"" inline>qqplot</syntaxhighlight> will return a Quantile-Quantile plot. This plot allows for a visual inspection on how your model residuals behave in relation to a normal distribution. On the y-axis there are the standardised residuals and on the x-axis the theoretical quantiles. The simple answer is, if your data points are on this line you are fine, you have normal errors, and you can stop reading here. If you want to know more about the theory behind this please continue. \nResiduals is the difference of your response variable and the fitted values. \n<br>\n<br>\n\'\'\'Residuals = response variable - fitted values\'\'\'\n<br>\n<br>\nFor a regression analysis this would be the difference of your data points to the regression line. \nThe standardised residuals depend on the model function you are applying.\n\nIn the following example, the standardised residuals are the residuals divided by the standard deviation. Let\'s take the caterpillar data set as an example. On the right you can see the table with the data: growth of caterpillars in relation to tannin content of their diet. Below, we will discuss some correlation plots between these two factors.\n\n[[File:Plot caterpillar.png|thumb|left|2. Plotting the data in an x-y plot already gives you an idea that growth probably depends on the tannin content.]]\n[[File:Qqplot2.png|thumb|right|4. The qqplot for this model looks good. Here the points are mostly on the line with point 4 and point 7 being slightly above and below the line. Still you would consider the residuals in this case to behave normally.]]\n[[File:Plot regression.png|thumb|center|3. Plotted regression line of the regression model \n<syntaxhighlight land = ""R"" inline>lm(growth~tannin)</syntaxhighlight> for testing the relation between two factors]]\n\n[[File:Qqplot notnomral.jpg|thumb|left|5. A gamma distribution, where the variances increases with the square of the mean.]]\n[[File:Qqplot negbinom.jpg|thumb|center|6. A negative binomial distribution that is clearly not following a normal distribution. In other words here the points are not on the line, the visual inspection of this qqplot concludes that your residuals are not normally distributed.]]\n\n===Non-normal distributions===\n\'\'\'Sometimes the world is [https://www.statisticshowto.com/probability-and-statistics/non-normal-distributions/ not normally distributed].\'\'\' At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which [https://www.isixsigma.com/tools-templates/normality/dealing-non-normal-data-strategies-and-tools/ reasons] exists why data is not normally distributed. \n\n==== The Poisson distribution ====\n[[File:Bildschirmfoto 2020-04-08 um 12.05.28.png|thumb|500px|\'\'\'This picture shows you several possible poisson distributions.\'\'\' They differ according to the lambda, the rate parameter.]]\n\n[https://www.youtube.com/watch?v=BbLfV0wOeyc Things that can be counted] are often [https://www.britannica.com/topic/Poisson-distribution not normally distributed], but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. \'\'\'How many people do you think drink one or two cups of coffee per day? Quite many, I guess.\'\'\' How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes.', ""'''In short:''' The (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''ν = n−1'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value μ, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i</sub>'' deviates significantly from a specified value ''μ'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''μ'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''μ.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''σ'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.\n\n==Two-sample t-test==\nWhen wanting to find out whether the means of two samples of a distribution are deviating significantly. If the two samples are independent from each other, an independent two-sample t-test has to be used. If the samples are dependent, which means that the values being tested stem from the same samples or that the two samples are paired, a paired t-test can be used.\n===Independent Samples===\nFor independent samples with similar variances (a maximum ratio of 2), the t-statistic is calculated in the following way:\n[[File:prbdst7.png|700px|frameless|center]]\nwith the estimated pooled standard deviation\n[[File:prbdst8.png|700px|frameless|center]]\nIn accordance with the One-sample t-test, the sample sizes, means and standard deviations of the samples 1 and 2 are denoted by ''n<sub>1/2</sub>'', \u2002''x<sub>1/2</sub>'' and ''σ<sub>x<sub>1/2</sub></sub>'' respectively.\nThe degrees of freedom which are required for conducting the hypothesis testing is given as ''ν = n<sub>1</sub> + n<sub>2</sub> − 2''.\nFor samples with unequal variances, meaning that one sample variance is more than twice as big as the other, Welch’s t-test has to be used, leading to a different t-statistic t and different degrees of freedom ''ν'':\n[[File:prbdst9.png|700px|frameless|center]]\nAn approximation for the degrees of freedom can be calculated using the Welch-Satterthwaite equation:\n[[File:prbdst10.png|700px|frameless|center]]\nIt can be easily shown, that the t-statistic simplifies for equal sample sizes:\n[[File:prbdst11.png|700px|frameless|center]]\n\n===Paired Samples===\nWhen testing whether the means of two paired samples are differing significantly, the t-statistic consists of variables that differ from the ones used in previous tests:\n[[File:prbdst12.png|700px|frameless|center]]\nInstead of the independent means and standard deviations of the samples, new variables are used, that depend on the differences between the variable pairs. ''x<sub>d</sub>'' is given as the average of the differences of the sample pairs and ''σ<sub>D</sub>'' denotes the corresponding standard deviation. The value of ''μ<sub>0</sub>'' is set to zero to test whether the mean of the differences takes on a significant value.\n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n[[Category:Quantitative]]\n\nThe [[Table_of_Contributors| authors]] of this entry is Moritz Wohlstein."", '\'\'\'The type of correlation that you need to do depends on your data distribution.\'\'\'\n\n<imagemap>Image:Statistics Flowchart - Normal Distribution.png|650px|center|\npoly 288 3 154 137 289 269 421 136 [[Data distribution]]\npoly 135 151 1 285 136 417 268 284 268 284  [[Correlations|Pearson]]\npoly 438 152 304 286 439 418 571 285 [[Correlations|Spearman]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it\'s normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>\'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\n\n=== Clear dependencies ===\n\'\'\'Your dependent variable is explained by one at least one independent variable.\'\'\' Is the dependent variable normally distributed?\n<imagemap>Image:Statistics Flowchart - Dependent - Normal Distribution.png|650px|center|\npoly 289 2 152 139 290 267 423 13 [[Data distribution]]\npoly 137 151 0 288 138 416 271 281 [[An_initial_path_towards_statistical_analysis#Normally_distributed_dependent_variable:_Linear_Regression|Linear Regression]]\npoly 441 149 304 286 442 414 575 279 [[An_initial_path_towards_statistical_analysis#Not_normally_distributed_dependent_variable|Non-linear distribution of dependent variable]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it\'s normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>\'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\n\n==== Normally distributed dependent variable: Linear Regression ====\n\'\'\'If your dependent variable(s) is/are normally distributed, you should do a Linear Regression.\'\'\'<br/>\nA linear regression is a linear approach to modelling the relationship between one (simple regression) or more (multiple regression) independent and a dependent variable. It is basically a correlation with causal connections between the correlated variables. Check the entry on [[Regression Analysis]] to learn more.\n\n\'\'\'There may be one exception to a plain linear regression:\'\'\' if you have several predictors (= independent variables), there is one more decision to make:\n\n\n===== Is there a categorical predictor? =====\n\'\'\'You have several predictors (= independent variables) in your dataset.\'\'\' But is (at least) one of them categorical?\n<imagemap>Image:Statistics Flowchart - Categorical predictor.png|650px|center|\npoly 387 1 208 184 385 359 563 183 [[Data formats]]\npoly 180 197 1 380 178 555 356 379 [[An_initial_path_towards_statistical_analysis#ANCOVA|ANCOVA]]\npoly 584 196 405 379 582 554 760 378 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Multiple Regression]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. Pay attention to the data format of your independent variable(s). \'\'integer\'\' and \'\'numeric\'\' data is not \'\'categorical\'\', while \'\'factorial\'\', \'\'ordinal\'\' and \'\'character\'\' data is categorical.\n\n\n===== ANCOVA =====\n\'\'\'If you have at least one categorical predictor, you should do an ANCOVA\'\'\'. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more.\n\n\n==== Not normally distributed dependent variable ====\n\'\'\'The dependent variable(s) is/are not normally distributed.\'\'\' Which kind of distribution does it show, then? For both Binomial and Poisson distributions, your next step is the Generalised Linear Model. However, it is important that you select the proper distribution type in the GLM.\n<imagemap>Image:Statistics Flowchart - Dependent - Distribution type.png|650px|center|\npoly 290 4 154 140 288 269 423 138 [[Data distribution]]\npoly 138 151 2 287 136 416 270 284 271 285 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\npoly 440 152 304 288 438 417 572 285 573 286 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n\'\'\'You have arrived at a Generalised Linear Model (GLM).\'\'\' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>']"|0.004273504273504274|1.0
17|Why is the Delphi method chosen over traditional forecasting methods?|The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.|"['The researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. \'\'\'The responses from the respective previous round were re-distributed to the participants\'\'\' (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again. \n\nAfter the third round, the participants were additionally asked to fill out survey questions on a 1-5 [[Likert Scale]] about how reliable their considered their own forecasts, and how much attention they had paid to the others\' forecasts and comments when these were re-distributed, both regarding each variable individually. This was done to better understand each individual\'s thought process.\n<br>\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 results.png|800px|thumb|center|\'\'\'The results for the Delphi process.\'\'\' It shows that the mean estimates of the group became better over time, and were most often quite close to the actual realisation. Source: Kauko & Palmroos 2014, p.326.]]\n<br>\nThe forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. \'\'\'They found that the individual responses did indeed converge over the Delphi process, and that the ""Delphi group improved between rounds 1 and 3 in 13 of the questions.""\'\'\' (p.320). They also found that ""[d]isagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement"" (p.322) and that the Delphi process ""clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future"", which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that ""paying attention to each others\' answers made the forecasts more accurate"" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.\n\n\n== Key Publications ==\n* Linstone, H. Turoff, M. 1975. \'\'The Delphi Method: Techniques and Applications\'\'. Addison-Wesley, Boston.\nAn extensive description of the characteristics, history, pitfalls and philosophy behind the Delphi method.\n* Dalkey, N. Helmer, O. 1963. An experimental application of the Delphi method to the use of experts. Management Science 9(3). 458-467.\nThe original document illustrating the first usage of the \'\'Delphi\'\' method at RAND.\n* Gordon, T.J. Helmer, O. 1964. Report on a long-range forecasting study. RAND document P-2982.\nThe report that popularized \'\'Delphi\'\' outside of the military defense field.\n\n\n== References ==\n* (1) Dalkey, N. Helmer, O. 1963. \'\'An experimental application of the Delphi method to the use of experts.\'\' Management Science 9(3). 458-467.\n* (2) Rayens, M.K. Hahn, E.J. 2000. \'\'Building Consensus Using the Policy Delphi Method\'\'. Policy, Politics, & Nursing Practice 1(4). 308-315.\n* (3) Okoli, C. Pawlowski, S.D. 2004. \'\'The Delphi Method as a Research Tool: An Example, Design Considerations and Applications.\'\' Information & Management 42(1). 15-29.\n* (4) Linstone, H. Turoff, M. 1975. \'\'The Delphi Method: Techniques and Applications\'\'. Addison-Wesley, Boston.\n* (5) Gordon, T.J. 2009. \'\'The Delphi Method.\'\' Futures Research Methodology V 3.0.\n* (6) Hallowell, M.R. Gambatese, J.A. 2010. \'\'Qualitative Research: Application of the Delphi method to CEM Research\'\'. Journal of Construction Engineering and Management 136(1). 99-107.\n* (7) Boulkedid et al. 2011. \'\'Using and Reporting the Delphi Method for Selecting Healthcare Quality Indicators: A Systematic Review.\'\' PLoS ONE 6(6). 1-9.\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.', 'Next, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group\'s positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be ""(...) a highly motivating experience for participants"" (Rayens & Hahn 2000, p.309) due to the feedback on the group\'s opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which ""(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one\'s mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others."" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)\n\nHowever, several potential pitfalls and challenges may arise during the Delphi process:\n* Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue (4, 5, 6).\n* A specific Delphi format that was useful in one study must not work as well in another context. Instead, the process must be adapted to the research design and underlying problem (4).\n* The proper selection of participating experts constitutes a major challenge for Delphi processes (3, 4, 5, 6). In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors (4).\n* The monitor (= researcher) must not impose their own preconceptions upon the respondents when developing the questionnaire but be open for contributions from the participants. The questions should be concise and understandable and should not incentivise the participant to ""get the job over with"" (Linstone & Turoff 1975, p.568; 5).\n* Diverse forms of [[Glossary|bias]] might occur on the part of the participants that need to be anticipated by the researcher. These include discount of the future, over-optimism / over-pessimism, misinterpretations with regard to the complexity and uncertainty involved in forecasting the future as well as other forms of bias that may be imposed through the feedback process (4, 6).\n* The responses must be adequately summarized, analyzed and presented to the participants (see the variety of measures for \'consensus\' in What the method does)). ""Agreement about a recommendation, future event, or potential decision does not disclose whether the individuals agreeing did so for the same underlying reasons. Failure to pursue these reasons can lead to dangerously false results."" (Linstone & Turoff 1975, p.568).\n* Disagreements between participants should be explored instead of being ignored so that the final consensus is not artificial (4).\n* The participants should be recompensated for their demanding task (4)\n\n\n== Normativity ==\n==== Connectedness / nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).\n\n==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the \'real-world\'.\n\n\n== Outlook ==\n==== Open questions ====\n* The diverse fields in which the Delphi method was applied has diversified and thus potentially confounded its methodological homogeneity, raising the need for a more comparable application and reporting of the method (6, 7)\n\n\n== An exemplary study ==\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 title.png|600px|frameless|center|The title of the exemplary study for Delphi method. Source: kauko & Palmroos 2014]]\nIn their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. \'\'\'The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets.\'\'\' Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks\' foreign net assets etc.) with simple point estimates. These variables were chosen to ""fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible."" (p.316). The participants were provided with information on the past developments of each of these variables.', '[[File:ConceptDelphi.png|450px|left|frameless|[[Sustainability Methods:About|Method categorization for Delphi]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Individual|Individual]] || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| [[:Category:Present|Present]] || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/>\n<br/>\n\'\'\'In short:\'\'\' The Delphi Method is an interactive form of data gathering in which expert opinions are summarized and consensus is facilitated.\n\n== Background ==\nThe Delphi method originates from work at RAND Corporation, a US think-tank that advises the US military, in the late 1940s and 1950s (2, 3, 5). RAND developed ""Project Delphi"" as a mean of obtaining ""(...) the most reliable consensus of opinion of a group of experts."" (Dalkey & Helmer 1963, p.1). At the time, the alternative - extensive gathering and analysis of quantitative data as a basis for forecasting and deliberating on future issues - was not technologically feasible (4, 5). Instead, experts were invited and asked for their opinions - and Delphi was born (see (1)).\n\n[[File:Delphi Method SCOPUS.png|400px|thumb|right|\'\'\'SCOPUS hits for the Delphi method until 2019.\'\'\' Search terms: \'Delphi Panel\', \'Delphi Method\', \'Delphi Methodology\', \'Delphi Study\', \'Delphi Survey\' in Title, Abstract, Keywords. Source: own.]]\n\nIn 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).\n\n\n== What the method does ==\nThe Delphi method is ""(...) a systematic and interactive research technique for obtaining the judgment of a panel of independent experts on a specific topics"" (Hallowell & Gambatese 2010, p.99). It is used ""(...) to obtain, exchange, and develop informed opinion on a particular topic"" and shall provide ""(...) a constructive forum in which consensus may occur"" (Rayens & Hahn 2000, p.309). Put simply, experts on a topic are gathered and asked in a systematic process what they think about the future, until consensus is found. \n\n==== The Delphi procedure ====\n[[File:ResultVisualisationDelphi.png|400px|thumb|right|Questionnaire results for the original RAND study, asking for an estimate of bomb requirements. The estimated numbers per participant converge over the course of the Delphi procedure. Source: Dalkey & Helmer 1963, p.15]]\n\n\'\'\'A Delphi process typically undergoes four phases\'\'\' (see (4), (6)):\n\n1. A group of experts / stakeholders on a specific issue is identified and invited as participants for the Delphi. These participants represent different backgrounds: academics, government and non-government officials as wel, as practitioners. They should have a diverse set of perspectives and profound knowledge on the discussed issues. They may be grouped based on their organizations, skills, disciplines or qualifications (3). Their number typically ranges from 10 up to 30, depending on the complexity of the issue (2, 3, 5, 6). \n\nThe researchers then develop a questionnaire. It is informed by previous research as well as input from external experts (not the participants) who are asked to contribute knowledge and potential questions on the pertinent issue (2, 5). The amount of [[Glossary|consultation]] depends on the expertise of the researchers on the respective issue (2).\n\n2. The questionnaire is used to ask for the participants\' opinions and positions related to the underlying issue. The questions often take a \'ranking-type\' (3): they ask about the likelihood of potential future situations, the desirability of certain goals, the importance of specific issues and the feasibility of potential policy options. Participants may be asked to rank the answer options, e.g. from least to most desirable, least to most feasible etc. (2). Participants may also be asked yes/no questions, or to provide an estimate as a number. They can provide further information on their answers in written form. (8)\n\nThe questioning is most commonly conducted in form of a questionnaire but has more recently also been realized as individual, group, phone or digital interview sessions (2, 5). Digital questioning allows for real-time assessments of the answers and thus a quicker process. However, a step-by-step procedure provides more time for the researchers to analyze the responses (4).\n\n3. After the first round, the participants\' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. \'Consensus\' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).\n\nNext, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group\'s positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).']"|0.005263157894736842|0.5
18|What is the main goal of Sustainability Science and what are the challenges it faces?|The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.|"[""== Knowledge for action-oriented sustainability science ==\nAt the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and context-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science:\n[[File:Types of knowledge for action-oriented sustainability science.png|900px|thumb|center|'''Types of knowledge for action-oriented sustainability science.''' Source: Caniglia et al. 2020, p.4]]\n\nThis showcases that this knowledge - and more importantly - the perspective from a philosophy-of-science viewpoint is only starting to emerge, and much more work will be needed until our methodological canon and the knowledge we want to produce enable us to solve the problems we are facing, but also to create these solution in ways that are closer to a mode how we want to create these solutions. We may well be able to solve certain things, and to produce knowledge that can be seen as solutions. I would however argue, that it also matters how we create these solutions and how we create knowledge. Only if people are empowered and society and science work seamlessly together - with ethical restrictions and guidelines in place, of course - will we not only produce the knowledge needed, but we also produce it in a way how we should as scientists. '''Science is often disconnected and even arrogant, and building an educational system that is reflexive and interconnected will be maybe the largest challenge we face.''' This is why we give you these criteria here, because I think that we need to consider what further design criteria can be in order to enhance and diversify our conceptual thinking about the scientific methodological canon. Plurality on scientific methods will necessarily mean to evolve, and in this age of interconnectedness, our journey is only beginning.\n\n== Interaction with stakeholders == \nScientific methods can engage with non-scientific actors on diverse levels, depending on the extent of their involvement in the process of scientific inquiry. Interaction with stakeholder may be especially relevant in [[Transdisciplinarity|transdisciplinary]] research. \n<br/>'''Here, we refer to four levels of interaction:'''\n* ''Information'': Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation.\n* ''Consultation'': A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process.\n* ''Collaboration'': Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue.\n* ''Empowerment'': The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.\n\nYou can find more on these four categories in Brandt et al 2013, where a general introduction to the research landscape of transdisciplinary research in sustainability science is given. More will follow later on such approaches, and so much more still has to follow in science overall, since the declared distinction of science being in an ivory tower is only slowly crumbling. We need to question this [[Levels of Theory|paradigm]], and [[Questioning the status quo in methods|be critical of the status quo of normal science]]. More knowledge is needed, and especially, different knowledge.\n\n== References ==\n* Wiek et al. 2011. ''Key competencies in sustainability: a reference framework for academic program development''. Sustainability Science 6. 203-218.\n* Caniglia, G., Luederitz, C., von Wirth, T., Fazey, I., Martín-López, B., Hondrila, K., König, A., von Wehrden, H., Schäpke, N.A., Laubichler, M.D. and Lang, D.J., 2020. ''A pluralistic and integrated approach to action-oriented knowledge for sustainability.'' Nature Sustainability, pp.1-8.\n* Brandt, P., Ernst, A., Gralla, F., Luederitz, C., Lang, D.J., Newig, J., Reinert, F., Abson, D.J. and Von Wehrden, H., 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological economics, 92, pp.1-15.\n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz."", ""The course '''Methods of Environmental Sciences''' covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective. \n\n__TOC__\n<br/>\n==== Definition & History of Methods ====\nWithin this lecture we deeply focus on the formation of a new arena in science that is including not only system knowledge, but also normative knowledge as well as transformative knowledge. In order to create solution for the problems we currently face, a solution orientated agenda is necessary.  This may demand the creation of novel methodological pathways to knowledge creation. Here, we give a tentative overview on the developments up until now.\n* [[History of Methods in Sustainability Science]]\n\n==== Design Criteria of Methods in Sustainability Science ====\nThere are several design criteria that allow you to systematise methods. Many of these criteria are part of the “usual suspects” in normal science ''sensu strictu'' Kuhn. Here, we discuss further design criteria and knowledge types that can be relevant to systematise knowledge production through methods for sustainability science.\n* [[Design Criteria of Methods in Sustainability Science]]\n\n==== [[Thought Experiments]] & [[Legal Research]] ====\n\n==== Quantitative Methods in the Humanities ====\n* [[Causality and correlation]]\n\n==== [[Geographical Information Systems]] ====\n\n==== [[Grounded Theory]] ====\n\n==== Interviews ====\n* [[Semi-structured Interview]]<br/>\n* [[Open Interview]]\n\n==== The ecological experiment ====\n* [[Experiments and Hypothesis Testing]]\n\n==== Causal Loop Diagrams ====\n* [[System Thinking & Causal Loop Diagrams]]\n\n==== Questioning the status quo in method-driven research ====\n* [[Questioning the status quo in methods]]\n\n==== [[Social Network Analysis]] ====\n\n==== Meta-Analysis ====\n* [[Meta-Analysis]]\n* [[Systematic Literature Review]]\n\n==== Mixed Methods in transdisciplinary research ====\n* [[Transdisciplinarity]]\n* [[Visioning & Backcasting]]\n* [[Scenario Planning]]\n* [[Living Labs & Real World Laboratories]]\n\n----\n[[Category: Courses]]"", 'It can however be stated that Visioning and Backcasting may provide essential benefits for Sustainability research in the future. Sustainability Science has been framed to aim for System, Target and Transformation Knowledge and targets real-world problems and their solutions in integrated, participatory modes (see Brandt et al. 2013. \'\'A review of transdisciplinary research in sustainability science\'\'). To this end, a Visioning process builds on system knowledge, and aims for target knowledge, while Backcasting provides transformation knowledge, all in transdisciplinary environments.\n\n\n== An exemplary study ==\n[[File:Visioning exemplary study title.png|450px|frameless|center|The title of Iwaniec & Wiek 2014]]\nIn their 2014 study, \'\'\'Iwaniec & Wiek present the process and results of a case study from Phoenix, Arizona, USA.\'\'\' Here, the City\'s Planning Manager approached Arizona State University, which resulted in new planning framework for sustainable urban planning and related educational outcomes. Part of this collaboration was a Visioning workshop, in which a sustainability vision for Phoenix in 2050 was set to be developed with public participation, and later be incorporated in the city\'s planning processes.\n\n\'\'\'The Visioning process followed six stages\'\'\' (p.546):<br>\n\'\'1) Framing the process\'\'<br>\nIn the first step, the temporal, spatial, methodological and thematical scope of the visioning process was determined by the legislative requirements and demands of the City of Phoenix Planning Department. They wanted to focus on the pillars Environment, Community, Economy and Infrastructure as well as the year 2050, decided for public participation in the process, and the visioning procedure was intended to become a new paradigm of sustainability-oriented city planning.\n\n[[File:Visioning Example.png|400px|thumb|right|\'\'\'The identified main vision elements\'\'\' for the whole city arranged in the four domains \'\'Environment\'\', \'\'Community\'\', \'\'Economic\'\' and \'\'Infrastructure\'\' according to Step 1. More central elements were more highly prioritized in the initial participatory meetings. Source: Iwaniec & Wiek 2014, p.554.]]\n\n\'\'2) Creating visiong statements and priorities\'\'<br>\nIn 30 small participatory meetings (two in every of the 15 city villages of Phoenix), 13-40 individuals each were introduced to the visioning process, and asked for vision statements, i.e. answers to the question ""Imagine Phoenix as the best it can be in 2050 - What do you see?"". The created statements (759 unique statements across the meetings) were prioritized in a voting activity, which resulted in 15 lists of prioritized vision statements.\n\n\'\'3) Analyzing the vision drafts\'\'<br>\nIn the next step, [[Content Analysis]] was used to deconstruct all vision statements into a standardized element (what should be there in 2050?) and a value proposition (how should it be?) each, which additional descriptive codes ""to specify actors\' role, impact, location and spatial scale."" (p.548). All identified \'vision elements\' (1717 in total) then were analyzed using [[System Analysis]], [[Cluster Analysis]], [[Network Analysis]] and further statistical measures. This was done to assess for each village how the elements relate to each other, if there are missing elements, and if elements are contradictory, as well as across the villages to compare the different visions and to see how consistent the overall city vision was. In the end, topical subsystems for the city were extracted, and an initial narrative for the city vision was created based on the results, which was visualized in form of maps for each village and the whole city. The process in step 3 was iterative and a cooperation of the city planners and the researchers.\n\n[[File:Visioning exemplary study result 2.png|400px|thumb|right|\'\'\'The revised narrative for subsystem \'mass transit\', with images and a story ""In the life of a Phoenician..."".\'\'\' Source: Iwaniec & Wiek 2014, p.561]]\n\'\'4) Reviewing and revising the drafts\'\'<br>\nThe fourth phase consisted of a workshop, to which all participants of the 30 smaller meetings were invited, and additional citizens, with a focus on inclusive participation (Hispanic, youth). In a plenary session, all participants were introduced to the results of steps 2-3, i.e. the preliminary vision in form of maps, visuals and a narrative. Then, everyone was distributed into 12 groups of 9-10 participants, with two groups working on one of six urban subsystems each. These subsystems were: Open Governance, Enhanced Roads/ Highways, Responsible Water use, Comprehensive Mass Transit, Lots of Open Space, Dense Urban Cores, Abundant Vegetation. For each subsystem, vision elements were physically presented to the group on gameboards, with replaceable arrows indicating relationships between these elements.\n\nThen, each participant was asked to illustrate all vision elements, and the group created a collage and narrative for their respective subsystem. The physical systemic representation of the subsystem was discussed and adjusted by the group, and vision elements were reprioritized. The new version of the subsystem vision was again discussed, also in terms of its sustainability concerning resources, environmental impact, and social justice. In the end, a concluding narrative for the subsystem was developed.\n\nIn the final plenum, all subsystem narratives were presented, leading to a range of results for the whole city visioning: ""illustrative collages, various photo-documented game-board configurations and descriptions and the final narrations of the visions."" (p550). Post-workshop feedback was also gathered.\n\n\'\'5) Finalizing the vision\'\'<br>\nNow that the initial vision elements (Step 2) and the results of their systemic analysis (Step 3) were revised (Step 4), the ""revised vision subsystems were reintegrated into a new system map of the city vision."" (p.550) All visioning results so far were compiled and analyzed again, and a final report was created that ""represented the vision for Phoenix 2050 as a systemic, coherent and sustainable city vision."" (p.551). The main results of this report found their way into the city\'s planning documents.\n\n\'\'6) Final review and dissemination\'\'<br>\nThe plan for the final step in this case study was to have the public vote for or against the document which the newly created vision had become a part of. However, due to political reasons, in the end another visioning process was started which built on the results of the presented first process. However, as the researchers highlight, this is in line with the iterative and adaptive underlying visioning methodology. Also, they state that the presented visioning process already helped build capacity in terms of visioning both for the planners and the public, as suggested by a post-workshop survey.\n\n\n== Key Publications ==\n\'\'\'Theoretical\'\'\'\n* Robinson, J.B. 1982. \'\'Energy backcasting: A proposed method of policy analysis.\'\' Energy Policy 10(4). 337-344.\nExplains the Backcasting process and the role of a vision in this process.\n\n* Constanza, R. 2000. \'\'Visions of alternative (unpredictable) futures and their use in policy analysis.\'\' Conservation Ecology 4(1).\nIllustrates four visions for the year 2100 and outlines how one might choose from these.\n\n* Wiek, A. Iwaniec, D.M. 2014. \'\'Quality criteria for visions and visioning in sustainability science.\'\' Sustainability Science 9. 497–512.\nPresent ten quality criteria and a set of tools and techniques for Visioning, including exemplary publications.']"|0.007407407407407408|0.5555555555555556
19|Why are critical theory and ethics important in modern science?|Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.|"['The course \'\'\'Scientific methods - Different paths to knowledge\'\'\' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br/>\n=== Definition & History of Methods ===\n\'\'\'Epochs of scientific methods\'\'\'\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n\'\'\'Critique of the historical development and our status quo\'\'\'\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n\'\'\'Interaction of scientific methods with philosophy and society\'\'\'\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n\'\'\'Quantitative vs. qualitative\'\'\'\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n\'\'\'Inductive vs. deductive\'\'\'\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n\'\'\'Scales\'\'\'\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n\'\'\'Critical theory\'\'\'\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n\'\'\'The pragmatism of [[Glossary|bias]]\'\'\'\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n\'\'\'Limitations in science\'\'\'\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n\'\'\'The scientific method?\'\'\'\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n\'\'\'Forming hypotheses\'\'\'\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n\'\'\'Limitations of hypothesis\'\'\'\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - ‘causal’ form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]\n\n=== Causality & Correlation ===\n\'\'\'Defining causality\'\'\'\nBuilding on the criteria from David Hume, we define causality through temporal links (""if this - then this""), as well as through similarities and dissimilarities. If A and B cause C, then there must be some characteristic that makes A and B similar, and this similarity causes C. If A causes C, but B does not cause C, then there must be a dissimilarity between A and B. Causal links can be clearly defined, and it is our responsibility as scientists to build on this understanding, and understand its limitations. \n\n\'\'\'Understanding correlations\'\'\' Correlations statistically test the relation between two continuous variables. A relation that - following probability - is not a coincidence but from a statistical standpoint meaningful, can be called a significant correlation. \n\n\'\'\'The difference between causality and correlation\'\'\'\nWith increasing statistical analysis being conducted, we sometimes may find significant correlations that are non-causal. Disentangling causal form correlative relations is deeply normative and needs to acknowledge that we often build science based on deeply constructed ontologies.', ""===Establish postdisciplinary freedom===\nScientific disciplines are a testimony of the oppressive evolving of science out of the [[History of Methods|Enlightenment]], leading to silos of knowledge that we call scientific discipline. '''While it is clear that this minimises the chances of a more holistic knowledge production, scientific disciplines are still necessary from a perspective of depth of knowledge.''' Medicine is a good example where most researchers are highly specialised, because there is hardly any other way to contribute to the continuous evolution of knowledge. We may thus conclude that focus in itself is necessary, and often helpful. There are however also other factors about the existence of scientific disciplines that are important to raise. First of all, scientific disciplines are in a fight about priorities of knowledge and interpretation. Many disciplines claim that their knowledge is indeed of a higher value than the knowledge of the other discipline. It is clear that this notion needs to be rejected once we take a step back and look at the whole picture, since such claims of superiority do not make any sense. Yet from a perspective of [[Bias and Critical Thinking|critical realism]], one could claim that ethics and maybe even philosophy are on a different level, because the can transcend epistemological perspective, and may even create ontological truths. While other disciplines thus vanish in the future, philosophy, and more importantly [[Ethics and Statistics|ethics]], are about our responsibility as researchers, and may thus play a pivotal role. I would propose that statistics could contribute to this end, because statistics is at its heart not disciplinary. Instead, statistics could provide a reflexive link between different domains of knowledge, despite it being almost in an opposite position today, since statistics is often the methodological dogma of many scientific disciplines.\n\n===Clarify the role of theory===\nStatistics today is stuck between a rock and a hard place. Statistics can help to test hypotheses, leading to a accepting or rejection of our questions that are rooted in our theories, making [[:Category:Deductive|deductive]] research often rigid and incremental. At the extreme opposite end, there is the [[:Category:Inductive|inductive]] line of thinking, which claims an open mind independent of theory, yet often still looks at the world through the lens of a theoretical foundation. '''Science builds on theory, yet the same theories can also lock us into a partial view of the world.''' This is not necessarily bad, yet the divide between inductive and deductive approaches has been haunting statistics just as many other branches of science. Some approaches in statistics almost entirely depend on deductive thinking, such as the [[ANOVA]]. Other approaches such as [[Clustering Methods|cluster analysis]] are widely inductive. However, all these different analyses can be used both in inductive and deductive fashion, and indeed they are. No wonder that statistics created great confusion. The ANOVA for example was a breakthrough in psychological research, yet the failure to reproduce many experiments highlights the limitations of the theories that are being pursued. Equal challenges can be raised for ecology, economy, and many other empirical branches of science. Only when we understand that our diverse theories offer mere partial explanations, shall these theories be settled in their proper places.\n\n===Reduce and reflect bias===\n'''[[Bias and Critical Thinking|Bias]] has been haunting research from the very beginning, because all humans are biased.''' Statistics has learned to quantify or even overcome some biases, for instance the one related to sampling or analysis are increasingly tamed. However, there are many more biases out there, and to this day most branches of science only had a rather singular focus on biases. In the future we may pool our knowledge and build on wider experience, and may learn to better reflect our biases, and through transparency and open communication, we may thus reduce them. It seems more than unclear how we will do this, but much is to be gained.\n\n===Allow for comparability===\n'''How can we compare different dimensions of knowledge?''' To give an example, how much worth in coin is courage? Or my future happiness? Can such things be compared, and evaluated? Derek Parfit wrote that we are irrational in the way how we value the distant future less as compared to the presence, even if we take the likelihood of this distant future becoming a reality into account. This phenomenon is called temporal discounting. Humans are strangely incapable of such comparisons, yet statistics have opened a door into a comparability that allows to unravel a new understanding of the comparisons in our head with other comparisons, or in other words, to contextualise our perspectives. Temporal discounting is already today playing less of a role because of teleconnections and global market chains. What would however be more important, is if people gained - through statistics - a deeper insight into their existence compared to everybody else. Such a radical contextualisation of ourselves would surely change our perspective on our role in the world.\n\n===Evolve information theory===\nWhile frequentist statistics evolve around [[A matter of probability|probability]], there are other ways to calculate the value of models. Information theory is - in a nutshell - already focusing on diverse approaches to evaluate information gained through statistical analysis. The shortcoming of [[Simple Statistical Tests|p-values]] have been increasingly moved in the focus during the last one or two decades, yet we are far away from alternative approaches (e.g. AIC) being either established or accepted. Instead, statistics are scattered when it comes to analysis pathways, and model reduction is currently at least in the everyday application of statistics still closer to philosophy than our way of conducting statistics. The 20th century was somewhat reigned by numbers, and probability was what more often than not helped to evaluate the numbers. New approaches are emerging, and probability and other measures may be part of the curriculum of high school students in the future, leaving the more advanced stuff that we have no idea about today to higher education.\n\n\n==The future contribution of statistics to society==\n===Generate societal education===\nIt is highly likely to assume that even advanced statistics may become part of the education of young schoolchildren. After all, today's curriculum is vastly different from what was taught 100 years ago. Statistics could be one stepping stone towards a world with a higher level of reflection, where more and more people can make up their own mind based on the data, and can take reflected decisions based on the information available. '''Inequalities can only be diminished if they are visible, and statistics are one viewpoint that can help to this end, not as a universal answer, but as part of the picture.''' The COVID-19 pandemic has shown how the demand for data, patterns and mechanism went through the roof, and more people got into the data and analysis, and acted accordingly - given that they had the capability. The greatest opportunity of a more dispersed statistical education is surely within other cultures. While Europe and North America are widely governed by knowledge gained from statistics, much could be learned ensuring these approaches with different knowledge from other cultures. We only started to unravel the diversities of knowledge that is out there. Statistics may also be a staple in the future, yet knowledge become more exciting if it is combined with other knowledge."", '[[File:Bildschirmfoto 2020-06-22 um 08.25.14.png|400px|thumb|right|\'\'\'Derek parfit is one of the most famous philosophers of the 20th and early 21st century.\'\'\' His thesis on normative ethics can be seen as his most important work.]]\n\nFollowing Sidgwicks ""Methods of ethics"", ethics can be defined as \'\'the world how it ought to be\'\'. Derek Parfit argued that if philosophy be a mountain, Western philosophy climbs it from three sides: \n* The first side is Utilitarianism, which is widely preoccupied with the question how we can evaluate the outcome of an action. The most ethical choice would be the action that creates the most good for the largest amount of people.\n* The second side is reason, which can be understood as the human capability to reflect what one is ought to do. Kant said much to this end, and Parfit associated it to the individual, or better, the reasonable individual.\n* The third side of the mountain is the social contract, which states that a range of moral obligations are agreed upon in societies, a thought that was strongly developed by Locke. Parfit associated this even wider, referring to the whole society in his triple theory. \n\n\n== Do ethics matter for statistics? ==\nPersonally, I think ethics matters deeply for statistics. Let me try to convince you. \nLooking at the epistemology of statistics, we learned that much of modern civilisation is built on statistical approaches, such as the design and analysis of [[Experiments]] or the [[Correlations|correlation]] of two continuous variables. Statistics propelled much of the exponential growth in our [[Scientific methods and societal paradigms|society]], and Statistics is responsible for many of the problems we currently face through our unsustainable behaviour. After all, Statistics was willing to utilize means and accomplish goals that led us more into the direction of a more unsustainable path. Many would argue now that if statistics were a weapon, itself would not kill. Instead, it would be the human hand that uses it. This is insofar true as statistics would not exist without us, just as weapons were forged by us. However, I would argue that statistics are still deeply [[Normativity of Methods|normative]], as they are associated with our culture, society, social strata, economies and so much more. This is why we should embrace a critical perspective on statistics. Much in our society depends on statistics, and many decisions are taken because of statistics. As we have learned, some of these statistics might be problematic or even wrong, and consequently, this can render the decisions wrong as well. More strangely, our statistics can be correct, but as we have learned, statistics can even then contribute to our downfall, for instance when they contribute to a process that leads to unsustainable production processes. \'\'\'We may calculate something correctly, but the result can be morally wrong.\'\'\' Ideally, our statistics would always be correct, and the moral implications that follow out of our actions that are informed by statistics are also right.\n \n[[File:Bildschirmfoto 2020-06-22 um 08.35.42.png|600px|left|thumb|\'\'\'GDP is a good example of a construct used in statistics which influences our daily life and especially politics to a huge extent.\'\'\']]\n\'\'\'However, statistics is more often than not seen as something that is not normative, and some people consider statistics to create objective knowledge.\'\'\' This is rooted in the deep traditions and norms of the disciplines where statistics are an established methodological approach, and in the history and theory of science that governs our research. Many scientists are regrettably still [[Bias and Critical Thinking|positivists]], and often consider the knowledge they create to be objective. More often than not, this is not a conscious choice, but the combination of unreflected teachers in some education system in general.  Today, obvious moral dilemmas and ambiguities are generally part of complex ethical pre-checks in many study designs, for which medicine provides the gold standard. Here, preventing blunders was established early on, and is now part of the canon of many disciplines, with medicine leading the way. Such problems often deal with questions of sample size, randomisation and the question when a successful treatment should be given to all participants. These are indirect reflections on validity and plausibility within the study design, acknowledging that failures or flaws in these elements may lead to biased or even plain wrong results of the study. \n\nWhat is however widely lacking within the broader debates in science is how the utilisation of statistics can have wider normative consequences, both during the application of statistics, but also due to the consequences that arise out of results that were propelled by statistics. In her book ""Making comparisons count"", Ruth Chang explores one example of such relations, but the gap between ethics and statistics is so large, we might define it altogether as widely undiscovered country. More will need to be done to overcome the gap between ethics and statistics, and a connection is hampered by the fact that both fields are widely unclear in terms of the overall accepted norms. While many statistical textbooks exist, these are more often than not disciplinary, and consolidating the field of ethics this diversity is certainly no small endeavour. Creating connections between statistics and ethics is a challenge,  because there is a long history of flaws in this connection that triggered blurred, if not wrong decisions. We will therefore look at specific examples from both directions, starting with a view on ethics from the perspective of statistics.\n\n== Looking at ethics from statistics ==\n[[File:Food-supply-kcal.png|600px|right|thumb|[https://slides.ourworldindata.org/hunger-and-food-provision/#/kcalcapitaday-by-world-regions-mg-png \'\'\'Our World in Data] provides plenty information\'\'\', for example on how the worldwide food provision changed over the last decades.]]\n\nWhen looking at the relation between ethics and statistics from the perspective of statistics, there are several items which can help us understand their interactions. First and foremost, the concept of validity can be considered to be highly normative. Two extreme lines of thinking come into mind: One technical, orientating on mathematical formulas; and the other line of thinking, being informed actually by the content. Both are normative, but of course there is a large difference between a model being correct in terms of a statistical validation, and a model approximating a reality that is valid. Empirical research makes compromises by looking at pieces of the puzzle of reality. Following [[Bias and Critical Thinking|critical realism]], we may be able to unlock the strata of everything we can observe (the ‘empirical’, as compared to the ‘real’ and the ‘actual’), but ultimately this will always be a subjective perspective. Hence empirical research will always have to compromise as we choose versions of reality in our pursuit for knowledge.']"|0.04639175257731959|1.0
20|What is system thinking?|System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.|"['\'\'\'The system is at the basis of System Thinking.\'\'\' System Thinking is a form of scientific approach to organizing and understanding \'systems\' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of \'holistic\' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that ""(...) all system dynamics are in principle non-linear"" and that ""(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions"" (Haraldsson 2004, p.6).\n\n\'\'\'Peter Checkland introduced the notion that there are two main types of System Thinking:\'\'\' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a \'system\' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a \'system\' an ""(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world"" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a \'system\'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|\'\'\'Hard System Thinking and Soft System Thinking\'\'\' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: \'\'System Analysis\'\' and \'\'System Dynamics\'\' (1).\n\n\'\'System Analysis\'\' ""(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships."" (Haraldsson 2004, p.5). \'\'System Analysis\'\', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].\n\n\'\'System Dynamics\'\', then, focuses on the interaction part of the system. It ""(...) refers to the re-creation of the understanding of a system and its feedbacks. It aims at exploring dynamic responses to changes within or from outside the system. (...) System Dynamics deals with mathematical representation of our mental models and is a secondary step after we have developed our mental model."" (Haraldsson 2004, p.5). System Dynamics, as the name suggests, enables the researcher to observe and measure the behavior of the system. The interactions between the individual elements are not just recreated, but the consequences of these interactions are quantified and assessed.\n\nSystem Thinking allows for a shift in the perception of [[Causality|causality]]. Instead of assuming linear causality (A causes B, B causes C) it allows for the integration of further influencing factors, as well as a more neutral depiction of the system at hand. C may now be seen as a property that [[Agency, Complexity and Emergence|emerges]] from the relation between A and B, instead of perceiving it as a direct consequence of B. Haraldsson (2004, p.21) provides an illustrative example here: ""We start by asking the initial question: ""I want to understand how water flows into the glass and what I do to fill it up."" Instead of looking at the action from an individual point of view, where the ""I am"" is the active part and at the centre of focus, we shift our perception to the structure of the action. The ""I am"" simply becomes a part of the feedback process, not standing apart from it. Suddenly we have shifted out attention to the structure of the behavior and we can observe that the structure is causing the behavior. (...) We have now transformed the traditional linear thinking into a circular argument.""\n\n==== Causal Loop Diagrams ====\nCausal Loop Diagrams (CLDs) are a crucial method for System Analysis since they allow for the modelling of systems. They make it possible ""(...) to \'map\' the complexity of a problem of interest"" (McGlashan et al. 2016, p.2). A CLD allows us to not suppose a linear relationship between system elements, but rather understand cause and effect (1). It makes it possible to ""(...) understand how a behavior has been manifesting itself in a system so we can develop strategies to work with, or counteract the behavior. We also want to know to what extent and how the problem is connected with other \'systems\'"" (Haraldsson 2004, p.20). By developing a CLD, one can visualize and thus better understand the feedback mechanisms that happen in a system (1). Therefore, they enable both a look at the structure as well as the processes in a system. The CLD also helps transfer system understanding (1). CLDs ""(...) provide a language for articulating our understanding of the dynamic, interconnected nature of our world. (...) By stringing together several loops, we can create a coherent story about a particular problem or issue."" (Team TIP 2011, p.1)\n\n\'\'\'A Causal Loop Diagram consists of rather few elements:\'\'\' Variables, Causal Relationships and Polarity.\n\n\'\'\'Variables\'\'\' represent the individual elements of a system, but are adapted to the research intent underlying the creation of the CLD. They are ""(...) dynamic causes or effects of the problem under study"" (McGlashan et al. 2016, p.2). For example, in a CLD that recreates a forest ecosystem, the system element may be the soil, whereas the variable that we use in the CLD would be, for example, the change in the amount of carbon in the soil - if this is of interest for the research.\n\nBetween these variables, the CLD depicts \'\'\'causal relationships\'\'\'. Causal relationships ""(...) are represented by arrows that represent a directed cause from one variable to another"" (McGlashan et al. 2016, p.2). When a number of causal relationships (i.e. arrows) connects two or more variables to a circular structure, a loop constitutes.', ""'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries. \n\n== The problem with System Boundaries ==\nWhile boundaries of some isolated systems as well as simple theoretical models can be rather clearly defined, much attention of the recent literature has actually focussed on the effects that systemic interactions create across distances, which have historically been considered separate (see for example [['https://www.ncdc.noaa.gov/teleconnections/'|'teleconnections']] in atmospheric research). It is now evident that inland watersheds are linked to oceangraphic processes through freshwater and nutrient inputs, although these have historically and disciplinarily been examined separately. Furthermore, the nestedness of systems is another challenge - the idea that smaller systems are contained within, and are the constituent components of, larger systems such as organs within the larger integrated functionality of the human body. Some organs can be removed or replaced, but only in relation to the overall functions they provide and how they fit or match the context of the overall body (i.e, blood type or immune response). Government jurisdictions give other clear examples, where countries may contain several provinces as part of a federal system, which in turn can contain smaller administrative units, creating a multi-level administrative whole. \n\nThe debate often resonates around a void between the global and the local levels of social and ecological [[Glossary|scales]] such as space, jurisdiction, administration and even time (e.g., slow to fast). While the global level of most scales is trivial to define, the local level of a scale is less clear. What becomes clear in the definition process between global and local, is the ambiguity and often arbitrary nature of defining what the reality of the system being examined is. Normative choices have to be made, especially for defining research questions and hypotheses, i.e., what is a part of the specific system, what is not considered to be a part of it, and what is an important element that you want to know about? This has implications for the relationships and outcomes you want to investigate. '''Standardization of system definitions, or system defining [[Glossary|processes]], would also be a useful consensus activity to increase comparability and exchange within system science.''' Entire fields of research have thus emerged around normative understandings of what the optimal system level unit to examine might be, such as rural sociology, landscape ecology, coastal governance or micro-economics. What is evident - and becomes unclear - is the spectrum of definitions and choices between the two ends of a scale, and the degree to which the categorical differences in system boundary definitions are meaningful for how we analyse and interpret their functionality.\n\n[[File:System Boundaries - Farm.png|600px|thumb|center|'''Defining the system boundaries influences the scope and content of analysis'''. Source: [https://www.researchgate.net/publication/323959122_D11_Report_on_resilience_framework_for_EU_agriculture Meuwissen et al. 2018.]]]\n\n== Defining System Boundaries ==\nOut of these challenges to define system boundaries we recognize a clear gap on how to approach a globally coherent definition, which can recognize the wide array of contextual differences in how systems are defined and measured. We need to consider that some systems are either divided from larger or parallel systems, or that smaller systems are nested into other systems. \n\n'''System boundaries can be defined based on a variety of parameters.''' For the simplicity of the argument, we focus on one parameter out of several, yet prefer not to discuss in depth the interaction of different parameters. Many people would define such interactions as [[Agency, Complexity and Emergence|complexity]] within systems thinking, but the examination of system complexity is premised on a coherent understanding of what is in and out of the system being examined. This is an inherent precondition for analysis, which is often discussed or taken as an assumption that is often not clearly defined. For example, spatially explicit parameters are an intuitive aspect of many defining processes, and therefore shape our perceptions of what an appropriate system boundary might be. To use a spatial example: '''a larger city can be divided into smaller neighbourhoods, while neighbourhoods can be defined based on different parameters, such as function, ethnic groups or cultural settings.''' While the definition of 'neighbourhoods' can be also informal, it is also well established within urban planning. There is an evident awareness that system boundaries exist, yet many would not be able to define why two systems differ. It is overall easier to define system boundaries based on spatial parameters, such as structural elements, ecosystems, or the design of a city, while informal parameters such as those based on normative dimensions are less clearly definable. Being initially clear within a research project about the boundaries of a system may help to clarify a ranking of what relevant parameters of the system are in terms of its boundaries, but also considering the variance of these parameters within a system. Basically, some parameters can be dramatically different within a system than outside, have no relevance within a system, or can have only relevance within the system. All [[Glossary|assumption]]s are plausible parameters to serve as system boundaries.\n\n'''Independent of the given parameter, we propose that both the state and the variance can be clear delimiters in defining boundaries between systems.''' When looking at a larger system, it can have an overall average value as well as an overall variance regarding a system parameter. If we would now divide the larger system into two smaller parts, it might be that the two systems have a different average value regarding this parameter. However, two smaller systems could also be different regarding their variance, where one smaller system has an overall large heterogeneity, and the other one a small heterogeneity.\n\nA good example of this could be two neighbourhoods within one city. One could be very heterogeneous in terms of green infrastructure, having many smaller parks, trees, and green rooftops. The other one could be highly homogenous, being widely dominated by houses without any green infrastructure. Clearly, both systems vary both in terms of the average value, as well as the variance. Thus, where boundaries are drawn to define those neighbourhoods in that city will dictate the analysis and conclusions about its variance and values.\n\nAnother example would be two smaller systems, one again being highly heterogeneous, and the other consisting of one half that is a park, and another half that are large apartment houses. Many would opt to divide the second smaller system into two sub-systems. This example illustrates how different parameters allow for a different division of systems according to the habitat recognition. However, the park could be functionally related directly to the apartments, constituting its own unit in an urban planning context. This illustrates how system boundaries depend on the parameters we observe, but also the reasons why we want to measure them and what we want to know and value about the system."", '==== Connectedness ====\n* While Causal Loop Diagrams are a simplified form of [[System Analysis]], the latter involves more diverse approaches.\n* To support the analysis of how systemic variables interact with each other, [[Agent Based Modelling]] may be applied.\n* CLDs may be analyzed using [[Social Network Analysis]] (see 4).\n* System Thinking is an important foundation for any method that views interactions between different agents. This is of relevance to methods that support thinking about and planning for the future, most notably [[Scenario Planning]] as well as [[Visioning & Backcasting]].\n\n\n== Outlook ==\nArnold and Wade (2015) describe the increasing growth of complex systems in our daily lives due to increasingly tied international trade relations, technological advancements and international policy decisions that influence other nations. They claim that \'[s]ystems, if ever they were separated, are indomitably moving towards interconnectedness as we hurtle into a globalized future. (...) Now, more than ever, systems thinkers are needed to prepare for an increasingly complex, globalized, system of systems future in which everything (...) will produce ripple effects throughout the globe."" (p.2)\n\n\n== An exemplary study ==\n[[File:System Thinking - Exemplary Study Hanspach et al. 2014 - Title.png|600px|frameless|center]]\nIn their 2014 publication, Hanspach et al. (see References) present - among other methodological approaches - a Causal Loop Diagram of the social-ecological system in Southern Transylvania. To create it, they conducted \'\'\'stakeholder workshops\'\'\' with ""(...) all relevant ethnic groups, political parties, churches, and schools, as well as local police officers and organizations concerned with nature conservation, regional development, forestry, agriculture, and tourism."" (p.34) First, they held individual workshops with each stakeholder group, in which these were asked to present their ""(...) understandings of changes in the regions"" for the past and the future ""as well as of social-ecological system dynamics and key uncertainties"" (p.35). Based on the insights from these workshops, the researchers drafted causal effect chains and first causal loop diagrams, before \'\'\'they combined all individual results into one single CLD\'\'\', which they refined after receiving feedback in a second set of workshops. They further used the workshop insights to develop scenarios for the region - for more on this, please refer to [[Scenario Planning]].\n\n[[File:Result Visualisation Causal Loop Diagrams.png|700px|thumb|center|\'\'\'Causal loop diagram summarizing the dynamics of the\nregional social-ecological system in Southern Transylvania.\'\'\' R1 refers to the reinforcing feedback loop around local\neconomy, poverty, conflicts, and social capital. Source: Hanspach et al. 2014. p.36]]\n\n\'\'\'As the researchers highlight, this Causal Loop Diagram shows that local stakeholders think about their region that:\'\'\'\n* there is a strong link between the economy and the social capital of a given village\n* the low profitability of small-scale farming stimulates youth emigration and land abandonment\n* there is a negative influence of poor infrastructure on economic conditions\n* the collapse of the communist regime negatively influenced the social capital in the region\n* economic development could lead to short-term financial benefits, but could harm natural resources\n* there is a reinforcing feedback loop around poverty, conflict, low social capital, and poor education (R2 in the diagram), which caused rural emigration\n* the dual processes of farmland intensification in some areas and abandonment in others lead to a decrease in traditional small-scale farming and consequently negatively affect farmland biodiversity, as well as cultural, regulating, and supporting ecosystem services, and\n* forest exploitation for timber and firewood is a threat to forest biodiversity and the ecosystem services provided by forests.\n\nAs this study shows, systematically assessing causalities in a system and taking into account the interconnectedness between elements provides profound insights into system dynamics. Research can thus find further aspects to investigate, and policy and local actors may act according to these insights.\n\n== Key Publications ==\nForrester, J. W. 1961. \'\'Industrial dynamics.\'\' Pegasus Communications, Waltham, MA.\n* The publication that widely introduced system thinking to the world.\n\nHaraldsson, H.V. 2004. \'\'Introduction to System Thinking and Causal Loop Diagrams.\'\' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n* A brief explanation of System Thinking and a dailed description of how to develop a Causal Loop Diagram.\n\nMeadows, D. 2008. \'\'Thinking in Systems. A Primer.\'\' Chelsea Green Publishing, Vermont.\n* An good introduction to the topic in the field of sustainability science.\n\nCheckland, P. Systems Thinking. In: Curry, W.L. Galliers, B. 1999. \'\'Rethinking Managagement Information Systems.\'\' Oxford University Press. 45-56.\n* Representative for the various contributions Checkland made since the 1970s.\n\n\n== References ==\n\n(1) Haraldsson, H.V. 2004. \'\'Introduction to System Thinking and Causal Loop Diagrams.\'\' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n\n(2) Team TIP. 2011. \'\'Guidelines for drawing causal loop diagrams.\'\' Systems Thinker 22(1). 5-7.\n\n(3) Toole, M.T. 2005. \'\'A Project Management Causal Loop Diagram.\'\' ARCOM Conference, London, UK.\n\n(4) McGlashan et al. 2016. \'\'Quantifying a Systems Map: Network Analysis of a Childhood Obesity Causal Loop Diagram.\'\' PLoS ONE 11(10):\n\n(5) Hanspach et al. 2014. \'\'A holistic approach to studying social-ecological systems and its application to Southern Transylvania.\'\' Ecology and Society 19(4). 32-45.\n\n(6) Arnold, R.D. Wade, J.P. 2015. \'\'A Definition of Systems Thinking: A Systems Approach.\'\' 2015 Conference on Systems Engineering Research.\n\n(7) Chu, D. 2011. \'\'Complexity: Against Systems.\'\' Theory in Biosciences. 182-196.\n\n(8) Mitchell, S. 2004. \'\'Why integrative pluralism?\'\' Emergence: Complexity and Organization 1. 1-14.\n\n(9) Zexian, Y. Xuhui, Y. 2010. \'\'A Revolution in the Field of Systems Thinking - A Review of Checkland\'s System Thinking.\'\' Systems Research and Behavioral Science 27. 140-155.\n\n(10) Checkland, P. 2000. \'\'Soft Systems Methodology: A Thirty Year Retrospective\'\'. Systems Research and Behavioral Science 17.11–58.']"|0.11049723756906077|0.0
21|What is the main principle of the Feynman Method?|The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n\'\'Teaching is the best way to learn.\'\'\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# \'\'\'Select the topic you want to learn more about.\'\'\' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don\'t go to broad - focus on a specific topic. You will not be able to explain ""Economics"" or ""Physics"" in one go.\n# \'\'\'Find someone to talk to\'\'\'. Ideally, this person does not know anything about this topic. If you don\'t have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# \'\'\'Make notes.\'\'\' You will quickly realize yourself which parts of the topic you are not able to explain, and/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# \'\'\'Have a look at your notes and try to find more information.\'\'\' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# \'\'\'Now explain the topic again.\'\'\' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https://karrierebibel.de/feynman-methode/ Karrierebibel]\n* [https://blog.doist.com/feynman-technique/ ToDo-ist]\n* [https://www.goodwall.io/blog/feynman-technique/ Goodwall]\n* [https://www.youtube.com/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.', 'Learning a scientific method, any scientific method, takes time. It is like learning an instrument or learning martial arts. Learning a method is not just about a mere internalisation of knowledge, but also about gaining experience. Be patient. Take the time to build this experience. Within this Wiki, I give you a basic introduction of how statistics can help you to create knowledge, and how you can build this experience best.\n\n====Occam\'s razor====\n[[File:William of Ockham.jpg|thumb|left|William of Occam]]\n""Entities should not be multiplied without necessity.""\n\nThe Franciscan friar [https://en.wikipedia.org/wiki/William_of_Ockham William of Occam] almost single-handedly came up with one of the most fundamental principles to date in science. He basically concluded, that ""everything should be as simple as possible, but as complex as necessary."" Being a principle, it is suggested that this thought extends to all. While in his time it was rooted in philosophy or more specifically in logic,  [https://science.howstuffworks.com/innovation/scientific-experiments/occams-razor.htm Occam\'s razor] turned out to propel many scientific fields later on, such as physics, biology, theology, mathematics and many more. It is remarkable how this principle purely rooted in theoretical consideration generated the foundation for the scientific method, which would surface centuries later out of it. It also poses as one of the main building blocks of modern statistics as William of Occam came up with the principle of parsimony. While this is well known in science today, we are up until today busy discussing whether things are simple or complex. Much of the scientific debate up until today is basically a pendulum swing between these two extremes, with some people oversimplifying things, while others basically saying that everything is so complex we may never understand it. Occam\'s razor concludes that the truth is in between.\n\n====The scientific method====\n[[File:Francis Bacon.jpg|thumb|left|Francis Bacon]]\nThe [https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology scientific method] was a true revolution since it enabled science to inductive observations and thus indirectly paved the road towards the testing of hypotheses through observation. Before, science was vastly dominated by theorizing -that is developing theories- yet testing theories proved to be more difficult. While people tended to observe since the dawn of humans, making such observations in a systematic way opened a new world in science. Especially [https://www.britannica.com/science/Baconian-method Francis Bacon] influenced this [https://www.khanacademy.org/humanities/monarchy-enlightenment/baroque-art1/beginners-guide-baroque1/a/francis-bacon-and-the-scientific-revolution major shift], for which he laid the philosophical foundation in his ""Novum Organon"". \n\n\'\'\'All observations are normative, as they are made by people.\'\'\' This means that observations are constructs, where people tend to see things through a specific ""lens"". A good example of such a specific normative perspective is the number zero, which was kind of around for a long time, but only recognised as such in India and Arabia in the 8th-9th century ([https://en.wikipedia.org/wiki/History_of_the_Hindu–Arabic_numeral_system 0]). Today, the 0 seems almost as if it was always there, but in the antique world, there was no certainty whether the 0 is an actual number or not. This illustrates how normative perspectives change and evolve, although not everybody may be aware of such radical developments as [https://www.sciencealert.com/more-than-half-of-americans-could-be-confused-about-arabic-numbers Arabic numbers].\n\n==A very short history of statistics==\nBuilding on Occam\'s razor and the scientific method, a new mode of science emerged. Rigorous observation and the testing of hypotheses became one important building block of our civilisation. One important foundation of statistics was [https://www.youtube.com/watch?v=XQoLVl31ZfQ probability theory], which kind of hit it off during the period known as \'\'[https://en.wikipedia.org/wiki/Age_of_Enlightenment Enlightenment]\'\'. \n\nProbability was important as it enabled differentiation between things happening by chance, or following underlying principles that can be calculated by probability. Of course, probability does not imply that it can be understood why something is not happening by chance, but it is a starting point to get out of a world that is not well understood. Statistics, or more importantly probability, was however not only an important scholar development during the enlightenment, but they also became a necessity.\nOne of the first users of probability was [https://www.britannica.com/science/probability/Risks-expectations-and-fair-contracts Jan de Wit], leader in the Netherlands from 1652 to 1672. He applied the probabilistic theory to determine proper rates of selling annuities. Annuities are payments which are made yearly but back in the days states often collected them during times of war. He stated that annuities and also life insurances should be connected to probability calculations and mortality records in order to determine the perfect charge of payment. \n\nFollowing the [https://www.youtube.com/watch?v=c-WO73Dh7rY Peace of Westphalia], nations were formed at a previously unknown extent, effectively ending the European religious wars. Why is that relevant, you wonder? States need governance, and governance builds at least partly on numbers. Statistics enabled states to get information on demographics and economic development. Double-entry bookkeeping contributed as well. Numbers became a central instrument of control of sovereigns over their nations. People started showing graphs and bar charts to show off their development, something we have not quite recovered from ever since. For example in 1662 [https://en.wikipedia.org/wiki/John_Graunt John Graunt] estimated the population of London by using records on the number of funerals per year, the death rate and the average family size and came to the conclusion that London should have about 384 000 citizens.\nStatisticians were increasingly in demand to account for data, find relations between observations, and basically to find meaning in an increasing plethora of information. The setup and calculation of experiments created another milestone between the two world wars, effectively propelling modern agriculture, [https://www.verywellmind.com/history-of-intelligence-testing-2795581#stanford-binet-intelligence-test psychology], [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1181278/ clinical trials] and many other branches of science.\n[[File:Fisher.jpg|thumb|Ronald Fisher propelled agriculture in the 20th century due to his systemic approach on data analysis. Moreover, his statistical approach found its way in many other scientific fields.]]\n\nHence, statistics became a launchpad for much of the exponential development we observe up until today. While correlation and the Analysis of Variance ([[ANOVA]]) are a bit like the daily bread and butter of statistics, the research focus of the inventor of the ANOVA -[https://www.youtube.com/watch?v=A8gD3CcbDTY Ronald Fisher] - on Eugenics is a testimony that statistics can also be misused or used to create morally questionable or repugnant assumptions. Despite these drawbacks, statistics co-evolved with the rise of computers and became a standard approach in the quantitative branch of science.', '[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Bayesian Inference until 2020.\'\'\' Search terms: \'Bayesian\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.\'\'\' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes\' name only became associated with the method in the 1900s (3).\n\n\'\'\'The family of methods based on the concept of Bayesian analysis has risen the last 50 years\'\'\' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n\'\'\'Bayesian analysis relies on using probability figures as an expression of our beliefs about events.\'\'\' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is ""prior"", and the probability figure assigned to the prior is called ""prior probability"". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called ""posterior probability"", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|\'\'\'The probability distribution for prior, evidence, and posterior.\'\'\']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes\' Theorem.\n\n==== Bayes\' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes\' theorem.\n\n\'\'\'Conditional Probability\'\'\' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n\'\'\'Joint Probability\'\'\', also called ""conjoint probability"", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n\'\'\'Marginal Probability\'\'\' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for ""Bayes theorem"" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n\'\'Caution:\'\' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n\'\'Note:\'\' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let\'s see how this formulation of Bayes\' Theorem can be applied in a simple coin toss example:\n\n\n=== \'\'\'Example I: Classic coin toss\'\'\' ===\n\'\'\'Imagine, you are flipping 2 fair coins.\'\'\' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)\n\n\'\'\'Solution:\'\'\'\nIf we represent the outcome of Heads by a H and the outcome of Tails by a T, then the possible outcomes of the two coins being tossed simultaneously can be written as: `HH`, `HT`, `TH`, `TT`\n\nThe two events A, and B are:\n\nA = the outcome is 2 heads (also called ""prior probability"")\n\nB = one of the outcomes was a head (also called ""marginal probability"")\n\nSo, essentially, the problem can be stated as: what is the probability of getting 2 heads, given that we\'ve already gotten 1 head? This is the posterior probability, which can be represented mathematically as:\n\n    P(2\\ heads|1\\ head)\n\nIn this case, the prior probability is the probability of getting 2 heads. We can see from our representation above that the prior probability is 1/4 as there is only one outcome where there are 2 Heads.']"|0.00510204081632653|1.0
22|What is the difference between fixed and random factors in ANOVA designs?|Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.|"[""model7<-aov(yield~N+K+Error(block),data=npk)\nsummary(model7)\n\n</syntaxhighlight>\n\nOnce we have a minimum adequate model, we might want to check the explained variance as well as the unexplained variance. Within a block experiment we may want to check how much variance is explained on a block level. In a nested experiment, the explained variance among all levels should be preferably checked. \nIn a last step, it could be beneficial to check the residuals across all factor levels. While this is often hampered by a smaller sample size, it might be helpful to understand the behaviour of the model, especially when initial inspection in a boxplot showed flaws or skews in the distribution.\n\n==Fixed effects vs. Random effects==\n[[File:Smoking trooper.jpg|thumb|right|If smoking is a fixed or a random effect depends on the study design]]\nWithin [[ANOVA]] designs, the question whether a variable is a [https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To this end, it is helpful to consult existing studies to differentiate between [https://www.youtube.com/watch?v=Vb0GvznHf8U random and fixed factors]. Current medical trials may consider many variables, and have to take even more random factors into account. Testing the impact of random factors on the raw data is often a first step when looking at initial data, yet this does not help if it is a purely deductive design. In this case, simplified pre-tests are often a first step to make initial attempts to understand the system and also check whether variables - both fixed or random - are feasible and can be utilised in the respective design. Initial pre-tests at such smaller scales are a typical approach in medical research, yet other branches of research reject them as being too unsystematic. Fisher himself championed small sample designs, and we would encourage pre-tests in field experiments if at all possible. Later flaws and errors in the design can be prevented, although form a statistical standpoint the value of such pre-tests may be limited at best.\n\n==Unexplained variance==\n[[File:Karl Popper.jpg|thumb|left|Karl Popper emphasised that facts can only be approximated, i.e. are only considered to be true as long as they are not falsified.]]\n[[File:Ronald Fisher.jpg|thumb|right|Fisher's approach of considering the variance of the real world in statistics can be seen as perfectly complementary to Popper's epistemological reflections.]]\nAcknowledging unexplained variance was a breakthrough in modern science, as we should acknowledge that understanding a phenomena fairly well now is better than understanding something perfectly, but never. In a sense was the statistical developing of uncertainty reflected in philosophical theory, as [https://sustainabilitymethods.org/index.php/Hypothesis_building#Testing_of_hypothesis Karl Popper] highlighted the imperfection of experiments in testing or falsification of theories. Understanding the limitations of the scientific endeavour thus became an important baseline, and the scientific experiments tried partly to take this into account through recognising the limitations of the result. What is however often confused is whether the theory is basically imperfect - hence the results are invalid or implausible - or whether the experiment was conducted in an imperfect sense, making the results unreliable. The imperfection to understand the difference between flaws in theory and flaws in conducting the experiment is a continuous challenge of modern science. When looking at unexplained variance, we always have to consider that our knowledge can be limited through theory and empirical conduct, and these two flaws are not clearly separated. Consequently, unexplained variance remains a blank in our knowledge, and should always be highlighted as such. As much as it is important to acknowledge what we know, it is at least equally important to highlight what we do not know.\n\n==Interpretation of field experiments==\nInterpreting results from field experiments demands experience. First of all, we shall interpret the p-value, and check which treatments and interactions are significant. Here, many researchers argue that we should report the full model, yet I would disagree. P-values in ANOVA summaries differ between the so called full models -which include all predictors- and minimum adequate models -which thrive to be the most parsimonious models. Model reduction is essential, as the changing p-values may make a difference between models that are reporting true results, or flawed probabilities that vaporize once the non-significant terms are subsequently reduced. Therefore, one by one we need to minimize the model in its complexity, and reduce the model until it only contains significant interaction terms as well as the maybe even non-significant single terms, which we have to include if the interaction is significant. This will give us a clear idea which treatments have a significant effect on the dependent variable. \nSecond, when expecting model results we should interpret the sum of squares, thereby evaluating how much of the respective treatment is explain the effect of the dependent variable. While this is partly related to the p-value, it is also important to note how much variance is explained by potential block factors. In addition, it is also important to notice how much remains unexplained in total, as this residual variance indicates how much we do not understand using this experimental approach. This is extremely related to the specific context, and we need to be aware that knowledge of previous studies may aid us in understanding the value of our contribution. \nLastly, we need to take further flaws into our considerations when interpreting results from field experiments. Are there extreme outliers. How do the residuals look like? Is any treatment level showing signs of an uneven distribution or gaps? Do the results seem to be representative? We need to be very critical of our own results, and always consider that the results reflect only a part of reality."", 'Depending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) \'\'\'First of all, is the statistics the right approach to begin with?\'\'\' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem.', 'Designs with multiple factors or \'[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]\' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a \'\'\'[https://www.statisticshowto.com/post-hoc/ posthoc test]\'\'\'. A prominent example is the [https://sciencing.com/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https://www.statisticshowto.com/post-hoc/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.\n\n====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong. \n\nFurthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the deductive world become intertwined, and this poses a risk for the validity of scientific results.\n\nFor more on the Analysis of Variance, please refer to the [[ANOVA]] entry.\n\n==Examples==\n[[File:Guinea pig computer.jpg|thumb|right|The tooth growth of guinea pigs is a good R data set to illustrate how the ANOVA works]]\n===Toothgrowth of guinea pigs===\n<syntaxhighlight lang=""R"" line>\n#To find out, what the ToothGrowth data set is about: ?ToothGrowth\n\n#The code is partly from boxplot help (?boxplot). If you like to know the meaning of the code below, you can look it up there\ndata(ToothGrowth)\n\n# to create a boxplot \nboxplot(len ~ dose, data = ToothGrowth,\n                           boxwex = 0.25, \n                           at = 1:3 - 0.2,\n                           subset = supp == ""VC"", col = ""yellow"",\n                           main = ""Guinea Pigs’ Tooth Growth"",\n                           xlab = ""Vitamin C dose mg"",\n                           ylab = ""tooth length"",\n                           xlim = c(0.5, 3.5), ylim = c(0, 35), yaxs = ""i"")\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n                           boxwex = 0.25,\n                           at = 1:3 + 0.2,\n                           subset = supp == ""OJ"", col = ""orange"")\nlegend(2, 9, c(""Ascorbic acid"", ""Orange juice""),\n       fill = c (""yellow"", ""orange""))\n\n#to apply an ANOVA\nmodel1<-aov(len ~ dose*supp, data = ToothGrowth)\nsummary(model1) \n#Interaction is significant\n</syntaxhighlight>\n\n====Insect sprays====\n[[File:A man using RIPA insecticide to kill bedbugs Wellcome L0032188.jpg|thumb|right|To find out, which insectide works effictively, you can approach an ANOVA]]\n<syntaxhighlight lang=""R"" line>\n#To find out, what the InsectSprays data set is about: ?InsectSprays\ndata(InsectSprays)\nattach(InsectSprays)\ntapply(count, spray, length)\nboxplot(count~spray) \n# can you guess which sprays are effective by looking at the boxplot?\n# to find out which sprays differ significantly without applying many t-tests, you can use a postdoc test\nmodel2<-aov(count~spray)\nTukeyHSD(model2)\n# compare the results to the boxplot if you like\n</syntaxhighlight>\n\n==Balanced vs. unbalanced designs==\nThere is such a thing as a perfect statistical design, and then there is reality.\n\nStatistician often think in so called balanced designs, which indicate that the samples across several levels were sampled with the same intensity. Take three soil types, which were sampled for their agricultural yield in a \'\'mono crop\'\'. Ideally, all soil types should be investigated with the same amount of samples. If we would have three soil types -clay, loam, and sand- we should not sample sand 100 times, and clay only 10 times. If we did, our knowledge about sandy soils would be much higher compared to clay soil. This does not only represent a problem when it comes to the general knowledge, but also creates statistical problems. \n\nFirst of all, the sandy soil would be represented much more in an analysis that does not or cannot take such an unbalanced sampling into account.']"|1.0|1.0
23|What is the replication crisis and how does it affect modern research?|The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.|"[""==Replication of experiments==\n[[File:Bildschirmfoto 2020-05-21 um 17.10.27.png|thumb|One famous example from the discipline of psychology is the Milgram shock experiment carried out by Stanley Milgram a professor from the Yale University in 1963.]]\nField experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for [https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 agriculture] and [https://revisesociology.com/2016/01/17/field-experiments-sociology/ other fields] of production to thrive, but then also did medicine, [https://www.simplypsychology.org/milgram.html psychology], ecology and even [https://www.nature.com/articles/s41599-019-0372-0 economics] use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased. Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam’s razor also when designing a study.\n\n==External Links==\n===Articles===\n\n[https://explorable.com/field-experiments Field Experiments]: A definition\n\n[https://www.tutor2u.net/psychology/reference/field-experiments Field Experiments]: Strengths & Weaknesses\n\n[https://en.wikipedia.org/wiki/Field_experiment#Examples Examples of Field Experiments]: A look into different disciplines\n\n[https://conjointly.com/kb/experimental-design/ Experimental Design]: Why it is important\n\n[https://isps.yale.edu/node/16697 Randomisation]: A detailed explanation\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design Experimental Design in Agricultural Experiments]: Some basics\n\n[https://www.ndsu.edu/faculty/horsley/RCBD.pdf Block Design]: An introduction with some example calculations\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ Difference between crossed & nested factors]: A short article\n\n[https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/ Model reduction]: A helpful article\n\n[https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https://www.simplypsychology.org/milgram.html Field Experiments in Psychology]: A famous example\n\n[https://www.nature.com/articles/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https://revisesociology.com/2016/01/17/field-experiments-sociology/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https://www.youtube.com/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction\n\n[https://www.youtube.com/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."", ""For more details on field experiments, please refer to the entry on [[Field experiments]].\n\n\n== Enter the Natural experiments ==\n[[File:Easter Island.jpg|thumb|right|Although this seems to be a little contradictory here, the impossibility of replication is a problem in the case of the Easter Island.]]\n\nOut of a diverse rooting in discussions about complexity, [https://learningforsustainability.net/systems-thinking/ system thinking] and the need to understand specific contexts more deeply, the classic experimental setting did at some point become more and more challenged. What emerged out of the development of [https://sustainabilitymethods.org/index.php/Interactions#The_field_experiment field experiments] was an almost exact opposite trend considering the reduction of complexity. What do we learn from singular cases? How do we deal with cases that are of pronounced importance, yet cannot be replicated? And what can be inferred from the design of such case studies? A famous example from ethnographic studies is the [http://www.eisp.org/818/ Easter Island]. Why did the people there channel much of their resources into building gigantic statues, thereby bringing their society to the brink of collapse? While this is a surely intriguing question, there are no replicates of the Easter Islands. This is at a first glance a very specific and singular problem, yet it is often considered to be an important example on how unsustainable behaviour led to a collapse of a while civilisation. Such settings are referred to as [https://www.britannica.com/science/natural-experiment Natural Experiments]. From a certain perspective, our whole planet is a Natural Experiment, and it is also from a statistical perspective a problem that we do not have any replicates, besides other ramifications and unclarity that derives such single case studies, which are however often increasingly relevant on a smaller scale as well. With a rise in qualitative methods both in diversity and abundance, and an urge for understanding even complex systems and cases, there is clearly a demand for the integration of knowledge from Natural Experiments. '''From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant, plausible and valid.''' To this end, the concept of the niche in order to illustrate and conceptualise how single cases can still contribute to the production and canon of knowledge.\n\nFor example the [https://academic.oup.com/rcfs/article/4/2/155/1555737#113865691 financial crisis from 2007], where many patterns where comparable to previous crisis, but other factors were different. Hence this crisis is comparable to many previous factors and patterns regarding some layers of information, but also novel and not transferable regarding other dynamics. We did however understand based on the single case of this financial crisis that certain constructs in our financial systems are corrupt if not broken. The contribution to develop the financial world further is hence undeniable, even so far that many people agree that the changes that were being made are certainly not enough. \n\nAnother prominent example of a single case or phenomena is the Covid pandemic that emerges further. While much was learned from previous pandemics, this pandemic is different, evolves different, and creates different ramifications. The impact of our societies and the opportunity to learn from this pandemic is however undeniable. While classical experiments evolve knowledge like pawns in a chess game, moving forward step by step, a crisis such as the Covid pandemic is more like the horse in a chess game, jumping over larger gaps, being less predictable, and certainly harder to master. The evolution of knowledge in an interconnected world often demands a rather singular approach as a starting point. This is especially important in normative sciences, where for instance in conservation biology many researchers approach solutions through singular case studies. Hence the solution orientated agenda of sustainability science emerged to take this into account, and further.\n\n[[File:Lueneburg 2030.jpg|thumb|left|A wonderful example for a bunch of real world experiments is the project Lüneburg 2030+. This map provides an overview of the different experiments.]]\n\nTo this end, [https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 real world experiments] are the latest development in the diversification of the arena of experiments. These types of experiments are currently widely explored in the literature, yet there is no coherent understanding of what real-world experiments are to date in the available literature, yet approaches are emerging. These experiments can however be seen as a continuation of the trend of natural experiments, where a solution orientated agenda tries to generate one or several interventions, the effects of which are tested often within singular cases, but the evaluation criteria are clear before the study was conducted. Most studies to date have defined this with vigour; nevertheless, the development of real-world experiments is only starting to emerge.\n\nFor more details on natural experiments, please refer to the entry on [[Case studies and Natural experiments]].\n\n\n== Experiments up until today ==\n[[File:Experiment Scopus.png|400px|thumb|right|'''SCOPUS hits per year for Experiment until 2020.''' Search terms: 'Experiment' in Title, Abstract, Keywords. Source: own.]]\nDiverse methodological approaches are thus integrated under the umbrella of the term 'experiment'. While simple manipulations such as medical procedures were already known as experiments during the enlightenment, the term 'experiment' gained in importance during the 20th century. Botanical experiments had been conducted long before, but it was the agricultural sciences that evolved the necessary methodological designs together with the suitable [[Bachelor Statistics Lecture|statistical analyses]], creating a statistical revolution that created ripples in numerous scientific fields. The Analysis of Variance ([[ANOVA]]) become the most important statistical approach to this end, allowing for the systematic design of experimental settings, both in the laboratory and in agricultural fields."", 'Another development that emerged during the last decades is the conducting of so called real-world experiments, which are often singular case studies with interventions, yet typically less or no control of variables. These approaches are slowly being developed in diverse branches of research, and allow to open a [[Meta-Analysis|meta-analytical]] dimension, where a high number of case studies is averaged in terms of the research results. The combination of different studies enables a different perspective, yet currently such approaches are either restricted to rigid clinical trials or to meta-analyses with more variables than cases. \n\nReal-world experiments are thus slowly emerging to bridge experimental rigour with the often perceived messiness of the problems we face and how we engage with them as researchers, knowing that one key answer involving these is the joint learning together with stakeholders. This development may allow us to move one step further in current [[System Thinking & Causal Loop Diagrams|systems thinking]], where still many phenomena we cannot explained are simply labeled as complex. We will have to acknowledge in the future which phenomena we may begin to understand in the future, and which phenomena we may never be able to fully understand. [[Non-equilibrium dynamics|Non-equilibrium theory]] is an example where unpredictable dynamics can still be approaches by a scientific theory. Chaos theory is another example, where it is clear that we may not be able to grasp the dynamics we investigate in a statistical sense, yet we may be able to label dynamics as chaotic and allow a better understanding of our own limitations. Complexity is somewhat inbetween, leaning partly towards the explainable, yet also having stakes in the unexplainable dynamics we face. \'\'\'Statistics is thus at a crossroad, since we face the limitations of our approaches, and have to become better in taking these into account.\'\'\' \n\nWithin statistics, new approches are rapidly emerging, yet to date the dominion of scientific disciplines still haunts our ability to apply the most parsimonious model. Instead, the norms of our respective discipline still override our ability to acknowledge not only our limitations, but also the diverse biases we face as statisticians, scientists and as a people. Civil society is often still puzzled how to make sense of our contributions that originate in statistics, and we have to become better in contextualising statistical results, and translate the consequences of these to other people. \'\'\'To date, there is a huge gap between [[Ethics and Statistics|statistics and ethics,]] and the 20th century has proven that a perspective restricted to numbers will not suffice, but instead may contribute to our demise.\'\'\' We need to find ways to not only create statistical results, but also face the responsibility of the consequences of such analyses and interpretations. In the future, more people may be able to approximate knowlegde though statistics, and to be equally able to act based on this knowledge in a reasonable sense, bridging societal demands with our capacity for change. \n\n\n==What was missing==\nEverybody who actively participated in this module now has a glimpse of what statistics is all about. I like to joke that if statistics is like the iceberg that sank the Titanic, then you now have enough ice for a Gin-Tonic, and you should enjoy that. The colleagues I admire for their skills in terms of statistics spent several thousand hours of their life on statistics, some even tens of thousands of hours. By comparison, this module encapsulates about 150 hours, at least according to the overall plan. Therefore, this module focuses on knowledge. It does not include the advanced statistics that demand experience. Questions of models reductions, [[Mixed Effect Models|mixed effect models]], [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|multivariate statistics]] and many other approaches were never touched upon, because this would have been simply too much. \n\n\'\'\'In itself, this whole module is already daring endeavour, and you are very brave that you made it through.\'\'\' We never had such a course when we were students. We learned how to calculate a mean value, or how to make a t-test. That was basically it. Hence this course is designed to be a challenge, but it is also supposed to give you enough of an overview to go on. Deep insights and realisation happen in your head. We gave you a head start, and gave you the tools to go on. \'\'\'Now it is up to you to apply the knowledge you have, to deepen it, transfer it into other contexts and applications, and thus move from knowledge to experience.\'\'\' Repetition and reflection forge true masters. Today, there are still too few people willing to spend enough time on statistics to become truly versatile in this arena of science. If you want to go elsewhere now, fine. You now learned enough to talk to experts in statistics, given that they are willing to talk to you. You gained data literacy. You can build bridges, the problems we face demand that we work in teams, and who knows what the future has in stock for you. \n\nNevertheless, maybe some of you want to go on, moving from apprenticeship to master level. Statistics is still an exciting, emerging arena, and there is much to be learned. One colleague of mine once said about me that I could basically ""smell what a dataset is all about"". I dare you to do better. I am sure that the level of expertise, skill and experience I gained is nothing but a stepping stone to deeper knowledge and more understanding, especially between all of us, regarding the interconnectedness of us all. \'\'\'I hope that all of you find a way how you can contribute best, and maybe some of you want to give statistics a try.\'\'\' If so, then the next section is for you.\n\n\n==How to go on==\n\'\'\'Practise, practise, practise.\'\'\' One path towards gaining experience is by analysing any given dataset I could get my hands on. Granted, in the past this was still possible because the Internet was not overflowing with data, yet still there is surely enough out there to spend time with, and learn from it. The Internet is full of a lot of information on statistics. Not all of it is good, necessarily, yet all is surely worth checking out. After a few hundred hours of doing statistics you will realise that you develop certain instincts. However, in order to get there, I suggest you find some like-minded peers to move and develop together. I had some very patient colleagues/friends who were always there for a fruitful exchange, and we found our way into statistics together. It certainly helped to have a supervisor who was patient and experienced. Yet one of the biggest benefits I had was that I could play with other peoples data. If you become engaged in statistics, people will seek your advise, and will ask you for help when it comes to the analysis of their data. The diversity of datasets was one of the biggest opportunities for learning I had, and this goes on up until today. Having a large diversity of data rooted in the actual experience of other people can be ideal to build experience yourself. \n\nIf I can give you one last advice: There is certainly still a shortage of people having experience in statistics, hence it may be this skill that may allow you to contribute to the bigger picture later in life. What I am however most certain about that there is next to no person that has practical experience in statistics and a deeper understanding about ethics. While this may be the road less taken, I personally think it may be one of the most important ones we ever faced. I might be biased, though. In the end, I started this Wiki not only to overcome this bias, but to invite others on the road less taken.\n\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.']"|0.04938271604938271|1.0
24|What is the purpose and process of the flashlight method in group discussions?|The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || \'\'\'[[:Category:Team Size 30+|30+]]\'\'\'\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> \'\'Is everyone on the same page?\'\'\n<br> \'\'Are there important issues that have been neglected so far?\'\'\n<br> \'\'Is there unspoken dissonance?\'\'\n<br> \'\'Is there an elephant in the room?\'\'\n<br> \'\'What are we actually talking about?\'\'\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== \'\'Please note further\'\' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> \'\'What song would you choose to characterize the current state of discussion, and why?\'\' <br> ...\n\n== Links ==\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/blitzlicht/\n<br> https://www.bpb.de/lernen/formate/methoden/62269/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar Mölleken.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\'\'\'Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique\'\'\' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe \'\'\'Dreamers\'\'\'...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: \'\'Which ideas come to mind? What would be an ideal solution to the problem?\'\'\n\nThe \'\'\'Realists\'\'\'...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: \'\'How does the idea feel? How could it be implemented? Who should do it and at what cost?\'\'\n\nThe \'\'\'Critics\'\'\'...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: \'\'What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?\'\'\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. \'\'\'The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.\'\'\' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to \'neutralize\' their minds) in order to understand the other roles\' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n\'\'Sources:\'\'\n* Tools Hero - [https://www.toolshero.com/creativity/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https://arbeitdigital.de/wirtschaftslexikon/kreativitaetstechniken/walt-disney-methode/ Walt-Disney-Methode]\n* Karrierebibel - [https://karrierebibel.de/disney-methode/ Disney Methode: Denkblockaden überwinden]\n* Impulse - [https://www.impulse.de/management/selbstmanagement-erfolg/walt-disney-methode/3831387.html Walt Disney Methode]\n\n[https://www.youtube.com/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in \'Saving Mr. Banks\', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || \'\'\'[[:Category:Team Size 30+|30+]]\'\'\'\n|}\n\n== What, Why & When ==\nThe World Café is a method for facilitating discussions in big groups. With many participants, discussion rounds tend to be sprawling, slow and dominated by strong speakers. If you want to facilitate a discussion that is more effective, energetic, and inclusive, the World Café is a helpful technique. It divides participants into moderated subgroups, who then wander together through a parcours of stations with different questions, all the while the atmosphere is relaxed and casual like in a café.\n\n== Goals ==\nSplitting big groups into subgroups fosters inclusive, energetic, effective and in-depth discussions: \n* reserved speakers can feel more comfortable speaking in a smaller group\n* the parcours format allows people to physically move through the room in between discussions\n* the moderator can steer the discussion towards unexplored issues with every new subgroup\n* every participant contributed to the collective results in the end\n\n== Getting started ==\nDepending on group size, room capacities and questions you want to discuss, different stations are set up (can be tables, boards, flipcharts etc.) with a moderator who will introduce the question and lead the discussion. The participants will be divided into as many subgroups as there are stations. Each subgroup will visit every station. The moderator welcomes the subgroup participants and introduces the question. Within a given time slot, the subgroups will discuss the question and write down their ideas and insights, before they then wander to the next station. The moderators remain with their station and welcome the next group. They present the question plus a broad overview of the insights of the former group and deepen the discussion with the new group. After the parcours has been completed by all subgroups, the moderators present the collective discussion results of each question to the full group. \n\nIt is helpful to have one moderator who is in charge of the clock and who manages the parcours direction.\n\n== Links & Further reading ==\nhttp://www.theworldcafe.com/\n\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/world-cafe/\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Dagmar Mölleken.']"|0.04081632653061224|1.0
25|What types of data can Generalized Linear Models handle and calculate?|Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n\'\'\'Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.\'\'\' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.', 'Depending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) \'\'\'First of all, is the statistics the right approach to begin with?\'\'\' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem.', '<syntaxhighlight lang=""Python"" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=""Python"" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term ‘residual’ refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=""Python"" line>\nmodel_1 = smf.ols(formula=\'points ~ ID + quanti\', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).\n\nresult = model_1.fit() ## estimates the regression\n\nprint(result.summary()) ## print the result\n</syntaxhighlight>\n\n[[File:new_attempt.png]]\n\nLooking at the Breusch-Pagan test, we can see that we cannot reject the assumption of homoscedasticity. \nConsidering the correlation coefficients, no statistically significant relationship can be identified. The positive relationship between quanti and points can be found again, but it is not statistically significant.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_2 = smf.ols(formula=\'points ~ ID + sex\', data=data)\nresult_bp2 = model_2.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp2.resid, result_bp2.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp2_test[1])\n\n# Set the value for maxlags\nmaxlags = 25  # Update this value as needed\n\nresult = model_2.fit(cov_type=\'HAC\', cov_kwds={\'maxlags\': maxlags})\nprint(result.summary())\n\n# Assuming \'sex\' is a column in the DataFrame named \'data\'\nsex_counts = data[\'sex\'].value_counts()\n\n# Print the frequency table\nprint(sex_counts)\n</syntaxhighlight>\n\n[[File:OLS 2.png]]\n\nBased on the Breusch-Pagan test, the assumption of homoscedasticity needs to be rejected to the 0.1% significance level. Therefore, we correct for heteroscedasticity with HAC (for more details see here)\nLooking at the results, being female (""sex"") has a large negative effect on points and is highly statistically significant. However, looking at the number of females in the dataset, we need to be very cautious to draw any conclusions. Since there are only four females in the dataset (and 73 males), the sample size is considered too small to make any statements about gendered effects on total points achieved. The correlation between ID and points can be ignored, since the last number of the matricle numbers follow no pattern.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_3 = smf.ols(formula=\'points ~ ID + exam\', data=data)\nresult_bp2 = model_3.fit()']"|0.00423728813559322|0.5
26|What is a heatmap and why is it useful?|A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function and \'\'\'mtcars\'\'\' dataset.\nIt is important to note that the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=""R"" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=""R"" line>\ndata(""mtcars"")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=""R"" line>\nfullcolnames <- c(""Miles per Gallon"", ""Number of Cylinders"",\n                  ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                  ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                  ""Number of Gears"", ""Number of Carburetors"")\n</syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = ""column"", labCol = fullcolnames, \n        margins = c(11,5))\n</syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let’s look at the feature <syntaxhighlight lang=""R"" inline>“Number of Carburetors”</syntaxhighlight>. We can see that \'\'\'Maserati Bora\'\'\' has the darkest color, hence it has the largest number of carburetors, followed by \'\'\'Ferrari Dino\'\'\', which has the second-largest number of carburetors. While other models such as \'\'\'Fiat X1-9\'\'\' or \'\'\'Toyota\'\'\' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.\n\n==Explanation of used arguments==\n* <syntaxhighlight lang=""R"" inline>Colv = NA</syntaxhighlight> and <syntaxhighlight lang=""R"" inline>Rowv = NA</syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.\n* <syntaxhighlight lang=""R"" inline>scale = “column”</syntaxhighlight> is used to normalize the columns of the matrix (to absorb the variation between columns). As it was stated previously, normalization is needed due to the algorithm by which the colors are set. Here in our dataset, the values of features “Gross horsepower” and “Displacement” are much larger than the rest. Therefore, without normalization, these two columns will be all marked approximately equally high and all the other columns equally low. Normalizing means that we keep the relative values in each column but not the real numbers. In the interpretation sense it means that, for example, the same color of features “Miles per Gallon” and “Number of Cylinders” of Mazda RX4 does not mean that the actual values are the same or approximately the same (placed in the same bin). It only means that the relative values of each of these cells in corresponding columns are the same or are in the same bin.\n* <syntaxhighlight lang=""R"" inline>margins</syntaxhighlight> is used to fit the columns and rows names into the graph. The reason we used it here is because of the renaming of the columns, which is resulted in longer names that did not fit well by themselves.\n\nColoring options for the heatmap\nThe choice of color for the heatmap is one of the most important aspects of creating an understandable and nice-looking representation of the data. If you do not specify the color (as in the example above) then the default color palette will be applied. However, you can use the argument <syntaxhighlight lang=""R"" inline>col</syntaxhighlight> and choose from a wide variety of palettes for coloring your heatmap.\n\nThere are two options of setting a color palette for the heatmap:\n* First option is to use the palettes from R: <syntaxhighlight lang=""R"" inline>cm.colors()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>heat.colors()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>rainbow()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>terrain.color()</syntaxhighlight>  or <syntaxhighlight lang=""R"" inline>topo.colors()</syntaxhighlight> \n* The second option is to install color palettes packages such as <syntaxhighlight lang=""R"" inline>RColorBrewer</syntaxhighlight>', '\'\'\'Note:\'\'\' This entry revolves specifically around Treemaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br>\n<br>\n\'\'\'In short:\'\'\' A treemap is a rectangle-based visualization method for large, hierarchical data sets. Originally designed to visualize files on a hard drive and developed by Shneiderman and Johnson. They capture two types of information in the data: (1) the value of individual data points; (2) the structure of the hierarchy.\n__TOC__\n<br>\n\n== Definition ==\nTreemaps display hierarchical (tree-structured) [[Glossary|data]]. They are composed of a series of nested rectangles (tiles) whose areas are proportional to the data they represent. Each branch of the tree is given a rectangle, which is then subdivided into smaller rectangles representing sub-branches. The conceptual idea is to break down the data into its constituent parts and quickly identify its large and small components.\n<br>\n<br/>\n[[File:Switzerlandtreemap.png|400px|thumb|right|Fig.1: Switzerland imports in 2017. Source: commons.wikimedia.org]]\n\'\'\'Treemaps are used:\'\'\' <br>\n1. To study data with respect to two quantitative values: <br>\n– positive quantitative value standing for the size of the rectangle (area cannot be negative) and<br>\n– second or categorical quantitative value standing for the color of the individual rectangles.<br>\n2. To display very large amount of hierarchial data in a limited space.<br> \n3. To make a quick, high-level summary of the similarities and differences within one category as well as between multiple categories (not precise comparisons).\n<br>\n<br>\nThe efficient use of physical space and the intelligent color management make treemaps powerful visualization technique applied to a wide variety of domains. They are used to display significant amounts of information in financial, commercial, governmental and similar fields. The treemap on Fig.1 shows Switzerland imports in 2017.\n[[File:Motorbikestreemap.png|300px|thumb|right|Fig.2: Category-wise sales figure for motorbikes. Source: www.fusioncharts.com]]\n\'\'\'Adding new Dimensions.\'\'\' With the intelligent use of colors, new dimensions can be added to the diagram. The usual practice is to use color in different rectangles to indicate a second categorical or quantitative value. If color is used to express a quantitative value, it’s strongly encouraged to use only one color (if all the numbers are positive) or two colors (one for negative and one for positive), and vary the intensity of the color to express precise value.\n<br>\n<br>\nThe following treemap (Fig.2) illustrates the category-wise (Street, Cruiser and etc.) sales figure for motorbikes. The size of the rectangles within each category indicates the relative number of sales. Different colors and color intensities show growth and declines of the motorbike sales. “Static” shows that sales neither grew nor declined. Very intense orange indicates a big shift downward, and very intense green indicates a big shift upwards.\n\nFrom Fig.2 it can be concluded that appropriate use of color enables us to use tree maps to represent losses, declines in sales or other non-positive values. The second quantitative value is not represented by the area of the rectangle.\n<br>\n<br>\nThe way the rectangle is divided and arranged into sub-rectangles depends on \'\'\'the tiling algorithm\'\'\' used.\n<br>\n<br>\nMany tiling algorithms have been developed and here are some of them:\n<br>\n<br>\n\'\'\'Squarified\'\'\' - keeps each rectangle as square as possible. It also tries to order the consecutive elements of the dataset (blocks, tiles) in descending order from the upper left corner to the lower right corner of the graph.\n<br>\n<br>\n\'\'\'Slice and Dice\'\'\' uses parallel lines to divide a root into branches (large rectangles). Then they are subdivided into smaller rectangles representing sub-branches again by using parallel lines. At each level of the hierarchy the orientation of the lines is switched (vertical vs. horizontal).\n\n== R Code ==\nImagine you have book A, consisting of 200 pages, which you use in your statistics course. This book contains of 2 main sections: B (80pages) and C (120pages). B section covers topics of Descriptive Statistics and C section covers topics of Inferential Statistics.\n<br>\n<br>\nTopics of B section are: D(30pages) and E(50pages). D is about sample mean and sample standard deviation while E is about Skewness and Kurtosis.\n<br>\n<br>\nTopics of C section are: F(20pages), G(40pages) and H(60pages). F is about Hypothesis Testing, G covers Confidence Intervals and H focuses on Regression Analysis.\n<br>\n<br>\nYou have tree-structured data and want to make a treemap for displaying the constituent sections of book and make comparisons of its\nsmall and large components.\n[[File:Customtreemap.png|300px|thumb|right|Fig.3]]\n<syntaxhighlight lang=""R"">\n#Fig.3\nlibrary(treemap) \ngroup = c(rep(""B"",2), rep(""C"",3)) \nsubgroup = c(""D"",""E"",""F"",""G"",""H"") \nvalue = c(30,50,20,40,60) \ndata= data.frame(group,subgroup,value) \ntreemap(data,index=c(""group"",""subgroup""),\n        vSize = ""value"",\n        palette = ""Set2"",\n        title=""A"",\n        type=""index"",\n        bg.labels=c(""white""),\n        align.labels=list(c(""center"", ""center""), \n                          c(""right"", ""bottom"")))\n</syntaxhighlight>\n\n==References and further reading material==\n# Ben Shneiderman (1992). “Tree visualization with tree-maps: 2-d space-filling approach”. ACM Transactions on Graphics. 11: 92–99.\n# Ben Shneiderman, April 11, 2006, Discovering Business Intelligence Using Treemap Visualizations, http://www.perceptualedge.com/articles/b-eye/treemaps.pdf\n# https://towardsdatascience.com/treemaps-why-and-how-cfb1e1c863e8\n# https://www.nngroup.com/articles/treemaps/\n# https://www.fusioncharts.com/resources/chart-primers/treemap-chart/\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Shahlo Hasanova.', 'There are two options of setting a color palette for the heatmap:\n* First option is to use the palettes from R: <syntaxhighlight lang=""R"" inline>cm.colors()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>heat.colors()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>rainbow()</syntaxhighlight>, <syntaxhighlight lang=""R"" inline>terrain.color()</syntaxhighlight>  or <syntaxhighlight lang=""R"" inline>topo.colors()</syntaxhighlight> \n* The second option is to install color palettes packages such as <syntaxhighlight lang=""R"" inline>RColorBrewer</syntaxhighlight> \n\n==Additional materials==\n* [https://www.r-graph-gallery.com/heatmap Other functions for building a heatmap]\n* [https://www.datanovia.com/en/blog/how-to-normalize-and-standardize-data-in-r-for-great-heatmap-visualization/ How and why we should normalize data for a heatmap]\n* [https://vwo.com/blog/heatmap-colors/ How to choose the color palette for a heatmap]\n* [https://blog.bioturing.com/2018/09/24/heatmap-color-scale/ Do\'s and Dont\'s in choosing a color palette for a heatmap]\n* [https://www.displayr.com/what-is-dendrogram/ What is a dendrogram]\n* [https://sustainabilitymethods.org/index.php/Clustering_Methods More about clustering methods and how to build a dendrogram in R]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Evgeniya Chetneva.']"|0.09278350515463918|0.5
27|How did Alhazen contribute to the development of scientific methods?|Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.|"['Viele konkrete Schritte brachten uns der konkreten Anwendung wissenschaftlicher Methoden näher, darunter - insbesondere - der Ansatz des kontrollierten Testens durch den arabischen Mathematiker und Astronomen [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]). Aus der Mathematik der Antike hervorgegangen und diese mit der allgemein aufkommenden Erforschung der Physik verbindend, war Alhazen der erste, der [[Field experiments|Versuchsbedingungen]] in einem systematischen Sinne manipulierte und damit den Weg zu der wissenschaftlichen Methode ebnete, die Jahrhunderte später aufkommen sollte. Alhazen ist auch deshalb relevant, weil er als Universalgelehrter betrachtet werden kann, was den Aufstieg von mehr Wissen unterstreicht, das solche Charaktere ermöglichte, aber immer noch zu weit von der wahren Bildung des vielfältigen Kanons der [[Design Criteria of Methods|Designkriterien für Methoden]] wissenschaftlichen Disziplinen entfernt ist, die ihn wahrscheinlich als Experten auf dem einen oder anderen Gebiet begrüßt hätten. Natürlich steht Alhazen hier nur als einer von vielen, die den Aufstieg der Wissenschaft über \'\'\'die islamische Welt im Mittelalter, die als Wiege der westlichen Wissenschaft angesehen werden kann, und auch als eine Kontinuität von den Eskapaden, als in Europa viel vom unmittelbaren griechischen und römischen Erbe verloren ging\'\'.\n\n==== Vor der Aufklärung - \'\'Messen und Lösen\'\' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|\'\'\'Die Mercator-Weltkarte.\'\'\' Quelle: [https://de.wikipedia.org/wiki/Mercator-Projektion Wikipedia]]] \n\n\'\'\'Ein weiterer Durchbruch, der auch in der Geometrie wurzelte, war die Erstellung von frühen Handelskarten.\'\'\' Während viele europäische Karten zwar detailliert, aber sicher nicht maßstabsgetreu waren (Ebstorfer Weltkarte), ermöglichten frühe Karten dennoch einen überregionalen Handel. Der wirkliche Durchbruch war die [[Geographical Information Systems|Mercator-Karte]], die - beschränkt auf das damalige Wissen - die erste Karte war, die eine klare Navigation über die Ozeane ermöglichte und damit Kolonialismus und westliche (Gewalt-)Herrschaft ermöglichte. Dies ist insofern von hoher methodischer Relevanz, als man argumentieren kann, dass der Überschuss aus den Kolonien und fernen Ländern eine der Haupttriebkräfte für das Gedeihen der europäischen Kolonialherren, ihrer Wirtschaft und damit auch ihrer Wissenschaft war. Es kann eine direkte Verbindung zwischen der [[Normativity of Methods|Normativität der Methoden]], die durch den Kolonialismus verstärkt wurde, und dem Gedeihen der westlichen Wissenschaft, einschließlich der Entwicklung wissenschaftlicher Methoden, hergestellt werden.\n\n[[File:printing-press-2.jpg|300px|thumb|left|\'\'\' Die Erfindung des Buchdrucks\'\'\'. Quelle: [[https://www.ecosia.org/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \n\nMit einer steigenden Zahl von Texten, die durch die [https://www.britannica.com/biography/Johannes-Gutenberg Erfindung des Buchdrucks] verfügbar wurden, war die seit langem bekannte Methode der [[Hermeneutics|Hermeneutik]], die eine tiefe und systematische Analyse der Schrift ermöglichte, eine der ersten Methoden, die sich entwickelte. Es kann eine Verbindung zu Übersetzungen und Interpretationen der Bibel hergestellt werden, und viele andere religiöse Diskurse unterscheiden sich in der Tat nicht von einer tiefen Textanalyse und der Ableitung von Interpretationen. Die Hermeneutik geht eindeutig einen Schritt weiter und ist daher eine der frühesten und bis heute wichtigsten wissenschaftlichen Methoden. [[Thought experiments|Gedankenexperimente]] waren eine weitere Denkrichtung, die sich schneller herauszubilden begann. Indem sie sich auf ""Was-wäre-wenn""-Fragen konzentrierten, betrachteten die Wissenschaftler*innen alle möglichen Fragen, um sogar Muster in Naturgesetzen abzuleiten ([https://plato.stanford.edu/entries/galileo/ Galileo]) und richteten ihren Blick auch in die Zukunft. Gedankenexperimente waren seit der Antike auf informelle Weise bekannt, doch im Mittelalter wurden diese Annahmen zum Auslöser, vieles von dem, was angeblich bekannt war, in Frage zu stellen. Durch Experimente wurden sie zu frühen Ansatzpunkten für spätere systematischere Experimente. \n\n[[File:Somer_Francis_Bacon.jpg|thumb|300px|\'\'\'Francis Bacon.\'\'\' Quelle: [https://de.wikipedia.org/wiki/Francis_Bacon Wikipedia]]]', ""[[File:printing-press-2.jpg|300px|thumb|left|'''Invention of the printing press'''. Source: [[https://www.ecosia.org/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \nWith a rising number of texts being available through the [https://www.britannica.com/biography/Johannes-Gutenberg invention of printing], the long known method of [[Hermeneutics]], which enabled deep and systematic analysis of writing, was one of the first methods to grow. A link can be made to translations and interpretations of the Bible, and many other religious discourses are indeed not different from deep text analysis and the derivation of interpretations. Hermeneutics go clearly a step further, and hence are one of the earliest and to date still most important scientific methods. [[Thought Experiments]] were another line of thinking that started to emerge more rapidly. By focussing on ''What if'' questions, scientists considered all sorts of questions to derive patterns of even law of nature ([https://plato.stanford.edu/entries/galileo/ Galileo]) and also directed their view into the future. Thought experiments were in an informal way known since antiquity, yet during medieval times these assumptions became a trigger to question much of what was supposedly known. Through experiments they became early staring points for subsequent more systematic experimentation. \n\n[[File:Somer_Francis_Bacon.jpg|thumb|300px|'''Francis Bacon.''' Source: [https://de.wikipedia.org/wiki/Francis_Bacon Wikipedia]]] \nIt was [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon] who ultimately moved out of this dominance of thinking and called for the importance of empirical inquiry. '''By observing nature and its phenomena, we are able to derive conclusion based on the particular.''' Bacon is thus often coined to be the father of 'the scientific method', an altogether misleading term, as there are many scientific methods. However, empiricism, meaning the empirical investigation of phenomena and the derivation of results or even rules triggered a scientific revolution, and also a specialization of different scientific branches. While Leonardo da Vinci (1452-1519) had been a polymath less than a century ago, Bacon (1561-1626) triggered an increase in empirical inquiry that led to the deeper formation of scientific disciplines. Many others contributed to this development: [https://plato.stanford.edu/entries/locke/ Locke] claimed that human knowledge builds on experience, and [https://plato.stanford.edu/entries/hume/ Hume] propelled this into skepticism, casting the spell of doubt on anything that is rooted in belief or dogma. By questioning long standing knowledge, science kicked in - among other things - the door of god himself, a circumstance that is too large to be presented here in any sense. What is important however is that - starting from the previous age of diverse approaches to knowledge - the combination of empiricism on the one end, and the associated philosophical developments on the other end, the age of reason had dawned.\n\n[[File:Timeline of methods V2.jpg|600px|frame|center|'''A timeline of major breakthroughs in science.''' Source: own]]\n\n==== Enlightenment - ''Pathways To Scientific Disciplines'' ====\nThe age of reason triggered - among many other changes - the development of scientific disciplines much further as ever before. Scientific inquiry was enabled by an increasing influx of resources from the colonies, and biology, medicine, mathematics, astronomy, physics and so much more prospered. '''Early inquiries into these disciplines had often existed since centuries if not millennia, but now knowledge got explored at an exponential pace.''' Much of it was descriptive at first, including what would be considered natural history today. Yet, the early inquiries into the systematical determination and differentiation of organisms ([https://www.britannica.com/biography/Carolus-Linnaeus Linné]) laid the foundation of many other concepts. It was again the colonies that triggered a surplus, this time in basic material to generate more insight. Darwins theory, after all, would have been impossible without specimen from his journeys. While some thus looked into the natural world, other branches of science investigated society, and the first truly systematic case studies were designed in medicine. Hence we moved from experiments as a means of observation into [[Experiments|experimental inquiry]], paving the road to 20th century hypothesis testing. This demanded the testing of the observational outcome, a phenomenon that equally challenged scientists in astronomy. There, just as in experiments, it was recognized that although they tried to be diligent and use the latest available optical tools, there could still be an error that needed to estimated. This constituted a true application opportunity for statistics, leading to the rise of [[Category:Quantitative|quantitative]] methods. \n\n[[File:860-header-explainer-correlationchart.jpg|500px|left|thumb|'''Correlations can be deceitful.''' Source: [http://www.tylervigen.com/spurious-correlations Spurious correlations]]] Probability was the core underlying principle, or in other words, the statistical answer to the question whether something was truly by chance, or if there was a statistical relation. Under the growing economies, more and more numbers surfaced, and building on the Dutch resolution of double book keeping, questions arose whether there were patterns to be found in the data. Could crop outcome of cotton predict its market value later in the year? And would crop failures be clearly linked to famines, or could this be compensated? '''Statistical [[Correlations|correlations]] were able to relate to variables, and to find if one is related to the other, or if the two are unrelated.''' This triggered a hefty move of utilitarianism from philosophy into economics, which is one of the core disciplines up until today. Much can be said about the calculation of utility, [[Big problems for later|but this is beyond the point here]]. In science, learned academies grew everywhere, and universities thrived, also because of the economic paradigm of a growth-focused and consumption-orientated baseline that emerged, and more and more knowledge emerges to criticize and question this [[Glossary|paradigm]]. This triggered what is often called the age of reflection, when empiricism became basically untamed, and the depth of inquiry led to disciplines that started to entrench themselves into even deeper sub-disciplines. The impact on societies was severe. Mechanization, new approaches in agriculture, the rise of modern medicine, an increasingly refined biology and the developments in chemistry are testimony of the physical world being ever more in the focus of science. The general lines of philosophical thinking - reason, social contract and utilitarianism - became partly uncoupled from philosophy, and developed for better or worse to become distinct disciplines such as psychology, social science, political science, cultural studies and economics. We thus observe a deviance from empirical sciences from the previously all-encompassing philosophy. Hence empiricism was on the rise, and with it a pronounced shift in science and society."", ""'''Note:''' The German version of this entry can be found here: [[Scientific methods and societal paradigms (German)]].\n\n'''In short:''' This entry discusses how [[Glossary|scientific methods]] have influenced society - and vice versa.\n__NOTOC__\n== The role of scientific paradigms for society ==\nFrom early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar. However, science was for the most part of the last two millennia in a critical relation to the metaphysical, as there was often a quest for ontological truths between religions and science. While the East was more open to allow science to thrive and made active use of its merits; in Europe, many developments were seen as critical, with Galileo Galileo being a prominent example. Since this changed with the [[History of Methods|Enlightenment]], science paved the way for the rise of the European empires, and with it the associated paradigms.\n\n== Three examples for an active interaction ==\nWhile the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples. \n\n==== Medicine ====\nA prominent example of a strong interaction is medicine, which has at its heart the care for the patient. However, this naive assumption cannot hold the diverse paradigms that influenced and build medicine over time. Today, ananmesis - the information gained by a physician by asking specific questions of a patient - gained in importance, and the interdisciplinary conferences of modern treatments combine different expertise with the goal of a more holistic recognition of the diseases or challenges of the individual patient. \n\n==== Engineering ====\nEngineering is another branch of science which builds on a long tradition, and has at its early stages quite literally paved the road for many developments of modernity. While factories and production processes are today also seen more critically, it has become clear already since Marx that the working condition of modern production are not independent of questions of inequality. In addition, production processes are shifting in order to enable more sustainable production processes, indicating another paradigm shift in engineering. \n\n==== Agricultural science ====\nThe last example, agricultural science, is also widely built around positivistic methodology of modern science, allowing of an optimisation of agricultural production in order to maximise agricultural yield, often with dire consequences. The so-called [https://www.thoughtco.com/green-revolution-overview-1434948 'Green Revolution'] wreaked havoc on the environment, destroyed local livelihoods across the globe, and untangled traditional social-ecological systems into abusive forms that led ultimately to their demise in many parts of the world. \n\nThese three examples showcase how the development of modern science led to abusive, unbalanced, and often unsustainable developments that would in the long run trigger new paradigms such as the post-modernity, degrowths and other often controversially discussed alternatives to existing paradigms. Science was clearly an accomplice in driving many negative developments, and willingly developed the basis for many methodological foundations and paradigms that were seen in a different light after they were utilised over a longer time.\n\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently, science often acted morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational [[Glossary|change]]. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n\n==Science looking at parts of reality==\nSince the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world. The second mistake is that scientific progress can be seen as continuous (see Laudan: Progress and its Problems), while societal paradigms are often utilising snapshots of scientific theories and tend to ignore further development in the respective branch of science. This makes science in turn vulnerable, as it has to claim responsibility for mistakes society made in interpreting scientific theories, and translating them into societal paradigms. In the following message I will illustrate these capital mistakes of science based on several examples. \n\n==== Social Darwinism ====\nThe evolutionary theory of Charles Darwin can be seen as a first example that illustrates how a scientific theory had catastrophic consequences when it was adapted as a societal paradigm. Ideas that the poor in late Victorian England were unworthy of state intervention, and that social welfare was hence a mistake were build on a misunderstanding of Darwins theory, and Darwin opposed the application of his theory for societal debates. Furthermore, he was horrified that his ideas was also taken as a basis to claim superiority of some races over other races, a crude and scientifically wrong claim that paved the road for some of the worst atrocities of the 20th century. \n\n==== The Friedman doctrine ====\nAnother example is Milton Friedman's theory of shareholders, which claims that corporations have first and foremost responsibility against their shareholders. While this seems like a reasonable thought, the consequences for the global economy were considered to be catastrophic by many. Friedman's theory demanded privatisation at a country-wide scale, also for many countries outside of the USA this was attempted and destroyed entire economies. Finally, the stakeholder theory offered a sound development that could counter Friedman's doctrine, and allowed for a recognition of resources, market as well as other important factors such as corporate social responsibility. While the word 'stakeholder' is deeply ambiguous, stakeholder theory and the relation to Friedman's doctrine showcases direct and drastic interactions between science and society.""]"|0.04201680672268908|1.0
28|How can multivariate data be graphically represented?|Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.|"['\'\'\'In short:\'\'\' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically \'\'\'barplots\'\'\'. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.\n\n\'\'\'Correlation plots\'\'\' (\'xyplots\') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a [[Regression Analysis|regression]] line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.\n\n\'\'\'Boxplots\'\'\' are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant.\n\n[[File:Xyplot.png|250px|thumb|left|\'\'\'A Correlation plot.\'\'\' The line shows the regression, the dots are the data points.]]\n[[File:Boxplot3.png|250px|thumb|right|\'\'\'Boxplots.\'\'\']]\n[[File:2Barplots.png|420px|thumb|center|\'\'\'Barplots.\'\'\' The left diagram shows absolute, the right one relative Barplots.]]\n\n\n[[File:Histogram structure.png|300px|thumb|right|\'\'\'A Histogram.\'\'\']]\nA \'\'\'histogram\'\'\' is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n\nSimple \'\'\'pie charts\'\'\' are not really ideal, as they camouflage the real proportions of the data they show. \'\'\'Venn diagrams\'\'\' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a \'\'\'bipartite plot\'\'\', if the levels are within two groups, or, if multiple interconnections exist, then a \'\'\'structural equation model\'\'\' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by \'\'\'circular network plots\'\'\' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|\'\'\'A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.\'\'\' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|\'\'\'A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.\'\'\' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|\'\'\'A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.\'\'\' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: \'\'\'ordination plots\'\'\', \'\'\'cluster diagrams\'\'\' or \'\'\'network plots\'\'\'. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|\'\'\'An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.\'\'\' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|\'\'\'A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic\'s importance in the research, and the connections show if topics were covered alongside each other.\'\'\' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n\'\'\'Descriptive Infographics\'\'\' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.\n\n\n\'\'\'Of course, there is more.\'\'\' While the figures introduced above represent a vast share of the visual representations of data that you will encounter, there are different forms that have not yet been touched. \'\'\'We have found the website [https://www.data-to-viz.com/#connectedscatter ""From data to Viz""] to be extremely helpful when choosing appropriate data visualisation.\'\'\' You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.\n\n\n== How to visualize data in R ==\n\'\'\'The following overview includes all forms of data visualisation that we consider important.\'\'\' <br>\nBased on your data, have a look which forms of visualisation might be relevant for you. Just hover over the individual visualisation type and it will show you its name. It will also show you a quick example which this kind of visualisation might be helpful for. \'\'\'By clicking, you will be redirected to a dedicated entry with exemplary R code.\'\'\'<br>', '<imagemap>Image:Statistical Figures Overview 27.05.png|1050px|frameless|center|\ncircle 120 201 61 [[Big problems for later|Factor analysis]]\ncircle 312 201 61 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA, TREE SPECIES IN AMERICA as three colors, with joint species in the overlaps]]\ncircle 516 190 67 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA as two colors, with joint species in the overlaps]]\ncircle 718 178 67 [[Stacked Barplots|Stacked Barplot, e.g. count data of different species (colors) for the variable TREES]]\ncircle 891 179 67 [[Barplots, Histograms and Boxplots#Barplots|Barplot, e.g. different kinds of trees (x) as count data (y) for the variable TREES]]\ncircle 1318 184 67 [[Barplots, Histograms and Boxplots#Histograms|Histogram, e.g. the variable EXAM POINTS as count data (y) per interval (x)]]\ncircle 1510 187 67 [[Correlation_Plots#Line_chart|Line Chart, e.g. TIME (x) and BITCOIN VALUE (y)]]\ncircle 1689 222 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), POPULATION (bubble size)]]\ncircle 1896 238 67 [[Big problems for later|Ordination, e.g. numeric variables (AGE, INCOME, HEIGHT) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\ncircle 202 326 67 [[Treemap|Treemap, e.g. FORESTS (colors) and count data of the included species (rectangles)]]\ncircle 410 323 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. different species of trees (absolute count data per color) for the variables TREES in ASIA, TREES IN AMERICA, TREES IN AFRICA, TREES IN EUROPE (x)]]\ncircle 608 295 67 [[Stacked Barplots|Proportions Stacked Barplot, e.g. relative count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 812 277 67 [[Pie Charts|Pie Chart, e.g. different kinds of trees (relative count data per color) for the variable TREE SPECIES]]\ncircle 1015 308 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE HEIGHT (y) for beeches]]\ncircle 1228 287 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS per point (x)]]\ncircle 1422 294 67 [[Correlation_Plots#Scatter_Plot|Scatter Plot, e.g. RUNNER ENERGY LEVEL (y) per KILOMETERS (x)]]\ncircle 1574 379 67 [[Big problems for later|Heatmap with lines]]\ncircle 1788 401 67 [[Correlation_Plots#Correlogram|Correlogram, e.g. the CORRELATION COEFFICIENT (shade) for each pair of the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 297 441 67 [[Wordcloud|Wordcloud]]\ncircle 516 434 67 [[Big problems for later|Spider Plot, e.g. relative count data of different species (shape) for the variables TREES IN EUROPE (green), TREES IN ASIA (blue), TREES IN AMERICA (red)]]\ncircle 710 402 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 1323 410 67 [[Regression Analysis#Simple linear regression in R|Linear Regression Plot, e.g. INCOME (y) per AGE (x)]]\ncircle 392 558 67 [[Chord Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites]]\ncircle 621 521 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites (if connections are valued: 3 variables)]]\ncircle 853 496 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. different TREE SPECIES (x) and TREE HEIGHT (y)]]\ncircle 1014 521 67 [[Stacked Area Plot|Stacked Area Plot, e.g. INCOME (x) and count data (y) of BOUGHT ITEMS (colors) (if y is EXPENSES: three variables)]]\ncircle 1174 502 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS IN MATHS (blue) and EXAM POINTS in HISTORY (green) per point (x)]]\ncircle 1438 521 67 [[Big problems for later|Multiple Regression, e.g. INCOME (y) per AGE (x) in different COUNTRIES]]\ncircle 1657 554 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]', '\'\'\'Note:\'\'\' This entry revolves specifically around Bubble plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nA Bubble plot is a graphical representation of multivariate data table. One can think of it as an XY scatter plot with two additional variables. X and Y variables are numeric, and two additional variables, either continuous or categorical, can be represented by the bubble colour and bubble size. \n\n==Overview==\nThis wiki entry will elaborate what a bubble plot is, how to implement such a plot and how to customize your own bubble plot.\n\nA bubble plot is able to present up to four variables, without actually being a four dimensional plot. We can first start with trying to plot three variables. For that the input data should be a triplet (Note: the data should be quantitative and non-categorical). One variable is represented by the x-axis, another one by the y-axis and the third by the size of the data points. Therefore the data points differ in size which makes the plot look like an accumulation of bubbles. We will then incorporate the fourth variable as a color later in our example.\n\nA lot of bubble plot examples can be seen online in the Gapminder data tool [https://www.gapminder.org/tools/#$chart-type=bubbles&url=v1 Bubbles]. Check it out, it’s worth it!\n\n==Preliminaries==\nWe will use <syntaxhighlight lang=""R"" inline>ggplot</syntaxhighlight> to create the bubble plot. In order to use <syntaxhighlight lang=""R"" inline>ggplot</syntaxhighlight> you need to install the packages <syntaxhighlight lang=""R"" inline>gapminder</syntaxhighlight> and <syntaxhighlight lang=""R"" inline>tidyverse</syntaxhighlight> (use the command <syntaxhighlight lang=""R"" inline>install.packages(“name”)</syntaxhighlight>). Depending on your computer system you may also need to install other dependencies. More information on how to install packages can be found [https://www.datacamp.com/community/tutorials/r-packages-guide?utm_source=adwords_ppc&utm_medium=cpc&utm_campaignid=12492439676&utm_adgroupid=122563405841&utm_device=c&utm_keyword=load%20library%20r&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=504100768473&utm_targetid=kwd-385495694086&utm_loc_interest_ms=&utm_loc_physical_ms=9044405&gclid=Cj0KCQiAkZKNBhDiARIsAPsk0WhoeIpvXklFGyd0Slr1grk4JcrYaQGj6EtVfPVf68Mo1RIL_kbRJQEaApUZEALw_wcB here]. After installing the packages, we need to activate their libraries:\n\n<syntaxhighlight lang=""R"" line>\nlibrary(tidyverse)\nlibrary(gapminder)\n</syntaxhighlight>\n[[File:mtcarshead.png|500px|thumb|right|Fig.1: First six entries in the mtcars dataset]]\nIf everything is set up you can choose and take a look at your data. I decided to use the <syntaxhighlight lang=""R"" inline>mtcars</syntaxhighlight> data set, because it is well-known and common to use in examples.\n\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nhead(mtcars)\n</syntaxhighlight>\n\nFor further information on the variables and what this data set is about run the command <syntaxhighlight lang=""R"" inline>?mtcars</syntaxhighlight>.\n\n==Code==\nAfter installing and including the <syntaxhighlight lang=""R"" inline>gapminder</syntaxhighlight> and <syntaxhighlight lang=""R"" inline>tidyverse</syntaxhighlight> packages we are ready to create the plot. I decided to set the theme via <syntaxhighlight lang=""R"" inline>theme_set()</syntaxhighlight> of the plot here. The theme is the overall design and background of your plot. An overview of <syntaxhighlight lang=""R"" inline>ggplot</syntaxhighlight> themes can be found [https://ggplot2.tidyverse.org/reference/ggtheme.html here].\n\n<syntaxhighlight lang=""R"" line>\n#theme\ntheme_set(theme_linedraw())\n</syntaxhighlight>\n\nA bubble plot can take three variables as the code below shows: two for both of the axis (x- and y-axis) and one for the bubble-size. In order to map the variables to the axis and the size the function <syntaxhighlight lang=""R"" inline>aes()</syntaxhighlight> is used. The function <syntaxhighlight lang=""R"" inline>geom_point()</syntaxhighlight> defines the overall type (“points”) of the plot. If there is no input to that function (leaving the brackets empty) the plot would just be a scatter plot. The command <syntaxhighlight lang=""R"" inline>aes(size = variable3)</syntaxhighlight> maps the third variable as the size of points within the function <syntaxhighlight lang=""R"" inline>geom_point()</syntaxhighlight>. That is all the magic!\n\n[[File:bubbleplotmtcars.png|450px|thumb|right|Fig. 2: Cars\' fuel consumption (miles/gallon), their weight (in 1000 lbs) and horsepower visualized with a bubble plot. Dataset: mtcars.]]\n\n<syntaxhighlight lang=""R"" line>\nbubbleplot <- ggplot(data = mtcars, aes(x = mpg, y = wt)) + #variable 1 and variable 2\n                                                            #(x,y-axis)\n  geom_point(aes(size = hp)) #variable 3 (point size)\n\n#Fig.2\n#print the plot\nprint(bubbleplot)\n</syntaxhighlight>\n\nOf course this plot is missing proper labels. So far <syntaxhighlight lang=""R"" inline>ggplot</syntaxhighlight> used the column names of the data set to name the axis and the size. The function <syntaxhighlight lang=""R"" inline>labs()</syntaxhighlight> allows us to customize and add the labels and a title:\n\n[[File:bubplotmtcars.png|450px|thumb|right|Fig. 3: mtcars bubble plot visualization with labels.]]\n\n<syntaxhighlight lang=""R"" line>\nlabelled_bubbleplot <- ggplot(data = mtcars, aes(x = mpg, y = wt)) +\n\n   geom_point(aes(size = hp)) +\n\n   labs(title = ""Labelled Bubbleplot"", #add labels and title\n        x = ""Fuel economy in mpg"",\n        y = ""Weight in 1000 lbs"",\n        size = ""Power in hp"")\n\n#Fig.3\nprint(labelled_bubbleplot)\n</syntaxhighlight>\n\nNow anyone who does not know the data set can interpret and understand what we plotted.']"|0.059113300492610835|1.0
29|What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?|Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.|"['[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""|  \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>\n\n__NOTOC__\n\n\'\'\'In short:\'\'\' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|\'\'\'SCOPUS hits per year for Machine Learning until 2019.\'\'\' Search term: \'Machine Learning\' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you \'\'\'need\'\'\' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be \'\'noisy\'\' or the outputs are expected to be different in each case, you cannot hand-craft the ""[[Glossary|rules]]"" that account for \'\'every\'\' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: \'\'\'Machine Learning\'\'\'. The core idea behind Machine Learning is that instead of being required to hand-craft \'\'all\'\' the rules that take inputs and provide outputs in a fairly accurate manner, you can \'\'train\'\' the machine to \'\'learn\'\' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term ""Machine Learning"" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to ""learn"" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=""text"" inline>(X, y)</syntaxhighlight> pairs where <syntaxhighlight lang=""text"" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=""text"" inline>y = (y_1, y_2, ..., y_n)</syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called \'\'true label\'\'. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=""text"" inline>(X, y)</syntaxhighlight> and one of the many \'\'training algorithms\'\', you can use the model with new data <syntaxhighlight lang=""text"" inline>(X_new)</syntaxhighlight> to make predictions <syntaxhighlight lang=""text"" inline>(y_hat)</syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of ""learning from data"" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, ""a computer program is said to learn from experience \'\'\'E\'\'\' with respect to some class of tasks \'\'\'T\'\'\' and performance measure \'\'\'P\'\'\', if its performance at tasks in \'\'\'T\'\'\', as measured by \'\'\'P\'\'\', improves with experience \'\'\'E\'\'\'.""[3]\n\nIf you deconstruct this definition, here is what you have:\n* \'\'\'Task (T)\'\'\' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification/clustering), or solve problems in an adaptive way (reinforcement learning).\n* \'\'\'Experience (E)\'\'\' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* \'\'\'Performance measure (P)\'\'\' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the ""learned"" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book ""Learning from Data"" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:\n\n[[File:Mostafa Supervised Learning.png|thumb|The Learning Diagram from ""Learning from Data"" by Abu-Mostafa, Magdon-Ismail, & Lin (2012)]]\n\nThe \'\'target function\'\' is assumed to be unknown and the training data is assumed to be based on the target function that is to be estimated. The models that are trained based on the training data are assessed on some error measures (called \'\'metrics\'\') which guide the strategy for improving upon the Machine Learning algorithm that was ""learned"" in the previous steps. If you have enough data, and adopted a good Machine Learning strategies, you can expect the learned algorithm to perform quite well with new input data in the future.\n\n\'\'\'Supervised learning tasks can be broadly sub-categorized into \'\'regression learning\'\' and \'\'classification learning\'\'.\'\'\'', '== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231.\n\n(5) [https://www.vodafone-institut.de/aiandi/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305–306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP’06 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1–12).\n\n== Further Information ==\n* [https://www.datacamp.com/community/tutorials/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https://www.datacamp.com/community/tutorials/machine-learning-in-r Machine Learning in R for Beginners]\n* [https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https://www.youtube.com/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]</nowiki>]\n* [https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https://www.repricerexpress.com/amazons-algorithm-a9/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal.', 'The \'\'target function\'\' is assumed to be unknown and the training data is assumed to be based on the target function that is to be estimated. The models that are trained based on the training data are assessed on some error measures (called \'\'metrics\'\') which guide the strategy for improving upon the Machine Learning algorithm that was ""learned"" in the previous steps. If you have enough data, and adopted a good Machine Learning strategies, you can expect the learned algorithm to perform quite well with new input data in the future.\n\n\'\'\'Supervised learning tasks can be broadly sub-categorized into \'\'regression learning\'\' and \'\'classification learning\'\'.\'\'\' \n\nIn \'\'regression learning\'\', the objective is to predict a particular value when certain input data is given to the algorithm. An example of a regression learning task is predicting the price of a house when certain features of the house (eg. PLZ/ZIP code, no. of bedrooms, no. of bathrooms, garage size, energy raging, etc.) are given as an input to the Machine Learning algorithm that has been trained for this specific task.\n\nIn \'\'classification learning\'\', the objective is to predict which \'\'class\'\' (or \'\'category\'\') a given observation/sample falls under given the characteristics of the observation. There are 2 specific classification learning tasks: \'\'binary classification\'\' and \'\'multiclass classification\'\'. As their names suggest, in binary classification, a given observation falls under one of the two classes (eg. classifying whether an email is spam or not), and in multiclass classification, a given observation falls under one of many classes (eg. in \'\'digit recognition\'\' task, the class for a picture of a given hand-written digit can range from 0 to 9; there are 10 classes).\n\n==== Unsupervised Learning ====\nWhereas supervised learning is based on pairs of input and output data, unsupervised learning algorithms function based only on the inputs: <syntaxhighlight lang=""text"" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> and are rather used for recognizing patterns than for predicting specific value or class.\n\nSome of the methods that are categorized under unsupervised learning are \'\'[https://en.wikipedia.org/wiki/Principal_component_analysis Principal Component Analysis]\'\', \'\'[[Clustering_Methods|Clustering Methods]]\'\', \'\'[https://en.wikipedia.org/wiki/Collaborative_filtering Collaborative Filtering]\'\', \'\'[https://en.wikipedia.org/wiki/Hidden_Markov_model Hidden Markov Models]\'\', \'\'[https://brilliant.org/wiki/gaussian-mixture-model/ Gaussian Mixture Models]\'\', etc.\n\n==== Reinforcement Learning ====\nThe idea behind reinforcement learning is that the ""machine"" learns from experiences much like a human or an animal would [1,2]. As such, the input data ""does not contain the target output, but instead contains some possible output together with a measure of how good that output is"" [1]. As such, the data looks like: <syntaxhighlight lang=""text"" inline>(X, y, c)</syntaxhighlight> where <syntaxhighlight lang=""text"" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> is the input, <syntaxhighlight lang=""text"" inline>y = (y_1, y_2, ... , y_n)</syntaxhighlight> is the list of corresponding labels, and <syntaxhighlight lang=""text"" inline>c = (c_1, c_2, ..., c_n)</syntaxhighlight> is the list of corresponding scores for each input-label pair. The objective of the machine is to perform such that the overall score is maximized.\n\n=== Approaches to Training Machine Learning Algorithms ===\n\n==== Batch Learning ====\nThis Machine Learning approach, also called \'\'offline learning\'\', is the most common approach to Machine Learning. In this approach, a Machine Learning model is built from the entire dataset in one go.\n\nThe main disadvantage of this approach is that depending on the computational infrastructure being used, the data might not fit into the memory and/or the training process can take a long time. Additionally, models based on batch learning need to be retrained on a semi-regular basis with new training examples in order for them to keep performing well.\n\nSome examples of batch learning algorithms are \'\'[http://pages.cs.wisc.edu/~jerryzhu/cs540/handouts/dt.pdf Decision Trees]\'\'(C4.5, ID3, CART), \'\'[https://www.datacamp.com/community/tutorials/support-vector-machines-r Support Vector Machines]\'\', etc.*\n\n==== Online Learning ====\nIn this Machine Learning approach, data is ordered and fed into the training algorithm in a sequential order instead of training on the entire data set at once. This approach is adopted when the dataset is so large that batch learning is infeasible, or when the nature of the data makes it so that more data is available over time (eg. stock prices, sales data, etc.)\n\nSome examples of online learning algorithms are \'\'[https://en.wikipedia.org/wiki/Perceptron Perceptron Learning Algorithm]\'\', \'\'[https://en.wikipedia.org/wiki/Stochastic_gradient_descent stochastic gradient descent] based classifiers and regressors\'\', etc.\n\n=== What about Neural Networks? ===\nNeural networks are a specific approach to Machine Learning that can be adapted to solve tasks many different settings. As a result, they have been used for detecting spams in emails, identifying different objects in images, beating humans at games like Chess and Go, grouping customers based on their preferences and so on. In addition, neural networks can be used in all three types of Machine Learning tasks mentioned above - supervised, unsupervised, and reinforcement learning.\n\nPlease refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].']"|0.005434782608695652|0.30612244897959184
30|What are some of the challenges faced by machine learning techniques?|Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.|"['[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""|  \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>\n\n__NOTOC__\n\n\'\'\'In short:\'\'\' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|\'\'\'SCOPUS hits per year for Machine Learning until 2019.\'\'\' Search term: \'Machine Learning\' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you \'\'\'need\'\'\' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be \'\'noisy\'\' or the outputs are expected to be different in each case, you cannot hand-craft the ""[[Glossary|rules]]"" that account for \'\'every\'\' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: \'\'\'Machine Learning\'\'\'. The core idea behind Machine Learning is that instead of being required to hand-craft \'\'all\'\' the rules that take inputs and provide outputs in a fairly accurate manner, you can \'\'train\'\' the machine to \'\'learn\'\' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term ""Machine Learning"" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to ""learn"" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=""text"" inline>(X, y)</syntaxhighlight> pairs where <syntaxhighlight lang=""text"" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=""text"" inline>y = (y_1, y_2, ..., y_n)</syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called \'\'true label\'\'. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=""text"" inline>(X, y)</syntaxhighlight> and one of the many \'\'training algorithms\'\', you can use the model with new data <syntaxhighlight lang=""text"" inline>(X_new)</syntaxhighlight> to make predictions <syntaxhighlight lang=""text"" inline>(y_hat)</syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of ""learning from data"" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, ""a computer program is said to learn from experience \'\'\'E\'\'\' with respect to some class of tasks \'\'\'T\'\'\' and performance measure \'\'\'P\'\'\', if its performance at tasks in \'\'\'T\'\'\', as measured by \'\'\'P\'\'\', improves with experience \'\'\'E\'\'\'.""[3]\n\nIf you deconstruct this definition, here is what you have:\n* \'\'\'Task (T)\'\'\' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification/clustering), or solve problems in an adaptive way (reinforcement learning).\n* \'\'\'Experience (E)\'\'\' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* \'\'\'Performance measure (P)\'\'\' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the ""learned"" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book ""Learning from Data"" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:\n\n[[File:Mostafa Supervised Learning.png|thumb|The Learning Diagram from ""Learning from Data"" by Abu-Mostafa, Magdon-Ismail, & Lin (2012)]]\n\nThe \'\'target function\'\' is assumed to be unknown and the training data is assumed to be based on the target function that is to be estimated. The models that are trained based on the training data are assessed on some error measures (called \'\'metrics\'\') which guide the strategy for improving upon the Machine Learning algorithm that was ""learned"" in the previous steps. If you have enough data, and adopted a good Machine Learning strategies, you can expect the learned algorithm to perform quite well with new input data in the future.\n\n\'\'\'Supervised learning tasks can be broadly sub-categorized into \'\'regression learning\'\' and \'\'classification learning\'\'.\'\'\'', '== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231.\n\n(5) [https://www.vodafone-institut.de/aiandi/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305–306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP’06 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1–12).\n\n== Further Information ==\n* [https://www.datacamp.com/community/tutorials/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https://www.datacamp.com/community/tutorials/machine-learning-in-r Machine Learning in R for Beginners]\n* [https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https://www.youtube.com/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]</nowiki>]\n* [https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https://www.repricerexpress.com/amazons-algorithm-a9/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal.', '===Challenges===\n* Even if bootstrapping is asymptotically consistent, it does not provide general finite-sample guarantees.\n* The result may depend on the representative sample.\n* The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples).\n* Bootstrapping can be time-consuming.\n* There is a limitation of information that can be obtained through resampling even if number of bootstrap samples is large.\n\n===Normativity===\n\nBootstrap methods offer considerable potential for modelling in complex problems, not least because they enable the choice of estimator to be separated from the assumptions under which its properties are to be assessed.\n\nAlthough the bootstrap is sometimes treated as a replacement for ""traditional statistics"". But the bootstrap rests on ""traditional"" ideas, even if their implementation via simulation is not ""traditional"".\n\nThe computation can not replace thought about central issues such as the structure of a problem, the type of answer required, the sampling design and data quality.\n\nMoreover, as with any simulation experiment, it is essential to monitor the output to ensure that no unanticipated complications have arisen and to check that the results make sense. The aim of computing is insight, not numbers.[2]\n\n==Outlook==\nOver the years bootstrap method has seen a tremendous improvement in its accuracy level. Specifically improved computational powers have allowed for larger possible sample sizes used for estimation. As bootstrapping allows to have much better results with less amount of data, the interest for this method rises substantially in research field. Furthermore, bootstrapping is applied in machine learning to assess and improve models. In the age of computers and data driven solutions bootstrapping has good perspectives for spreading and development.\n\n==Key Publications==\n* Efron, B. (1982) The Jackknife, the Bootstrap, and Other Resampling Plans. Philadelphia: Society for Industrial and Applied Mathematics.\n* Efron, B. and Tibshirani, R. J. (1993) An Introduction to the Bootstrap. New York: Chapman and Hall.\n==References==\n# Dennis Boos and Leonard Stefanski. Significance Magazine, December 2010. Efron\'s Bootstrap.\n# A. C. Davison and Diego Kuonen. Statistical Computing & Statistical Graphics Newsletter. Vol.13 No.1. An Introduction to the Bootstrap with Applications in R\n# Michael Wood. Significance, December 2004. Statistical inference using bootstrap confidence interval.\n# Jeremy Orloff and Jonathan Bloom. Bootstrap confidence intervals. Class 24, 18.05.\n# CRAN documentation. Package ""bootstrap"", June 17, 2019.\n\n==Further Information==\n\nYoutube video. Statquest. [https://www.youtube.com/watch?v=isEcgoCmlO0&ab_channel=StatQuestwithJoshStarmer Bootstrapping main ideas]\n\nYoutube video. MarinStatsLectures. [https://www.youtube.com/watch?v=Om5TMGj9td4&ab_channel=MarinStatsLectures-RProgramming%26Statistics Bootstrap Confidence Interval with R]\n\nYoutube video. [https://www.youtube.com/watch?v=9STZ7MxkNVg&ab_channel=MarinStatsLectures-RProgramming%26Statistics MarinStatsLectures. Bootstrap Hypothesis Testing in Statistics with Example]\n\nArticle. [https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/#:~:text=The%20bootstrap%20method%20is%20a,the%20mean%20or%20standard%20deviation.&text=That%20when%20using%20the%20bootstrap,and%20the%20number%20of%20repeats. A Gentle Introduction to the Bootstrap Method]\n\nArticle. [https://statisticsbyjim.com/hypothesis-testing/bootstrapping/ Jim Frost. Introduction to Bootstrapping in Statistics with an Example]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Andrei Perov.']"|0.041237113402061855|0.0
31|What are the characteristics of scientific methods?|Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.|"['\'\'\'This sub-wiki deals with scientific methods.\'\'\' <br/>\n\n=== What are scientific methods? ===\nWe define \'\'Scientific Methods\'\' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either \'\'\'reproducible\'\'\' and \'\'\'learnable\'\'\'; can be \'\'\'documented\'\'\' and are \'\'\'learnable\'\'\'; or are \'\'\'reproducible\'\'\', can be \'\'\'documented\'\'\', and are \'\'\'learnable\'\'\'. \n* From a systematic perspective, methods are approaches that help us \'\'\'gather\'\'\' data, \'\'\'analyse\'\'\' data, and/or \'\'\'interpret\'\'\' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n\'\'\'This Wiki describes each presented method in terms of\'\'\' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki\'s underlying [[Design Criteria of Methods]].<br/>\n\'\'\'This means that each method fulfills one or more categories of each of the following criteria:\'\'\'\n<br/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=""pages"" hideroot=""on"">Methods</categorytree>\n<br>\nWe also have what we call \'\'\'Level 2\'\'\' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* \'\'\'[[Statistics]]\'\'\': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* \'\'\'[[Interviews]]\'\'\': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read.', 'The course \'\'\'Scientific methods - Different paths to knowledge\'\'\' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br/>\n=== Definition & History of Methods ===\n\'\'\'Epochs of scientific methods\'\'\'\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n\'\'\'Critique of the historical development and our status quo\'\'\'\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n\'\'\'Interaction of scientific methods with philosophy and society\'\'\'\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n\'\'\'Quantitative vs. qualitative\'\'\'\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n\'\'\'Inductive vs. deductive\'\'\'\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n\'\'\'Scales\'\'\'\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n\'\'\'Critical theory\'\'\'\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n\'\'\'The pragmatism of [[Glossary|bias]]\'\'\'\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n\'\'\'Limitations in science\'\'\'\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n\'\'\'The scientific method?\'\'\'\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n\'\'\'Forming hypotheses\'\'\'\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n\'\'\'Limitations of hypothesis\'\'\'\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - ‘causal’ form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]\n\n=== Causality & Correlation ===\n\'\'\'Defining causality\'\'\'\nBuilding on the criteria from David Hume, we define causality through temporal links (""if this - then this""), as well as through similarities and dissimilarities. If A and B cause C, then there must be some characteristic that makes A and B similar, and this similarity causes C. If A causes C, but B does not cause C, then there must be a dissimilarity between A and B. Causal links can be clearly defined, and it is our responsibility as scientists to build on this understanding, and understand its limitations. \n\n\'\'\'Understanding correlations\'\'\' Correlations statistically test the relation between two continuous variables. A relation that - following probability - is not a coincidence but from a statistical standpoint meaningful, can be called a significant correlation. \n\n\'\'\'The difference between causality and correlation\'\'\'\nWith increasing statistical analysis being conducted, we sometimes may find significant correlations that are non-causal. Disentangling causal form correlative relations is deeply normative and needs to acknowledge that we often build science based on deeply constructed ontologies.', '\'\'\'Note:\'\'\' This entry focuses especially on Methods of Sustainability Science. For a more general conceptual view on Methods, please refer to the entry on the [[Design Criteria of Methods]].\n\n\'\'\'In short:\'\'\' In this entry, you will find out more about how we can distinguish and design our methodological approaches in Sustainability Science.\n\n== Design Criteria in Sustainability Science - \'\'Why?\'\' ==\nThe [[Design Criteria of Methods|design criteria of methods]] that I propose for all methods - quantitative vs. qualitative, inductive vs. deductive, spatial and temporal scales - are like the usual suspects of scientific methods. Within normal science, these design criteria are what most scientists may agree upon to be central for the current debate and development about methods. Consequently, \'\'\'it is important to know these \'\'normal science\'\' design criteria also when engaging with sustainability science.\'\'\' However, some arenas in science depart from the current paradigm of science - \'\'sensu strictu\'\' [https://plato.stanford.edu/entries/thomas-kuhn/#DeveScie Kuhn] - and this means also that they depart potentially from the design criteria of the normal sciences. \n\nThe knowledge science currently produces is not enough to solve the problems we currently face. In order to arrive at new solutions, we need to re-consider, adapt and innovative the current raster of methods in science. Many methods that are long established are valuable, and all scientific methods have their [[History of Methods|history and origin]], and thus are important. Nevertheless, it becomes more and more clear that in order to arrive at solutions for the problems we currently face, we need to consider if we need methodological innovation. In the past,  methodological innovation worked in one of these ways:\n# new methods were invented,\n# new combinations of methods were attempted,\n# and methods were re-designed to be applied in a novel context where they had never been used before. \nWhile all this is fascinating in itself, here I present a different approach towards an amendment of the methodological canon, for one simple reason: Developing the methodological canon of science takes experience in the application of scientific methods, and ideally also contextual experience in diverse methodological approaches. While I am personally deeply critical about scientific disciplines, this is a good argument that in order to become versatile in scientific methods, you may need to channel your development towards being versatile in scientific methods based on textbook knowledge. Normal sciences have textbooks to teach their methodological canon, and while I think we need to be critical of these methodological approaches, they can have a lot of value. If you ask why, the I would say simply, because you cannot re-apply methods in a different context or attempt recombinations of methods if you are not experienced in the application of methods. To this end, it is important to highlight that these methods only look at parts of reality, which is the main reason for being critical. [[Bias and Critical Thinking|Bias]] and the fact that accompanying [[Levels of Theory|paradigms and theoretical viewpoints]] of specific methods restrict the validity of specific methods makes me even more critical. Nevertheless, there is no alternative to building experience than through applying methods within active research. You need to get your hands dirty to get experience. Here, I propose that we start with the knowledge we want to produce, and the goal we aim at to produce this knowledge. If we want to empower stakeholders, we need to be aware which methods out of the existing canon might help us, and how we may need to combine these methods in order to produce the knowledge we aim at. Therefore, we present three types of design criteria that serve as a basis for reflection of what knowledge we want to produce.\n\n== Key Competencies in Sustainability ==\nIn 2011, Wiek et al. analyzed the prevalent literature and presented five Key Competencies that students of Sustainability Science should strive for. This is an excellent scheme to be reflexive about the [[Glossary|competencies]] you want to gain, and to get a better understanding on which competencies can be aimed at through specific methods. These competencies are as follows:\n* \'\'Systems Thinking\'\': Systems Thinking competency is the ability to analyze and understand [[Glossary|complex systems]] including the dynamics in the interrelation of their parts. Systems thinking integrates different domains (society, environment, economy, etc.) as well as different scales (from local to global). \n\n* \'\'Anticipatory\'\': Anticipatory competency describes the ability to develop realistic scenarios of future trajectories within complex systems, including positive (e.g. a carbon-neutral city) and negative (e.g. flooding stemming from climate change) developments. This may include rigorous concepts as well as convincing narratives and visions.\n\n* \'\'Normative\'\': Normative competency refers to the ability to evaluate, discuss and apply (sustainability) values. It is based on the acquisition of normative knowledge such as concepts of justice or equality.\n\n* \'\'Strategic\'\': In simple terms, this competence is about being able to ""get things done"". Strategic competency is the capability to develop and implement comprehensive strategies (i.e. interventions, projects, measures) that lead to sustainable future states across different societal domains (social, economic, ecologic, ...) and scales (local to global). It requires an intimate understanding of strategic concepts such as path dependencies, barriers and alliances as well as knowledge about viability, feasibility, effectiveness and efficiency of systemic interventions as well as potential of unintended consequences.\n\n* \'\'Interpersonal\'\': Interpersonal competence is the ability to motivate, enable, and facilitate collaborative and participatory sustainability research and problem solving. This capacity includes advanced skills in communicating, deliberating and negotiating, collaborating, [[Glossary|leadership]], pluralistic and trans-cultural thinking and empathy.\n\n[[File:SustainabilityCompetencies.png|750px|thumb|center|\'\'\'Key Competencies for Sustainability.\'\'\' Source: Wiek et al. 2011, p.206]]\nThe criteria from Wiek et al are outstanding in the capacity to serve as boundary object, since I do not see these categories as mutually exclusive, but instead strongly interwoven with each other. I can whole-heartedly recommend to return to these criteria then and again, and to reflect on yourself through the lense of these criteria.\n\n== Knowledge for action-oriented sustainability science ==\nAt the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and context-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science:\n[[File:Types of knowledge for action-oriented sustainability science.png|900px|thumb|center|\'\'\'Types of knowledge for action-oriented sustainability science.\'\'\' Source: Caniglia et al. 2020, p.4]]']"|0.08823529411764706|1.0
32|What is the main goal of practicing mindfulness?|The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || [[:Category:Productivity Tools|Productivity Tools]] || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Noble Eightfold Path - Mindfulness.png|350px|thumb|right|\'\'\'The Noble Eightfold Path, with Mindfulness being the seventh practice.\'\'\' Source: [https://en.wikipedia.org/wiki/Noble_Eightfold_Path Wikipedia], Ian Alexander, CC BY-SA 4.0]]\nMindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the context of many diverse but often religious schools of thinking.\n\n== Goals ==\nSince the goal of mindfulness is basically having ""no mind"", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free. \n\n== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen – sitting meditation – is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|\'\'\'[https://www.headspace.com/de Headspace] is an app that can help you meditate\'\'\', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around – often referred to as the monkey mind –, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions – just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!\n\n== Links & Further Reading ==\n* [https://www.headspace.com Headspace] \n* [https://www.youtube.com/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https://thichnhathanhfoundation.org/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https://en.wikipedia.org/wiki/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on \'\'Zen Mind, Beginner\'s Mind\'\' is classic introduction to Zen.\n* [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn.', ""'''Studying teaches you to try things out'''<br>\nBeside the three staples of academics - reading, group work and writing - learning at a University is also about many other opportunities to learn and grow. This list is very specific and context depended for each and every single person. Still, the general consensus is that studying is about trying things out, how you can learn best, and find out what you are good at, and how you can contribute best. Here are some points that I consider to be relevant.\n\n'''Soft skills'''<br>\nAmong the diverse term of soft skills are personal traits and approaches that basically help us to interact. While this could be associated to group work (see above), I think it is good to make a mind map that you keep updating and exchange about with others. This is nothing that you need to obsess about, but more like a conscious and reflexive diary of your own personal development. Actually, a research diary can be a good first step. Also, if you witness others that excel at a certain soft skill, approach them and ask them how they learned their respective skills. It is also quite helpful -surprise- to practice. Presentations are something that are often not right the first time, and constructive feedback from critical people that you trust goes a long way. Much of the literature and other resources on soft skills are often over-enthusiastic, and promise the one and only best approach. Do not let yourself be fooled by such simple fixes, some of the soft skill literature is rather fringe. Still, new approaches to knowledge and interaction await, much can be gained, and only a bit of time may be lost. Why not giving another soft skill a go? The most important step is then to try it really out. Doing meditation once will tell you nothing about it, yet after some weeks you may perceive some changes. Your first World Café was a failure? Well, try it again, several times, in different settings. For soft skills you need to stay open minded.\n\n'''Digital natives?'''<br>\nWe are awash with information to the brim, and continuously on the edge of drowning in it. Mastering all things digital may be one of the most important skills in this age and place. I think the most important rule to this end is: Less is more. Evidence how bad too much exposure to the digital world seems to be is mounting. Social media made many promises, yet I am not sure how many were kept. I can acknowledge that it can create meaningful linkages, build capacity, and even be a lifeline to your distant friend. Nevertheless, I would propose to be very reflexive which emotions are triggered by social media within you. This may lead to the conclusion to detox. The same holds true for all things extreme, namely binge watching, youtube or Spiegel Online. Instead you need to become versatile in a word processor, Powerpoint, maybe a graphical software, and get a hold of your direct digital communication. E-mail is still a thing, and writing a good e-mail is a skill that is equally admirable and often missed out on by students. I have been there. Again, practice goes a long way. Also, be conscious about data structure, backups, and online plans. You should be able to impress others with you digital skills. This will open many doors, and tilt many opinions towards your direction. Get at it!\n\n'''Work-life-balance and motivation'''<br>\nCurb your enthusiasm. There are no simple answers here. Work-life-balance became a pretty big thing lately, and we all hope that the prices we paid as long as it was ignored will now not become the flip-side of the coin, since we basically talk non-stop about work-life-balance these days. Personally, I never really quite understood this hype. It is undeniable that having a balanced work-life dynamic is central and often overseen. However, having a filled curriculum after hours that is supposed to relax you by adding to your already busy schedule further to the brink may not be doing the trick. Having a nine to five schedule is no guarantee for a happy life, just like long working hours can be ok if you are ok with this. It is currently 21:36 when I write this text, and I do that because I want to do this. The danger is -I believe- if we let the system dictate us what we should do, and when. It does not matter wether it is about work or about relaxation. After all, it is really hard to relax on command, especially when you are hyped, and have still energy. All in all, I still give a note of caution. I overworked myself in the past, not only because of societal expectations, but also because I basically had no radar about my own balance, and how easy it can be thrown off. It took me a long time to figure this one out for myself, and I think in the spirit of being better safe than sorry, go easy on yourself. We are currently in an epidemic of psychological challenges, especially among the younger generation. We cannot go on like this. Being motivated is like the worst pressure point we ever discovered. If I can only put one piece of advise here, then I would suggest that you should always try to establish a path, and not goals. Being on a way and establishing the associated mindset is the most fundamental change we need. If we keep rushing towards goals, and keep changing these goals like all the time, and never acknowledge when we reach these goals, then all is lost. I much prefer to just be on a path, even if I am not clear in all points where it will lead me. You may write this one down on a teabag.\n\nStudying is a privilege, and a challenge. Practice reading, talk to other students, and start writing a learning diary. Being an academic to me means committing to lifelong learning, because neither the world nor knowledge stands still. It took me a while to figure learning against my other commitments in life, and I am still learning to this end as well. I am very glad to keep learning.\n\n== Christopher (B.Sc. Biogeowissenschaften, M.Sc. Sustainability Science) ==\n'''Being overwhelmed'''<br>\nWelcome to university! If you've come straight from school, you might be overwhelmed by your first semester. I certainly was - after all, each semester requires at least as much studying as your school finals. Every semester! It is natural to be challenged by this initially. I struggled with my first and third semester, then my bachelor thesis, then the first master semester. However, as cheesy as it may sound: you'll grow with these challenges. It is alright to skip a class if you cannot spend the proper time on it, or aren't quite feeling yourself. Not a question. Still, you won't be doing yourself a favor by postponing everything to eternity just because you can. Take on the challenge: You'll become more efficient and pragmatic by going through those three months, and next time, you'll better know how to deal with what awaits you. Cherish this process. I guess it's part of getting older."", '[[File:ConceptWALKINGEXERCISE.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[WALKING EXERCISE]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/><br/>\n\n\n\'\'\' In short:\'\'\' Mental maps are a visual representation of how people perceive their daily environment and how people orient themselves in it. The \'\'Walking Exercise\'\' makes use of this strategy to initially build sustainability competencies in higher education settings.\n\n== Background ==\n[[File:VisualisationMentalMapsWALKINGEXERCISE.png|thumb|right|mental maps of Phoenix (a + b) and Hamburg (c + d). Source: (1)]]\nThe world and environment are in a critical state as there are certain sustainability challenges, such as biodiversity loss, global warming, limited resources, and increased inequalities. From this, the need to react to them arises, both locally and globally. Developing certain sustainability competencies (skills, abilities) can be a start to learn how to do so. Wiek et al. (1) have sketched out five competencies that should be considered for academic program development. These are as follows: systems thinking, anticipatory competence, normative competence, strategic competence, and interpersonal/collaborative competence. Based on these, Caniglia et al. (2) have worked out a method, aiming at building some of the aforementioned competencies, which is called \'\'Walking exercise\'\'. The method combines mental mapping and exploratory walking.\n\n[[File:Transect-walk.png|thumb|right|source: https://www.spool.ac/index.php/spool/article/view/35]]\n\'\'\'Mental mapping\'\'\' stems from the field of behavioural geography and was especially coined by Kevin Lynch and his work “The Image of the City” (3). It captures how people perceive their urban environment. Practically speaking, a person’s image of a city is their mental map of it. The map usually entails the following characteristics:\n* paths: routes along which people move throughout the city; \n* edges: boundaries and breaks in continuity; \n* districts: areas characterized by common characteristics; \n* nodes: strategic focus points for orientation like squares and junctions;\n* landmarks: external points of orientation, usually an easily identifiable physical object in the urban landscape.\n\n\'\'\'Exploratory walking\'\'\' (or transect walking) is a method from the field of city planning often used for observation-based community improvement. Its aim is the gathering of data and experience of one’s own daily environment in a systematic way that transform one’s own perception of it, thereby gaining deeper understanding. This means walking through the environment along a defined path across an area and taking notes on what stands out. Often, it is done in small groups in order to be able to exchange with others.\n\n\n==What the method does==\n\n[[File:Goals.png|thumb|right|Goals of the Walking exercise. Source: (1)]]\n[[File:Walking exercise spiral.png|thumb|right|Phases and steps of the Walking Exercise. Source: (1)]]\n\'\'\'The \'\'walking exercise\'\' is a bottom-up, student-centered, and experience-based method in higher education settings to develop sustainability competencies in local contexts.\'\'\' It is meant for students with no or little previous knowledge in sustainability science, for example first-semester students in the environmental sciences realm and spans over one semester.\n\nThe goal is to actively engage with sustainability problems in one’s surroundings from the beginning on and thereby understand concepts, principles, methods of sustainability and think about solution options.\n\nEssential for this is the \'\'\'development of [https://www.youtube.com/watch?v=1vHY2RJnr3A sustainability competencies]\'\'\', especially systems thinking, normative, and collaborative competencies as named by Wiek et al. (2011).\n\nSystems thinking means the ability to analyze complex systems and problems across different domains (society, economy, environment) and scales (local to global) in order to engage with and tackle them. \nNormative competencies, or “value-focused thinking”, stands for the evaluation of sustainability through different sustainability principles and the ability to discuss and apply values, habits, perceptions and experiences. It is tightly connected with ethics and touches upon reflecting on one’s own position as well.\nBeing able to motivate people and facilitate group processes using non-violent and empathetic communications, as well as actively listening, is the essence of collaborative competencies. As it describes practices between people, it is called interpersonal competence also.\n\nIn order to foster these competencies, students shall perceive and explore their urban environment using mental mapping activities and walking activities (connecting the learning objectives directly with one’s own experience). \nTo do so, both phases (mapping and walking) are performed after one another and share the same four sub-steps: preparation, data gathering and analysis, interpretation and reflection, and lastly, sharing. \n\nIn their learning experience, students are supported by an instructor who guides them by preparing methodological/theoretical inputs, reflection questions, and facilitates in-class discussions. However, it is by no means a frontal teaching style but rather a source of support if needed, as students should learn from their own experiences.']"|0.02564102564102564|1.0
33|How is information arranged in a Mindmap?|In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\'\'\'Mindmapping is a tool for the visual organisation of information\'\'\', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|\'\'\'MindMaps can take the form of trees, with the words on the branches, or clusters/bubbles, as in this example.\'\'\' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https://www.thetutorteam.com/wp-content/uploads/2019/07/shutterstock_712786150.jpg thetutorteam.com]]]\n\n\'\'\'A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.\'\'\' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). \'\'\'This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.\'\'\' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. \'\'\'Elements may be subordinate to other elements, indicating nestedness of the information.\'\'\' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n\'\'Sources:\'\'\n* [https://www.mindmapping.com/de/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http://methodenpool.uni-koeln.de/download/mindmapping.pdf Uni Köln Methodenpool - Mind-Mapping]]\n* [https://kreativitätstechniken.info/problem-verstehen/mindmapping/ Kreativitätstechniken.info - Mindmapping]]\n* [https://www.lifehack.org/articles/work/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https://www.thetutorteam.com/blog/mind-maps-how-they-can-help-your-child-achieve/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConcept Maps are a \'\'\'form of visually organizing information\'\'\' for groups and individuals. Conceptual terms or short statements are presented in boxes or bubbles and interlinked with other terms through commented arrows, resulting in a hierarchical network-like structure that provides an overview on a topic.\n\n== Goals ==\n* Organize \'\'\'conceptual knowledge\'\'\' in a visual form to identify learning processes and knowledge gaps.\n* Create new knowledge, ideas and solutions by \'\'\'arranging information in a structured manner\'\'\' around a focus question, topic, or problem.\n\n== Getting started ==\n[[File:Concept Maps - Example 1.png|500px|thumb|right|\'\'\'An example for a Concept Map on the knowledge structure required for understanding why we have seasons.\'\'\' . Source: [http://cmap.ihmc.us/docs/theory-of-concept-maps.php Novak & Cañas 2008, Cmap]]]\nConcept Maps result from the work of \'\'\'Joseph Novak and colleagues at Cornell University in the 1970s\'\'\' in the field of education, and have since been applied in this area and beyond. The original underlying idea was to analyze how children learn, assuming that they do so by assimilating new concepts and positioning these in a cognitive conceptual framework. The idea of Concept Maps emerged from the demand for a form of assessing these conceptual understandings.\n\nImportantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of [[Group Concept Mapping]]. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a [[Mindmap]] in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.\n\nConcept Maps \'\'\'help identify a current state of knowledge\'\'\' and show gaps within this knowledge, e.g. a lack of understanding on which elements are of importance to a question or topic, and how concepts are interrelated. Identifying these gaps helps fill knowledge gaps. Therefore, they can be a helpful tool for students or anyone learning a new topic to monitor one\'s own progress and understanding of the topic, and how this changes as learning units continue.\n\nFurther, Concept Maps can be \'\'\'a way of approaching a specific question\'\'\' or problem and support a systematic solution-development process. The visual representation of all relevant elements nconcerning a specific topic can thus help create new knowledge. There are even more imaginable purposes of the approach, including management and planning, creative idea-generation and more.\n\n[[File:Concept Map Example 2.png|500px|thumb|center|\'\'\'This example from Novak (2016, p.178) shows how a student\'s conceptual understanding of a topic develops over time.\'\'\' It illustrates how Concept Maps can help identify knowledge gaps or flaws (which are existent still in the map below), how much (and what kind of) learning process was made over time, and what to focus on in future learning. Source: Novak 2016, p.178.]]\n\n\n== Step by step ==\n[[File:Concept Map - Step by Step 1.png|400px|thumb|right|\'\'\'A list of concepts (left) and a ""string map"" that focuses on cross-links between concepts, as a groundwork for the concept map.\'\'\' Source: [http://cmap.ihmc.us/docs/theory-of-concept-maps.php CMap]]]\n\n# When learning to work with Concept Maps, Novak & Canas (2008) recommend to \'\'\'start with a domain of knowledge that one is familiar with\'\'\'. Here, the learner should focus on a specific focus question and/or text item, activity, or problem to structure the presented elements and hierarchies around, and to contextualize the map by. As the authors highlight, often, ""learners tend to deviate from the focus question and build a concept map that may be related to the domain, but which does not answer the question. It is often stated that the first step to learning about something is to ask the right questions"".\n# After defining the domain of knowledge and focus question or problem, 15-25 \'\'\'key concepts should be identified and ranked\'\'\' according to their specificity. In a concept map, more general, broad concepts are on the top of the map, while more specific concepts are found below, resulting in a hierarchy from top to bottom. So the concepts should be ordered accordingly in a list first.\n# Then, a preliminary concept map is created, either digitally (with the \'official\' IHMC CmapTools software, see below) or by using Post-Its and a whiteboard. Concepts can and should be moved around iteratively. Propositions  - connections between concepts with text - are added to the map to highlight and explain relationships between concepts. When a first good draft exists, one identifies cross-links, which are relationships between separate areas of the map, to further highlight conceptual linkages. Lastly, \'\'\'constant revision is applied\'\'\' and concepts can be re-positioned to further improve the map.\n\n\n== Links & Further reading ==\n* This entry was created based on the extensive entry by Novak & Canas (2008), available [http://cmap.ihmc.us/docs/theory-of-concept-maps.php here]. The authors are responsible for the introduction of Concept Maps in the 1970s and the development of the CmapTools.\n\n* Novak, J.D. 2016. The origins of the concept mapping tool and the continuing evolution of the tool. Information Visualisation 5. 175-184.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.', '\'\'\'Note:\'\'\' This entry revolves specifically around Treemaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br>\n<br>\n\'\'\'In short:\'\'\' A treemap is a rectangle-based visualization method for large, hierarchical data sets. Originally designed to visualize files on a hard drive and developed by Shneiderman and Johnson. They capture two types of information in the data: (1) the value of individual data points; (2) the structure of the hierarchy.\n__TOC__\n<br>\n\n== Definition ==\nTreemaps display hierarchical (tree-structured) [[Glossary|data]]. They are composed of a series of nested rectangles (tiles) whose areas are proportional to the data they represent. Each branch of the tree is given a rectangle, which is then subdivided into smaller rectangles representing sub-branches. The conceptual idea is to break down the data into its constituent parts and quickly identify its large and small components.\n<br>\n<br/>\n[[File:Switzerlandtreemap.png|400px|thumb|right|Fig.1: Switzerland imports in 2017. Source: commons.wikimedia.org]]\n\'\'\'Treemaps are used:\'\'\' <br>\n1. To study data with respect to two quantitative values: <br>\n– positive quantitative value standing for the size of the rectangle (area cannot be negative) and<br>\n– second or categorical quantitative value standing for the color of the individual rectangles.<br>\n2. To display very large amount of hierarchial data in a limited space.<br> \n3. To make a quick, high-level summary of the similarities and differences within one category as well as between multiple categories (not precise comparisons).\n<br>\n<br>\nThe efficient use of physical space and the intelligent color management make treemaps powerful visualization technique applied to a wide variety of domains. They are used to display significant amounts of information in financial, commercial, governmental and similar fields. The treemap on Fig.1 shows Switzerland imports in 2017.\n[[File:Motorbikestreemap.png|300px|thumb|right|Fig.2: Category-wise sales figure for motorbikes. Source: www.fusioncharts.com]]\n\'\'\'Adding new Dimensions.\'\'\' With the intelligent use of colors, new dimensions can be added to the diagram. The usual practice is to use color in different rectangles to indicate a second categorical or quantitative value. If color is used to express a quantitative value, it’s strongly encouraged to use only one color (if all the numbers are positive) or two colors (one for negative and one for positive), and vary the intensity of the color to express precise value.\n<br>\n<br>\nThe following treemap (Fig.2) illustrates the category-wise (Street, Cruiser and etc.) sales figure for motorbikes. The size of the rectangles within each category indicates the relative number of sales. Different colors and color intensities show growth and declines of the motorbike sales. “Static” shows that sales neither grew nor declined. Very intense orange indicates a big shift downward, and very intense green indicates a big shift upwards.\n\nFrom Fig.2 it can be concluded that appropriate use of color enables us to use tree maps to represent losses, declines in sales or other non-positive values. The second quantitative value is not represented by the area of the rectangle.\n<br>\n<br>\nThe way the rectangle is divided and arranged into sub-rectangles depends on \'\'\'the tiling algorithm\'\'\' used.\n<br>\n<br>\nMany tiling algorithms have been developed and here are some of them:\n<br>\n<br>\n\'\'\'Squarified\'\'\' - keeps each rectangle as square as possible. It also tries to order the consecutive elements of the dataset (blocks, tiles) in descending order from the upper left corner to the lower right corner of the graph.\n<br>\n<br>\n\'\'\'Slice and Dice\'\'\' uses parallel lines to divide a root into branches (large rectangles). Then they are subdivided into smaller rectangles representing sub-branches again by using parallel lines. At each level of the hierarchy the orientation of the lines is switched (vertical vs. horizontal).\n\n== R Code ==\nImagine you have book A, consisting of 200 pages, which you use in your statistics course. This book contains of 2 main sections: B (80pages) and C (120pages). B section covers topics of Descriptive Statistics and C section covers topics of Inferential Statistics.\n<br>\n<br>\nTopics of B section are: D(30pages) and E(50pages). D is about sample mean and sample standard deviation while E is about Skewness and Kurtosis.\n<br>\n<br>\nTopics of C section are: F(20pages), G(40pages) and H(60pages). F is about Hypothesis Testing, G covers Confidence Intervals and H focuses on Regression Analysis.\n<br>\n<br>\nYou have tree-structured data and want to make a treemap for displaying the constituent sections of book and make comparisons of its\nsmall and large components.\n[[File:Customtreemap.png|300px|thumb|right|Fig.3]]\n<syntaxhighlight lang=""R"">\n#Fig.3\nlibrary(treemap) \ngroup = c(rep(""B"",2), rep(""C"",3)) \nsubgroup = c(""D"",""E"",""F"",""G"",""H"") \nvalue = c(30,50,20,40,60) \ndata= data.frame(group,subgroup,value) \ntreemap(data,index=c(""group"",""subgroup""),\n        vSize = ""value"",\n        palette = ""Set2"",\n        title=""A"",\n        type=""index"",\n        bg.labels=c(""white""),\n        align.labels=list(c(""center"", ""center""), \n                          c(""right"", ""bottom"")))\n</syntaxhighlight>\n\n==References and further reading material==\n# Ben Shneiderman (1992). “Tree visualization with tree-maps: 2-d space-filling approach”. ACM Transactions on Graphics. 11: 92–99.\n# Ben Shneiderman, April 11, 2006, Discovering Business Intelligence Using Treemap Visualizations, http://www.perceptualedge.com/articles/b-eye/treemaps.pdf\n# https://towardsdatascience.com/treemaps-why-and-how-cfb1e1c863e8\n# https://www.nngroup.com/articles/treemaps/\n# https://www.fusioncharts.com/resources/chart-primers/treemap-chart/\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Shahlo Hasanova.']"|0.046948356807511735|1.0
34|Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?|Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.|"['[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""|\'\'\' [[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\'\'\'In short:\'\'\' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Mixed Effects Models until 2020.\'\'\' Search terms: \'Mixed Effects Model\' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher\'s introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher\'s experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: \'\'\'Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables\'\'\'. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. \'\'\'The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.\'\'\' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n\'\'\'The biggest strength of Mixed Effect Models is how versatile they are.\'\'\' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done \'\'better\'\' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. \'\'\'All positive factors\' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.\'\'\' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.\n\nThe greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.', ""While psychology, medicine, agricultural science, biology and later ecology thus thrived in their application of experimental designs and studies, there was also an increasing recognition of information that was creating [[Bias and Critical Thinking|biases]] or otherwise falsely skewed the results. '''The ANOVA hence became amended by additional modifications, ultimately leading to more advanced statistics that were able to focus on diverse statistical effects''', and reduce the influence of skews rooted, for instance, in sampling bias, statistical bias or other flaws. Hence, [[Mixed-Effect Models]] became an advanced next step in the history of statistical models, leading to more complex statistical designs and experiments, taking more and more information into account. In addition, meta-analytical approaches led to the combination and summarising of several case studies into a systematic overview. This was the dawn of a more integrational understanding of different studies that were combined into a [[Meta-Analysis]], taking different contexts of the numerous studies into account as well. In addition, research also focused more and more onto a deeper understanding of individual case studies, with a stronger emphasis on the specific context of the respective case. Such singular cases have been of value in medical research for decades now, where new challenges or solutions are frequently published despite the obvious lack of a wider contribution. Such medical case studies report novel findings, emerging problems or other so far unknown case dynamics, and often serve as a starting point for further research. Out of such diverse origins such as [[System Thinking & Causal Loop Diagrams|system thinking]], Urban Research, [[Ethnography]] and other fields in research, [[Living Labs & Real World Laboratories|real world experiments]] emerged, which take place in everyday social or cultural settings. The rigid designs of laboratory or field experiments is traded off for a deeper understanding of the specific context and case. While real world experiments emerged some decades ago already, they are only starting to gain wider recognition. All the while, the reproducibility crisis challenges the classical laboratory and field experiments, as a wider recognition that many results - for instance from psychological studies - cannot be reproduced. All this indicates that while much of our scientific knowledge is derived from experiments, much remains to be known, also about the conduct of experiments themselves.\n\n----\n[[Category: Normativity_of_Methods]]\n[[Category: Methods]]\n[[Category: Statistics]]\n[[Category: Qualitative]]\n[[Category: Deductive]]\n[[Category: Individual]]\n[[Category: System]]\n[[Category: Global]]\n[[Category: Present]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."", ""The lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.\n\n'''Mixed Effect Models are built on a litany of preconditions, most of which most researchers choose to conveniently ignore.''' In my experience, this is more often than not ok, because it does not matter. Mixed Effect Models are - bless the maximum likelihood estimation - very sturdy. It is hard to find a model that does not have some predictive or explanatory value, even if hardly any pre-conditions are met. Still, this does not mean that we should ignore these. In order to sleep safe and sound at night, I am almost obsessed with model inspection, checking variance across levels, looking at the residuals, and looking for gaps and flaws in the model's fit. We should be really conservative to this end, because by focusing on fixed and random variance, we potentiate things that could go wrong. As I said, more often than not, this is not the case, but I propose to be super conservative when it comes to your model outcome. In order to get there, we need yet another thing: Model simplification.\n\nMixed Effect Models lead the forefront of statistics, and this might be the reason why the implementation of AIC (Akaike Information Criterion) as a parsimony-based evaluation criterion is more abundant here when compared to other statistical approaches. P-values fell widely out of fashion in many branches of science, as did the reporting of full models. Instead, model reduction based on information criteria approaches is on the rise, reporting parsimonious models that honour [[Why_statistics_matters#Occam.27s_razor | Occam's razor]]. Starting with the maximum model, these approaches reduce the model until it is the minimum adequate model - in other words, the model that is as simple as possible, but as complex as necessary. The AIC is about the equivalent of a p-value of 0.12, depending on the sample size, hence beware that the main question may be the difference from the null model. In other words, a model that is better from the Null model, but only just so based on the AIC, may not be significant because the p-value would be around 0.12. This links to the next point: Explanatory power.\n\nThere has been some sort of a revival of r<sup>2</sup> values lately, mainly based on the suggestion of r<sup>2</sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2</sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2</sup>  value, and even have their model selection not independent of the r<sup>2</sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2</sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2</sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2</sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.\n\nAnother very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.""]"|0.007575757575757576|0.0
35|How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?|Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.|"[""The lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.\n\n'''Mixed Effect Models are built on a litany of preconditions, most of which most researchers choose to conveniently ignore.''' In my experience, this is more often than not ok, because it does not matter. Mixed Effect Models are - bless the maximum likelihood estimation - very sturdy. It is hard to find a model that does not have some predictive or explanatory value, even if hardly any pre-conditions are met. Still, this does not mean that we should ignore these. In order to sleep safe and sound at night, I am almost obsessed with model inspection, checking variance across levels, looking at the residuals, and looking for gaps and flaws in the model's fit. We should be really conservative to this end, because by focusing on fixed and random variance, we potentiate things that could go wrong. As I said, more often than not, this is not the case, but I propose to be super conservative when it comes to your model outcome. In order to get there, we need yet another thing: Model simplification.\n\nMixed Effect Models lead the forefront of statistics, and this might be the reason why the implementation of AIC (Akaike Information Criterion) as a parsimony-based evaluation criterion is more abundant here when compared to other statistical approaches. P-values fell widely out of fashion in many branches of science, as did the reporting of full models. Instead, model reduction based on information criteria approaches is on the rise, reporting parsimonious models that honour [[Why_statistics_matters#Occam.27s_razor | Occam's razor]]. Starting with the maximum model, these approaches reduce the model until it is the minimum adequate model - in other words, the model that is as simple as possible, but as complex as necessary. The AIC is about the equivalent of a p-value of 0.12, depending on the sample size, hence beware that the main question may be the difference from the null model. In other words, a model that is better from the Null model, but only just so based on the AIC, may not be significant because the p-value would be around 0.12. This links to the next point: Explanatory power.\n\nThere has been some sort of a revival of r<sup>2</sup> values lately, mainly based on the suggestion of r<sup>2</sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2</sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2</sup>  value, and even have their model selection not independent of the r<sup>2</sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2</sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2</sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2</sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.\n\nAnother very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration."", ""Another very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."", 'Depending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) \'\'\'First of all, is the statistics the right approach to begin with?\'\'\' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem.']"|0.08176100628930817|0.0
36|Why should stepwise procedures in model reduction be avoided?|Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.|"['== \'\'\'Starting to engage with model reduction - an initial approach\'\'\' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam\'s razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam\'s razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis.', '===Challenges===\n* Even if bootstrapping is asymptotically consistent, it does not provide general finite-sample guarantees.\n* The result may depend on the representative sample.\n* The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples).\n* Bootstrapping can be time-consuming.\n* There is a limitation of information that can be obtained through resampling even if number of bootstrap samples is large.\n\n===Normativity===\n\nBootstrap methods offer considerable potential for modelling in complex problems, not least because they enable the choice of estimator to be separated from the assumptions under which its properties are to be assessed.\n\nAlthough the bootstrap is sometimes treated as a replacement for ""traditional statistics"". But the bootstrap rests on ""traditional"" ideas, even if their implementation via simulation is not ""traditional"".\n\nThe computation can not replace thought about central issues such as the structure of a problem, the type of answer required, the sampling design and data quality.\n\nMoreover, as with any simulation experiment, it is essential to monitor the output to ensure that no unanticipated complications have arisen and to check that the results make sense. The aim of computing is insight, not numbers.[2]\n\n==Outlook==\nOver the years bootstrap method has seen a tremendous improvement in its accuracy level. Specifically improved computational powers have allowed for larger possible sample sizes used for estimation. As bootstrapping allows to have much better results with less amount of data, the interest for this method rises substantially in research field. Furthermore, bootstrapping is applied in machine learning to assess and improve models. In the age of computers and data driven solutions bootstrapping has good perspectives for spreading and development.\n\n==Key Publications==\n* Efron, B. (1982) The Jackknife, the Bootstrap, and Other Resampling Plans. Philadelphia: Society for Industrial and Applied Mathematics.\n* Efron, B. and Tibshirani, R. J. (1993) An Introduction to the Bootstrap. New York: Chapman and Hall.\n==References==\n# Dennis Boos and Leonard Stefanski. Significance Magazine, December 2010. Efron\'s Bootstrap.\n# A. C. Davison and Diego Kuonen. Statistical Computing & Statistical Graphics Newsletter. Vol.13 No.1. An Introduction to the Bootstrap with Applications in R\n# Michael Wood. Significance, December 2004. Statistical inference using bootstrap confidence interval.\n# Jeremy Orloff and Jonathan Bloom. Bootstrap confidence intervals. Class 24, 18.05.\n# CRAN documentation. Package ""bootstrap"", June 17, 2019.\n\n==Further Information==\n\nYoutube video. Statquest. [https://www.youtube.com/watch?v=isEcgoCmlO0&ab_channel=StatQuestwithJoshStarmer Bootstrapping main ideas]\n\nYoutube video. MarinStatsLectures. [https://www.youtube.com/watch?v=Om5TMGj9td4&ab_channel=MarinStatsLectures-RProgramming%26Statistics Bootstrap Confidence Interval with R]\n\nYoutube video. [https://www.youtube.com/watch?v=9STZ7MxkNVg&ab_channel=MarinStatsLectures-RProgramming%26Statistics MarinStatsLectures. Bootstrap Hypothesis Testing in Statistics with Example]\n\nArticle. [https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/#:~:text=The%20bootstrap%20method%20is%20a,the%20mean%20or%20standard%20deviation.&text=That%20when%20using%20the%20bootstrap,and%20the%20number%20of%20repeats. A Gentle Introduction to the Bootstrap Method]\n\nArticle. [https://statisticsbyjim.com/hypothesis-testing/bootstrapping/ Jim Frost. Introduction to Bootstrapping in Statistics with an Example]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Andrei Perov.', '\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==Introduction==\nThis is a brief overview of modeling simple initial value problems in Python. Here, we will look into the mathematical concepts and the Python libraries involved. Therefore, a basic understanding of differential equations and programming (Python preferred), would be extremely helpful before diving into the guidelines that follow.\n\n==What are Initial Value Problems (IVP)?==\n\nBefore getting to Initial Value Problems, we describe Ordinary Differential Equations (ODE). An ODE involves unknowns of one(or more) functions of one variable along with its derivatives. A simple example would be:\n\ny\' = 6x - 3\n\nwhere y\' is the first derivative of the function y w.r.t. x. The solution to an ODE is not a numerical value but a set of functions. For the above example, the set of infinite solutions would be:\n\ny = 6x^2/2 - 3x + c\n\nwhere c is a constant. One can differentiate the above expression or integrate the ODE to verify.\n\nAn Initial Value Problem (IVP), is just an ODE with an initial condition, providing the value of the function at some point in the domain. This allows us to achieve a finite set of solutions for the ODE.\n\nFor instance, if we were given values y=4 at x=0 for the above ODE, we can substitute their values in the solution expression to get c = 4 and thus, to a single solution.\n\n==Modelling ODEs==\nODEs are used to model various practical scenarios. Here, we present an instance in the simplest of terms. These events are more complex in reality and require much more detail.\n\nWe take an instance from motorsport of a racecar braking before entering the pit lane. When entering the pit lane, a racecar needs to lower its speed under a designated limit before the entry line. Our aim here would be to predict if a racecar can stay under the regulations for any given speed and distance remaining after braking.\n\nFirst of the variables, we need to consider is the distance \'s\' from the entry line. The change of \'s\' over some interval of time, which is the derivative of \'s\' w.r.t. time, is the velocity.\n\nv = s\'\n\nSimilarly, acceleration is the change in velocity over a certain period.\n\na = v\'\na = s\'\'\n\nWe can update distance by velocity, but we still need to determine how the acceleration will vary through the braking period to update the velocity accordingly.\n\nWe will consider the deceleration of the racecar to be incurred by a combination of the \'bkf\', the braking force of the car, and \'dgf\', the drag which is dependent on the velocity. The forces are negative as they act in the opposite direction.\n\na = -bkf - dgf*v\n\n===SciPy\'s solve_ivp===\nMoving onto the programming part, we discuss SciPy\'s \'solve_ivp\' function. Ensure that you install the [https://scipy.org/install/ SciPy library] in your system before importing the \'solve_ivp\' function as follows.\n\n<syntaxhighlight lang=""Python"" line>\nfrom scipy.integrate import solve_ivp  \n</syntaxhighlight>\n      \nLooking at the [https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html documentation for \'solve_ivp\'], the most important parameter required is the function to update the variables we are interested in. The \'updater\' function below computes the changes in distance and velocity as we modeled before.\n\n<syntaxhighlight lang=""Python"" line>\n# variable update function\ndef updater(t, y):\n\n  bkf = 12 # brakes\n  dwf = 0.3 # drag\n  \n  # variable values for the current iteration\n  s, v = y\n  \n  # computing change in variables for the current iteration\n  dsdt = v\n  dvdt = (-1)*(bkf + dgf*v) if v > 0 else 0\n\n  return dsdt, dvdt\n</syntaxhighlight>\n\nThe functional argument should have the first parameter as the independent variable, which is time in our case. The second parameter is an n-dimensional list containing the current state of all the dependent variables, which are distance and velocity in our case.\n\nThe function should return a list of the same dimension as the second parameter representing the changes in the input variables w.r.t. the independent parameter in respective order.\n\nOther required parameters for \'solve_ivp\' include \'t_span\' & \'t_eval\' which take in the bounds and specific values for computation for the independent variable.\n\n<syntaxhighlight lang=""Python"" line>\n# time bounds for \'t_span\'\nstart, end = 0, 5\n# time step\ndt = 0.25\n# list of desired time values for \'t_eval\' \ntimes = np.arange(start, end + dt, dt)\n</syntaxhighlight>\n\n\'y0\' is another required parameter that takes in initial values for our dependent variables.\n\n<syntaxhighlight lang=""Python"" line>\n# initial values for distance & velocity\ns0 = 0\nv0 = 75\nivs = [s0, v0]\n</syntaxhighlight>\n\nFinally, \'solve_ivp\' is called. The attribute \'y\' from the return object provides the required solution lists in the input order, whereas attribute \'t\' gives you back the inputs provided for \'t_eval\'.\n\n<syntaxhighlight lang=""Python"" line>\nvals = solve_ivp(fun=updater, t_span=(start, end), y0=ivs, t_eval=times)\n# gives you the list of values for the dependent variables\ns, v = vals.y\n# list of values for the independent variable\nt = vals.t\n</syntaxhighlight>\n\n===Visualization with Matplotlib===\nWe make use of the [https://matplotlib.org/ Matplotlib] library for visualizing our models for simplicity. The visualization we are interested in is between the distance and the velocity.\n\nIn addition to plotting \'v\' vs. \'s\', we plot two lines indicating the distance to the pit entry line for the race car, and the velocity it should be under by that point.\n\n<syntaxhighlight lang=""Python"" line>\n# plot v vs. s\nfig, axs = plt.subplots()\naxs.grid(False)\naxs.set_facecolor(\'black\')\naxs.plot(s, v, \'bo-\')\n\n# lines representing pit entry line and speed limit\naxs.plot([100, 100], [v[0], v[-1]], \'r--\', label=\'Pit lane entry\')\naxs.plot([0, max(s[-1], pl_dist + 10)], [20, 20], \'g--\', label=\'Speed limit in pit lane\')\n\naxs.set_xlabel(\'Distance travelled (m)\')\naxs.set_ylabel(\'Velocity (m/s)\')\naxs.set_title(\'Deceleration of a racecar from point of braking\')\nplt.legend()\nplt.show()\n</syntaxhighlight>\n\n[[File:IVP.png]]\n\nOur initial values here for the velocity at the time of braking, the distance from the pit entry line, and the speed limit are 75 m/s, 100 m, and 20 m/s respectively. From the graph, we can conclude, the racecar driver will just be able to slow the car down under the speed limit before entering the pit lane.\n\nSticking to motorsport, one follow-up could be to model lap time degradation, which is crucial for a team to decide when to stop for a tyre change for the optimal race strategy.']"|0.004347826086956522|1.0
37|What are the methods to identify redundancies in data for model reduction?|The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.|"['== \'\'\'Starting to engage with model reduction - an initial approach\'\'\' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam\'s razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam\'s razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis.', '\'\'\'Check for missing values\'\'\'\nTo check if the data has any missing values, we can use the isnull and any methods from Pandas, like this:\n<syntaxhighlight lang=""Python"" line>\nprint(data.isnull().any())\n</syntaxhighlight>\n\nThis will return a table with a True value for each column that has missing values, and a False value for each column that does not have missing values.\nChecking for missing values is important because most modeling techniques cannot handle missing data. If your data contains missing values, you will need to\neither impute the missing values (i.e. replace them with estimated values) or remove the rows with missing values before fitting a model.\nIf there are missing values in the data, you can use the sum method to check the number of missing values per column, like this:\n\n<syntaxhighlight lang=""Python"" line>\nprint(data.isnull().sum())\n</syntaxhighlight>\n\nAlternatively, you can use the shape attribute to calculate the percentage of missing values per column, like this:\n\n<syntaxhighlight lang=""Python"" line>\nprint(data.isnull().sum()/data.shape[0]*100)\n</syntaxhighlight>\n\nThis can help you understand the extent of the missing values in your data and decide how to handle them.\n\n===Check for duplicate entries===\nTo check if there are duplicate entries in the data, we can use the duplicated method from Pandas, like this:\n\n<syntaxhighlight lang=""Python"" line>\nprint(data.duplicated())\n</syntaxhighlight>\n\nThis will return a True value for each row that is a duplicate of another row, and a False value for each unique row.\nIf there are any duplicate entries in the data, we can remove them using the drop_duplicates method, like this:\n\n<syntaxhighlight lang=""Python"" line>\ndata = data.drop_duplicates()\n</syntaxhighlight>\n\nThis will return a new dataframe that contains only unique rows, with the duplicate rows removed. This can be useful for ensuring that the data is clean and ready for analysis.\n\n\'\'\'Short introduction to data visualization\'\'\'\nData visualization can be a powerful tool for inspecting data and identifying patterns, trends, and anomalies. It allows you to quickly and easily explore the data, and get insights that might not be immediately obvious when looking at the raw data.\nFirst, we start by getting all columns with numerical data by using the select_dtypes method and filtering for in64 and float64:\n\n<syntaxhighlight lang=""Python"" line>\n# get all numerical data columns\nnumeric_columns = data.select_dtypes(include=[\'int64\', \'float64\'])\nprint(numeric_columns.shape)\n# Print the numerical columns\nprint(numeric_columns)\n\n===Boxplot===\nThey are particularly useful for understanding the range and interquartile range of the data, as well as identifying outliers and comparing data between groups.\nIn the following, we want to plot multiple boxplots using the subplots method and the boxplot method:\n\n<syntaxhighlight lang=""Python"" line>\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a boxplot for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i // 3, i % 3]\nax.boxplot(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n[[File:Boxplot inspection.PNG|centre right]]\n\n===Histogram===\nHistograms are a type of graphical representation that shows the distribution of a dataset. They are particularly useful for understanding the shape of a distribution and identifying patterns, trends, and anomalies in the data.\nIn the following, we want to plot multiple histograms using the subplots method and the hist method:\n\n<syntaxhighlight lang=""Python"" line>\n12/23/22, 4:25 PM Data_Inspection_ASDA_Wiki - Jupyter Notebook\nlocalhost:8888/notebooks/1000/Data_Inspection_ASDA_Wiki.ipynb 9/9\nIn [15]:\nAt this point, we have most of the information needed to start the Data Cleaning process.\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a histogram for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i // 3, i % 3]\nax.hist(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Hist inspection.PNG|centre right]]\n\nAt this point, we have most of the information needed to start the data-cleaning process.\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is Sian-Tang Teng. Edited by Milan Maushart', '[[File:scatter plot.png|700px|center|]]\n<small>Figure 2: Scatter plot visualizing the number of times a certain frequency of electricity usage occurred</small>\n\n==Statistical Analysis==\nThere are a multitude of statistical analysis methods that can be used to analyze time series data. While simple statistical tests like the t-test, ANOVA, and regression analysis can be used with time series data to identify relationships and dependencies in the data, there are also more specific methods to analyze time series data.\n\n===Decomposition===\nTime series data and be decomposed into three components: trend, seasonality, and residuals. The trend represents the long-term direction of the data, the seasonality represents the repeating patterns in the data, and the residual represents the noise in the data. Decomposing the data in this way can help to better understand the underlying structure of the data and identify any patterns or trends.\n\n<syntaxhighlight lang=""Python"" line>\nfrom statsmodels.tsa.seasonal import seasonal_decompose ## imports the necessary functions from statsmodel\n\ndecomp = seasonal_decompose(df[\'usage_kwh\'], model=\'additive\', period=15)\n## creates decomp which is the result of decomposing the electricity ##usage data to analyze seasonality, trend, and residuals. ""Period= ##15"" indicates that data was collected every 15 minutes and ##""model=addtiive"" makes the assumption that the three components add ##up linearly.\n\ndf1 = pd.DataFrame({\'value\': decomp.trend, \'name\': \'trend\'}) ## analyzes trend, so long-term direction\ndf2 = pd.DataFrame({\'value\': decomp.seasonal, \'name\': \'seasonal\'}) ##analyzes seasonal changes, so repeating patterns\ndf3 = pd.DataFrame({\'value\': decomp.resid, \'name\': \'resid\'}) ## analyzes residuals, so noise\n\nfull_df = pd.concat([df1, df2, df3]) ## combines all three data frames\nfig = px.line(full_df, y=\'value\', facet_row=\'name\', title=\'Decomposition of electricity usage\') ## creates plot\nfig.update_layout(height=1000) ## adapts height so that all three plots can be seen\n</syntaxhighlight>\n\n\n[[File:decomposition 1.png|700px|center|]]\n[[File:decomposition 2.png|700px|center|]]\n[[File:decomposition 3.png|700px|center|]]\n<small>Figure 3: The graphs for the decomposition of trend, seasonality, and the residuals respectively.</small>\n\nNo long-term direction or seasonal patterns can be detected. But we can see that there is a large variance in the residuals, so quite a lot of noise.\n\n===Moving Average===\nWhen there is a lot of noise in the data, it can be useful to calculate the moving (or rolling) average. The moving average is a statistical measure that calculates the average of a window around a data point. It smooths out the data and helps identify patterns or trends that may not be immediately apparent.\n\n<syntaxhighlight lang=""Python"" line>\ndf_full = pd.concat([\n    pd.DataFrame({\'value\': df[\'usage_kwh\'], \'name\': \'raw\'}), ## creates a dataframe the actual residuals\n    pd.DataFrame({\'value\': df[\'usage_kwh\'].rolling(window=24*4).mean(), \'name\': \'day\'}) ## creates a dataframe with the daily means (1-day time window, 24 hours times 4 measurements per hour (every 15 min)\n]) ##the concat command combines these two dataframes\npx.line(df_full, y=\'value\', color=\'name\',\n        title=\'Usage vs. a 1-day moving average of usage\',\n        labels={\'start_date\': \'Date\', \'value\': \'Usage (KWh)\'})\n</syntaxhighlight>\n\n[[File:MA plot.png|700px|center|]]\n<small>Figure 4: Electricity Usage with and without rolling average calculations</small>\n\nWe can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.api as sm ## needed to use the autocorrelation function\nautocorr = sm.tsa.acf(df[\'usage_kwh\'], nlags=24*4*2)## determines #autocorrelation with a lag of 15 minutes over 2 days (24 hours * 4 (every 15 min) * 2 for two days) \nsteps = np. arange (0, len(autocorr) * 15, 15) / 60 ## produces an ##array of the length of the autocorrelation data times 15 (so per #minute) and indicates that each lag is 15 minutes apart. By dividing it by 60, the values are converted from minutes to hours.\npx.line(x=steps, y=autocorr, markers=True,\n        title=\'Autocorrelation of electricity usage\',\n        labels={\'x\': \'Lag (hours)\', \'y\': \'Correlation\'}) ## creates #plot of the autocorrelation function\n</syntaxhighlight>\n[[File:ACF plot.png|700px|center|]]\n<small>Figure 5: Autocorrelation of electricity usage over two days</small>\n\nThe autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df[\'usage_kwh\'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel(\'Lag (hours)\') ## title x axis\nplt.ylabel(\'Autocorrelation of electricity usage with confidence interval\') ## title y axis\nplt.title(\'\') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n</syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days</small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.']"|0.004464285714285714|1.0
38|How are 'narratives' used in Narrative Research?|'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.|"['[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Narrative Research describes qualitative field research based on narrative formats which are analyzed and/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Narrative Research until 2020.\'\'\' Search terms: \'Narrative Research\', \'narrative inquiry\', \'narrative analysis\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years\'\'\' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). \'Storytelling\' is defined as the distinct tonality, format and presentation in which a story is told. The term \'narrative\' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term \'narrative\' has been used used predominantly as a synonym for \'story\' in academia for decades (Barrett & Stauffer 2009).\n\n\'\'\'Psychologist Jerome Bruner introduced the notion of \'narrative\' as being one of two forms of distinct modes of thinking in 1984\'\'\' - the other being the \'logico-scientific\' mode (Barrett & Stauffer 2009). While the latter is ""(...) more concerned with establishing universal truth conditions"" (Barrett & Stauffer 2009, p.9), the \'narrative\' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that \'narratives\' are a central form of human learning about - and [[Glossary|sense-making]] of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.\n\n\'\'\'As a scientific method, Narrative Research - often just phrased \'narrative\' - is a rather recent phenomenon\'\'\' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of \'narrative\' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).\n\n== What the method does ==\n\'\'\'First, there is a distinction to be made:\'\'\' \'Narrative\' can refer to a form of Science Communication, in which research findings are presented in a story format (as opposed to classical representation of data) but not extended through new insights. \'Narrative\' can also be understood as a form of scientific inquiry, generating new knowledge during its application. This entry will focus on the latter understanding.\n\n\'\'\'Next, it should be noted that Narrative Research entails different approaches\'\'\', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.\n\nNarrative Research -  or \'Narrative Inquiry\' - is shaped by and focussing on a conceptual understanding of \'narratives\' (Barrett & Stauffer 2009, p.15). Here, \'narratives\' are seen as a format of [[Glossary|communication]] that people apply to make sense of their life experiences. ""Communities, social groups, and subcultures tell stories with words and meanings that are specific to their experience and way of life. The lexicon of a social group constitutes its perspective on the world, and it is assumed that narrations preserve particular perspectives in a more genuine form"" (Jovchelovitch & Bauer 2000, p.2). \'\'\'Narratives are therefore not merely forms of representing a chain of events, but a way of making sense of what is happening.\'\'\' Through the telling of a story, people link events in meaning. The elements that are conveyed in the story, and the way these are conveyed, indicates how the story-teller - and/or their social surrounding - sees the world. They are a form of putting reality into cultural and individual perspective. Also, narratives are never final but change over time as new events arise and perspectives develop (Jovchelovitch & Bauer 2000, Webster & Mertova 2007, Squire et al. 2014, Moen 2006). \n\nNarrative Research is ""(...) the study of stories"" (Polkinghorne 2007, p.471) and thus ""(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience."" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the \'narratology\', i.e. the structure and grammar of a story; the \'narrative content\', i.e. the themes and meanings conveyed through the story; and/or the \'narrative context\', which revolves around the effects of the story (Squire et al. 2014).', '\'\'\'One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.\'\'\' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, \'narrative\' is the underlying ""frame of reference"" (Moen 2006, p.57) for the research. An example for the latter understanding is the \'Narrative Interview\'.\n\n==== Narrative Interviews ====\n[[File:Narrative Research Phases.png|400px|thumb|right|\'\'\'Basic phases of the narrative Interview.\'\'\' Source: Jovchelovitch & Bauer 2000, p.5.]]\n\nThe Narrative Interview is an Interview format that ""(...) encourages and stimulates an interviewee (...) to tell a story about some significant event in their life and social context."" (Jovchelovitch & Bauer 2000, p.3). \'Narrative Interviewing\' ""is considered a form of unstructured, in-depth interview with specific features."" (Jovchelovitch & Bauer 2000, p.4) To this end, it is a form of the [[Open Interview]], which - compared to [[Semi-structured Interview|Semi-structured]] and [[Survey|Structured Interviews]] - is relatively free from deductively pre-developed question schemata: ""To elicit a less imposed and therefore more \'valid\' rendering of the informant\'s perspective, the influence of the interviewer should be minimal (...) \'\'\'The [Narrative Interview] goes further than any other interview method in avoiding pre-structuring the interview.\'\'\' (...) It uses a specific type of everyday communication, namely story-telling and listening, to reach this objective."" (Jovchelovitch & Bauer 2000, p.4) Here, it is central that the interviewer appreciate the interviewee\'s perspective, by using the subject\'s language and by posing ""as someone who knows nothing or very little about the story being told, and who has no particular interests related to it"" (ibid, p.5). The interview is then transcribed and analyzed using different forms of coding (see Content Analysis), with a focus on the narrative elements. For more information on the methodological foundations of conducting and analyzing narrative Interviews, please refer to Jovchelovitch & Bauer 2000.\n\n==== Narrative Inquiry as collaborative story-creation ====\nIn a third understanding of Narrative Research, which is most commonly referred to as \'Narrative Inquiry\', \'narratives\' are more than the underlying phenomenon - they are a central methodological element. \'\'\'\'This approach dissolves the barrier between researcher and subject further, and the collaboration between both is central to the methodological design\'\'\' (Clandinin 2006, Moen 2006). This type of narrative research does not apply an \'outsider\'s perspective\', but instead is ""(...) collaboration between researcher and participants, over time, in a place or series of places, and in social interaction with milieus. An inquirer enters this matrix in the midst and progresses in the same spirit, concluding the inquiry still in the midst of living and telling, reliving and retelling, the stories of the experiences that made up people\'s lives, both individual and social."" (Clandinin 2006, p.20, citing Clandinin & Connelly 2000). To this end, Clandinin (2006) distinguishes between the re-telling of stories that Interview participants tell the researchers, and the telling of stories that the researchers experience themselves, e.g. in ethnographic studies. The difference is not so much of methodological nature as it is in the purpose and perspective of the research (Barrett & Stauffer 2009). In this understanding, \'Narrative Inquiry\' is a reflexive and iterative process, with the researchers entering into a field of experiences, telling their own stories, telling the stories of other participants, and dialogically co-creating joined stories with them (Moen 2006). The created narratives serve as a way of presenting the research experiences, but also as forms of data for the analysis of the joint and conveyed experiences.\n\nIn terms of practical methodology, this form of narrative inquiry is very closely related to methods of [[Ethnography]], which are based on the active [[Glossary|participation]] in the field, i.e. the social situation of interest, and the creation of [[:Category:Qualitative|qualitative]] data in form of field notes. The distinctive component of Narrative Inquiry is the focus on narratives. Clandinin (2006, p.47f) explains the methodological approach as following: ""As we enter into narrative inquiry relationships, we (...) negotiate relationships, research purposes, transitions, as well as how we are going to be useful in those relationships. These negotiations occur moment by moment, within each encounter, sometimes in ways that we are not awake to"" or ""in intentional, wide awake ways as we work with our participants throughout the inquiry. As we live in the field with our participants, whether the field is a classroom, a hospital room or a meeting place where stories are told, we begin to compose (...) a range of kinds of field texts from photographs, field notes, and conversation transcripts to Interview transcripts. As narrative inquirers work with participants we need to be open to the myriad of imaginative possibilities for composing field texts. (...)  As we continue to negotiate our relationships with participants, at some points, we do leave the field to begin to compose research texts. This leaving of the field and a return to the field may occur and reoccur as there is a fluidity and recursiveness as inquirers compose research texts, negotiate them with participants, compose further field texts and recompose research texts."" Data may be gathered in form of ""field notes; journal records; interview transcripts; one\'s own and other\'s observations; storytelling; letter writing; autobiographical writing; documents (...); and pictures"" (Moen 2006, p.61) and analyzed by forms of Content Analysis.', '== Strengths & Challenges ==\n* Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000)\n* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research ""(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates"". Further, ""Narrative inquiry attempts to capture the \'whole story\', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important \'intervening\' stages"" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).\n* \'\'\'The insights gained through narratives are subjective to the narrator, which implies advantages and challenges.\'\'\' Compared to an \'objective\' description of, e.g. a chain of events, the narration provides insights about the individual\'s interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: ""Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived."" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the \'real\' world. Squire et al. (2014) refer to this distinction as different forms of \'truth\' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical \'truth\'. They should try ""(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather."" Moen (2006, p.63), by comparison, explains that ""(...) there is no static and everlasting truth"", anyway.\n* This [[Bias and Critical Thinking|conflict between different \'truths\']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, \'usefulness\' and \'persuasiveness\' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: ""Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be \'well grounded\' and \'supportable\' (...) Narrative research does not claim to represent the exact \'truth\', but rather aims for \'verisimilitude\'"". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a \'trustworthy\' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one\'s research results (see Moen 2006, p.64). Also, researchers ""(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts"" (Polkinghorne 2007, p.484).\n* Further challenges may arise during the active collaboration of the researcher in the field. For example, ""(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher"" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]].\n\n\n== Normativity ==\n* As mentioned before, Narrative Inquiry methodology is in many regards similar to methods in [[Ethnography]], and shares elements with [[Hermeneutics]], [[Content Analysis|Qualitative Content Analysis]], and [[Open Interview|Open Interviews.]] Generally, it can be seen as a purely qualitative and inductive approach that focuses on limited temporal and spatial scales.\n* ""(...) [A] distinguishing feature of Narrative Research is a ""(...) move away from an objective conception of the researcher-researched relationship"" (...) to one in which the researcher is deeply involved in the research relationship. (...) In this process, narrative inquiry, becomes to varying degrees a study of self, or self alongside others, as well as of the inquiry participants and their experience of the world."" (Barrett & Stauffer 2009, p.11f, quote from Pinnegar & Daynes 2007, p.11). \'\'\'The autobiography of the researcher, as well as personal beliefs and practices and ethical positionality is brought into the research endeavor, which should be acknowledged in the process and results.\'\'\' ""Narrative inquirers cannot bracket themselves out of the inquiry but rather need to find ways to inquire into participants’ experiences, their own experiences as well as the co-constructed experiences developed through the relational inquiry process."" (Clandinin 2006, p.47)\n* Within this interaction in the field, the researcher must be aware of the consequences of creating new narratives in the studied subjects and situations, which is why Clandinin (2006, p.53) mentions ""(...) the importance of thinking in responsive and responsible ways about how narrative inquiry can shift the experiences of those with whom we engage"". In addition, Moen (2006, p.62) mentions that the narratives written down as a result of the research inquiry are themselves subject to interpretation: ""The story has been liberated from its origin and can enter into new interpretive frames, where it might assume meanings not intended by the persons involved in the original event. (...) [T]he narrative that is fixed in a text is thus considered an “open work” where the meaning is addressed to those who read and hear about it. Looking on narrative as an open text makes it possible to engage in a wide range of interpretations"".\n\n== Outlook ==\nBarrett & Stauffer (2009, p.16) claim that ""[n]arrative inquiry is still in its early stages of development (...). It will be subject to contestation over the years as the methodology develops, and other pathways are marked out."" This can be substantiated by the dispersed literature with its diverse understandings (see References), and the rather recent development of the discourse around narratives.\n\n\n== Key Publications ==\nVeroff et al. 1993. \'\'NEWLYWEDS TELL THEIHR STORIES: A NARRATIVE METHOD FOR ASSESSING MARITAL EXPERIENCES.\'\' Journal of Social and Personal Relationships 10. 437-457.\n* An example study that applied Narrative Interviews.']"|0.08256880733944955|1.0
39|What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?|Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n\'\'\'Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.\'\'\' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.', 'Depending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to ""rule out"", the answer to this question is \'yes\'.\n\n\n= Multivariate statistics =\n\'\'\'You are dealing with Multivariate Statistics.\'\'\' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n\'\'\'You are doing an ordination.\'\'\' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as \'discrete\' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as \'integer\' in R, i.e. it exists in full numbers, and continuous data is \'numeric\' - it has a comma.\n\n\n== Cluster Analysis ==\n\'\'\'So you decided for a Cluster Analysis - or Classification in general.\'\'\' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n\'\'\'How do I know?\'\'\'\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n\'\'\'You have decided to do a Network Analysis.\'\'\' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n\'\'\'How do I know what I want?\'\'\'\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) \'\'\'First of all, is the statistics the right approach to begin with?\'\'\' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem.', '[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""|\'\'\' [[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\'\'\'In short:\'\'\' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Mixed Effects Models until 2020.\'\'\' Search terms: \'Mixed Effects Model\' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher\'s introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher\'s experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: \'\'\'Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables\'\'\'. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. \'\'\'The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.\'\'\' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n\'\'\'The biggest strength of Mixed Effect Models is how versatile they are.\'\'\' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done \'\'better\'\' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. \'\'\'All positive factors\' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.\'\'\' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.\n\nThe greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.']"|0.005|0.0
40|What are the three conditions under which Poisson Distribution can be used?|Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.|"['==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, λ, its average value is λ.\n\n2. Variance of the distribution is also λ, implying when λ increases the width of the distribution goes as square root lambda √λ.\n\n3. A converse in Raikov’s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.\n\nConsider the example of radioactive decay for long-lived isotopes, in a radioactive sample containing a large number of nuclei, each of which has a tiny probability of decaying during some time interval, T. Let’s say the rate of decay is 0.31 decay/second and is monitored for 10 seconds. That gives us the λ1 0.31 * 10 = 3.1 which means the probability equals,\n[[File:3 equation.PNG|center]]\nConsider another example where λ2 is 2.7, the probability is\n[[File:4 equation.PNG|center]]\n\nTherefore,\n\'\'\'If we look at the total number k = k1 + k2 of a radioactive decay in a time T, the result is also a Poisson distribution with λ = λ1 + λ2 -> λ = 3.1 + 2.7 = 5.8 : P(k, λ1 + λ2 )\'\'\'\n[[File:5 equation.PNG|center]]\n\n\'\'If we have two independent Poisson-distributed variables, their sum is Poisson distributed too.\'\'\n\n4. The skewness is measured by 1/√λ\n\n5. Excess kurtosis is measured by 1/λ. See the difference between excess kurtosis and kurtosis [https://www.investopedia.com/terms/e/excesskurtosis.asp here]. In a nutshell, excess kurtosis compares the kurtosis of the distribution with the kurtosis of a normal distribution and can therefore tell you if an (extreme) event is more or less likely in this case than if the distribution followed a normal distribution.\n\n6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> ∞ and λ = np, rate parameter, λ is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean λ > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = λ = μ =45\n\nStandard deviation = ∂ = √λ = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https://builtin.com/data-science/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=""Python"" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n</syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=""Python"" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n</syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1\n\n<syntaxhighlight lang=""Python"" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align=\'center\')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n</syntaxhighlight>\n[[File: plot 1.png|center|700px]]', '<syntaxhighlight lang=""Python"" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align=\'center\')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n</syntaxhighlight>\n[[File: plot 1.png|center|700px]]\n\nThe resulting pmf confirms that the closest integer value to lamba i.e., 3 has the most number of years out of 10,000 meaning most years will have 3 duststorms. You can also see that the data is slightly skewed to the right since there is a larger variance to the right than to the left. Looking at the distribution, it looks fairly normally distributed. However, the low lambda does not allow to use the Poisson distribution as an approximation for a normal distribution. Most probably, the large dataset allows us to see it as a normal distribution, since most distributions converge to a normal distribution with increasing sample size.\n==6. References==\n* Brownlee, Jason: ""A Gentle Introduction to Probability Distributions"", 14.11.2019. Retrieved from: https://machinelearningmastery.com/what-are-probability-distributions/#:~:text=A%20probability%20distribution%20is%20a,properties%20that%20can%20be%20measured, last checked: 21.05.2023\n* Koehrsen, Will: "" The Poisson Process and Poisson Distribution Explained (With Meteors!), 28.10.2022. Retrieved from: https://builtin.com/data-science/poisson-process, last checked: 21.05.2023\n* Papoulis, Athanasios; Pillai, S. Unnikrishna: ""Probability, Random Variables, and Stochastic Processes"" (4th ed.).\n* Raikov, Dmitry (1937): ""On the decomposition of Poisson laws"". Comptes Rendus de l\'Académie des Sciences de l\'URSS. 14: 9–11.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart', 'THIS ARTICLE IS STILL IN EDITING MODE\n==1. Introduction to Probability Distribution==\nA probability distribution summarizes the probabilities for the values of a random variable. The general properties/features that define a distribution include the four mathematical moments: \n\n1.\t\'\'\'Expected value E(X)\'\'\': This is the outcome with the highest probability or likelihood or an average or mean value for the variable X.\n \n2.\t\'\'\'Variance Var(X)\'\'\': Variance denotes the spread of the values from the expected value. The standard deviation is the normalized value of variance obtained by taking a square root of the variance. The covariance summarizes the linear relationship for how the two variables change with respect to each other. \n\n3.\t\'\'\'Skewness\'\'\': Skewness is the measure of asymmetry of a random variable X about the mean E(X) of a probability distribution. It can be positive (tail on the right side), negative (tail on left side), zero (balanced tail on both sides) or undefined. Zero skewness does not always mean symmetric distribution as one tail can be long and thin and the other can be short and fat. \na\n4.\t\'\'\'Kurtosis\'\'\': Kurtosis is the measure of ‘ *peaked-ness* ’ of distribution. It can be differentiating tool between distributions that have the same mean and variance. Higher kurtosis means a peak and fatter/extended tails. \n\nEach random variable has its own probability distribution that may have a similar shape to other variables, however, the structure will differ based on whether the variable is discrete or continuous, since probability distributions of continuous variables have an infinite number of values and probability functions of discrete variables assign a probability to each concrete data point. \n\n==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=""wikitable""\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}\n\nPoisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https://studiousguy.com/poisson-distribution-examples/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===\n\n\'\'The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time.\'\'\n\n[[File:1 equation.PNG|center|250px]]\nT = Time interval \ne = natural logarithmic base \nk = number of events \nThe K! means K factorial. This means that we multiply all integers from K down to 1. Say K is 4 then K!= 4* 3* 2* 1= 24\n\nIntroducing lambda, λ, as a rate parameter (events/time)*T into the equation gives us \n[[File:2 equation.PNG|center]]\n\nSo, λ is basically the expected number of event occurrences in an interval, a function of \'\'\'number of events\'\'\' and the \'\'\'time interval\'\'\'. Changing λ means changing the probability of event occurrence in one interval.\n\nFor example, Ladislaus Bortkiewicz calculated the deaths of soldiers by horse kicks in the Prussian Army using Poisson Distribution in the late 1800s. He analyzed two decades of data from up to 10 army corps, equivalent to two centuries of observations for one army corps.\n[[File:Horse.png|center|500px]]\nFigure 1: Poisson Distribution for deaths by horse kick by Ladislaus Bortkiewicz. Source: [https://www.scribbr.com/statistics/poisson-distribution/#:~:text=You%20can%20use%20a%20Poisson,days%20or%205%20square%20inches. Scribbr]\n\nAccording to his observation, an average of 0.61 soldiers died every year. However, the deaths were random, for example, in one year four soldiers died and for most years no deaths were caused by horses. Figure 1 shows the probability mass function graph.\nIn Poisson Distribution terms,\n* Death caused by a horse kick is an ‘event’\n* Mean per time interval, represented by λ, is 0.61\n* The number of deaths by horse kick in a specific year is k.\n\n[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (λ) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to λ. The Lamda, λ, also represents the mean and variance of the distribution.\n\n==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, λ, its average value is λ.\n\n2. Variance of the distribution is also λ, implying when λ increases the width of the distribution goes as square root lambda √λ.\n\n3. A converse in Raikov’s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.']"|0.02577319587628866|1.0
41|How does the Pomodoro technique work?|The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don\'t want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n\'\'\'Pomodoro is very simple. All you need is work to be done and a timer.\'\'\'  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to \'\'25 minutes = 1 ""Pomodoro""\'\').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15–30 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https://play.google.com/store/apps/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https://apps.apple.com/us/app/focus-keeper-time-management/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\'\'\'Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique\'\'\' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe \'\'\'Dreamers\'\'\'...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: \'\'Which ideas come to mind? What would be an ideal solution to the problem?\'\'\n\nThe \'\'\'Realists\'\'\'...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: \'\'How does the idea feel? How could it be implemented? Who should do it and at what cost?\'\'\n\nThe \'\'\'Critics\'\'\'...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: \'\'What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?\'\'\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. \'\'\'The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.\'\'\' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to \'neutralize\' their minds) in order to understand the other roles\' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n\'\'Sources:\'\'\n* Tools Hero - [https://www.toolshero.com/creativity/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https://arbeitdigital.de/wirtschaftslexikon/kreativitaetstechniken/walt-disney-methode/ Walt-Disney-Methode]\n* Karrierebibel - [https://karrierebibel.de/disney-methode/ Disney Methode: Denkblockaden überwinden]\n* Impulse - [https://www.impulse.de/management/selbstmanagement-erfolg/walt-disney-methode/3831387.html Walt Disney Methode]\n\n[https://www.youtube.com/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in \'Saving Mr. Banks\', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.', '{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || \'\'\'[[:Category:Software|Software]]\'\'\' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nMiro is an online collaboration tool which allows you to hold virtual workshops, develop common ideas and designs, have a digital workspace or design presentations in a different way.\n\n== Goals ==\n* Work better together online\n* Integrate many different functions (project management, online collaboration, presentation) into one\n\n== Getting started ==\nMiro is an online-tools which allows you to collaborate with others (but also work by yourself). \'\'\'It is basically an infinite canvas onto which you can put all kinds of elements:\'\'\' shapes, text, videos, documents, interactive elements such as a [[Kanban]] Board or sticky notes. As collaboration happens in real-time, it can very much substitute the classical whiteboard or brown-paper and allow you to collaboratively brainstorm ideas, design a presentation, or manage your team.\n\nWe recommend using it for digital workshops, organizing your teamwork or as an alternative to traditional presentation formats such as Prezi, PowerPoint or Keynote.\n\nIt can be a bit overwhelming at first, but once you get the hang of it, it becomes really natural to use and hopefully will make your teamwork more productive and fun.\n\nTo get started, create an account on [https://miro.com the Miro website]. If you are eligible (e.g. when you\'re a member of Leuphana University), you can apply for a free Miro Education account which comes with all the premium features [https://miro.com/education-whiteboard/ here].\n\n== Links & Further Reading ==\nHere\'s a video that covers (almost) everything you need to know to get started working with Miro:\n{{#evu:https://www.youtube.com/watch?v=pULLAEmhSho\n|alignment=center\n}}\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.']"|0.06474820143884892|0.8
42|What is the 'curse of dimensionality'?|The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.|"['== Ordinations ==\n\nOrdination techniques evolved already more than a century ago in mathematics, and allowed fro a reduction of information that makes these analysis approaches timely up until today. Ordination techniques rely strongly on a profound knowledge of the underlying data format of the respective dataset that is being analysed. Since ordination allow for both inductive and deductive analysis, they often pose a risk for beginners, who typically get confused by the diversity of approaches and the answers these analysis may provide. This conduction is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques. What is more, ordinations are often deeply entangled in disciplinary cultures, with some approaches such as factor analysis being almost exclusive to some disciplines, and other approaches such as principal component analysis being utilised in quite diverse ways within different disciplines. This makes norms and rules when and how to apply different techniques widely scattered and intertwined with disciplinary norms, while the same disciplines are widely ignorant about other approach from different disciplines. Here, we try to diver a diverse and reflected overview of the different techniques, and what their respective strengths and weaknesses are. This will necessary demand a certain simplification, and will in addition trigger controversy within certain branches of science, and these controversies are either rooted in partial knowledge or in experiential  identity. Unboxing the whole landscape of ordinations is also a struggle because these analysis are neither discrete nor always conclusive. Instead they pose starting points that often serve initial analysis, or alternatively enable analysis widely uncoupled from the vast landscape of univariate statistics. We need to acknowledge to this end that there is a vast difference between the diverse approaches not only in the underlying mathematics, yet also how these may be partly ignored. This is probably the hardest struggle that you can fin neither in textbooks nor in articles. The empirical reality is that many applications of ordinations violate much of the mathematical assumptions or rules, yet the patterns derived from these analyses are still helpful if not even valid. Mathematicians can choose to live in a world where much of ordination techniques is perfect in every way, yet the datasets the world gives to ordinations are simply not. Instead, we have to acknowledge that multivariate data is almost always messy, contains a high amount of noise, many redundancies, and even data errors. Safety comes in numbers. Ordinations are so powerful exactly because they can channel all these problems through the safety of the size of the data, and thus derive either initial analysis or even results that serve as endpoints. However, there is a difference between initial or final results, and this will be our first starting point here.\n\nOrdination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through experience can you pick what serves your dataset best. This is associated to the reality that many analysis made with ordinations are indeed compromises. Ecology and psychology are two examples of disciplines why imagined ordinations deeply enough into the statistical designs to derive datasets where more often than not assumptions for statistical analysis of a respective ordination are met. However, many analyses based on ordinations are indeed compromises, and from a mathematical standpoint are real world analysis based on ordinations a graveyard of mathematical assumptions, and violation of analytical foundations that borderline ethical misconduct. In other words, much of ordinations are messy. This is especially true because ordinations are indeed revealing mostly continuous results in the form of location on ordination axes. While multivariate analyis based on cluster analysis are hence more discrete through the results being presented as groups, ordinations are typically nice to graphically inspect, but harder to analytical embedded into a wider framework. More on this point later. Let us now begin with a presentation of the diverse ordination types and their respective origins. \n\n=== Correspondence analysis ===\n\nThis ordination is one of the most original ordination techniques, and builds form its underlying mechanics on the principal component analysis. However, since it is based on the chi square test, it is mainly applied for categorical data, although it can also be applied to count data, given that the dataset contains enough statistical power for this. In a nutshell, the correspondence analysis creates orthogonal axis that represent a dimension reduction of the input data, thereby effectively reducing the multivariate categorical data into artificial exes, out of which the first contains the most explanatory power. Typically, the second and third axis contain still meaningful information, yet for most datasets the first two axis may suffice. The correspondence analysis is today mostly negotiable in terms of its direct application, yet serves as an important basis for other approaches, such as the Detrended Correspondence analysis or the Canonical Correspondence analysis. This is also partly related to the largest flaw in the Correspondence analysis, namely the so called Arch-effect, where information on the first two axis is skewed due to mathematical representation of the data. Still, the underlying calculation, mainly the reciprocal averaging approach make it stand out as a powerful tool to sort large multivariate datasets based on categorical or count data. Consequently, the basic reciprocal averaging was initially very relevant for scientific disciplines such as ecology and psychology. \n\n=== Detrended Correspondence analysis ===', 'Finally, we look at the biplot of the analysis. This reveals the underlying patterns of the dataset:\n\n<syntaxhighlight lang=""R"" line>\nfviz_pca_biplot(data.pca, label = ""var"", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlot.png|center|500px]]\n\nThe two axes of this plot are the newly created PCs. There are two main part of information presented on the plot:\n\n* The arrows show how the original variables correlate with the two PCs, and in turn, with each others. For example, from the way the moisture arrow presents itself we can infer a strong negative correlation of the variable and the second PC. Fat and sodium have a very strong positive correlation, and the more carbohydrates a pizza contains, the less protein it has. Adding argument <code>label = ""var""</code> in the function allows for the variable names to be printed.\n* The points show how each individual pizza is plotted on this new coordinate system. Here, we go a step further and grouping those pizza under different brands (a categorical variable) using the arguement <code>habillage = data$brand</code>. By doing this, we unearth additional information about those brands. For example:\n** Brand A typically produce pizzas with a high level of fat and sodium.\n** Pizzas from brand B, C, and D are rich in proteins and ash, as opposed to pizza from brand E, F, G, H which are high in carbohydrates.\n** If you favorite pizza brand F goes out of business (for whatever reason), a pizza from brand E, G or H would be a good substitute in terms of nutritional value.\n\n=== R Example: Is standardization that important? ===\nTo answer this question, let us try an alternative scenario, where we conduct PCA without centering and scaling the variables.\n\n<syntaxhighlight lang=""R"" line>\ndata.pca <- data %>% select(mois:cal) %>% prcomp(scale = FALSE)\nfviz_pca_biplot(data.pca, label = ""var"", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlotNoScale.png|center|500px]]\n\n[[File: PCA_ContribPlotNoScale.png|center|500px]]\n\nSuddenly all that matters are only the carbohydrate, moisture and fat level of the pizzas. Why is that the case?\n\nBy plotting the distribution of the original data using boxplots (on the left), we can see that the value range of data for the variables are vastly different. For example, the variable carbohydrates has much higher mean and variance compared to calories. By nature, PCA tries to form PCs where there is a widest spread in the data, so it will always prefer those variables with high ""absolute"" variance. It\'s like comparing 1000 milliseconds and 5 kilometers and putting more weight on the 1000 milliseconds because 1000 is bigger than 5.\n\n[[File: PCA_BoxPlot.png|center|700px]]\n\nThis is why standardization, or sometimes called feature scaling (scale data to mean 0 and standard deviation 1) is a crucial pre-processing step in many data analysis procedures and machine learning algorithms, including PCA. This allows the analysis to pay attention to all features equally, so that no variable dominates the others (equal importance).\n== Strengths & Weaknesses ==\n\'\'\'Strengths\'\'\'\n* Reduce complexity of data\n* Allows for concise visualization of main patterns in data\n* Remove correlated features\n* Enhance performance of algorithms\n* Reduce overfitting\n\'\'\'Weaknesses\'\'\'\n* Principle components are created based on linear assumptions\n* The created principle components are hard to interpret\n* Information loss through reduction of dimensionality (oftentimes acceptable)\n== Key Publications ==\n* Wold, S., Esbensen, K., & Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3), 37-52.\n\n* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150202.\n== See Also ==\n\n* [https://www.youtube.com/watch?v=FgakZw6K1QQ&t A simple visual explanation of PCA] from StatQuest with Josh Starmer\n\n* [https://www.youtube.com/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&index=4 An in-depth walkthrough of PCA] and its mathematical root with Victor Lavrenko\n----\n\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Chân Lê.', '[[File: PCAEigenvector01.png|center|500px]]\n\nThis prompts the questions: Can we find directly find the vector which already lies on this ""widest spread direction"". The answer is yes, with the help of eigenvectors. Simply put, an eigenvectors of a certain matrix is a vector that, when transformed by the matrix, does not rotate. It remains on its own span, and the only thing that changes is its magnitude. This (constant) change ratio in magnitude corresponding to each eigenvector is called eigenvalue. It indicates how much of the data variability can be explained by its eigenvector.\n\nFor this toy dataset, since there are two dimensions, we get (at most) two egenvectors and two corresponding eigenvalues. Even if we only plot the eigenvectors scaled by their eigenvalues, we will basically have a summary data (and its spreading). At this point, the eigenpairs are be viewed as the principle components of the data.\n\n[[File: PCAEigenvector02.png|center|500px]]\n\n==== Ranking the principle components ====\nAs you may have noticed, the eigenvectors are perpendicular to each other. This is no coincidence. You can think of it this way: because we want to maximize the variance explained by each of the principle components, these components need to be independent from one another, therefore their orthogonality.  Thus, to define a set of principle components, you find the direction which can explain the variability in the data the most: that is your first principle component (the eigenvector with the highest eigenvalue). The second principle compent will be percepdicular to the first, and explain most of what is left of the variability. This continues until the d-th principle component is found.\n\nBy doing so, you are also sorting the ""importance"" of the principle components in terms of the information amount it contains what is used to explain the data. To be clear, the sum of all eigenvalues is the total variability in the data. From here, you can choose to discard any PCs whose percentage of explained variances are low. In many cases, if around 80% of the variance can be explained by the first k PCs, we can discard the other (d - k) PCs. Of course, this is only one of the heuristics method to determine k.  You can also use thr elbow method (the scree plot) like in k-means.\n\n==== Summary ====\n* PCA is a feature extraction technique widely used to reduce dimensionality of datasets.\n* PCA works by calculating the eigenvectors and the corresponding eigenvalues of the initial variables in the data. These are the principle components. Number of PCs = number of eigenvectors = number of features.\n* The PCs are ranked by the eigenvalues, and iteratively show the directions in which the data spreads the most (after accounting for the previous PCs).\n* We can choose to keep a few of the first PCs that cummulatively explains the data well enough, and these are the new reduced dimension of the data.\n* Standardization is a crucial step in data pre-processing to ensure the validity of the PCA results.\n\n\n=== R Example ===\nGoing back to the example in the introduction, the dataset can be found here: https://www.kaggle.com/sdhilip/nutrient-analysis-of-pizzas\n\n<syntaxhighlight lang=""R"" line>\n# Loading library\nlibrary(tidyverse) # For pre-processing data\nlibrary(factoextra) # For visualization\ntheme_set(theme_bw()) # Set theme for plots\n\n# Load data\ndata <- read_csv(""Pizza.csv"")\nhead(data)\n</syntaxhighlight>\n\nAs shown here, there are seven measurements of nutrients for each pizza. Our goal is to reduce these seven dimensions of information down to only two, so that we can present the main patterns in our data on a flat piece of paper.\n\nTo conduct Principle Component Analysis in R, we use <code>prcomp</code>. From the original data, we only select the seven nutrient measurements as input for the function. In this function, we set <code>scale = TRUE</code> to perform scaling and centering (so that all variables will have a mean of 0 and standard deviation of 1). The result object is saved in <code>data.pca</code>\n\n<syntaxhighlight lang=""R"" line>\ndata.pca <- prcomp(data[, mois:cal]), scale = TRUE)\n</syntaxhighlight>\n\nNow we can visualize and inspect the result of the analysis. We start off by looking at the contribution of each created principle component (PC) to the overall variance in the dataset. For this, we create a spree plot:\n\n<syntaxhighlight lang=""R"" line>\nfviz_eig(data.pca)\n</syntaxhighlight>\n\n[[File: PCA_ScreePlot.png|center|500px]]\n\nLike I mentioned before, the number of PC created is equal to the number of input variables (in this cases, seven). Looking at the plot, the first two PCs combined can explain more than 90% of the dataset, an amazing number. This means this 7-dimensional dataset can be presented on a 2-dimensional space, and we still only lose less than 10% of the information. In other words, the first two PCs are the most important, and we can discard the other ones.\n\nNext, we look at the contribution of the original variables to the building of the two PCs, respectively.\n\n<syntaxhighlight lang=""R"" line>\nfviz_contrib(data.pca, choice = ""var"", axes = 1)\nfviz_contrib(data.pca, choice = ""var"", axes = 2)\n</syntaxhighlight>\n\n[[File: PCA_ContribPlot.png|center|500px]]\n\nThe red, dashed line refers to the case where all of the variable contribute equally. In the left plot, ash, fat, sodium and carbohydrates contribute substantially to the forming of the first PC. On the other hand, moisture and calories influence the second PC heavily, as seen in the right plot.\n\nFinally, we look at the biplot of the analysis. This reveals the underlying patterns of the dataset:\n\n<syntaxhighlight lang=""R"" line>\nfviz_pca_biplot(data.pca, label = ""var"", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlot.png|center|500px]]\n\nThe two axes of this plot are the newly created PCs. There are two main part of information presented on the plot:']"|0.005235602094240838|0.0
43|Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?|Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.|"['<syntaxhighlight lang=""Python"" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=""Python"" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term ‘residual’ refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=""Python"" line>\nmodel_1 = smf.ols(formula=\'points ~ ID + quanti\', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).\n\nresult = model_1.fit() ## estimates the regression\n\nprint(result.summary()) ## print the result\n</syntaxhighlight>\n\n[[File:new_attempt.png]]\n\nLooking at the Breusch-Pagan test, we can see that we cannot reject the assumption of homoscedasticity. \nConsidering the correlation coefficients, no statistically significant relationship can be identified. The positive relationship between quanti and points can be found again, but it is not statistically significant.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_2 = smf.ols(formula=\'points ~ ID + sex\', data=data)\nresult_bp2 = model_2.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp2.resid, result_bp2.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp2_test[1])\n\n# Set the value for maxlags\nmaxlags = 25  # Update this value as needed\n\nresult = model_2.fit(cov_type=\'HAC\', cov_kwds={\'maxlags\': maxlags})\nprint(result.summary())\n\n# Assuming \'sex\' is a column in the DataFrame named \'data\'\nsex_counts = data[\'sex\'].value_counts()\n\n# Print the frequency table\nprint(sex_counts)\n</syntaxhighlight>\n\n[[File:OLS 2.png]]\n\nBased on the Breusch-Pagan test, the assumption of homoscedasticity needs to be rejected to the 0.1% significance level. Therefore, we correct for heteroscedasticity with HAC (for more details see here)\nLooking at the results, being female (""sex"") has a large negative effect on points and is highly statistically significant. However, looking at the number of females in the dataset, we need to be very cautious to draw any conclusions. Since there are only four females in the dataset (and 73 males), the sample size is considered too small to make any statements about gendered effects on total points achieved. The correlation between ID and points can be ignored, since the last number of the matricle numbers follow no pattern.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_3 = smf.ols(formula=\'points ~ ID + exam\', data=data)\nresult_bp2 = model_3.fit()', ""==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()</code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n\n'''Depending on the variances of your variables, the type of t-test differs.'''\n\n<imagemap>Image:Statistics Flowchart - Equal variances.png|850px|center|\npoly 146 5 0 150 145 290 289 148 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 557 2 408 147 556 286 700 144 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 392 165 243 310 391 450 535 308 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\npoly 716 160 567 305 715 444 859 302 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\n</imagemap>\n\n'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()</code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()</code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE</code>.\n\n\n==== More than two factor levels: ANOVA ====\n'''Your categorical variable has more than two factor levels: you are heading towards an ANOVA.'''<br/>\n\n'''However, you need to answer one more question''': what about the distribution of your dependent variable?\n<imagemap>Image:Statistics Flowchart - 2+ Factor Levels - normal distribution.png|650px|center|\npoly 291 5 150 140 291 270 423 136 [[Data distribution]]\npoly 141 152 0 287 141 417 273 283 261 270  [[ANOVA]]\npoly 442 152 301 287 442 417 574 283 562 270 [[ANOVA]]\n</imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\nNow, let us have another look at your variables. '''Do you have continuous and categorical independent variables?'''\n\n'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\nIf your answer is NO, you should stick to the ANOVA - more specifically, to the kind of ANOVA that you saw above (based on regression analysis, or based on a generalised linear model). An ANOVA compares the means of more than two groups by extending the restriction of the t-test. An ANOVA is typically visualised using [[Introduction_to_statistical_figures#Boxplot|Boxplots]].</br> The key R command is <code>aov()</code>. Check the entry on the [[ANOVA]] to learn more.\n\nIf your answer is YES, you are heading way below. Click [[An_initial_path_towards_statistical_analysis#Is_there_a_categorical_predictor?|HERE]].\n\n\n== Only continuous variables ==\nSo your data is only continuous.<br/>\nNow, you need to check if there dependencies between the variables.\n<imagemap>Image:Statistics Flowchart - Continuous - Dependencies.png|650px|center|\npoly 383 5 203 181 380 359 559 182 [[Causality]]\npoly 182 205 2 381 179 559 358 382 [[An_initial_path_towards_statistical_analysis#No_dependencies:_Correlations|No dependencies]]\npoly 585 205 405 381 582 559 761 382 [[An_initial_path_towards_statistical_analysis#Clear_dependencies|Clear dependencies]]\n</imagemap>\n\n'''How do I know?'''  \n* Consider the data from a theoretical perspective. Is there a clear direction of the dependency? Does one variable cause the other? Check out the entry on [[Causality]].\n\n\n=== No dependencies: Correlations ===\n'''If there are no dependencies between your variables, you should do a Correlation.'''<br/>\nA correlation test inspects if two variables are related to each other. The direction of the connection (if or which variable influences another) is not set. Correlations are typically visualised using [[Introduction_to_statistical_figures#Scatter_Plot|Scatter Plots]] or [[Introduction_to_statistical_figures#Line_chart|Line Charts]]. Key R commands are <code>plot()</code> to visualise your data, and <code>cor.test()</code> to check for correlations. Check the entry on [[Correlations]] to learn more.\n\n'''The type of correlation that you need to do depends on your data distribution.'''\n\n<imagemap>Image:Statistics Flowchart - Normal Distribution.png|650px|center|\npoly 288 3 154 137 289 269 421 136 [[Data distribution]]\npoly 135 151 1 285 136 417 268 284 268 284  [[Correlations|Pearson]]\npoly 438 152 304 286 439 418 571 285 [[Correlations|Spearman]]\n</imagemap>"", '#Lets split the dataset into 3 datasets for each species:\nversicolor <- subset(iris, Species == ""versicolor"")\nvirginica <- subset(iris, Species == ""virginica"")\nsetosa <- subset(iris, Species == ""setosa"")\n\n#Let us test if the difference in petal length between versicolor and virginica is significant:\n#H0 hypothesis: versicolor and virginica do not have different petal length\n#H1 hypothesis: versicolor and virginica have different petal length\n\n#What does the t-test say?\nt.test(versicolor$Petal.Length,virginica$Petal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it\'s almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis. \n\n#--------------additional remarks on Welch\'s t-test----------------------\n#The widely known Student\'s t-test requires equal variances of the samples.\n#Let\'s check whether this is the case for our example from above by means of an f-test.\n#If variances are unequal, we could use a Welch t-test instead, which is able to deal with unequal variances of samples.\nvar.test(versicolor$Petal.Length, virginica$Petal.Length)\n\n#the resulting p-value is above 0.05 which means the variances of the samples are not different.\n#Hence, this would justify the use of a Student\'s t-test. \n#Interestingly, R uses the Welch t-test by default, because it can deal with differing variances. Accordingly, what we performed above is a Welch t-test.\n#As we know that the variances of our samples do not differ significantly, we could use a Student\'s t-test.\n#Therefore, we change the argument ""var.equal"" of the t-test command to ""TRUE"". R will use the Student\'s t-test now.\n\nt.test(versicolor$Petal.Length,virginica$Petal.Length, var.equal = TRUE)\n\n#In comparison to the above performed Welch t-test, the df changed a little bit, but the p-value is exactly the same.\n#Our conclusion therefore stays the same. What a relief. \n</syntaxhighlight>\n|}\n\n\n====Paired t-test====\n\'\'\'[https://www.statisticssolutions.com/manova-analysis-paired-sample-t-test/ Paired t-tests] are the third type of simple statistics.\'\'\' These allow for a comparison of a sample before and after an intervention. Within such an experimental setup, specific individuals are compared before and after an event. This way, the influence of the event on the dataset can be evaluated. If the sample changes significantly, comparing start and end state, you will receive again a p-value below 0,05.\n\nImportant: for the paired t-test, a few assumptions need to be met.\n* Differences between paired values follow a [[Data distribution|normal distribution]].\n* The data is continuous.\n* The samples are paired or dependent.\n* Each unit has an equal probability of being selected\n\n\'\'\'Example:\'\'\' An easy example would be the behaviour of nesting birds. The range of birds outside of the breedig season dramatcally differs from the range when they are nesting. \n\nFor more details on t-tests, please refer to the [[T-Test]] entry.\n\n{| class=""wikitable mw-collapsible mw-collapsed"" style=""width: 100%; background-color: white""\n|-\n! Expand here for an R example on paired t-tests.\n|-\n|<syntaxhighlight lang=""R"" line>\n\n# R Example for a Paired Samples T-Test\n# The paired t-test can be used when we want to see whether a treatment had a significant effect on a sample, compared to when the treatment was not applied. The samples with and without treatment are dependent, or paired, with each other.\n\n# Setting up our Hypotheses (Two-Tailed):\n# H0: The pairwise difference between means is 0 / Paired Population Means are equal\n# H1: The pairwise difference between means is not 0 / Paired Population Means are not equal\n\n# Example:\n# Consider the following supplement dataset to show the effect of 2 supplements on the time \n#it takes for an athlete (in minutes) to finish a practice race\n\ns1 <- c(10.9, 12.5, 11.4, 13.6, 12.2, 13.2, 10.6, 14.3, 10.4, 10.3)\ns2 <- c(15.7, 16.6, 18.2, 15.2, 17.8, 20.0, 14.0, 18.7, 16.0, 17.3)\nid <- c(1,2,3,4,5,6,7,8,9,10)\nsupplement <- data.frame(id,s1,s2)\nsupplement$difference <- supplement$s1 - supplement$s2\nView(supplement)\n\n# Exploring the data\nsummary(supplement)\nstr(supplement)\n\n# Now, we can go for the Paired Samples T-Test\n\n# We can define our Hypotheses as (Two-Tailed):\n# H0: There is no significant difference in the mean time taken to finish the race\n#     for the 2 supplements\n# H1: There is a significant difference in the mean time taken to finish the race\n#     for the 2 supplements\n\nt.test(supplement$s1, supplement$s2, paired = TRUE)\n\n# As the p-value is less than the level of significance (0.05), we have\n# sufficient evidence to reject H0 and conclude that there is a significant \n# difference in the mean time taken to finish the race with the 2 supplements\n\n# Alternatively, if we wanted to check which supplement gives a better result, \n# we could use a one tailed test by changing the alternate argument to less or greater\n</syntaxhighlight>\n|}\n\n\n====Chi-square Test of Stochastic Independence====\n[[File:Chi square example.png|frameless|500px|right|Source: John Oliver Engler]]\nThe Chi-Square Test can be used to check if one variable influences another one, or if they are independent of each other. The Chi Square test works for data that is only categorical.\n\n\'\'\'Example\'\'\': Do the children of parents with an academic degree visit a university more often, for example because they have higher chances to achieve good results in school? The table on the right shows the data that we can use for the Chi-Square test.\n\nFor this example, the chi-quare test yields a p-value of 2.439e-07, which is close to zero. We can reject the null hypothesis that there is no dependency, but instead assume that, based on our sample, the education of parents has an influence on the education of their children.\n\n{| class=""wikitable mw-collapsible mw-collapsed"" style=""width: 100%; background-color: white""\n|-\n! Expand here for an R example for the Chi-Square Test.\n|-\n|<syntaxhighlight lang=""R"" line>\n\n#R example for Chi-Square Test\n#We create a table in which two forests are differentiated according to their distribution of male, female and juvenile trees.']"|0.0036363636363636364|0.5116279069767442
44|How did Shell contribute to the advancement of Scenario Planning?|"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."|"['[[File:ConceptVisualisationScenarioPlanning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Scenario Planning]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Individual|Individual]] || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| [[:Category:Present|Present]] || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Scenario Planning is a systematic designation of potential futures to enable long term strategic planning.\n\n==Background==\n[[File:Scenario planning.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Scenario Planning until 2019.\'\'\' Search terms: \'scenario planning\', \'scenario construction\', \'scenario-based\', \'scenario study\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s\'\'\'. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were ""(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]"" in the US. (Amer et al. 2013, p.24).\n\n\'\'\'Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.\'\'\' At the time, corporate planning was traditionally ""(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude."" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the ""Unified Planning Machinery"". The idea was to listen to planners\' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell\'s management to deal with the 1973 and 1981 oil crises (1). Shell\'s success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders\' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.\n\n\n== What the method does ==\nScenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called ""scenarios"". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them ""(...) in coherent script-like or narrative fashion."" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all ""fundamentally different"" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios ""(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties."" (Schoemaker 1993, p.196).\n\n\'\'\'There is no \'\'one\'\' procedure for Scenario Planning\'\'\' (5). A commonly cited approach by Schoemaker (2, 3) includes the following steps:<br>\n1) Definition of time frame, scope, decision variables and major actors of the issue in question.\n\n2) Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. \n\n3) Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes.\n\n4) Elimination of implausible or impossible futures and, based on the themes that emerged from these, the creation of new scenarios. This process is repeatedly done until internally consistent scenarios are found. The number of scenarios developed depends on the scope and purpose of the planning process (1, 5).\n\n[[File:Scenario Planning Process.png|800px|thumb|center|\'\'\'Evaluation of the number of scenarios developed.\'\'\' Source: Amer et al. 2013. p.33]]\n\n5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.\n\n[[File:Scenario Planning Example.png|800px|thumb|center|\'\'\'Exemplary scenarios for life in Australian cities.\'\'\' Source: Gaziulusoy & Ryan 2017, p.1920]]', '5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.\n\n[[File:Scenario Planning Example.png|800px|thumb|center|\'\'\'Exemplary scenarios for life in Australian cities.\'\'\' Source: Gaziulusoy & Ryan 2017, p.1920]]\n\n\n== Strengths & Challenges ==\n* Scenario Planning allows for any organization that deploys it to be more innovative, flexible and thus better prepared for unforeseen disruptions and changes. In a corporate context, this can reduce costs, provide market benefits and improve internal communication (3, 5).\n* Scenario Planning broadens the structural perspective of an actor to think about the future (5). For example, an oil company may well be able to assess risks in their technical processes of oil exploration and extraction, but only through a more detailed scenario analysis they may be enabled to include economic, political and societal trends into their planning (2).\n* Scenarios are psychologically attractive. They are a way of transforming seemingly disparate data into relatable, coherent narratives. They present uncertainty across scenarios instead of providing probabilistic information for all elements within each individual one. In addition, they reduce the complexity and uncertainty of the future into graspable states (2).\n* Scenario Planning differs from adjacent methodological approaches. While a \'\'scenario\'\' illustrates a possible state of the future, a \'\'vision\'\' (see [[Visioning & Backcasting]] revolves around a desirable state of the future without taking its likelihood into consideration. Compared to Visioning, Scenario Planning might therefore be more useful for actual [[Glossary|decision-making]] but might as well be too narrow to envision holistic systemic changes (6). Additionally, a \'\'prediction\'\' as the classical method of economic \'\'forecasting\'\' describes likely states of the future as an extension of current developments, without the openness for [[Glossary|change]] that is inherent to Scenario Planning. \n\n\'\'\'Good scenarios should fulfill a range of characteristics\'\'\' (3, 5):\n* they need to be plausible and internally consistent, i.e. capable of happening.\n* they should be relevant, i.e. of help for decision making and connected to the issue that is to be solved.\n* they should be archetypal, i.e. not represent variations on the same theme but describe distinct futures.\n* they should describe a future that is in a state in which ""the system might exist for some length of time, as opposed to being highly transient."" (Schoemaker 1995, p.30)\n* they should challenge the existent way of thinking about the future.\n* while Scenario Planning generally permits actors to broaden their perspective, this only works if the construction of scenarios is not biased, which easily happens (2, 3). One may unconsciously look for confirming evidence for personal presuppositions when identifying trends and uncertainties. Also, overconfidence that certain trends will (not) prevail may distort one\'s assessment. This should be paid attention to during the process (3). As Schoemaker (1995, p.38) puts it: ""When contemplating the future, it is useful to consider three classes of knowledge: 1. Things we know we know. 2. Things we know we don\'t know. 3. Things we don\'t know we don\'t know. Various biases (...) plague all three, but the greatest havoc is caused by the third."" Ignorance in terms of the future should be acknowledged and addressed in order to challenge biases. ""And this is where scenario planning excels, since it is essentially a study of our collective ignorance."" (ibid).\n\n[[File:Scenario Planning They Believed It.png|450px|thumb|right|\'\'\'Throughout history, smart minds have underestimated technological, economic and political developments.\'\'\' Scenario Planning can be a mean of addressing the overconfidence in thinking that things will not change. Source: Schoemaker 1995, p.26]]\n\n\n== Normativity ==\n==== Connectedness ====\n* Scenario Planning is connected to various other methodological approaches. First, it is based on a [[System Thinking and Causal Loop Diagrams|System Thinking]] approach, recognizing the interconnectedness and causal interference of elements within a system.\n* To gather a good understanding of the relevant stakeholders and trends, useful approaches are [[Social Network Analysis]] or [[Stakeholder Analysis]]. To support the analysis of how systemic variables interact with each other, [[Agent Based Modelling]] may be applied.\n* Scenario Planning may be done after a [[Visioning & Backcasting|Visioning]] process: after envisioning what is desirable, stakeholders may think about what is actually realistic (4).\n* Scenario Planning is to some extent comparable to the [[Delphi]], where experts share their opinions on the future of an issue and come to a joint prediction. However, Scenario Planning differs in that it attempts to develop several complex scenarios instead of reaching one single (often quantitative) result to a question.\n\n==== Everything normative about this method ====\n* The aforementioned distinct ways of planning for the future differ from each other not only in the process, but also on a normative dimension. The \'\'world how it should be\'\'-way of thinking in a Visioning approach is highly normative. In forecasting, it is assumed that the world does not change much, which is often the case, but also often not. Scenario Planning, by comparison, assumes that the world could be changing and takes the possible changes and their consequences into consideration. It is a matter of how a company, a person or a scientist views the world that decides the approach that is taken to think about the future (see (1)). In a highly complex and dynamic world such as ours today, however, it may be advisable to consider the possibility of changing circumstances. As Wack (1985, p.73) already stated in the 1980s: ""The future is no longer stable; it has become a moving target. No single ""right"" projection can be deduced from past behavior.""\n* As Wack (1985) highlights, developing good scenarios is not sufficient to foster action in decision-makers. Scenarios should also be able to ""(...) change the decision-makers\' assumptions about how the world works and compel them to reorganize their [[Glossary|mental model]] of reality."" (Wack 1985, p.74). Scenario Planning must be accompanied by a transformational mindset shift in the respective organization (i.e. state, company, community). This may be hard, because it challenges fond concepts of how the world works. Therefore, ""[s]cenario planning requires intellectual courage to reveal evidence that does not fit our current conceptual maps, especially when it threatens our very existence. (...) But (...) opportunities can be perceived only when you actively look for them. (...) In addition to perceiving richer options, however, we must also have the courage and vision to act on them."" (Schoemaker 1995, p.39)', ""* Constanza, R. 2000. ''Visions of alternative (unpredictable) futures and their use in policy analysis.'' Conservation Ecology 4(1).\nIllustrates four visions for the year 2100 and outlines how one might choose from these.\n\n* Wiek, A. Iwaniec, D.M. 2014. ''Quality criteria for visions and visioning in sustainability science.'' Sustainability Science 9. 497–512.\nPresent ten quality criteria and a set of tools and techniques for Visioning, including exemplary publications.\n\n'''Empirical'''\n* Iwaniec, D.M. Wiek, A. 2014. ''Advancing Sustainability Visioning Practice in planning - The General Plan Update in Phoenix, Arizona.'' Planning Practice and Research.\nDescribes the Visioning process step by step in an exemplary project and draws conclusions on how the method may better be implemented in planning practice.\n\n* Gaziulusoy, a.I. Ryan, C. 2017. Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning. The Design Journal 20(1). 1916-1926.\nPresents a project combining Visioning with Scenario Planning and other approaches in order to develop pathways for low-carbon cities in Australia.\n\n* Elkins, L.A. Bivins, D. Holbrook, L. 2009. ''Community Visioning Process. A Tool for Successful Planning.'' Journal of Higher Education Outreach and Engagement 13(4). 75-84.\nIllustrates a project in a small US town where citizens were invited to develop visions and, subsequently, strategies for the future of their community.\n\n\n== References ==\n(1) Wiek, A. Iwaniec, D.M. 2014. ''Quality criteria for visions and visioning in sustainability science.'' Sustainability Science 9. 497–512.\n\n(2) Robinson, J.B. 1982. ''Energy backcasting: A proposed method of policy analysis''. Energy Policy 10(4). 337-344.\n\n(3) Davies, A.R. Doyle, R. Pape, J. 2012. ''Future visioning for sustainable household practices: spaces for sustainability learning?'' Area 44(1). 54-60.\n\n(4) Constanza, R. 2000. ''Visions of alternative (unpredictable) futures and their use in policy analysis.'' Conservation Ecology 4(1).\n\n(5) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design \nJournal 20(1). 1916-1926.\n\n(6) Dreborg, K.H. 1996. ''Essence of backcasting''. Futures 28(9). 813-828.\n\n(7) Elkins, L.A. Bivins, D. Holbrook, L. 2009. ''Community Visioning Process. A Tool for Successful Planning.'' Journal of Higher Education \nOutreach and Engagement 13(4). 75-84.\n\n(8) Vergragt, P.J. Quist, J. 2011. ''Backcasting for sustainability: Introduction to the special issue.'' Technological Forecasting & Social Change 78. 747-755.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.""]"|0.040697674418604654|0.3333333333333333
45|Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?|Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.|"['[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n|\'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n\'\'\'In short:\'\'\' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Social Network Analysis until 2019.\'\'\' Search terms: \'Social Network Analysis\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel\'\'\'. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued ""against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions."" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that ""(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. \'\'\'Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.""\'\'\' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as ""entitites"", ""actors"", ""vertices"" or ""agents"" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are ""(...) tied to one another through socially meaningful relations"" (Prell et al. 2009, p.503), which can be ""(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection"" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.\n\n[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|\'\'\'Types of Ties in a Social Network.\'\'\' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations ""(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns."" (Prell et al. 2009, p.503). \'\'\'Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as ""dyads"", but rather the study of patterns within a network.\'\'\' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors\' behavior emerges from the interaction between them: ""Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions."" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent\'s actions (Borgatti et al. 2009)', '==== Step by Step ====\n* \'\'\'Type of Network:\'\'\' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (\'\'whole networks\'\'), or focus on the network surrounding a specific node of interest (\'\'ego networks\'\'). They also decide for either \'\'one-mode networks\'\', focusing on one type of node that could be connected with any other; or \'\'two-mode networks\'\' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* \'\'\'Network boundaries:\'\'\' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** \'\'position-based approach\'\': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** \'\'event-based\'\' approach: those who had participated in key events are believed to define the population\n** \'\'relation-based approach\'\': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the \'\'exogenously defined boundaries\'\', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* \'\'\'Type of ties:\'\'\' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: \'\'directed\'\' ties, which go from one node to another, and \'\'undirected ties\'\', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, ""(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact."" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* \'\'\'Data Collection\'\'\': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a \'\'prompted recall\'\' approach, they are asked which people they would think of with regards to a specific topic (e.g. ""To whom would you go for advice at work?"") while they are shown a pre-determined list of potentially relevant individuals. In the \'\'free list\'\' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* \'\'\'Data Analysis\'\'\': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the \'\'\'visual representation\'\'\' of the network. For this, the researcher ""(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections."" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the ""number of lines which it is necessary to traverse in order to get from one point to another."" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|\'\'\'Different network structures.\'\'\' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|\'\'\'An exemplary network structure.\'\'\' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]\n\n== Strengths & Challenges ==\n* There is a range of challenges in the gathering of network data through [[Semi-structured Interview|Interviews]] and [[Survey|Surveys]], which can become long and cumbersome, and in which the interviewees may differently understand and recall their relations with other actors, or misinterpret the connections between other actors. (Marin & Wellman 2010, p.15)\n* The definition of network boundaries is crucial, since ""(...) the inappropriate inclusion or exclusion of a small number of entities can have ramifications which extend well beyond those entities themselves"". Apart from the excluded entities and their relations, all relations between these entities and the rest of the network, and thus the network\'s structural properties, are affected. (Butts 2008). For more insights on the topic of System Boundaries, please refer to [[System Boundaries|the respective article]].', '== Normativity ==\n* The structure of any network and thus the conclusions that can be drawn in the analysis very much depend on the relation that is observed. A corporation may be differently structured in terms of their informal compared to their official communication structures, and an individual may not be part of one network but central in another one that focuses on a different relational quality (Butts 2008)\n* Further, the choice of network boundaries as well as the underlying research intent can have normative implications. Also, actors within the network may be characterized using specific attributes, which may be a normative decision (such as for attributes of ethnicity, violence, or others).\n* The way in which a Social Network is visualized plays an important role. Researchers may choose from a variety of visualization forms in terms of symmetry, distribution and color of the represented network structures. It is important to highlight that these choices heavily influence how a Social Network is perceived, and that properly visualizing the available data is a matter of experience.\n* The method of social network analysis is connected to the methods Stakeholder Analysis as well as [[Clustering Methods|Clustering]]. Further, as mentioned above, the data necessary for Social Network Analysis can be gathered in [[Survey|Surveys]] or [[Semi-structured Interview|Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] or similar methods of data gathering. Last, the whole idea of analyzing systemic interactions between actors is the foundational idea of [[System Thinking & Causal Loop Diagrams|Systems Thinking.]]\n\n\n== An exemplary study ==\n[[File:Social Network Analysis - Lam et al. 2021 - Exemplary study title.png|600px|frameless|center|Title of Lam et al. 2021]]\n\'\'\'In their 2021 publication, researchers from Leuphana (Lam et al., see References) investigated the network ties between 32 NGOs driving sustainability initiatives in Southern Transylvania.\'\'\' Based on the [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226895/ [[Glossary|leverage points]] concept], they attempted to identify how these NGOs contribute to systemic change. For this, they focused on the positions of the NGOs in different networks, with each network representing relations that target different system characteristics (parameters, feedbacks, design, intent). As a basis for this, the authors identified types of ties between the NGOs that contribute to each of these four system characteristics.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Leverage Points.png|900px|frameless|center|Leverage Points and Social Network Analysis. From Lam et al. 2021]]<br>\n\nFurther, they investigated the amplification processes the NGOs applied to their initiatives to foster transformative change, grouped into four kinds of amplification processes: \'\'within\'\', \'\'out (dependent)\'\', \'\'out (independent)\'\' and \'\'beyond\'\'.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Amplification.png|750px|frameless|center|Survey questions on amplification processes by NGOs in Social Network Analysis on Leverage Points. Source: Lam et al. 2021]]<br>\n\nBased on these conceptual structures, \'\'\'the authors conducted an online survey\'\'\' in which 30 NGOs participated. In this survey, the NGOs were asked about their relations to other NGOs in terms of the kinds of questions shown above relating to the four leverage points. The NGOs were asked to rate the strength of the respective relations to another NGOs over the past five years with either ""not at all"" (0), low extent (1), moderate extent (2), high extent (3), and ""I don\'t know"". Further, the survey asked the NGOs four questions about the four kinds of amplification as shown above. \n\nWith the survey data, the authors created four networks using [https://gephi.org/ Gephi] and [https://nodexl.com/ NodeXL] software, one for each system characteristic. \'\'\'For each NGO (= node), they calculated three measures\'\'\' (Lam et al. 2021, p.816):\n* the \'\'weighted degree\'\', which measures the relations of the node to other nodes in the network, taking into consideration the weight of the relations. This measure provides insights on each node\'s individual interconnectedness.\n* the \'\'betweenness\'\', which highlights how often a node links other nodes that would otherwise be unconnected. The higher the betweenness of a node is, the more power it exerts on the network. (By the way, the node with the highest betweenness is often called a \'broker\').\n* the \'\'eigenvector centrality\'\', which measures the influence of a node in the network, weighted by the influence of its neighboring nodes. This highlights the future influence of a node.\n\nThey additionally tested for the influence of each NGO\'s amplification actions on these network measures by comparing those NGOs that applied a specific amplification action to those that did not, using a Mann-Whitney U test (also known as [[Simple_Statistical_Tests#Wilcoxon_Test|Wilcoxon rank-sum test]]).\n\nIn their results, they ""(...) found that while some NGOs had high centrality metrics across all four networks (...), other NGOs had high weighted degree, betweenness, or eigenvector in one particular network."" (p.817)\n[[File:Social Network Analysis - Lam et al. 2021 - Result visualisations.png|750px|frameless|center|Results from Lam et al. 2021, p.817]]\n\nWithout going into too many details at this point, we can see that the created networks tell us a lot about different aspects of the relation between actors. Based on the shown and further results, the authors concluded that\n# actors (NGOs) may have central roles either concerning all kinds of networks, or just in specific networks,\n# actors that amplify their own impact actively are potentially more central in networks, and\n# that Social Network Analysis with a leverage points perspective can help identify how and which actors play an important role for different kinds of sustainability transformations.\n\nFor more on this study, please refer to the References.\n\n\n== Key publications ==\n* Scott, J. 1988. \'\'Trend Report Social Network Analysis\'\'. Sociology 22(1). 109-127.\n* Borgatti, S.P. et al. 2009. \'\'Network Analysis in the Social Sciences.\'\' Science 323. 892-895.\n* Rowley, TJ. 1997. \'\'Moving beyond dyadic ties: A network theory of stakeholder influences.\'\' Academy of Management Review 2284). 887-910.\n* Bodin, Ö., Crona, B., Ernstson, H. 2006. \'\'Social networks in natural resource management: What is there to learn from a structural perspective?\'\' Ecology and Society 11(2).\n* Wasserman, S., Faust, K. 1994. \'\'Social network analysis: Methods and applications\'\' (Vol. 8). Cambridge university press.\n* Prell, C. (2012): \'\'Social network analysis: History, theory and methodology\'\', London.\n* Reed, M.S., Graves, A., Dandy, N., Posthumus, H., Hubacek, K., Morris, J., Prell, C., Quinn, C.H., Stringer, L.C. 2009. \'\'Who’s in and why? A typology of stakeholder analysis methods for natural resource management.\'\' Journal of Environmental Management 90, 1933-1949.']"|0.006622516556291391|1.0
46|What are the limitations of Stacked Area Plots?|Stacked Area Plots are not suitable for studying the evolution of individual data series.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nThis entry aims to introduce Stacked Area Plot and its visualization using R’s <syntaxhighlight lang=""R"" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> has the following syntax:\n\n\'\'\'Syntax\'\'\': <syntaxhighlight lang=""R"" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=""R"" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n</syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=""R"" inline>""uspopage""</syntaxhighlight> using the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> from the <syntaxhighlight lang=""R"" inline>ggplot2 package</syntaxhighlight>:\n\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n</syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of “Blues”, and add labels.\n\n<syntaxhighlight lang=""R"" line>\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = ""black"", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = ""Blues"")+\n  labs(title = ""US Population by Age"", \n       subtitle = ""Between 1900 and 2000"",\n       x = ""Year"",\n       y = ""Population (Thousands)"")\n</syntaxhighlight>\n\n==References==\n\n* R Graphics Cookbook, 2nd edition by Winston Chang\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Maria Jose Machuca.', '\'\'\'In short:\'\'\' Stacked bar plots show the quantitative relationship that exists between a main category and its subcategories. This entry helps visualise two different types of stacked bar plots, \'\'Simple Stacked Plots\'\' and \'\'Proportions Stacked Plots\'\', and explains the difference between them. For more on the basics of barplots, please refer to the [[Barplots, Histograms and Boxplots]] entry.\n\n\n==Stacked Barplots: Proportions vs. Absolute Values==\nStacked bar plots show the quantitative relationship that exists between a main category and its subcategories. Each bar represents a principal category and it is divided into segments representing subcategories of a second categorical variable. The chart shows not only the quantitative relationship between the different subcategories with each other but also with the main category as a whole. They are also used to show how the composition of the subcategories changes over time.\n\nStacked bar plots should be used for Comparisons and Proportions but with emphasis on Composition. This composition analysis can be static for a certain moment in time, or dynamic for a determined period of time.\n\nStacked bar Plots are two-dimensional with two axes: one axis shows categories, the other axis shows numerical values. The axis where the categories are indicated does not have a scale (*) to highlight that it refers to discrete (mutually exclusive) groups. The axis with numerical values must have a scale with its corresponding measurements units.\n\n\n===When you should use a stacked bar plot===\nThe main objective of a standard bar chart is to compare numeric values between levels of a categorical variable. One bar is plotted for each level of the categorical variable, each bar’s length indicating numeric value. A stacked bar chart also achieves this objective, but also targets a second goal.\n\nWe want to move to a stacked bar chart when we care about the relative decomposition of each primary bar based on the levels of a second categorical variable. Each bar is now comprised of a number of sub-bars, each one corresponding with a level of a secondary categorical variable. The total length of each stacked bar is the same as before, but now we can see how the secondary groups contributed to that total.\n\n\n==Two types of Stacked Barplots==\n1. \'\'\'Simple Stacked Plots\'\'\'\xa0place the\xa0\'\'\'absolute value\'\'\'\xa0of each subcategory after or over the previous one. The numerical axis has a scale of numerical values. The graph shows the absolute value of each subcategory and the sum of these values indicates the total for the category. Usually, the principal bars have different final heights or lengths.\n   \nWe use simple stacked plots when relative and absolute differences matter. Ideal for comparing the total amounts across each group/segmented bar.\n\n[[File:Simple_stacked_barplot.png|400px|frameless|right]]\n<syntaxhighlight lang=""R"" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecie <- c(rep(""sorgho"" , 3) , rep(""poacee"" , 3) , rep(""banana"" , 3) , rep(""triticum"" , 3) )\ncondition <- rep(c(""normal"" , ""stress"" , ""Nitrogen"") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(specie,condition,value)\n \n# Stacked\nggplot(data, aes(fill=condition, y=value, x=specie)) + \n    geom_bar(position=""stack"", stat=""identity"")\n</syntaxhighlight>\n\n2. \'\'\'Proportions Stacked Plots\'\'\' place the \'\'\'percentage\'\'\' of each subcategory after or over the previous one. The numerical axis has a scale of percentage figures. The graph shows the percentage of each segment referred to the total of the category. All the principal bars have the same height.\n\nIn proportions stacked plots the emphasis is on the percentage composition of each subcategory since the totals by category are not shown; in other words, they are used when the key message is the percentage of composition and not the total within the categories. We use proportions stacked plots only when relative differences matter.\n\n[[File:Proportions_stacked_barplot.png|400px|frameless|left]]\n<syntaxhighlight lang=""R"" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecies <- c(rep(""sorgho"" , 3) , rep(""poacee"" , 3) , rep(""banana"" , 3) , rep(""triticum"" , 3) )\ncondition <- rep(c(""normal"" , ""stress"" , ""Nitrogen"") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(species,condition,value)\n \n# Stacked + percent\nggplot(data, aes(fill=condition, y=value, x=species)) + \n    geom_bar(position=""fill"", stat=""identity"")\n</syntaxhighlight>\n\n----\n[[Category:Statistics]] [[Category:R examples]]', '<imagemap>Image:Statistical Figures Overview 27.05.png|1050px|frameless|center|\ncircle 120 201 61 [[Big problems for later|Factor analysis]]\ncircle 312 201 61 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA, TREE SPECIES IN AMERICA as three colors, with joint species in the overlaps]]\ncircle 516 190 67 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA as two colors, with joint species in the overlaps]]\ncircle 718 178 67 [[Stacked Barplots|Stacked Barplot, e.g. count data of different species (colors) for the variable TREES]]\ncircle 891 179 67 [[Barplots, Histograms and Boxplots#Barplots|Barplot, e.g. different kinds of trees (x) as count data (y) for the variable TREES]]\ncircle 1318 184 67 [[Barplots, Histograms and Boxplots#Histograms|Histogram, e.g. the variable EXAM POINTS as count data (y) per interval (x)]]\ncircle 1510 187 67 [[Correlation_Plots#Line_chart|Line Chart, e.g. TIME (x) and BITCOIN VALUE (y)]]\ncircle 1689 222 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), POPULATION (bubble size)]]\ncircle 1896 238 67 [[Big problems for later|Ordination, e.g. numeric variables (AGE, INCOME, HEIGHT) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\ncircle 202 326 67 [[Treemap|Treemap, e.g. FORESTS (colors) and count data of the included species (rectangles)]]\ncircle 410 323 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. different species of trees (absolute count data per color) for the variables TREES in ASIA, TREES IN AMERICA, TREES IN AFRICA, TREES IN EUROPE (x)]]\ncircle 608 295 67 [[Stacked Barplots|Proportions Stacked Barplot, e.g. relative count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 812 277 67 [[Pie Charts|Pie Chart, e.g. different kinds of trees (relative count data per color) for the variable TREE SPECIES]]\ncircle 1015 308 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE HEIGHT (y) for beeches]]\ncircle 1228 287 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS per point (x)]]\ncircle 1422 294 67 [[Correlation_Plots#Scatter_Plot|Scatter Plot, e.g. RUNNER ENERGY LEVEL (y) per KILOMETERS (x)]]\ncircle 1574 379 67 [[Big problems for later|Heatmap with lines]]\ncircle 1788 401 67 [[Correlation_Plots#Correlogram|Correlogram, e.g. the CORRELATION COEFFICIENT (shade) for each pair of the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 297 441 67 [[Wordcloud|Wordcloud]]\ncircle 516 434 67 [[Big problems for later|Spider Plot, e.g. relative count data of different species (shape) for the variables TREES IN EUROPE (green), TREES IN ASIA (blue), TREES IN AMERICA (red)]]\ncircle 710 402 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 1323 410 67 [[Regression Analysis#Simple linear regression in R|Linear Regression Plot, e.g. INCOME (y) per AGE (x)]]\ncircle 392 558 67 [[Chord Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites]]\ncircle 621 521 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites (if connections are valued: 3 variables)]]\ncircle 853 496 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. different TREE SPECIES (x) and TREE HEIGHT (y)]]\ncircle 1014 521 67 [[Stacked Area Plot|Stacked Area Plot, e.g. INCOME (x) and count data (y) of BOUGHT ITEMS (colors) (if y is EXPENSES: three variables)]]\ncircle 1174 502 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS IN MATHS (blue) and EXAM POINTS in HISTORY (green) per point (x)]]\ncircle 1438 521 67 [[Big problems for later|Multiple Regression, e.g. INCOME (y) per AGE (x) in different COUNTRIES]]\ncircle 1657 554 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]']"|0.010101010101010102|0.0
47|What is the purpose of Thought Experiments?|"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."|"['[[File:Thought Experiment Concept Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Thought Experiments]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.\n\n== Background ==\n[[File:Scopus hits Thought Experiment.png|450px|thumb|right|\'\'\'SCOPUS hits per year for Thought Experiments until 2019.\'\'\' Search term: \'thought experiment\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'The Thought Experiment may well be the oldest scientific method.\'\'\' The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the \'\'What if\'\' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early [[History of Methods|enlightenment]]. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the \'\'Origin of Species\'\' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term ""Thought Experiment"", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.\n\n== What the method does ==\nThought Experiments are the philosophical method that asks the ""What if"" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word ""experiment"" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.  \n\nThe easiest way to set up a Thought Experiment is to ask yourself a ""What if"" question. Many Thought Experiments resolve around decisions, choices or potential states of the future. A famous example is the Trolley experiment, where a train rolls towards five train track workers, who would all be killed be the oncoming train, unaware of their potential demise. You can now change the direction of the train, and lead it to another track. There, one worker would be killed. Uncountable numbers of variations of this experiment exist (see Further Information), as it can teach much about choice, ethics, and responsibility. For instance, many people would change the train direction, but hardly anyone would push a person onto the track to derail the train. This would save the other five, and the outcome would be the same. However the difference between pushing a lever or pushing a person has deep psychological ramifications that resolve around guilt. This exemplifies that the Thought Experiment does not necessarily have a \'\'best\'\' outcome, as the outcome depends - in this example - on your moral choices. Some might argue that you should hurl yourself onto the track to stop the train, thereby not changing the countable outcome, but performing a deeply altruistic act that saves everybody else. Most people would probably be unable to do this. \n\n\'\'\'Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world.\'\'\' Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schrödinger\'s Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences. \n\n[[File:Schrödingers Cat.png|400px|thumb|left|\'\'\'Schrödinger\'s Cat.\'\'\' Source: [https://ad4group.com/schrodingers-cat-and-the-marketing-strategy/ AD4Group].]]', '[[File:Schrödingers Cat.png|400px|thumb|left|\'\'\'Schrödinger\'s Cat.\'\'\' Source: [https://ad4group.com/schrodingers-cat-and-the-marketing-strategy/ AD4Group].]]\n\nSo far, we see that there are Thought Experiments that resolve exclusively about a - subjective - human decision, and other types of Thought Experiments that are designed around setting in the physical world. The difference between these two is in the design of the experiment itself, as the first are always focused on the normative decisions or people, while the second focuses on our normative interpretation of anticipation of a design that is without a human influence. This distinction is already helpful, yet another dimension is about time. Many Thought Experiments are independent of time, while others try to reinterpret the past to make assumptions about a future about which we have no experience. Thought Experiments that focus on re-interpretation of the past (""What if the assassination of Franz Ferdinands failed? Would the first World War still have happened?"") look for alternative pathways of history, often to understand the historical context and - more impotantly - the consequences of this context better. Most Thought Experiments are independent of a longer time scale. These experiments - such as the Trolley experiment - look at a very close future, and are often either very constructed or lack a connection to a specific context. Thought Experiments that focus on the future are often build around utopian or at least currently unthinkable examples that question the status quo, either form an ethical, societal, cultural or any other perspective. Such desirable futures are deeply normative yet can build an important bridge to our current reality through backcasting (""What if there is no more poverty, and how can we achieve this?""). \n\nAll this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https://en.wikipedia.org/wiki/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. \'\'\'Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.\'\'\'\n\n== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people\'s actions or decision, and the \'\'What if\'\' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. \'\'\'Schrödinger\'s Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.\'\'\' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment ""What if I have an unlimited amount of ice cream?"" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the ""unlimited"", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.\n\n== Normativity ==\nThought experiments are as we have already learned as normative as are our assumptions about the world. Hume argued that Thought Experiments are based on the laws of nature, yet here I would disagree. While many famous Thought Experiments are about the physical world, others are only made up in our minds. Many Thought Experiments are downright impossible to be matched with our reality, and are even explicitly designed to do this, such as Thomas Nagels Encyclopaedia of the Universe. It is therefore important to realise that basically all Thought Experiments are in essence normative, and one could say even downright subjective. Building on Derek Parfit, I would however propose a different interpretation, and propose that we should not measure the normativity of Thought Experiments through the design and setting, but instead by their outcome. Many people might come to the same conclusions within a given Thought Experiment, and some conclusion drawn from Thought Experiments may matter more than others. Consequently the penultimate question - also for Derek Parfit - is whether there are some Thought Experiments that are not normative. \n\n== Outlook ==\nMuch of [[Glossary|art]] and the media can be seen as a Thought Experiment, and there are ample examples that Thought Experiments in the media and the arts triggered or supported severe societal transformations. Thought Experiments are of equal importance in ethics and physics, and the bridge-building of the methodological approach should not be overestimated. Examples from the past prove that Thought Experiments can enable a great epistemological flexibility and diversity. This flexibility is even so large that Thought Experiments serve as a bridge between the epistemological and the ontological, or in other words between everything we know - and how we know it - and everything we believe. By enabling the transformation of our own most individual thoughts, Thought Experiments may provide a boat or a bridge to link the metaphysical with the world of knowledge.\n\n== Key Publications ==\nParfit, D. (2011). \'\'On what matters.\'\' Oxford University Press.\n\nParfit, D. (1984). \'\'Reasons and persons\'\'. OUP Oxford.\n\nKamm, F. M. (2015). The trolley problem mysteries. Oxford University Press.\n\nhttps://www.thoughtexperiments.net/people-who-divide-like-an-amoeba/', '\'\'\'Note:\'\'\' The German version of this entry can be found here: [[Experiments and Hypothesis Testing (German)]]<br/>\n\n\'\'\'Note:\'\'\' This entry is a brief introduction to experiments. For more details, please refer to the entries on [[Experiments]], [[Case studies and Natural experiments]] as well as [[Field experiments]].\n\n\'\'\'In short:\'\'\' This entry introduces you to the concept of hypotheses and how to test them systematically.\n\n[[File:Concept visualisation - Experiments.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Experiments]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n== Hypothesis building ==\n[[File:James Lind, A Treatise on the Scurvy, 1757 Wellcome M0013130.jpg|thumb|James Lind\'s ""A Treatise on Scurvy"", in which he systematically assessed what\'s most useful healing scurvy.]]\n\n\'\'\'Life is full of hypotheses.\'\'\' We are all puzzled by an [https://www.youtube.com/watch?v=HZ9xZHWY0mw endless flow of questions]. We constantly test our surrounding for potential \'\'what ifs\'\', cluttering our mind, yet more often is our cluttered head unable to produce any generalizable knowledge. Still, our brain helps us filter our environment, also by constantly testing questions - or hypotheses - to enable us to better cope with the challenges we face. It is not the really the big questions that keep us pre-occupied every day, but these small questions keep us going.\n \nDid you ever pick the wrong food in a restaurant? Then your brain writes a little note to itself, and next time you pick something else. This enables us to find our niche in life. However, at some point, [https://www.sciencedirect.com/science/article/pii/S0092867408009537 people started observing patterns in their environment] and communicated these to other people. Collaboration and communication were our means to become communities and much later, societies. This dates back to before the dawn of humans and can be observed for other animals as well. Within our development, it became the backbone of our modern civilization – for better or worse - once we tested our questions systematically. \nJames Lind, for example, conducted one of these [https://journals.sagepub.com/doi/pdf/10.1177/014107680309601201 first systematic approaches] at sea and managed to find a cure for the disease scurvy, which caused many problems to sailors in the 18th century.\n \n==== Hypothesis Testing ====\n[[File:Bildschirmfoto 2020-01-17 um 11.18.08.png|thumb|left|Here you can see one example for both the H0 and the H1.]]\n\nSpecific questions rooted in an initial idea, a theory, were thus tested repeatedly. Or perhaps it\'s better to say these hypotheses were tested with replicates and the influences disturbing our observation were kept constant. This is called <strong><i>testing of hypothesis</i></strong>, the repeated systematic investigation of a preconceived theory, where during the testing ideally everything is kept constant except for what is being investigated. While some call this the ""[https://www.youtube.com/watch?v=ptADSmJCVwQ scientific method],"" Many researchers are not particularly fond of this label, as it implies that it is THE [https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology scientific method], which is certainly not true. Still, the systematic [https://www.youtube.com/watch?v=ZzeXCKd5a18 testing of the hypothesis] and the knowledge process that resulted from it was certainly a scientific revolution (for more information, [[History_of_Methods|click here]]). \n\nWhat is important regarding this knowledge is the notion that a confirmed hypothesis indicates that something is true, but other previously unknown factors may falsify it later. This is why hypothesis testing allows us to approximate knowledge, many would argue, with a certain confidence, but [https://www.brgdomath.com/philosophie/erkenntnistheorie-tk10/falsifikation-popper/ we will never truly know something]. While this seems a bit confusing, a prominent example may help illustrate this. Before discovering Australia, all swans known in Europe were white. With the discovery of Australia, and to the surprise of Natural History researchers at the time, black swans were discovered. Hence before that, everybody would have hypothesized: [https://www.youtube.com/watch?v=wf-sGqBsWv4 all swans are white]. Then Australia was discovered, and the world changed, or at least, the knowledge of Western cultures about swans. \n\n\'\'\'At this point it is important to know that there are two types of hypotheses.\'\'\' Let us imagine that you assume, there are not only white swans in the world, but also black swans.\nThe null hypothesis (H0) states that this is not the case - everything is as we assumed before you came around the corner with your crazy claim. Therfore, the null hypothesis would be: all swans are white. The alternative hypothesis (H1) now challenges this null hypothesis: there are also black swans. This is what you claimed in the first place. We do not assume however right away that whatever you came up with is true, so we make it the alternative hypothesis. The idea is now to determine whether there is enough evidence to support the alternative hypothesis. To do so, we test the likelihood of the H0 being true in order to decide whether to accept the H0, or if we reject it and rather accept the H1. Both go together - when we accept one of both hypotheses, we simultaneously reject the other hypothesis, and vice-versa. Caution: Hypotheses can only be rejected, they cannot be verified based on data. When it is said that a hypothesis is \'accepted\' or \'verified\' or \'proven\', this only means that the alternative cannot be supported, and we therefore stick to the more probable hypothesis.']"|0.05405405405405406|1.0
48|What is temporal autocorrelation?|Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.|"['Values in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered “close” to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object\'s neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object\'s neighborhood is significantly lower than the densities of its neighbors\' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don\'t know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df[\'usage_kwh\'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (""to_numpy()"") and reshapes it to a single column (""reshape (-1, 1)""), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf[\'outlier\'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df[\'outlier\']] ## creates  a column where only outliers are selected\n\nplt.plot(df[\'usage_kwh\']) ## plots the dataframe\nplt.scatter(x=outliers.index, y=outliers[\'usage_kwh\'], color=\'tab:red\', marker=\'x\') ## creates a scatter plot with the outliers, marked with a red x\nplt.title(\'Outliers in the dataset (marked in red)\') ## title of the plot\nplt.xlabel(\'Date\') ## titlex axis\nplt.ylabel(\'Usage (KWh)\')## title y axis\nplt.show()\n</syntaxhighlight>\n[[File:outlier plot.png|700px|center|]]\n<small>Figure 7: Outlier in the chosen part of the dataset (marked with red cross)</small>\n\nIn this case, these are probably false-positive outliers. The dataset is already pretty clean and does likely not contain many outliers.\n\nThe imputation of missing values is the next challenge after detecting outliers. The naive solution to this problem is to replace a missing point with the mean of all points. While this approach will not skew the data distribution significantly, the imputed values might be far off from the actual value and make messy graphics. An alternative is applying regression models that use predictive models to estimate missing points.\n\nA combined approach to both detects and replaces outliers is the Kalman Filter, which predicts new points based on the previous points and estimates a noise measure for new points. Thus, when an anomaly is detected, it is replaced by the prediction from the model using historical data (Hoogendoorn & Funk 2018).\n\n===Forecasting Time Series Data===\n\nForecasting time series data can be of tremendous value in information gain. Since this is a large topic, this article will only touch on one method for forecasting: AutoRegressive Integrated Moving Average models, or ARIMA for short. It consists of three parts:\n\n* The autoregressive (AR) component, which models the autocorrelation of the time series\n* The integrated (I) component, which models the non-stationary bits of the time series\n* The moving average (MA) component, which models the noise or error in the time series.\n\nEach component requires an appropriate selection of parameters. Choosing the right parameters can yield a powerful model. However, finding good values of the parameters can be difficult and requires further complicated techniques.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.api as sm ## needed to employ ARIMA\n\n# Fit the model on all data except for the last day (last 96 measurements). Order refers to the order of the different processes. The three numbers refer to the order of autoregression, the order of integration, and the order of the moving average process, respectively. With this information, Python can correct these issues and therefore optimize the forecasting results. freq=\'15T\' indicates that the data has been collected every 15 minutes. \nmodel = sm.tsa.ARIMA(df[\'usage_kwh\'].iloc[:-96], order=(1, 0, 0), freq=\'15T\')\nmodel_fit = model.fit() ## estimates the best model\n\n# Predict how the last day is going to be\nforecast = model_fit.forecast(96)\n\nfull_df = pd.concat([\n    pd.DataFrame({\'value\': forecast, \'name\': \'forecast\'}),\n    pd.DataFrame({\'value\': df[\'usage_kwh\'].iloc[-192:], \'name\': \'existing\'})## -192 to get the data for the last two days\n])\n\npx.line(full_df, y=\'value\', color=\'name\',\n        title=\'Last day of electricity usage with prediction of the next 24 hours\',\n        labels={\'index\': \'Date\', \'value\': \'Usage (KWh)\'}) ##produce plot\n</syntaxhighlight>\n\n[[File:forecast plot.png|900px|center|]]\n<small>Figure 8: Forecast of the last day of the dataset compared to the actual values</small>\n\nARIMA models struggle with seasonality and often predict the mean of a highly seasonal time series a few steps ahead, just like in this example. Seasonal ARIMA (SARIMA) models take into account seasonality and are thus more suited for datasets that exhibit a strong seasonality (Hoogendoorn & Funk 2018). This would then be the next step to use another model, but this more simple example should suffice to show how forecasting with time series data generally works.', '[[File:scatter plot.png|700px|center|]]\n<small>Figure 2: Scatter plot visualizing the number of times a certain frequency of electricity usage occurred</small>\n\n==Statistical Analysis==\nThere are a multitude of statistical analysis methods that can be used to analyze time series data. While simple statistical tests like the t-test, ANOVA, and regression analysis can be used with time series data to identify relationships and dependencies in the data, there are also more specific methods to analyze time series data.\n\n===Decomposition===\nTime series data and be decomposed into three components: trend, seasonality, and residuals. The trend represents the long-term direction of the data, the seasonality represents the repeating patterns in the data, and the residual represents the noise in the data. Decomposing the data in this way can help to better understand the underlying structure of the data and identify any patterns or trends.\n\n<syntaxhighlight lang=""Python"" line>\nfrom statsmodels.tsa.seasonal import seasonal_decompose ## imports the necessary functions from statsmodel\n\ndecomp = seasonal_decompose(df[\'usage_kwh\'], model=\'additive\', period=15)\n## creates decomp which is the result of decomposing the electricity ##usage data to analyze seasonality, trend, and residuals. ""Period= ##15"" indicates that data was collected every 15 minutes and ##""model=addtiive"" makes the assumption that the three components add ##up linearly.\n\ndf1 = pd.DataFrame({\'value\': decomp.trend, \'name\': \'trend\'}) ## analyzes trend, so long-term direction\ndf2 = pd.DataFrame({\'value\': decomp.seasonal, \'name\': \'seasonal\'}) ##analyzes seasonal changes, so repeating patterns\ndf3 = pd.DataFrame({\'value\': decomp.resid, \'name\': \'resid\'}) ## analyzes residuals, so noise\n\nfull_df = pd.concat([df1, df2, df3]) ## combines all three data frames\nfig = px.line(full_df, y=\'value\', facet_row=\'name\', title=\'Decomposition of electricity usage\') ## creates plot\nfig.update_layout(height=1000) ## adapts height so that all three plots can be seen\n</syntaxhighlight>\n\n\n[[File:decomposition 1.png|700px|center|]]\n[[File:decomposition 2.png|700px|center|]]\n[[File:decomposition 3.png|700px|center|]]\n<small>Figure 3: The graphs for the decomposition of trend, seasonality, and the residuals respectively.</small>\n\nNo long-term direction or seasonal patterns can be detected. But we can see that there is a large variance in the residuals, so quite a lot of noise.\n\n===Moving Average===\nWhen there is a lot of noise in the data, it can be useful to calculate the moving (or rolling) average. The moving average is a statistical measure that calculates the average of a window around a data point. It smooths out the data and helps identify patterns or trends that may not be immediately apparent.\n\n<syntaxhighlight lang=""Python"" line>\ndf_full = pd.concat([\n    pd.DataFrame({\'value\': df[\'usage_kwh\'], \'name\': \'raw\'}), ## creates a dataframe the actual residuals\n    pd.DataFrame({\'value\': df[\'usage_kwh\'].rolling(window=24*4).mean(), \'name\': \'day\'}) ## creates a dataframe with the daily means (1-day time window, 24 hours times 4 measurements per hour (every 15 min)\n]) ##the concat command combines these two dataframes\npx.line(df_full, y=\'value\', color=\'name\',\n        title=\'Usage vs. a 1-day moving average of usage\',\n        labels={\'start_date\': \'Date\', \'value\': \'Usage (KWh)\'})\n</syntaxhighlight>\n\n[[File:MA plot.png|700px|center|]]\n<small>Figure 4: Electricity Usage with and without rolling average calculations</small>\n\nWe can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.api as sm ## needed to use the autocorrelation function\nautocorr = sm.tsa.acf(df[\'usage_kwh\'], nlags=24*4*2)## determines #autocorrelation with a lag of 15 minutes over 2 days (24 hours * 4 (every 15 min) * 2 for two days) \nsteps = np. arange (0, len(autocorr) * 15, 15) / 60 ## produces an ##array of the length of the autocorrelation data times 15 (so per #minute) and indicates that each lag is 15 minutes apart. By dividing it by 60, the values are converted from minutes to hours.\npx.line(x=steps, y=autocorr, markers=True,\n        title=\'Autocorrelation of electricity usage\',\n        labels={\'x\': \'Lag (hours)\', \'y\': \'Correlation\'}) ## creates #plot of the autocorrelation function\n</syntaxhighlight>\n[[File:ACF plot.png|700px|center|]]\n<small>Figure 5: Autocorrelation of electricity usage over two days</small>\n\nThe autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df[\'usage_kwh\'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel(\'Lag (hours)\') ## title x axis\nplt.ylabel(\'Autocorrelation of electricity usage with confidence interval\') ## title y axis\nplt.title(\'\') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n</syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days</small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.', 'THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=""Python"" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n</syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=""Python"" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir(\'/path/to/your/directory\')\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndf = pd.read_csv(\'D202.csv\')\ndf.head()\n</syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let\'s choose the most relevant columns for our research:\n\n<syntaxhighlight lang=""Python"" line>\n## Let\'s choose the most relevant columns for our research:\ndf[\'start_date\'] = pd.to_datetime(df[\'DATE\'] + \' \' + df[\'START TIME\'])\ndf[\'cost_dollars\'] = df[\'COST\'].apply(lambda x: float(x[1:]))\ndf.rename(columns={\'USAGE\': \'usage_kwh\'}, inplace=True)\ndf = df.drop(columns=[\'TYPE\', \'UNITS\', \'DATE\', \'START TIME\', \'END TIME\', \'NOTES\', \'COST\']).set_index(\'start_date\')\n</syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe “cost_dollars” by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don\'t specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=""Python"" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n</syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https://lg4ml.org/grounddetection/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=""Python"" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y=\'usage_kwh\',\n        title=\'Usage of Electricity over 2 Weeks\',\n        labels={\'start_date\': \'Date\', \'usage_kwh\': \'Usage (KWh)\'}) ## uses the data from ""start_date"" called ""Date"", and the data of ""usage_kwh"" called ""usage (KwH)""\n</syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time</small>\n\n<syntaxhighlight lang=""Python"" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df[\'usage_kwh\'])) ## transforms into frequencies\nfrequencies = rfftfreq(df[\'usage_kwh\'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)\n\nn = 100 ##plots the first 100 frequencies\npx.line(x=frequencies[:n], y=transform[:n], markers=True,\n           title=\'Magnitude of Frequencies\',\n           labels={\'x\': \'Frequency\', \'y\': \'Magnitude\'}) ## creates plot with n=100 frequencies\n</syntaxhighlight>\n\n[[File:scatter plot.png|700px|center|]]\n<small>Figure 2: Scatter plot visualizing the number of times a certain frequency of electricity usage occurred</small>\n\n==Statistical Analysis==\nThere are a multitude of statistical analysis methods that can be used to analyze time series data. While simple statistical tests like the t-test, ANOVA, and regression analysis can be used with time series data to identify relationships and dependencies in the data, there are also more specific methods to analyze time series data.']"|0.003952569169960474|0.0
49|What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?|The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models.|"['(4) Stein, E. D., White, B. P., Mazor, R. D., Jackson, J. K., Battle, J. M., Miller, P. E., ... & Sweeney, B. W. (2014). Does DNA barcoding improve performance of traditional stream bioassessment metrics?. Freshwater Science, 33(1), 302-311.\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau.', 'Dies löste zwei allgemeine Entwicklungen aus. Erstens gab es eine allgemeine und notwendige Zunahme der Befragung von Gesellschaft und Wissenschaft im Sinne einer kritischen Theorie, die zumindest bis zur Lösung dieser Probleme eine zentrale Rolle spielen muss. Zweitens löste die kritische Perspektive eine Anerkennung der Notwendigkeit aus, die Schaffung von Wissen zu differenzieren, was letztlich zu schwerwiegenden methodischen Entwicklungen führte, die Menschen tiefer in den methodischen Fokus zu rücken ([[Open Interview|Interviews]], [[Delphi]], [[Ethnography|Ethnographie]]). Die steigende Zahl der Ansätze in der Statistik führte zu immer mehr Disziplinen (Sozialwissenschaft, Ökologie, Psychologie) und Teildisziplinen. Ein prominentes Beispiel ist die Psychologie, die die [[ANOVA]] als Hauptanalysewerkzeug für das psychologische Experiment verwendete. In den 1960er Jahren erwiesen sich Interviews als eines der Hauptinstrumente für tiefe soziale Untersuchungen. Die [[Scenario Planning|Szenarioplanung]] ermöglichte die systematische Schaffung verschiedener Zukünfte. Die ""[[Grounded Theory]]"" ermöglichte eine induktivere Sicht auf die Welt. Das [[System Thinking & Causal Loop Diagrams|Systemdenken]] und die Netzwerk- und Systemtheorie entstanden, um die Interaktion und Komplexität der Welt zu berücksichtigen. Und Satelliten und die Anerkennung von [https://www.bfi.org/about-fuller/big-ideas/spaceshipearth \'Raumschiff Erde\'] erschlossen eine neue globale Perspektive auf unseren Planeten. Die [[Meta-Analysis|Meta-Analyse]] ermöglichte eine \'supra\'-Perspektive, um die Vielfalt der unterschiedlichen Ergebnisse zu integrieren, und Kuhn verkündete sogar eine Perspektive, wie Wissenschaft als Ganzes entsteht. [https://plato.stanford.edu/entries/feyerabend/ Feyerabend] lehnte das Dogma der Wissenschaft ganz und gar ab, was ein entscheidender Schritt hin zu einer kritischen Methodenperspektive war. \'\'\'Dabei verzweigten sich die Disziplinen immer weiter in immer kleinere Disziplinen, Bereiche, Teildisziplinen usw.\'\'\'\n\n==== Internet und Computer - \'\'Die neue Wissenschaft der Vernetzung\'\' ====\n[[File:disciplinary.png|thumb|600px|right|\'\'\'Unterschiedliche Formen disziplinärer Zusammenarbeit\'\'\'. Quelle: [http://makinggood.design/thoughts/tasty/ Jo Bailey makinggood.design]]] \nAus der allgemeinen Erkenntnis heraus, dass aufgrund der Mängel des gegenwärtigen Systems neue Paradigmen entwickelt oder identifiziert werden müssen, und ebenso aus der Erkenntnis der Bedeutung neuer Wissensformen entstand ein neuer Wissenschaftsmodus, der eine kritische Reflexion und eine Integration von Wissenschaft und Gesellschaft erfordert. Mit zunehmender Anerkennung der Komplexität der Probleme, mit denen die Menschheit konfrontiert ist, wurde klar, dass eine einzige Disziplin nicht in der Lage sein würde, die notwendigen Lösungen anzunähern. \'\'\'Stattdessen würden die Disziplinen zusammenarbeiten müssen, was keine Kleinigkeit ist.\'\'\' Gegensätzliche Denkrichtungen, unterschiedliche Schwerpunkte, eine unvereinbare Sprache und schließlich ein Wettbewerb um begrenzte Ressourcen und Aufmerksamkeit sowie Denkschulen, die in ihrer Bedeutung Überlegenheit beanspruchen - all dies sind keine Bausteine wissenschaftlicher Zusammenarbeit. Doch all diese Probleme sind vernachlässigbar im Vergleich zu der Herausforderung, eine gemeinsame Wissensproduktion zwischen Wissenschaft und Gesellschaft zu schaffen. Die Wissenschaft hielt eine teilweise arrogante Distanz zur Gesellschaft, oder zumindest ein Großteil der Wissenschaft vermied eine direkte Interaktion. Interviews, Beobachtungen, vielleicht noch Interaktion, waren bereits bekannte Ansätze. Die gemeinsame Problemdefinition und das gegenseitige Lernen von Wissenschaft und Gesellschaft stellten einen radikal neuen Forschungsmodus dar, und wir stehen erst am Anfang eines Paradigmenwechsels, der die Wissenschaft seit Jahrhunderten geprägt hat. So entstand die trandisziplinäre Forschung als ein neuer Forschungsmodus und ein inklusiver, reflexiver und lösungsorientierter Weg, der die Gräben in der Wissenschaft, aber auch zwischen Wissenschaft und Gesellschaft überwindet. \n\nDiese radikale Entwicklung fällt mit einer weiteren Revolution zusammen, die die Wissenschaft erschüttert hat, nämlich dem digitalen Zeitalter. \'\'\'Computer ermöglichten schnellere Berechnungen und neuartige methodische Ansätze.\'\'\' Das Internet trieb neue Wissensquellen und -formen an, und die damit verbundenen neuen Kommunikationsformen lösten einen Austausch zwischen Forscher*innen in einem beispiellosen Tempo aus. Alle Mittel der elektronischen Kommunikation, Online-Zeitschriften und die Tatsache, dass viele Forscher*innen heute über einen eigenen Computer verfügen, führten zu einer exponentiellen Zunahme der wissenschaftlichen Zusammenarbeit. Während dies manchmal auch Opportunismus und eine Verschiebung hin zu Quantität statt Qualität in der Forschung mit sich bringt, ist es unbestreitbar, dass heute viele wissenschaftliche Informationen nicht weiter von uns entfernt sind als ein Mausklick. Technologie kann kein Selbstzweck sein, aber als Mittel zum Zweck ermöglicht sie heute ein exponentielles Forschungstempo, das sich am deutlichsten in der Corona-Krise manifestiert hat. Die globale Gemeinschaft der Forscher*innen ist in ihrer größten Stärke vereint, und die Geschwindigkeit und Vielfalt der Wissensschöpfung ist in der Geschichte unserer Zivilisation beispiellos. Nie zuvor gab es mehr Interaktion zwischen den neuesten wissenschaftlichen Untersuchungen oder Ergebnissen und der Gesellschaft.\n\n== Weitere Infos ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html Mehr Infos] über Kuhns Theorie der Paradigmenwechsel.\n----\n[[Category:Normativity_of_Methods]]', '* Allington, G. R. H., M. E. Fernandez-Gimenez, J. Chen, and D. G. Brown. 2018. \'\'Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau.\'\' Ecology and Society 23(2):9. \n\n* Pfeiffer, C. Schodl, K. Fuerst-Waltl, B. Willam, A. Leeb, C. Winckler, C. 2018. \'\'Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach.\'\' Journal fo Central European Agriculture 19(4). 858-864.\n\n== Weitere Informationen ==\n* Das [http://intrepid-cost.ics.ulisboa.pt/about-intrepid/ INTREPID]-Netzwerk dreht sich um transdisziplinäre und interdisziplinäre Forschung und führt hilfreiche Akteure und Erkenntnisse aus diesem Feld an.\n* [http://www.transdisciplinarity.ch/td-net/Aktuell/td-net-News.html TD-NET] ist eine umfangreiche Schweizer Plattform, die Aktivitäten in diesem Feld organisiert und präsentiert. Dasselbe trifft auf den [https://complexitycontrol.org/methods-of-transdisciplinary-research/ ""Complexity or Control""-Blog] zu, der an der Leuphana angesiedelt ist.\n* Das [https://www.reallabor-netzwerk.de/ Reallabor-Netzwerk] bietet ebenfalls nützlilche Informationen über Reallabore und TD-Forschung an.\n* Mauser et al. (2013, p.420) nennen in ihrem Paper die ""[https://futureearth.org/initiatives/ Future Earth Initiative]"", die aus der Rio+20 Konferenz heraus entstand und ""eine neue Plattform und Paradigmen für integrierte globale Umweltveränderungsforschung anbieten wird, die in Partnerschaft mit der Gesellschaft designed und durchgeführt wird, um das für gesellschaftliche Transformation hin zur Nachhaltigkeit notwendige Wissen zu erzeugen.""\n----\n[[Category:Normativity of Methods]]']"|0.0|0.0
