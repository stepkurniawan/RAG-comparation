|question|ground_truths|contexts|context_precision|context_recall
0|What is the advantage of A/B testing?|The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.|"['\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==A/B Testing in a nutshell==\nA/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n\'\'\'For example:\'\'\'\n\n""To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.""\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.']"|0.041666666666666664|1.0
1|What is the ANOVA a powerful for?|Reducing variance in field experiments or complex laboratory experiments|"[""== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis.""]"|0.25|1.0
2|What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?|Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.|"[""[[File:Bildschirmfoto 2020-04-15 um 17.18.08.png|thumb|Another simple example for calculating probability which you have probably also discussed in school is flipping a coin. Here there are only two options: head or tail.]]\n\n'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while [https://365datascience.com/bayesian-vs-frequentist-approach/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this theory explored deeply, and builds the foundation of branches in data science and machine learning. The two approaches are also often coined as objectivists for frequentist probability fellows, and subjectivists for folllowers of [https://www.youtube.com/watch?v=9TDjifpGj-k Bayes theorem]. \n\nAnother perspective on the two approaches can be built around the question whether we design studies - or whether we base our analysis on the data we just have. This debate is the basis for the deeply entrenched conflicts you have in statistics up until today, and was already the basis for the conflicts between Pearson and Fisher. From an epistemological perspective, this can be associated with the question of inductive or [[Glossary|deductive reasoning]], although not many statisticians might not be too keen to explore this relation deeply, since they are often stuck in either deductive or inductive thinking, but not both.""]"|0.46153846153846156|1.0
3|Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?|Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.|"['\'\'\'What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency.\'\'\' Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy\'s law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of agency. There is thus a metaphysical problem when we try to differentiate the agent, their actions, and the consequences of their actions. One could claim that this can be solved by focussing on the consequences of the actions of agents alone. However, this consequentialist view is partly a theoretical consideration, as this view can create many interesting experiments, but does not really help us to solve the problem of unintentional acts per se. Still, consequentialism and a focus on mere actions is also relevant because it allows to step away from a single agent and focus instead on the interactions within the agency of several actors, which is a good start for the next concept.\n\n\n== Complexity ==\nMany phenomena in nature are simple, and follow simple rules. Many other phenomena are not simple, but instead can be defined as ""complex"". In order to negotiate between the two there is the fundamental law of Occam\'s razor, which defines that all things are as simple as possible, and as complex as necessary. Complex system theory strongly developed over the last decades, and created ripples into all empirical science and beyond. \'\'\'What is however problematic is that no unified and universally accepted definition of complexity exists, which is quite ironic.\'\'\' It seems that complexity in itself is complex. Here, I will therefore focus on some of the main arguments and characteristics that are relevant from a methodological standpoint. First of all, I will restrict all considerations to \'complex systems\'. To this end, I simply define [[System Thinking & Causal Loop Diagrams|„systems“]] as any number of individuals or elements that interact. Complex Systems, then, are systems that are composed of many components which may interact with each other in various ways and which are therefore difficult to model. Specific properties include [[Glossary|non-linearity]], emergence, adaptation and [[Glossary|feedback loops]]. From a methodological standpoint we should be able to observe these interactions, while from a philosophical standpoint we should be able to reflect upon them. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]']"|0.08333333333333333|1.0
4|What is the recommended course of action for datasets with only categorical data?|For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.|['Ordination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through experience can you pick what serves your dataset best. This is associated to the reality that many analysis made with ordinations are indeed compromises. Ecology and psychology are two examples of disciplines why imagined ordinations deeply enough into the statistical designs to derive datasets where more often than not assumptions for statistical analysis of a respective ordination are met. However, many analyses based on ordinations are indeed compromises, and from a mathematical standpoint are real world analysis based on ordinations a graveyard of mathematical assumptions, and violation of analytical foundations that borderline ethical misconduct. In other words, much of ordinations are messy. This is especially true because ordinations are indeed revealing mostly continuous results in the form of location on ordination axes. While multivariate analyis based on cluster analysis are hence more discrete through the results being presented as groups, ordinations are typically nice to graphically inspect, but harder to analytical embedded into a wider framework. More on this point later. Let us now begin with a presentation of the diverse ordination types and their respective origins. \n\n=== Correspondence analysis ===']|0.0|0.0
5|What is a Generalised Linear Model (GLM)?|A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.']"|0.0625|1.0
6|What is Cluster Analysis?|Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).|"['[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n\'\'\'In short:\'\'\' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|\'\'\'SCOPUS hits for Clustering until 2019.\'\'\' Search terms: \'Clustering\', \'Cluster Analysis\' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as \'\'similarity measure\'\' or \'\'distance measure\'\', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named ""unsupervised learning"", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term ""clustering"" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.']"|0.2222222222222222|0.0
7|What is the purpose of Network Analysis?|Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.|"[""== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) '''First of all, is the statistics the right approach to begin with?''' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem. \n\n2) '''What are the efforts you need to put into the initial data gathering?''' If you decided that statistics would be valuable to be applied, the question then would be, how? To rephrase this statement: How exactly? Your sampling with all its constructs, sample sizes and replicates decides about the fate of everything you going to do later. A flawed dataset or a small or biased sample will lead to failure, or even worse, wrong results. Play it safe in the beginning, do not try to overplay your hand. Slowly edge your way into the application of statistics, and always reflect with others about your sampling strategy.""]"|0.1111111111111111|1.0
8|What is the purpose of ANCOVA in statistical analysis?|ANCOVA is used to compare group means while controlling for the effect of a covariate.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.']"|0.22857142857142856|1.0
9|What are the key principles and assumptions of ANCOVA?|ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.']"|0.22857142857142856|0.0
10|What are the assumptions associated with ANCOVA?|ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.']"|0.05714285714285714|1.0
11|What are the strengths and challenges of Content Analysis?|Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.|"['== Strenghts & Challenges ==\n* Content Analysis counteracts biases because it ""(...) assures not only that all units of analysis receive equal treatment, whether they are entered at the beginning or at the end of an analysis but also that the process is objective in that it does not matter who performs the analysis or where and when."" (see Normativity) (Krippendorff 1989, p.404). In this case, \'objective\' refers to \'intersubjective\'. Yet, biases cannot be entirely prevented, e.g. in the sampling process, the development of the coding scheme, or the interpretation and evaluation of the coded data.\n* Content Analysis allows for researchers to apply their own social-scientific constructs ""by which texts may become meaningful in ways that a culture may not be aware of."" (Krippendorff 1989, p.404)\n* However, especially in case of qualitative analysis of smaller data sample sizes, the theory and hypotheses derived from data cannot be generalized beyond this data (1). Triangulation, i.e. the comparison of the findings to other knowledge on the same topic, may provide more validity to the conclusions (see Normativity).\n\n\n== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.\n* Validity of the inferred results - i.e. the fitting of the results to the intended kind of knowledge - may be reached a) through the researchers\' knowledge of contextual factors of the data and b) through validation with other sources, e.g. by the means of triangulation. However, the latter can be difficult to do due to the uniqueness of the data (1-3). Any content analysis is a reduction of the data, which should be acknowledged critically, and which is why the coding scheme should be precise and include the aforementioned explanation, examples and exclusion criteria.\n\n==== Connectedness ====\n* Since verbal discourse is an important data source for Content Analysis, the latter is often used as an analysis method of transcripts of standardized, [[Open Interview|open]], or [[Semi-structured Interview|semi-structured interviews]].\n* Content Analysis is one form of textual analysis. The latter also includes other approaches such as discourse analysis, rhetorical analysis, or [[Ethnography|ethnographic]] analysis (2). However, Content Analysis differs from these methods in terms of methodological elements and the kinds of questions it addresses (2).']"|0.391304347826087|0.6
12|What are the three main methods to calculate the correlation coefficient and how do they differ?|The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.|"['\'\'\'There are different forms of correlation analysis.\'\'\' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https://365datascience.com/students-t-distribution/ Student\'s t-distribution]. Alternative correlation measures like [https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ Kendall\'s tau and Spearman\'s rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman\'s rho is the most robust correlation measure when it comes to non-normally distributed data.\n\n==== Calculating Pearson\'s correlation coefficient r ====\nThe formula to calculate [https://www.youtube.com/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br/>\n\n\'\'\'1) Is the relationship between two variables positive or negative?\'\'\' If one variable increases, and the other one increases, too, we have  a positive (""+"") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\xa0[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534609/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (""-""): for example, the relationship between \'pizza eaten\' and \'pizza left\' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship.']"|0.2727272727272727|0.0
13|What is the purpose of a correlogram and how is it created?|A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.|"['# Plot a basic line chart\nplot(eu_stocks$DAX,  # simply select a stock index\n     type=\'l\')       # choose \'l\' for line chart\n</syntaxhighlight>\n\n[[File:Line chart.png|350px|thumb|right|Fig.4]]\nAs you can see, the plot is very simple. We can enhance the way this plot looks by making a few tweaks, making it more informative and aesthetically pleasing.\n\n<syntaxhighlight lang=""R"">\n# Fig.4\n# get the data\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX, # select the data\n     type=\'l\',      # choose \'l\' for line chart\n     col=\'blue\',    # choose the color of the line\n     lwd = 2,       # choose the line width \n     main = \'Line Chart of DAX Index (1991-1998)\',         # title of the plot\n     xlab = \'Time (1991 to 1998)\', ylab = \'Prices in EUR\') # x- and y-axis labels\n</syntaxhighlight>\n\nYou can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=""R"" inline>corrplot</syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n</syntaxhighlight>']"|0.09090909090909091|0.3333333333333333
14|What is telemetry?|Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.|"['Scientists hope that [[Citizen_Science|citizen science]] data may fill data gaps, for instance in the tropics. However, comparisons to data from Bird Life International have shown that the bird abundances for rare species in the tropics are strongly over-estimated. A likely reason is that bird observers are very determined to find these rare species and thereby overlook or under-record more common species and focus on rare ones (10). \nNevertheless, as technology progresses, identification apps are becoming better and easier to use. Moreover, scientists are working on new approaches to include such data into their research in meaningful ways. This trend of increasing citizen participation is thus likely to continue.\n\nTelemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is “the system of determining information about an animal through the use of radio signals from or to a device carried by the animal” (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===\n\nFuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\nSutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\nRobertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work. British Trust for Ornithology, Tring.\n\n=== Empirical ===\n\nGibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tamaño de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n== References ==\n(1) Sutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.']"|0.14705882352941177|1.0
15|What is a common reason for deviation from the normal distribution?|A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.|"[""==== Why some distributions are skewed ====\n[[File:SkewedDistribution.png|thumb|500px|right|'''Data can be skewed.''' These graphs show you how distributions can differ according to mode, median and mean of the displayed data.]]\n\nThe most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.\n\n'''Distributions can have different [https://www.youtube.com/watch?v=XSSRrVMOqlQ skews].''' There is the symmetrical skew which is basically a normal distributions or bell curve that you can see on the picture. But normal distributions can also be skewed to the left or to the right depending on how mode, median and mean differ. For the symmetrical normal distribution they are of course all the same but for the right skewed distribution (mode < median < mean) it's different.\n\n\n==== Detecting the normal distribution ====\n[[File:Car Accidents Barplot 2.jpg|thumb|400px|left|'''This is a time series visualized through barplots.''']]\n[[File:Car Accidents Histogram 2.jpg|thumb|400px|left|'''This is the same data as a histogram.''']]\n[[File:Car Accidents Boxplot 2.jpg|thumb|400px|left|'''And this the data as a boxplot.''' You can see that the data is normally distributed because the whiskers and the quarters have nearly the same length.]]\n'''But when is data normally distributed?''' And how can you recognize it when you have a [[Barplots, Histograms and Boxplots|boxplot]] in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot's whiskers are largely uneven.""]"|0.20833333333333334|1.0
16|How can the Shapiro-Wilk test be used in data distribution?|The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.|"['You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n\'\'\'If you would like to know how one can create the diagrams which you see here, this is the R code:\'\'\'\n\n<syntaxhighlight lang=""R"" line>\n\n# If you want some general information about the ""Seatbelt"" dataset, at which we will have look, you can use the ?-function.\n# As ""Seatbelts"" is a dataset in R, you can receive a lot of information here. You can see all datasets available in R by typing data().\n\n?Seatbelts\n     \n# to have a look a the dataset ""Seatbelts"" you can use several commands\n  \n## str() to know what data type ""Seatbelts"" is (e.g. a Time-Series, a matrix, a dataframe...)\nstr(Seatbelts)\n        \n## use show() or just type the name of the dataset (""Seatbelts"") to see the table and all data it\'s containing\nshow(Seatbelts)\n# or\nSeatbelts\n      \n## summary() to have the most crucial information for each variable: minimum/maximum value, median, mean...\nsummary(Seatbelts)\n\n     \n# As you saw when you used the str() function, ""Seatbelts"" is a Time-Series, which makes it hard to work with it. We should change it into a dataframe (as.data.frame()). We will also name the new dataframe ""seat"", which is more handy to work with.\n  \nseat<-as.data.frame(Seatbelts)\n     \n# To choose a single variable of the dataset, we use the \'$\' operator. If we want a barplot with all front drivers,\n# who were killed or seriously injured:\n     \nbarplot(seat$front)\n     \n# For a histogram:\n     \nhist(seat$front)\n  \n## To change the resolution of the histogram, you can use the ""breaks""-argument of the hist-command, which states\n## in how many increments the plot should be divided\n     \nhist(seat$front, breaks = 30)\nhist(seat$front, breaks = 100)\n\n# For a boxplot:\n     \nboxplot(seat$front)\n\n</syntaxhighlight>']"|0.02702702702702703|1.0
17|Why is the Delphi method chosen over traditional forecasting methods?|The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.|"['4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be ""(...) a highly motivating experience for participants"" (Rayens & Hahn 2000, p.309) due to the feedback on the group\'s opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which ""(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one\'s mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others."" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)']"|0.5789473684210527|0.5
18|What is the main goal of Sustainability Science and what are the challenges it faces?|The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.|"['\'\'\'Transdisciplinary research is of special importance to Sustainability Science and has received immense recognition in this field in the last years.\'\'\' This is because ""[s]ustainability is also inherently transdisciplinary"" (Stock & Burton 2011, p.1091), as it builds on the premise of solving real-world problems which are deeply nestled in ecological, political, economic and social processes and structures and therefore cannot be understood and solved without engaging with these spheres (Kates et al. 2015). Transdisciplinary research is a suitable approach for Sustainability Science: it allows to incorporate the knowledge of relevant stakeholders; it considers the normative dimensions involved in societal endeavors (that is, diverging norms, goals and visions of different societal spheres); and it increases [[Glossary|legitimacy]], ownership and accountability for the jointly developed solutions (Lang et al. 2012; Stock & Burton 2011). The integrative transdisciplinary approach highlights systemic interdependencies, enables a better understanding of complex issues and provides better knowledge to develop socially robust and applicable, effective solutions (Lang et al. 2012; Mauser et al. 2015).\n\n\n== How to do TD in Sustainability Science ==\nHere, we will refer to the ideal-typical model presented by Lang et al (2012). It introduces a conceptual guide, structured into three phases, on how transdisciplinary research for sustainability science should ideally be conducted. The relevant steps of each phase as well as general principles for the whole process will be listed below. The presented process is recursive: Problems emerging in scientific as well as societal practice are integrated in the transdisciplinary research approach. After its three phases have been realized, the newly generated knowledge and solutions are re-integrated into these two spheres, where they are applied and consolidated. Ultimately, new challenges emerge that demand renewed transdisciplinary collaboration.\n\n==== The ideal-typical model ====\n[[File:TDmodel.png|500px|thumb|left|\'\'\'The ideal-typical model for TD research in Sustainability Science.\'\'\' Source: Lang et al. 2012]]\n\n\'\'Phase A\'\'\n* Build a collaborative research team.\n* Create joint understandings and definitions of the problem.\n* Collaboratively define research objects and objectives, research questions, and success criteria.\n* Design a methodological [[Glossary|framework]].\n\n\'\'Phase B\'\'\n* Assign roles and responsibilities.\n* Apply research methods and settings to create the intended knowledge.\n\n\'\'Phase C\'\'\n* Realize two-dimensional integration.\n* Generate products for both parties (societal and scientific).\n\n\'\'General principles\'\'\n* Facilitate continuous evaluation\n* Mitigate conflict constellations\n* Enhance capabilities for and interest in participation\n<br>\n\n==== Challenges ====\nThere is a wide range of potential pitfalls challenging the achieval of the ideal-typical model.']"|0.0|0.4
19|Why are critical theory and ethics important in modern science?|Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.|"[""The disciplinary bias of modern science thus creates a deeply normative methodological bias, which some disciplines may try to take into account yet others clearly not. In other words, the dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research, and we need to be aware and reflexive about this. '''The largest bias concerning methods is the choice of methods per se.''' A critical perspective is thus not only of relevance from a perspective of societal responsibility, but equally from a view on the empirical. Clear documentation and reproducibility of research are important but limited stepping stones in a critique of the methodological. This cannot replace a critical perspective, but only amends it. Empirical knowledge will only look at parts - or strata according to Roy Bhaskar - of reality, yet philosophy can offer a generalisable perspective or theory, and Critical Theory, Critical Realism as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.\n\n== Further Information ==\n[https://www.thoughtco.com/critical-theory-3026623 Some words on Critical Theory]<br>\n[https://www.newworldencyclopedia.org/entry/Critical_realism#Contemporary_critical_realism A short entry on critical realism]\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.26666666666666666|1.0
20|What is system thinking?|System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.|"['\'\'\'The system is at the basis of System Thinking.\'\'\' System Thinking is a form of scientific approach to organizing and understanding \'systems\' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of \'holistic\' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that ""(...) all system dynamics are in principle non-linear"" and that ""(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions"" (Haraldsson 2004, p.6).\n\n\'\'\'Peter Checkland introduced the notion that there are two main types of System Thinking:\'\'\' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a \'system\' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a \'system\' an ""(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world"" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a \'system\'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|\'\'\'Hard System Thinking and Soft System Thinking\'\'\' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: \'\'System Analysis\'\' and \'\'System Dynamics\'\' (1).']"|0.3888888888888889|1.0
21|What is the main principle of the Feynman Method?|The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n\'\'Teaching is the best way to learn.\'\'\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:']"|0.05555555555555555|1.0
22|What is the difference between fixed and random factors in ANOVA designs?|Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.|"['==Fixed effects vs. Random effects==\n[[File:Smoking trooper.jpg|thumb|right|If smoking is a fixed or a random effect depends on the study design]]\nWithin [[ANOVA]] designs, the question whether a variable is a [https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To this end, it is helpful to consult existing studies to differentiate between [https://www.youtube.com/watch?v=Vb0GvznHf8U random and fixed factors]. Current medical trials may consider many variables, and have to take even more random factors into account. Testing the impact of random factors on the raw data is often a first step when looking at initial data, yet this does not help if it is a purely deductive design. In this case, simplified pre-tests are often a first step to make initial attempts to understand the system and also check whether variables - both fixed or random - are feasible and can be utilised in the respective design. Initial pre-tests at such smaller scales are a typical approach in medical research, yet other branches of research reject them as being too unsystematic. Fisher himself championed small sample designs, and we would encourage pre-tests in field experiments if at all possible. Later flaws and errors in the design can be prevented, although form a statistical standpoint the value of such pre-tests may be limited at best.']"|0.25|1.0
23|What is the replication crisis and how does it affect modern research?|The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.|['Field experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for [https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 agriculture] and [https://revisesociology.com/2016/01/17/field-experiments-sociology/ other fields] of production to thrive, but then also did medicine, [https://www.simplypsychology.org/milgram.html psychology], ecology and even [https://www.nature.com/articles/s41599-019-0372-0 economics] use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased. Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam’s razor also when designing a study.']|0.23809523809523808|1.0
24|What is the purpose and process of the flashlight method in group discussions?|The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.|"[""===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> ''What song would you choose to characterize the current state of discussion, and why?'' <br> ...\n\n== Links ==\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/blitzlicht/\n<br> https://www.bpb.de/lernen/formate/methoden/62269/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar Mölleken.""]"|0.12|0.75
25|What types of data can Generalized Linear Models handle and calculate?|Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.']"|0.0|0.0
26|What is a heatmap and why is it useful?|A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function and \'\'\'mtcars\'\'\' dataset.\nIt is important to note that the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=""R"" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=""R"" line>\ndata(""mtcars"")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.']"|0.16666666666666666|1.0
27|How did Alhazen contribute to the development of scientific methods?|Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.|"[""Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''\n\n==== Before Enlightenment - ''Measure And Solve'' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|'''Mercator world map.''' Source: [https://de.wikipedia.org/wiki/Mercator-Projektion Wikipedia]]] \n'''Another breakthrough that was also rooted in geometry was the compilation of early trade maps.''' While many European maps were detailed but surely not on scale (Ebstorfer Weltkarte), early maps still enabled a supra-regional trade. The real breakthrough was the [[Geographical Information Systems|Mercator map]], which was - restricted to the knowledge at the time - the first map that enabled a clear navigation across the oceans, enabling colonialism and Western dominions and domination of the world. This is insofar highly relevant for methods, because it can be argued that the surplus from the colonies and distant countries was one of the main drivers of the thriving of the European colonial rulers, their economies and consequently, their science. A direct link can be made between the [[Normativity of Methods|inequalities]] that were increased by colonialism and the thriving of Western science, including the development of scientific methods.""]"|0.3076923076923077|1.0
28|How can multivariate data be graphically represented?|Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.|"[""Simple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|'''A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.''' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|'''A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.''' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]""]"|0.5|0.25
29|What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?|Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.|"['== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.']"|0.2222222222222222|0.5
30|What are some of the challenges faced by machine learning techniques?|Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.|"['== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.']"|0.3333333333333333|1.0
31|What are the characteristics of scientific methods?|Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.|"[""'''This sub-wiki deals with scientific methods.''' <br/>\n\n=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n'''This Wiki describes each presented method in terms of''' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki's underlying [[Design Criteria of Methods]].<br/>\n'''This means that each method fulfills one or more categories of each of the following criteria:'''\n<br/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br/>""]"|0.1875|1.0
32|What is the main goal of practicing mindfulness?|The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.|"[""[[File:Headspace - Mindfulness.png|300px|thumb|left|'''[https://www.headspace.com/de Headspace] is an app that can help you meditate''', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around – often referred to as the monkey mind –, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions – just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!""]"|0.21052631578947367|1.0
33|How is information arranged in a Mindmap?|In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.|"[""The central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). '''This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.''' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. '''Elements may be subordinate to other elements, indicating nestedness of the information.''' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n''Sources:''\n* [https://www.mindmapping.com/de/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http://methodenpool.uni-koeln.de/download/mindmapping.pdf Uni Köln Methodenpool - Mind-Mapping]]\n* [https://kreativitätstechniken.info/problem-verstehen/mindmapping/ Kreativitätstechniken.info - Mindmapping]]\n* [https://www.lifehack.org/articles/work/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https://www.thetutorteam.com/blog/mind-maps-how-they-can-help-your-child-achieve/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz.""]"|0.06896551724137931|1.0
34|Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?|Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.|"['[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""|\'\'\' [[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\'\'\'In short:\'\'\' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Mixed Effects Models until 2020.\'\'\' Search terms: \'Mixed Effects Model\' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher\'s introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher\'s experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.']"|0.08333333333333333|0.0
35|How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?|Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.|"[""== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know.""]"|0.4444444444444444|1.0
36|Why should stepwise procedures in model reduction be avoided?|Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.|['The most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values.']|0.15384615384615385|1.0
37|What are the methods to identify redundancies in data for model reduction?|The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.|"['The simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant.']"|0.2777777777777778|1.0
38|How are 'narratives' used in Narrative Research?|'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.|"['[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Narrative Research describes qualitative field research based on narrative formats which are analyzed and/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Narrative Research until 2020.\'\'\' Search terms: \'Narrative Research\', \'narrative inquiry\', \'narrative analysis\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years\'\'\' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). \'Storytelling\' is defined as the distinct tonality, format and presentation in which a story is told. The term \'narrative\' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term \'narrative\' has been used used predominantly as a synonym for \'story\' in academia for decades (Barrett & Stauffer 2009).']"|0.045454545454545456|0.3333333333333333
39|What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?|Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.']"|0.0625|0.0
40|What are the three conditions under which Poisson Distribution can be used?|Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.|"['==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=""wikitable""\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}\n\nPoisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https://studiousguy.com/poisson-distribution-examples/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===\n\n\'\'The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time.\'\'\n\n[[File:1 equation.PNG|center|250px]]\nT = Time interval \ne = natural logarithmic base \nk = number of events \nThe K! means K factorial. This means that we multiply all integers from K down to 1. Say K is 4 then K!= 4* 3* 2* 1= 24']"|0.024390243902439025|1.0
41|How does the Pomodoro technique work?|The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don\'t want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n\'\'\'Pomodoro is very simple. All you need is work to be done and a timer.\'\'\'  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to \'\'25 minutes = 1 ""Pomodoro""\'\').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15–30 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====']"|0.24242424242424243|1.0
42|What is the 'curse of dimensionality'?|The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.|"['How can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.\n\n=== Curse of dimensionality ===\nThis term was coined by Richard R. Bellman, an American applied mathematician. As the number of features / dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. \'\'\'Principle Component Analysis helps to alleviate this problem\'\'\'.\n[[File: PCA_BiPlot.png|center|500px]]\n\n== What the method does ==\nPrinciple Component Analysis is one of the foundational methods to combat the curse of dimensionality. It is an unsupervised learning algorithm whose goals is to reduce the dimensionality of the data, condensing its entirety down to a low number of dimensions (also called principle components, usually two or three). \n\nAlthough it comes with a cost of losing some information, it makes data visualization much easier, improves the space and time complexity required for machine learning algorithms tremendously, and allows for more intuitive intepretation of these models. PCA can also be categorized a feature extraction techniques, since it creates these principle components - new and more relevant features - from the original ones.\n\nThe essence of PCA lies in finding all directions in which the data ""spreads"", determining the extent in which the data spreads in those directions, keeping only few direction in which the data spreads the most. And voila, these are your new dimensions / features of the data.\n\n=== Road to PCA ===\n==== Standardization ====\nOftentimes the features in the data are measured on different scales. This step makes sure that all features contribute equally to the analysis. Otherwise, variables with large range will trump thoses with smaller range (for example: a time variable that ranges between 0ms and 1000ms with dominate over a distance variable that ranges between 0m and 10m). Each variable can be scaled by subtracting its mean and dividing by the standard deviation (this is the same as calculating the z-score, and in the end, all variables with have the same mean 0 and standard deviation of 1).\n\n==== Covariance matrix ====']"|0.25925925925925924|1.0
43|Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?|Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.|"['If the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]']"|0.25806451612903225|0.0
44|How did Shell contribute to the advancement of Scenario Planning?|"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."|"['\'\'\'Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.\'\'\' At the time, corporate planning was traditionally ""(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude."" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the ""Unified Planning Machinery"". The idea was to listen to planners\' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell\'s management to deal with the 1973 and 1981 oil crises (1). Shell\'s success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders\' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.\n\n\n== What the method does ==\nScenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called ""scenarios"". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them ""(...) in coherent script-like or narrative fashion."" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all ""fundamentally different"" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios ""(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties."" (Schoemaker 1993, p.196).']"|0.2|0.6666666666666666
45|Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?|Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.|"['[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.']"|0.35714285714285715|1.0
46|What are the limitations of Stacked Area Plots?|Stacked Area Plots are not suitable for studying the evolution of individual data series.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nThis entry aims to introduce Stacked Area Plot and its visualization using R’s <syntaxhighlight lang=""R"" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> has the following syntax:\n\n\'\'\'Syntax\'\'\': <syntaxhighlight lang=""R"" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.']"|0.06451612903225806|1.0
47|What is the purpose of Thought Experiments?|"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."|"['== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people\'s actions or decision, and the \'\'What if\'\' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. \'\'\'Schrödinger\'s Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.\'\'\' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment ""What if I have an unlimited amount of ice cream?"" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the ""unlimited"", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.']"|0.4117647058823529|1.0
48|What is temporal autocorrelation?|Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.|"['Values in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered “close” to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object\'s neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object\'s neighborhood is significantly lower than the densities of its neighbors\' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don\'t know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function']"|0.0|0.0
49|What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?|The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models.|"['To illustrate the choice of methods in transdisciplinary research, how the selected methods can be combined, and how they relate to agency, see one of the following examples.\n\n\n== Examples ==\n[[File:Besatzfisch.png|450px|thumb|center|The research project ""Besatzfisch"". [http://besatz-fisch.de/content/view/90/86/lang,german/  Source]]]\n* The research project [http://besatz-fisch.de/content/view/34/57/lang,german/ ""Besatzfisch""] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to \'\'\'understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.\'\'\' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (\'\'Cooperation\'\'). Second, anglers were questioned about fish population sizes and their economic implications (\'\'Consultation\'\') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (\'\'Consultation\'\'). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (\'\'Empowerment\'\'). Fifth, social-ecological models were developed based on the previous empirical results. (\'\'Consultation\'\') Sixth, the project results are published in different forms of media for different target groups (\'\'Information\'\').\n\n* Another interesting example is the article [https://www.ecologyandsociety.org/vol23/iss2/art9/ ""Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau""]. In order to \'\'\'assess potential future social-ecological developments in Mongolia\'\'\', researchers first held a workshop with stakeholders, mostly from academia but with diverse disciplinary backgrounds, and a few non-scientific stakeholders. In this workshop, first, key elements that influence sustainable development in the region were identified. Step by step, these were then ranked to identify critical uncertainties, which led to the development of potential future scenarios (\'\'Consultation\'\'). Also, the stakeholders\' opinions on the workshop were later assessed through interviews, indicating positive impacts on their work and perspectives (\'\'Empowerment\'\'). The insights from the workshops were translated into system dynamics models by the researchers that were iteratively feedbacked by the stakeholders (\'\'Consultation\'\'), which led to a final model (\'\'Information\'\'). This approach was not purely transdisciplinary but illustrates a transdisciplinary workflow.']"|0.35|1.0
