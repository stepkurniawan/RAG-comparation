|question|ground_truths|contexts|context_precision|context_recall
0|What is the advantage of A/B testing?|The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.|"['\'\'\'THIS ARTICLE IS STILL IN EDITING MODE\'\'\'\n==A/B Testing in a nutshell==\nA/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n\'\'\'For example:\'\'\'\n\n""To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.""\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\n===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.\n\nThe sample size is determined by the percentage of users included in the test variants (control and treatment) and the duration of the experiment. As the experiment runs for a longer period of time, more visitors are exposed to the variants, resulting in an increase in the sample size. Because many external factors vary over time, it is important to randomize over time by running the control and treatment variants simultaneously at a fixed percentage throughout the experiment. Thereby the goal is to obtain adequate statistical power, where the statistical power of an experiment is the probability of detecting a particular effect if it exists. In practice, one can assign any percentages to the control and treatment, but 50% gives the experiment maximum statistical power.\n\nFurthermore, it is important to analyze only the subset of the population/users that were potentially affected. For example, in an A/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.']"|0.2631578947368421|0.0
1|What is the ANOVA a powerful for?|Reducing variance in field experiments or complex laboratory experiments|"[""== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis.\n\n\n== Normativity ==\nDesigning an ANOVA-based design demands experience, and knowledge of the previous literature. The deductive approach of an ANOVA is thus typically embedded into an incremental development in the literature. ANOVA-based designs are therefore more often than not part of the continuous development in normal science. However, especially since the millennium, other more advanced approaches gained momentum, such as mixed effect models, information theoretical approaches, and structural equation models. The rigid root of the normal distribution and the basis of p-values is increasingly recognised as rigid if not outright flawed, and model reduction in more complex ANOVA designs is far from coherent between different branches of sciences. Some areas of science reject p-driven statistics altogether, while other branches of science are still publishing full models without any model reduction whatsoever. In addition, the ANOVA is today also often used to analyse inductive datasets, which is technically ok, but can infer several problems from a statistical standpoint, as well as based on a critical perspective rooted in a coherent theory of science.""]"|0.14285714285714285|1.0
2|What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?|Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.|"[""* Even though the concept is intuitive to understand, the mathematical formulation and definitions can be intimidating for beginners.\n* Identifying correct prior distribution can be very difficult in real life problems which are not based on careful experimental design.\n* Solving complex models with Bayesian approach is still computationally expensive.\n\n\n== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin.\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future. \n\n\n== Key Publications ==\n* Bayes, T. 1997. LII. ''An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S''. Phil. Trans. R. Soc. 53 (1763). 370–418.\n* Box, G. E. P., & Tiao, G. C. 1992. ''Bayesian Inference in Statistical Analysis.'' John Wiley & Sons, Inc.\n* de Finetti, B. 2017. ''Theory of Probability''. In A. Machí & A. Smith (Eds.). ''Wiley Series in Probability and Statistics.'' John Wiley & Sons, Ltd.\n* Kruschke, J.K., Liddell, T.M. 2018. ''Bayesian data analysis for newcomers.'' Psychon Bull Rev 25. 155–177.""]"|0.1590909090909091|0.0
3|Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?|Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.|"[""== The way forward ==\nIf we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodologial ramifications. '''I can interview individuals, yet this will hardly allow me to prove agency.''' Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy. \n\nThe investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored [[Bayesian inference|Bayesian]] statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging. \n\nThe whole globe - although not a closed system – can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. '''Emergence is a truly cross-scalar phenomenon.''' Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.\n[[File:Climate model.png|400px|thumb|right|'''Climate models are increasingly getting more accurate, but the complexity and emergence of the global climate system may never be fully understood.''' Source: [https://blogs.egu.eu/geolog/2018/09/19/how-to-forecast-the-future-with-climate-models/ European Geosciences Union]]]""]"|0.0|0.0
4|What is the recommended course of action for datasets with only categorical data?|For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.|"[""'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\n\n=== Only categorical data: Chi Square Test ===\n'''You should do a Chi Square Test'''.<br/>\nA Chi Square test can be used to test if one variable influenced the other one, or if they occur independently from each other. The key R command here is: <code>chisq.test()</code>. Check the entry on [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi Square Tests]] to learn more.\n\n\n=== Categorical and continuous data ===\n'''Your dataset consists of continuous and categorical data.''' How many levels does your categorical variable have?\n<imagemap>Image:Statistics flowchart - Categorical factor levels.png|650px|center|\npoly 321 3 175 149 325 304 473 152 473 15 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data]]\npoly 149 172 3 318 153 473 301 321 301 321  [[An_initial_path_towards_statistical_analysis#One_or_two_factor_levels: t-test|One or two factor levels: t-test]]\npoly 489 172 343 318 493 473 641 321 641 321 [[An_initial_path_towards_statistical_analysis#More_than_two_factor_levels: ANOVA|More than two factor levels: ANOVA]]\n</imagemap>\n\n'''How do I know?'''\n* A 'factor level' is a category in a categorical variable. For example, when your variable is 'car brands', and you have 'AUDI' and 'TESLA', you have two unique factor levels. \n* Investigate your data using 'levels(categoricaldata)' and count the number of levels it returns. How many different categories does your categorical variable have? If your data is not in the 'factor' format, you can either convert it or use 'unique(yourCategoricalData)' to get a similar result.\n\n\n==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()</code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n\n'''Depending on the variances of your variables, the type of t-test differs.'''\n\n<imagemap>Image:Statistics Flowchart - Equal variances.png|850px|center|\npoly 146 5 0 150 145 290 289 148 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 557 2 408 147 556 286 700 144 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 392 165 243 310 391 450 535 308 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\npoly 716 160 567 305 715 444 859 302 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\n</imagemap>\n\n'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()</code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()</code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE</code>.\n\n\n==== More than two factor levels: ANOVA ====\n'''Your categorical variable has more than two factor levels: you are heading towards an ANOVA.'''<br/>""]"|0.08|0.0
5|What is a Generalised Linear Model (GLM)?|A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.']"|0.24242424242424243|1.0
6|What is Cluster Analysis?|Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).|"['[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n\'\'\'In short:\'\'\' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|\'\'\'SCOPUS hits for Clustering until 2019.\'\'\' Search terms: \'Clustering\', \'Cluster Analysis\' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as \'\'similarity measure\'\' or \'\'distance measure\'\', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named ""unsupervised learning"", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term ""clustering"" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=""R"" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(""x1"", ""x2"", ""cluster"")\n    return(df)\n}\n</syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns \'\'\'n\'\'\' examples to one of \'\'\'k\'\'\' clusters, where \'\'\'n\'\'\' is the sample size and  \'\'\'k\'\'\', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.']"|0.017241379310344827|1.0
7|What is the purpose of Network Analysis?|Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.|"['==== Step by Step ====\n* \'\'\'Type of Network:\'\'\' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (\'\'whole networks\'\'), or focus on the network surrounding a specific node of interest (\'\'ego networks\'\'). They also decide for either \'\'one-mode networks\'\', focusing on one type of node that could be connected with any other; or \'\'two-mode networks\'\' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* \'\'\'Network boundaries:\'\'\' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** \'\'position-based approach\'\': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** \'\'event-based\'\' approach: those who had participated in key events are believed to define the population\n** \'\'relation-based approach\'\': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the \'\'exogenously defined boundaries\'\', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* \'\'\'Type of ties:\'\'\' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: \'\'directed\'\' ties, which go from one node to another, and \'\'undirected ties\'\', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, ""(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact."" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* \'\'\'Data Collection\'\'\': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a \'\'prompted recall\'\' approach, they are asked which people they would think of with regards to a specific topic (e.g. ""To whom would you go for advice at work?"") while they are shown a pre-determined list of potentially relevant individuals. In the \'\'free list\'\' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* \'\'\'Data Analysis\'\'\': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the \'\'\'visual representation\'\'\' of the network. For this, the researcher ""(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections."" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.']"|0.25925925925925924|1.0
8|What is the purpose of ANCOVA in statistical analysis?|ANCOVA is used to compare group means while controlling for the effect of a covariate.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.']"|0.057971014492753624|0.2
9|What are the key principles and assumptions of ANCOVA?|ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.']"|0.11594202898550725|0.8571428571428571
10|What are the assumptions associated with ANCOVA?|ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n\'\'\'Regression assumptions\'\'\'  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n\'\'\'ANOVA assumptions\'\'\'\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n\'\'\'Specific ANCOVA assumptions\'\'\'\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = ""spearman"")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=""R"" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = ""Score by the treatment type"",\nxlab = ""Treatment type"",\nylab = ""Post treatment score"",\ncol = ""yellow"",\nborder = ""blue""\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.']"|0.10144927536231885|1.0
11|What are the strengths and challenges of Content Analysis?|Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.|"[""While there is a wide range of qualitative Content Analysis approaches, this entry will focus on joint characteristics of these. For more information on the different variations, please refer to Schreier (2014) in the Key Publications.\n\n==== General Methodological Process ====\n* Any Content Analysis starts with the ''Design Phase'' in which the research questions are defined, the potential sources are gathered, and the analytical constructs are established that connect the prospective data to the general target of the analysis. These constructs can be based on existing theories or practices, the experience and knowledge of experts, or previous research (2).\n* Next, the ''Unitizing'' is done, i.e. the definition of analysis units. It may be distinguished between sampling units (= sources of data, e.g. newspaper articles, interviews) and analysis units (= units of data that are coded and analyzed, e.g. single words or broader messages), with different approaches to identifying both (see Krippendorff 2004).\n* Also, the ''sampling'' method is determined and the sample is drawn.\n* Then, the ''Coding Scheme'' - or 'Codebook' - is developed and the data are coded. 'Coding' generally describes the transfer of the available data into more abstract groups. A code is a label that represents a group of words that share a similar meaning (3). Codes may also be grouped into categories if they thematically belong together. There are two typical approaches to developing the coding scheme: a rather theory-driven and a rather data-driven approach. In a theory-driven approach, codes are developed based on theoretical constructs, research questions and elements such as an interview guide, which leads to a rather deductive coding procedure. In a data-driven approach, the coding system is developed as the researcher openly scans subsamples of the available material and develops codes based on these initial insights, which is a rather inductive process. Often, both approaches are combined, with an initial set of codes being derived from theory, and then iteratively adapted through a first analysis of the available material.\n* With the coding scheme at hand, the researcher reads (repeatedly) through the data material and assigns each analysis unit to one of the codes. The ''coding process'' may be conducted by humans, or - if sufficiently explicit coding instructions are possible - by a computer. In order to provide reliability, the codes should be intersubjective, i.e. every researcher should be able to code similarly, which is why the codes must be exhaustive in terms of the overarching construct, mutually exclusive, clearly defined, have unambiguous examples as well as exclusion criteria.\n* Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural context of the message, or on the message's effect. The learnings may be validated in the face of other information sources, which is often difficult when Content Analysis is used in cases where there is no other possibility to access this knowledge. Finally, new hypotheses may also be formulated (all from 1, 2).\n\n[[File:Example Content Analysis (1).jpg|500px|thumb|right|'''An exemplary interview transcript from a person that was admitted to an emergency center.''' Source: Erlingsson & Brysiewicz 2017.]]\n\n[[File:Example Content Analysis (2).png|500px|thumb|right|'''The coding results for statements from the interview transcript above, grouped into one of several themes.''' Source: Erlingsson & Brysiewicz 2017.]]\n\nQualitative Content Analysis is a rather inductive process. The process is guided by the research questions - hypotheses may be tested, but this is not the main goal. As with much of qualitative research, this method attempts to understand the (subjective) meaning behind the analyzed data and to draw conclusions from there. Its purpose is to get hold of the 'bigger picture', including the communicative context surrounding the genesis of the data. The qualitative Content Analysis is a very iterative process. The coding scheme is often not determined prior to the coding process. Instead, its development is guided by the research questions and done in close contact to the data, e.g. by reading through all data first and identifying relevant themes. The codes are then re-defined iteratively as the researcher applies them to the data. As an example, the comparative analysis of two texts can be mentioned, where codes are developed based on the first text, re-iterated by reading through the second text, then jointly applied to the first text again. Also, new research questions may be added during the analysis if the first insights into the data raise them. This approach is closely related to the methodological concept of [[Grounded Theory]] approaches.""]"|0.2777777777777778|0.5
12|What are the three main methods to calculate the correlation coefficient and how do they differ?|The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n\'\'\'In short:\'\'\' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, \'\'\'correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!\'\'\'\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n\'\'\'A note on calculating the correlation coefficient:\'\'\'\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson\'s correlation coefficient, Spearman\'s rank correlation coefficient and Kendall\'s rank coefficient. \n\'\'\'Pearson\'s correlation coefficient\'\'\' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson\'s correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. \'\'\'Spearman\'s rank correlation\'\'\' coefficient calculates the rank order of the variables\' values using a monotonic function whereas \'\'\'Kendall\'s rank correlation coefficient\'\'\' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson\'s correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\n#Fig.1\ndata(""mtcars"")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = ""Correlation between Miles per Gallon and Horsepower"",\n     xlab = ""Miles per Gallon"",\n     ylab = ""Horsepower"",\n     pch = 16,\n     col = ""red"",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n</syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables “mpg” and “hp” from the “mtcars” dataset. The Pearson\'s correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=""R"" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n</syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=""R"" inline>plot()</syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:']"|0.08196721311475409|1.0
13|What is the purpose of a correlogram and how is it created?|A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.|"['=== R Code ===\n<syntaxhighlight lang=""R"" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n</syntaxhighlight>\n\nClear and meaningful coding and plots are important. In order to achieve this, we have to change the names of the variables from the “mtcars” dataset into something meaningful. One way to do this, is to change the names of the columns and rows of the correlation variable.\n<syntaxhighlight lang=""R"" line>\ncorrelations <- cor(mtcars)[1:11, 1:11]\ncolnames(correlations) <- c(""Miles per Gallon"", ""Cylinders"", \n                            ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                            ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                            ""Gears"", ""Carburetors"")\nrownames(correlations) <- c(""Miles per Gallon"", ""Cylinders"", \n                            ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                            ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                            ""Gears"", ""Carburetors"")\n</syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=""R"" line>\n# Fig.5\ncorrplot(correlations,\n         method = ""circle"",\n         type = ""upper"",\n         order = ""hclust"",\n         tl.col = ""black"",\n         tl.srt = 45,\n         tl.cex = 0.6)\n</syntaxhighlight>\n\nThe parameters are different from the previous scatter plots. Obviously, here you need the corrplot() function and define your parameters, regarding to your preferred taste, in this function. Some of the parameters will be explained briefly.\n\n* \'\'\'method\'\'\': which method should be used to visualize your correlation matrix. There are seven different methods (“circle”, “square”, “ellipse”, “number”, “shade”, “color”, “pie”), “circle” is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* \'\'\'type\'\'\': how the correlation matrix will be displayed. It can either be “upper”, “lower” or “full”. Full is called by default.\n* \'\'\'order\'\'\': order method for the correlation coefficients. The “hclust” method orders them in hierarchical order, but it also possible to order them alphabetical (“alphabetical”) or with a [[Principal_Component_Analysis|principal component analysis]] (“PCA”).\n* \'\'\'tl.col\'\'\': color of the labels.\n* \'\'\'tl.srt:\'\'\' rotation of the labels in degrees.\n* \'\'\'tl.cex:\'\'\' size of the labels.\n\n== Visualisation with ggplot ==\n=== Overview ===\n=== R code ===\nCOMING SOON\n\nAs you can see, there are many different ways to visualize correlations between variables. The right correlation plot depends on your data and on the number of variables you want to analyze. But never forget, correlation plots show you only the relationship between the variables and nothing about the causality.\n\n\n== References ==\n* https://sustainabilitymethods.org/index.php/Causality_and_correlation\n* https://en.wikipedia.org/wiki/Correlation_and_dependence\n* https://codingwithmax.com/correlation-vs-causation-examples/\n* https://towardsdatascience.com/what-it-takes-to-be-correlated-ce41ad0d8d7f\n* http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient\n\nA nice example that shows how easy it is to create a spurious correlation:\n\n* https://rstudio-pubs-static.s3.amazonaws.com/4192_1180a799cd6c4d2ba6e4ed2702860efb.html\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden and ?.']"|0.203125|0.6666666666666666
14|What is telemetry?|Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.|"['THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=""Python"" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n</syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=""Python"" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir(\'/path/to/your/directory\')\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\ndf = pd.read_csv(\'D202.csv\')\ndf.head()\n</syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let\'s choose the most relevant columns for our research:\n\n<syntaxhighlight lang=""Python"" line>\n## Let\'s choose the most relevant columns for our research:\ndf[\'start_date\'] = pd.to_datetime(df[\'DATE\'] + \' \' + df[\'START TIME\'])\ndf[\'cost_dollars\'] = df[\'COST\'].apply(lambda x: float(x[1:]))\ndf.rename(columns={\'USAGE\': \'usage_kwh\'}, inplace=True)\ndf = df.drop(columns=[\'TYPE\', \'UNITS\', \'DATE\', \'START TIME\', \'END TIME\', \'NOTES\', \'COST\']).set_index(\'start_date\')\n</syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe “cost_dollars” by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don\'t specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=""Python"" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n</syntaxhighlight>']"|0.15789473684210525|0.0
15|What is a common reason for deviation from the normal distribution?|A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.|"['==== Why some distributions are skewed ====\n[[File:SkewedDistribution.png|thumb|500px|right|\'\'\'Data can be skewed.\'\'\' These graphs show you how distributions can differ according to mode, median and mean of the displayed data.]]\n\nThe most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.\n\n\'\'\'Distributions can have different [https://www.youtube.com/watch?v=XSSRrVMOqlQ skews].\'\'\' There is the symmetrical skew which is basically a normal distributions or bell curve that you can see on the picture. But normal distributions can also be skewed to the left or to the right depending on how mode, median and mean differ. For the symmetrical normal distribution they are of course all the same but for the right skewed distribution (mode < median < mean) it\'s different.\n\n\n==== Detecting the normal distribution ====\n[[File:Car Accidents Barplot 2.jpg|thumb|400px|left|\'\'\'This is a time series visualized through barplots.\'\'\']]\n[[File:Car Accidents Histogram 2.jpg|thumb|400px|left|\'\'\'This is the same data as a histogram.\'\'\']]\n[[File:Car Accidents Boxplot 2.jpg|thumb|400px|left|\'\'\'And this the data as a boxplot.\'\'\' You can see that the data is normally distributed because the whiskers and the quarters have nearly the same length.]]\n\'\'\'But when is data normally distributed?\'\'\' And how can you recognize it when you have a [[Barplots, Histograms and Boxplots|boxplot]] in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot\'s whiskers are largely uneven.\n\nYou can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n\'\'\'If you would like to know how one can create the diagrams which you see here, this is the R code:\'\'\'\n\n<syntaxhighlight lang=""R"" line>\n\n# If you want some general information about the ""Seatbelt"" dataset, at which we will have look, you can use the ?-function.\n# As ""Seatbelts"" is a dataset in R, you can receive a lot of information here. You can see all datasets available in R by typing data().\n\n?Seatbelts\n     \n# to have a look a the dataset ""Seatbelts"" you can use several commands\n  \n## str() to know what data type ""Seatbelts"" is (e.g. a Time-Series, a matrix, a dataframe...)\nstr(Seatbelts)\n        \n## use show() or just type the name of the dataset (""Seatbelts"") to see the table and all data it\'s containing\nshow(Seatbelts)\n# or\nSeatbelts\n      \n## summary() to have the most crucial information for each variable: minimum/maximum value, median, mean...\nsummary(Seatbelts)']"|0.11363636363636363|1.0
16|How can the Shapiro-Wilk test be used in data distribution?|The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.|"[""'''In short:''' The (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student’s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''ν = n−1'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value μ, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i</sub>'' deviates significantly from a specified value ''μ'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''μ'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''μ.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''σ'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.""]"|0.08571428571428572|0.0
17|Why is the Delphi method chosen over traditional forecasting methods?|The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.|"['The researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. \'\'\'The responses from the respective previous round were re-distributed to the participants\'\'\' (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again. \n\nAfter the third round, the participants were additionally asked to fill out survey questions on a 1-5 [[Likert Scale]] about how reliable their considered their own forecasts, and how much attention they had paid to the others\' forecasts and comments when these were re-distributed, both regarding each variable individually. This was done to better understand each individual\'s thought process.\n<br>\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 results.png|800px|thumb|center|\'\'\'The results for the Delphi process.\'\'\' It shows that the mean estimates of the group became better over time, and were most often quite close to the actual realisation. Source: Kauko & Palmroos 2014, p.326.]]\n<br>\nThe forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. \'\'\'They found that the individual responses did indeed converge over the Delphi process, and that the ""Delphi group improved between rounds 1 and 3 in 13 of the questions.""\'\'\' (p.320). They also found that ""[d]isagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement"" (p.322) and that the Delphi process ""clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future"", which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that ""paying attention to each others\' answers made the forecasts more accurate"" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.\n\n\n== Key Publications ==\n* Linstone, H. Turoff, M. 1975. \'\'The Delphi Method: Techniques and Applications\'\'. Addison-Wesley, Boston.\nAn extensive description of the characteristics, history, pitfalls and philosophy behind the Delphi method.\n* Dalkey, N. Helmer, O. 1963. An experimental application of the Delphi method to the use of experts. Management Science 9(3). 458-467.\nThe original document illustrating the first usage of the \'\'Delphi\'\' method at RAND.\n* Gordon, T.J. Helmer, O. 1964. Report on a long-range forecasting study. RAND document P-2982.\nThe report that popularized \'\'Delphi\'\' outside of the military defense field.']"|0.03333333333333333|0.0
18|What is the main goal of Sustainability Science and what are the challenges it faces?|The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.|"[""The interconnectedness of different forms of knowledge; reflexivity of researchers and other actors; the recognition of [[Agency, Complexity and Emergence|complexity]] in systems; and the necessity of power relations in knowledge production are prominent examples of current challenges that methodologies as of yet have not solved. We are probably still far away from the creation of a solidified canon of knowledge to solve these riddles. One could argue that we will never solve these methodological problems altogether, but that the joined action and interaction while working on these problems will be the actual solution, and that a real solution as with past methods of normal science shall never be reached. This is unclear to date, yet we need to become aware that when we question the ''status quo'' of knowledge production and methodologies, we may not be looking for goals, but more for a path. There are several examples in sustainability science that follow this path, such as [[Transdisciplinarity|transdisciplinarity]] and [[System Thinking & Causal Loop Diagrams|system thinking]]. However, according to critical realism, it may be good to solidify innovative and novel methodologies to a point that enables as to take meaningful actions based on the knowledge we create.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.4444444444444444|0.4
19|Why are critical theory and ethics important in modern science?|Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.|"[""Coming back to bias, '''Critical Theory seems as an antidote to bias''', and some may argue Critical Realism even more so, as it combines the criticality with a certain humbleness necessary when exploring the empirical and causal. The explanatory characteristic allowed by Critical Realism might be good enough for the pragmatist, the practical may speak to the modern engagement of science with and for society, and the normative is aware of – well - all things normative, including the critical. Hence a door was opened to a new mode of science, focussing on the situation and locatedness of research within the world. This was surely a head start with Kant, who opened the globe to the world of methods. There is however a critical link in Habermas, who highlighted the duality of the rational individual on a small scale and the role of global societies as part of the economy (Habermas 1987). This underlines a crucial link to the original three foundational theories in philosophy, albeit in a dramatic and focused interpretation of modernity. Habermas himself was well aware of the tensions between these two approaches – the critical and the empirical -, yet we owe it to Critical Theory and its continuations that a practical and reflexive knowledge production can be conducted within deeply normative systems such as modern democracies. \n\nLinking to the historical development of methods, we can thus clearly claim that Critical Theory (and Critical Realism) opened a new domain or mode of thinking, and its impact can be widely felt way beyond the social science and philosophy that it affected directly. However, coming back to bias, the answer to an almost universal rejection of empiricism will [[Big problems for later|not be followed here]]. Instead, we need to come back to the three foundational theories of philosophy, and need to acknowledge that reason, social contract and utilitarianism are the foundation of the first empirical disciplines that are at their core normative (e.g. psychology, social and political science, and economics). Since bias can be partly related to these three theories, and consequentially to specific empirical disciplines, we need to recognise that there is an overarching methodological bias. This methodological bias has a signature rooted in specific design criteria, which are in turn related to specific disciplines. Consequently, this methodological bias is a disciplinary bias - even more so, since methods may be shared among scientific disciplines, but most disciplines claim either priority or superiority when it comes to the ownership of a method.\n\nThe disciplinary bias of modern science thus creates a deeply normative methodological bias, which some disciplines may try to take into account yet others clearly not. In other words, the dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research, and we need to be aware and reflexive about this. '''The largest bias concerning methods is the choice of methods per se.''' A critical perspective is thus not only of relevance from a perspective of societal responsibility, but equally from a view on the empirical. Clear documentation and reproducibility of research are important but limited stepping stones in a critique of the methodological. This cannot replace a critical perspective, but only amends it. Empirical knowledge will only look at parts - or strata according to Roy Bhaskar - of reality, yet philosophy can offer a generalisable perspective or theory, and Critical Theory, Critical Realism as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.\n\n== Further Information ==\n[https://www.thoughtco.com/critical-theory-3026623 Some words on Critical Theory]<br>\n[https://www.newworldencyclopedia.org/entry/Critical_realism#Contemporary_critical_realism A short entry on critical realism]\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.25|0.0
20|What is system thinking?|System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.|"['System Thinking can be applied in any discipline to understand underlying structures and develop models of the system that is analysed. However, sustainability science relies heavily on System Thinking since it acknowledges the complex and lagged interrelationships within and in between ecological, economic, social and further systems. A Key Publication in this field was \'Thinking in Systems\' by \'\'\'Donella Meadows\'\'\', who had been co-authoring the landmark \'Limits to Growth\' before. Sustainability Science, which attempts to bridge the world how it is with the world how it ought to be, relies on System Thinking to understand how the world is and in which \'[[Glossary|leverage points]]\' one needs to intervene to bring about change. System Thinking can also be applied outside of scientific research, e.g. to analyze company-internal processes, for marketing purposes etc. (see e.g. 3)\n\n\n== What the method does ==\nBefore explaining System Thinking, it should first be explained what is a \'system\'.\n\n==== Systems, System Thinking, System Analysis & System Dynamics ====\nA system is a ""(...) network of multiple variables that are connected to each other through causal relationships"", based on which the network ""(...) expresses some sort of behaviour, which can only be characterized through observation as a whole"" (Haraldsson 2004, p.11). This behavior remains persistent in a variety of circumstances. More simply put, a system is a ""(...) collection of connected things (...) that influence one another."" (Toole 2005, p.2) Both the collection of elements and their interactions are important elements of the system which is emphasized by saying that a system is \'more than a collection of its parts\' (Arnold & Wade 2015, p.2).\n\nEvery system is ""(...) defined by its boundaries"" (Haraldsson 2004, p.13). The borders we draw for our system analysis influence which level of detail we apply to our view on the system, and which elements we investigate. System elements can be animate (animals, humans) or inanimate (rocks, rain), conceptual (motivation) or real (harvest), quantifiable (money) or rather qualitative (well-being) (2). For example, a system could be a tree, with the leaves, the stem and such elements interacting with each other, but also the forest in which our tree interacts with the soil, the weather, other plants, animals and inanimate objects. The system could also be the globe, where this forest interacts with other ecosystems, or the system in which Planet Earth interacts with the rest of the universe - our solar system. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n\'\'\'The system is at the basis of System Thinking.\'\'\' System Thinking is a form of scientific approach to organizing and understanding \'systems\' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of \'holistic\' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that ""(...) all system dynamics are in principle non-linear"" and that ""(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions"" (Haraldsson 2004, p.6).\n\n\'\'\'Peter Checkland introduced the notion that there are two main types of System Thinking:\'\'\' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a \'system\' in this sense.']"|0.3103448275862069|0.5
21|What is the main principle of the Feynman Method?|The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || \'\'\'[[:Category:Personal Skills|Personal Skills]]\'\'\' || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n\'\'Teaching is the best way to learn.\'\'\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# \'\'\'Select the topic you want to learn more about.\'\'\' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don\'t go to broad - focus on a specific topic. You will not be able to explain ""Economics"" or ""Physics"" in one go.\n# \'\'\'Find someone to talk to\'\'\'. Ideally, this person does not know anything about this topic. If you don\'t have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# \'\'\'Make notes.\'\'\' You will quickly realize yourself which parts of the topic you are not able to explain, and/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# \'\'\'Have a look at your notes and try to find more information.\'\'\' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# \'\'\'Now explain the topic again.\'\'\' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https://karrierebibel.de/feynman-methode/ Karrierebibel]\n* [https://blog.doist.com/feynman-technique/ ToDo-ist]\n* [https://www.goodwall.io/blog/feynman-technique/ Goodwall]\n* [https://www.youtube.com/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.']"|0.13333333333333333|1.0
22|What is the difference between fixed and random factors in ANOVA designs?|Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.|"[""[https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ Difference between crossed & nested factors]: A short article\n\n[https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/ Model reduction]: A helpful article\n\n[https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https://www.simplypsychology.org/milgram.html Field Experiments in Psychology]: A famous example\n\n[https://www.nature.com/articles/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https://revisesociology.com/2016/01/17/field-experiments-sociology/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https://www.youtube.com/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction\n\n[https://www.youtube.com/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.""]"|0.0|1.0
23|What is the replication crisis and how does it affect modern research?|The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.|"['==Replication of experiments==\n[[File:Bildschirmfoto 2020-05-21 um 17.10.27.png|thumb|One famous example from the discipline of psychology is the Milgram shock experiment carried out by Stanley Milgram a professor from the Yale University in 1963.]]\nField experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for [https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 agriculture] and [https://revisesociology.com/2016/01/17/field-experiments-sociology/ other fields] of production to thrive, but then also did medicine, [https://www.simplypsychology.org/milgram.html psychology], ecology and even [https://www.nature.com/articles/s41599-019-0372-0 economics] use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased. Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam’s razor also when designing a study.\n\n==External Links==\n===Articles===\n\n[https://explorable.com/field-experiments Field Experiments]: A definition\n\n[https://www.tutor2u.net/psychology/reference/field-experiments Field Experiments]: Strengths & Weaknesses\n\n[https://en.wikipedia.org/wiki/Field_experiment#Examples Examples of Field Experiments]: A look into different disciplines\n\n[https://conjointly.com/kb/experimental-design/ Experimental Design]: Why it is important\n\n[https://isps.yale.edu/node/16697 Randomisation]: A detailed explanation\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design Experimental Design in Agricultural Experiments]: Some basics\n\n[https://www.ndsu.edu/faculty/horsley/RCBD.pdf Block Design]: An introduction with some example calculations\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ Difference between crossed & nested factors]: A short article\n\n[https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf Nested Designs]: A detailed presentation']"|0.34285714285714286|1.0
24|What is the purpose and process of the flashlight method in group discussions?|The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || [[:Category:Team Size 1|1]] || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || \'\'\'[[:Category:Team Size 11-30|11-30]]\'\'\' || \'\'\'[[:Category:Team Size 30+|30+]]\'\'\'\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> \'\'Is everyone on the same page?\'\'\n<br> \'\'Are there important issues that have been neglected so far?\'\'\n<br> \'\'Is there unspoken dissonance?\'\'\n<br> \'\'Is there an elephant in the room?\'\'\n<br> \'\'What are we actually talking about?\'\'\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== \'\'Please note further\'\' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> \'\'What song would you choose to characterize the current state of discussion, and why?\'\' <br> ...\n\n== Links ==\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/blitzlicht/\n<br> https://www.bpb.de/lernen/formate/methoden/62269/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar Mölleken.']"|0.15217391304347827|0.3333333333333333
25|What types of data can Generalized Linear Models handle and calculate?|Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.|"['[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Generalized Linear Models until 2020.\'\'\' Search terms: \'Generalized Linear Model\' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. \'\'\'Nelder\'s and Wedderburn\'s work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.\'\'\' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. \'\'\'Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution\'\'\'. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.']"|0.030303030303030304|1.0
26|What is a heatmap and why is it useful?|A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function and \'\'\'mtcars\'\'\' dataset.\nIt is important to note that the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=""R"" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=""R"" line>\ndata(""mtcars"")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=""R"" line>\nfullcolnames <- c(""Miles per Gallon"", ""Number of Cylinders"",\n                  ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                  ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                  ""Number of Gears"", ""Number of Carburetors"")\n</syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = ""column"", labCol = fullcolnames, \n        margins = c(11,5))\n</syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let’s look at the feature <syntaxhighlight lang=""R"" inline>“Number of Carburetors”</syntaxhighlight>. We can see that \'\'\'Maserati Bora\'\'\' has the darkest color, hence it has the largest number of carburetors, followed by \'\'\'Ferrari Dino\'\'\', which has the second-largest number of carburetors. While other models such as \'\'\'Fiat X1-9\'\'\' or \'\'\'Toyota\'\'\' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.']"|0.018867924528301886|1.0
27|How did Alhazen contribute to the development of scientific methods?|Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.|"[""Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''\n\n==== Before Enlightenment - ''Measure And Solve'' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|'''Mercator world map.''' Source: [https://de.wikipedia.org/wiki/Mercator-Projektion Wikipedia]]] \n'''Another breakthrough that was also rooted in geometry was the compilation of early trade maps.''' While many European maps were detailed but surely not on scale (Ebstorfer Weltkarte), early maps still enabled a supra-regional trade. The real breakthrough was the [[Geographical Information Systems|Mercator map]], which was - restricted to the knowledge at the time - the first map that enabled a clear navigation across the oceans, enabling colonialism and Western dominions and domination of the world. This is insofar highly relevant for methods, because it can be argued that the surplus from the colonies and distant countries was one of the main drivers of the thriving of the European colonial rulers, their economies and consequently, their science. A direct link can be made between the [[Normativity of Methods|inequalities]] that were increased by colonialism and the thriving of Western science, including the development of scientific methods.\n\n[[File:printing-press-2.jpg|300px|thumb|left|'''Invention of the printing press'''. Source: [[https://www.ecosia.org/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \nWith a rising number of texts being available through the [https://www.britannica.com/biography/Johannes-Gutenberg invention of printing], the long known method of [[Hermeneutics]], which enabled deep and systematic analysis of writing, was one of the first methods to grow. A link can be made to translations and interpretations of the Bible, and many other religious discourses are indeed not different from deep text analysis and the derivation of interpretations. Hermeneutics go clearly a step further, and hence are one of the earliest and to date still most important scientific methods. [[Thought Experiments]] were another line of thinking that started to emerge more rapidly. By focussing on ''What if'' questions, scientists considered all sorts of questions to derive patterns of even law of nature ([https://plato.stanford.edu/entries/galileo/ Galileo]) and also directed their view into the future. Thought experiments were in an informal way known since antiquity, yet during medieval times these assumptions became a trigger to question much of what was supposedly known. Through experiments they became early staring points for subsequent more systematic experimentation.""]"|0.19047619047619047|1.0
28|How can multivariate data be graphically represented?|Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.|"['Multivariate data can be principally shown by three ways of graphical representation: \'\'\'ordination plots\'\'\', \'\'\'cluster diagrams\'\'\' or \'\'\'network plots\'\'\'. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|\'\'\'An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.\'\'\' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|\'\'\'A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic\'s importance in the research, and the connections show if topics were covered alongside each other.\'\'\' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n\'\'\'Descriptive Infographics\'\'\' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.\n\n\n\'\'\'Of course, there is more.\'\'\' While the figures introduced above represent a vast share of the visual representations of data that you will encounter, there are different forms that have not yet been touched. \'\'\'We have found the website [https://www.data-to-viz.com/#connectedscatter ""From data to Viz""] to be extremely helpful when choosing appropriate data visualisation.\'\'\' You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.\n\n\n== How to visualize data in R ==\n\'\'\'The following overview includes all forms of data visualisation that we consider important.\'\'\' <br>\nBased on your data, have a look which forms of visualisation might be relevant for you. Just hover over the individual visualisation type and it will show you its name. It will also show you a quick example which this kind of visualisation might be helpful for. \'\'\'By clicking, you will be redirected to a dedicated entry with exemplary R code.\'\'\'<br>\n\nTip: If you are unsure whether you have qualitative or quantitative data, have a look at the entry on [[Data formats]]. Keep in mind: categorical (qualitative) data that is counted in order to visualise each category\'s occurrence, is not quantitative (= numeric) data. It\'s still qualitative data that is just transformed into count data. So the visualisations on the left do indeed display some kind of quantitative information, but the underlying data was always qualitative.']"|0.3125|1.0
29|What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?|Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.|"['Please refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.']"|0.1590909090909091|1.0
30|What are some of the challenges faced by machine learning techniques?|Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.|"['Please refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, \'\'k-means clustering\'\', \'\'hierarchical clustering\'\', various approaches to performing \'\'regression\'\', \'\'principle component analysis\'\' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other\' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called \'\'Differential Privacy\'\' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. ""Efficient backprop."" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. ""Support-vector networks."" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. ""A theory of the learnable."" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" the Journal of machine Learning research 3 (2003): 993-1022.']"|0.1590909090909091|1.0
31|What are the characteristics of scientific methods?|Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.|"['\'\'\'This sub-wiki deals with scientific methods.\'\'\' <br/>\n\n=== What are scientific methods? ===\nWe define \'\'Scientific Methods\'\' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either \'\'\'reproducible\'\'\' and \'\'\'learnable\'\'\'; can be \'\'\'documented\'\'\' and are \'\'\'learnable\'\'\'; or are \'\'\'reproducible\'\'\', can be \'\'\'documented\'\'\', and are \'\'\'learnable\'\'\'. \n* From a systematic perspective, methods are approaches that help us \'\'\'gather\'\'\' data, \'\'\'analyse\'\'\' data, and/or \'\'\'interpret\'\'\' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n\'\'\'This Wiki describes each presented method in terms of\'\'\' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki\'s underlying [[Design Criteria of Methods]].<br/>\n\'\'\'This means that each method fulfills one or more categories of each of the following criteria:\'\'\'\n<br/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=""pages"" hideroot=""on"">Methods</categorytree>\n<br>\nWe also have what we call \'\'\'Level 2\'\'\' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* \'\'\'[[Statistics]]\'\'\': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* \'\'\'[[Interviews]]\'\'\': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read.']"|0.16279069767441862|0.3333333333333333
32|What is the main goal of practicing mindfulness?|The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.|"[""If you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions – just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!\n\n== Links & Further Reading ==\n* [https://www.headspace.com Headspace] \n* [https://www.youtube.com/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https://thichnhathanhfoundation.org/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https://en.wikipedia.org/wiki/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on ''Zen Mind, Beginner's Mind'' is classic introduction to Zen.\n* [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn.""]"|0.13636363636363635|1.0
33|How is information arranged in a Mindmap?|In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| \'\'\'[[:Category:Collaborative Tools|Collaborative Tools]]\'\'\' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\'\'\'Mindmapping is a tool for the visual organisation of information\'\'\', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|\'\'\'MindMaps can take the form of trees, with the words on the branches, or clusters/bubbles, as in this example.\'\'\' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https://www.thetutorteam.com/wp-content/uploads/2019/07/shutterstock_712786150.jpg thetutorteam.com]]]\n\n\'\'\'A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.\'\'\' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). \'\'\'This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.\'\'\' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. \'\'\'Elements may be subordinate to other elements, indicating nestedness of the information.\'\'\' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n\'\'Sources:\'\'\n* [https://www.mindmapping.com/de/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http://methodenpool.uni-koeln.de/download/mindmapping.pdf Uni Köln Methodenpool - Mind-Mapping]]\n* [https://kreativitätstechniken.info/problem-verstehen/mindmapping/ Kreativitätstechniken.info - Mindmapping]]\n* [https://www.lifehack.org/articles/work/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https://www.thetutorteam.com/blog/mind-maps-how-they-can-help-your-child-achieve/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]']"|0.16279069767441862|1.0
34|Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?|Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.|"['[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | [[:Category:Qualitative|Qualitative]]\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""|\'\'\' [[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\'\'\'In short:\'\'\' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Mixed Effects Models until 2020.\'\'\' Search terms: \'Mixed Effects Model\' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher\'s introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher\'s experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: \'\'\'Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables\'\'\'. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. \'\'\'The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.\'\'\' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models.']"|0.02702702702702703|0.0
35|How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?|Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.|"[""== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.\n\nThe greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.""]"|0.25806451612903225|1.0
36|Why should stepwise procedures in model reduction be avoided?|Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.|"[""== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values.""]"|0.06060606060606061|1.0
37|What are the methods to identify redundancies in data for model reduction?|The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.|"['The simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find.']"|0.2564102564102564|1.0
38|How are 'narratives' used in Narrative Research?|'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.|"['[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| \'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| [[:Category:Deductive|Deductive]]\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Narrative Research describes qualitative field research based on narrative formats which are analyzed and/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Narrative Research until 2020.\'\'\' Search terms: \'Narrative Research\', \'narrative inquiry\', \'narrative analysis\' in Title, Abstract, Keywords. Source: own.]]\n\'\'\'[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years\'\'\' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). \'Storytelling\' is defined as the distinct tonality, format and presentation in which a story is told. The term \'narrative\' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term \'narrative\' has been used used predominantly as a synonym for \'story\' in academia for decades (Barrett & Stauffer 2009).\n\n\'\'\'Psychologist Jerome Bruner introduced the notion of \'narrative\' as being one of two forms of distinct modes of thinking in 1984\'\'\' - the other being the \'logico-scientific\' mode (Barrett & Stauffer 2009). While the latter is ""(...) more concerned with establishing universal truth conditions"" (Barrett & Stauffer 2009, p.9), the \'narrative\' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that \'narratives\' are a central form of human learning about - and [[Glossary|sense-making]] of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.\n\n\'\'\'As a scientific method, Narrative Research - often just phrased \'narrative\' - is a rather recent phenomenon\'\'\' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of \'narrative\' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).\n\n== What the method does ==\n\'\'\'First, there is a distinction to be made:\'\'\' \'Narrative\' can refer to a form of Science Communication, in which research findings are presented in a story format (as opposed to classical representation of data) but not extended through new insights. \'Narrative\' can also be understood as a form of scientific inquiry, generating new knowledge during its application. This entry will focus on the latter understanding.\n\n\'\'\'Next, it should be noted that Narrative Research entails different approaches\'\'\', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.']"|0.08108108108108109|1.0
39|What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?|Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.|"[""Over the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer [[Causality|causality]] when explaining the patterns that are being modeled. '''In other words, GAMs are able to increase the model fit or predictive power, but in the worst case, we are throwing our means to understand or explain the existing relations out of the window.''' \n\nUnder this spell, over the last decades, parts of the statistical modelling wandered more strongly into the muddy waters of superior model fit, yet understood less and less about the underlying mechanisms. Modern science has to date not sufficiently engaged with the questions how predictive modelling can lead to optimific results, and which role our lack of explicit understanding play in terms of worse outcomes. Preventing a pandemic based on a predictive model is surely good, but enforcing a space shuttle start when there is some lack of understanding of the new conditions at launch day led to the Challenger disaster. '''Many disasters of human history were a lack of understanding the unexpected.''' When our experience was pushed into the new realms of the previously unknown, and our expactation of the likelihood of such an extreme event happening was low, the impending surprise often came at a high price. Many developments - global change, increasing system [[Agency, Complexity and Emergence|complexity,]] growing inequalities - may further decrease our ability to anticipate infrequent dynamics. This calls for a shift towards maximizing the resilience of our models that may be needed under future circumstances. While this has been highlighted i.e. by the IPCC, the Stockholm resilience center and many other institutions, policy makers are hardly prepared enough, which became apparent during the COVID-19 pandemic. \n\n\n== The world can (not) be predicted ==\nFrom a predictive or explanatory viewpoint, we can break the world of statistics down into three basic types of dynamics: Linear dynamics, periodical dynamics, and non-linear dynamics. Let us differentiate between these three.\n\n'''Linear dynamics''' are increasing or decreasing at an estimate that does not change over time. All linear relations have an endpoint which these patterns do not surpass. Otherwise there would be plants growing on the top of Mount Everest, diseases would spread indefinitely, and we could travel faster than the speed of light. All this is not the case, hence based on our current understanding, there are linear patterns, which at some point cannot increase. Until then, we can observe linear dynamics: you have a higher plant diversity and biomass under increasingly favourable growing conditions, illnesses may affect more and more people in a pandemic, and we can travel faster if we - simply spoken - invest more energy and wit into our travel vehicle. All these are current facts of reality that were broken down by humans into linear patterns and mechanisms. \n\n'''Periodical dynamics''' are recognized by humans since the beginning of time itself. Within seasonal environments, plants grow in spring and summer, diseases show increases and decreases that show up-and-down patterns, and travel vehicles do not maintain their speed because of friction or gravity. Hence we can understand and interpret many of the linear phenomena we observe as cyclic or periodical phenomena as well. Many of the laws and principles that are at the basis of our understanding are well able to understand such periodic fluctuations, and the same statistics that can catch linearly increasing or decreasing phenomena are equally enabling us to predict or even understand such periodic phenomena.""]"|0.16666666666666666|1.0
40|What are the three conditions under which Poisson Distribution can be used?|Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.|"['6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> ∞ and λ = np, rate parameter, λ is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean λ > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = λ = μ =45\n\nStandard deviation = ∂ = √λ = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https://builtin.com/data-science/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=""Python"" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n</syntaxhighlight>\n\n<syntaxhighlight lang=""Python"" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n</syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=""Python"" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n</syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1\n\n<syntaxhighlight lang=""Python"" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align=\'center\')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n</syntaxhighlight>\n[[File: plot 1.png|center|700px]]']"|0.0|1.0
41|How does the Pomodoro technique work?|The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.|"['{|class=""wikitable"" style=""text-align: center; width: 100%""\n! colspan = ""4"" | Type !! colspan = ""4"" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || \'\'\'[[:Category:Productivity Tools|Productivity Tools]]\'\'\' || \'\'\'[[:Category:Team Size 1|1]]\'\'\' || \'\'\'[[:Category:Team Size 2-10|2-10]]\'\'\' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don\'t want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n\'\'\'Pomodoro is very simple. All you need is work to be done and a timer.\'\'\'  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to \'\'25 minutes = 1 ""Pomodoro""\'\').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15–30 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https://play.google.com/store/apps/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https://apps.apple.com/us/app/focus-keeper-time-management/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin.']"|0.21428571428571427|1.0
42|What is the 'curse of dimensionality'?|The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.|"['By doing so, you are also sorting the ""importance"" of the principle components in terms of the information amount it contains what is used to explain the data. To be clear, the sum of all eigenvalues is the total variability in the data. From here, you can choose to discard any PCs whose percentage of explained variances are low. In many cases, if around 80% of the variance can be explained by the first k PCs, we can discard the other (d - k) PCs. Of course, this is only one of the heuristics method to determine k.  You can also use thr elbow method (the scree plot) like in k-means.\n\n==== Summary ====\n* PCA is a feature extraction technique widely used to reduce dimensionality of datasets.\n* PCA works by calculating the eigenvectors and the corresponding eigenvalues of the initial variables in the data. These are the principle components. Number of PCs = number of eigenvectors = number of features.\n* The PCs are ranked by the eigenvalues, and iteratively show the directions in which the data spreads the most (after accounting for the previous PCs).\n* We can choose to keep a few of the first PCs that cummulatively explains the data well enough, and these are the new reduced dimension of the data.\n* Standardization is a crucial step in data pre-processing to ensure the validity of the PCA results.\n\n\n=== R Example ===\nGoing back to the example in the introduction, the dataset can be found here: https://www.kaggle.com/sdhilip/nutrient-analysis-of-pizzas\n\n<syntaxhighlight lang=""R"" line>\n# Loading library\nlibrary(tidyverse) # For pre-processing data\nlibrary(factoextra) # For visualization\ntheme_set(theme_bw()) # Set theme for plots\n\n# Load data\ndata <- read_csv(""Pizza.csv"")\nhead(data)\n</syntaxhighlight>\n\nAs shown here, there are seven measurements of nutrients for each pizza. Our goal is to reduce these seven dimensions of information down to only two, so that we can present the main patterns in our data on a flat piece of paper.\n\nTo conduct Principle Component Analysis in R, we use <code>prcomp</code>. From the original data, we only select the seven nutrient measurements as input for the function. In this function, we set <code>scale = TRUE</code> to perform scaling and centering (so that all variables will have a mean of 0 and standard deviation of 1). The result object is saved in <code>data.pca</code>\n\n<syntaxhighlight lang=""R"" line>\ndata.pca <- prcomp(data[, mois:cal]), scale = TRUE)\n</syntaxhighlight>\n\nNow we can visualize and inspect the result of the analysis. We start off by looking at the contribution of each created principle component (PC) to the overall variance in the dataset. For this, we create a spree plot:\n\n<syntaxhighlight lang=""R"" line>\nfviz_eig(data.pca)\n</syntaxhighlight>\n\n[[File: PCA_ScreePlot.png|center|500px]]\n\nLike I mentioned before, the number of PC created is equal to the number of input variables (in this cases, seven). Looking at the plot, the first two PCs combined can explain more than 90% of the dataset, an amazing number. This means this 7-dimensional dataset can be presented on a 2-dimensional space, and we still only lose less than 10% of the information. In other words, the first two PCs are the most important, and we can discard the other ones.\n\nNext, we look at the contribution of the original variables to the building of the two PCs, respectively.\n\n<syntaxhighlight lang=""R"" line>\nfviz_contrib(data.pca, choice = ""var"", axes = 1)\nfviz_contrib(data.pca, choice = ""var"", axes = 2)\n</syntaxhighlight>\n\n[[File: PCA_ContribPlot.png|center|500px]]\n\nThe red, dashed line refers to the case where all of the variable contribute equally. In the left plot, ash, fat, sodium and carbohydrates contribute substantially to the forming of the first PC. On the other hand, moisture and calories influence the second PC heavily, as seen in the right plot.\n\nFinally, we look at the biplot of the analysis. This reveals the underlying patterns of the dataset:\n\n<syntaxhighlight lang=""R"" line>\nfviz_pca_biplot(data.pca, label = ""var"", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlot.png|center|500px]]']"|0.06666666666666667|0.0
43|Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?|Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.|"['<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=""Python"" line>\nmodel_1 = smf.ols(formula=\'points ~ ID + quanti\', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).\n\nresult = model_1.fit() ## estimates the regression\n\nprint(result.summary()) ## print the result\n</syntaxhighlight>\n\n[[File:new_attempt.png]]\n\nLooking at the Breusch-Pagan test, we can see that we cannot reject the assumption of homoscedasticity. \nConsidering the correlation coefficients, no statistically significant relationship can be identified. The positive relationship between quanti and points can be found again, but it is not statistically significant.\n\n<syntaxhighlight lang=""Python"" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_2 = smf.ols(formula=\'points ~ ID + sex\', data=data)\nresult_bp2 = model_2.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp2.resid, result_bp2.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(""Breusch-Pagan test p-value:"", bp2_test[1])\n\n# Set the value for maxlags\nmaxlags = 25  # Update this value as needed']"|0.0|0.0
44|How did Shell contribute to the advancement of Scenario Planning?|"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."|"['[[File:ConceptVisualisationScenarioPlanning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Scenario Planning]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n| [[:Category:Inductive|Inductive]] || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Individual|Individual]] || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| [[:Category:Present|Present]] || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Scenario Planning is a systematic designation of potential futures to enable long term strategic planning.\n\n==Background==\n[[File:Scenario planning.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Scenario Planning until 2019.\'\'\' Search terms: \'scenario planning\', \'scenario construction\', \'scenario-based\', \'scenario study\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s\'\'\'. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were ""(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]"" in the US. (Amer et al. 2013, p.24).\n\n\'\'\'Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.\'\'\' At the time, corporate planning was traditionally ""(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude."" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the ""Unified Planning Machinery"". The idea was to listen to planners\' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell\'s management to deal with the 1973 and 1981 oil crises (1). Shell\'s success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders\' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.']"|0.19444444444444445|1.0
45|Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?|Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.|"['[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br/>\n{|class=""wikitable"" style=""text-align: center; width: 50%""\n! colspan = 3 | Method categorization\n|-\n| \'\'\'[[:Category:Quantitative|Quantitative]]\'\'\' || colspan=""2"" | \'\'\'[[:Category:Qualitative|Qualitative]]\'\'\'\n|-\n|\'\'\'[[:Category:Inductive|Inductive]]\'\'\' || colspan=""2""| \'\'\'[[:Category:Deductive|Deductive]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Individual|Individual]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:System|System]]\'\'\' || \'\'\'[[:Category:Global|Global]]\'\'\'\n|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n\'\'\'In short:\'\'\' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|\'\'\'SCOPUS hits per year for Social Network Analysis until 2019.\'\'\' Search terms: \'Social Network Analysis\' in Title, Abstract, Keywords. Source: own.]]\n\n\'\'\'One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel\'\'\'. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued ""against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions."" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.']"|0.14285714285714285|0.5
46|What are the limitations of Stacked Area Plots?|Stacked Area Plots are not suitable for studying the evolution of individual data series.|"['\'\'\'Note:\'\'\' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nThis entry aims to introduce Stacked Area Plot and its visualization using R’s <syntaxhighlight lang=""R"" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> has the following syntax:\n\n\'\'\'Syntax\'\'\': <syntaxhighlight lang=""R"" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=""R"" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n</syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=""R"" inline>""uspopage""</syntaxhighlight> using the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> from the <syntaxhighlight lang=""R"" inline>ggplot2 package</syntaxhighlight>:\n\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n</syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of “Blues”, and add labels.']"|0.08163265306122448|1.0
47|What is the purpose of Thought Experiments?|"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."|"['All this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https://en.wikipedia.org/wiki/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. \'\'\'Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.\'\'\'\n\n== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people\'s actions or decision, and the \'\'What if\'\' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. \'\'\'Schrödinger\'s Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.\'\'\' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment ""What if I have an unlimited amount of ice cream?"" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the ""unlimited"", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.\n\n== Normativity ==\nThought experiments are as we have already learned as normative as are our assumptions about the world. Hume argued that Thought Experiments are based on the laws of nature, yet here I would disagree. While many famous Thought Experiments are about the physical world, others are only made up in our minds. Many Thought Experiments are downright impossible to be matched with our reality, and are even explicitly designed to do this, such as Thomas Nagels Encyclopaedia of the Universe. It is therefore important to realise that basically all Thought Experiments are in essence normative, and one could say even downright subjective. Building on Derek Parfit, I would however propose a different interpretation, and propose that we should not measure the normativity of Thought Experiments through the design and setting, but instead by their outcome. Many people might come to the same conclusions within a given Thought Experiment, and some conclusion drawn from Thought Experiments may matter more than others. Consequently the penultimate question - also for Derek Parfit - is whether there are some Thought Experiments that are not normative. \n\n== Outlook ==\nMuch of [[Glossary|art]] and the media can be seen as a Thought Experiment, and there are ample examples that Thought Experiments in the media and the arts triggered or supported severe societal transformations. Thought Experiments are of equal importance in ethics and physics, and the bridge-building of the methodological approach should not be overestimated. Examples from the past prove that Thought Experiments can enable a great epistemological flexibility and diversity. This flexibility is even so large that Thought Experiments serve as a bridge between the epistemological and the ontological, or in other words between everything we know - and how we know it - and everything we believe. By enabling the transformation of our own most individual thoughts, Thought Experiments may provide a boat or a bridge to link the metaphysical with the world of knowledge.\n\n== Key Publications ==\nParfit, D. (2011). \'\'On what matters.\'\' Oxford University Press.']"|0.325|1.0
48|What is temporal autocorrelation?|Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.|"['The autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df[\'usage_kwh\'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel(\'Lag (hours)\') ## title x axis\nplt.ylabel(\'Autocorrelation of electricity usage with confidence interval\') ## title y axis\nplt.title(\'\') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n</syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days</small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered “close” to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object\'s neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object\'s neighborhood is significantly lower than the densities of its neighbors\' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don\'t know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df[\'usage_kwh\'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (""to_numpy()"") and reshapes it to a single column (""reshape (-1, 1)""), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf[\'outlier\'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df[\'outlier\']] ## creates  a column where only outliers are selected']"|0.0|0.0
49|What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?|The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models.|"[""[[File:Lueneburg 2030.jpg|thumb|left|Lüneburg 2030+ ist ein wunderbares Beispiel für eine Reihe von Realwelt-Eperimenten. Diese Karte bietet einen Überblick über die verschiedenen Experimente.]]\n\nIn dieser Hinsicht sind [https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 Realwelt-Experimente] die neueste Entwicklung in der Diversifizierung des Experimentierfeldes. Diese Art von Experimenten wird derzeit in der Literatur umfassend untersucht, doch ich erkenne in der verfügbaren Literatur bisher kein einheitliches Verständnis dessen, was Experimente aus der realen Welt sind. Diese Experimente können jedoch als eine Fortsetzung des Trends der natürlichen Experimente angesehen werden, bei denen eine lösungsorientierte Agenda versucht, eine oder mehrere Interventionen zu generieren, deren Wirkungen oft in Einzelfällen getestet werden, wobei die Bewertungskriterien jedoch bereits vor der Durchführung der Studie klar sind. Die meisten Studien haben dies bisher mit Nachdruck definiert; dennoch zeichnet sich die Entwicklung von Experimenten aus der realen Welt erst langsam ab.\n\n\n== Experimente bis heute ==\nUnter dem Oberbegriff 'Experiment' werden somit unterschiedliche methodische Ansätze zusammengefasst. Während einfache Manipulationen wie medizinische Verfahren bereits in der Aufklärung als Experiment bekannt waren, gewann der Begriff 'Experiment' im Laufe des 20. Jahrhunderts an Bedeutung. Botanische Experimente wurden schon lange vorher durchgeführt, aber es waren die Agrarwissenschaften, die die notwendigen methodischen Entwürfe zusammen mit den geeigneten [[Bachelor Statistics Lecture|statistischen Analysen]] entwickelten und damit eine statistische Revolution auslösten, die in zahlreichen Wissenschaftsbereichen Wellen schlug. Die Varianzanalyse ([[ANOVA]]) wurde zum wichtigsten statistischen Ansatz zu diesem Zweck und ermöglichte die systematische Gestaltung von Versuchsanordnungen, sowohl im Labor als auch im landwirtschaftlichen Bereich.""]"|0.0|0.0
