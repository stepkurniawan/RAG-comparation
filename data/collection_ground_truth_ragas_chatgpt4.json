[
    {
        "contexts" : "A/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.",
        "summary" : "A/B testing is a method used to compare two versions of a product or content by assigning audiences to view either the control version or the treatment version. The effect is measured on a specific metric like clicks or conversions. This method is often used in website optimization, where the control version is the default and the treatment version has a single variable change.",
        "question" : "What is A/B testing and how is it commonly used?",
        "ground_truths" : "A/B testing is a method used to compare the performance of two versions of a product or content. It is done by randomly assigning similarly sized audiences to view either the control version or the treatment version over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website."
    },
    {
        "contexts" : "An important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of scientific experiments is followed, consisting of a planning phase, an execution phase, and an evaluation phase.",
        "summary" : "A/B testing has the advantage of establishing causal relationships with high probability, transforming decision making into a scientific, evidence-based process.",
        "question" : "What is the advantage of A/B testing?",
        "ground_truths" : "The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process."
    },
    {
        "contexts" : "During the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.",
        "summary" : "In the planning phase of A/B testing, a goal and hypothesis are formulated, and a study design is developed specifying the sample size, study duration, and metrics to be measured. This phase is crucial for ensuring the test's reliability and validity.",
        "question" : "What happens during the planning phase of A/B testing?",
        "ground_truths" : "During the planning phase of A/B testing, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured."
    },
    {
        "contexts" : "The execution phase involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan. During this phase, users are randomly assigned to the control or treatment group ensuring that the study is conducted in a controlled and unbiased manner.",
        "summary" : "The execution phase of A/B testing involves implementing the study design, collecting data, and monitoring the study. Users are randomly assigned to the control or treatment group to ensure the study is conducted in a controlled and unbiased manner.",
        "question" : "What happens in the execution phase in A/B testing?",
        "ground_truths" : "The execution phase in A/B testing involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan."
    },
    {
        "contexts" : "The evaluation phase involves analyzing the data collected during the study and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about whether there was a statistical significant difference between the treatment group and the control group. One commonly used method is calculating the p-value of the statistical test, or by using Bayes' theorem calculating the probability that the treatment had a positive effect based on the observed data and the prior beliefs about the treatment.",
        "summary" : "The evaluation phase of A/B testing involves analyzing the collected data and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about the difference between the treatment and control group. Common methods include calculating the p-value of the statistical test or using Bayes' theorem to calculate the probability of the treatment having a positive effect.",
        "question" : "What is the purpose of the evaluation phase in A/B testing?",
        "ground_truths" : "The purpose of the evaluation phase in A/B testing is to analyze the data collected during the study and interpret the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions."
    },
    {
        "contexts" : "A/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.",
        "summary" : "A/B testing has several advantages over traditional methods of evaluation. It allows for a more controlled and systematic comparison of the treatment and control version, reduces potential bias through random assignment of users to groups, and allows for data collection over time, providing insights into the long-term effects of the treatment.",
        "question" : "What are the advantages of A/B testing over traditional methods of evaluation?",
        "ground_truths" : "A/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. It allows for a more controlled and systematic comparison of the treatment and control version. It also allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Furthermore, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment."
    },
    {
        "contexts" : "Despite its advantages, A/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.",
        "summary" : "A/B testing has limitations. It is only applicable to products or designs that can be easily compared in a controlled manner. The results may not always be generalizable to the broader population if the sample used in the test is not representative. It requires a clear separation between control and treatment and may not be suitable for testing complex products or processes where the relationship between the versions is not easily defined or isolated from other factors.",
        "question" : "What are the limitations of A/B testing?",
        "ground_truths" : "The limitations of A/B testing include its applicability only to products or designs that can be easily compared in a controlled manner. The results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. It requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome."
    },
    {
        "contexts": "With a rise in knowledge during the Enlightenment, it became apparent that the controlled setting of a laboratory were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the real world. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions.",
        "summary": "The rise in knowledge during the Enlightenment led to the realization that laboratory settings were not sufficient for experiments, as there was more knowledge to be discovered in the real world. Observations can be unreliable and errors in measurements were prevalent in the 18th and 19th century. The demand for more food and the availability of seed varieties and fertilizers raised the question of how to conduct experiments under field conditions.",
        "question": "What led to the realization that laboratory settings were not sufficient for experiments?",
        "ground_truths": "The rise in knowledge during the Enlightenment"
    },
    {
        "contexts": "Consequently, building on the previous development of the t-test, Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). It allowed for the comparison of variables from experimental settings, comparing how a continuous variable fared under different experimental settings. Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic testing of hypotheses under field conditions, taming variance through replicates. The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in mean values between different treatments. For instance, it became possible to compare different levels of fertiliser to optimise plant growth.",
        "summary": "Fisher proposed the Analysis of Variance (ANOVA) as a way to compare variables from experimental settings and to compare how a continuous variable fared under different experimental settings. Experiments shifted from the laboratory to the field, increasing sample size to account for factors that could not be controlled. ANOVA allowed for systematic testing of hypotheses under field conditions and comparing mean values between different treatments.",
        "question": "What did Fisher propose as a way to compare variables from experimental settings?",
        "ground_truths": "Analysis of Variance (ANOVA)"
    },
    {
        "contexts": "The ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments. In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.",
        "summary": "ANOVA is a deductive statistical method that allows for the comparison of a continuous variable under different treatments in a designed experiment. It is one of the most important statistical models and can compare multiple categories in terms of their mean value regarding a continuous variable.",
        "question": "What does ANOVA allow for in a designed experiment?",
        "ground_truths": "Comparison of a continuous variable under different treatments"
    },
    {
        "contexts": "Single factor analysis that are also called 'one-way ANOVAs' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called randomisation, which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or 'two way ANOVAs' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.",
        "summary": "Single factor analysis, also known as one-way ANOVAs, investigate one factor variable while keeping all other variables constant. Designs with multiple factors, or two-way ANOVAs, test for two or more factors and also test for interactions. This increases the necessary sample size and the degrees of freedom may dramatically increase depending on the number of factor levels and their interactions.",
        "question": "What is the difference between one-way and two-way ANOVAs?",
        "ground_truths": "One-way ANOVA investigate one factor variable, while two-way ANOVA investigates two or more factors and their interactions. "
    },
    {
        "contexts": "The data that is of relevance to ANOVAs can be ideally visualised in boxplots, which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the regression model, and thus demands data that is normally distributed. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels.",
        "summary": "Boxplots are ideal for visualizing the data relevant to ANOVAs as they allow for an initial visualization of the data distribution. The classical ANOVA relies on the regression model and requires normally distributed data. One can use boxplots to determine if there is a significant difference between factor levels, but caution must be taken with skewed distributions. The p-values are the overarching guideline for ANOVA and indicate the significance of the difference between factor levels.",
        "question": "What is the overarching guideline for ANOVA?",
        "ground_truths": "p-values"
    },
    {
        "contexts": "The ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses.",
        "summary": "ANOVA is a powerful tool for reducing variance in field experiments or complex laboratory experiments by accounting for variance in repeated measures. It is the most robust method for deductive experiments, but has also been used for inductive data analysis, which goes against Fisher's original idea of clear robust designs and rigid hypothesis testing.",
        "question": "What is the ANOVA a powerful for?",
        "ground_truths": "Reducing variance in field experiments or complex laboratory experiments"
    },
    {
        "contexts": "The ANOVA is equally limited as the regression, as both build on the normal distribution. Extensions of the ANOVA translated its analytical approach into the logic of generalised linear models, enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs.",
        "summary": "ANOVA and regression are limited by their reliance on the normal distribution. Extensions of ANOVA allow for the implementation of other distributions through generalised linear models. ANOVA has high demands in terms of data and sample sizes, making it suitable for testing constructed settings. ANOVA attempts to approximate causality by creating more rigid designs.",
        "question": "What does ANOVA attempt to approximate?",
        "ground_truths": "Causality"
    },
    {
        "contexts": "The ANOVA can serve as a robust basis if its limitations are clearly indicated and if it adds to parts of a larger picture of knowledge. The ANOVA was one of the most relevant contributions of statistics to the developments of the 20th century, but it has limitations and is increasingly being criticized for its reliance on p-values. New approaches, such as mixed effect models, are gaining momentum and there is a recognition of the limitations of ANOVA-based designs. The ANOVA remains one of the fundamental models of deductive statistics, but more complex analysis methods are evolving with the wider availability of computers.",
        "summary": "ANOVA can serve as a robust basis if its limitations are acknowledged and if it contributes to a larger picture of knowledge. It was a significant contribution to statistics in the 20th century, but is increasingly criticized for its reliance on p-values. New approaches, such as mixed effect models, are gaining momentum and there is recognition of the limitations of ANOVA-based designs. ANOVA remains a fundamental model of deductive statistics, but more complex analysis methods are emerging.",
        "question": "What model is gaining momentum as an alternative to ANOVA?",
        "ground_truths": "Mixed effect models"
    },
    {
        "contexts": "Designing an ANOVA-based design demands experience and knowledge of the previous literature. ANOVA-based designs are typically part of the continuous development in normal science. However, other more advanced approaches have gained momentum, such as mixed effect models, information theoretical approaches, and structural equation models. The ANOVA is often used to analyze inductive datasets, but this can infer several problems from a statistical standpoint and a critical perspective rooted in a coherent theory of science. ANOVA is widely used for group comparison for a continuous variable, but simple designs are being questioned as complexity increases.",
        "summary": "Designing an ANOVA-based design requires experience and knowledge of the previous literature. ANOVA is often part of the continuous development in normal science, but more advanced approaches have gained momentum. ANOVA is used to analyze inductive datasets, but this can lead to statistical and critical problems. ANOVA is widely used for group comparison, but simple designs are being questioned as complexity increases.",
        "question": "What model is often used for group comparison for a continuous variable?",
        "ground_truths": "ANOVA"
    },
    {
        "contexts": "The Analysis of Variance was one of the most relevant contributions of statistics to the developments of the 20th century. Lately, frequentist statistics was increasingly criticized for its reliance on p-values. Also, the reproducibility crisis highlights the limitations of ANOVA-based designs, which are often not reproducible. Psychological research faces this challenge for instance by pre-registering studies, indicating their statistical approach before approaching the data, and other branches of science are also attempting to do more justice to the limitations of the knowledge of experiments. In addition, new ways of experimentation of science evolve, introducing a systematic approach to case studies and solution oriented approaches. This may open a more systematic approach to inductive experiments, making documentation a key process in the creation of a canonized knowledge. Scientific experiments were at the forefront of developments that are seen more critically regarding their limitations. Taking more complexities into account, ANOVAs become a basis for more advanced statistics, and they can indeed serve as a robust basis if the limitations are clearly indicated, and the ANOVA designs add to parts of a larger picture of knowledge.",
        "summary": "ANOVA was a significant contribution to statistics in the 20th century, but has faced criticism for its reliance on p-values and the reproducibility crisis. Psychological research is addressing this challenge through pre-registering studies and other branches of science are recognizing the limitations of experimental knowledge. New ways of experimentation are evolving, introducing a systematic approach to case studies and solution-oriented approaches. ANOVA can serve as a basis for more advanced statistics if its limitations are acknowledged and if it contributes to a larger picture of knowledge.",
        "question": "What is addressing the challenge of relying too much on p-values and the reproducibility crisis?",
        "ground_truths": "Psychological research"
    },
    {
        "contexts": "Theoretical\n\nCrawley, M. J. (2007). The R book. John Wiley & Sons.",
        "summary": "Crawley, M. J. (2007). The R book. John Wiley & Sons.",
        "question": "What is the title of a key publication on ANOVA?",
        "ground_truths": "The R book"
    },
    {
        "contexts": "Probability indicates the likelihood whether something will occur or not. Typically, probabilities are represented by a number between zero and one, where one indicates the hundred percent probability that an event may occur, while zero indicates an impossibility of this event to occur. The concept of probability goes way back to Arabian mathematicians and was initially strongly associated with cryptography. With rising recognition of preconditions that need to be met in order to discuss probability, concepts such as evidence, validity, and transferability were associated with probabilistic thinking. Probability plays also a role when it came to games, most importantly rolling dice. With the rise of the Enlightenment many mathematical underpinnings of probability were explored, most notably by the mathematician Jacob Bernoulli.",
        "summary": "Probability is a measure of the likelihood of events occurring, represented on a scale from zero (impossibility) to one (certainty). It has a historical background dating back to Arabian mathematicians and was initially linked with cryptography. Over time, it became associated with concepts like evidence and validity, and it played a role in games like rolling dice.",
        "question": "What is probability?",
        "ground_truths": "Probability is a measure of the likelihood of events occurring, represented on a scale from zero (impossibility) to one (certainty)."
    },
    {
        "contexts": "Gauss presented a real breakthrough, due to the discovery of the normal distribution. It allowed the feasible approach to link sample size of observations with an understanding of the likelihood how plausible these observations were. Again building on Sir Francis Bacon, the theory of probability reached its final breakthrough once it was applied in statistical hypothesis testing. It is important to notice that this would throw modern statistics into an understanding through the lens of so-called frequentist statistics. This line of thinking dominates up until today, and is widely built on repeated samples to understand the distribution of probabilities across a phenomenon.",
        "summary": "Gauss made a significant contribution by discovering the normal distribution, which connected sample size with the plausibility of observations. This development was influenced by Sir Francis Bacon and became crucial in statistical hypothesis testing, particularly in the contexts of frequentist statistics.",
        "question": "What was Gauss's contribution to probability?",
        "ground_truths": "Gauss made a significant contribution by discovering the normal distribution, which connected sample size with the plausibility of observations. "
    },
    {
        "contexts": "Centuries ago, Thomas Bayes proposed a dramatically different approach. Here, an imperfect or a small sample would serve as a basis for statistical inference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while Bayesian statistics work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this theory explored deeply, and builds the foundation of branches in data science and machine learning. The two approaches are also often coined as objectivists for frequentist probability fellows, and subjectivists for followers of Bayes theorem.",
        "summary": "Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data. Frequentist statistics have been foundational in modern science, including methodological design and statistical tests, while Bayesian statistics gained prominence with the advent of modern computers, forming the basis of data science and machine learning.",
        "question": "What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?",
        "ground_truths": "Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data."
    },
    {
        "contexts": "Another perspective on the two approaches can be built around the question whether we design studies - or whether we base our analysis on the data we just have. This debate is the basis for the deeply entrenched conflicts you have in statistics up until today, and was already the basis for the conflicts between Pearson and Fisher. From an epistemological perspective, this can be associated with the question of inductive or deductive reasoning, although not many statisticians might not be too keen to explore this relation deeply, since they are often stuck in either deductive or inductive thinking, but not both.",
        "summary": "The conflict between frequentist and Bayesian approaches in statistics revolves around whether to design studies in advance or base analysis on available data. This debate has led to long-standing conflicts in statistics, including the historical disputes between Pearson and Fisher. It touches on questions of inductive and deductive reasoning, although statisticians may not always delve deeply into this aspect.",
        "question": "What is the ongoing conflict in statistics related to the approach of designing studies versus analyzing available data?",
        "ground_truths": "The conflict between frequentist and Bayesian approaches in statistics revolves around whether to design studies in advance or base analysis on available data."
    },
    {
        "contexts": "While probability today can be seen as one of the core foundations of statistical testing, probability as such is increasingly criticized. It would exceed this chapter to discuss this in depth, but let me just highlight that without understanding probability, much of the scientific literature building on quantitative methods is hard to understand. What is important to notice, is that probability has trouble considering Occam's razor. This is related to the fact that probability can deal well with the chance of an event to occur, but it widely ignores the complexity that can influence such a likeliness. Modern statistics explore this thought further but let us just realize here: without learning probability we would have trouble reading the contemporary scientific literature.",
        "summary": "Probability is a core foundation of statistical testing, but it faces increasing criticism. It is essential for understanding quantitative scientific literature, but it struggles with considering Occam's razor and the complexity that can affect likelihood. Modern statistics delve into these issues, highlighting the importance of learning probability to comprehend contemporary scientific literature.",
        "question": "Why is probability both essential and criticized in the contexts of statistical testing and scientific literature?",
        "ground_truths": "Probability is a core foundation of statistical testing, but it faces increasing criticism. It is essential for understanding quantitative scientific literature, but it struggles with considering Occam's razor and the complexity that can affect likelihood."
    },
    {
        "contexts": "The probability can be best explained with the normal distribution. The normal distribution basically tells us through probability how a certain value will add to an array of values. Take the example of the height of people, or more specifically people who define themselves as males. Within a given population or country, these have an average height. This means in other words, that you have the highest chance to have this height when you are part of this population. You have a slightly lower chance to have a slightly smaller or larger height compared to the average height. And you have a very small chance to be much smaller or much taller compared to the average. In other words, your probability is small to be very tall or very small. Hence the distribution of height follows a normal distribution, and this normal distribution can be broken down into probabilities. In addition, such a distribution can have a variance, and these variances can be compared to other variances by using a so-called f test. Take the example of height of people who define themselves as males. Now take the people who define themselves as females from the same population and compare just these two groups. You may realize that in most larger populations these two are comparable. This is quite relevant when you want to compare the income distribution between different countries. Many countries have different average incomes, but the distribution across the average as well as the very poor and the filthy rich can still be compared. In order to do this, the f test is quite helpful.",
        "summary": "Probability is best explained using the normal distribution, which describes how values add to a dataset. For instance, consider the height of males in a population; it follows a normal distribution, with the highest probability near the average height. Deviating significantly from the average has a low probability. This distribution can be further analyzed with variance and compared using an f test. The f test is useful for comparing income distributions between countries with varying average incomes.",
        "question": "How does the f test help in comparing income distributions among different countries?",
        "ground_truths": "In normal distribution, deviating significantly from the average has a low probability. This distribution can be further analyzed with variance and compared using an f test. The f test is useful for comparing income distributions between countries with varying average incomes."
    },
    {
        "contexts": "Let us perform an F test in R using the 'women' dataset. We want to compare the variances of height and weight for American women aged 30-39. First, we need to test the normality of our samples by creating q-q plots for both height and weight. Both plots show that the data is normally distributed. Next, we conduct the F-Test to test for the equality of variance, where the null hypothesis (H0) is that the ratio of variance is equal to 1, and the alternative hypothesis (H1) is that the ratio of variance is not equal to 1. The result of the F test indicates that we reject the null hypothesis due to a low p-value.",
        "summary": "An F test is performed in R to compare the variances of height and weight for American women aged 30-39. The data's normality is confirmed through q-q plots. The test's result leads to the rejection of the null hypothesis (H0) in favor of the alternative hypothesis (H1).",
        "question": "What is the purpose of performing an F test in a dataset?",
        "ground_truths": "An F test is performed in R to compare the variance. The data's normality is confirmed through q-q plots. The test's result leads to confirmation of the null hypothesis (H0) or the alternative hypothesis (H1)."
    },
    {
        "contexts" : "Agency, Complexity and Emergence are three concepts that can be understood as lenses through which we can see results that originate in theoretical considerations, as well as information we access through empirical research. Since such concepts are binding the two realms of scientific inquiry - theory and practice - I call the three concepts of agency, complexity and emergence boundary objects. They are good examples of ideas that are most relevant within the philosophy of science. In addition, they strongly determine our fundamental and normative view of the world as well as the consequences of the knowledge we gain from the world. If I have for instance the world view that no free will exists, then this will also be a fundamental influence on how I interpret results from empirical inquiry of the actions of people. Many would agree that people have the potential to have free will, just as all milk can be turned into butter. There are other concepts besides agency, complexity and emergence that are worth pursuing form the perspective of philosophy of science, but for the sake of simplicity and priority, we will only deal with these three boundary object here. Let us start with an attempt to show the diversity of definitions of these concepts.",
        "summary" : "Agency, Complexity and Emergence are three concepts that bind the realms of scientific theory and practice, referred to as 'boundary objects'. They influence our fundamental view of the world and how we interpret empirical results. The belief in free will, for example, can influence how one interprets the actions of people. While there are other concepts worth exploring, this discussion will focus on these three.",
        "question" : "What are the three concepts that bind the realms of scientific theory and practice?",
        "ground_truths" : "The three concepts that bind the realms of scientific theory and practice are Agency, Complexity and Emergence."
    },
    {
        "contexts" : "The most simple definition of agency that is widely considered to be most relevant from a methodological standpoint is that agency is defined as the capacity of an individual to act intentionally with the assumption of a causal outcome based on this action. However, within different discourses and disciplines there is a wide variety of approaches that consider broader or more narrow definitions of agency. There is a first disagreement whether an agent should have a mental state that is able to anticipate the outcome of the action, and such diverse realms as neuroscience and philosophy disagree on that point already. For instance, reflexes are instincts, and it is very difficult to track down whether such actions are intentional and contain assumptions about the outcome. Therefore, broader definitions of agency include the action-based adaptation of an agent to his, her or its environment, which includes also much of the animal kingdom. This broad definition of agency will be ignored here, although it should be stated that the actions of some higher animals indicate intention and mental states that may be able to anticipate. These are however difficult to be investigated from a methodological standpoint, although much can be expected from this direction in the future.",
        "summary" : "Agency is defined as the capacity of an individual to act intentionally with the assumption of a causal outcome. However, there are disagreements on whether an agent should have a mental state that can anticipate the outcome. Broader definitions of agency include the action-based adaptation of an agent to its environment, which includes much of the animal kingdom. The actions of some higher animals indicate intention and mental states that may be able to anticipate, but these are difficult to investigate methodologically.",
        "question" : "What is the basic definition of agency in the contexts of scientific theory and practice?",
        "ground_truths" : "In the contexts of scientific theory and practice, the basic definition of agency is the capacity of an individual to act intentionally with the assumption of a causal outcome based on this action."
    },
    {
        "contexts" : "What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency. Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy's law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of agency. There is thus a metaphysical problem when we try to differentiate the agent, their actions, and the consequences of their actions. One could claim that this can be solved by focussing on the consequences of the actions of agents alone. However, this consequentialist view is partly a theoretical consideration, as this view can create many interesting experiments, but does not really help us to solve the problem of unintentional acts per se. Still, consequentialism and a focus on mere actions is also relevant because it allows to step away from a single agent and focus instead on the interactions within the agency of several actors, which is a good start for the next concept.",
        "summary" : "Actions of agents must be wilful, and serendipitous acts are not part of agency. Non-anticipated consequences based on causal chains pose a problem in agency. Acknowledging serendipity and Murphy's law is challenging. The metaphysical problem arises when differentiating agents, their actions, and consequences. Focusing solely on consequences is a theoretical consideration that doesn't fully address unintentional acts. Consequentialism and a focus on actions allow the study of interactions within agencies of multiple actors.",
        "question" : "Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?",
        "ground_truths" : "Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency."
    },
    {
        "contexts" : "Many phenomena in nature are simple, and follow simple rules. Many other phenomena are not simple, but instead can be defined as \"complex\". In order to negotiate between the two there is the fundamental law of Occam's razor, which defines that all things are as simple as possible, and as complex as necessary. Complex system theory strongly developed over the last decades, and created ripples into all empirical science and beyond. What is however problematic is that no unified and universally accepted definition of complexity exists, which is quite ironic. It seems that complexity in itself is complex. Here, I will therefore focus on some of the main arguments and characteristics that are relevant from a methodological standpoint. First of all, I will restrict all considerations to 'complex systems'. To this end, I simply define systems as any number of individuals or elements that interact. Complex Systems, then, are systems that are composed of many components which may interact with each other in various ways and which are therefore difficult to model. Specific properties include non-linearity, emergence, adaptation and feedback loops. From a methodological standpoint we should be able to observe these interactions, while from a philosophical standpoint we should be able to reflect upon them. For more background on the definition of System Boundaries, please refer to this entry.",
        "summary" : "Occam's razor states that all things should be as simple as possible, yet as complex as necessary. Complex system theory has strongly developed in recent decades. However, there is no universally accepted definition of complexity, making it complex in itself. This discussion focuses on characteristics of complex systems, defined as systems with many components that interact in various ways, making them difficult to model. These systems exhibit non-linearity, emergence, adaptation, and feedback loops.",
        "question" : "What is the fundamental law of Occam's razor and how does it relate to complexity?",
        "ground_truths" : "The fundamental law of Occam's razor states that all things should be as simple as possible and as complex as necessary. It relates to complexity by emphasizing the need for simplicity in understanding complex phenomena."
    },
    {
        "contexts" : "Such complex interactions can be simply adaptive, which is a key characteristic of many organisms and systems, but we may not be able to anticipate such adaptations. Through interactions, networks may form that can be observed, yet not be anticipated. Interactions can create feedback mechanisms or actions that reinforce patterns, creating what many call 'complex interactions'. I would argue that these interactions become often unpredictable because they exceed our original assumptions about the dynamics that are possible in the given system. The complexity of such dynamics is then rooted in our own imperfect assumptions about such system dynamics.",
        "summary" : "Adaptive interactions are common in organisms and systems, but anticipating adaptations can be challenging. Networks formed through interactions may be observable but not predictable. Interactions can lead to unpredictable complex interactions that defy our assumptions about system dynamics, rooted in our imperfect understanding.",
        "question" : "Why do interactions in complex systems often become unpredictable?",
        "ground_truths" : "Interactions in complex systems often become unpredictable because they exceed our original assumptions about the dynamics possible in the system."
    },
    {
        "contexts" : "Another phenomenon in complex systems is the question of non-linearity. Non-linear behavior is - mechanically speaking - changes that occur strongly and suddenly at the same time, and not gradually and continuously. Non-linear dynamics in systems are often occurring because the dynamics we observe are mediated by another factor. This additional and unknown factor or variable is the original trigger that in interaction with other parts of the system leads to non-linear system dynamics. The third characteristic in complex systems is related to spontaneous order. Spontaneous order relates to \"the spontaneous emergence of order out of seeming chaos\" (wikipedia), i.e. the creation of structures and order where it could not be anticipated. An example is the synchronisation of clapping noises in a crowd. Such spontaneous orders can often not be anticipated, and provide thus a problem when considering agency of systems. A lack of possible anticipation of feedback loops and non-linear behavior can thus lead to a spontaneous order that highlights our own imperfections when understanding systems. Such system states can be called emergent.",
        "summary" : "Non-linear behavior in complex systems involves sudden and strong changes, not gradual ones. It often occurs due to an additional, unknown factor triggering non-linear dynamics when interacting with other system parts. Spontaneous order is the emergence of order from apparent chaos, like synchronized clapping in a crowd. Such orders are often unpredictable and pose challenges when considering agency in systems, as the anticipation of feedback loops and non-linear behavior is lacking, leading to emergent system states.",
        "question" : "What is spontaneous order in complex systems?",
        "ground_truths" : "Spontaneous order in complex systems refers to the spontaneous emergence of order out of apparent chaos, for example a synchronized clapping in a crowd. "
    },
    {
        "contexts" : "If two or more entities together have a characteristic or behaviour that could not be anticipated only based on their individual parts, then this non-anticipated property is called \"emergent\". To me, a good example of emergence are the Beatles. The Beatles were much more than just John, Paul, George and Ringo. Another prominent examples is water, which can extinguish fire. Oxygen and hydrogen do quite the opposite with fire, they propel it. These two examples show one of the key problems of emergence: it is quite simple to find examples for it, yet it is very hard to find underlying laws or even lines of thinking that can be used to approximate principles of emergence. Again, emergence is - just as complexity - a fluid if not untamable concept in itself. Empirical researchers are fascinated when they encounter it, yet it cannot be solved in itself, because that is what emergence is all about. While I will leave this to philosophy, one might argue that some things are not meant to be anticipated. Humans today are unlocking ever more strata of reality which has allowed us to capture many emergent phenomena, such as quantum physics, organic chemistry, behavior biology and many higher system dynamics. All emergent phenomena build on interconnectedness of the parts that interact and thus jointly emerge. Another relevant criterion is defined as fallacy of division - parts of a whole may show differing behavior than the whole. This is again related to interconnectedness, but also to the variance that might exist within a population. Many phenomena of the natural world show patterns of emergence, such as the physics of sand. Chemical reactions, biological interactions and our economical system in itself are all emergent. Yet, if we cannot predict them, how can we empirically approach them?",
        "summary" : "Emergence occurs when two or more entities together exhibit characteristics or behaviors that cannot be anticipated from their individual parts. Examples include the Beatles, who were more than their individual members, and water, which extinguishes fire. However, finding underlying laws or principles for emergence is challenging. Emergence, like complexity, is a fluid concept. Empirical researchers encounter it, but it cannot be fully solved. Emergent phenomena rely on interconnectedness, and sometimes parts of a whole behave differently than the whole. Natural phenomena, like sand physics, chemical reactions, biological interactions, and the economy, exhibit emergence. However, predicting emergent phenomena remains a challenge.",
        "question" : "What is emergence, and why is it challenging to find underlying laws or principles for it?",
        "ground_truths" : "Emergence is when two or more entities together exhibit characteristics or behaviors that cannot be anticipated from their individual parts. It is challenging to find underlying laws or principles for emergence because it is a fluid concept, and while examples are abundant, identifying overarching principles remains difficult."
    },
    {
        "contexts" : "Considering these three concept, from a methodological standpoint we should consequently ask which entities lack agency, which systems are simple, and which interconnected phenomena defy any emergence. Falsification is one of the most important principles of science, and applying this approach to these three concepts can help us approximate the current methodological frontier in science. The word \"current\" is of central importance here, because we have to understand that all three concepts are at least partly normative, because they revolve around our current state of knowledge. Individuals that we deny agency now may be attributed agency by our grid of theory and methods at some point. Systems that seem complex today may be simple in the future. Emergent phenomena today may enable predictability of future interconnections. To this end, these three concepts are merely snapshots of our own ignorance. Right now we cannot know whether in the very long run all agency will be tracked, all systems will be simple, and all interconnections predictable, which right now only philosophy can examine. I think empirical research right now is incapable to answer how much people in the future may be able to understand, but it is not for us to decide how much will be explained in the future.",
        "summary" : "From a methodological standpoint, we should inquire which entities lack agency, which systems are simple, and which interconnected phenomena defy emergence. Falsification, a fundamental scientific principle, can help approximate the current methodological frontier. However, it's essential to recognize that these concepts are partly normative and based on our current knowledge. Things denied agency today may gain agency with evolving theories and methods. Complex systems today may become simple in the future, and emergent phenomena may become predictable. These concepts serve as snapshots of our current knowledge and limitations, with the potential for evolution in the future.",
        "question" : "What should we consider about agency, complexity, and emergence, and how do they relate to the current state of knowledge?",
        "ground_truths" : "From a methodological standpoint, considering agency, complexity, and emergence is that these concepts are partly normative and based on our current knowledge. However, they serve as snapshots of our current knowledge and limitations, with the potential for evolution in the future."
    },
    {
        "contexts" : "What is clearer is that agency widely revolves around complex experimental designs that bridge different realms of science, such as neuroscience and psychology, psychology and microeconomics or behavioral science and zoology. The concept of agency links different fields of inquiry. Besides our diverse theories when it comes to agency, we only start to fully acknowledge the challenges related to its measurement as well as the opportunities to bridge different domains of sciences. From an empirical standpoint, agency has not been systematically accessed, which is why much emphasis is given to some few studies, and existing schools of thought often build on diverse premises. A good example to access agency is for instance in the combination of psychological or behavior focused experiments with neurological examinations, although even such complicated experimental settings are not free of ambiguous results. Another methodological approach to agency could be in gamification, since such approaches would be able to test behavior of players within a complex environment, including reactions and motivations. The concept of altruism does however illustrate that, for the time being, the explanation of such behaviors may be unattainable. Just like agency, altruism can be divided into evolutionary or metaphysical explanations, roughly speaking. Time will tell if these schools of thought can be bridged.",
        "summary" : "Agency involves complex experimental designs bridging different scientific fields, such as neuroscience and psychology, psychology and microeconomics, or behavioral science and zoology. It links various fields of inquiry, but its measurement and challenges are only beginning to be fully recognized. Empirically, agency has not been systematically accessed, and emphasis is placed on a few studies. Schools of thought on agency often have diverse premises. Methods for accessing agency may include combining psychological or behavioral experiments with neurological examinations or using gamification. However, explaining behaviors like altruism remains challenging and can be divided into evolutionary or metaphysical explanations.",
        "question" : "How does the concept of agency connect different scientific fields?",
        "ground_truths" : "The concept of agency connects different scientific fields by involving complex experimental designs bridging neuroscience, psychology, microeconomics, behavioral science, and zoology."
    },
    {
        "contexts" : "Complex system theory is increasingly in the focus of many fields of research, which is no surprise. The dynamics of complex systems are an Eldorado for many empirical researchers, and many disciplines are engaged in this arena. From a methodological standpoint, methods such as network analysis or structural equation models and also theoretical work such as the Ostrom framework are examples of the increasing recognition of complex systems. The Ostrom framework builds a bridge to this end, as it links a conceptual focus on resources with a methodological line of thinking to allow for a structured recognition of system dynamics. Several approaches try to implement the dynamics of complex systems within an analytical framework, yet these approaches often suffer from a gap between empirical data and sufficiently complex theory-driven models, since we only start to approach the gap between individual case studies and a broader view of empirical results. Here, the Ostrom framework is recognized as a landmark approach, although we must understand that even this widely popular framework was implemented in surprisingly few empirical studies (Partelow 2019).",
        "summary" : "Complex system theory has gained increasing focus across various research fields due to its appeal to empirical researchers. Many disciplines are involved in studying complex systems. Methodologically, network analysis, structural equation models, and theoretical frameworks like the Ostrom framework have been recognized for addressing complex systems. The Ostrom framework connects resource-focused concepts with a methodological approach for understanding system dynamics. Implementing complex system dynamics into analytical frameworks is attempted, but often faces a gap between empirical data and sufficiently complex theory-driven models. The Ostrom framework is considered a significant approach, even though its implementation in empirical studies has been limited.",
        "question" : "What are some methodological approaches for studying complex systems?",
        "ground_truths" : "Network analysis, structural equation models, and theoretical frameworks like the Ostrom framework are recognized methodological approaches for studying complex systems."
    },
    {
        "contexts" : "To this end, it is important to realize that the problems that complex systems may encompass can be often diagnosed - which is called system knowledge - however the transformative knowledge necessary to solve these problems is ultimately what is complex. For instance do many people know that we already understood that climate change is real, and what we could do against it. What we could really do to solve the deeper problems - i.e. changing people's behavior and consumption patterns - is however much more difficult to achieve, and this is what complex system thinking often about. Problem diagnosis is often simpler than creating solutions. Another challenge in complex systems are temporal dynamics. For instance, meteorological models are built on complex theories yet are also constantly refined based on a diversity of data. These models showcase that we are able to predict the weather for some days, and coarse patterns even for weeks, but there is still a clear limitation of our understanding of the weather when we go further into the future. A recent example of complex system dynamics is the Corona pandemic. While in the early stage of the pandemic our knowledge about the spread of the virus and the associated dynamics grew quickly, effective solutions are a long-term goal. There are clear relations how certain actions lead to certain developments in the pandemic, but the local and regional contexts can cascade into severe dynamics and differences. In addition, the development of potential solutions - such as a vaccine - is very complex and difficult to achieve. While thus many developments during this pandemic can be understood, it is certainly more complex to create a solution.",
        "summary" : "Complex systems often involve diagnosable problems, known as system knowledge, but the transformative knowledge required to solve these problems is complex. For instance, addressing climate change's deeper issues, such as changing behavior and consumption patterns, is challenging. Problem diagnosis is simpler than finding solutions. Temporal dynamics pose another challenge in complex systems. Meteorological models can predict weather patterns for days and weeks, but long-term predictions remain limited. The COVID-19 pandemic is a recent example of complex system dynamics. While our understanding of the virus and its spread quickly grew, effective solutions remain a long-term goal due to the complexity of local and regional contexts and the development of solutions like vaccines.",
        "question" : "How do these complex system challenges relate to issues like climate change and the COVID-19 pandemic?",
        "ground_truths" : "Addressing issues like climate change, which involve changing behavior and consumption patterns, is particularly challenging. Temporal dynamics in complex systems, such as weather prediction, remain limited in long-term forecasting. The COVID-19 pandemic is a recent example of complex system dynamics, where understanding the virus and its spread quickly grew."
    },
    {
        "contexts" : "Tracking down emergence has become a key focus in many areas of science, but organic chemistry can serve as an example of how much has been done, yet how much is still on the horizon. Many negative effects of chemicals were not anticipated, with prominent examples being FCKW, pesticides and antibiotics. The effect of different medical drugs on people is yet another example, since interactions between different medications are hardly understood at all, as the field only slowly unravels the negative side-effects of interacting medications or treatments. We are far away from understanding the impact that the concepts agency, complex systems, and emergence have on our knowledge. Yet, we need to diversify our canon of methods in order to approach these concepts from an empirical standpoint. Otherwise we will not be able to unlock new strata of knowledge. This includes the combination of different methods, the utilization of specific methods in a different contexts, as well as the development of novel methods.",
        "summary" : "Emergence is a significant focus in various scientific fields, but challenges remain, as seen in organic chemistry. Many negative effects of chemicals, such as FCKW, pesticides, and antibiotics, were not anticipated. The effects of different medical drugs and interactions between medications are still not fully understood. The impact of the concepts of agency, complex systems, and emergence on our knowledge is not fully grasped. To advance our understanding, diversification of research methods is essential, including combining different methods, adapting methods to new contexts, and developing new methods.",
        "question" : "Give me examples of emergence posed challenges in fields like organic chemistry and medicine?",
        "ground_truths" : "Challenges are evident in fields like organic chemistry and medicine, where many negative effects of chemicals and drug interactions were not anticipated. The impact of concepts like agency, complex systems, and emergence on our knowledge is not fully understood, highlighting the need for diversification of research methods."
    },
    {
        "contexts" : "What is however clear is that the three concepts - agency, complexity, and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalizable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We cannot answer these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The uncertainty principle of Heisenberg in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.",
        "summary" : "The concepts of agency, complexity, and emergence raise questions about the nature of empirical knowledge. It's unclear if anything is universally generalizable, if all valid arguments have a limited validity, or if some aspects will always elude reliable measurement. While these questions remain unanswered, it's essential to distinguish between what we know, what we might learn, and what may forever remain unknowable. The Heisenberg uncertainty principle in quantum mechanics, which deals with the limits of observation, applies to larger phenomena, like personal identity. Therefore, agency, complex systems, and emergence serve as methodological boundary objects and highlight our current limitations.",
        "question" : "How do the concepts of agency, complexity, and emergence challenge our understanding of empirical knowledge?",
        "ground_truths" : "The concepts of agency, complexity, and emergence raise questions about the nature of empirical knowledge, including its generalizability, validity over time, and limitations in measurement. These questions remain unresolved, emphasizing the need to distinguish between known, potential knowledge, and the permanently unknowable."
    },
    {
        "contexts": "If we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodological ramifications. I can interview individuals, yet this will hardly allow me to prove agency. Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy.",
        "summary": "Empirical investigation of agency often involves studying individuals or the actions of non-variable entities. This choice influences the applied research methods and has methodological implications. While interviewing individuals is possible, it may not be sufficient to prove agency. Current knowledge about agency is primarily derived from deductive experimental settings, hypothesis testing, or philosophical inquiries related to logic and metaphysics.",
        "question": "What are the challenges and methodological implications of empirically investigating agency?",
        "ground_truths": "Empirically investigating agency often involves studying individuals or the actions of non-variable entities. This choice influences the applied research methods and has methodological implications. While interviewing individuals is possible, it may not be sufficient to prove agency. Current knowledge about agency is primarily derived from deductive experimental settings, hypothesis testing, or philosophical inquiries related to logic and metaphysics."
    },
    {
        "contexts": "The investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on causality a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored Bayesian statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging.",
        "summary": "Research into complex systems has thrived in recent decades, both empirically and conceptually. Various methods have emerged or been adapted to explore these systems, leading to a deeper understanding. Complex systems often involve inductive exploration of novel data. Investigating causality in complex systems is challenging due to their scale, resulting in many purely correlative relationships. Normative questions are common in research on human systems, leading researchers to assume causality or discuss relationships as if they were causal. Non-linear relationships pose methodological challenges, as traditional statistics rely on probability and linearity. Bayesian statistics offer an alternative for inductively learning about complex systems without relying on linearity, but adoption is still emerging in disciplines that rely on traditional statistics.",
        "question": "Why does non-linear relationships in complex system pose methodolical challenges?",
        "ground_truths": "Non-linear relationships pose methodological challenges because traditional statistics rely on probability and linearity."
    },
    {
        "contexts": "The whole globe - although not a closed system  can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. Emergence is a truly cross-scalar phenomenon. Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.",
        "summary": "Viewing the whole globe as a global system, although not a closed one, has methodological significance. However, global dynamics involve diverse data, making the task of integrating this data almost impossible at present. Globalization and technology have triggered numerous emergent events, including global conflicts, climate change, increased cancer rates, and biodiversity loss. These outcomes were not planned but emerged from unpredictable combinations of human actions and data representing them. From a methodological standpoint, these events are as unpredictable as the effects of two molecules on each other and the environment. Emergence is a cross-scalar phenomenon, and many methodological approaches to address threats to human societies are correlative when they are empirical. A deep understanding of emergence and what makes phenomena emergent is still distant.",
        "question": "What are the emergent events caused by globalization and technology advancements?",
        "ground_truths": "Globalization and technology have triggered numerous emergent events, including global conflicts, climate change, increased cancer rates, and biodiversity loss."
    },
    {
        "contexts": "Based on the current methodological canon we can say: Much is not (yet?) observed, much is not known, and it will be crucial to understand what we will not be able to know, at least for the time being. These three different qualities of knowledge clearly highlight that research will have to collaborate - agency, many complex systems and several phenomena that are emergent cannot be sufficiently investigated by one discipline alone. While chemistry may find emergence in chemical reactions, and medicine may find interactions between different drugs, these results become normative as soon as they are applicable in the real world. To this claim, one could make the exception of ethics or broadly speaking philosophy here, which can meaningfully engage with all three domains of knowledge - unobserved, not known and never known - yet may not be able to contribute to all empirical problems. On the other end, everything but philosophy may only be able to go into the un-observed and unknown. We need to combine our methodological approaches to create the knowledge that is needed now. The need for collaboration is a question of responsibility, and only if all disciplines dissolve (or at least redefine their main goal to unite and not differentiate), agency may be fully explored and complex problems may be solved. Many of the solutions we have in our hands right now already would seem like wizardry to our ancestors. It is our responsibility to continue on this path, not as technocrats or positivists that have arrogant pride of their achievement, but as mere contributors to a wider debate that should ultimately encompass all society.",
        "summary": "In the current methodological landscape, much remains unobserved, unknown, and potentially unknowable, at least for the time being. This highlights the necessity for interdisciplinary research collaboration since agency, complex systems, and emergent phenomena cannot be adequately studied within the confines of a single discipline. While disciplines like chemistry may find emergence in chemical reactions and medicine may uncover interactions between drugs, these findings become normative once they are applied in the real world. Ethics and philosophy can engage with all three domains of knowledge - unobserved, not known, and never known - though they may not contribute to all empirical problems. All disciplines except philosophy may delve into the unobserved and unknown. Combining methodological approaches is essential to create necessary knowledge. Collaboration is a matter of responsibility, and for agency to be fully explored and complex problems solved, all disciplines must either dissolve or redefine their primary goal as unity rather than differentiation. The solutions at our disposal may already appear like wizardry to our ancestors, underscoring our responsibility to continue progressing without arrogance but as contributors to a broader societal debate.",
        "question": "Why is interdisciplinary research collaboration crucial in addressing the challenges posed by unobserved, unknown, and potentially unknowable aspects of agency, complex systems, and emergent phenomena? How does philosophy fit into this contexts?",
        "ground_truths": "In the current methodological landscape, much remains unobserved, unknown, and potentially unknowable, at least for the time being. This highlights the necessity for interdisciplinary research collaboration since agency, complex systems, and emergent phenomena cannot be adequately studied within the confines of a single discipline. Ethics and philosophy can engage with all three domains of knowledge - unobserved, not known, and never known - though they may not contribute to all empirical problems. All disciplines except philosophy may delve into the unobserved and unknown."
    },
    {
        "contexts": "To say it with Derek Parfit: \"Some things (...) matter, and there are better and worse way to live. After many thousands of years of responding to reasons in ways that helped them to survive and reproduce, human beings can now respond to other reasons. We are a part of a Universe that is starting to understand itself. And we can partly understand, not only what is in fact true, but also what ought to be true, and what we might be able to make true. What now matters the most is that we avoid ending human history. If there are no rational beings elsewhere, it may depend on us and our successors whether it will all be worth it, because the existence of the Universe will have been on the whole good.\"",
        "summary": "Derek Parfit emphasizes that certain things hold significance, and there are preferable and less preferable ways to live. After millennia of responding to survival and reproductive reasons, humans can now respond to other motivations. Humanity is part of a Universe that is beginning to comprehend itself, grasping not only factual truths but also ethical truths and the potential to shape reality. The paramount concern is preventing the termination of human history. If rational beings are scarce beyond Earth, the responsibility for the worthiness of the Universe's existence may rest on humanity and its descendants.",
        "question": "What is Derek Parfit's message about the significance of human existence in the universe?",
        "ground_truths": "Derek Parfit emphasizes that certain things hold significance, and there are preferable and less preferable ways to live. Humanity is starting to understand itself. And we can partly understand, not only what is in fact true, but also what ought to be true, and what we might be able to make true."
    },
    {
        "contexts": "Learning statistics takes time, as it is mostly experience that allows us to be able to approach the statistical analysis of any given dataset. '''While we cannot take off of you the burden to gather experience yourself, we developed this interactive page for you to find the best statistical method to analyze your given dataset.''' This can be a start for you to dive deeper into statistical analysis, and helps you better [[Designing studies|design studies]].<br/>\n\n<u>This page revolves around statistical analyses of data that has at least two variables.</u> If you only have one variable, e.g. height data of a dozen trees, or your ratings for five types of cake, you might be interested in simpler forms of data analysis and visualisation. Have a look at [[Descriptive statistics]] and [[Introduction to statistical figures]] to see which approaches might work for your data.\n\n<u>If you have more than one variable, you have come to the perfect place!</u> <br>\nGo through the images step by step, click on the answers that apply to your data, and let the page guide you. <br>\nIf you need help with data visualisation for any of these approaches, please refer to the entry on [[Introduction to statistical figures]].<br>If you are on mobile and/or just want a list of all entries, please refer to the [[Statistics]] overview page.",
        "summary": "Learning statistics takes time, and experience is crucial for approaching statistical analysis effectively. An interactive page has been developed to help users find the best statistical method for analyzing their datasets. This page is designed for data with at least two variables.",
        "question": "Why is experience important in statistical analysis?",
        "ground_truths": "Experience is important in statistical analysis because it enables a better understanding of how to approach different datasets."
    },
    {
        "contexts": "'''Start here with your data! This is your first question.'''\n\n<imagemap>Image:Statistics Flowchart - First Step.png|center|650px\npoly 289 5 151 140 289 270 423 137 [[An_initial_path_towards_statistical_analysis|This is where you start!]]\npoly 136 150 0 288 137 418 271 284 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|Multivariate Statistics]]\npoly 441 151 302 289 437 416 571 287 [[An_initial_path_towards_statistical_analysis#Univariate_statistics|Univariate Statistics]]\n</imagemap>\n\n'''How do I know?''' <br/>\n* What does the data show? Does the data logically suggest dependencies - a causal relation - between the variables? Have a look at the entry on [[Causality]] to learn more about causal relations and dependencies.\n\n= Univariate statistics =\n'''You are dealing with Univariate Statistics.''' Univariate statistics focuses on the analysis of one dependent variable and can contain multiple independent variables. But what kind of variables do you have?\n<imagemap>Image:Statistics Flowchart - Univariate Statistics.png|650px|center|\npoly 386 5 203 186 385 359 563 182 [[Data formats]]\npoly 180 200 3 380 181 555 359 378 [[An_initial_path_towards_statistical_analysis#At_least_one_categorical_variable|At least one categorical variable]]\npoly 584 202 407 378 584 556 762 379 [[An_initial_path_towards_statistical_analysis#Only_continuous_variables|Only continuous variables]]\n</imagemap>\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric (including continuous) variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'', and ''character'' data is ''categorical''.",
        "summary": "This section guides users to start analyzing their data and determine the appropriate statistical method. It focuses on Univariate Statistics and helps users identify the types of variables they are dealing with, whether categorical or continuous.",
        "question": "What is the purpose of the 'Start here with your data!' section?",
        "ground_truths": "The 'Start here with your data!' section serves as a starting point for users to begin their data analysis journey."
    },
    {
        "contexts": "== At least one categorical variable ==\n'''Your dataset does not only contain continuous data.''' Does it only consist of categorical data, or of categorical and continuous data?\n<imagemap>Image:Statistics Flowchart - Data Formats.png|650px|center|\npoly 288 2 151 139 289 271 424 138 [[Data formats]]\npoly 137 148 0 285 138 417 273 284  [[An_initial_path_towards_statistical_analysis#Only_categorical_data:_Chi_Square_Test|Only categorical data: Chi Square Test]]\npoly 436 151 299 288 437 420 572 287 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data|Categorical and continuous data]]\n</imagemap>\n'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'', and ''character'' data is categorical.",
        "summary": "This section addresses datasets that contain both categorical and continuous data. It prompts users to determine the composition of their dataset, whether it consists of only categorical data or a combination of categorical and continuous data.",
        "question": "What is the focus of the 'At least one categorical variable' section?",
        "ground_truths": "The 'At least one categorical variable' section focuses on helping users identify the composition of their dataset, specifically whether it contains only categorical data or a mix of categorical and continuous data."
    },
    {
        "contexts": "=== Only categorical data: Chi Square Test ===\n'''You should do a Chi Square Test'''.<br/>\nA Chi Square test can be used to test if one variable influenced the other one, or if they occur independently from each other. The key R command here is: <code>chisq.test()</code>. Check the entry on [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi Square Tests]] to learn more.",
        "summary": "For datasets consisting of only categorical data, users are recommended to perform a Chi Square Test. This test assesses whether there is a significant relationship between two categorical variables in the dataset.",
        "question": "What is the recommended course of action for datasets with only categorical data?",
        "ground_truths": "For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset."
    },
    {
        "contexts": "'''How do I know?'''\n* A 'factor level' is a category in a categorical variable. For example, when your variable is 'car brands', and you have 'AUDI' and 'TESLA', you have two unique factor levels. \n* Investigate your data using 'levels(categoricaldata)' and count the number of levels it returns. How many different categories does your categorical variable have? If your data is not in the 'factor' format, you can either convert it or use 'unique(yourCategoricalData)' to get a similar result.",
        "summary": "To determine the number of factor levels in a categorical variable, you can use the 'levels(categoricaldata)' or 'unique(yourCategoricalData)' functions. This helps identify the unique categories in the variable.",
        "question": "How can you find the number of factor levels in a categorical variable?",
        "ground_truths": "You can find the number of factor levels in a categorical variable by using 'levels(categoricaldata)' or 'unique(yourCategoricalData)'."
    },
    {
        "contexts": "==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()</code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n'''Depending on the variances of your variables, the type of t-test differs.'''",
        "summary": "For datasets with one or two factor levels, performing a t-test is recommended. This test allows for the comparison of datasets, and there are two types: one-sample t-test and two-sample t-test, depending on the nature of the comparison and variances of the variables.",
        "question": "What is the recommended statistical test for datasets with one or two factor levels?",
        "ground_truths": "For datasets with one or two factor levels, a t-test is recommended. There are two types: the one-sample t-test, which compares a dataset to a specified value, and the two-sample t-test, which compares two different datasets or samples to determine if their means differ significantly."
    },
    {
        "contexts": "'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()</code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()</code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE</code>.",
        "summary": "To determine whether the variances of two datasets are equal, you can use an f-Test, with the key R command being <code>var.test()</code>. If the result of the test is insignificant (p-value > 0.05), you can assume equal variances and proceed with a Student's t-test, which can be performed by setting <code>var.equal = TRUE</code> in the <code>t.test()</code> function.",
        "question": "Why the equality of variances between two datasets is important for t-tests?",
        "ground_truths": "The equality of variances between two datasets is important for t-tests because if the variances are equal, you can perform a Student's t-test; otherwise, you may need to use a Welch t-test to account for unequal variances."
    },
    {
        "contexts": "==== More than two factor levels: ANOVA ====\n'''Your categorical variable has more than two factor levels: you are heading towards an ANOVA.'''<br/>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.",
        "summary": "For datasets with categorical variables having more than two factor levels, an ANOVA (Analysis of Variance) is recommended. This statistical test is chosen based on the presence of multiple factor levels in the categorical variable and the assumption of normal data distribution.",
        "question": "What is the suggested approach for datasets with categorical variables having more than two factor levels?",
        "ground_truths": "For datasets with categorical variables containing more than two factor levels, it is recommended to perform an ANOVA (Analysis of Variance). It is important that when using ANOVA, the datasets is assumed to be normally distributed."
    },
    {
        "contexts": "'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\nIf your answer is NO, you should stick to the ANOVA - more specifically, to the kind of ANOVA that you saw above (based on regression analysis, or based on a generalised linear model). An ANOVA compares the means of more than two groups by extending the restriction of the t-test. An ANOVA is typically visualised using [[Introduction_to_statistical_figures#Boxplot|Boxplots]].</br> The key R command is <code>aov()</code>. Check the entry on the [[ANOVA]] to learn more.",
        "summary": "To determine whether your dataset includes continuous and categorical independent variables, you can use the <code>str</code> or <code>summary</code> functions in R. If the dataset contains continuous variables, it is recommended to stick to ANOVA (Analysis of Variance) methods, specifically those based on regression analysis or generalised linear models.",
        "question": "What is the suggested statistical approach in case that my data contains both continuous and categorical independent variables?",
        "ground_truths": "If your dataset contains both continuous and categorical independent variables, it is recommended to use ANOVA methods, particularly those based on regression analysis or generalised linear models."
    },
    {
        "contexts": "'''How do I know?'''\n* Consider the data from a theoretical perspective. Is there a clear direction of the dependency? Does one variable cause the other? Check out the entry on [[Causality]].",
        "summary": "To determine if there are clear dependencies between variables, you can consider the data from a theoretical perspective and assess whether one variable causes the other. This approach helps identify causal relationships between variables.",
        "question": "How can you determine the presence of clear dependencies between variables?",
        "ground_truths": "You can determine the presence of clear dependencies between variables by considering the data from a theoretical perspective and assessing whether one variable causes the other."
    },
    {
        "contexts": "=== No dependencies: Correlations ===\n'''If there are no dependencies between your variables, you should do a Correlation.'''<br/>\nA correlation test inspects if two variables are related to each other. The direction of the connection (if or which variable influences another) is not set. Correlations are typically visualised using [[Introduction_to_statistical_figures#Scatter_Plot|Scatter Plots]] or [[Introduction_to_statistical_figures#Line_chart|Line Charts]]. Key R commands are <code>plot()</code> to visualise your data, and <code>cor.test()</code> to check for correlations. Check the entry on [[Correlations]] to learn more.\n'''The type of correlation that you need to do depends on your data distribution.'''",
        "summary": "When there are no dependencies between variables, conducting a correlation analysis is recommended. Correlation tests assess the relationship between two variables, and the type of correlation depends on the data distribution. Visualizations such as Scatter Plots and Line Charts can be used, along with R commands like <code>plot()</code> and <code>cor.test()</code>.",
        "question": "What is the appropriate action when no dependencies are present between variables?",
        "ground_truths": "When no dependencies exist between variables, it is advisable to perform a correlation analysis such as scatter plots and line charts."
    },
    {
        "contexts": "'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.",
        "summary": "To determine the appropriate distribution for your dependent variable, consider the data type and what it measures. For example, Binomial distribution is suitable for yes/no questions, while Poisson distribution is appropriate for count data. You can also refer to the entry on non-normal distributions for guidance.",
        "question": "What are some examples using of suitable distributions based on data types?",
        "ground_truths": "Some examples are: a Binomial distribution is suitable for yes/no questions, while a Poisson distribution is appropriate for count data."
    },
    {
        "contexts": "==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to 'rule out', the answer to this question is 'yes'.",
        "summary": "When you reach a Generalised Linear Model (GLM), you are working with a versatile family of models that extend ordinary linear regressions. GLMs are used to model relationships between variables, and the key R command is <code>glm()</code>. The choice between Mixed Effect Models and GLMs based on regressions depends on the presence of random variables and the desire to account for uncontrolled effects.",
        "question": "What is a Generalised Linear Model (GLM)?",
        "ground_truths": "A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables."
    },
    {
        "contexts": "'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.",
        "summary": "To determine which type of multivariate analysis to conduct, you can consider whether you want to arrange data along gradients, group data points by similarity, or analyze network connections.",
        "question": "How can you decide which type of multivariate analysis to perform?",
        "ground_truths": "You can decide which type of multivariate analysis to perform by considering whether you want to arrange data along gradients, group data points by similarity (clustering), or analyze network connections."
    },
    {
        "contexts": "== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.",
        "summary": "When conducting an ordination, data is arranged alongside underlying gradients in variables to determine which variables strongly define data points. The choice of distance metric (Euclidean for abundance data, Jaccard for continuous data) depends on the data type.",
        "question": "What is involved in performing an ordination, and how does the choice of distance metric relate to the type of data?",
        "ground_truths": "Performing an ordination entails arranging data alongside gradients in variables to identify variables that strongly define data points. The choice of distance metric depends on the type of data, with Euclidean distances used for abundance data and Jaccard distances for continuous data."
    },
    {
        "contexts": "== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).",
        "summary": "Cluster Analysis involves grouping data points based on similarity to create a tree structure. It can be supervised (Classification) or unsupervised (Clustering), depending on the presence of prior knowledge.",
        "question": "What is Cluster Analysis?",
        "ground_truths": "Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering)."
    },
    {
        "contexts": "'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.",
        "summary": "Network Analysis involves arranging data in a network structure to analyze connections and distances between data points. It is a complex field with various approaches.",
        "question": "What is the purpose of Network Analysis?",
        "ground_truths": "Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure."
    },
    {
        "contexts": "= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) '''First of all, is the statistics the right approach to begin with?''' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem. \n\n2) '''What are the efforts you need to put into the initial data gathering?''' If you decided that statistics would be valuable to be applied, the question then would be, how? To rephrase this statement: How exactly? Your sampling with all its constructs, sample sizes and replicates decides about the fate of everything you going to do later. A flawed dataset or a small or biased sample will lead to failure, or even worse, wrong results. Play it safe in the beginning, do not try to overplay your hand. Slowly edge your way into the application of statistics, and always reflect with others about your sampling strategy. \n\n3) '''The analysis then demands hand-on skills, as implementing tests within a software is something that you learn best through repetition and practice.''' I suggest you to team up with other peers who decide to go deeper into statistical analysis. If you however decide against that, try to find geeks that may help you with your data analysis. Modern research works in teams of complementary people, thus start to think in these dimensions. If you chip in the topical expertise of the effort to do the sampling, other people may be glad about the chance to analyse the data.\n\n4) '''This is also true for the interpretation, which most of all builds on experience.''' This is the point were a supervisor or a PhD student may be able to glance at a result and tell you which points are relevant, and which are negotiable. Empirical research typically produces results where in my experience about 80 % are next to negliable. It takes time to learn the difference between a trivial and an innovative result. Building on knowledge of the literature helps again to this end, but be patient as the interpretation of statistics is a skill that needs to ripen, since contexts matters. It is not so much about the result itself, but more about the whole contexts it is embedded in.\n\n5) The last and most important point explores this thought further. '''What are the limitations of your results?''' Where can you see flaws, and how does the multiverse of biases influence your results and interpretation? What are steps to be taken in future research? And what would we change if we could start over and do the whole thing again? All these questions are like ghosts that repeatedly come to haunt a researcher, which is why we need to remember we look at pieces of the puzzle. Acknowledging this is I think very important, as much of the negative connotation statistics often attracts is rooted in a lack of understanding. If people would have the privilege to learn about statistics, they could learn about the power of statistics, as wells its limitations. \n\n'''Never before did more people in the world have the chance to study statistics.''' While of course statistics can only offer a part of the puzzle, I would still dare to say that this is reason for hope. If more people can learn to unlock this knowledge, we might be able to move out of ignorance and more towards knowledge. I think it would be very helpful if in a controversial debate everybody could dig deep into the available information, and make up their own mind, without other people telling them what to believe. Learning about statistics is like learning about anything else, it is lifelong learning. I believe that true masters never achieve mastership, instead they never stop to thrive for it.",
        "summary": "The provided guidance offers a checklist for applying or reading statistics, covering aspects such as the suitability of statistics, data gathering, analysis, interpretation, and considering limitations. It emphasizes the importance of learning and understanding statistics.",
        "question": "What are some key considerations and steps to keep in mind when applying or reading statistics?",
        "ground_truths": "1) Question yourself, is statistics the right approach? \n2) Question the effort to collect the data. \n3) Do you have the skills to analyse the data? \n4) Intepretation of the data, is the data trivial or innovative? \n5) What are the limitations of your results?"
    },
    {
        "contexts": "== 1. What is Anaconda? ==\nAnaconda is an open-source Python and R distribution that includes access to different IDEs, such as Spyder and pyCharm, as well as Jupyter Notebook among others. It is aiming at simplifying package management and claims to be the world's most popular data science plattform (1).",
        "summary": "Anaconda is an open-source distribution for Python and R, featuring various IDEs and tools for data science.",
        "question": "What is Anaconda, and what does it offer in terms of Python and R distributions and tools?",
        "ground_truths": "Anaconda is an open-source distribution that includes Python and R, along with various IDEs and tools for data science."
    },
    {
        "contexts": "== 2. What's included in Anaconda Distribution? ==\nIncluded in the basic Anaconda distribution is the Conda package and environment management system as well as the applications Anaconda Navigator, a Powershell terminal, JupyterLab and Jupyter Notebook, as well as the Spyder IDE. These modules are briefly introduced in the following.\nConda\n  Conda is an open-source package and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies. It also easily creates, saves, loads, and switches between environments (2) on a local computer. \nAnaconda Navigator\n  Desktop application to easily manage integrated applications, packages, and environments (2) with a graphical user interface instead of using the command line.\nJupyter Notebook\n  Web-based, interactive computing notebook environment to edit and run human-readable documents while describing data analysis. (3)\nJupyterLab\n  Extensible Environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture. (3)\nSpyder IDE\n  A scientific Integrated Python Development Environment with advanced editing, interactive testing, debugging and runtime introspection features. (3)",
        "summary": "The basic Anaconda distribution includes Conda for package and environment management, Anaconda Navigator for GUI-based management, Jupyter Notebook for interactive computing, JupyterLab for extensible computing, and Spyder IDE for scientific Python development.",
        "question": "What components are included in the basic Anaconda distribution, and what are their respective functionalities?",
        "ground_truths": "The basic Anaconda distribution includes Conda, Anaconda Navigator, Jupyter Notebook, JupyterLab, and Spyder IDE."
    },
    {
        "contexts": "== 3. Basic features/properties of Anaconda ==\nThe three main features Anaconda claims is that the distribution is \n\n1. \"open-source\", <br>\n2. \"user-friendly\" and <br>\n3. \"trusted\". \n\nThis means that the code is freely accessible and assessable for projects in any field. Moreover, the intuitive platform allows for easy package and environment handling, all while the securely hosted packages and artifacts are methodically tested and regularly updated (2) to avoid conflicts, bugs or scams.",
        "summary": "Anaconda is characterized by being open-source, user-friendly, and trusted. It offers freely accessible code, intuitive package and environment management, and regularly updated packages to prevent conflicts and issues.",
        "question": "What are the main features of Anaconda, and what do they entail?",
        "ground_truths": "Anaconda is open-source, user-friendly, and trusted. It provides freely accessible code, easy package and environment management, and regularly updated packages."
    },
    {
        "contexts": "== 4. How to install Anaconda on my machine ==\nAnaconda Distribution can be installed free of charge for private use for Windows, macOS and Linux. The newest version currently features Python 3.9.\n\nIts installation process is straight forward: Download the installation file, choose a path and follow the steps. No account is needed. As a student you can choose to sign up for Anaconda Nucleus for Community membership, unlimited environment backups (and) exclusive data science content (4) for free.\n\nClick [https://www.anaconda.com/products/distribution#Downloads here] to directly download the installer for your operating system. For Windows and MacOS, graphical installers are provided. For Linux, there are only installation scripts that have to be run from the terminal.\n\nFor an in-depth description and links to download Anaconda for older operating systems, you can make use of their extensive [https://docs.anaconda.com/anaconda/install/ documentation].\n\nIf a more minimalistic approach is preferred, Miniconda can be installed. This version just includes conda, Python and necessary dependencies:\n[https://docs.conda.io/en/latest/miniconda.html Install Miniconda]    \n\nIn case of issues during the installation process, please refer to [https://docs.anaconda.com/anaconda/user-guide/troubleshooting/ the help page].",
        "summary": "Anaconda Distribution can be installed for free on Windows, macOS, and Linux. The installation process involves downloading the installer, choosing a path, and following the steps. Students can sign up for Anaconda Nucleus for additional benefits. Miniconda is an alternative for a more minimalistic installation. Troubleshooting help is available.",
        "question": "What are the steps to install Anaconda Distribution? ",
        "ground_truths": "Anaconda Distribution can be installed for free on various operating systems. The installation involves downloading the installer, choosing a path, and following the steps."
    },
    {
        "contexts": "== 5. Using Conda ==\nConda is a command line tool used in a terminal to interact with Anaconda. It is a package and environment management software. It can be used to install or update packages, create, save and load environments. To start using conda, open a terminal, type ''conda'' and press enter.\n\nTo open a terminal on windows, press CTRL + R, type ''cmd. exe'' (write this without a space, we're sorry, this is due to Wiki formatting) and press enter. On macOS, open launcher and type ''terminal'' into the search box, clicking the icon when it appears. On Linux, the shortcut Super + T should do the job, otherwise it can be found in the applications menu.",
        "summary": "Conda is a command line tool used in a terminal to manage packages and environments in Anaconda. It can be used for package installation, updates, environment creation, saving, and loading.",
        "question": "What is Conda, and how can it be used to manage packages and environments in Anaconda?",
        "ground_truths": "Conda is a command line tool for managing packages and environments in Anaconda. It can be used for package management, environment creation, and more."
    },
    {
        "contexts": "== 6. Using Anaconda Navigator ==\nAnaconda Navigator is a desktop application to manage environments and packages via a graphical user interface rather than the conda command line interface. It is automatically installed with Anaconda Distribution version 4.0.0 or higher. In the left column of the Navigator you can switch between the program's four main components. On the '''Learning page''' and '''Community page''' you can access documentation and training related to the Navigator and find out about support forums and social networking activities in the field. The following section describe in more depth the '''Home page''' and the '''Environments page''' that allow you to manage applications and environments.\n\n=== Home page ===\n[[File:Anaconda Homepage.png|thumb|left|Source: https://docs.anaconda.com/navigator/overview/]]\n\nOn the Home page, you can launch and install applications in the active environment by clicking the respective button on one of the application tiles. Additionally, you can use the gear icon in the top-right corner of the tiles to update, remove, or install a specific version of an application. Note that these actions will only apply to the currently-active environment, which you can select with the '''Applications on''' drop-down located on top of the tiles menu.\nTwo important built-in applications are Spyder for writing and executing code and Jupyter Notebook for generating notbooke files. The Anaconda documentation includes a [https://docs.anaconda.com/navigator/overview/#home-page comprehensive list] of all applications that are included by default and instructions on [https://docs.anaconda.com/navigator/tutorials/nav-app/#building-anaconda-navigator-applications how to build your own Navigator applications].\n\n=== Environments page ===\n\n[[File:Anaconda Navigation.png|thumb|right|Source: https://docs.anaconda.com/navigator/overview/]]\n\nThe Environments page, like Conda, allows you to create and activate environments as well as install, update, and remove packages.\n\n==== Create an environment ====\n\nClick on the '''Create''' button at the bottom of the Environments list. A dialog box appears where you can name your environment and select the versions of Python and/or R you want to work with. You can also choose to import an environment from your local drive or Anaconda Nucleus ('''Import''' button) or clone an existing environment ('''Clone''' button). \n\n==== Activate the environment ====\n\nIn the environments list in the left column, click on the arrow button next to the name to activate the environment you want to use. \n\n==== Installing, updating and removing packages ====\n\nBy default, the right table displays all packages that are installed in the active environment. You can use the filter on top of the list to view not installed, updatable, selecte, or all packages instead. Note that you can use the '''Channels''' button to adjust the locations where Navigator searches for packages. See [https://docs.anaconda.com/navigator/tutorials/manage-channels/ Managing channels] for further information on channels. To install a new package, click the '''Apply''' button for an uninstalled package. To update a package, remove a package or install a different package version, click on the checkbox infront of the package name, select the respective action, and click '''Apply'''.  \n\nIf you run into problems or want to explore the functions of Anaconda Navigator further, you can access the [https://docs.anaconda.com/navigator/ documentation] via the '''Help''' button in the top-left corner of the Navigator.",
        "summary": "Anaconda Navigator is a desktop application for managing environments and packages with a graphical user interface. It includes a Home page for launching and installing applications, and an Environments page for managing environments and packages. Conda and Anaconda Navigator offer similar functionality.",
        "question": "What is Anaconda Navigator?",
        "ground_truths": "Anaconda Navigator is a desktop application with a GUI for managing environments and packages."
    },
    {
        "contexts": "Analysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.",
        "summary": "ANCOVA is a statistical test that compares means of multiple groups while controlling for the influence of a covariate.",
        "question": "What is the purpose of ANCOVA in statistical analysis?",
        "ground_truths": "ANCOVA is used to compare group means while controlling for the effect of a covariate."
    },
    {
        "contexts": "Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.",
        "summary": "ANCOVA compares group means while controlling for covariate influence. It involves hypothesis testing with null and alternative hypotheses and considers Sum of Squares partitioning. Meeting assumptions from linear regression and ANOVA is important before conducting ANCOVA.",
        "question": "What are the key principles and assumptions of ANCOVA?",
        "ground_truths": "ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset."
    },
    {
        "contexts": "Regression assumptions\n# The relationship between dependent and independent variables must be linear for each treatment group.\nANOVA assumptions\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\nSpecific ANCOVA assumptions\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.",
        "summary": "ANCOVA has various assumptions, including linear relationships between variables, homogeneity of variances between groups, normal distribution of residuals, and optionally, homogeneity of slopes.",
        "question": "What are the assumptions associated with ANCOVA?",
        "ground_truths": "ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes."
    },
    {
        "contexts": "One-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the covariate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well-developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.",
        "summary": "One-way ANCOVA compares group variances with one independent variable having three or more categorical groups while considering a covariate. It requires a well-developed research question.",
        "question": "What is One-way ANCOVA?",
        "ground_truths": "One-way ANCOVA compares group variances with one independent variable and a covariate."
    },
    {
        "contexts": "Based on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.",
        "summary": "The boxplot suggests that anxiety mean scores vary among exercise groups, with the highest in the basal group (grp1) and the lowest in the high group (grp3). However, further statistical analysis using ANCOVA is needed to confirm these observations.",
        "question": "What does the boxplot of anxiety scores among different exercise groups suggest?",
        "ground_truths": "Based on the boxplot of anxiety scores among different exercise groups, we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises."
    },
    {
        "contexts": "1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.",
        "summary": "The linearity assumption in ANCOVA can be assessed using regression fitted lines plots for each treatment group to check if the relationship between variables is linear.",
        "question": "How is the linearity assumption assessed in ANCOVA?",
        "ground_truths": "The linearity assumption is assessed using regression fitted lines plots for each treatment group to check for linearity."
    },
    {
        "contexts": "2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].",
        "summary": "The assessment of residuals in ANCOVA involves checking if residuals are unbiased and if the variances between groups are homogeneous. A plot of residuals vs. the dependent variable helps determine homoscedasticity.",
        "question": "Why should we check residuals in ANCOVA?",
        "ground_truths": "Residuals in ANCOVA are assessed to check for unbiasedness and homogeneity of variances between groups, which is important for valid results."
    },
    {
        "contexts": "3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.",
        "summary": "Homogeneity of residuals in ANCOVA can be examined using a residual histogram and the Shapiro-Wilk test for normality.",
        "question": "How is homogeneity of residuals assessed in ANCOVA?",
        "ground_truths": "Homogeneity of residuals is assessed using a residual histogram and the Shapiro-Wilk test for normality."
    },
    {
        "contexts": "4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:",
        "summary": "The assumption of homogeneity of regression slopes in ANCOVA checks for the absence of a significant interaction between the covariate and the grouping variable. It is assessed using a specific test.",
        "question": "What is the assumption of homogeneity of regression slopes in ANCOVA?",
        "ground_truths": "The assumption of homogeneity of regression slopes checks for the absence of a significant interaction between the covariate and the grouping variable, and it is assessed using a specific test."
    },
    {
        "contexts": "When running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.",
        "summary": "When conducting ANCOVA in R, attention should be given to the order of variables, with the goal of removing the covariate's effect. ANCOVA involves regression, identifying residuals, and performing ANOVA on residuals.",
        "question": "What are the general steps involved in conducting ANCOVA in R?",
        "ground_truths": "ANCOVA in R involves regression, identifying residuals, and performing ANOVA on residuals to remove the covariate's effect."
    },
    {
        "contexts": "<syntaxhighlight lang=\"R\" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = \"II\")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = \"II\")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of before treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the \"noise\" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.",
        "summary": "The ANOVA tests without and with covariate adjustment are compared. The adjustment decreases the Sum of Squares of Errors, indicating the removal of covariate noise.",
        "question": "How does covariate adjustment affect the Sum of Squares of Errors in ANOVA tests?",
        "ground_truths": "Covariate adjustment in ANOVA reduces the Sum of Squares of Errors, indicating the removal of covariate noise and allowing evaluation of treatment effects."
    },
    {
        "contexts": "So let us recall our main question \"What treatment type has the most effect on anxiety level?\"\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will help us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.",
        "summary": "The main research question is about the treatment type's effect on anxiety level. ANOVA shows a statistically significant difference among groups, but post-hoc tests are needed to identify which specific group comparisons are significant.",
        "question": "What is the main research question?",
        "ground_truths": "The main research question is about the treatment type's effect on anxiety level."
    },
    {
        "contexts": "<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group')\n</syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.",
        "summary": "A Tukey Post-Hoc test is performed to identify significant differences among groups after ANOVA. It reveals that treatment type 1 has the most effect on anxiety level.",
        "question": "What does the Tukey Post-Hoc test reveal about the treatment types' effects on anxiety level?",
        "ground_truths": "The Tukey Post-Hoc test identifies significant group differences and reveals that treatment type 1 has the most effect on anxiety level."
    },
    {
        "contexts": "A two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable  and also examines whether the two factors affect each other to influence the continuous variable.",
        "summary": "A two-way ANCOVA examines the effects of two factors on a dependent variable and assesses if these factors interact to influence the variable.",
        "question": "What is a two-way ANCOVA?",
        "ground_truths": "A two-way ANCOVA examines the effects of two factors on a dependent variable and assesses if these factors interact."
    },
    {
        "contexts": "Anki is a flashcard app for your computer or smartphone. When you want to learn facts or simple concepts in an effective manner, Anki might be your tool to go.",
        "summary": "Anki is a flashcard app for effective learning of facts and concepts on computers and smartphones.",
        "question": "What is Anki?",
        "ground_truths": "Anki is a flashcard app used for effective learning of facts and concepts."
    },
    {
        "contexts": "Goals:\n\n- Learn effectively with Flashcards\n- Incorporate learning into your daily life\n- Use spaced repetition with ease (spaced repetition is just a fancy way of saying that the app will estimate when the optimal point in time for you to repeat a flashcard would be)",
        "summary": "Anki's goals include effective learning with flashcards, integrating learning into daily life, and facilitating spaced repetition for optimal learning.",
        "question": "What are the goals of Anki?",
        "ground_truths": "Anki's goals include effective learning with flashcards, integrating learning into daily life, and facilitating spaced repetition."
    },
    {
        "contexts": "To get started with Anki, you can download the app, create an AnkiWeb account (optional but allows backup and synchronization), and start creating your decks and flashcards. You can add various types of content, including pictures, audio, videos, and formulas (via LateX).",
        "summary": "To begin using Anki, download the app, optionally create an AnkiWeb account for backup and synchronization, and start creating decks and flashcards with various content types.",
        "question": "What are the steps to get started with Anki?",
        "ground_truths": "To begin using Anki, download the app, optionally create an AnkiWeb account, and start creating decks and flashcards with various content types."
    },
    {
        "contexts": "Tips & Tricks:\n\n- Make it a habit to Anki your notes after the lecture\n- Establish a habit to use Anki regularly\n- Share decks with your fellow students",
        "summary": "Anki tips include making it a habit to review notes after lectures, using Anki regularly as a habit, and sharing decks with fellow students.",
        "question": "What are some tips and tricks for using Anki effectively?",
        "ground_truths": "Anki tips include reviewing notes after lectures, establishing a regular usage habit, and sharing decks with others."
    },
    {
        "contexts": "Links & Further reading:\n\n- Anki Website\n- Official Anki Manual\n- Download Anki\n- Blog Tutorial\n- Derek Banas - How I Learn Everything (Full Anki Tutorial)",
        "summary": "Links and further reading resources related to Anki, including the official website, manual, download, blog tutorial, and a video tutorial by Derek Banas.",
        "question": "What are some useful links and resources related to Anki for further reading and learning?",
        "ground_truths": "Useful links and resources related to Anki include the official website, manual, download, blog tutorial, and a video tutorial by Derek Banas."
    },
    {
        "contexts": "The functions of the 'apply()'-family are useful tools to go through slices of your data and repetitively perform operations on them. In more sophisticated terms, these functions offer a concise and convenient way of implementing the SPLIT-APPLY-COMBINE-strategy of data analysis, i.e. splitting up some data into smaller pieces, applying a function to each piece and combining the results.",
        "summary": "The 'apply()' family of functions in R allows for efficient data analysis by splitting data into smaller pieces, applying functions, and combining results.",
        "question": "What is the purpose of the 'apply()' family of functions in R?",
        "ground_truths": "The 'apply()' family of functions in R facilitates data analysis by splitting data, applying functions, and combining results."
    },
    {
        "contexts": "One main advantage of the 'apply()'-functions is that they let you iterate over your data without having to use regular loops. While loops in R are not necessarily much slower than the apply()-collection, their syntax is more complicated and redundant. The various apply()-functions offer a way of accessing specific elements of data in a convenient and simplified way. ",
        "summary": "The 'apply()' functions in R offer advantages over regular loops, including simpler syntax and the ability to iterate over data efficiently.",
        "question": "What advantages do the 'apply()' functions in R offer over regular loops?",
        "ground_truths": "The 'apply()' functions in R offer advantages over regular loops due to their simpler syntax and efficient data iteration."
    },
    {
        "contexts": "The 'apply()' function takes three arguments: 'apply(X, MARGIN, FUN, ...)' where X is the input matrix or array, MARGIN specifies whether to operate on columns, rows, or cells, and FUN is the applied function.",
        "summary": "The 'apply()' function requires three arguments: 'apply(X, MARGIN, FUN, ...)' where X is the input matrix or array, MARGIN determines the operation on columns, rows, or cells, and FUN is the function applied.",
        "question": "What are the three main arguments of the 'apply()' function?",
        "ground_truths": "The 'apply()' function has three main arguments: X (input matrix or array), MARGIN (specifies operation on columns, rows, or cells), and FUN (applied function)."
    },
    {
        "contexts": "lapply() is quite similar in its use to apply(), but whereas the latter iterates over elements of a matrix, lapply() iterates over elements of a list. The returned object is also always a list.",
        "summary": "lapply() is similar to apply() but operates on elements of a list, returning a list as the output object.",
        "question": "How does lapply() differ from apply() in terms of input data and output object?",
        "ground_truths": "lapply() operates on elements of a list and returns a list as the output object, distinguishing it from apply()."
    },
    {
        "contexts": "Back of the envelope calculations are rough estimates that are made on the small piece of paper, hence the name. They are extremely helpful to get a quick estimate about the basic numbers for a given formula of principle, thus enabling us to get a quick calculation that allows us either to check for the plausibility of an assumption, or to derive a simple explanation for a complex issue. Back of the envelope statistics can be for instance helpful when you want to get a rough estimate about an idea that can be expressed in numbers. Prominent examples include the dominant character coding of the World Wide Web, as well as the development of the laser. Back of the envelope calculations are fantastic within sustainability science, because they can help us illustrate complex issues in a simple form, and they can serve as a guideline for quick planning.",
        "summary": "Back of the envelope calculations offer quick, rough estimates for complex issues. They can validate assumptions, provide guidelines for planning, and are useful in fields like sustainability science and technology.",
        "question": "What are some fields where back of the envelope calculations are particularly useful?",
        "ground_truths": "Back of the envelope calculations are fantastic within sustainability science, as well as in technology such as the development of the laser and the dominant character coding of the World Wide Web."
    },
    {
        "contexts": "We often hear numbers about probability and chances. While many of these are arbitrary, such as the chances of winning the Lottery, there are others probabilities that matter in our day-to-day life. For instance, the chances of catching a COVID-19 infection outside is 19 times lower than inside. One common misconception is that the chances are 0 - they are not, they are just lower, but considerably so!",
        "summary": "Probability affects daily life, like the 19 times lower chance of catching COVID-19 outside versus inside. Misconceptions often include assuming some risks are zero, when they are merely lower.",
        "question": "How does the risk of catching COVID-19 vary between being inside and outside?",
        "ground_truths": "The chances of catching a COVID-19 infection outside is 19 times lower than inside."
    },
    {
        "contexts": "Another prominent example of back of the envelope calculations is knowledge about predictions. Unfortunately, many predictions that we make based on rather small samples can be wrong. There are prominent examples of recent elections where much was at stake, and it was ambiguous what would come out in the end.",
        "summary": "Back of the envelope calculations can also apply to predictions, but small sample sizes often result in inaccurate outcomes, as seen in recent elections.",
        "question": "Why can predictions made using back of the envelope calculations be wrong?",
        "ground_truths": "Many predictions that we make based on rather small samples can be wrong."
    },
    {
        "contexts": "While there are advanced statistical tests to compare groups and calculate differences between individual groups, comparing mean values between groups is the most relevant calculation. Variance and mean values are not the same. We should be aware of this, and ask how things vary between individuals, and how much a mean value can really say about groups of these individuals.",
        "summary": "Comparing mean values between groups is crucial, but variance and mean are different. Understanding these can give more accurate insights into individual and group behaviors.",
        "question": "Why should one be cautious when interpreting mean values in group comparisons?",
        "ground_truths": "Variance and mean values are not the same. We should be aware of how things vary between individuals, and how much a mean value can really say about groups of these individuals."
    },
    {
        "contexts": "The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes. His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace. Bayes' name only became associated with the method in the 1900s.",
        "summary": "Bayesian methods were originally formulated by Thomas Bayes and later published by Richard Price in 1763. It didn't receive much attention until it was independently expanded upon by Pierre Simon Laplace. The method was only named after Bayes in the 1900s.",
        "question": "Who rediscovered and expanded upon Bayesian methods?",
        "ground_truths": "Pierre Simon Laplace rediscovered and expanded upon Bayesian methods."
    },
    {
        "contexts": "The family of methods based on the concept of Bayesian analysis has risen the last 50 years alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc.",
        "summary": "Over the last 50 years, Bayesian methods have gained prominence due to increased computing power and widespread computer availability. These methods are now used in various scientific fields like image processing and spam filtration.",
        "question": "What contributed to the rise of Bayesian methods in the last 50 years?",
        "ground_truths": "The increasing computing power and the availability of computers to more people contributed to the rise of Bayesian methods in the last 50 years."
    },
    {
        "contexts": "Bayesian analysis relies on using probability figures as an expression of our beliefs about events. Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is 'prior', and the probability figure assigned to the prior is called 'prior probability'. Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample.",
        "summary": "In Bayesian analysis, probability figures express beliefs about events, including our ignorance. These probabilities are initially subjective and depend on current knowledge. The initial belief and its probability are known as 'prior' and 'prior probability' respectively.",
        "question": "What is the initial belief in Bayesian analysis known as?",
        "ground_truths": "The initial belief in Bayesian analysis is known as 'prior'."
    },
    {
        "contexts": "To make things a bit easier to understand in this line of thinking, one way of making this formulation a bit easier to understand is by replacing the As and Bs that we have been using until now with hypothesis (H)/belief and data (D) respectively. Posterior Probability is the probability that we are interested in. We want to establish if our belief is consistent given the data that we have been able to acquire/observe. Prior Probability is the probability of the belief before we see any data.",
        "summary": "In Bayesian thinking, terms like 'As and Bs' can be replaced with 'hypothesis' and 'data'. Posterior Probability assesses the consistency of a belief with observed data, while Prior Probability is the initial belief before data observation.",
        "question": "What does Posterior Probability aim to assess?",
        "ground_truths": "Posterior Probability aims to assess the consistency of a belief with observed data."
    },
    {
        "contexts": "There are two key assumptions that are made about hypotheses here: 1. Hypotheses are mutually exclusive - at most one hypothesis can be true. 2. Hypotheses are collectively exhaustive - assuming any given hypothesis being considered comes from a pool of hypotheses, at least one of the hypotheses in the pool has to be true.",
        "summary": "Two assumptions about hypotheses in Bayesian analysis are: they are mutually exclusive and collectively exhaustive, meaning only one can be true, but at least one must be true.",
        "question": "What are the two key assumptions made about hypotheses in Bayesian analysis?",
        "ground_truths": "The two key assumptions made about hypotheses in Bayesian analysis are that they are mutually exclusive and collectively exhaustive."
    },
    {
        "contexts": "The Monty Hall problem is based on the famous show called \"Let's Make a Deal\" where Monty Hall was the host. In one of the games on the show, Monty would present the following problem to the contestant: There are three doors (Door 1, Door 2, and Door 3). Behind two of those doors are worthless items (a goat), and behind one of the door is something valuable (a car). You don't know what is behind which door, but Monty knows. You are asked to choose one of the doors. Then, Monty opens one of the 2 remaining doors. He only ever opens the door with a goat behind it.",
        "summary": "The Monty Hall problem originates from a game show where contestants choose from three doors. One door hides a car, and the other two have goats. Monty Hall, who knows what's behind each door, opens another door revealing a goat after the contestant makes a choice.",
        "question": "Who knows what is behind each door in the Monty Hall problem?",
        "ground_truths": "Monty knows what is behind each door in the Monty Hall problem."
    },
    {
        "contexts": "Step 1: Let's summarize the problem in a more mathematical way: Initially, we don't know anything so we choose Door 1: Hypothesis (H): Door 1 has a car behind it. Then, Monty opens a door. This changes our knowledge about the system. Data (D): Monty has revealed a door with a goat behind it. What is the probability that Door 1 has a car behind it? Problem: P(H|D) = ?",
        "summary": "The problem is framed mathematically. Initially, the hypothesis is that Door 1 has a car. After Monty opens a door revealing a goat, the data changes, prompting the question of the probability that Door 1 still has the car.",
        "question": "What changes after Monty opens a door?",
        "ground_truths": "After Monty opens a door, our knowledge about the system changes because Monty has revealed a door with a goat behind it."
    },
    {
        "contexts": "Conclusion: So, the probability that your hypothesis (or rather belief) that the car is behind the door you choose (Door 1) is only 1/3. Conversely, the probability that the the car is behind a door that you did not choose is 2/3. How can we use this to guide our decision? Since the probability that the car is behind a door that we have not chosen 2/3 and Monty has already opened one door with a goat behind it, it would make more sense to switch.",
        "summary": "The probability that the car is behind the chosen door is 1/3, while it's 2/3 for a door not chosen. Given that Monty reveals a goat behind another door, it's logical to switch choices.",
        "question": "Why would it make sense to switch your door choice?",
        "ground_truths": "It would make more sense to switch your door choice because the probability that the car is behind a door that you did not choose is 2/3."
    },
    {
        "contexts": "Bayesian approaches incorporate prior information into its analysis. This means that any past information one has can be used in a fruitful way. Bayesian approach provides a more intuitive and direct statement of the probability that the hypothesis is true, as opposed to the frequentist approach where the interpretation of p-value is convoluted.",
        "summary": "Bayesian approaches use prior information in analysis and provide a more direct understanding of a hypothesis' probability, unlike the frequentist approach which has a convoluted interpretation of p-values.",
        "question": "What kind of information does Bayesian analysis incorporate?",
        "ground_truths": "Bayesian analysis incorporates prior information into its analysis."
    },
    {
        "contexts": "In contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously.",
        "summary": "Unlike the Frequentist approach, Bayesian statistics consider events as dynamic phenomena whose probabilities can change with new data.",
        "question": "How do Bayesian statistics view events in experiments?",
        "ground_truths": "Bayesian statistics view events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously."
    },
    {
        "contexts": "The Belbin Team Inventory is a role test for groups. The behavioral test developed by Meredith Belbin fosters effective teamwork and is based on personal questions that you answer online. As a result, you receive an evaluation of your individual preferences regarding the following nine team roles: plants, monitor evaluators, specialists, resource investigators, co-ordinators, team workers, shapers, implementers and completer-finishers.",
        "summary": "The Belbin Team Inventory is an online behavioral test designed to evaluate an individual's preferences for nine distinct team roles.",
        "question": "What is the Belbin Team Inventory designed to evaluate?",
        "ground_truths": "The Belbin Team Inventory is designed to evaluate an individual's preferences for nine distinct team roles: plants, monitor evaluators, specialists, resource investigators, co-ordinators, team workers, shapers, implementers and completer-finishers."
    },
    {
        "contexts": "To form an effective team, diverse roles should be represented. Therefore, it is recommended to do the test before forming groups. However, existing groups can also use the test in order to gain a deeper understanding of competencies and responsibilities. The ideal is of course a team in which all nine different roles are distributed.",
        "summary": "The test is advised before forming groups to ensure a diverse set of roles for effective teamwork. Existing teams can also benefit from the insights.",
        "question": "Why is it recommended to take the Belbin test?",
        "ground_truths": "It is recommended to do the test before forming groups to ensure that a diverse set of roles are represented for effective teamwork. The ideal is a team in which all nine different roles are distributed."
    },
    {
        "contexts": "There are some things to take care of: First of all, this method involves personal questions. Thus, it is important to make sure that everyone feels comfortable in the process. Second, the Belbin roles only refer to group work and therefore are not to be equated with character traits. And finally, the roles should not be seen as fixed constructs. Team members can also take on different roles at the same time or change roles in the course of group work.",
        "summary": "Care should be taken with personal questions, and it's important to know that Belbin roles are specific to group work, not fixed, and can change.",
        "question": "What considerations should be taken into account when using the Belbin test?",
        "ground_truths": "It is important to make sure that everyone feels comfortable with the personal questions involved in the test. The Belbin roles are specific to group work and should not be equated with character traits. Roles are not fixed and can change during the course of group work."
    },
    {
        "contexts": "Do the Belbin test online. There is a free questionnaire in pdf-format, which is linked below. Alternatively, you can do the test on the official Belbin homepage for a fee. After you have received your individual team role, discuss and analyse the results in your large group. Based on this you can now form diverse teams. Once you have found your team, it is important to discuss the roles explicitly and to clarify the tasks and responsibilities associated with them.",
        "summary": "Perform the online Belbin test either for free or for a fee. Discuss the results to form diverse teams and clarify roles and responsibilities.",
        "question": "How can one get started with the Belbin Team Inventory?",
        "ground_truths": "To get started with the Belbin Team Inventory, one can take the test online either through a free questionnaire or on the official Belbin homepage for a fee. After receiving individual team roles, discuss and analyze the results in a large group to form diverse teams. Once the team is formed, it is important to discuss the roles explicitly and clarify tasks and responsibilities."
    },
    {
        "contexts": "A bias is the action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment. In other words, bias clouds our judgment and often action in the sense that we act wrongly. We are all biased, because we are individuals with individual experiences, and are unconnected from other individuals and/or groups, or at least think we are unconnected. Recognising bias in research is highly relevant, because bias exposes the myth of objectivity of research and enables a better recognition and reflection of our flaws and errors. To this end, acknowledging bias is understanding the limitations of oneself.",
        "summary": "Bias refers to the unfair influence of personal opinions on judgment. Everyone is biased due to their unique experiences. Recognizing bias, particularly in research, is crucial as it dispels the myth of objectivity and helps us understand our limitations.",
        "question": "Why is recognizing bias especially important in research?",
        "ground_truths": "Recognizing bias in research is highly relevant because it exposes the myth of objectivity in research and enables better recognition and reflection of flaws and errors."
    },
    {
        "contexts": "While qualitative research is often considered prone to many biases, it is also often more reflexive in recognising its limitations. Many qualitative methods are defined by a strong subjective component - i.e. of the researcher - and a clear documentation can thus help to make an existing bias more transparent. Many quantitative approaches have a reflexive canon that focuses on specific biases relevant for a specific approach, such as sampling bias or reporting bias. These are often less considered than in qualitative methods, since quantitative methods are still  falsely - considered to be more objective. This is not true.",
        "summary": "Qualitative research is viewed as susceptible to biases but is often better at acknowledging limitations. Quantitative approaches focus on specific biases like sampling bias, but are falsely assumed to be more objective.",
        "question": "How do qualitative and quantitative research differ in their approach to bias?",
        "ground_truths": "Qualitative research is often more reflexive in recognising its limitations and is defined by a strong subjective component. Quantitative approaches focus on specific biases such as sampling bias and reporting bias but are often falsely considered to be more objective."
    },
    {
        "contexts": "Another general differentiation can be made between inductive and deductive approaches. Many deductive approaches are affected by bias that is associated to sampling. Inductive approaches are more associated to bias during interpretation. Deductive approaches often build around designed experiments, while the strongpoint of inductive approaches is being less bound by methodological designs, which can also make bias more hidden and thus harder to detect.",
        "summary": "Deductive approaches are often biased in sampling, while inductive approaches face bias during interpretation. Deductive methods focus on designed experiments, whereas inductive methods are less constrained by design, making bias harder to detect.",
        "question": "What types of bias commonly affect deductive and inductive research methods?",
        "ground_truths": "Many deductive approaches are affected by bias that is associated to sampling. Inductive approaches are more associated to bias during interpretation."
    },
    {
        "contexts": "The three steps of the application of a method are clearly worth investigating, as it allows us to dismantle at which stage we may inflict a bias into our application of a method. Gathering data is strongly associated with cognitive bias, yet also to statistical bias and partly even to some bias in academia. Bias associated to sampling can be linked to a subjective perspective as well as to systematic errors rooted in previous results. This can also affect the analysis of data, yet here one has to highlight that quantitative methods are less affected by a bias through analysis than qualitative methods. Overall, we need to recognise that some methods are less associated to certain biases because they are more established concerning the norms of their application, while other methods are new and less tested by the academic community.",
        "summary": "Investigating the three steps in applying a method reveals where bias may occur. Gathering data is linked to cognitive and statistical biases and sometimes to academic biases. Sampling bias arises from subjective perspectives or systematic errors. Quantitative methods are less susceptible to bias during data analysis compared to qualitative ones. Established methods usually have well-understood biases compared to newer, less-tested methods.",
        "question": "Why are quantitative methods less susceptible to bias during data analysis?",
        "ground_truths": "Quantitative methods are less affected by a bias during data analysis than qualitative methods because they are more established concerning the norms of their application."
    },
    {
        "contexts": "When it comes to bias, there can be at least a weak effect that safety - although not diversity - concerning methods comes in numbers. More and diverse methods may offer new insights on biases, since one method may reveal a bias that another method cannot reveal. Methodological plurality may reduce bias. For a fully established method the understanding of its bias is often larger, because the number of times it has been applied is larger.",
        "summary": "Safety in methods can come from their frequent use, allowing biases to be better understood. Using a variety of methods can reveal different biases, potentially reducing overall bias. Established methods often have well-understood biases due to their frequent application.",
        "question": "How can methodological plurality potentially reduce bias?",
        "ground_truths": "Methodological plurality may reduce bias because more and diverse methods may offer new insights on biases, since one method may reveal a bias that another method cannot reveal."
    },
    {
        "contexts": "Reason, social contract and utilitarianism are the three key theories of Western philosophy relevant for empiricism, and all biases can be at least associated to one of these three foundational theories. Many cognitive bias are linked to reason or unreasonable behaviour. Much of bias relates to prejudices and society can be linked to the wide field of social contract. Lastly, some bias is clearly associated with utilitarianism.",
        "summary": "Biases can be tied to one of three key Western philosophical theories: reason, social contract, or utilitarianism. Cognitive biases often relate to reason, social biases to the social contract, and some biases are associated with utilitarianism.",
        "question": "What are the three key theories of Western philosophy relevant for understanding biases?",
        "ground_truths": "The three key theories of Western philosophy relevant for understanding biases are reason, social contract, and utilitarianism."
    },
    {
        "contexts": "Out of the growing empiricism of the enlightenment there grew a concern which we came to call Critical Theory. At the heart of critical theory is the focus on critiquing and changing society as a whole, in contrast to only observing or explaining it. Originating in Marx, Critical Theory consists of a clear distancing from previous theories in philosophy - or associated with the social - that try to understand or explain. By embedding society in its historical contexts (Horkheimer) and by focussing on a continuous and interchanging critique (Benjamin) Critical Theory is a first and bold step towards a more holistic perspective in science. Remembering the Greeks and also some Eastern thinkers, one could say it is the first step back to a holistic thinking.",
        "summary": "Critical Theory aims to critique and change society, distancing itself from theories that simply observe or explain. Originating in Marx, it takes a holistic perspective in science and embeds society in its historical contexts.",
        "question": "What is the primary aim of Critical Theory?",
        "ground_truths": "The primary aim of Critical Theory is to critique and change society as a whole, in contrast to only observing or explaining it."
    },
    {
        "contexts": "From a methodological perspective, Critical Theory is radical because it seeks to distinguish itself not only from previously existing philosophy, but more importantly from the widely dominating empiricism, and its societal as well as scientific consequences. A Critical Theory should thus be explanatory, practical and normative, and what makes it more challenging, it needs to be all these three things combined (Horkheimer).",
        "summary": "Methodologically, Critical Theory is radical, aiming to distinguish itself from existing philosophy and dominating empiricism. It strives to be explanatory, practical, and normative, all combined.",
        "question": "What three characteristics should a Critical Theory possess according to Horkheimer?",
        "ground_truths": "According to Horkheimer, a Critical Theory should be explanatory, practical, and normative."
    },
    {
        "contexts": "Critical Theory to this end is now developing to connect to other facets of the discourse, and some may argue that its focus onto the social science can be seen critical in itself, or at least as a normative choice that is clearly anthropocentric, has a problematic relationship with the empirical, and has mixed relations with its diverse offspring that includes gender research, critique of globalisation, and many other normative domains that are increasingly explored today.",
        "summary": "Critical Theory is evolving to engage with different discourses. It focuses on social science and has complex relationships with empirical methodologies and diverse subjects like gender research and globalisation critique.",
        "question": "How is Critical Theory evolving in its relationship with other domains?",
        "ground_truths": "Critical Theory is developing to connect to other facets of the discourse, including diverse subjects like gender research and the critique of globalisation."
    },
    {
        "contexts": "The disciplinary bias of modern science thus creates a deeply normative methodological bias, which some disciplines may try to take into account yet others clearly not. In other words, the dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research, and we need to be aware and reflexive about this.",
        "summary": "Modern science has a deeply normative methodological bias due to disciplinary bias. The dogmatic selection of methods can create flaws in empirical research.",
        "question": "What kind of bias does the dogmatic selection of methods create in empirical research?",
        "ground_truths": "The dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research."
    },
    {
        "contexts": "Researchers are humans, and humans are always biased to some extent - therefore, researchers are biased, if they are aware of it or not. This understanding is rooted in our assumption that epistemological knowledge is subjective. It is however better to be aware of one's biases in order to be able to counteract them in one's research. So, in this entry, let us focus on the biases that influence your research that revolves around any form of Interviews.",
        "summary": "Researchers, being human, are inherently biased whether they recognize it or not. The article stresses the importance of acknowledging these biases, especially in research involving interviews.",
        "question": "Why is it important for researchers to be aware of their biases?",
        "ground_truths": "It is important for researchers to be aware of their biases in order to counteract them in their research, since researchers are humans, and humans are always biased to some extent."
    },
    {
        "contexts": "In preparation to any form of Interview, you may conduct a systematic literature review or some other form of literature-based theoretical preparation to be able to construct and conduct the Interviews. Here, a Reporting / Publication Bias will influence your literature search, which refers to the issue that journals and researchers often only report specific types of information, and leave out others.",
        "summary": "Before conducting interviews, researchers often prepare by reviewing literature, a process affected by Reporting / Publication Bias, where only specific types of information are made public.",
        "question": "What is the impact of Reporting / Publication Bias on literature review?",
        "ground_truths": "Reporting / Publication Bias influences the literature search by causing journals and researchers to often only report specific types of information, and leave out others."
    },
    {
        "contexts": "Further, you might commit a Congruence Bias if you only test for their hypothesis directly but not for possible alternative hypotheses. There may be other explanations for a specific phenomenon, and by limiting yourself to one perspective, you are limiting your research results and the discourse around a given topic.",
        "summary": "Researchers may fall into Congruence Bias by focusing solely on confirming their hypotheses, thereby ignoring other possible explanations and limiting research results.",
        "question": "How does Congruence Bias limit research results?",
        "ground_truths": "Congruence Bias limits research results by focusing only on confirming the researchers' hypotheses and ignoring other possible explanations for a specific phenomenon."
    },
    {
        "contexts": "In the creation of a sample for any kind of Interview, Sampling / Selection Bias is a most common problem. We might only sample data that we are aware of, because we are aware of it, thereby ignoring other data points that lack our recognition.",
        "summary": "Sampling / Selection Bias occurs frequently in the creation of interview samples. It happens when researchers choose data they are familiar with, ignoring other potentially relevant data points.",
        "question": "What is the issue with Sampling / Selection Bias in creating interview samples?",
        "ground_truths": "The issue with Sampling / Selection Bias is that researchers might only sample data that they are aware of, thereby ignoring other data points that lack their recognition."
    },
    {
        "contexts": "While conducting an observation as part of our sampling you might have fallen under the Observer Bias: you (dis)favor one or more groups because of one reason or another, thereby influencing your sample and thus your data subconsciously.",
        "summary": "Observer Bias can affect research during the observation stage, influencing the sample by subconsciously favoring or disfavoring certain groups.",
        "question": "How does Observer Bias influence data collection?",
        "ground_truths": "Observer Bias influences data collection by causing the researcher to subconsciously favor or disfavor one or more groups, thereby affecting the sample."
    },
    {
        "contexts": "A challenge one needs to be aware of when conducting and analyzing Focus Groups is the censorship of certain  e.g. minority or marginalized  viewpoints, which can arise from the group composition. As Parker and Tritter (2006, p. 31) note: At the collective level, what often emerges from a focus group discussion is a number of positions or views that capture the majority of the participants standpoints. Focus group discussions rarely generate consensus but they do tend to create a number of views which different proportions of the group support.\" Further, considering the effect that group dynamics have on the viewpoints expressed by the participants is important, as the same people might answer differently in an individual interview. Depending on the focus of the study, either a Focus Group, an individual interview or a combination of both might be appropriate (Kitzinger 1994).",
        "summary": "Conducting Focus Groups can lead to the censorship of minority viewpoints due to group dynamics. The same participants may respond differently in individual interviews. The methodology should align with the study's objectives.",
        "question": "How do group dynamics affect the views expressed in Focus Groups?",
        "ground_truths": "Group dynamics can lead to the censorship of minority or marginalized viewpoints. Focus group discussions rarely generate consensus but tend to create a number of views which different proportions of the group support."
    },
    {
        "contexts": "When survey participants do not completely and accurately remember events, personal motivations, or behaviors from the past, this is referred to as 'recall bias'. Their difficulties to recall information that is queried can stem from their motivation, memory, and ability to respond (Bhattacherjee, 2012, p. 82).",
        "summary": "Recall bias occurs when survey participants fail to accurately remember past events, influenced by their motivation, memory, and ability to respond.",
        "question": "What factors contribute to recall bias in surveys?",
        "ground_truths": "Recall bias can stem from participants' motivation, memory, and ability to respond."
    },
    {
        "contexts": "The 'common method bias' relates to the fallacy that there is a certain covariance shared between independent and dependent variables that are measured at the same point in time [] using the same instrument (Bhattacherjee, 2012, p. 82). Statistical tests can be used to identify this type of bias. (Bhattacherjee, 2012)",
        "summary": "Common method bias arises from covariance between independent and dependent variables measured simultaneously. Statistical tests can identify it.",
        "question": "How can common method bias be identified?",
        "ground_truths": "Statistical tests can be used to identify common method bias."
    },
    {
        "contexts": "Lastly, there is a group of biases revolving around how you perceive your Interviewees. This may influence how you conduct your Interview, and how your transcribe recordings, even before analyzing them further. 'Attribution Bias' is about systematic errors based on flawed perception of others' or one's own behavior, for example misinterpreting an individual's behavior. We have the 'Halo / Horn Effect', which means that an observer's overall impression of an entity influences their feelings about specifics of that entity's properties. If you like an Interviewee, you may interpret their responses differently than if you didn't like them as much, and will ask different kinds of questions therefrom. There are 'Cultural Biases,' which makes you interpret and judge behavior and phenomena by standards inherent to your own culture. And again, we have 'Sexism, Lookism, Racism, and Classism', which may lead you to interpreting specific responses in a misguided way due to preconceived ideas about how specific groups of people live or see the world.",
        "summary": "Various biases like Attribution, Halo/Horn Effect, Cultural, and Sexism can affect how you perceive interviewees, thus influencing the interview and analysis process.",
        "question": "What biases can affect an interviewer's perception and conduct?",
        "ground_truths": "Biases such as Attribution Bias, Halo/Horn Effect, Cultural Biases, Sexism, Lookism, Racism, and Classism can influence how an interviewer perceives interviewees and conducts the interview."
    },
    {
        "contexts": "There are certainly more aspects, as this is a complex and emerging topic. Not all biases are easy to detect, and it is not always easy to avert them. A first important step is to acknowledge how you and your cultural, institutional, or societal background may influence how you set up and conduct your research. This will help you limit their influence, and there are some additional tips for how to conduct the different types of Interviews, which are mentioned in the respective entries. In your critical reflection of your methodology - which you should always do - you should develop and access knowledge about potential biases, and how much these biases may influence or shape your research.",
        "summary": "The subject of biases in research is intricate. Acknowledging one's own background can help minimize biases. Critical reflection on methodology is necessary to understand and limit biases.",
        "question": "Why is acknowledging one's own background important in conducting research?",
        "ground_truths": "Acknowledging one's own cultural, institutional, or societal background may help limit the influence of biases in setting up and conducting research."
    },
    {
        "contexts": "A bias is the action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment. In other words, biases cloud our judgment and often action in the sense that we act wrong. We are all biased, because we are individuals with individual experience, and are unconnected from other individuals and/or groups, or at least think we are unconnected.",
        "summary": "Bias is the unfair support or opposition to something or someone based on personal opinions. Everyone has biases due to their unique experiences.",
        "question": "Why do all individuals have biases?",
        "ground_truths": "All individuals have biases because they are individuals with unique experiences and are unconnected from other individuals and/or groups."
    },
    {
        "contexts": "If our actions are biased, mistakes happen and the world becomes more unfair or less objective. We are all biased. Therefore, we need to learn of consciously reflect our biases if at all possible. Biases are abundantly investigated and discussed both in science and society. However, we  only start to understand the complex mechanism behind the diverse sets of biases, and how to cope with the different biases.",
        "summary": "Biased actions can lead to mistakes and an unfair world. While biases are widely studied, understanding and coping mechanisms are still evolving.",
        "question": "What is the importance of understanding biases?",
        "ground_truths": "Understanding biases is important because biased actions can lead to mistakes and make the world more unfair or less objective."
    },
    {
        "contexts": "There are a lot of biases. Within statistics 'sample bias' and 'selection bias' are among the most common problems that occur. Starting with the sampling of data we might have a distorted view of reality through selection bias: we might only sample data because we are aware of it, thereby ignoring other data points that lack our recognition.",
        "summary": "In statistics, common biases include 'sample bias' and 'selection bias'. These biases can distort our view of reality by affecting data sampling.",
        "question": "How can selection bias distort our view of reality?",
        "ground_truths": "Selection bias can distort our view of reality by causing us to only sample data we are aware of, thereby ignoring other relevant data points."
    },
    {
        "contexts": "Scientists are notoriously prone to another bias, the 'hindsight bias', which highlights the omnipotence of scientists in general, who are quite sure of themselves when it comes to analyzing their data. Furthermore the 'IKEA effect' might underline how brilliant our analysis is, since we designed the whole study by ourselves.",
        "summary": "Scientists are susceptible to 'hindsight bias' and the 'IKEA effect', which can affect their confidence and perception of their own work.",
        "question": "What is the 'IKEA effect' related to in scientific research?",
        "ground_truths": "In scientific research, the 'IKEA effect' might underline how brilliant scientists think their analysis is, especially since they designed the entire study themselves."
    },
    {
        "contexts": "As part of the first group we can consider that whenever we report everything we made in the study in a transparent way, then even if our result are influenced by some sort of bias, then at least our results should be reproducible or the very least, other researchers may be able to understand that we conducted our study in a way that inferred a bias. This happens all the time, and it is often a matter of experience whether we are aware of a bias. Still, some simple basics can also be taught regarding statistics that may help us to report information to understand a bias, and then there are even some ways to minimise a bias. Sample size as part of the design, as well as sample selection are the first steps to minimise sampling bias. Stratified designs may ensure a more representative sampling, as we often tend to make deliberate samples that inflict a certain bias.",
        "summary": "Transparency in reporting studies can help in the reproducibility of results even if they are biased. Experience can help in recognizing these biases and statistical basics can teach ways to minimize them. Sampling methods like stratified designs can further reduce bias.",
        "question": "How can one minimize biases in scientific studies?",
        "ground_truths": "One can minimize biases by reporting studies transparently for reproducibility, gaining experience to recognize these biases, and employing statistical methods like stratified designs for sampling."
    },
    {
        "contexts": "The second bias in statistics is the analysis bias. Here, we should be aware that it can be beneficial to contact a statistician in order to inquiry which model would be best for our data. We need to apply the most parsimonious model, yet also should report all results in a most unbiased and thus most reproducible fashion. Quite often one is able to publish details about the analysis in the online appendix of a paper. While this makes the work often more reproducible, I am sorry to report that in the past it also led to some colleagues utilising the data and analysing it only partly or wrong, hence effectively making a scientific misconduct.",
        "summary": "Analysis bias is another concern that may require consulting a statistician for the best model. Reporting all results openly and in detail aids in reproducibility but has led to misuses like scientific misconduct in the past.",
        "question": "What are the risks involved in making the research more reproducible?",
        "ground_truths": "The risks involved include the potential for misuse of data, which may lead to scientific misconduct."
    },
    {
        "contexts": "Some disciplines tend to add disclaimers to the interruption of results, and in some fields of science it is considered as good scientific practice to write about the limitations of the study. While this is in theory a good idea, these fields of science devalue their studies by adding such a section generically. I would argue, that if such a section is added every time, it should be added never, since it is totally generic. Everyone with a firm understanding of statistics as well as theory of science knows that studies help to approximate facts, and future insights might lead to different and more often to more precise results.",
        "summary": "Some scientific fields add disclaimers about study limitations, which can devalue the research. However, disclaimers can become redundant and thus lose their impact, despite their theoretical usefulness.",
        "question": "Why might disclaimers in scientific studies be counterproductive?",
        "ground_truths": "Disclaimers might be counterproductive because they can devalue the research and become redundant, losing their intended impact."
    },
    {
        "contexts": "While any \"why\" question can easily be dismissed by a simple \"why not?\", times have been hard for many, and a general and individual pessimism is haunting many people these days. Indeed in my surrounding I recognise in many the focus on the general decreasing state of everything, the pending apocalypse, and the end of the world as we know it. I feel to be in a very privileged position indeed, because I will remain a radical optimist until the end. Here are my arguments.",
        "summary": "The article opens by acknowledging the general sense of pessimism pervasive in society today. Despite this, the author insists on maintaining a stance of radical optimism and aims to present his arguments for it.",
        "question": "Why does the author insist on maintaining a stance of radical optimism?",
        "ground_truths": "The author insists on maintaining a stance of radical optimism despite the prevailing pessimism in society. He feels privileged and aims to present his arguments for optimism."
    },
    {
        "contexts": "Pessimism has never made anything better. While this seems like a very easy line of argumentation, it is actually not. Many people argue that pessimism can make people fearful, and such a nudge will help people to recognise to change their behaviour. There is much research that nudges can actually change the behaviour of people. I would dispute this on two terms.",
        "summary": "The author challenges the notion that pessimism, through fear-based 'nudges', can effectively change people's behavior. He questions the long-term effectiveness and ethical implications of using nudges.",
        "question": "What does the author question about the effectiveness of using nudges?",
        "ground_truths": "The author questions the long-term effectiveness and ethical implications of using fear-based nudges to change people's behavior."
    },
    {
        "contexts": "Radical optimism is just like any form of pessimism or optimism a normative choice. Martha Nussbaum rightly stated that hope is a practice, and a choice. The question would be then, if we can make this choice, and if we should. It is clear that this normative choice is an effort, and it is unclear who can actively make this choice, and who may merely not have the capability to make that choice.",
        "summary": "The author asserts that radical optimism, like any form of optimism or pessimism, is a normative choice. He questions who has the capability to actively make such a choice and suggests it's not easy for everyone.",
        "question": "What does the author say about the capability to choose optimism?",
        "ground_truths": "The author questions who has the capability to actively choose radical optimism, suggesting it's not an easy choice for everyone to make."
    },
    {
      "contexts": "Cultural identity is equally such a place of phantasy, because it is not about who a person is, but about who we are as a united group, interacting with each other. Cultural identity would not make sense if you are alone. Cultural identity can be thus seen as a construct that helps us to belong, and create some sort of unity among a group of people. In the past, this unity was often inherited, yet today in a globalised world, there are many cultural groups that are not inherited. The world grew more diverse, and there is a larger recognition of many different facets of cultural identity.",
      "summary": "Cultural identity serves as a construct that creates unity among a group and becomes more diverse in a globalized world. It used to be inherited but now can be self-selected.",
      "question": "Why has cultural identity become more diverse?",
      "ground_truths": "Cultural identity has become more diverse due to the influence of globalization and the recognition of many different facets of cultural identity."
    },
    {
      "contexts": "However, we shall not forget that some of the worst atrocities in the history of people can be associated to cultural identity. It is however not the exclusion of people that do not belong to an identity group, that is the actual problem. Instead, it is the actions that may arise out of the exclusion of \"others\" from an identity group that is the true problem. Consequently, cultural identity should never enable members of a group to take negative actions against other people. This is in itself a very difficult assumption, not only because it would be hard to achieve.",
      "summary": "Cultural identity can lead to atrocities, not due to exclusion itself but due to negative actions against others resulting from that exclusion. Achieving non-discriminatory culture is challenging.",
      "question": "What is the major challenge in preventing cultural identity from causing harm?",
      "ground_truths": "The major challenge is preventing members of a cultural identity group from taking negative actions against others, particularly those who are excluded."
    },
    {
      "contexts": "Utilitarianism claims to aim at the best (overall) outcome, yet our trouble to evaluate exactly what is better or worse is one of the most central mishaps - to me - in Western thinking. To this end I believe we make two main mistakes, that are strangely intertwined and can best be condensed by two big words: Epistemology and deontology. We make an epistemological mistake by attempting to evaluate consequences of our actions through observational knowledge. The second fallacy we make is based on the error to ethically evaluate our actions against some higher rules, instead of the consequences of our actions, which is, simply put, a deontological mistake.",
      "summary": "Utilitarianism aims for the best outcome, but is often misunderstood due to two intertwined errors: epistemology, which focuses on observational knowledge, and deontology, which considers higher rules over consequences.",
      "question": "What are the two main mistakes in Western thinking about utilitarianism?",
      "ground_truths": "The two main mistakes in Western thinking about utilitarianism are epistemological and deontological errors. The epistemological error is evaluating consequences through observational knowledge, and the deontological error is focusing on higher rules over consequences."
    },
    {
      "contexts": "Deontology focussed on the evaluations of our actions based on the principles or rules these are based on instead of the consequences of our actions. While Bentham as an early advocate of utilitarianism was surely focussing on consequences instead of mere actions and their underlying principles, this problem has within societal debates hardly been resolved. Many religious groups and cults are obsessed with a rule based world up until today, and the current cancel culture and social media wars are a mirror of a similarly rule-obsessed world. These current critical realities and societal debates clearly show why deontology must fail, because as much as we try to act right, it almost aways seems we all fail.",
      "summary": "Deontology focuses on evaluating actions based on principles rather than consequences. This approach fails to resolve societal debates and is reflected in the rule-obsessed world of cancel culture and social media wars.",
      "question": "Why is deontology considered to be a failing approach?",
      "ground_truths": "Deontology is considered to be a failing approach because it focuses on evaluating actions based on rules or principles rather than consequences. This has not resolved societal debates and contributes to a rule-obsessed culture."
    },
    {
        "contexts": "In other words, we widely lost our ontological roots. Now I am far from making a plea for religion here, but simply want to highlight that we seem to have lost any glimpse of ontological truths that we could all agree upon. However, if we cannot agree on anything, then what matters? Critical realism clearly claims that there might be such ontological truths in the world, yet we may never find these principles. These may not be deontological rules, however.",
        "summary": "We have largely lost touch with ontological truths that society can universally accept. Critical realism suggests that such truths could exist but might remain undiscovered.",
        "question": "Why is it difficult to find universal ontological truths?",
        "ground_truths": "It is difficult to find universal ontological truths because society has largely lost touch with them and critical realism suggests they might remain undiscovered."
    },
    {
        "contexts": "We have to conclude that our observational powers as well as the deviance of the real world from our expectations do not always give us sufficient reason for rules we can agree upon. We therefore need to take the intention of our actions into account, and thereby modify our viewpoint. Instead of a rule-based world view or an act consequentialists world view, we need to settle on an intention-based worldview, where no one objects our intentions. Our intentions may follow certain rules, which in many cases cannot be neglected to give some general guideline. If we thus continue to agree to act based on certain rules, we equally need to teach the capacity or allow within a system to deviate from certain rules.",
        "summary": "Our observational abilities and the difference between reality and our expectations make it hard to find universally agreed-upon rules. An intention-based worldview is suggested, allowing room for rule deviation.",
        "question": "How should one's worldview be adapted to accommodate the complexities of real-world observations and expectations?",
        "ground_truths": "One's worldview should be adapted to an intention-based perspective that allows for deviations from certain rules, as universally agreed-upon rules are difficult to find due to observational complexities and real-world deviations."
    },
    {
        "contexts": "The biggest lack to date has probably been in recognising global inequalities. First attempts have been made, notably the global sustainability goals of the taxing of global cooperation that try to evade national tax laws. Yet while many global inequalities decreased, some inequalities such as global income disparity, have increased over the last decades. Pessimists claim we shall never overcome these inequalities, but that these inequalities would even increase. This claim does not only contradict past developments, but is also leading nowhere.",
        "summary": "Global inequalities remain a significant issue despite attempts to address them through sustainability goals. Pessimistic views on never overcoming these inequalities are counterproductive and contradict historical trends.",
        "question": "What is the impact of pessimistic views on the efforts to address global inequalities?",
        "ground_truths": "Pessimistic views are counterproductive and contradict historical trends in efforts to address global inequalities."
    },
    {
        "contexts": "A bubble plot is able to present up to four variables, without actually being a four dimensional plot. We can first start with trying to plot three variables. For that the input data should be a triplet (Note: the data should be quantitative and non-categorical). One variable is represented by the x-axis, another one by the y-axis and the third by the size of the data points. Therefore the data points differ in size which makes the plot look like an accumulation of bubbles. We will then incorporate the fourth variable as a color later in our example.",
        "summary": "A bubble plot can display up to four variables. It starts with plotting three variables: one on the x-axis, another on the y-axis, and the third by the size of the data points. The data points thus vary in size, making the plot look like bubbles. A fourth variable can be added later as color.",
        "question": "How does a bubble plot represent multiple variables?",
        "ground_truths": "A bubble plot can display up to four variables. One variable is represented by the x-axis, another on the y-axis, the third by the size of the data points, and a fourth variable can be added as color."
    },
    {
        "contexts": "After installing and including the gapminder and tidyverse packages we are ready to create the plot. I decided to set the theme via theme_set() of the plot here. The theme is the overall design and background of your plot. An overview of ggplot themes can be found here.",
        "summary": "After installing gapminder and tidyverse packages, one can set the theme of the plot using theme_set(). The theme influences the overall design and background of the plot.",
        "question": "How do you set the overall design and background of a bubble plot?",
        "ground_truths": "One can set the theme of the plot using theme_set(). The theme influences the overall design and background of the plot."
    },
    {
        "contexts": "If you took a look at the Gapminder data tool [link], you might have noticed that the bubbles are colored to indicate the world regions. This type of color grouping can be easily implemented within our plot. We just add within the function geom_point() the type color in the function aes(). By this we map another variable, in this case the number of forward gears, to the type color. And last but not least, we can change the color palette with the function scale_color_brewer(), if we do not like the default color palette.",
        "summary": "In Gapminder, bubbles are colored to signify world regions. This color grouping can be added to our bubble plot by using the function geom_point() and specifying color in aes(). The color can represent another variable, such as the number of gears. The color palette can be customized using scale_color_brewer().",
        "question": "How can color be added to differentiate variables in a bubble plot?",
        "ground_truths": "Color grouping can be added to the bubble plot by using the function geom_point() and specifying color in aes()."
    },
    {
        "contexts": "Out of a diverse rooting in discussions about complexity, system thinking and the need to understand specific contexts more deeply, the classic experimental setting did at some point become more and more challenged. What do we learn from singular cases? How do we deal with cases that are of pronounced importance, yet cannot be replicated? And what can be inferred from the design of such case studies? A famous example from ethnographic studies is the Easter Island. From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant, plausible and valid.",
        "summary": "Challenges in classic experimental settings have led to a focus on singular, non-replicable cases like Easter Island. From a statistical standpoint, these cases are problematic but can still yield valid, relevant knowledge.",
        "question": "Why are singular, non-replicable cases difficult from a statistical standpoint?",
        "ground_truths": "These cases are difficult because they lack reproducibility, which challenges their statistical validity. However, they can still yield relevant and valid knowledge."
    },
    {
        "contexts": "For example the financial crisis from 2007, where many patterns where comparable to previous crisis, but other factors were different. Hence this crisis is comparable to many previous factors and patterns regarding some layers of information, but also novel and not transferable regarding other dynamics. We did however understand based on the single case of this financial crisis that certain constructs in our financial systems are corrupt if not broken.",
        "summary": "The 2007 financial crisis shared patterns with past crises but also had unique factors. The single case revealed systemic corruption or flaws in our financial systems.",
        "question": "What did the single case of the 2007 financial crisis reveal?",
        "ground_truths": "The single case of the 2007 financial crisis revealed systemic corruption or flaws in our financial systems."
    },
    {
        "contexts": "Another prominent example of a single case or phenomena is the Covid pandemic that emerges further while I am writing these lines. While much was learned from previous pandemics, this pandemic is different, evolves different, and creates different ramifications. The impact of our societies and the opportunity to learn from this pandemic is however undeniable.",
        "summary": "The Covid pandemic is a unique case with its own set of challenges and impacts, different from previous pandemics. It has an undeniable societal impact and provides learning opportunities.",
        "question": "Why is the Covid pandemic considered a unique case?",
        "ground_truths": "The Covid pandemic is considered a unique case because it has its own set of challenges and impacts, different from previous pandemics. It also provides unique learning opportunities."
    },
    {
        "contexts": "Sustainability science shall thus realize that making cases comparable is the key to coherent knowledge, and ultimately a more systematic approach to new knowledge. Statistics will certainly not be the only contribution to this end, but it is quite certain that it will contribute to a holistic understanding.",
        "summary": "Sustainability science should focus on making cases comparable for coherent knowledge. Statistics will play a role in achieving a holistic understanding.",
        "question": "How can sustainability science achieve coherent knowledge?",
        "ground_truths": "Sustainability science can achieve coherent knowledge by making cases comparable. Statistics will also contribute to a holistic understanding."
    },
    {
        "contexts": "What if single cases are not complex but simple? Many single case studies are very simple. And even the more complicated, some may have an inner working or a narrative. When looking at single case studies, we ought to remember that there can be a causal connection in the world, so there might be some in our case studies as well. Therefore I suggest that the supposably complexity of single cases should not repel us but instead energise and motivate us to find reason and causality within the case.",
        "summary": "Single cases may appear complex but could be simple upon closer examination. Identifying causal connections can offer valuable insights, thus complexity shouldn't deter research efforts.",
        "question": "Why should complexity not deter efforts in single case studies?",
        "ground_truths": "Complexity shouldn't deter efforts because it can energize and motivate researchers to find reason and causality within the case."
    },
    {
        "contexts": "No case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new. The more you stand on the shoulders of giants, the further you can see. We need to realize that science is build on failure and iteration.",
        "summary": "All cases have connections to previous research, and building upon prior knowledge enhances the scope and quality of the current study. Science advances through failure and iteration.",
        "question": "How does standing on the shoulders of giants benefit research?",
        "ground_truths": "It enhances the scope and quality of the current study, allowing researchers to see further by building upon prior knowledge."
    },
    {
        "contexts": "What are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems. We hence acknowledge that this contextualisation is place based, and places have boundaries and borders.",
        "summary": "Linking a case to global data enables better understanding and generalization. Contextualizing research is essential and acknowledges geographical boundaries.",
        "question": "What is the advantage of embedding a case into global information?",
        "ground_truths": "It enables better understanding and allows for generalization of results to other systems."
    },
    {
        "contexts": "In other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social contexts of the case's dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly.",
        "summary": "Identifying injustices involves understanding financial, social, and cultural elements. Statistics and systems analysis can clarify case complexities.",
        "question": "How can statistics contribute to understanding injustices?",
        "ground_truths": "Statistics can offer ways for clear documentation through observation and can partly clarify the complexities of a case."
    },
    {
        "contexts": "While many would connect this to the last point, it may be worthwhile to consider this on its own. Livelihoods are what defines groups of people and their possibility to thrive. While this is equally bound to culture and justice, it repeatedly proves that it is much more than the mere sum of the two. Livelihoods are often characterised by a deeply qualitative manifestation, which is where statistics need be to aware of its limitations.",
        "summary": "Livelihoods, shaped by culture and justice, have qualitative aspects that go beyond quantitative measures. Statistics have limitations in capturing these nuances.",
        "question": "What are the limitations of statistics in understanding livelihoods?",
        "ground_truths": "Statistics have limitations in capturing the deeply qualitative aspects of livelihoods."
    },
    {
        "contexts": "In sustainability science we did not learn to date which information is vital, and which one trivial. This is vastly different in medicine, where much of the canon of knowledge has been consolidated into a system that allows for a clear presentation of singular case studies. Medicine is able to and actually demands such a presentation of knowledge, since much of the previous knowledge is compiled into a system that allows for a clear integration of new knowledge. In sustainability science such an established system is still widely lacking. People talk of complexity and the importance of recognising the individual case, yet I would argue that the world of medicine is equally complex.",
        "summary": "Sustainability science lacks a systematic approach to knowledge integration, unlike medicine, which has a consolidated canon. Despite the complexities involved, medicine has a way to clearly present and integrate new case studies.",
        "question": "Why does medicine have a more systematic approach to integrating knowledge than sustainability science?",
        "ground_truths": "Medicine has a consolidated canon of knowledge that allows for a clear presentation and integration of new case studies, while sustainability science lacks such an established system."
    },
    {
        "contexts": "Chess games have certain openings. Most opening are known, and while some are famous, others are more obscure. Every chess player knows however the main 4-5 opening moves, since they determine the continuation of the game. Around the 8-12 moves however, something remarkable happens. We leave the range of known chess games, and enter a game that has probably never been played on this planet.",
        "summary": "In chess, initial moves are well-known but around moves 8-12, the game transitions into an entirely new setup, potentially never played before.",
        "question": "When does a chess game typically transition into a unique configuration?",
        "ground_truths": "The game typically transitions into a unique configuration around moves 8-12."
    },
    {
        "contexts": "Statistics offer a certain perspective on the world. It makes a difference whether I count something or construct it into another data format. Many patterns follow a linear trend, yet some few may not. Variables can be grouped. When looking at a single case we can look beyond the complexities by looking at the case through the lens of knowledge we have from statistics.",
        "summary": "Statistics provide a lens through which complexities can be understood. Counting and data formats differ, and while most patterns are linear, some are not.",
        "question": "How can statistics help in simplifying complexities?",
        "ground_truths": "Statistics provide a lens that allows for a more structured view of complexities, by categorizing variables and identifying patterns."
    },
    {
        "contexts": "What if single cases are not complex but simple? Many single case studies are very simple. And even the more complicated, some may have an inner working or a narrative. When looking at single case studies, we ought to remember that there can be a causal connection in the world, so there might be some in our case studies as well.",
        "summary": "Single case studies may seem complex but often have a simpler core. It's crucial to remember the possibility of a causal connection.",
        "question": "What should one remember while examining single case studies?",
        "ground_truths": "It's crucial to remember the possibility of a causal connection while examining single case studies."
    },
    {
        "contexts": "No case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new.",
        "summary": "Though some cases like wicked problems may seem novel, their components often resemble previous cases. Novelty usually lies in the combination of familiar building blocks.",
        "question": "How can novelty in case studies often be broken down?",
        "ground_truths": "Novelty in case studies often lies in the unique combination of familiar building blocks."
    },
    {
        "contexts": "Several layers of information are to be considered when linking statistics to natural experiments. Contextualising natural experiments can be structured into at least 4 steps. What are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems.",
        "summary": "Contextualizing natural experiments involves considering connections to other systems, dependencies, and gaps. A wealth of global information allows for the integration of the case into a broader contexts, potentially enabling inferences to be drawn that are applicable elsewhere.",
        "question": "Why is it important to consider global information when contextualizing a natural experiment?",
        "ground_truths": "Considering global information allows for the integration of the case into a broader contexts, potentially enabling inferences to be drawn that are applicable elsewhere."
    },
    {
        "contexts": "In other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social contexts of the case's dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly.",
        "summary": "Identifying the causes of injustices in a place involves examining both tangible factors like resource distribution and intangible elements like cultural perception. Statistics can help in documenting these factors, providing a partial understanding of the case's complexities.",
        "question": "How can statistics help in understanding the roots of injustices?",
        "ground_truths": "Statistics can help in documenting factors such as resource distribution and cultural perception, providing a partial understanding of the case's complexities."
    },
    {
        "contexts": "Experiments in sustainability science are characterised by an intervention and the production of empirical evidence. These two key criteria are essential for experiments in sustainability science, which can be in addition also differentiated into a problem focused and a solution orientated focus. Nevertheless, problem orientated studies with full control over all variables (except for the one(s) being investigated) are still clearly timely in sustainability science, as many experiments conducted in ecology prove. Less control over variables is equally important, and again ecology, agricultural science but also experiments in psychology come to mind.",
        "summary": "Sustainability science experiments are defined by interventions and empirical evidence. They can have a problem-focused or solution-oriented approach. Complete control over variables is important in problem-oriented studies, but limited control is also valuable in other disciplines like ecology, agriculture, and psychology.",
        "question": "What distinguishes a problem-focused study from a solution-oriented study in sustainability science?",
        "ground_truths": "Problem-focused studies have full control over all variables except the one being investigated, while solution-oriented studies may have limited control over variables."
    },
    {
        "contexts": "With a rising number of case studies, there is currently a trend to integrate more inductively designed studies as well into a meta-analytical scheme of analysis. Due to a more diverse study design and publication culture this is often a difficult and work intense endeavour. Still, it is vital to evolve science into an endeavour where more singular cases can be integrated to create meta-analytical knowledge. This is a task for the current generation, since the mode of science needs to be evolved. The knowledge and understanding of singular cases may be to precious to be lost and now integrated to build a deeper understanding.",
        "summary": "An increasing number of case studies are being integrated into meta-analytical frameworks, despite challenges due to diverse study designs. This integration is crucial for evolving scientific methods and ensuring that the knowledge gained from singular cases is not lost but contributes to a deeper understanding.",
        "question": "Why is the integration of diverse study designs into meta-analytical frameworks considered important?",
        "ground_truths": "The integration of diverse study designs into meta-analytical frameworks is crucial for evolving scientific methods and ensuring that the knowledge gained from singular cases is not lost but contributes to a deeper understanding."
    },
    {
        "contexts": "Causality is one of the most misused and misunderstood concepts in statistics. All the while it is at the heart of the fact that all statistics are normative. While many things can be causally linked, other things are not causally linked. The problem is that we dearly want certain things to be causally linked, while we want other things not to be causally linked. This confusion has many roots, and spans across such untameable arenas such as faith, psychology, culture, social constructs and so on.",
        "summary": "Causality is often misunderstood in statistics and has roots in various disciplines like faith and psychology. While people desire certain causal links, they reject others, causing confusion.",
        "question": "What factors contribute to the misunderstanding of causality?",
        "ground_truths": "The misunderstanding of causality is contributed by roots in various disciplines like faith, psychology, culture, and social constructs."
    },
    {
        "contexts": "Let's take the first extreme case, building on the theory that storks bring the babies. Obviously this is not true. Creating a causal link between these two is clearly a mistake. Now lets take the other extreme case, you fall down a flight of stairs, and in the fall break your leg. There is however some form of causal link between these two actions, that is falling down the stairs caused you to break your leg.",
        "summary": "Two extreme examples of causality include the false theory that storks bring babies and the clear causal relationship between falling down stairs and breaking a leg.",
        "question": "What makes an example an extreme case in causality?",
        "ground_truths": "An extreme case in causality is one where the causal link is either clearly false or clearly true."
    },
    {
        "contexts": "Causality is a construction that is happening in our mind. We create an abstract view of the world, and in this abstract view we come of with a version of reality that is simplified enough to explain for instance future events, but it is not too simple, since this would not allow us to explain anything specific or any smaller groups of events.",
        "summary": "Causality is an abstract construct of the mind, simplifying reality enough to explain future events but not so simple that it becomes meaningless.",
        "question": "Why is causality considered a mental construct?",
        "ground_truths": "Causality is considered a mental construct because it is an abstract view of the world created in our mind, used to simplify reality to explain events."
    },
    {
        "contexts": "Statistical correlations imply causality if one variable A is actively driven by a variable B. If B is taken away or changed, A changes as well or becomes non-existent. This relation is among the most abundantly known relations in statistics, but it has certain problems. First and foremost, two variables may be causally linked but the relation may be weak.",
        "summary": "Statistical correlations imply causality when one variable drives another. Despite its popularity, this approach has limitations, such as the potential for weak causal relationships.",
        "question": "What is a limitation of using statistical correlations to establish causality?",
        "ground_truths": "A limitation is that two variables may be causally linked but the relation may be weak."
    },
    {
        "contexts": "Building on the thought that our mind wanders to find causal relations, and then having the scientific experiment as a powerful tool, scientists started deriving and revising theories that were based on the experimental setups. Sometimes it was the other way around, as many theories were only later proven by observation or scientific experiments. Having causality hence explained by scientific theories creates a combination that led to physical laws, societal paradigms and psychological models, among many other things.",
        "summary": "The human quest for causal relations led to scientific experiments that either proved or spawned theories. This interplay between causality and science has led to physical laws and societal paradigms.",
        "question": "How has the quest for understanding causality impacted science?",
        "ground_truths": "The quest for understanding causality has led to scientific experiments that either proved or spawned theories, resulting in physical laws, societal paradigms, and psychological models."
    },
    {
        "contexts": "Temporal causal chains can be defined as relations where an effect has a cause. An event A may directly follow an action B. In other words, A is caused by B. The complex debate on vaccinations and autism can be seen as such an example. Many such temporal relations are hence assumed, but our mind often deceives us, as we want to get fooled.",
        "summary": "Temporal causal chains establish a relationship between cause and effect. For example, the vaccination-autism debate is often misunderstood due to the human mind's inclination to assume causality where there might be none.",
        "question": "Why do humans often misunderstand temporal relations between events?",
        "ground_truths": "The human mind is inclined to assume causality where there might be none."
    },
    {
        "contexts": "Paraphrasing his words, causality is contiguous in space and time, the cause is prior to the effect, and there is a constant union between cause and effect. Hume claims that the same cause always produces the same effect. In statistics, this pinpoints at the criterion of reproducibility, which is one of the backbones of the scientific experiment.",
        "summary": "Hume's criteria for causality include contiguity in space and time, the cause preceding the effect, and consistent outcomes. This aligns with the scientific principle of reproducibility.",
        "question": "What are Hume's criteria for establishing causality?",
        "ground_truths": "Hume's criteria for causality include contiguity in space and time, the cause preceding the effect, and consistent outcomes."
    },
    {
        "contexts": "While a correlation tests a mere relation between two variables, a regression implies a certain theory on why two variables are related. In times of big data there is a growing amount of correlations that are hard to explain. From a theory-driven perspective, this can even be a good thing. After all, many scientific inventions were nothing short of being accidents.",
        "summary": "Correlation evaluates the relationship between variables, while regression suggests a theoretical reason for the relationship. Big data is revealing increasingly complex correlations, some of which challenge existing theories.",
        "question": "What is the difference between correlation and regression?",
        "ground_truths": "Correlation evaluates the relationship between variables, while regression suggests a theoretical reason for the relationship."
    },
    {
        "contexts": "Modern medicine is at the forefront of this trend, as medicine had a long tradition to have the cure in the focus of their efforts, instead of being able to explain all patterns. Modern genetics correlate specific gene sequences with potential diseases, which can be helpful, but also creates a burden or even harm.",
        "summary": "Modern medicine focuses on cures more than explaining patterns. The correlation of gene sequences with diseases offers advantages but also poses ethical issues.",
        "question": "Why can the correlation of gene sequences with diseases be both helpful and harmful?",
        "ground_truths": "The correlation of gene sequences with diseases offers advantages but also poses ethical issues."
    },
    {
        "contexts": "Care is necessary when considering interpolation or extrapolation, as validity decreases when we dwell beyond the data we have. It takes experience to know when prediction is possible, and when it is dangerous. We need to consider if our theoretical assumptions can be reasonable expanded beyond the data of our sample. Our statistical model may be less applicable outside or our data range. Mitigating or interacting factors may become more relevant in the space outside of our sample range.",
        "summary": "Interpolation and extrapolation must be approached cautiously. Their validity decreases as they move beyond the data range and may be affected by assumptions and external factors.",
        "question": "What factors affect the validity of interpolation and extrapolation?",
        "ground_truths": "Their validity decreases as they move beyond the data range and may be affected by assumptions and external factors."
    },
    {
        "contexts": "Probability is one of the most important concepts in modern statistics. The question whether a relation between two variables is purely by chance, or following a pattern with a certain probability is the basis of all probability statistics. In the case of linear relation, another quantification is of central relevance, namely the question how much variance is explained by the model. Between these two numbers - the amount of variance explained by a linear model, and the fact that two variables are not randomly related - are related at least to some amount. If a model is highly significant, it typically shows a high R2 value.",
        "summary": "In modern statistics, the role of probability is central to understanding whether a relationship between two variables is by chance or patterned. For linear relationships, the model's R2 value and p-value are significant metrics, and both are influenced by the sample size.",
        "question": "How does sample size influence the significance metrics of a linear model?",
        "ground_truths": "The sample size influences both the p-value and the R2 value in a linear model. A smaller sample size may produce a high R2 value but may not be statistically significant, as evidenced by the p-value."
    },
    {
        "contexts": "All models are wrong, some models are useful. This famous sentence from Box cannot be highlighted enough when thinking about models. In the contexts of residuals it comes in handy once more. Residuals are the sum of the deviance the individual data points have from a perfectly fitted model. This is relevant for two things. First, the model assumes a perfect model. Second, reality bags to differ.",
        "summary": "The phrase 'All models are wrong, some models are useful' is particularly relevant when discussing residuals in statistical models. Residuals quantify the deviation of data points from a model, indicating the model's limitations.",
        "question": "Why are residuals important in statistical models?",
        "ground_truths": "Residuals are important because they quantify the deviation of data points from the model, essentially highlighting the limitations or inaccuracies of the model."
    },
    {
        "contexts": "The short answer: Mostly yes, otherwise not. The long answer: The world is more linear than you think it is. I believe that many people are puzzled by the complexity of the world, while indeed many phenomena as well as the associated underlying laws are rather simple. There are many phenomena that are linear, and this is worth noticing. People today often think that the world is non-linear.",
        "summary": "While people often perceive the world as complex and non-linear, many phenomena actually follow simple, linear patterns or laws.",
        "question": "What is the common perception about the linearity of the world?",
        "ground_truths": "The common perception is that the world is complex and non-linear, whereas in reality, many phenomena are actually linear."
    },
    {
        "contexts": "A check-in at the beginning of each meeting, workshop or seminar can help teams and groups to get a better understanding of each other's energy level, personal environment or the status of work. You can basically use it with every group of people, even your clique or your family. The check-out helps to summarize decisions, distributed tasks and supports evaluating the atmosphere in the group. Both the check-in and check-out can become a habit for each meeting.",
        "summary": "A check-in at meetings helps understand each other's energy and work status, useful for any group. A check-out summarizes decisions and evaluates group atmosphere. Both can become a meeting habit.",
        "question": "Why should check-ins and check-outs be part of meetings?",
        "ground_truths": "Check-ins and check-outs should be part of meetings to help teams understand each other's energy levels and work status. Check-outs further help in summarizing decisions and evaluating the group atmosphere. They can be a regular practice for all kinds of groups."
    },
    {
        "contexts": "It may happen that - especially in meetings in which the members do not know each other well - the members might not want to share their very personal stories or want to be honest about their current feelings. It can help that you as a moderator start the round and share a private insight if you like. Additionally, you should mention that the members should only share what they feel comfortable with.",
        "summary": "In meetings where members are unfamiliar, they may hesitate to share personal feelings. A moderator starting with personal insight can help. Members should share only what they're comfortable with.",
        "question": "How can a moderator encourage participation in check-ins?",
        "ground_truths": "A moderator can encourage participation in check-ins by initiating the sharing session with their own personal insights. This sets the tone and creates a more open environment, allowing members to feel more comfortable sharing their own thoughts or feelings."
    },
    {
        "contexts": "Using correct citation is crucial for academic writing and the basis of all scholarly work. Therefore it is most important that you understand and apply the following rules. Whenever you research a topic, you will read other peoples books and articles and use the information found there for your own interpretation of the subject. Therefore you need to make sure to respect others intellectual property and do not violate copyright laws. Moreover, your statements become more credible if you can show that someone else has had similar results.",
        "summary": "Correct citation is vital in academic writing to respect intellectual property and boost credibility. It's essential to apply specific rules when using others' works for your research.",
        "question": "Why is proper citation important in academic writing?",
        "ground_truths": "Proper citation is important in academic writing to respect intellectual property and make your statements more credible."
    },
    {
        "contexts": "One of the difficulties of correct citation is that there are innumerable ways of doing it. Nearly every academic discipline and country has their own preference. The most important aspect here is that you stay consistent and do not mix several methods within one text (you can always ask your lecturers for their preferred style). There are two prominent ways of referencing you can use within your text: In-Text-Citation and Footnotes. By using this method the reader can keep his attention on the main text without having to skip to the bottom of the page for references.",
        "summary": "Correct citation varies across disciplines and countries, but consistency in method is crucial. Two main methods are In-Text-Citation and Footnotes.",
        "question": "What are the main methods for citation in academic works?",
        "ground_truths": "The main methods for citation in academic works are In-Text-Citation and Footnotes."
    },
    {
        "contexts": "Today, Citizen Science is frequently used in biology, conservation and ecology (topics include e.g. birds, insects or water quality), followed by geographic information research and epidemiology (1, 2, 6, 8). It has also found other fields, including Astronomy, Agriculture, Arts and Education (see for example [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6410884/ this agricultural study] and [https://www.aavso.org/variable-stars-main this participatory approach] in Astronomy.) Its presence in scientific publications has increased significantly since the early 2000s, which may be attributed to an increased usage of digital platforms for citizen engagement (1).",
        "summary": "Citizen Science is predominantly used in biology, conservation, and ecology but also appears in fields like Astronomy, Agriculture, Arts, and Education. Its visibility in scientific publications has risen significantly since the early 2000s, possibly due to increased digital engagement.",
        "question": "What might be the reason for the increased visibility of Citizen Science in scientific publications?",
        "ground_truths": "The increased visibility of Citizen Science in scientific publications may be attributed to increased usage of digital platforms for citizen engagement."
    },
    {
        "contexts": "Citizen Science's unique characteristic is that it enables researchers to sample data across different scales - from regional up to global spatial scales - as well as repeated measurements spanning several years. This allows a scaling of data based on contribution of many participants, allowing a broader and more holistic perspective that would be not possible solemnly on the workload of the researchers. Bird-watching may take place in gardens of a smaller area for one season while the method can likewise integrate geographic information upscaled to a global scale over several years. Often, the collected data is numeric, e.g. number of birds counted, and at which date. This allows to accumulate larger observations about bird migration.",
        "summary": "Citizen Science enables data collection across various scales, from regional to global, and over extended periods. This makes it possible to generate a broader perspective that a single researcher's workload could not achieve. Bird-watching serves as an example.",
        "question": "How does Citizen Science provide a broader perspective than traditional methods?",
        "ground_truths": "Citizen Science enables data collection across various scales, from regional to global, and over extended periods, allowing a broader perspective that would not be possible based solely on the workload of researchers."
    },
    {
        "contexts": "\"Citizen Science (...) has over the past decade become a very influential and widely discussed concept with many scientists and commentators seeing it as the future of genuine interactive and inclusive science engagement (...)\" (Riesch & Potter 2014, p.107). As new areas of academia develop participatory research designs, the potential of these approaches are becoming more and more visible (8). Therefore, after years of development and despite a lack of definite conceptualization, Citizen Science should be taken seriously and supported from more areas within academia (8).",
        "summary": "Over the past decade, Citizen Science has gained prominence as the future of interactive and inclusive science engagement. Despite lacking a definitive concept, it is increasingly recognized and supported in academia.",
        "question": "Why should Citizen Science be taken seriously in academia?",
        "ground_truths": "Citizen Science should be taken seriously in academia because it has become a very influential and widely discussed concept seen as the future of genuine interactive and inclusive science engagement."
    },
    {
        "contexts": "A Code of Conduct provides clear Statements on how individuals want to work together in a specific organization. The Goal of the Code of Conduct can be summarized under the following three aspects: (1) Unity (2) Standards (3) Direction. Unity: Certain ethical rules should be abided by everyone (e.g. letting everyone be heard, respecting each others arguments, being accountable). Standards: It should be clarified what the ethical standards and procedures within an organization should be in order to provide every individual with explicit procedures on how to behave. Direction: Should provide each individual about the purposes and goals of the organization to provide guidance on each respective role within that collective endeavor.",
        "summary": "A Code of Conduct has three main goals: Unity, Standards, and Direction. Unity entails ethical rules for all; Standards refer to explicit behavioral guidelines, and Direction offers purpose and role clarity for individuals in the organization.",
        "question": "What are the three main goals of a Code of Conduct?",
        "ground_truths": "The three main goals of a Code of Conduct are Unity, Standards, and Direction."
    },
    {
        "contexts": "All groups are sooner or later riddled by challenges when collaborating as a group. Diverse ideas, different backgrounds and experience, reliability and splitting of the workload, giving each other critical feedback and emotional support are mentioned when people perceive group work to be positive. However, these experiences can be flipped into losing time to gain coherence, endless feedback loops, lack of trust, unreliable work ethics, loss of energy due to repetitions, and the whole array of negative emotions we humans are capable of.",
        "summary": "Group collaborations are a mix of positives like diversity and workload sharing, but they also come with challenges such as lack of trust and energy loss due to redundancy.",
        "question": "What are some pros and cons of group collaboration?",
        "ground_truths": "The pros of group collaboration include diversity, workload sharing, and emotional support. The cons involve a lack of trust, energy loss due to redundancy, and negative emotions."
    },
    {
        "contexts": "The term 'cohort' historically refers to a 300-600 man unit in the Roman army (3). It found its way into scientific research thanks to Wade Hampton Frost, a medical doctor from the US, who invented the Cohort study method after his pioneering work in the field of epidemiology in the early 20th Century. In a study that was published after his death in 1938, he focused on the mortality rates of tuberculosis patients in Massachusetts and - presumably for the first time - assessed not only differences between age groups in any given year, but also in the development of each cohort over time. The method and the term 'cohort' were subsequently adapted by the social sciences, mostly focusing on the role of 'generations' in historical developments as well as in demography (4). More appropriate synonyms of the cohort study include incidence, longitudinal, forward-looking, follow-up, concurrent or prospective studies (3).",
        "summary": "The term 'cohort' originally referred to a unit in the Roman army and was adapted for scientific research by Wade Hampton Frost. He pioneered the cohort study method in epidemiology, focusing on mortality rates of tuberculosis patients. The concept has also been adapted by social sciences.",
        "question": "Who first applied the term 'cohort' to scientific research?",
        "ground_truths": "Wade Hampton Frost first applied the term 'cohort' to scientific research."
    },
    {
        "contexts": "A cohort study tracks two or more groups forward from exposure to outcome. In its simplest form, a cohort study compares the experience of a group exposed to some factor with another group not exposed to the factor. If the former group has a higher or lower frequency of an outcome than the unexposed, then an association between exposure and outcome is evident. The exposure must be clearly defined, with the option of defining several degrees of exposure (e.g. light and heavy smokers compared to non-smokers) (3). The cohorts may be tracked over years, but also over decades, depending on the research question.",
        "summary": "A cohort study compares groups based on exposure to certain factors, tracking them over time. The aim is to identify any association between the exposure and the outcomes, which may vary in frequency and intensity.",
        "question": "What is the primary aim of a cohort study?",
        "ground_truths": "The primary aim of a cohort study is to identify any association between exposure to certain factors and the outcomes."
    },
    {
        "contexts": "Cohort studies are '(...) the best way to identify incidence and natural history of a disease, and can be used to examine multiple outcomes after a single exposure' (3, p.341). In addition, they are useful when studying rare exposures that happen in limited numbers of environments (e.g. a factory). They can reduce the risk of survivor bias when studying diseases that are rapidly fatal, and they allow for the calculation of incidence rates, relative risks, and confidence intervals (3, p.342).",
        "summary": "Cohort studies excel at identifying the incidence and natural history of diseases. They are particularly useful for studying rare exposures and can minimize survivor bias while allowing for calculations like incidence rates and relative risks.",
        "question": "Why are cohort studies useful for studying rare exposures?",
        "ground_truths": "Cohort studies are useful for studying rare exposures because they can happen in limited numbers of environments, such as a factory."
    },
    {
        "contexts": "However, Cohort Studies are less useful for diseases and outcomes that are very rare or take a long time to develop (3). Also, loss to follow-up can be an issue with prospective Cohort Studies, especially for longitudinal studies that span over years or decades. In addition, the exposure status of the subjects may change over time (3).",
        "summary": "Cohort studies have limitations in studying very rare diseases and outcomes that take a long time to manifest. They may also suffer from issues like loss to follow-up and changing exposure statuses over time.",
        "question": "What are some challenges of longitudinal cohort studies?",
        "ground_truths": "Some challenges of longitudinal cohort studies include loss to follow-up and the potential for changing exposure statuses over time."
    },
    {
        "contexts": "Concept Maps are a form of visually organizing information for groups and individuals. Conceptual terms or short statements are presented in boxes or bubbles and interlinked with other terms through commented arrows, resulting in a hierarchical network-like structure that provides an overview on a topic.",
        "summary": "Concept Maps visually organize information in a hierarchical network, using boxes or bubbles for terms and commented arrows for links.",
        "question": "What is the structural basis of Concept Maps?",
        "ground_truths": "The structural basis of Concept Maps is a hierarchical network where terms are in boxes or bubbles and are linked with commented arrows."
    },
    {
        "contexts": "Concept Maps result from the work of Joseph Novak and colleagues at Cornell University in the 1970s in the field of education, and have since been applied in this area and beyond. The original underlying idea was to analyze how children learn, assuming that they do so by assimilating new concepts and positioning these in a cognitive conceptual framework. The idea of Concept Maps emerged from the demand for a form of assessing these conceptual understandings.",
        "summary": "Concept Maps originated from Joseph Novak's work at Cornell in the 1970s to understand children's learning by assessing their cognitive conceptual framework.",
        "question": "Who pioneered the development of Concept Maps and for what purpose?",
        "ground_truths": "Joseph Novak and his colleagues at Cornell University pioneered the development of Concept Maps to analyze how children learn by assessing their cognitive conceptual frameworks."
    },
    {
        "contexts": "Importantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of Group Concept Mapping. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a Mindmap in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.",
        "summary": "Concept Maps are used in various fields but differ from Group Concept Mapping and Mindmaps in structure and application.",
        "question": "How do Concept Maps differ from Group Concept Mapping and Mindmaps?",
        "ground_truths": "Concept Maps differ from Group Concept Mapping in that they are not part of a more structured, multi-step process. They differ from Mindmaps because Mindmaps are spontaneous, unstructured, and lack a hierarchy or linguistic homogeneity."
    },
    {
        "contexts": "Concept Maps help identify a current state of knowledge and show gaps within this knowledge, e.g. a lack of understanding on which elements are of importance to a question or topic, and how concepts are interrelated. Identifying these gaps helps fill knowledge gaps. Therefore, they can be a helpful tool for students or anyone learning a new topic to monitor one's own progress and understanding of the topic, and how this changes as learning units continue.",
        "summary": "Concept Maps reveal the current state of knowledge and knowledge gaps, aiding students and learners in tracking progress.",
        "question": "Why are Concept Maps useful for students and learners?",
        "ground_truths": "Concept Maps are useful for students and learners because they help identify the current state of knowledge and knowledge gaps, thereby aiding in tracking one's learning progress."
    },
    {
        "contexts": "Conceptual figures - or \"made-up figures\", as we like to call them - are visualisations that do not represent data itself, but help illustrate a theoretical idea or a methodological design. They are often relevant for scientific publications, and can help improve presentations and pitches by making an idea or approach more understandable for the audience.",
        "summary": "Conceptual figures are visualizations designed to clarify theoretical ideas or methodological designs. They are crucial for scientific publications and can enhance presentations.",
        "question": "Why are conceptual figures important?",
        "ground_truths": "Conceptual figures are important because they make theoretical ideas or methodological designs more understandable. They are crucial for scientific publications and can enhance presentations."
    },
    {
        "contexts": "It should include as much information as necessary, but as little as possible. Useless information that the figure does not benefit from should be avoided. Also, the information provided in the figure should be complementary to the text, not (entirely) identical, which would render the figure itself redundant. Good figures help us better understand the text that we read already, but do not take up space without providing anything new.",
        "summary": "Conceptual figures should contain essential information and avoid redundancy with the text. They should complement the text and add value.",
        "question": "What should a good conceptual figure achieve?",
        "ground_truths": "A good conceptual figure should contain only essential information and should add value by complementing the text. It should avoid redundancy and provide new understanding."
    },
    {
        "contexts": "A figure with a lot of complex information usually overwhelms the reader. You want your figure to stimulate further engagement with what it shows. A good figure is graspable at first sight, at least in terms of the core elements. This also means that there should usually be no more than 3 - 5 five core elements within the figure, although there can be accompanying smaller elements around those.",
        "summary": "Figures should be easy to understand at first glance, containing 3-5 core elements to engage the reader without overwhelming them.",
        "question": "How many core elements should a good conceptual figure have?",
        "ground_truths": "A good conceptual figure should have no more than 3-5 core elements."
    },
    {
        "contexts": "Fonts are certainly a matter of taste, but some fonts are just more adequate for scientific publications or presentations than others. The text should be readable at first glance (see 2), so choose a font that is easy to decipher and ideally fits to the rest of the document. Colors, then, should first make a figure more visually appealing. This can be done by using colors that fit together, for example by relying on a Wes-Anderson-Color-Palette, which is obviously the best set of colors.",
        "summary": "Select readable fonts and a harmonious color palette to make figures more visually appealing and fit the document.",
        "question": "What elements make a figure visually appealing?",
        "ground_truths": "Readable fonts and a harmonious color palette make a figure more visually appealing."
    },
    {
        "contexts": "Decide on one software solution to implement your figure in, and learn what it can do. Acknowledge the expectations of your audience, or deliberately decide to break with their expectations when appropriate. Always keep in mind how your figure relates to the character and content of the document it is placed in. Follow your intuition: we all have our own ideas about what looks good, and we do our best work when we are convinced of what we are doing ourselves. Last but not least, however, get feedback from peers when in doubt.",
        "summary": "Choose a single software, consider your audience's expectations, and ensure the figure fits the document's character. Follow your intuition but seek peer feedback.",
        "question": "Why is feedback important in figure design?",
        "ground_truths": "Feedback is important in figure design to overcome tunnel vision and ensure the figure is understandable and effective."
    },
    
    {
        "contexts": "Early approaches to text analysis can be found in Hermeneutics, but quantitative Content Analysis itself emerged during the 1930s based on the sociological work of Lazarsfeld and Lasswell in the USA (4). The method prospered in the study of mass communications in the 1950s (2). Starting in the 1960s, the method was subsequently adapted to the specific needs of diverse disciplines, with the Qualitative Content Analysis emerging from a critique on the lack of communicative contextualization in the earlier purely quantitative approaches (4). Today, the method is applied in a wide range of social sciences and humanities, including communicative science, anthropology, political sciences, psychology, education, and literature, among others (1, 2).",
        "summary": "Quantitative Content Analysis, a method for text analysis, originated in the 1930s from the sociological work of Lazarsfeld and Lasswell in the USA. It was widely used in mass communications studies in the 1950s and adapted to various disciplines in the 1960s. Today, it's used in numerous social sciences and humanities fields.",
        "question": "When and where did the quantitative Content Analysis method originate?",
        "ground_truths": "The quantitative Content Analysis method originated in the 1930s in the USA."
    },
    {
        "contexts": "Content Analysis is a \"(...) systematic, replicable technique for compressing many words of text into fewer content categories based on explicit rules of coding\" (Stemler 2000, p.1). However, the method entails more than mere word-counting. Instead, Content Analysis relies on the interpretation of the data on behalf of the researcher. The mostly qualitative data material is assessed by creating a category system relevant to the material, and attributing parts of the content to individual categories (Schreier 2014). Not only the content of a source is evaluated, but also formal aspects as well as contextual psychological, institutional, and cultural elements of the communication process (1, 4). \"[Content Analysis] seeks to analyze data within a specific contexts in view of the meanings someone - a group or a culture - attributes to them.\" (Krippendorff 1989, p.403). Because of this, Content Analysis is a potent method to identify trends and patterns in (text) sources, to determine authorship, or to monitor (public) opinions on a specific topic (3).",
        "summary": "Content Analysis is a systematic technique for reducing text into fewer content categories based on coding rules. It involves interpreting data, creating a category system, and attributing content to these categories. It evaluates not just content, but also formal aspects and contextual elements of the communication process. It's used to identify trends, determine authorship, or monitor opinions.",
        "question": "What is the purpose of Content Analysis and what does it evaluate?",
        "ground_truths": "The purpose of Content Analysis is to reduce text into fewer content categories based on coding rules, interpret data, and attribute content to these categories. It evaluates not just the content, but also formal aspects and contextual elements of the communication process."
    },
    {
        "contexts": "Apart from text, a diverse set of data can be analyzed using Content Analysis. \"Anything that occurs in sufficient numbers and has reasonably stable meanings for a specific group of people may be subjected to content analysis.\" (Krippendorff 1989, p.404). The data must convey a message to the receiver and be durable (2, 3). Often, Content Analysis focuses on data that are difficult or impossible to interpret with other methods (3). The data may exist 'naturally' and be publicly available, for example verbal discourse, written documents, or visual representations from mass media (newspapers, books, films, comics etc.); or be rather unavailable to the public, such as personal letters or witness accounts. The data may also be generated for the research purpose (e.g. interview transcripts) (1, 2, 4).",
        "summary": "Content Analysis can be used on a diverse set of data that conveys a message and is durable. It's often used on data that's hard to interpret with other methods. The data can be naturally existing and publicly available, like mass media, or not publicly available, like personal letters. It can also be generated for research purposes.",
        "question": "What types of data can be analyzed using Content Analysis?",
        "ground_truths": "Content Analysis can be used on a diverse set of data that conveys a message and is durable. This includes naturally existing and publicly available data like mass media, not publicly available data like personal letters, and data generated for research purposes."
    },
    {
        "contexts": "Any Content Analysis starts with the ''Design Phase'' in which the research questions are defined, the potential sources are gathered, and the analytical constructs are established that connect the prospective data to the general target of the analysis. These constructs can be based on existing theories or practices, the experience and knowledge of experts, or previous research (2). Next, the ''Unitizing'' is done, i.e. the definition of analysis units. It may be distinguished between sampling units (= sources of data, e.g. newspaper articles, interviews) and analysis units (= units of data that are coded and analyzed, e.g. single words or broader messages), with different approaches to identifying both (see Krippendorff 2004). Also, the ''sampling'' method is determined and the sample is drawn. Then, the ''Coding Scheme'' - or 'Codebook' - is developed and the data are coded. 'Coding' generally describes the transfer of the available data into more abstract groups. A code is a label that represents a group of words that share a similar meaning (3). Codes may also be grouped into categories if they thematically belong together.",
        "summary": "Content Analysis begins with the Design Phase where research questions are defined, sources gathered, and analytical constructs established. This is followed by Unitizing, where analysis units are defined and distinguished between sampling units and analysis units. The sampling method is then determined and the sample drawn. Finally, the Coding Scheme is developed and the data are coded into more abstract groups.",
        "question": "What are the main steps in the process of Content Analysis?",
        "ground_truths": "The main steps in the process of Content Analysis are the Design Phase, Unitizing, determining the sampling method and drawing the sample, and developing the Coding Scheme and coding the data."
    },
    {
        "contexts": "With the coding scheme at hand, the researcher reads (repeatedly) through the data material and assigns each analysis unit to one of the codes. The ''coding process'' may be conducted by humans, or - if sufficiently explicit coding instructions are possible - by a computer. In order to provide reliability, the codes should be intersubjective, i.e. every researcher should be able to code similarly, which is why the codes must be exhaustive in terms of the overarching construct, mutually exclusive, clearly defined, have unambiguous examples as well as exclusion criteria. Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural contexts of the message, or on the message's effect.",
        "summary": "Once the coding scheme is developed, the researcher assigns each analysis unit to a code. This process can be done by humans or computers. To ensure reliability, codes should be intersubjective, exhaustive, mutually exclusive, clearly defined, and have unambiguous examples and exclusion criteria. Finally, the coded data are analyzed and interpreted, with inferences drawn focusing on various aspects of the message.",
        "question": "How is the coding process conducted in Content Analysis?",
        "ground_truths": "In Content Analysis, the coding process is conducted by assigning each analysis unit to a code, which can be done by humans or computers. The codes should be intersubjective, exhaustive, mutually exclusive, clearly defined, and have unambiguous examples and exclusion criteria to ensure reliability. The coded data are then analyzed and interpreted."
    },
    {
        "contexts": "Qualitative Content Analysis is a rather inductive process. The process is guided by the research questions - hypotheses may be tested, but this is not the main goal. As with much of qualitative research, this method attempts to understand the (subjective) meaning behind the analyzed data and to draw conclusions from there. Its purpose is to get hold of the 'bigger picture', including the communicative contexts surrounding the genesis of the data. The qualitative Content Analysis is a very iterative process. The coding scheme is often not determined prior to the coding process. Instead, its development is guided by the research questions and done in close contact to the data, e.g. by reading through all data first and identifying relevant themes. The codes are then re-defined iteratively as the researcher applies them to the data.",
        "summary": "Qualitative Content Analysis is an inductive process guided by research questions. It aims to understand the meaning behind the data and draw conclusions. The process is iterative, with the coding scheme often not determined prior to coding. Instead, it's developed in close contact with the data, with codes re-defined iteratively as they're applied to the data.",
        "question": "What is the main goal of Qualitative Content Analysis?",
        "ground_truths": "The main goal of Qualitative Content Analysis is to understand the meaning behind the analyzed data and draw conclusions from it."
    },
    {
        "contexts": "Content Analysis counteracts biases because it \"(...) assures not only that all units of analysis receive equal treatment, whether they are entered at the beginning or at the end of an analysis but also that the process is objective in that it does not matter who performs the analysis or where and when.\" (see Normativity) (Krippendorff 1989, p.404). In this case, 'objective' refers to 'intersubjective'. Yet, biases cannot be entirely prevented, e.g. in the sampling process, the development of the coding scheme, or the interpretation and evaluation of the coded data. Content Analysis allows for researchers to apply their own social-scientific constructs \"by which texts may become meaningful in ways that a culture may not be aware of.\" (Krippendorff 1989, p.404) However, especially in case of qualitative analysis of smaller data sample sizes, the theory and hypotheses derived from data cannot be generalized beyond this data (1). Triangulation, i.e. the comparison of the findings to other knowledge on the same topic, may provide more validity to the conclusions (see Normativity).",
        "summary": "Content Analysis helps counteract biases by ensuring equal treatment of all units of analysis and maintaining objectivity. However, biases can still occur in the sampling process, development of the coding scheme, and interpretation of data. Researchers can apply their own social-scientific constructs in Content Analysis. In qualitative analysis of smaller data samples, theories and hypotheses cannot be generalized beyond the data. Triangulation can provide more validity to the conclusions.",
        "question": "What are the strengths and challenges of Content Analysis?",
        "ground_truths": "Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples."
    },
    {
        "contexts": "The usage of automated coding with the use of computers may be seen as one important future direction of the method (1, 5). To date, the human interpretation of ambiguous language imposes a high validity of the results which cannot (yet) be provided by a computer. Alas, the development of an appropriate algorithm and text recognition software pose a challenge. The meaning of words changes in different contexts and several expressions may mean the same. Especially in terms of qualitative analyses, this currently makes human coders indispensable. Yet, the emergence of big data, Artificial Intelligence and Machine Learning might make it possible in the foreseeable future to use automated coding more regularly and with a high validity. Another relevant direction is the shift from text to visual and audio data as a primary form of data (5). With ever-increasing amounts of pictures, video and audio files on the internet, future studies may refer to these kinds of sources more often.",
        "summary": "Automated coding using computers is a potential future direction for Content Analysis, but the development of suitable algorithms and text recognition software is challenging. The meaning of words can change in different contexts, making human coders currently indispensable for qualitative analyses. However, the rise of big data, AI, and Machine Learning could enable more regular use of automated coding. There is also a shift towards using visual and audio data as primary data sources.",
        "question": "What are the potential future directions for Content Analysis?",
        "ground_truths": "Potential future directions for Content Analysis include the use of automated coding with computers, enabled by the development of suitable algorithms and text recognition software, and the rise of big data, AI, and Machine Learning. There is also a shift towards using visual and audio data as primary data sources."
    },
    {
        "contexts": "Sometimes, changes beyond our own control can impact our mental well-being to the extent that we are unable to cope with it just by ourselves. Many of us have never been taught how to cope with mental hardship, and societal norms of individualism and toughness can make it hard to acknowledge that at some point, we can simply not carry on with our lives using our usual coping mechanisms. The pandemic amplified these issues manyfold, and we know how many students are currently struggling with their mental health, especially when university, jobs and life are still setting high demands on our ability to \"function\". We know this is for many parts a societal problem. Nevertheless, if you feel depressed, anxious or overwhelmed at any one moment, you have little options but to start with yourself. This is why we want to provide this article: as a means to help you get through difficult times.",
        "summary": "Uncontrollable changes can negatively affect our mental health, making it difficult to cope. Many people are not equipped to handle mental hardship, and societal norms often discourage seeking help. The pandemic has exacerbated these issues, particularly for students juggling university, jobs, and life. Despite these societal issues, individuals feeling depressed, anxious, or overwhelmed must start with themselves to seek help.",
        "question": "Why is it important for individuals struggling with mental health to start with themselves in seeking help?",
        "ground_truths": "Despite societal issues contributing to mental health struggles, individuals must start with themselves to seek help because they are the ones experiencing the feelings of depression, anxiety, or overwhelm. They are the ones who can best identify their feelings and seek appropriate help."
    },
    {
        "contexts": "Getting professional help is by no means easy, but it is very possible. Below, we tried to draw an overview on how this can work. The truth is that the medical system is messy and reality might be a little different. Still, sticking to some form of structured process should help you in getting professional support. Here's an overview:",
        "summary": "Obtaining professional help for mental health issues can be challenging due to the complexity of the medical system. However, adhering to a structured process can aid in navigating this system and securing the necessary support.",
        "question": "How can a structured process assist in obtaining professional help for mental health issues?",
        "ground_truths": "A structured process can assist in obtaining professional help for mental health issues by providing a clear pathway to navigate the complex medical system and secure the necessary support."
    },
    {
        "contexts": "The first thing besides all the technical and organisational stuff is finding a way to think about this that doesn't make you feel worse about your situation than it needs to. For me personally, it was really helpful to understand that **psychological problems are an illness that we bear no more personal responsibility for than for a cold - possibly a lot less**. Before looking for a therapist, I started reading and watching some content about the topic which helped me to somewhat normalise the situation.",
        "summary": "Before seeking professional help, it's important to understand that psychological problems are an illness, not a personal failing. Consuming content about mental health can help normalize the situation and reduce feelings of guilt or shame.",
        "question": "Why is it important to understand that psychological problems are an illness and not a personal failing?",
        "ground_truths": "Understanding that psychological problems are an illness and not a personal failing is important because it can help reduce feelings of guilt or shame, normalize the situation, and encourage individuals to seek professional help."
    },
    {
        "contexts": "If you're a ''student'' in L\u00fcneburg, you have the right to get counselling from the psychological counsel of the Studierendenwerk. They offer open walk-ins which you can attend rather spontaneously when calling upfront, and up to 8 sessions of individual counselling. Note, however, that they cannot provide a diagnosis or therapy and are therefore no full substitute for a therapist. Still, they can confidentially listen to what you have to say and are struggling with, and can support you in making decisions on the next steps. Unfortunately, the demand is usually very high, so it is not necessarily faster to get an appointment here than with a professional therapist.",
        "summary": "Students in L\u00fcneburg can access counselling from the Studierendenwerk's psychological counsel. They offer walk-ins and up to 8 individual counselling sessions. However, they can't provide diagnosis or therapy, but they can listen and help with decision-making. The demand is high, so getting an appointment may not be faster than with a professional therapist.",
        "question": "What services does the psychological counsel of the Studierendenwerk offer to students in L\u00fcneburg?",
        "ground_truths": "The psychological counsel of the Studierendenwerk offers walk-ins and up to 8 individual counselling sessions to students in L\u00fcneburg. They can listen to students' concerns and help with decision-making, but they can't provide diagnosis or therapy."
    },
    {
        "contexts": "Your doctor (\u201dHausarzt\u201d) is typically mentioned as one of the first points of contact in usual therapy guidance. If you have one and sufficiently trust him or her, you can absolutely make an appointment and talk things through with them. They will also be able to hand out a recipee which can speed up the process of finding a therapist, and may forward you to other specialists e.g. for medication. However, it is not necessary to immediately see a \u201cnormal\u201d doctor. At some point you will have to meet one since they are required to ensure that your mental illness is not caused by any somatic problems (such as a lack of vitamins or neurological problems).",
        "summary": "Your doctor is often the first point of contact for therapy guidance. They can provide a referral to speed up the process of finding a therapist and may refer you to other specialists. However, it's not necessary to see a doctor immediately. At some point, you'll need to see one to rule out physical causes for your mental illness.",
        "question": "Why might someone consult their doctor when seeking therapy for mental health issues?",
        "ground_truths": "Someone might consult their doctor when seeking therapy for mental health issues because the doctor can provide a referral to speed up the process of finding a therapist, refer them to other specialists, and rule out physical causes for their mental illness."
    },
    {
        "contexts": "Basically, finding a therapist is like finding any other specialist medical practitioner: you look them up, you call them, you make an appointment. The only problem is that the supply of therapy slots is very thin, and there\u2019s no good system in place that makes it easy to find open slots. So you kind of have to just call a lot of therapists and find out whether or not they have a slot available.",
        "summary": "Finding a therapist involves looking them up, calling them, and making an appointment, similar to finding any other specialist. However, the availability of therapy slots is limited and there's no efficient system to find open slots, so you may need to call many therapists to find an available slot.",
        "question": "What challenges might someone face when trying to find a therapist?",
        "ground_truths": "Someone trying to find a therapist might face challenges such as limited availability of therapy slots and the lack of an efficient system to find open slots, requiring them to call many therapists to find an available slot."
    },
    {
        "contexts": "You have the right to five preliminary sessions with each therapist. This is on the one hand useful to get a first diagnosis, and on the other to see if you can confide in the person you\u2019re talking to - recall that one of the most important factors for successful therapy is the relationship between therapist and client. Many therapists will offer you preliminary sessions, but don\u2019t have a full therapy slot available. Therefore, don\u2019t be tempted to do too many preliminary sessions with different therapists and ask beforehand if there\u2019s a realistic chance to get a full treatment within a reasonable timeframe.",
        "summary": "You are entitled to five initial sessions with each therapist, which can help in getting a diagnosis and assessing the therapist-client relationship. However, many therapists may not have a full therapy slot available, so it's important not to overdo preliminary sessions and to inquire about the possibility of full treatment.",
        "question": "Why are preliminary sessions with therapists important?",
        "ground_truths": "Preliminary sessions with therapists are important for getting a first diagnosis and to see if you can confide in the therapist, as the relationship between therapist and client is a crucial factor for successful therapy."
    },
    {
        "contexts": "As we stated above, there are a few things that can help you to lift yourself out of the worst phases. Here\u2019s what we found helpful. Meditation can help you get out of negative thought spirals and liften your mood. If you haven\u2019t tried meditation yet, it might be easiest to download an app or listen to guided meditations on YouTube. Apps we tried out and found helpful are Headspace and 10% Happier. You can try them for free!",
        "summary": "There are several practices that can help improve your mood during difficult times. Meditation, for instance, can help break negative thought cycles. If you're new to meditation, consider using apps like Headspace or 10% Happier, or guided meditations on YouTube.",
        "question": "How can meditation help during difficult times?",
        "ground_truths": "Meditation can help lift your mood and get you out of negative thought spirals during difficult times."
    },
    {
        "contexts": "An appreciation journal might also be helpful in lightening your mood. It\u2019s rather trivial: just take a few minutes everyday to write down what you\u2019re thankful for. This might feel awkward, or stupid, and you might find it hard to find things you\u2019re sincerely thankful for. But with time, you might be able to direct your focus to the things that are actually good, even if they are comparably small.",
        "summary": "Keeping an appreciation journal, where you write down what you're thankful for each day, can help improve your mood. Although it may feel awkward at first, over time, it can help shift your focus to positive aspects of your life.",
        "question": "What is the purpose of keeping an appreciation journal?",
        "ground_truths": "The purpose of keeping an appreciation journal is to help lighten your mood and direct your focus to the positive aspects of your life."
    },
    {
        "contexts": "These might seem trivial, and they certainly do not solve any underlying causes of whatever problem you might have. They do however help to get out of the hole and into a state where you can actually do something about your situation, and that is incredibly important. As a student, this can be especially frustrating. People might be partying and you don\u2019t want to leave at 11 to go to bed. After all, this is supposed to be the time of your life. Everyone else might seem like they just live into their day and do whatever feels right at the moment, and having a structured everyday life might prevent you from doing the same. What might help you is to think about this as a transitional phase: you\u2019re not going to do this forever. It\u2019s a phase you need in order to get back up on your feet and enjoy life again. It might suck, but it won\u2019t suck forever.",
        "summary": "While the suggested coping mechanisms may seem basic, they can help improve your situation. This can be challenging for students who may feel they're missing out on social activities. However, viewing this as a temporary phase can be beneficial. It's a necessary step to regain control and enjoy life again.",
        "question": "Why might it be difficult for students to adopt these coping mechanisms?",
        "ground_truths": "It might be difficult for students to adopt these coping mechanisms because they may feel they're missing out on social activities and the spontaneity of student life."
    },
    {
        "contexts": "If you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables! With a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations: Scatter Plot, Scatter Plot Matrix, Line chart, Correlogram.",
        "summary": "Correlation plots are useful tools for understanding the relationship between two or more variables. They can indicate if a relationship is weak or strong, positive, negative, or non-existent. However, they cannot determine causality. There are various ways to visualize correlation, including Scatter Plot, Scatter Plot Matrix, Line chart, and Correlogram.",
        "question": "What are correlation plots and what can they tell us about the relationship between variables?",
        "ground_truths": "Correlation plots are tools used to understand the relationship between two or more variables. They can indicate the strength and type of relationship (positive, negative, or non-existent) but cannot determine causality. They can be visualized in various ways, including Scatter Plot, Scatter Plot Matrix, Line chart, and Correlogram."
    },
    {
        "contexts": "Generally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. Pearson's correlation coefficient is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. While Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. Spearman's rank correlation coefficient calculates the rank order of the variables' values using a monotonic function whereas Kendall's rank correlation coefficient computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.",
        "summary": "There are three main methods to calculate the correlation coefficient: Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.",
        "question": "What are the three main methods to calculate the correlation coefficient and how do they differ?",
        "ground_truths": "The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks."
    },
    {
        "contexts": "Scatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables. In this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.",
        "summary": "Scatter plots are simple to construct and are ideal for two numeric variables. Each observation is represented as a dot on the graph, with the position of the dot representing the values of the variables. For example, a scatter plot of the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset shows a strong negative relationship, with a Pearson's correlation coefficient of -0.7761684.",
        "question": "What are scatter plots and how do they represent the relationship between two numeric variables?",
        "ground_truths": "Scatter plots are graphical representations ideal for two numeric variables. Each observation is represented as a dot on the graph, with the position of the dot indicating the values of the variables. For instance, a scatter plot of the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset shows a strong negative relationship, with a Pearson's correlation coefficient of -0.7761684."
    },
    {
        "contexts": "The normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.",
        "summary": "While a normal scatter plot is useful for visualizing the relationship between two variables, a scatter plot matrix can visualize multiple variables. This can be achieved using the PerformanceAnalytics package in R, which requires installation and importing. Once done, you can select the variables to be displayed in the plot.",
        "question": "How can you visualize the relationship between multiple variables in a scatter plot?",
        "ground_truths": "The relationship between multiple variables can be visualized using a scatter plot matrix. This can be achieved using the PerformanceAnalytics package in R, which requires installation and importing. Once done, you can select the variables to be displayed in the plot."
    },
    {
        "contexts": "A line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value. Multiple categories can be displayed in the same view, each represented by a unique line. Sometimes a point (circle/dot) is also used to substantiate the visibility of individual values. The lines used in a line chart will generally be straight. However, sometimes curved line interpolation may be used as a method of estimating values between known data points. This approach can be useful to help emphasise a general trend. While this might slightly compromise the visual accuracy of discrete values if you already have approximations, this will have less impact.",
        "summary": "Line charts display quantitative changes over time, with values plotted on a y-axis against a temporal x-axis. The slopes between points indicate local trends, and multiple categories can be represented by unique lines. Points may be used to highlight individual values, and curved line interpolation can estimate values between known data points, emphasizing general trends.",
        "question": "How does a line chart represent quantitative changes over time?",
        "ground_truths": "A line chart represents quantitative changes over time by plotting values on a y-axis against a temporal x-axis. The slopes between points indicate local trends. Multiple categories can be represented by unique lines, and points may be used to highlight individual values. Curved line interpolation can estimate values between known data points, emphasizing general trends."
    },
    {
        "contexts": "The correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package corrplot and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.",
        "summary": "A correlogram visualizes correlation coefficients for multiple variables, allowing quick determination of relationships, strength, and direction. The R package corrplot is required to create a correlogram. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.",
        "question": "What is the purpose of a correlogram and how is it created?",
        "ground_truths": "A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code."
    },
    {
        "contexts": "Karl Pearson is considered to be the founding father of mathematical statistics; hence it is no surprise that one of the central methods in statistics - to test the relationship between two continuous variables - was invented by him at the brink of the 20th century (see Karl Pearson's \"Notes on regression and inheritance in the case of two parents\" from 1895). His contribution was based on work from Francis Galton and Auguste Bravais. With more data becoming available and the need for an \u201cexact science\u201d as part of the industrialization and the rise of modern science, the Pearson correlation paved the road to modern statistics at the beginning of the 20th century. While other approaches such as the t-test or the Analysis of Variance (ANOVA) by Pearson's arch-enemy Fisher demanded an experimental approach, the correlation simply required data with a continuous measurement level. Hence it appealed to the demand for an analysis that could be conducted based solely on measurements done in engineering, or on counting as in economics, without being preoccupied too deeply with the reasoning on why variables correlated. Pearson recognized the predictive power of his discovery, and the correlation analysis became one of the most abundantly used statistical approaches in diverse disciplines such as economics, ecology, psychology and social sciences. Later came the \u200bregression analysis, which implies a causal link between two continuous variables. This makes it different from a correlation, where two variables are related, but not necessarily causally linked. This article focuses on correlation analysis and only touches upon regressions. For more, please refer to the entry on Regression Analysis.",
        "summary": "Karl Pearson, the founding father of mathematical statistics, invented the method to test the relationship between two continuous variables in the late 19th century. His work paved the way for modern statistics and the use of correlation analysis in various disciplines such as economics, ecology, psychology, and social sciences. Unlike other statistical methods that required an experimental approach, correlation analysis could be conducted based on measurements alone. This made it a popular choice for analysis in fields like engineering and economics. Pearson's correlation analysis differs from regression analysis in that it does not imply a causal link between the variables.",
        "question": "Who is considered the founding father of mathematical statistics and what method did he invent?",
        "ground_truths": "Karl Pearson is considered the founding father of mathematical statistics and he invented the method to test the relationship between two continuous variables."
    },
    {
        "contexts": "Correlation analysis examines the relationship between two continuous variables, and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called correlation coefficient indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. Correlations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including Surveys and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date.",
        "summary": "Correlation analysis is a statistical method that examines the relationship between two continuous variables and tests its significance. It considers the sample size and the strength of the relation, which is indicated by the correlation coefficient ranging from -1 to 1. A coefficient close to 0 indicates a weak correlation, while a coefficient close to 1 or -1 indicates a strong positive or negative correlation respectively. Correlation analysis can be applied to various types of quantitative continuous data and is used in both inductive and deductive approaches.",
        "question": "What does the correlation coefficient in correlation analysis indicate and what is its range?",
        "ground_truths": "In correlation analysis, the correlation coefficient indicates the strength of the relation between two continuous variables and it ranges from -1 to 1."
    },
    {
        "contexts": "There are different forms of correlation analysis. The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a Student's t-distribution. Alternative correlation measures like Kendall's tau and Spearman's rho are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data.",
        "summary": "There are various forms of correlation analysis. Pearson correlation is typically used for normally distributed data, specifically data that shows a Student's t-distribution. For data that is not normally distributed, alternative correlation measures like Kendall's tau and Spearman's rho are used. Spearman's rho is considered the most robust correlation measure for non-normally distributed data.",
        "question": "Which correlation measure is considered the most robust for non-normally distributed data?",
        "ground_truths": "Spearman's rho is considered the most robust correlation measure for non-normally distributed data."
    },
    {
        "contexts": "On the other hand, you might encounter data of two variables that is scattered all the way in a scatter plot and you cannot find a significant relationship. The correlation coefficient ''r'' might be around 0.1, or 0.2. Here, you can assume that there is no strong relationship between these two variables, and that one variable does not explain the other one. The stronger the correlation coefficient of a relation is, the more may these relations matter, some may argue. If the points are distributed like stars in the sky, then the relationship is probably not significant and interesting. Of course this is not entirely generalisable, but it is definitely true that a neutral relation only tells you, that the relation does not matter. At the same time, even weaker relations may give important initial insights into the data, and if two variables show any kind of relation, it is good to know the strength. By practising to quickly grasp the strength of a correlation, you become really fast in understanding relationships in data. Having this kind of skill is essential for anyone interested in approximating facts through quantitative data.",
        "summary": "When analyzing data of two variables, if the correlation coefficient is around 0.1 or 0.2, it can be assumed that there is no strong relationship between the variables. The stronger the correlation coefficient, the more significant the relationship may be. However, even weaker relations can provide important initial insights into the data. Understanding the strength of a correlation is a valuable skill for anyone interested in quantitative data analysis.",
        "question": "What does a low correlation coefficient indicate about the relationship between two variables?",
        "ground_truths": "A low correlation coefficient, such as 0.1 or 0.2, indicates that there is no strong relationship between the two variables."
    },
    {
        "contexts": "This is already an advanced skill, and is rather related to regression analysis. So if you have looked at the strength of a correlation, and its direction, you are good to go generally. But sometimes, these measures change in different parts of the data. To illustrate this, let us have a look at the example of the percentage of people working in Agriculture within individual countries. Across the world, people at a low income (<5000 Dollar/year) have a high variability in terms of agricultural employment:  half of the population of the Chad work in agriculture, while in Zimbabwe it is only 10 %. However, at an income above 15000 Dollar/year, there is hardly any variance in the percentage of people that work in agriculture: it is always very low. If you plotted this, you would see that the data points are rather broadly spread in the lower x-values (with x as the income), but are more linearly spread in the higher income areas (= x values). This has reasons, and there are probably one or several variables that explain this variability. Maybe there are other factors that have a stronger influence on the percentage of farmers in lower income groups than for higher incomes, where the income is a good predictor.",
        "summary": "Understanding the strength and direction of a correlation is an advanced skill related to regression analysis. For example, the percentage of people working in agriculture varies greatly among low-income countries, but is consistently low in high-income countries. This suggests that other factors may have a stronger influence on agricultural employment in low-income groups, while income is a good predictor for high-income groups.",
        "question": "How does the percentage of people working in agriculture vary among low-income and high-income countries?",
        "ground_truths": "In low-income countries, there is a high variability in the percentage of people working in agriculture, while in high-income countries, the percentage of people working in agriculture is consistently low."
    },
    {
        "contexts": "As you can see, correlation analysis is first and foremost a matter of identifying if and how two variables are related. We do not necessarily assume that we can predict the value of one variable based on the value of the other variable - we only see how they are related. People often show a correlation in a scatter plot - the x-axis is one variable, the y-axis the other one. You can see this in the example below. Then, they put a line on the data. This line - the regression line - represents the correlation coefficient. It is the best approximation for all data points. This means that this line has the minimum distance to all data points. If all data points are exactly on the line, we have a correlation of +1 or -1 (depending on the direction of the line). However, the further the data points are from the line, the closer the correlation coefficient is to 0, and the less meaningful the correlation is.",
        "summary": "Correlation analysis is primarily used to identify if and how two variables are related. A scatter plot is often used to visualize this relationship, with a regression line representing the correlation coefficient. If all data points are on the line, the correlation is +1 or -1. The further the data points are from the line, the closer the correlation coefficient is to 0, indicating a less meaningful correlation.",
        "question": "What does a correlation coefficient of +1 or -1 indicate in a scatter plot?",
        "ground_truths": "A correlation coefficient of +1 or -1 in a scatter plot indicates that all data points are exactly on the regression line, suggesting a strong correlation between the two variables."
    },
    {
        "contexts": "Correlation analysis can be a powerful tool both for inductive reasoning, without a theoretical foundation; or deductive reasoning, which is based on theory. This makes it versatile and has enabled new discoveries as well as the support of existing theories. The versatility of the method expands over all spatial and temporal scales, and basically any discipline that uses continuous data. This makes it clear why correlation analysis has become such a powerhorse for many researchers over time, and is so prevalent also in public debates. Correlations are rather easy to apply, and most software allows to derive simple scatterplots that can then be analyzed using correlations. However, you need some minimal knowledge about data distribution, since for instance the Pearson correlation is based on data that is normally distributed.",
        "summary": "Correlation analysis is a versatile tool used for both inductive and deductive reasoning, enabling new discoveries and supporting existing theories. It is applicable across all spatial and temporal scales and in any discipline that uses continuous data. Correlations are easy to apply with most software allowing for the derivation of simple scatterplots for analysis. However, some knowledge about data distribution is required, especially for Pearson correlation which is based on normally distributed data.",
        "question": "Why is correlation analysis considered a versatile tool?",
        "ground_truths": "Correlation analysis is considered a versatile tool because it can be used for both inductive and deductive reasoning, it can be applied across all spatial and temporal scales, and in any discipline that uses continuous data."
    },
    {
        "contexts": "With the power of correlations comes a great responsibility for the researcher. It can be tempting to infer causality and a logical relatoinship between two variables purely from the results of correlations. Economics and other fields have a long history of causal interpretation based on observed associations from the results of correlation analyses. However, researchers should always question whether there is a plausible connection between two variables, even if - or especially when - the correlation seems so clear. A regression analysis, that allows for the prediction of data beyond what can be observed, should especially only be done if there is a logical underlying connection. Keep in mind that regression = correlation + causality. For more thoughts on the connection between correlations and causality, have a look at this entry: [[Causality and correlation]].",
        "summary": "The power of correlations brings great responsibility for researchers, as it can be tempting to infer causality from correlation results. Fields like economics have a history of causal interpretation based on correlation analyses. However, researchers should always question the connection between two variables, even with clear correlation. Regression analysis, which predicts data beyond observation, should only be done if there is a logical underlying connection, as regression equals correlation plus causality.",
        "question": "What is the relationship between correlation and causality in research?",
        "ground_truths": "In research, it can be tempting to infer causality from correlation results. However, researchers should always question the connection between two variables, even with clear correlation. Regression analysis, which predicts data beyond observation, should only be done if there is a logical underlying connection, as regression equals correlation plus causality."
    },
    {
        "contexts": "Correlations are among the foundational pillars of frequentist statistics. Nonetheless, with science engaging in more complex designs and analysis, correlations will increasingly become less important. As a robust working horse for initial analysis, however, they will remain a good starting point for many datasets. Time will tell whether other approaches - such as [[Bayesian Inference|Bayesian statistics]] and [[Machine Learning|machine learning]] - will ultimately become more abundant. Correlations may benefit from a clear comparison to results based on Bayesian statistics. Until then, we should all be aware of the possibilities and limits of correlations, and what they can - and cannot - tell us about data and its underlying relationships.",
        "summary": "Correlations are foundational in frequentist statistics but may become less important as science engages in more complex designs and analysis. They remain a good starting point for many datasets. It remains to be seen if other approaches like Bayesian statistics and machine learning will become more prevalent. Correlations may benefit from comparison to results based on Bayesian statistics. It's important to understand the possibilities and limits of correlations and what they can and cannot reveal about data and its relationships.",
        "question": "What is the future outlook for the use of correlations in statistical analysis?",
        "ground_truths": "The future outlook for the use of correlations in statistical analysis is that they may become less important as science engages in more complex designs and analysis. However, they will remain a good starting point for many datasets. It remains to be seen if other approaches like Bayesian statistics and machine learning will become more prevalent."
    },
    {
        "contexts": "Territory mapping is a method used to study populations of territorial breeding species such as some ducks, gamebirds, raptors and most passerines. These species are territorial during the breeding season. Males sing in their territory and defend the borders of their territory by disputes. Therefore, the breeding territory is used as a census unit. Territory mapping determines the number of territories of each species in a given area. Multiple species can be mapped during one visit (1). In the first step, the land cover of the study plot is mapped at a scale of commonly 1:2500. Then, obvious features of the plot, such as houses, isolated trees, hedges, ponds etc. are marked on the map. In closed habitats such as temperate woodlands plot sizes of 15-20 ha are often sufficient. In a tropical forest, even half of this area could already be suitable. However, in more open habitats, an area of 60-80 ha is needed (1).",
        "summary": "Territory mapping is a technique used to study populations of territorial breeding species like ducks, gamebirds, raptors and most passerines. The breeding territory is used as a census unit. The method involves mapping the land cover of the study plot and marking obvious features. The size of the plot varies depending on the habitat, with smaller plots for closed habitats like temperate woodlands and larger plots for open habitats.",
        "question": "What is the purpose of territory mapping in bird studies?",
        "ground_truths": "Territory mapping is used to study populations of territorial breeding species. It determines the number of territories of each species in a given area."
    },
    {
        "contexts": "In practical terms, in the beginning of the breeding season, several plot maps are produced, one for each visit. The map is attached to a clipboard that can be covered with a large polyethylene bag in case of rain. By walking at a slow pace, the plot is covered with several routes in parallel with a 50 m distance in between the routes. Each bird that was heard or observed is marked at the location with a species code. Comments are made for evidence of nesting such as nests or birds carrying nesting material (1).",
        "summary": "At the start of the breeding season, multiple plot maps are created, one for each visit. The observer walks slowly across the plot, marking each bird heard or observed at the location with a species code. Any evidence of nesting is also noted.",
        "question": "How is the territory mapping process conducted in the field?",
        "ground_truths": "The territory mapping process involves creating multiple plot maps at the start of the breeding season, one for each visit. The observer walks slowly across the plot, marking each bird heard or observed at the location with a species code. Any evidence of nesting is also noted."
    },
    {
        "contexts": "Point counts are suitable for assessing the relative abundance of vocal or at least highly visible bird species such as passerines. They can be applied in a wide variety of habitats and at any time of the year, not restricted to the breeding season. Fixed counting stations are placed across the study area, either in a grid or in a random manner. From these locations, birds are observed for a fixed time period, usually between 3 and 10 minutes. Often, five minutes are adequate (3).",
        "summary": "Point counts are used to assess the relative abundance of vocal or highly visible bird species. They can be conducted in various habitats and at any time of the year. The method involves placing fixed counting stations across the study area and observing birds from these locations for a fixed time period.",
        "question": "What is the purpose and process of point counts in bird studies?",
        "ground_truths": "Point counts are used to assess the relative abundance of vocal or highly visible bird species. The process involves placing fixed counting stations across the study area and observing birds from these locations for a fixed time period."
    },
    {
        "contexts": "The distance between the counting stations should at least be 200 m to prevent counting the same individual twice. If the distance between the points is too large, it takes too much time to travel from one counting station to the next. Per study plot, at least 20 counting stations should be installed. To keep the minimum distance of 200 m in between the points, the plot should not be too small (1). After arriving at the counting station, wait a few minutes for the birds to resume their normal behavior. Then, during the observation time, all birds seen or heard are counted. As most of the birds will be counted in the first few minutes, spending more than 10 minutes is usually inefficient and may lead to increased double counting. Only in areas with very rich bird fauna or where species are especially hard to detect, spending more than 10 minutes might be necessary.",
        "summary": "Counting stations should be at least 200 m apart to avoid double counting and should be at least 20 per study plot. After arriving at a station, wait for birds to resume normal behavior before counting. Spending more than 10 minutes is usually inefficient unless in areas with rich bird fauna or hard-to-detect species.",
        "question": "What are the guidelines for setting up counting stations and conducting bird counts?",
        "ground_truths": "Counting stations should be at least 200 m apart to avoid double counting and there should be at least 20 per study plot. After arriving at a station, observers should wait for birds to resume normal behavior before counting. Spending more than 10 minutes is usually inefficient unless in areas with rich bird fauna or hard-to-detect species."
    },
    {
        "contexts": "Point counts are suitable for quickly collecting large amounts of data, even outside of the breeding season. This method can be applied for all birds that can easily be detected by song. The observer, however, needs to have a high level of experience in determining different bird species by song. Compared to territory mapping, counting stations can be distributed relatively easily in a random pattern (1). Since point counts often rely on the birds singing (or sometimes detecting them by eye), this method is unsuitable for less detectable species. The approach may also not be suited for open habitats where birds are likely to flee from the observer (1).",
        "summary": "Point counts are good for collecting large data quickly and can be used for birds easily detected by song. However, the observer needs experience in identifying bird species by song. This method is not suitable for less detectable species or open habitats where birds may flee.",
        "question": "What are the strengths and limitations of the point counts method in bird counting?",
        "ground_truths": "Point counts are good for collecting large data quickly and can be used for birds easily detected by song. However, the observer needs experience in identifying bird species by song. This method is not suitable for less detectable species or open habitats where birds may flee."
    },
    {
        "contexts": "Line transects are a method for counting birds of extensive open habitats. It is suitable for shrub-steppe, moorland, offshore seabirds and waterbirds. The observer moves freely through the land, sea or air. Observers move along a route and note down all birds they see on either side. In the first step, a route is determined that should be followed. This route should be positioned relatively randomly. It should not follow a path, hedge, stream, road or similar features as the results obtained here may strongly differ from the surrounding area. Moreover, counting seabirds from fishing trawlers should be prevented as they may attract birds (1).",
        "summary": "Line transects are used for counting birds in extensive open habitats like shrub-steppe, moorland, and offshore areas. Observers move along a randomly positioned route, noting down all birds they see. The route should not follow a path, hedge, stream, road or similar features to avoid skewed results.",
        "question": "How are line transects used in bird counting?",
        "ground_truths": "Line transects are used for counting birds in extensive open habitats like shrub-steppe, moorland, and offshore areas. Observers move along a randomly positioned route, noting down all birds they see. The route should not follow a path, hedge, stream, road or similar features to avoid skewed results."
    },
    {
        "contexts": "A transect route does not have to be straight, but can also be rectangular allowing the observer to end at the starting point. The route should be planned so that following it during subsequent visits is as easy as possible. Therefore, circular routes are not recommended (1). It is recommended to split the total length of the transect into smaller distances. These can directly continue into each other or they can be separated. If several transects should be undertaken, they should be at least 150 m apart in closed habitats and at least 250 m apart in open habitats to prevent the observer from counting the same individual twice. Additionally, the observers need to decide how often they want to visit the transects. It makes sense to complete the same transect several times as species detectability varies seasonally (1).",
        "summary": "Transect routes can be straight or rectangular, but not circular. They should be easy to follow on subsequent visits. The total length should be split into smaller distances, and if multiple transects are undertaken, they should be at least 150 m apart in closed habitats and 250 m apart in open habitats. Observers should decide how often to visit the transects as species detectability varies seasonally.",
        "question": "What are the guidelines for planning and conducting line transects in bird counting?",
        "ground_truths": "Transect routes can be straight or rectangular, but not circular. They should be easy to follow on subsequent visits. The total length should be split into smaller distances, and if multiple transects are undertaken, they should be at least 150 m apart in closed habitats and 250 m apart in open habitats. Observers should decide how often to visit the transects as species detectability varies seasonally."
    },
    {
        "contexts": "Territory mapping has a slight bias towards under-estimating the number of breeding pairs as paired males of some species sing less than unpaired males (6). The assumption of this method, that birds live in pairs in non-overlapping territories, is false for some species such as polygynous species and polyterritorial species. As there is a large proportion of subjectivity involved even when a standard protocol is used, inter-observer variation can be an issue (1). The main bias for point counts is that the length of time spent in the field is rather short. Therefore, the results can be strongly influenced by weather conditions. For this reason, strong winds, rain or cold weather are unsuitable for point counts (1). Moreover, the presence of the observer might repel or attract birds that are close to the observer which can seriously impact calculations of bird density, as the area sampled by a point count increases geometrically with the distance from the observer (7).",
        "summary": "Territory mapping in bird counting can underestimate the number of breeding pairs due to biases such as paired males singing less and the assumption that birds live in pairs in non-overlapping territories. Point counts can also be biased due to short field times and weather conditions. The presence of the observer can also impact bird density calculations.",
        "question": "What are some biases in bird counting methods?",
        "ground_truths": "Some biases in bird counting methods include underestimation of breeding pairs in territory mapping due to paired males singing less and the assumption of non-overlapping territories. In point counts, biases can arise from short field times, weather conditions, and the presence of the observer."
    },
    {
        "contexts": "The development of bird census data sets is increasingly impacted by the growing movement of citizen science. Especially during the Covid-19 pandemic where many other leisure activities were not available due to restrictions, many people turned to outdoor activities such as bird watching. This trend in citizen participation is supported by technical development making bird identification easier through the usage of identification apps. Smartphone apps such as \u201ceBirds\u201d help to identify species and also upload photos and the location where the individual was observed. This leads to a growing global online-community of bird watchers who create new data points in observation databases that scientists and environmentalists can make use of for conservation research or planning. For instance, in Canada, the number of people submitting photos to eBirds increased by 30% between 2019 and 2020 (8).",
        "summary": "The growth of citizen science, particularly during the Covid-19 pandemic, has impacted the development of bird census data sets. Identification apps like \u201ceBirds\u201d have made bird watching more accessible, leading to a global online community of bird watchers. This has resulted in new data points for scientists and environmentalists, with a 30% increase in photo submissions to eBirds in Canada between 2019 and 2020.",
        "question": "How has citizen science impacted the development of bird census data sets?",
        "ground_truths": "Citizen science has impacted the development of bird census data sets by increasing participation in bird watching, particularly during the Covid-19 pandemic. The use of identification apps like \u201ceBirds\u201d has made bird watching more accessible and has led to a global online community of bird watchers, providing new data points for scientists and environmentalists. This is evidenced by a 30% increase in photo submissions to eBirds in Canada between 2019 and 2020."
    },
    {
        "contexts": "Telemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is \u201cthe system of determining information about an animal through the use of radio signals from or to a device carried by the animal\u201d (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12).",
        "summary": "Telemetry, a method used in wildlife ecology for decades, has been further developed in recent years. It uses radio signals to gather information about an animal and can be applied to birds in various contexts, from restricted breeding territories to international migratory patterns. It can also track the distribution of infectious diseases among migratory species, though it can have negative effects on some birds' nesting behavior.",
        "question": "What is telemetry and how is it used in bird studies?",
        "ground_truths": "Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal. In bird studies, it can be applied in various contexts, from studying restricted breeding territories to tracking international migratory patterns. It can also be used to track the distribution of infectious diseases among migratory species. However, it can have negative effects on some birds' nesting behavior."
    },
    {
        "contexts": "The Courses section will be dedicated to entry points into diverse topics of science. More specifically, we will curate online courses, consisting of selected Wiki entries, videos, and exercises. These will help you gain an overview on the basics of specific thematical fields, understand how topics relate to each other, and receive further material to engage with. Each course may take between a day and several weeks, depending on your intended focus.",
        "summary": "The Courses section provides entry points into various science topics through curated online courses. These courses, which include Wiki entries, videos, and exercises, offer an overview of specific fields, show how topics interrelate, and provide additional material for engagement. The duration of each course varies.",
        "question": "What does the Courses section offer and how long does each course take?",
        "ground_truths": "The Courses section offers curated online courses on various science topics, which include Wiki entries, videos, and exercises. Each course may take between a day and several weeks."
    },
    {
        "contexts": "Currently, there is a selection of Wiki entries for two Leuphana university courses: Different paths to knowledge This course introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for current and future academic inquiry. Methods of Environmental Sciences This course covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective.",
        "summary": "There are Wiki entries for two Leuphana university courses: Different paths to knowledge and Methods of Environmental Sciences. The former introduces the basics of scientific work and the latter covers scientific methods central to sustainability research.",
        "question": "What are the two Leuphana university courses available in Wiki entries and what do they cover?",
        "ground_truths": "The two Leuphana university courses available in Wiki entries are Different paths to knowledge and Methods of Environmental Sciences. Different paths to knowledge introduces the basics of scientific work, while Methods of Environmental Sciences covers scientific methods central to sustainability research."
    },
    {
        "contexts": "In the contexts of test-reliability Cronbach's Alpha is one way of measuring the extent to which a given measurement is a consistent measure of a concept. The concept of Internal consistency can be connected to the interrelatedness of a set of items. It describes the extent to which all items in a test measure the same concept; it is connected to the inter-relatedness of the items within the test. Imagine an individual takes a Happiness Survey. Your calculated happiness score would be highly reliable (consistent) if your test produces a similar result when the same individual re-takes the survey, under the same conditions. However, the measure would not be reliable at all if an individual at the same level of real happiness takes the Survey twice back-to-back and receives one high and one low happiness score. Cronbach's Alpha is used under the assumption that you have multiple items measuring the same underlying construct. In this contexts you might have five questions asking different things, but when combined, could be said to measure overall happiness.",
        "summary": "Cronbach's Alpha is a measure of test-reliability, assessing the internal consistency or interrelatedness of a set of items in a test. It assumes multiple items measure the same underlying construct. For example, in a Happiness Survey, a reliable score would produce similar results when the same individual re-takes the survey under the same conditions.",
        "question": "What does Cronbach's Alpha measure in the contexts of a test?",
        "ground_truths": "Cronbach's Alpha measures the internal consistency or interrelatedness of a set of items in a test, assuming that multiple items measure the same underlying construct."
    },
    {
        "contexts": "Cronbach's Alpha is calculated by correlating the score for each scale item with the total score for each observation (usually individual test takers), and then comparing that to the variance for all individual item scores. Data available at Google Drive. For the sake of clarity, only 4 items are selected in this example to measure the construct 'Happiness' (Happiness.Score), among others.",
        "summary": "Cronbach's Alpha is calculated by correlating each scale item score with the total score for each observation, and comparing that to the variance for all individual item scores. In an example, only 4 items are selected to measure the construct 'Happiness'.",
        "question": "How is Cronbach's Alpha calculated?",
        "ground_truths": "Cronbach's Alpha is calculated by correlating the score for each scale item with the total score for each observation, and then comparing that to the variance for all individual item scores."
    },
    {
        "contexts": "The resulting alpha coefficient ranges from 0 to 1. \u03b1 = 0 if all the items share no covariance and are not correlated which means that all the scale items are entirely independent from one another. \u03b1 will approach 1 if the items have high covariances and are highly correlated. The higher the coefficient the more the items probably measure the same underlying concept. Negative values for \u03b1 indicate problems within your data e.g. you may have forgotten to reverse score some items. When interpreting Cronbach's Alpha understandings for what makes a 'good' coefficient may differ according to your application field and depend on one's theoretical knowledge. Most often methodologists recommend a minimum coefficient between 0.65 and 0.8. Values above 0.8 are considered as best in many cases where values less than 0.5 are unacceptable.",
        "summary": "The alpha coefficient ranges from 0 to 1. A score of 0 indicates no covariance and no correlation among items, while a score approaching 1 indicates high covariance and correlation. Negative values indicate data problems. Interpretation of a 'good' coefficient varies, but it's often recommended to be between 0.65 and 0.8, with values above 0.8 considered best and below 0.5 unacceptable.",
        "question": "What does the alpha coefficient range in Cronbach's Alpha indicate?",
        "ground_truths": "The alpha coefficient in Cronbach's Alpha ranges from 0 to 1. A score of 0 indicates no covariance and no correlation among items, meaning they are entirely independent. A score approaching 1 indicates high covariance and correlation, suggesting the items measure the same underlying concept. Negative values indicate data problems."
    },
    {
        "contexts": "Too high \u03b1 coefficient as a possible sign for redundancy. When interpreting a scale\u2019s \u03b1 coefficient, one should think about the alpha being a function of covariances among items and the number of items in the analysis. Therefore a high coefficient is not solely a mark of a reliable set of items as alpha can simply be increased by increasing the number of items in the analysis. Therefore a too high \u03b1 coefficient (i.e. > 0.95) can be a sign of redundancy in the scale items. Cronbach's Alpha is also affected by the length of the test. A larger number of items can result in a larger \u03b1, smaller number of items in a smaller \u03b1. If alpha is too high, your analysis may include items asking the same things whereas a low value for alpha may mean that there aren\u2019t enough questions on the test.",
        "summary": "A high \u03b1 coefficient can indicate redundancy, as it's a function of covariances among items and the number of items in the analysis. A coefficient over 0.95 may suggest redundancy in scale items. Cronbach's Alpha is also affected by test length, with a larger number of items resulting in a larger \u03b1, and vice versa. A high alpha may indicate repetitive items, while a low alpha may suggest insufficient questions.",
        "question": "What can a high \u03b1 coefficient in Cronbach's Alpha indicate?",
        "ground_truths": "A high \u03b1 coefficient in Cronbach's Alpha can indicate redundancy, as it's a function of covariances among items and the number of items in the analysis. A coefficient over 0.95 may suggest redundancy in scale items."
    },
    {
        "contexts": "Data inspection is an important and necessary step in data science because it allows data scientists to understand the characteristics of their data and identify potential problems or issues with it. By carefully inspecting the data, data scientists can ensure that the data is accurate, complete, and most importantly relevant to the problem they are trying to solve.",
        "summary": "Data inspection is crucial in data science as it helps data scientists understand their data's characteristics and identify any potential issues. This process ensures the data is accurate, complete, and relevant to the problem at hand.",
        "question": "Why is data inspection important in data science?",
        "ground_truths": "Data inspection is important in data science because it allows data scientists to understand the characteristics of their data and identify potential problems or issues with it. It ensures the data is accurate, complete, and relevant to the problem they are trying to solve."
    },
    {
        "contexts": "For our data inspection, we will be using the following packages: 1. '''Pandas''', which is a standard data analysis package, and 2. '''Matplotlib''', which can help us create beautiful visualizations of our data. We can import the packages as follows: import pandas as pd import matplotlib.pyplot as plt # shows more columns pd.set_option('display.max_columns', 500)",
        "summary": "The data inspection will utilize two packages: 'Pandas', a standard data analysis package, and 'Matplotlib', used for data visualization. These packages can be imported using specific Python commands.",
        "question": "What packages are used for data inspection and how are they imported?",
        "ground_truths": "The data inspection uses 'Pandas', a standard data analysis package, and 'Matplotlib', used for data visualization. They can be imported using the commands 'import pandas as pd' and 'import matplotlib.pyplot as plt'."
    },
    {
        "contexts": "To load the data, we need to use the appropriate command depending on the type of data file we have. For example, if we have a CSV file, we can load it using the read_csv method from Pandas, like this: data = pd.read_csv('Los_Angeles_Crime.csv') If we have an Excel file, we can load it using the read_excel method, like this: data = pd.read_excel('data_name.xlsx')",
        "summary": "Data loading requires the correct command based on the data file type. For instance, a CSV file can be loaded using the 'read_csv' method from Pandas, while an Excel file can be loaded using the 'read_excel' method.",
        "question": "How can different types of data files be loaded in Python?",
        "ground_truths": "Different types of data files can be loaded in Python using appropriate commands. A CSV file can be loaded using the 'read_csv' method from Pandas, and an Excel file can be loaded using the 'read_excel' method."
    },
    {
        "contexts": "We check the size of the data by using the shape method, like this: num_rows, num_cols = data.shape print('The data has {} rows and {} columns'.format(num_rows, num_cols)) The data has 407199 rows and 28 columns. The first number is the number of rows (data entries). The second number is the number of columns (attributes).",
        "summary": "The size of the data can be checked using the 'shape' method, which provides the number of rows (data entries) and columns (attributes).",
        "question": "How can the size of the data be determined in Python?",
        "ground_truths": "The size of the data can be determined in Python using the 'shape' method, which provides the number of rows (data entries) and columns (attributes)."
    },
    {
        "contexts": "To get basic information about the data, we can use the info method from Pandas, like this: print(data.info()) This will print out a range of information about the data, including the number of entries and columns, the data types of each column, and the number of non-null entries.",
        "summary": "The 'info' method from Pandas can be used to obtain basic information about the data, including the number of entries and columns, the data types of each column, and the number of non-null entries.",
        "question": "How can basic information about the data be obtained in Python?",
        "ground_truths": "Basic information about the data can be obtained in Python using the 'info' method from Pandas, which provides the number of entries and columns, the data types of each column, and the number of non-null entries."
    },
    {
        "contexts": "To get a first look at the data, we can use the head and tail methods from Pandas, like this: print(data.head()) print(data.tail()) The head method will print the first few rows of the data, while the tail method will print the last few rows.",
        "summary": "The 'head' and 'tail' methods from Pandas can be used to get a first look at the data, printing the first and last few rows respectively.",
        "question": "How can the first and last few rows of the data be viewed in Python?",
        "ground_truths": "The first and last few rows of the data can be viewed in Python using the 'head' and 'tail' methods from Pandas."
    },
    {
        "contexts": "To get a descriptive statistical overview of the data, we can use the described method from Pandas, like this: print(data.describe()) This will calculate basic statistics for numeric columns, such as the mean, standard deviation, minimum and maximum values, and other summary statistics.",
        "summary": "The 'describe' method from Pandas can be used to get a statistical overview of the data, calculating basic statistics for numeric columns like mean, standard deviation, minimum and maximum values.",
        "question": "How can a statistical overview of the data be obtained in Python?",
        "ground_truths": "A statistical overview of the data can be obtained in Python using the 'describe' method from Pandas, which calculates basic statistics for numeric columns like mean, standard deviation, minimum and maximum values."
    },
    {
        "contexts": "To check if the data has any missing values, we can use the isnull and any methods from Pandas, like this: print(data.isnull().any())",
        "summary": "The 'isnull' and 'any' methods from Pandas can be used to check if the data has any missing values.",
        "question": "How can missing values in the data be checked in Python?",
        "ground_truths": "Missing values in the data can be checked in Python using the 'isnull' and 'any' methods from Pandas."
    },
    {
        "contexts": "This will return a table with a True value for each column that has missing values, and a False value for each column that does not have missing values. Checking for missing values is important because most modeling techniques cannot handle missing data. If your data contains missing values, you will need to either impute the missing values (i.e. replace them with estimated values) or remove the rows with missing values before fitting a model. If there are missing values in the data, you can use the sum method to check the number of missing values per column, like this: print(data.isnull().sum()) Alternatively, you can use the shape attribute to calculate the percentage of missing values per column, like this: print(data.isnull().sum()/data.shape[0]*100) This can help you understand the extent of the missing values in your data and decide how to handle them.",
        "summary": "Missing values in data can be problematic for most modeling techniques. These can be handled by either imputing the missing values or removing the rows with missing values. Python provides methods to check the number and percentage of missing values per column.",
        "question": "How can missing values in data be handled in Python?",
        "ground_truths": "Missing values in data can be handled by either imputing the missing values or removing the rows with missing values. Python provides methods to check the number and percentage of missing values per column."
    },
    {
        "contexts": "To check if there are duplicate entries in the data, we can use the duplicated method from Pandas, like this: print(data.duplicated()) This will return a True value for each row that is a duplicate of another row, and a False value for each unique row. If there are any duplicate entries in the data, we can remove them using the drop_duplicates method, like this: data = data.drop_duplicates() This will return a new dataframe that contains only unique rows, with the duplicate rows removed. This can be useful for ensuring that the data is clean and ready for analysis.",
        "summary": "Duplicate entries in data can be identified using the duplicated method in Python. These duplicates can be removed using the drop_duplicates method, resulting in a new dataframe with only unique rows.",
        "question": "How can duplicate entries in data be identified and removed in Python?",
        "ground_truths": "Duplicate entries in data can be identified using the duplicated method in Python. These duplicates can be removed using the drop_duplicates method, resulting in a new dataframe with only unique rows."
    },
    {
        "contexts": "Data visualization can be a powerful tool for inspecting data and identifying patterns, trends, and anomalies. It allows you to quickly and easily explore the data, and get insights that might not be immediately obvious when looking at the raw data. First, we start by getting all columns with numerical data by using the select_dtypes method and filtering for in64 and float64: numeric_columns = data.select_dtypes(include=['int64', 'float64']) print(numeric_columns.shape) Print the numerical columns print(numeric_columns)",
        "summary": "Data visualization is a useful tool for exploring data and identifying patterns. It can be used to quickly gain insights from the data. In Python, the select_dtypes method can be used to get all columns with numerical data.",
        "question": "What is the role of data visualization and how can it be implemented in Python?",
        "ground_truths": "Data visualization is a useful tool for exploring data and identifying patterns. It can be used to quickly gain insights from the data. In Python, the select_dtypes method can be used to get all columns with numerical data."
    },
    {
        "contexts": "As data projects evolve more data is collected, annotated and modified, and models are built, optimized, and re-built on new datasets. If we manage several versions of a code with different modifications and complementary explanatory comments, we call that versioning. It is especially important to guarantee the reproducibility of an experiment. Most data scientists work with GitHub for versioning code, but it is not ideal to version datasets, models, and metrics as Github has a strict file limit & it cannot handle large files & directories. It can also get very tricky for comparing different versions of data. DVC (Data Version Control) is built to make different data projects shareable and reproducible. It is an ideal tool for versioning binary data and is especially designed to handle large files, data sets, machine learning models, and metrics. However, we cannot view the DVC tracked files on GitHub. DAGsHub platform enables data scientists and machine learning engineers to version their data, models, experiments, and code.",
        "summary": "As data projects grow, more data is collected and models are built and optimized. Versioning, or managing different versions of code, is crucial for experiment reproducibility. While GitHub is commonly used for code versioning, it's not ideal for datasets, models, and metrics due to file size limitations. DVC (Data Version Control) is designed to handle large files and make data projects shareable and reproducible. DAGsHub platform allows data scientists to version their data, models, experiments, and code.",
        "question": "What are the limitations of using GitHub for data versioning and what tools can overcome these limitations?",
        "ground_truths": "GitHub has a strict file limit and cannot handle large files and directories, making it not ideal for versioning datasets, models, and metrics. DVC (Data Version Control) and DAGsHub are tools designed to overcome these limitations, allowing for versioning of large files, data sets, machine learning models, and metrics."
    },
    {
        "contexts": "Machine Learning Operations (MLOps) is a term used to describe the set of processes and tools that help manage the end-to-end lifecycle of machine learning models. This includes everything from data preparation, feature engineering, model training, model deployment as well as model monitoring.",
        "summary": "Machine Learning Operations (MLOps) refers to the processes and tools used to manage the entire lifecycle of machine learning models, including data preparation, feature engineering, model training, deployment, and monitoring.",
        "question": "What does the term Machine Learning Operations (MLOps) encompass?",
        "ground_truths": "Machine Learning Operations (MLOps) encompasses the set of processes and tools that help manage the end-to-end lifecycle of machine learning models. This includes everything from data preparation, feature engineering, model training, model deployment as well as model monitoring."
    },
    {
        "contexts": "DVC is the main tool for MLOps and enables data versioning through codification and captures the versions of the data and models in Git commits. It also provides a mechanism to switch between different data contents. Importance of Data Versioning, *   Ensure better training data *   Track data schema *   Continuous model training",
        "summary": "DVC is a key tool in MLOps, enabling data versioning and capturing versions of data and models in Git commits. It allows switching between different data contents. Data versioning is important for ensuring better training data, tracking data schema, and continuous model training.",
        "question": "What are the benefits of data versioning in machine learning operations?",
        "ground_truths": "Data versioning ensures better training data, allows for tracking of data schema, and enables continuous model training."
    },
    {
        "contexts": "DAGsHub is a web platform that leverages popular open-source tools to version datasets models, track experiments, label data, and visualize results. It is a free-to-use web platform similar to GitHub for an open-source data science project. DAGsHub supports inbuilt tools like Git for source code tracking, DVC for data version tracking, and MLflow for experiment tracking, which allows us to connect everything in one place with zero configuration.",
        "summary": "DAGsHub is a free web platform that uses open-source tools to version datasets, track experiments, label data, and visualize results. It's similar to GitHub for open-source data science projects and supports tools like Git, DVC, and MLflow for tracking source code, data versions, and experiments.",
        "question": "What functionalities does DAGsHub offer for data science projects?",
        "ground_truths": "DAGsHub offers functionalities to version datasets, track experiments, label data, and visualize results. It supports inbuilt tools like Git for source code tracking, DVC for data version tracking, and MLflow for experiment tracking."
    },
    {
        "contexts": "Data distribution is the most basic and also a fundamental step of analysis for any given data set. On the other hand, data distribution encompasses the most complex concepts in statistics, thereby including also a diversity of concepts that translates further into many different steps of analysis. Consequently, without understanding the basics of data distribution, it is next to impossible to understand any statistics down the road. Data distribution can be seen as the fundamentals, and we shall often return to these when building statistics further.",
        "summary": "Data distribution is a fundamental step in data analysis and includes complex statistical concepts. Understanding data distribution is crucial for understanding statistics. It is often revisited when building further statistics.",
        "question": "Why is understanding data distribution important in statistics?",
        "ground_truths": "Understanding data distribution is crucial for understanding statistics because it is a fundamental step in data analysis and includes complex statistical concepts. It is often revisited when building further statistics."
    },
    {
        "contexts": "How wonderful, it is truly a miracle how almost everything that can be measured seems to be following the normal distribution. Overall, the normal distribution is not only the most abundantly occurring, but also the earliest distribution that was known. It follows the premise that most data in any given dataset has its majority around a mean value, and only small amounts of the data are found at the extremes.",
        "summary": "Almost everything measurable follows the normal distribution, which is the most common and earliest known distribution. It assumes that most data in a dataset is around a mean value, with small amounts at the extremes.",
        "question": "What is the premise of the normal distribution?",
        "ground_truths": "The normal distribution assumes that most data in any given dataset has its majority around a mean value, and only small amounts of the data are found at the extremes."
    },
    {
        "contexts": "Most things in their natural state follow a normal distribution. If somebody tells you that something is not normally distributed, this person is either very clever or not very clever. A small sample can hamper you from finding a normal distribution. If you weigh five people you will hardly find a normal distribution, as the sample is obviously too small. While it may seem like a magic trick, it is actually true that many phenomena that can be measured will follow the normal distribution, at least when your sample is large enough. Consequently, much of the probabilistic statistics is built on the normal distribution.",
        "summary": "Most natural phenomena follow a normal distribution. Small samples can prevent finding a normal distribution. Many measurable phenomena follow the normal distribution when the sample is large enough. Much of probabilistic statistics is based on the normal distribution.",
        "question": "Why is the size of the sample important in finding a normal distribution?",
        "ground_truths": "The size of the sample is important in finding a normal distribution because small samples can prevent finding a normal distribution. Many measurable phenomena follow the normal distribution when the sample is large enough."
    },
    {
        "contexts": "The most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take weight. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different.",
        "summary": "Human actions have caused deviations from the normal distribution, such as in weight distribution. Today's global weight distribution is complex and does not follow a normal distribution. However, studies from indigenous communities show a normal weight distribution.",
        "question": "What is a common reason for deviation from the normal distribution?",
        "ground_truths": "A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution."
    },
    {
        "contexts": "But when is data normally distributed? And how can you recognize it when you have a boxplot in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot's whiskers are largely uneven.",
        "summary": "Data is normally distributed when it follows a bell curve. This can be recognized in histograms and boxplots. If a histogram shows a long tail or multiple peaks, or if a boxplot's whiskers are uneven, the data is not normally distributed.",
        "question": "How can you recognize if data is normally distributed?",
        "ground_truths": "Data is normally distributed when it follows a bell curve. This can be recognized in histograms and boxplots. If a histogram shows a long tail or multiple peaks, or if a boxplot's whiskers are uneven, the data is not normally distributed."
    },
    {
        "contexts": "You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution. This barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).",
        "summary": "The Shapiro-Wilk test can be used to check for normal distribution. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed. A barplot representing the number of front-seat passengers killed or seriously injured annually in the UK from 1969 to 1985 shows a normal distribution when sorted from lowest to highest.",
        "question": "How can the Shapiro-Wilk test be used in data distribution?",
        "ground_truths": "The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed."
    },
    {
        "contexts": "The command qqplot will return a Quantile-Quantile plot. This plot allows for a visual inspection on how your model residuals behave in relation to a normal distribution. On the y-axis there are the standardised residuals and on the x-axis the theoretical quantiles. The simple answer is, if your data points are on this line you are fine, you have normal errors, and you can stop reading here. If you want to know more about the theory behind this please continue. Residuals is the difference of your response variable and the fitted values.",
        "summary": "The qqplot command returns a Quantile-Quantile plot, which allows for a visual inspection of how model residuals behave in relation to a normal distribution. The y-axis represents the standardised residuals and the x-axis represents the theoretical quantiles. If the data points are on the line, it indicates normal errors.",
        "question": "What does a Quantile-Quantile plot represent in data distribution?",
        "ground_truths": "A Quantile-Quantile plot allows for a visual inspection of how model residuals behave in relation to a normal distribution. The y-axis represents the standardised residuals and the x-axis represents the theoretical quantiles. If the data points are on the line, it indicates normal errors."
    },
    {
        "contexts": "Sometimes the world is not normally distributed. At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which reasons exists why data is not normally distributed.",
        "summary": "Not all data in the world is normally distributed. Upon closer examination, this makes sense under specific circumstances. It's important to understand the reasons why data may not be normally distributed.",
        "question": "Why might data not be normally distributed?",
        "ground_truths": "Data might not be normally distributed due to specific circumstances. It's important to understand the reasons why this may occur."
    },
    {
        "contexts": "Things that can be counted are often not normally distributed, but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. How many people do you think drink one or two cups of coffee per day? Quite many, I guess. How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes.",
        "summary": "Countable data is often not normally distributed, but is skewed to the right. For example, the number of people who drink one or two cups of coffee per day is likely higher than those who drink 10 cups. Similarly, more people likely engage in 30 minutes of sports activities per day than those who do 5 hours. In countable phenomena, most people tend towards a lower amount, with few reaching higher amounts.",
        "question": "How does countable data typically distribute?",
        "ground_truths": "Countable data is often not normally distributed, but is skewed to the right. Most people tend towards a lower amount, with few reaching higher amounts."
    },
    {
        "contexts": "Now here comes the funny surprise. Transform the data following a Poisson distribution, and it will typically follow the normal distribution if you use the decadic logarithm (log). Hence skewed data can be often transformed to match the normal distribution. While many people refrain from this, it actually may make sense in such examples as island biogeography. Discovered by MacArtur & Wilson, it is a prominent example of how the log of the numbers of species and the log of island size are closely related. While this is one of the fundamental basic of ecology, a statistician would have preferred the use of the Poisson distribution.",
        "summary": "Data following a Poisson distribution can be transformed to match the normal distribution using the decadic logarithm. This can be useful in areas such as island biogeography, where the log of species numbers and island size are closely related. Despite this, many people avoid this transformation, even though it is a fundamental aspect of ecology.",
        "question": "How can data following a Poisson distribution be transformed to match the normal distribution?",
        "ground_truths": "Data following a Poisson distribution can be transformed to match the normal distribution using the decadic logarithm."
    },
    {
        "contexts": "Did you know that most people wear 20 % of their clothes 80 % of their time? This observation can be described by the Pareto distribution. For many phenomena that describe proportion within a given population, you often find that few make a lot, and many make few things. Unfortunately this is often the case for workloads, and we shall hope to change this. For such proportions the Pareto distribution is quite relevant. Consequently, it is rooted in income statistics. Many people have a small to average income, and few people have a large income. This makes this distribution so important for economics, and also for sustainability science.",
        "summary": "The Pareto distribution describes the phenomenon where a small proportion of a population contributes to a large proportion of a result. This is often seen in workloads and income distribution, where few people have a large income while many have a small to average income. This distribution is significant in economics and sustainability science.",
        "question": "What phenomenon does the Pareto distribution describe?",
        "ground_truths": "The Pareto distribution describes the phenomenon where a small proportion of a population contributes to a large proportion of a result."
    },
    {
        "contexts": "A nice way to visualize a data set is to draw a boxplot. You get a rough overview how the data is distributed and moreover you can say at a glance if it\u2019s normally distributed. The same is true for histograms, but we will focus on the boxplot for now. For more information on both these forms of data visualisation, please refer to the entry on Barplots, Histograms and Boxplots.",
        "summary": "Boxplots are a useful tool for visualizing data sets, providing a rough overview of data distribution and indicating if it\u2019s normally distributed. While histograms also serve this purpose, the focus here is on boxplots.",
        "question": "What information can a boxplot provide about a data set?",
        "ground_truths": "A boxplot can provide a rough overview of how the data is distributed and can indicate if it\u2019s normally distributed."
    },
    {
        "contexts": "The format of your data influences everything else you do further down the road. To paraphrase a proverb, data is in a format, and the format is the data. Therefore, it is essential to know which different data formats exist, and how these may be beneficial, and where you may encounter pitfalls. The most important difference is between quantitative data and qualitative data. Quantitative data can consist of integers and discrete data, while qualitative data can be factorial -meaning in truely different categories- nominal or ordinal, with the latter two providing a link to quantitative data.",
        "summary": "Data format is crucial as it influences subsequent actions. It's vital to understand different data formats, their benefits, and potential pitfalls. The key distinction is between quantitative (consisting of integers and discrete data) and qualitative data (factorial, nominal, or ordinal).",
        "question": "What is the main difference between quantitative and qualitative data?",
        "ground_truths": "Quantitative data can consist of integers and discrete data, while qualitative data can be factorial -meaning in truly different categories- nominal or ordinal."
    },
    {
        "contexts": "Continuous data is numerical data that cannot be counted because it exists on a finite or infinite number line. We are all familiar with continuous numbers. Much of our society is ruled by these numbers, and thus much of data analysed in statistics is represented by continuous numbers. Continuous data has a true zero. A true zero is defined as a total absence of something that can be represented in numbers. Although a weight of 0 kg or a length of 0 m is abstract, the values represent the absence of weight and length, respectively.",
        "summary": "Continuous data is numerical and exists on a finite or infinite number line, making it uncountable. It's prevalent in our society and statistical analysis. Continuous data has a 'true zero', representing the total absence of something.",
        "question": "What is the meaning of 'true zero' in the contexts of continuous data?",
        "ground_truths": "A true zero is defined as a total absence of something that can be represented in numbers. Although a weight of 0 kg or a length of 0 m is abstract, the values represent the absence of weight and length, respectively."
    },
    {
        "contexts": "Discrete data is numeric data that can be counted because it only exists as natural numbers (1, 2, 3, 4...). Examples of this are students in a lecture, where the use a fraction numbers is not helpful. Discrete data is often also referred to as 'abundance' or 'counting' data, and within the R language it is called 'integer'. Discrete data also has a true zero. Take again the number of students in a statistics lecture. Although the lecture is good, for example because it includes songs of Sesame Street, there might be no students in the lecture. 0 students in a lecture \u2013 there you got your true zero.",
        "summary": "Discrete data is countable numeric data, existing as natural numbers. It's often referred to as 'abundance' or 'counting' data. Like continuous data, discrete data also has a 'true zero', representing the absence of something.",
        "question": "What is an example of discrete data?",
        "ground_truths": "An example of discrete data is the number of students in a lecture."
    },
    {
        "contexts": "Interval data consists of measured or counted values, but it does not have a true zero. Also, the difference between two data points is equal no matter where on the scale you look. The best example is temperature if measured in \u00b0C. The difference between 30\u00b0C and 40\u00b0C is equal to the difference between 100\u00b0C and 110\u00b0C. However, there is no true zero to the Celsius scale: 0\u00b0C does not mean that there is no temperature. Rather, 0\u00b0C represents a specific value on the temperature scale.",
        "summary": "Interval data comprises measured or counted values without a true zero. The difference between two data points is consistent across the scale. For instance, temperature in \u00b0C, where 0\u00b0C doesn't mean absence of temperature but a specific value on the scale.",
        "question": "Why doesn't interval data have a 'true zero'?",
        "ground_truths": "Interval data does not have a true zero because 0 in interval data does not represent the absence of something, but rather a specific value on the scale."
    },
    {
        "contexts": "Qualitative (categorical) data in a statistical sense is data that can be stored in labeled categories which are independent from each other. Such categories are typically constructed, and thus contain information that is deeply normative or designed. An example would be hair color, which can be in human perceptions of colours, yet is often also described with different names when it comes to professional hair products. Within statistics, categories are often designed so that within a scientific experiment, categories are constructed in a sense that allows for a meaningful testing of the hypothesis, and meaningful is then in the eye of the beholder. Different levels of fertiliser would be such an example, and the categories would often be built around previous knowledge or pre-tests. Categories are thus of particular importance when it comes to the reduction of the complexity of the world, as it would not be possible to test all sorts of different levels of fertiliser in an experiment. Instead, you might go with \"little\", \"moderate\", \"much\" and \"very much\" fertilizer. Nevertheless, this demands a clear recognition that and how categories are constructed, and deeply normative.",
        "summary": "Qualitative data is statistically stored in independent labeled categories, often constructed and containing normative information. Examples include hair color and levels of fertilizer. These categories are designed for meaningful hypothesis testing in scientific experiments, and are crucial in reducing world complexity. It's important to recognize how these categories are constructed.",
        "question": "What is the significance of categories in qualitative data?",
        "ground_truths": "Categories in qualitative data are significant as they are designed for meaningful hypothesis testing in scientific experiments and are crucial in reducing world complexity. They contain normative information and are often constructed based on previous knowledge or pre-tests."
    },
    {
        "contexts": "Ordinal data is categorical data that can be ranked, but not calculated with, even if it is represented in numbers. Remember your school grades? A \"1\" is the best grade in the German grading system, but is it twice as good than a \"2\"? Hardly. Such grades are ordinal numbers. These are a system of numbers that are ranked in some sense, but the numbers per se do not necessarily reflect a numeric system. In other words, they are highly normative and contested. A \"2\" might be a good grade for some, and a disaster for others. Ordinal formats are often clearly defined scales that allow people to grade, evaluate or rank certain information. One of the most prominent examples is the Likert scale that is often used in Psychology. In this case, the scaling is often not reflected in numbers at all, but in levels such as \"Strongly Agree\" or \"Rather Disagree\". Such constructed scales may make a true statistician very unhappy, since these scales are hard to analyse, yet there is hardly any alternative since it also does not make any sense to ask: \"How happy are you on a scale from 1 to 100?\". Therefore, ordinal scales are often relevant in order to create a scaling system that allows for wide comparability or even becomes a norm, such as school grades.",
        "summary": "Ordinal data is categorical data that can be ranked but not calculated with, even if represented in numbers. Examples include school grades and the Likert scale used in Psychology. These scales are often not reflected in numbers but in levels like \"Strongly Agree\" or \"Rather Disagree\". Ordinal scales are relevant for creating a scaling system that allows for wide comparability or even becomes a norm.",
        "question": "What is the characteristic of ordinal data and how is it used?",
        "ground_truths": "Ordinal data is categorical data that can be ranked but not calculated with, even if represented in numbers. It is used in creating a scaling system that allows for wide comparability or even becomes a norm, such as school grades and the Likert scale used in Psychology."
    },
    {
        "contexts": "Whenever you have categorical data that cannot be ranked, it is called nominal data. An example would be different ethnicities, countries of birth, or different types of gender. This already highlights that we are here confronted by often completely different worldviews, thus nominal data represents a stark case of a normative view of the world. Gender is a prominent example, since some people still define gender by a biological stereotype (Female/Male) and thus binary (see below), which according to my worldview is clearly wrong, and I see Gender as nominal with more than two categories. Nominal data formats hence demand an even clearer reflection than ordinal data, where at least you may say that a certain school grade is higher than another one. This is not the case for nominal data. Therefore, one has to be extra careful about the implications, that a specific constructed scale may imply.",
        "summary": "Nominal data is categorical data that cannot be ranked, such as ethnicities, countries of birth, or different types of gender. It represents a normative view of the world and demands clearer reflection than ordinal data. One must be careful about the implications of a specific constructed scale.",
        "question": "What is nominal data and what considerations does it require?",
        "ground_truths": "Nominal data is categorical data that cannot be ranked, such as ethnicities, countries of birth, or different types of gender. It requires clearer reflection than ordinal data and one must be careful about the implications of a specific constructed scale."
    },
    {
        "contexts": "Binary data is the most reduced data format, which basically consists of two levels: 1 and 0. It is, strictly speaking, nominal data, but nominal data that only exists in two versions which can be translated into 1 and 0: On / Off, Yes / No. In computer science binary data is used directly as simple 0 and 1, but the great breakthrough of that dataset was early on in the insurance business as well as in medicine, where 'dead' or 'alive' are often the most fundamental questions. Binary information is clearly simplistic, but quite often this matches with a certain view of reality. Take the example of being able to play an instrument. If somebody asks you whether you can play the piano, you will probably say yes or no. You may most likely not qualify your answer by saying \"I play better than a monkey, but worse than Horowitz\". Some modest folks may say \"I can play a bit\", or \"I am not very good\", or \"I used to be better\", but very often people answer yes or no. Hence binary data allows for a simple view of reality, and this may often match with the world how we perceive it. But be aware: Other people may have a less simple view.",
        "summary": "Binary data, a reduced data format, consists of two levels: 1 and 0, or On/Off, Yes/No. It's used in computer science, insurance, and medicine, often matching a simplistic view of reality. For example, when asked if you can play an instrument, the answer is usually yes or no. Binary data allows for a simple view of reality, but it's important to remember that others may have a less simple view.",
        "question": "What is binary data and how does it reflect our perception of reality?",
        "ground_truths": "Binary data is a reduced data format that consists of two levels: 1 and 0, or On/Off, Yes/No. It often matches a simplistic view of reality, such as when asked if you can play an instrument, the answer is usually yes or no. However, it's important to remember that others may have a less simple view."
    },
    {
        "contexts": "You may wonder now how to choose the right data format for your data gathering. The answer to that is quite simple. '''Any data format should be as simple as possible, and as complex as necessary.''' Follow Occam's razor, and you will be fine. Of course this sounds appealing, but how to know what is too simple, and what is too complex? Here, I suggest you build on the available literature. Read other publications that examined a certain phenomenon before, these papers may guide you in choosing the right scale.",
        "summary": "Choosing the right data format for data gathering should be as simple as possible, and as complex as necessary. To determine the right level of complexity, one should refer to existing literature and previous studies.",
        "question": "How can one choose the right data format for data gathering?",
        "ground_truths": "The right data format for data gathering should be as simple as possible, and as complex as necessary. Existing literature and previous studies can guide in determining the right level of complexity."
    },
    {
        "contexts": "In economics and finance, an index is a statistical measure of change in a representative group of individual data points. A good example of the application of an index that most people know is the [https://www.investopedia.com/terms/g/gdp.asp GDP], the gross domestic product of a country. Although it has largely been criticised for being too generalised and not offering enough nuance to understand the complexity of the single country, many social, economical and other indicators are correlated with the GDP. In ecology, a prominent example for an index is the so-called [https://www.youtube.com/watch?v=ghhZClDRK_g Shannon Wiener index], which represents abundance corrected diversity measures. A prominent example from economy again is the [https://www.youtube.com/watch?v=_PXFVNWINQc Dow Jones index] while the [http://hdr.undp.org/en/content/human-development-index-hdi human development index] tries to integrate information about life expectancy education and income in order to get a general understanding about several components that characterise countries.",
        "summary": "An index is a statistical measure used in various fields like economics, finance, and ecology. Examples include the GDP, which measures a country's gross domestic product, the Shannon Wiener index in ecology, and the Dow Jones and Human Development indices in economics.",
        "question": "What is an index and what are some examples of it?",
        "ground_truths": "An index is a statistical measure of change in a representative group of individual data points used in various fields. Examples include the GDP, which measures a country's gross domestic product, the Shannon Wiener index in ecology, and the Dow Jones and Human Development indices in economics."
    },
    {
        "contexts": "Das Format Ihrer Daten beeinflusst alles Andere, was Sie im weiteren Verlauf Ihrer Arbeit tun. Um ein Sprichwort zu paraphrasieren: Die Daten haben ein Format, und das Format sind die Daten. Deshalb ist es wichtig zu wissen, welche verschiedenen Datenformate es gibt, und wie diese von Vorteil sein k\u00f6nnen, und wo Sie auf Fallstricke sto\u00dfen k\u00f6nnen. Weitere Informationen zu verschiedenen Messmethoden finden Sie im Eintrag To Rule And To Measure (German). Der wichtigste Unterschied besteht zwischen quantitativen Daten und qualitativen Daten. Quantitative Daten k\u00f6nnen aus ganzen Zahlen und diskreten Daten bestehen, w\u00e4hrend qualitative Daten faktoriell - d. h. in wirklich verschiedenen Kategorien -, nominal oder ordinal sein k\u00f6nnen, wobei die beiden letzteren eine Verbindung zu quantitativen Daten herstellen. Die Nomenklatur f\u00fcr Datenformate ist jedoch in den verschiedenen Wissenschaftsbereichen sehr unterschiedlich, und um ehrlich zu sein, ein einziges Durcheinander. Wir versuchen hier, konsistent zu sein, aber seien Sie sich bitte bewusst, dass diese Bezeichnungen in der Wissenschaft nicht einheitlich sind.",
        "summary": "Das Format der Daten beeinflusst die gesamte Arbeit. Es ist wichtig, die verschiedenen Datenformate zu kennen und zu verstehen, wie sie genutzt werden k\u00f6nnen. Der Hauptunterschied besteht zwischen quantitativen und qualitativen Daten. Quantitative Daten bestehen aus ganzen Zahlen und diskreten Daten, w\u00e4hrend qualitative Daten in verschiedenen Kategorien, nominal oder ordinal sein k\u00f6nnen. Die Bezeichnungen f\u00fcr Datenformate sind jedoch in den verschiedenen Wissenschaftsbereichen sehr unterschiedlich.",
        "question": "Was ist der Hauptunterschied zwischen quantitativen und qualitativen Daten?",
        "ground_truths": "Quantitative Daten bestehen aus ganzen Zahlen und diskreten Daten, w\u00e4hrend qualitative Daten in verschiedenen Kategorien, nominal oder ordinal sein k\u00f6nnen."
    },
    {
        "contexts": "Die meisten Datenformate k\u00f6nnen in andere Datenformate umgewandelt werden, was f\u00fcr viele Menschen oft verwirrend ist. Zum Beispiel k\u00f6nnen nominale Daten wiederholt gez\u00e4hlt werden, z. B. die Tassen Kaffee, die Sie jeden Tag trinken. Die Anzahl der Tassen w\u00fcrde sich dann aufaddieren, was diskrete Daten darstellen w\u00fcrde. Ein anderes Beispiel w\u00e4re die Temperatur, die als kontinuierliche Daten in Grad Celsius (besser nicht in Fahrenheit) dargestellt werden k\u00f6nnte. Diese kann zwar in Zahlen dargestellt werden, aber auch als Frosttemperatur oder Temperatur \u00fcber 0 \u00b0C.",
        "summary": "Datenformate k\u00f6nnen oft in andere Formate umgewandelt werden, was verwirrend sein kann. Beispielsweise k\u00f6nnen nominale Daten, wie die Anzahl der t\u00e4glich getrunkenen Tassen Kaffee, zu diskreten Daten werden. Ein weiteres Beispiel ist die Temperatur, die als kontinuierliche Daten in Grad Celsius dargestellt werden kann.",
        "question": "Wie k\u00f6nnen nominale Daten in diskrete Daten umgewandelt werden?",
        "ground_truths": "Nominale Daten, wie die Anzahl der t\u00e4glich getrunkenen Tassen Kaffee, k\u00f6nnen durch wiederholtes Z\u00e4hlen zu diskreten Daten werden."
    },
    {
        "contexts": "Quantitative (numerische) Daten sind Daten, die in Zahlen ausgedr\u00fcckt werden, mit denen sich rechnen l\u00e4sst. Es gibt drei Arten numerischer Daten: Kontinuierliche und diskrete Daten, sowie Intervalldaten.",
        "summary": "Quantitative Daten sind numerische Daten, die in Zahlen ausgedr\u00fcckt werden und mit denen gerechnet werden kann. Es gibt drei Arten von numerischen Daten: kontinuierliche, diskrete und Intervalldaten.",
        "question": "Welche drei Arten von numerischen Daten gibt es?",
        "ground_truths": "Es gibt drei Arten von numerischen Daten: kontinuierliche, diskrete und Intervalldaten."
    },
    {
        "contexts": "Kontinuierliche Daten sind numerische Daten die nicht gez\u00e4hlt werden k\u00f6nnen, weil sie auf einer endlichen oder unendlichen Skala existieren. Wir alle sind mit kontinuierlichen Zahlen vertraut. Ein Gro\u00dfteil unserer Gesellschaft wird von diesen Zahlen beherrscht, und daher wird ein Gro\u00dfteil der in der Statistik analysierten Daten durch kontinuierliche Zahlen dargestellt. Da ein Gro\u00dfteil der modernen Messungen innerhalb eines vorgegebenen Systems automatisiert ist, m\u00fcssen wir uns oft nicht allzu viele Gedanken dar\u00fcber machen, wie die Daten aussehen. Nehmen Sie zum Beispiel Gewicht oder Gr\u00f6\u00dfe. Innerhalb von Mitteleuropa wird dies eindeutig in Gramm oder Kilogramm bzw. in Zentimetern oder Metern gemessen. Wenn Sie jedoch in die USA ziehen, wird es eine ganz andere Geschichte, wegen des metrischen Systems, oder eher dessen mangelnder Nutzung. Pl\u00f6tzlich sind Sie einige Fu\u00df gro\u00df und wiegen vielleicht einige stones. Es gibt viele verschiedene Messsysteme, und man muss sich bewusst sein, wie diese genutzt werden. Diese Systeme sind also Konstrukte, und diese Konstrukte bauen auf kontinuierlichen Zahlen auf. Dies zeigt, dass kontinuierliche Zahlen weit verbreitet sind, um Daten auszudr\u00fccken, aber wir m\u00fcssen uns bewusst sein, dass es sich dabei immer noch um normative Informationen handelt.",
        "summary": "Kontinuierliche Daten sind numerische Daten, die auf einer endlichen oder unendlichen Skala existieren und nicht gez\u00e4hlt werden k\u00f6nnen. Sie sind weit verbreitet und ein Gro\u00dfteil der in der Statistik analysierten Daten wird durch kontinuierliche Zahlen dargestellt. Beispiele sind Gewicht und Gr\u00f6\u00dfe, die in verschiedenen Messsystemen ausgedr\u00fcckt werden k\u00f6nnen, abh\u00e4ngig von der Region.",
        "question": "Was sind kontinuierliche Daten und wo werden sie h\u00e4ufig verwendet?",
        "ground_truths": "Kontinuierliche Daten sind numerische Daten, die auf einer endlichen oder unendlichen Skala existieren und nicht gez\u00e4hlt werden k\u00f6nnen. Sie sind weit verbreitet und ein Gro\u00dfteil der in der Statistik analysierten Daten wird durch kontinuierliche Zahlen dargestellt."
    },
    {
        "contexts": "Diskrete Daten sind numerische Daten die gez\u00e4hlt werden k\u00f6nnen, da sie nur als nat\u00fcrliche Zahlen (1, 2, 3, 4...) vorliegen. Beispiele hierf\u00fcr sind Sch\u00fcler*innen in einer Klasse, oder das eigene Alter. Hier ergibt es keinen Sinn, mit kontinuierlichen Daten zu arbeiten. Nat\u00fcrlich kann man auch an einen halbierten Apfel denken, aber wenn wir \u00c4pfel, V\u00f6gel oder Studierende z\u00e4hlen, betrachten wir sie normalerweise als vollst\u00e4ndige Einheiten und halten uns an nat\u00fcrliche Zahlen. Diskrete Daten werden oft auch als H\u00e4ufigkeits- oder Z\u00e4hldaten bezeichnet, und in der Sprache R werden sie als integer (Ganzzahlen) bezeichnet.",
        "summary": "Diskrete Daten sind numerische Daten, die gez\u00e4hlt werden k\u00f6nnen und nur als nat\u00fcrliche Zahlen vorliegen. Beispiele sind die Anzahl der Sch\u00fcler in einer Klasse oder das eigene Alter. Sie werden oft als H\u00e4ufigkeits- oder Z\u00e4hldaten bezeichnet und in der Programmiersprache R als Ganzzahlen (integer) bezeichnet.",
        "question": "Was sind diskrete Daten und wie werden sie in der Programmiersprache R bezeichnet?",
        "ground_truths": "Diskrete Daten sind numerische Daten, die gez\u00e4hlt werden k\u00f6nnen und nur als nat\u00fcrliche Zahlen vorliegen. Sie werden in der Programmiersprache R als Ganzzahlen (integer) bezeichnet."
    },
    {
        "contexts": "Intervalldaten bestehen aus gemessenen oder gez\u00e4hlten Werten, allerdings gibt es keine echte Null. Au\u00dferdem ist der Unterschied zwischen zwei Messwerten auf der Skala immer gleich gro\u00df, egal, wo man schaut. Das beste Beispiel ist Temperatur, wenn man sie in \u00b0C misst. Der Unterschied zwischen 30\u00b0C und 40\u00b0C ist genauso gro\u00df wie der Unterschied zwischen 100\u00b0C und 110\u00b0C. Allerdings gibt es auf der Celsius-Skala keine echte Null: 0\u00b0C bedeutet nicht, dass es keine Temperatur g\u00e4be. Stattdessen stellt 0\u00b0C einfach einen bestimmten Wert auf der Temperaturskala dar. Daher kann man Temperaturen zwar addieren und subtrahieren, aber nicht sinnvoll multiplizieren oder dividieren. Au\u00dferdem f\u00fchrt dieser Mangel einer echten Null dazu, dass 40\u00b0C nicht doppelt soviel Energie wie 20\u00b0C bedeutet, auch wenn die Zahl doppelt so gro\u00df ist.",
        "summary": "Intervalldaten bestehen aus gemessenen oder gez\u00e4hlten Werten, haben aber keine echte Null. Der Unterschied zwischen zwei Messwerten ist immer gleich, unabh\u00e4ngig von ihrer Position auf der Skala. Ein Beispiel ist die Temperatur in \u00b0C, bei der 0\u00b0C nicht das Fehlen von Temperatur bedeutet, sondern einen bestimmten Wert auf der Skala darstellt.",
        "question": "Was sind Intervalldaten und was ist ein typisches Beispiel daf\u00fcr?",
        "ground_truths": "Intervalldaten bestehen aus gemessenen oder gez\u00e4hlten Werten, haben aber keine echte Null. Ein Beispiel ist die Temperatur in \u00b0C, bei der 0\u00b0C nicht das Fehlen von Temperatur bedeutet, sondern einen bestimmten Wert auf der Skala darstellt."
    },
    {
        "contexts": "Qualitative (kategorische) Daten sind qualitative Daten, die in benannten Kategorien gesammelt werden k\u00f6nnen, die voneinander unabh\u00e4ngig sind. Solche Kategorien sind typischerweise konstruiert und enthalten daher Informationen, die zutiefst normativ oder konstruiert sind. Ein Beispiel w\u00e4re die Haarfarbe, die in der menschlichen Wahrnehmung von Farben stattfinden kann, aber auch bei professionellen Haarprodukten oft mit unterschiedlichen Namen beschrieben wird. Innerhalb der Statistik werden Kategorien oft so gebildet, dass innerhalb eines wissenschaftlichen Experiments die Kategorien in einem Sinne konstruiert werden, der eine sinnvolle Pr\u00fcfung der Hypothese erm\u00f6glicht, und sinnvoll liegt dann im Auge des Betrachters. Unterschiedliche D\u00fcngemittelmengen w\u00e4ren ein solches Beispiel, und die Kategorien werden oft auf Basis von Vorwissen oder Vortests gebildet. Kategorien sind also von besonderer Bedeutung, wenn es um die Reduktion der Komplexit\u00e4t der Welt geht, da es nicht m\u00f6glich w\u00e4re, alle m\u00f6glichen unterschiedlichen D\u00fcngemittelmengen in einem Experiment zu testen. Stattdessen entscheidet man sich z.B. f\u00fcr \"wenig\", \"moderat\", \"viel\" und \"sehr viel\" D\u00fcnger. Dennoch muss man sich dar\u00fcber im Klaren sein, dass - und wie - Kategorien konstruiert und damit zutiefst normativ sind.",
        "summary": "Qualitative Daten sind in benannten Kategorien gesammelt, die unabh\u00e4ngig voneinander sind und oft konstruiert werden. Sie enthalten normative Informationen, wie zum Beispiel Haarfarbe oder D\u00fcngemittelmengen. In der Statistik werden Kategorien so gebildet, dass sie eine sinnvolle Pr\u00fcfung der Hypothese in einem wissenschaftlichen Experiment erm\u00f6glichen. Kategorien sind wichtig f\u00fcr die Reduktion der Komplexit\u00e4t der Welt.",
        "question": "Was sind qualitative Daten und warum sind sie wichtig in der Statistik?",
        "ground_truths": "Qualitative Daten sind Daten, die in benannten, unabh\u00e4ngigen Kategorien gesammelt werden. Sie sind oft konstruiert und enthalten normative Informationen. In der Statistik sind sie wichtig, weil sie eine sinnvolle Pr\u00fcfung der Hypothese in einem wissenschaftlichen Experiment erm\u00f6glichen und helfen, die Komplexit\u00e4t der Welt zu reduzieren."
    },
    {
        "contexts": "Ordinale Daten sind kategorische Daten, die in eine Reihenfolge gebracht werden k\u00f6nnen, mit denen sich aber nicht rechnen l\u00e4sst, selbst wenn sie als Zahlen ausgedr\u00fcckt werden. Erinnern Sie sich an Ihre Schulnoten? Eine \"1\" ist die beste Note im deutschen Notensystem, aber ist sie doppelt so gut wie eine \"2\"? Wohl kaum. Solche Noten sind ordinale Zahlen. Es handelt sich dabei um ein System von Zahlen, die in gewisser Weise geordnet sind, aber die Zahlen an sich spiegeln nicht unbedingt ein numerisches System wider. Mit anderen Worten: Sie sind h\u00f6chst normativ und umstritten. Eine \"2\" mag f\u00fcr die einen eine gute Note sein, f\u00fcr die anderen eine Katastrophe. Ordinale Formate sind oft klar definierte Skalen, die es Menschen erm\u00f6glichen, bestimmte Informationen zu benoten, zu bewerten oder in eine Rangfolge zu bringen. Eines der bekanntesten Beispiele ist die Likert-Skala, die h\u00e4ufig in der Psychologie verwendet wird. In diesem Fall wird die Skalierung oft gar nicht in Zahlen wiedergegeben, sondern in Stufen wie \"stimme voll zu\" oder \"stimme eher nicht zu\". Solche konstruierten Skalen k\u00f6nnen echten Statistiker*innen sehr ungl\u00fccklich machen, da die Ergebnisse schwer zu analysieren sind, aber es gibt kaum eine Alternative, da es auch keinen Sinn macht, zu fragen: \"Wie gl\u00fccklich sind Sie auf einer Skala von 1 bis 100?\" Daher sind Ordinalskalen oft relevant, um ein Skalensystem zu schaffen, das eine breite Vergleichbarkeit erm\u00f6glicht oder sogar zur Norm wird, wie z.B. Schulnoten.",
        "summary": "Ordinale Daten sind kategorische Daten, die in eine Reihenfolge gebracht werden k\u00f6nnen, aber nicht f\u00fcr Rechenoperationen geeignet sind. Sie sind normativ und umstritten, wie zum Beispiel Schulnoten. Sie werden oft in klar definierten Skalen ausgedr\u00fcckt, wie der Likert-Skala in der Psychologie. Ordinale Skalen sind relevant, um ein Skalensystem zu schaffen, das eine breite Vergleichbarkeit erm\u00f6glicht oder sogar zur Norm wird.",
        "question": "Was sind ordinale Daten und wie werden sie verwendet?",
        "ground_truths": "Ordinale Daten sind kategorische Daten, die in eine Reihenfolge gebracht werden k\u00f6nnen, aber nicht f\u00fcr Rechenoperationen geeignet sind. Sie sind oft normativ und umstritten. Sie werden in klar definierten Skalen ausgedr\u00fcckt, wie zum Beispiel der Likert-Skala in der Psychologie, und sind relevant, um ein Skalensystem zu schaffen, das eine breite Vergleichbarkeit erm\u00f6glicht oder sogar zur Norm wird."
    },
    {
        "contexts": "Wann immer Sie kategorische Daten haben, die nicht in eine Rangfolge gebracht werden k\u00f6nnen, nennt man sie nominale Daten. Ein Beispiel w\u00e4ren verschiedene Ethnien, Geburtsl\u00e4nder, oder verschiedene Arten von Geschlechtern. Dies verdeutlicht bereits, dass wir es hier mit oft v\u00f6llig unterschiedlichen Weltanschauungen zu tun haben, sodass nominale Daten einen krassen Fall einer normativen Sicht auf die Welt darstellen. Das Geschlecht ist ein prominentes Beispiel, da manche Menschen das Geschlecht immer noch \u00fcber ein biologisches Stereotyp (weiblich/m\u00e4nnlich) und damit bin\u00e4r (siehe unten) definieren, was nach meinem Weltbild eindeutig falsch ist, weshalb ich Geschlecht nominal mit mehr als zwei Kategorien definieren w\u00fcrde. Nominale Datenformate verlangen daher eine noch deutlichere Reflexion als ordinale Daten, bei denen man zumindest sagen kann, dass eine bestimmte Schulnote h\u00f6her ist als eine andere. Das ist bei nominalen Daten nicht der Fall. Deshalb muss man besonders vorsichtig sein mit den Implikationen, die eine bestimmte konstruierte Skala implizieren kann.",
        "summary": "Nominale Daten sind kategorische Daten, die nicht in eine Rangfolge gebracht werden k\u00f6nnen, wie zum Beispiel verschiedene Ethnien, Geburtsl\u00e4nder oder Geschlechter. Sie stellen eine normative Sicht auf die Welt dar und erfordern eine deutliche Reflexion. Im Gegensatz zu ordinalen Daten, bei denen eine Rangfolge besteht, gibt es bei nominalen Daten keine solche Hierarchie. Daher muss man vorsichtig mit den Implikationen einer konstruierten Skala sein.",
        "question": "Was sind nominale Daten und wie unterscheiden sie sich von ordinalen Daten?",
        "ground_truths": "Nominale Daten sind kategorische Daten, die nicht in eine Rangfolge gebracht werden k\u00f6nnen, wie zum Beispiel verschiedene Ethnien, Geburtsl\u00e4nder oder Geschlechter. Sie stellen eine normative Sicht auf die Welt dar und erfordern eine deutliche Reflexion. Im Gegensatz zu ordinalen Daten, bei denen eine Rangfolge besteht, gibt es bei nominalen Daten keine solche Hierarchie."
    },
    {
        "contexts": "Bin\u00e4re Daten sind das am meisten reduzierte Datenformat, das grunds\u00e4tzlich aus zwei Ebenen besteht: 1 und 0. Streng genommen sind bin\u00e4re Daten nominale Daten, aber eben nominale Daten, die nur in zwei Varianten vorliegen, die sich in 1 und 0 \u00fcbersetzen lassen: An / Aus, Ja / Nein. In der Informatik werden bin\u00e4re Daten direkt als einfache 0 und 1 genutzt, aber der gro\u00dfe Durchbruch dieses Datensatzes kam schon fr\u00fch in der Versicherungsbranche sowie in der Medizin, wo \"tot\" oder \"lebendig\" oft die grundlegendsten Fragen sind. Bin\u00e4re Informationen sind eindeutig vereinfachend, was aber oft mit einer bestimmten Sicht der Realit\u00e4t \u00fcbereinstimmt. Nehmen Sie das Beispiel, ein Instrument spielen zu k\u00f6nnen. Wenn Sie jemand fragt, ob Sie Klavier spielen k\u00f6nnen, werden Sie wahrscheinlich ja oder nein sagen. Sie werden Ihre Antwort h\u00f6chstwahrscheinlich nicht qualifizieren, indem Sie sagen \"Ich spiele besser als ein Affe, aber schlechter als Horowitz\". Einige bescheidene Leute sagen vielleicht \"Ich kann ein bisschen spielen\", oder \"Ich bin nicht sehr gut\", oder \"Ich war mal besser\", aber sehr oft antworten Menschen mit ja oder nein. Bin\u00e4re Daten erlauben also eine einfache Sicht auf die Realit\u00e4t, und diese mag oft mit der Welt \u00fcbereinstimmen, wie wir sie wahrnehmen. Aber seien Sie sich bewusst: Andere Menschen haben vielleicht eine weniger einfache Sichtweise.",
        "summary": "Bin\u00e4re Daten sind das am meisten reduzierte Datenformat, das aus zwei Ebenen besteht: 1 und 0. Sie sind eine Art nominale Daten, die nur in zwei Varianten vorliegen. Sie werden oft in der Informatik, Versicherungsbranche und Medizin verwendet. Bin\u00e4re Daten erlauben eine einfache Sicht auf die Realit\u00e4t, wie zum Beispiel die Frage, ob man ein Instrument spielen kann, die oft mit ja oder nein beantwortet wird.",
        "question": "Was sind bin\u00e4re Daten und wie werden sie in der Praxis verwendet?",
        "ground_truths": "Bin\u00e4re Daten sind das am meisten reduzierte Datenformat, das aus zwei Ebenen besteht: 1 und 0. Sie sind eine Art nominale Daten, die nur in zwei Varianten vorliegen. Sie werden oft in der Informatik, Versicherungsbranche und Medizin verwendet. Sie erlauben eine einfache Sicht auf die Realit\u00e4t, wie zum Beispiel die Frage, ob man ein Instrument spielen kann, die oft mit ja oder nein beantwortet wird."
    },
    {
        "contexts": "Sie fragen sich jetzt vielleicht, wie Sie das richtige Datenformat ausw\u00e4hlen. Die Antwort darauf ist ganz einfach. Jedes Datenformat sollte so einfach wie m\u00f6glich und so komplex wie n\u00f6tig sein. Folgen Sie Occams Rasiermesser, und Sie werden gut zurechtkommen. Das klingt nat\u00fcrlich verlockend, aber woher wei\u00df man, was zu einfach und was zu komplex ist? Hier schlage ich vor, dass Sie sich auf die vorhandene Literatur st\u00fctzen. Lesen Sie andere Ver\u00f6ffentlichungen, die ein bestimmtes Ph\u00e4nomen bereits untersucht haben, diese Ver\u00f6ffentlichungen k\u00f6nnen Ihnen bei der Wahl der richtigen Skala helfen.",
        "summary": "Die Auswahl des richtigen Datenformats kann durch das Prinzip von Occams Rasiermesser geleitet werden, das besagt, dass es so einfach wie m\u00f6glich und so komplex wie n\u00f6tig sein sollte. Um zu bestimmen, was zu einfach oder zu komplex ist, kann man sich auf vorhandene Literatur und fr\u00fchere Studien st\u00fctzen.",
        "question": "Wie kann man das richtige Datenformat ausw\u00e4hlen?",
        "ground_truths": "Das richtige Datenformat kann durch das Prinzip von Occams Rasiermesser ausgew\u00e4hlt werden, das besagt, dass es so einfach wie m\u00f6glich und so komplex wie n\u00f6tig sein sollte. Um zu bestimmen, was zu einfach oder zu komplex ist, kann man sich auf vorhandene Literatur und fr\u00fchere Studien st\u00fctzen."
    },
    {
        "contexts": "In der Wirtschafts- und Finanzwelt ist ein Index ein statistisches Ma\u00df f\u00fcr die Ver\u00e4nderung einer repr\u00e4sentativen Gruppe von einzelnen Datenpunkten. Ein gutes Beispiel f\u00fcr die Anwendung eines Indexes, den die meisten Menschen kennen, ist das BIP, das Bruttoinlandsprodukt eines Landes. Obwohl es weitgehend kritisiert wurde, weil es zu allgemein gehalten ist und nicht gen\u00fcgend Nuancen bietet, um die Komplexit\u00e4t des einzelnen Landes zu verstehen, sind viele soziale, wirtschaftliche und andere Indikatoren mit dem BIP korreliert.",
        "summary": "Ein Index ist in der Wirtschaft und Finanzwelt ein statistisches Ma\u00df f\u00fcr die Ver\u00e4nderung einer repr\u00e4sentativen Gruppe von Datenpunkten. Ein bekanntes Beispiel ist das Bruttoinlandsprodukt (BIP) eines Landes, das trotz Kritik an seiner Allgemeinheit und mangelnder Nuancen, mit vielen sozialen und wirtschaftlichen Indikatoren korreliert.",
        "question": "Was ist ein Index in der Wirtschaft und Finanzwelt und welches ist ein bekanntes Beispiel daf\u00fcr?",
        "ground_truths": "Ein Index ist in der Wirtschaft und Finanzwelt ein statistisches Ma\u00df f\u00fcr die Ver\u00e4nderung einer repr\u00e4sentativen Gruppe von Datenpunkten. Ein bekanntes Beispiel ist das Bruttoinlandsprodukt (BIP) eines Landes."
    },
    {
        "contexts": "In der \u00d6kologie ist ein bekanntes Beispiel f\u00fcr einen Index der so genannte Shannon Wiener index, der Diversit\u00e4tsma\u00dfe darstellt, welche hinsichtlich der H\u00e4ufigkeit bestimmter Spezies angepasst sind. Ein prominentes Beispiel aus der Wirtschaft ist wiederum der Dow Jones index, w\u00e4hrend der Index der menschlichen Entwicklung versucht, Informationen \u00fcber Lebenserwartung, Bildung und Einkommen zu integrieren, um ein allgemeines Verst\u00e4ndnis f\u00fcr verschiedene Komponenten zu erhalten, die L\u00e4nder charakterisieren. Der GINI-Koeffizient versucht, die Ungleichheit zu messen, was sicherlich ein gewagtes Unterfangen ist, aber dennoch sehr wichtig. In der Psychologie ist der Intelligenzquotient, der nat\u00fcrlich stark kritisiert wird, ein bekanntes Beispiel f\u00fcr die Reduzierung vieler komplexer Tests auf eine Gesamtzahl. Indizes und Quotienten sind also Konstrukte, die oft auf vielen Variablen beruhen und versuchen, die Komplexit\u00e4t dieser vielf\u00e4ltigen Indikatoren auf eine einzige Zahl zu reduzieren.",
        "summary": "Es gibt verschiedene Arten von Indizes in verschiedenen Bereichen. In der \u00d6kologie ist der Shannon Wiener Index bekannt, in der Wirtschaft der Dow Jones Index und der Index der menschlichen Entwicklung, der GINI-Koeffizient misst Ungleichheit und in der Psychologie ist der Intelligenzquotient bekannt. Alle diese Indizes versuchen, komplexe Daten auf eine einzige Zahl zu reduzieren.",
        "question": "Welche Arten von Indizes gibt es in verschiedenen Bereichen und was versuchen sie zu messen?",
        "ground_truths": "In der \u00d6kologie ist der Shannon Wiener Index bekannt, in der Wirtschaft der Dow Jones Index und der Index der menschlichen Entwicklung, der GINI-Koeffizient misst Ungleichheit und in der Psychologie ist der Intelligenzquotient bekannt. Alle diese Indizes versuchen, komplexe Daten auf eine einzige Zahl zu reduzieren."
    },
    {
        "contexts": "Structuralism, which emerged at the beginning of the 20th century, reached its peak in France in the 1960s. Structuralism, grounded in the theory of signs by the Swiss linguist Ferdinand de Saussure (3), is particularly concerned with the assumption that there are underlying structures to everything that produce objective, definitive meaning and provide the framework within which actions can be performed. Poststructuralism, which began to emerge in the mid-1960s, called into question the existence of such structures. It was now necessary to break up the previously assumed stable structures and let them collapse (3). An important approach to this was deconstruction, which goes back to the French philosopher Jacques Derrida (1930-2004). He first spoke of deconstruction in his book Of grammatology, published in 1967.",
        "summary": "Structuralism, a 20th-century theory by Ferdinand de Saussure, posits that underlying structures produce definitive meaning. Poststructuralism, emerging in the mid-1960s, questioned these structures. Deconstruction, an approach by Jacques Derrida, aimed to break up these structures.",
        "question": "Who first introduced the concept of deconstruction and in what contexts?",
        "ground_truths": "The French philosopher Jacques Derrida first introduced the concept of deconstruction in his book 'Of grammatology', published in 1967."
    },
    {
        "contexts": "With his approach of deconstruction, Derrida criticized structuralism and illustrated this with the example of Saussure's sign system (7). Fundamental to Saussure's sign system is the assumption that a linguistic sign always consists of a signified  (e.g., an object), and a signifier (e.g., a word for this object). The figure on the right exemplifies the linguistic sign tree with its two components - the content level (signified) and the expression level (signifier). According to Saussure, a signifier is assigned - arbitrarily but nevertheless unambiguously - to a signified, and the meaning of the signified results from the demarcation of the signifier from other signifiers - often as binary opposites (3, 6). The idea is that the tree  - the signified - can be unambiguously explained by the word \u201etree\u201c \u2013 the signifier - because the word \u201etree\u201c can clearly be differentiated from other words.",
        "summary": "Derrida's deconstruction criticized structuralism using Saussure's sign system. Saussure's system posits that a linguistic sign consists of a signified (object) and a signifier (word for the object), with the meaning of the signified resulting from its differentiation from other signifiers.",
        "question": "What is the fundamental assumption of Saussure's sign system?",
        "ground_truths": "The fundamental assumption of Saussure's sign system is that a linguistic sign always consists of a signified (an object) and a signifier (a word for this object), and the meaning of the signified results from its differentiation from other signifiers."
    },
    {
        "contexts": "Derrida saw a problem in this unambiguous assignment and the resulting binary opposition structure. For Derrida, signifier and signified do not stand in an unambiguous relationship; rather, their positions in the system can change. That is, a signified can become a signifier and vice versa. For the example of tree this can be explained as follows: If the sign tree is unknown in its meaning, one can look up the signifier  T R E E and receive a (simplified) meaning, i.e. the signified, plant. At the same time, plant in this case is also another linguistic sign. If one now also does not know the meaning of the sign plant, one again looks up the signifier P L A N T. This way, plant, which was originally the signified for the sign \u201etree\u201c becomes the signifier of another sign with ist own signified. Consequently, the previously assumed/existing structure of the sign is not rigid and unambiguous, but unstable and movable (3).",
        "summary": "Derrida critiqued the unambiguous assignment in Saussure's sign system, arguing that signifier and signified do not have a fixed relationship and can interchange. This makes the structure of the sign not rigid but unstable and movable.",
        "question": "What was Derrida's critique of Saussure's sign system?",
        "ground_truths": "Derrida critiqued the unambiguous assignment and binary opposition structure in Saussure's sign system. He argued that the relationship between signifier and signified is not fixed, and their positions can interchange, making the structure of the sign unstable and movable."
    },
    {
        "contexts": "In addition, Derrida assumed that an unambiguous meaning could never be achieved, since every sign with its signifier always refers to other signs that either precede or follow it, and accordingly deconstructs and shifts its own meaning (7). He therefore introduced the neologism \u201adiff\u00e9rance\u2018. Diff\u00e9rance describes the never-ending shift in meaning that results from the instability and ambiguity of concepts. It highlights that a term can have several meanings or its meaning can be shifted, depending on the contexts in which it exists and is read (5).",
        "summary": "Derrida believed that unambiguous meaning is unachievable as every sign refers to other signs, thus shifting its own meaning. He introduced the term 'diff\u00e9rance' to describe this never-ending shift in meaning due to the instability and ambiguity of concepts.",
        "question": "What does the term 'diff\u00e9rance' introduced by Derrida signify?",
        "ground_truths": "'Diff\u00e9rance', a term introduced by Derrida, describes the never-ending shift in meaning that results from the instability and ambiguity of concepts."
    },
    {
        "contexts": "In this sense, a sign is to be understood as a construct and deconstruction offers a possibility to deal with it (5). The main point is to emphasize the non-naturalness, the non-originality of a term and to show the external influences that have shaped it in the course of time (5). That is, terms can never depict absolutely unambiguously and universally what they try to depict, since they usually once held a completely different - or at least slightly shifted - meaning. Thus, they unconsciously always say more or something else than they want to say. With deconstruction, this historicity - Derrida spoke of \u201atrace\u2018 - of concepts is to be uncovered and made visible. It shall be shown that there is neither a first, original nor a last, final meaning (5).",
        "summary": "Signs are constructs and deconstruction is a way to handle them. It emphasizes the non-originality of terms and their external influences. Terms can't depict unambiguously what they try to depict as they once held different meanings. Deconstruction uncovers this historicity or 'trace' of concepts, showing there's no original or final meaning.",
        "question": "What is the main purpose of deconstruction according to Derrida?",
        "ground_truths": "The main purpose of deconstruction, according to Derrida, is to emphasize the non-originality of terms, show their external influences, and uncover the historicity or 'trace' of concepts. It shows that terms can't depict unambiguously what they try to depict as they once held different meanings, and that there's no original or final meaning."
    },
    {
        "contexts": "Especially the rigid, binary opposition structures of structuralism (e.g. language/writing, man/woman, reason/madness, heterosexuality/homosexuality) are supposed to be broken up with the help of deconstruction. Derrida perceived this in its strict hierarchy of values as not neutral and consequently not logical (3). Due to the hierarchical structure, one part would always be perceived as original/normal/superior, while the other part would be seen as derivative and thus less valuable. However, Derrida's goal in deconstruction was not to completely abandon the concepts or to reverse the hierarchy of values, because in his opinion the concepts depend on each other and constitute each other in the first place. Instead, deconstruction should shift the entire system, which with its rigid structures enables the formation of such hierarchies, by making visible the instability and ambiguity of the individual concepts (2).",
        "summary": "Deconstruction aims to break up the rigid, binary opposition structures of structuralism, which Derrida saw as non-neutral and illogical due to their hierarchical nature. Derrida didn't aim to abandon or reverse these hierarchies, but to shift the entire system by highlighting the instability and ambiguity of concepts.",
        "question": "What is the goal of deconstruction in relation to the binary opposition structures of structuralism?",
        "ground_truths": "The goal of deconstruction in relation to the binary opposition structures of structuralism is not to abandon or reverse these hierarchies, but to break up these rigid structures and shift the entire system by highlighting the instability and ambiguity of individual concepts."
    },
    {
        "contexts": "Deconstruction, however, was conceived by Derrida less as a method or some tool that you apply to something from the outside (Derrida & Caputo 2020, p.9). While scientific methods are seen as settled and stable (Feustel 2015, p.15), deconstruction instead emphasizes the instability not only of the respective object, but also of one's own approach and goal (5). Accordingly, deconstruction is to be seen more as an art of reading, of shifting meaning, which occurs from within but must be made visible (4). Derrida's deconstruction influenced, especially in the USA, the so-called Yale School, whose representatives included Paul de Man, Harold Bloom, Geoffrey Hartman and Hillis Miller. They conceived of deconstruction as a poststructuralist reading for the analysis and critique of texts, and developed it further in their own work (1).",
        "summary": "Derrida saw deconstruction less as a method and more as an art of reading and shifting meaning from within. Unlike stable scientific methods, deconstruction emphasizes instability. It influenced the Yale School in the USA, who saw it as a poststructuralist reading for text analysis and critique.",
        "question": "How did Derrida conceive deconstruction and who did it influence?",
        "ground_truths": "Derrida conceived deconstruction less as a method and more as an art of reading and shifting meaning from within, emphasizing instability. It influenced the Yale School in the USA, who saw it as a poststructuralist reading for text analysis and critique."
    },
    {
        "contexts": "The Delphi method originates from work at RAND Corporation, a US think-tank that advises the US military, in the late 1940s and 1950s (2, 3, 5). RAND developed \"Project Delphi\" as a mean of obtaining \"(...) the most reliable consensus of opinion of a group of experts.\" (Dalkey & Helmer 1963, p.1). At the time, the alternative - extensive gathering and analysis of quantitative data as a basis for forecasting and deliberating on future issues - was not technologically feasible (4, 5). Instead, experts were invited and asked for their opinions - and Delphi was born (see (1)).",
        "summary": "The Delphi method was developed by the RAND Corporation, a US think-tank, in the late 1940s and 1950s as a way to gather reliable consensus from a group of experts. The method was born out of the need for an alternative to the technologically unfeasible task of extensive data gathering and analysis for forecasting future issues.",
        "question": "When and why was the Delphi method developed?",
        "ground_truths": "The Delphi method was developed in the late 1940s and 1950s by the RAND Corporation as an alternative to the technologically unfeasible task of extensive data gathering and analysis for forecasting future issues."
    },
    {
        "contexts": "The Delphi method is \"(...) a systematic and interactive research technique for obtaining the judgment of a panel of independent experts on a specific topics\" (Hallowell & Gambatese 2010, p.99). It is used \"(...) to obtain, exchange, and develop informed opinion on a particular topic\" and shall provide \"(...) a constructive forum in which consensus may occur\" (Rayens & Hahn 2000, p.309). Put simply, experts on a topic are gathered and asked in a systematic process what they think about the future, until consensus is found.",
        "summary": "The Delphi method is a systematic and interactive research technique used to gather, exchange, and develop informed opinions from a panel of independent experts on a specific topic. The goal is to facilitate a constructive forum where consensus can be reached.",
        "question": "What is the purpose of the Delphi method?",
        "ground_truths": "The purpose of the Delphi method is to gather, exchange, and develop informed opinions from a panel of independent experts on a specific topic and to facilitate a constructive forum where consensus can be reached."
    },
    {
        "contexts": "A Delphi process typically undergoes four phases (see (4), (6)): 1. A group of experts / stakeholders on a specific issue is identified and invited as participants for the Delphi. These participants represent different backgrounds: academics, government and non-government officials as well, as practitioners. They should have a diverse set of perspectives and profound knowledge on the discussed issues. They may be grouped based on their organizations, skills, disciplines or qualifications (3). Their number typically ranges from 10 up to 30, depending on the complexity of the issue (2, 3, 5, 6). 2. The researchers then develop a questionnaire. It is informed by previous research as well as input from external experts (not the participants) who are asked to contribute knowledge and potential questions on the pertinent issue (2, 5). The amount of consultation depends on the expertise of the researchers on the respective issue (2). 3. The questionnaire is used to ask for the participants' opinions and positions related to the underlying issue. The questions often take a 'ranking-type' (3): they ask about the likelihood of potential future situations, the desirability of certain goals, the importance of specific issues and the feasibility of potential policy options. Participants may be asked to rank the answer options, e.g. from least to most desirable, least to most feasible etc. (2). Participants may also be asked yes/no questions, or to provide an estimate as a number. They can provide further information on their answers in written form. (8) 4. Finally, the results of the process are summarized and evaluated for all participants (4).",
        "summary": "The Delphi process typically involves four phases: 1. Identification and invitation of a diverse group of experts on a specific issue. 2. Development of a questionnaire informed by previous research and input from external experts. 3. Use of the questionnaire to gather participants' opinions and positions on the issue, often using ranking-type questions. 4. Summarization and evaluation of the results for all participants.",
        "question": "What are the four phases of the Delphi process?",
        "ground_truths": "The four phases of the Delphi process are: 1. Identification and invitation of a diverse group of experts on a specific issue. 2. Development of a questionnaire informed by previous research and input from external experts. 3. Use of the questionnaire to gather participants' opinions and positions on the issue, often using ranking-type questions. 4. Summarization and evaluation of the results for all participants."
    },
    {
        "contexts": "The literature indicates a variety of benefits that the Delphi method offers. Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process. Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics.",
        "summary": "The Delphi method offers several benefits. It emerged due to a lack of data for traditional forecasting methods and is still chosen for the same reason. It's also chosen when the collective judgment of experts is beneficial to problem-solving. Delphi can be used as group counseling when other forms are not feasible or detrimental due to counterproductive group dynamics.",
        "question": "Why is the Delphi method chosen over traditional forecasting methods?",
        "ground_truths": "The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving."
    },
    {
        "contexts": "However, several potential pitfalls and challenges may arise during the Delphi process: Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue. A specific Delphi format that was useful in one study must not work as well in another contexts. Instead, the process must be adapted to the research design and underlying problem. The proper selection of participating experts constitutes a major challenge for Delphi processes. In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors.",
        "summary": "Several challenges may arise during the Delphi process. It should not be used as a surrogate for all types of communication and is not feasible for every issue. A Delphi format that worked in one study may not work in another contexts. The process must be adapted to the research design and problem. Selecting participating experts is a major challenge, and researchers should be aware that experts may forecast based on their specific perspective and neglect other factors.",
        "question": "What are some challenges that may arise during the Delphi process?",
        "ground_truths": "Some challenges that may arise during the Delphi process include its infeasibility for every issue, the need to adapt the process to the research design and problem, and the difficulty in selecting participating experts. Researchers should also be aware that experts may forecast based on their specific perspective and neglect other factors."
    },
    {
        "contexts": "While Delphi is a common forecasting method, backcasting methods (such as Visioning) or Scenario Planning may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting. Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes. Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).",
        "summary": "Delphi is a common forecasting method, but backcasting methods like Visioning or Scenario Planning can also be used to evaluate future scenarios without the issues associated with forecasting. Delphi can be a starting point for further research and can be combined with qualitative or quantitative methods before and after the process to gain deeper insights and gather more data.",
        "question": "How can the Delphi method be integrated with other research methods?",
        "ground_truths": "The Delphi method can be integrated with other research methods by using it as a starting point for further research. It can also be combined with qualitative or quantitative methods before and after the process to gain deeper insights into the problem and gather more data."
    },
    {
        "contexts": "In their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets. Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks' foreign net assets etc.) with simple point estimates. These variables were chosen to fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible. (p.316). The participants were provided with information on the past developments of each of these variables.",
        "summary": "Kauko & Palmroos conducted a Delphi process with financial experts in Finland in 2014. The participants, from the Bank of Finland and the Financial Supervisory Authority of Finland, were asked to forecast 15 different variables related to domestic financial markets. The variables were chosen based on the participants' expertise and were as independent as possible.",
        "question": "What was the purpose of the Delphi process conducted by Kauko & Palmroos in 2014?",
        "ground_truths": "The purpose of the Delphi process conducted by Kauko & Palmroos in 2014 was to forecast 15 different variables related to domestic financial markets in Finland."
    },
    {
        "contexts": "The researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. The responses from the respective previous round were re-distributed to the participants (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again.",
        "summary": "The researchers conducted three rounds of questionnaires until consensus was reached. The first round was distributed by mail, and the participants had a week to respond. The second and third rounds were held on the same day, with the responses from the previous round redistributed to the participants for consideration in their new forecasts.",
        "question": "How were the three rounds of the Delphi process conducted by the researchers?",
        "ground_truths": "The three rounds of the Delphi process were conducted by distributing questionnaires to the participants. The first round was distributed by mail and participants had a week to respond. The second and third rounds were held on the same day, with the responses from the previous round redistributed to the participants for consideration in their new forecasts."
    },
    {
        "contexts": "The forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. They found that the individual responses did indeed converge over the Delphi process, and that the Delphi group improved between rounds 1 and 3 in 13 of the questions. (p.320). They also found that disagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement (p.322) and that the Delphi process clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future, which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that paying attention to each others' answers made the forecasts more accurate (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.",
        "summary": "The results of the Delphi process were verified with real developments over time. The researchers found that the responses converged and improved over the process, with the Delphi group outperforming simple trend extrapolations. The researchers also found that paying attention to others' answers improved the accuracy of the forecasts, and that participants were able to assess their own accuracy. This shows that the Delphi method leads to more accurate results over time.",
        "question": "What were the findings of the Delphi process conducted by the researchers?",
        "ground_truths": "The findings of the Delphi process conducted by the researchers were that the responses converged and improved over the process, with the Delphi group outperforming simple trend extrapolations. They also found that paying attention to others' answers improved the accuracy of the forecasts, and that participants were able to assess their own accuracy."
    },
    {
        "contexts": "Various scientists had been aware of tree rings and their connection to climate for several centuries already, however the term \u2018dendrochronology\u2019 was coined by A. E. Douglass from the University of Arizona in 1929. He used the method of tree-ring dating to determine the age of ancient indigenous buildings, which helped the Navajo peoples\u2019 compensatory claims to succeed (2). Tree ring analysis or dendrochronology is applied in many disciplines like climatology, archaeology, biology, hydrology and forestry. It can be applied to any plant species that has a woody growth type (including shrubs) and any woody parts of the plants (including branches, twigs and roots). Seasonal growth, the number of growth rings and the ring widths, is determined by the interaction of genetic and environmental factors. For example, a poplar forms wider rings than a bilberry growing under the same climatic conditions (3). Furthermore, in order to analyse seasonal growth rings only one environmental factor must dominate in limiting the growth. This limiting factor can be precipitation in arid, semi-arid or tropical parts of the world or temperature like in the temperate regions (e. g. dry and wet season in tropical forests, winter and summer in temperate forests).",
        "summary": "The term 'dendrochronology' was coined by A. E. Douglass in 1929, who used tree-ring dating to determine the age of ancient buildings. This method is used in various disciplines and can be applied to any plant species with a woody growth type. The number and width of growth rings are determined by genetic and environmental factors, with one environmental factor dominating in limiting the growth, such as precipitation or temperature.",
        "question": "Who coined the term 'dendrochronology' and what is its application?",
        "ground_truths": "A. E. Douglass coined the term 'dendrochronology' in 1929. It is used in various disciplines like climatology, archaeology, biology, hydrology and forestry and can be applied to any plant species with a woody growth type."
    },
    {
        "contexts": "The structure we see as rings is a sequence of earlywood and latewood in the secondary xylem tissue of a stem. Secondary growth reflects an increase in thickness, or lateral additions of new tissue, and essentially this secondary xylem is an important resource known as wood. The xylem is the water and nutrient conducting tissue in vascular plants. Cells for the secondary xylem are generated by the cambium layer. The cambium is a zone of undifferentiated cells that produce cells of the xylem to the centre of the stem and cells of phloem to the outer part of the stem. The phloem is the plant tissue for transport of sugars (assimilates). Xylem and phloem together form the vascular system of plants throughout the stems, branches and roots. For every vegetation period, governed by one limiting environmental factor, the cambium generates xylem to the inside of the stem and phloem to the outside. The cambium will generate more secondary xylem than phloem and old outer phloem tissue will be crushed and eventually become bark. This is why woody species accumulate more and more secondary xylem each year and not secondary phloem. The oldest rings are close to the centre of the tree stem, the youngest near the cambium and bark (Fig. 3).",
        "summary": "Tree rings are a sequence of earlywood and latewood in the secondary xylem tissue of a stem, reflecting an increase in thickness. The xylem, generated by the cambium layer, is the water and nutrient conducting tissue in plants. The cambium also produces phloem, which transports sugars. Together, xylem and phloem form the vascular system of plants. Each vegetation period, the cambium generates more xylem than phloem, leading to an accumulation of secondary xylem each year.",
        "question": "What are tree rings and how are they formed?",
        "ground_truths": "Tree rings are a sequence of earlywood and latewood in the secondary xylem tissue of a stem, reflecting an increase in thickness. They are formed by the cambium layer, which generates xylem and phloem. Each vegetation period, the cambium generates more xylem than phloem, leading to an accumulation of secondary xylem each year."
    },
    {
        "contexts": "Dendrochronologists are interested in the chronological sequence of growth rings. In order to establish a dendrochronology of a certain area, scientists compare tree rings of individuals of one species. For this purpose, this species must only add one ring per growing season, the growth-limiting climatic factor must vary in intensity from year to year, and should be uniformly effective over a large geographic area (i. e. the relative annual difference in intensity is similar throughout the macroclimatic region). The variation in the growth-limiting factor must be reflected in the width of the tree rings. If these conditions are fulfilled scientists can recognize specific sequences that are of use for cross-dating or matching ring patterns between specimens (see (2) for an introduction and Fig. 5). Which species makes most sense to study depends on many factors, like the research objective, conditions in the study area, abundance and properties of the species.",
        "summary": "Dendrochronologists study the chronological sequence of growth rings to establish a dendrochronology of a certain area. This requires comparing tree rings of one species that adds one ring per growing season, with the growth-limiting climatic factor varying in intensity from year to year and being uniformly effective over a large area. The variation in this factor must be reflected in the tree ring widths. If these conditions are met, scientists can use the sequences for cross-dating or matching ring patterns.",
        "question": "What are the requirements for establishing a dendrochronology of a certain area?",
        "ground_truths": "To establish a dendrochronology of a certain area, scientists need to compare tree rings of one species that adds one ring per growing season. The growth-limiting climatic factor must vary in intensity from year to year and be uniformly effective over a large area. The variation in this factor must be reflected in the tree ring widths."
    },
    {
        "contexts": "Dendrochronology is a method of data gathering and analysis. The findings can then be interpreted, for instance in regards to tree wellbeing or climate. To collect the data, core sampling of living trees is performed with an increment borer (Fig 6). The increment borer consists of a handle, a borer and an extraction spoon. This tool removes a sample from the stem leaving a hole of about 5 mm wide half the stem diameter deep. This hole quickly fills with sap; however, it is still an invasive method that could allow parasites or fungi to enter the bore hole. Therefore, the tool should be cleaned after each usage and sampling should be done with care and only when it is necessary.",
        "summary": "Dendrochronology involves data gathering and analysis, with findings interpreted in relation to tree wellbeing or climate. Data is collected through core sampling of living trees using an increment borer, which removes a sample from the stem. This method is invasive and could allow parasites or fungi to enter the bore hole, so the tool should be cleaned after each use and sampling should be done carefully and only when necessary.",
        "question": "How is data collected in dendrochronology and what precautions should be taken?",
        "ground_truths": "In dendrochronology, data is collected through core sampling of living trees using an increment borer. Because this method is invasive and could allow parasites or fungi to enter the bore hole, the tool should be cleaned after each use and sampling should be done carefully and only when necessary."
    },
    {
        "contexts": "Selection of sampling sites and individual trees depends on the research question. Usually, researchers are interested in investigating variability within years. To this end, trees that do not have underground water access are sampled. Trees growing under these conditions will have so called sensitive ring sequences useful for dating, in contrast to trees with underground water access, that will have so called complacent ring sequences with equally narrow or wide rings. Core samples are therefore preferably taken from trees growing on the top of the slope. The increment borer is placed at a 90\u00b0 angle on the stem facing the side slope. This means neither facing uphill nor downhill to avoid areas where the ring patterns are likely to be distorted",
        "summary": "Sampling sites and trees for dendrochronology depend on the research question, with a focus on trees without underground water access. These trees have sensitive ring sequences useful for dating. Core samples are taken from trees on the top of the slope, with the increment borer placed at a 90\u00b0 angle on the stem.",
        "question": "How are trees selected and sampled for dendrochronology research?",
        "ground_truths": "Trees for dendrochronology research are selected based on the research question and are usually those without underground water access. These trees have sensitive ring sequences useful for dating. Core samples are taken from trees on the top of the slope, with the increment borer placed at a 90\u00b0 angle on the stem."
    },
    {
        "contexts": "The core sample will shrink and bend while drying. Therefore, place the core sample in a slotted mount and fixate with pushpins for air drying. After drying for about one week, cut off the core top with a core microtome to create a smooth straight surface (Fig. 7).",
        "summary": "Core samples shrink and bend during drying, so they are placed in a slotted mount and fixed with pushpins for air drying. After about a week, the core top is cut off with a microtome to create a smooth surface.",
        "question": "What is the process for preparing a core sample for dendrochronology?",
        "ground_truths": "The core sample is placed in a slotted mount and fixed with pushpins for air drying. After about a week, the core top is cut off with a microtome to create a smooth surface."
    },
    {
        "contexts": "The visual inspection is done on a measuring table with a resolution of 0.01 mm and a microscope. Values are then digitalised, plotted and analysed with computer software (e. g. IML Software T-Tools Pro, Instrumenta Mechanik Labor GmbH). The number of rings and any distinct patterns (frequency of narrow or wide rings) are often what to look out for in this step.",
        "summary": "Visual inspection of the core sample is done on a measuring table with a microscope, then the values are digitalized and analyzed with software. The number of rings and distinct patterns are key factors in this step.",
        "question": "How is the analysis of the core sample conducted in dendrochronology?",
        "ground_truths": "The core sample is visually inspected on a measuring table with a microscope, then the values are digitalized and analyzed with software. The number of rings and distinct patterns are key factors in this analysis."
    },
    {
        "contexts": "An advantage of this method is, that it can be applied to all woody (parts of) plants. One can sample the living organisms by taking only a small-diameter core sample. After dendrochronologically analysing a representative amount of the population, one can statistically calculate an estimate, which allows the researcher to assume the age of other branches/roots/etc. from that population based on their diameter without sampling them. The method can also be applied to tree stumps of logged trees or wood used as building material and charcoal samples to date an archaeological site.",
        "summary": "Dendrochronology can be applied to all woody plants by taking a small-diameter core sample. After analyzing a representative amount of the population, an estimate can be calculated to determine the age of other parts of the plant based on their diameter. The method can also be used on tree stumps, building material, and charcoal samples.",
        "question": "What are the applications and advantages of dendrochronology?",
        "ground_truths": "Dendrochronology can be applied to all woody plants by taking a small-diameter core sample. It allows for the estimation of the age of other parts of the plant based on their diameter. The method can also be used on tree stumps, building material, and charcoal samples."
    },
    {
        "contexts": "One practical application of dendrochronology was the Navajo Land Claim Project. For 18 years from 1951 onwards, the University of Arizona\u2019s Laboratory of Tree-Ring Research and the Navajo Tribe worked together, closely linking science and society. They took thousands of samples from living trees and old Navajo hogans in and around the Navajo Reservation. Cross-dating the samples provided evidence, that the Navajo peoples had formerly occupied areas outside their reservation, which they had been evicted from (10). Therefore, the United States Land Claims Commission acknowledged the tribe to be eligible for compensation (2).",
        "summary": "Dendrochronology was used in the Navajo Land Claim Project from 1951 for 18 years. The University of Arizona\u2019s Laboratory of Tree-Ring Research and the Navajo Tribe collected and cross-dated thousands of samples from trees and old Navajo hogans. This provided evidence of the Navajo peoples' former occupation of areas outside their reservation, leading to their eligibility for compensation by the United States Land Claims Commission.",
        "question": "What was the outcome of the Navajo Land Claim Project that used dendrochronology?",
        "ground_truths": "The Navajo Tribe was acknowledged to be eligible for compensation by the United States Land Claims Commission."
    },
    {
        "contexts": "Tree ring analysis is also applied to determine the health status of individuals or forests and effects of climate change (dendroclimatology). Based on the growth of these long-lived species one can interfere with past environmental conditions and current stressors. Due to climate change, years with droughts increase but on the other hand the length of growing seasons also increases. How this affects different species and forest composition can be investigated through tree ring analysis. Thus, dendrochronology can help foresters in making their woods future-proof and sustainable. It could also play a role in the selection and management of city trees. On a local or regional scale, insights gained from tree ring analysis might inform climate adaptation measures which involve plants.",
        "summary": "Tree ring analysis, or dendrochronology, is used to determine the health of forests and the effects of climate change. It can reveal past environmental conditions and current stressors, such as increasing droughts and longer growing seasons. This information can help foresters make their woods sustainable and inform climate adaptation measures on a local or regional scale.",
        "question": "How can dendrochronology assist in making forests sustainable and adapting to climate change?",
        "ground_truths": "Dendrochronology can reveal past environmental conditions and current stressors, such as increasing droughts and longer growing seasons. This information can help foresters make their woods sustainable and inform climate adaptation measures on a local or regional scale."
    },
    {
        "contexts": "Descriptive stats are what most people think stats are all about. Many people believe that the simple observation of more or less, or the mere calculation of an average value, is what statistics are all about. Of course, this is not the case - statistics is more than descriptive statistics, or whimsical bar plots or even pie charts. Still, knowing the basics is important, and most of you probably already calculated things like mean and median in school. So let us have another look to refresh your memory.",
        "summary": "Descriptive statistics are often mistaken as the entirety of statistics, which involves more than just calculating averages or creating bar plots. However, understanding the basics, such as mean and median, is crucial.",
        "question": "What are the basic elements of descriptive statistics?",
        "ground_truths": "The basic elements of descriptive statistics include calculations like mean and median."
    },
    {
        "contexts": "The mean is the average of numbers you can simply calculate by adding up all the numbers and then divide them by how many numbers there are in total. The median is the middle number in a sorted set of numbers. It can be substantially different from the mean value, for instance when you have large gaps or cover wide ranges within your data. Therefore, it is more robust against outliers. The mode is the value that appears most often. It can be helpful in large datasets or when you have a lot of repetitions within the dataset. The range is simply the difference between the lowest and the highest value and consequently it can also be calculated like this.",
        "summary": "Mean is the average of a set of numbers, median is the middle number in a sorted set, mode is the most frequently occurring value, and range is the difference between the highest and lowest values.",
        "question": "What are the definitions of mean, median, mode, and range in descriptive statistics?",
        "ground_truths": "In descriptive statistics, mean is the average of a set of numbers, median is the middle number in a sorted set, mode is the most frequently occurring value, and range is the difference between the highest and lowest values."
    },
    {
        "contexts": "The standard deviation is calculated as the square root of variance by determining the variation between each data point relative to the mean. It is a measure of how spread out your numbers are. If the data points are further from the mean, there is a higher deviation within the data set. The higher the standard deviation, the more spread out the data.",
        "summary": "Standard deviation, calculated as the square root of variance, measures the spread of data points from the mean. The higher the standard deviation, the more spread out the data is.",
        "question": "What does a higher standard deviation indicate in a dataset?",
        "ground_truths": "A higher standard deviation in a dataset indicates that the data is more spread out."
    },
    {
        "contexts": "A strong reasoning why we need a different way to think about scientific methods is actually rooted in the history of science, and how the way we think about methods in the future should be different from these past developments. Kuhn talks about revolution and paradigm shifts in scientific thinking, and many would firmly argue that such a paradigm shift is immanent right now. Actually, it is already happening. The entry on the epistemology of scientific methods outlines in detail how we came to the current situation, and explains why our thinking about scientific methods - and with it our thinking about science in general - will shift due to and compared to the past. For now, let us focus on the present, and even more importantly, the future. For the epistemology please refer you to the respective article, since much is rooted in the history of science, and humankind as such.",
        "summary": "The need for a new perspective on scientific methods is rooted in the history of science. Kuhn discusses the concept of paradigm shifts in scientific thinking, arguing that such a shift is currently underway. The epistemology of scientific methods explains how we arrived at the current situation and why our thinking about scientific methods will change in the future.",
        "question": "Why is there a need for a different perspective on scientific methods?",
        "ground_truths": "The need for a different perspective on scientific methods is rooted in the history of science and the ongoing paradigm shift in scientific thinking."
    },
    {
        "contexts": "Today, we face unprecedented challenges in terms of magnitude and complexity. While some problems are still easy to solve, others are not. Due to the complex phenomena of globalisation we now face challenges that are truly spanning across the globe, and may as well affect the whole globe. Globalisation once started with the promise of a creation of a united and connected humankind. This is of course a very naive worldview and, as we see today, has failed in many regards. Instead, we see a rise in global inequality in terms of many indicators, among them global wealth distribution. Climate change, biodiversity loss and ocean acidification showcase that the challenges are severe, and may as well lead to an end of the world as we know it, if you are a pessimist.",
        "summary": "We are currently facing complex and unprecedented global challenges due to globalisation. These challenges include a rise in global inequality, climate change, biodiversity loss, and ocean acidification. The initial promise of globalisation, a united and connected humankind, has largely failed.",
        "question": "What are some of the global challenges we face due to globalisation?",
        "ground_truths": "The global challenges we face due to globalisation include a rise in global inequality, climate change, biodiversity loss, and ocean acidification."
    },
    {
        "contexts": "Science has long realised that due to globalisation, shared and joined perspectives between disciplines and the benefits of modern communication, we - scientists from all disciplinary and cultural backgrounds - often have a shared interest in specific topics. For every topic, there are different scientific disciplines that focus on it, albeit from different perspectives. Take climate change, where many disciplines offer their five cent to the debate. Will any of the singular scientific disciplines solve the problems alone? Most likely not. Werewolves may be killed by silver bullets, yet for most problems we currently face, there is no silver bullet. We will have to think of more complex solutions, often working in unisono across different disciplines, and together with society.",
        "summary": "Science has recognised the need for shared perspectives and collaboration across disciplines due to globalisation. For every issue, multiple scientific disciplines offer their insights, but no single discipline can solve these problems alone. Complex solutions often require collaboration across different disciplines and with society.",
        "question": "Why is collaboration across different scientific disciplines necessary?",
        "ground_truths": "Collaboration across different scientific disciplines is necessary because no single discipline can solve the complex global problems we face alone."
    },
    {
        "contexts": "However, most disciplines have established specific traditions when it comes to the methods that are being used. While these traditions guarantee experience, they may not allow us to solve the problem we currently face. Some methods are exclusive for certain disciplines, yet most methods are used by several disciplines. Sometimes these methods co-evolved, but more often than not the methods departed from a joined starting point and then become more specific in the respective contexts of the particular branch of science. For instance, interviews are widely used in many areas of science, yet the implementation is often very diverse, and specific branches of sciences typically favour specific types of interviews. This gets us stuck if we want to share insights, work together and combine knowledge gathered in different disciplines. Undisputedly, modern science increasingly builds on a canon of diverse approaches, and there is a clear necessity to unpack the \u2018silos\u2019 that methods are currently packed in.",
        "summary": "Most scientific disciplines have established traditions regarding the methods they use. While these traditions provide experience, they may not be sufficient to solve current problems. Some methods are exclusive to certain disciplines, but most are used by several. These methods often diverge from a common starting point to become more specific to each discipline. This can hinder collaboration and knowledge sharing across disciplines, highlighting the need to break down the 'silos' of methods.",
        "question": "What challenges do established traditions of methods pose in scientific disciplines?",
        "ground_truths": "Established traditions of methods in scientific disciplines can hinder collaboration and knowledge sharing across disciplines, as they often become specific to each discipline and may not be sufficient to solve current problems."
    },
    {
        "contexts": "We need to choose and apply methods depending on the type of knowledge we aim to create, regardless of the disciplinary background or tradition. We should aim to become more and more experienced and empowered to use the method that is most ideal for each research purpose and not rely solely on what our discipline has always been doing. In order to achieve this, design criteria of methods can help to create a conceptualization of the nature of methods. In other words: what are the underlying principles that guide the available scientific methods? First, we need to start with the most fundamental question:",
        "summary": "The selection and application of methods should be based on the type of knowledge we aim to create, not just on disciplinary tradition. Design criteria of methods can help conceptualize the nature of methods and the principles that guide them.",
        "question": "Why is it important to choose and apply methods based on the type of knowledge we aim to create?",
        "ground_truths": "It's important because it allows us to use the most ideal method for each research purpose and not rely solely on traditional disciplinary methods. This can be achieved through the use of design criteria of methods."
    },
    {
        "contexts": "Generally speaking, scientific methods create knowledge. This knowledge creation process follows certain principles and has a certain rigour. Knowledge that is created through scientific methods should be ideally reproducible, which means that someone else under the given circumstances would come up with the same insights when using the same respective methods. This is insofar important, as other people would maybe create different data under a similar setting, but all the data should answer research questions or hypotheses in the same way. However, there are some methods that may create different knowledge patterns, which is why documentation is pivotal in the application of scientific methods.",
        "summary": "Scientific methods create knowledge following certain principles and rigour. Ideally, the knowledge created should be reproducible, meaning the same insights would be reached under the same circumstances. Documentation is crucial in the application of scientific methods due to the potential for different knowledge patterns.",
        "question": "What is the importance of reproducibility and documentation in the application of scientific methods?",
        "ground_truths": "Reproducibility is important as it ensures that the same insights would be reached under the same circumstances, validating the research. Documentation is crucial because it allows for understanding and verification of the process, especially when different knowledge patterns may be created."
    },
    {
        "contexts": "Methods are about gathering data, analysing data, and interpreting data. Not all methods do all of these three steps, in fact most methods are even exclusive to one or two of these steps. For instance, one may analyse structured interviews - which are one way to gather data - with statistical tests, which are a form of analysis. The interpretation of the results is then built around the design of the interviews, and there are norms and much experience concerning the interpretation of statistical tests. Hence, gathering data, analysing it, and then interpreting the results are part of a process that we call design criteria of methods.",
        "summary": "Methods involve gathering, analysing, and interpreting data, though not all methods encompass all three steps. For example, structured interviews can be analysed with statistical tests, with interpretation built around the interview design. These steps form the process known as design criteria of methods.",
        "question": "What are the three main steps involved in the process known as design criteria of methods?",
        "ground_truths": "The three main steps involved in the process known as design criteria of methods are gathering data, analysing data, and interpreting data."
    },
    {
        "contexts": "One of the strongest discourses regarding the classification of methods revolves around the question whether a method is quantitative or qualitative. Quantitative methods focus on the measurement, counting and constructed generalisation, linking the statistical or mathematical analysis of data, as well as the interpretation of data that consists of numbers to extract patterns or support theories. Simply spoken, quantitative methods are about numbers. Qualitative methods, on the other hand, focus on the human dimensions of the observable or conceptual reality, often linking observational data or interpretation of existing data directly to theory or concepts, allowing for deep contextual understanding. Both quantitative and qualitative methods are normative.",
        "summary": "Methods can be classified as quantitative or qualitative. Quantitative methods focus on measurement, counting, and generalisation, using statistical or mathematical analysis of numerical data. Qualitative methods focus on human dimensions of reality, linking observational data or interpretation of existing data to theory or concepts.",
        "question": "What is the difference between quantitative and qualitative methods?",
        "ground_truths": "Quantitative methods focus on measurement, counting, and generalisation, using statistical or mathematical analysis of numerical data. Qualitative methods, on the other hand, focus on human dimensions of reality, linking observational data or interpretation of existing data to theory or concepts."
    },
    {
        "contexts": "Methods can further be distinguished into whether they are inductive or deductive. Deductive reasoning builds on statements or theories that are confirmed by observation or can be confirmed by logic. This is rooted deeply in the tradition of Francis Bacon and has become an important baseline of especially the natural sciences and research utilising quantitative methods. While Bacon can be clearly seen as being more leaning towards the inductive, he paved the road towards a clearer distinction between the inductive and the deductive. If there is such a thing as the scientific method at all, we owe much of the systematic behind it to Bacon. Inductive methods draw conclusions based on data or observations. Many argue that these inferences are only approximations of a probable truth. Deriving the knowledge from data and not from theory in inductive research can be seen as a counterpoint to hypothesis-driven deductive reasoning, although this is not always the case. While research even today is mostly stated to be clearly either inductive or deductive, this is often a hoax. You have many folks using large datasets and stating clear hypotheses. If these folks were honest, it would become clear that they analysed the data until they had these hypotheses, yet they publish the results with hypotheses they pretend they had from the very beginning. This is a clear indicator of scientific tradition getting back at us which, up until today, is often built on a culture that demands hypothesis building. Then again, other research is equally dogmatic in demanding an inductive approach, widely rejecting any deduction. Here we will not discuss the merits of inductive reasoning here, but will only highlight that its openness and urge to avoid being deterministic provides an important and relevant counterpoint to deductive research. Just as in the realms of quantitative and qualitative methods, there is a deep trench between inductive and deductive research. One possibility to overcome this in an honest and valuable approach would be abductive reasoning, which consists of iterations between data and theory. Modern research is actually framed and conducted in such a way with a constant exchange between data and theory. Considering the complexity of modern research, this also makes sense: on the one hand, not all research can be broken down into small hypothesisable units, and on the other hand, so much knowledge already exists, which is why much research builds on previous theories and knowledge. Still, today, and for years to come, we will have to acknowledge that much of modern research to date will fall into the traditionally established inductive or deductive category. It is therefore not surprising that critical theory and other approaches cast doubt over these rigid criteria, and the knowledge they produce. Nevertheless, they represent an important categorisation for the current state of the art of many areas of science, and therefore deserve critical attention, least because many methods imply either one or the other approach.",
        "summary": "Methods can be categorized as inductive or deductive. Deductive reasoning is based on confirmed statements or theories, while inductive reasoning draws conclusions from data or observations. Francis Bacon played a significant role in distinguishing between these two methods. Modern research often involves a mix of these approaches, with a constant exchange between data and theory. However, much of the research still falls into either the inductive or deductive category, despite criticism from critical theory and other approaches.",
        "question": "What is the difference between inductive and deductive methods in research?",
        "ground_truths": "Deductive reasoning in research is based on confirmed statements or theories, while inductive reasoning draws conclusions from data or observations."
    },
    {
        "contexts": "Different scientific methods are focussing on or even restricted to certain spatial scales, yet others may span across scales. Some methods are operationalised on a global scale, while others focus on the individual. For instance, a statistical correlation analysis can be applied on data about individuals gathered in a survey, but also be applied to global economic data. A global spatial scale of sampling is defined by data that covers the whole globe, or a non-deliberately chosen part of the globe. Such global analyses are of increasing importance, as they allow us to have a truly united understanding of societal and natural phenomena that concern the global scale. An individual scale of sampling is a scale that focuses on individual objects, which includes people and other living entities. The individual scale can give us deep insight into living beings, with a pronounced focus on research on humans. Since our concepts of humans are diverse, there are also many diverse approaches found here, and many branches of science focus on the individual. This scale certainly gained importance over the last decades, and we are only starting to understand the diversity of knowledge that an individual scale has to offer. In between, there is a huge void that can have different names in different domains of science - landscapes, systems, institutions, catchments, and others. Different branches of science focus on different mid-scales, yet this is often considered to be one of the most abundantly investigated scales, since it can generate a great diversity of data. We will call it the system scale. A system scale of sampling is defined by any scale that contains several individual objects that interact or are embedded in a wider matrix surrounding. Regarding our understanding of systems, this is a relevant methodological scale, because it can generate a great diversity of empirical data, and many diverse methods are applied at this scale. In addition, it is also quite a relevant scale in terms of normativity because it can generate knowledge about change in systems which can be operationalised with the goal to foster or hinder such change. Hence, this system scale can be quite relevant in terms of knowledge that focuses on policy. Changing the globe takes certainly longer, and understanding change in people has many deeper theoretical foundations and frameworks.",
        "summary": "Scientific methods can focus on different spatial scales, from global to individual. Global scale methods cover the entire globe, while individual scale methods focus on individual objects, including humans. There is also a system scale, which includes several individual objects interacting within a wider matrix. This scale is particularly relevant for generating empirical data and knowledge about system changes, which can be used to inform policy.",
        "question": "What are the different spatial scales that scientific methods can focus on?",
        "ground_truths": "Scientific methods can focus on different spatial scales, including global scale, individual scale, and system scale."
    },
    {
        "contexts": "The last criterion to conceptualise methods concerns time. The vast majority of empirical studies look at one slice of time (the present), and this makes the studies that try to understand the past or make the bold attempt to understand more about the future all the more precious. Analysis of the past can be greatly diverse, from quantitative longitudinal data analysis to deep hermeneutical text analysis and interpretation of writers long gone. There are many windows into the past, and we only start to unravel many of the most fascinating perspectives, as much of the treasures available are only now investigated by science. Still, there is a long tradition in history studies, cultural studies, and many more branches of sciences that prove how much we can learn from the past. With the utilisation of predictive models, the development of scenarios about the future and many other available approaches, science also attempts to generate more knowledge about what might become. The very concept of the future is a human privilege that science increasingly focuses on because of the many challenges we face. New approaches emerged over the last decades, and our knowledge about the future certainly grows. To date, we have to acknowledge that these studies are still all too rare, and much of our empirical knowledge builds on snapshots of a certain point in time. If you want to learn more about the design criterion of time, please refer to the entry on Time.",
        "summary": "The last criterion to conceptualise methods is time. Most empirical studies focus on the present, but those that study the past or future are valuable. The past can be analysed in many ways, and we are only beginning to explore these perspectives. There is a long tradition of learning from the past in various scientific fields. Science also tries to predict the future using various methods, and our knowledge of the future is growing, although these studies are still rare.",
        "question": "What is the last criterion to conceptualise methods and how is it utilised in scientific studies?",
        "ground_truths": "The last criterion to conceptualise methods is time. Most empirical studies focus on the present, but those that study the past or future are valuable. The past can be analysed in many ways, and we are only beginning to explore these perspectives. There is a long tradition of learning from the past in various scientific fields. Science also tries to predict the future using various methods, and our knowledge of the future is growing, although these studies are still rare."
    },
    {
        "contexts": "Based on these considerations, one needs to remember the following criteria when you approach a concrete scientific method: Quantitative - Qualitative, Inductive - Deductive, Spatial scale: Individual - System - Global, Temporal scale: Past - Present - Future. Try to understand how methods can be operationalised in view of these criteria. Some methods only tick one option per criterion, and others may span across many different combinations of criteria. Out of the great diversity of methods, this categorization gives you a first positioning of each specific method. There are literally hundreds of methods out there. Instead of understanding all of them, try to understand the criteria that unite and differentiate them. Only through an understanding of these methodological design criteria may you be able to choose the method that creates the knowledge that may be needed.",
        "summary": "When approaching a scientific method, remember the following criteria: Quantitative - Qualitative, Inductive - Deductive, Spatial scale: Individual - System - Global, Temporal scale: Past - Present - Future. Understand how methods can be operationalised based on these criteria. Some methods fit one criterion, others span many. There are hundreds of methods, but understanding the criteria that unite and differentiate them can help choose the right method.",
        "question": "What are the criteria to consider when approaching a scientific method and how can they help in choosing the right method?",
        "ground_truths": "When approaching a scientific method, remember the following criteria: Quantitative - Qualitative, Inductive - Deductive, Spatial scale: Individual - System - Global, Temporal scale: Past - Present - Future. Understand how methods can be operationalised based on these criteria. Some methods fit one criterion, others span many. There are hundreds of methods, but understanding the criteria that unite and differentiate them can help choose the right method."
    },
    {
        "contexts": "The [[Design Criteria of Methods|design criteria of methods]] that I propose for all methods - quantitative vs. qualitative, inductive vs. deductive, spatial and temporal scales - are like the usual suspects of scientific methods. Within normal science, these design criteria are what most scientists may agree upon to be central for the current debate and development about methods. Consequently, '''it is important to know these ''normal science'' design criteria also when engaging with sustainability science.''' However, some arenas in science depart from the current paradigm of science - ''sensu strictu'' [https://plato.stanford.edu/entries/thomas-kuhn/#DeveScie Kuhn] - and this means also that they depart potentially from the design criteria of the normal sciences.",
        "summary": "The design criteria of methods, including quantitative vs. qualitative, inductive vs. deductive, and spatial and temporal scales, are central to the current debate and development about methods in normal science. These criteria are also important when engaging with sustainability science. However, some areas of science depart from these criteria and the current paradigm of science.",
        "question": "Why are the design criteria of methods important in sustainability science?",
        "ground_truths": "The design criteria of methods are important in sustainability science because they are central to the current debate and development about methods. However, some areas of science depart from these criteria and the current paradigm of science."
    },
    {
        "contexts": "In 2011, Wiek et al. analyzed the prevalent literature and presented five Key Competencies that students of Sustainability Science should strive for. This is an excellent scheme to be reflexive about the [[Glossary|competencies]] you want to gain, and to get a better understanding on which competencies can be aimed at through specific methods. These competencies are as follows:\n* ''Systems Thinking'': Systems Thinking competency is the ability to analyze and understand [[Glossary|complex systems]] including the dynamics in the interrelation of their parts. Systems thinking integrates different domains (society, environment, economy, etc.) as well as different scales (from local to global).\n* ''Anticipatory'': Anticipatory competency describes the ability to develop realistic scenarios of future trajectories within complex systems, including positive (e.g. a carbon-neutral city) and negative (e.g. flooding stemming from climate change) developments. This may include rigorous concepts as well as convincing narratives and visions.\n* ''Normative'': Normative competency refers to the ability to evaluate, discuss and apply (sustainability) values. It is based on the acquisition of normative knowledge such as concepts of justice or equality.\n* ''Strategic'': In simple terms, this competence is about being able to \"get things done\". Strategic competency is the capability to develop and implement comprehensive strategies (i.e. interventions, projects, measures) that lead to sustainable future states across different societal domains (social, economic, ecologic, ...) and scales (local to global). It requires an intimate understanding of strategic concepts such as path dependencies, barriers and alliances as well as knowledge about viability, feasibility, effectiveness and efficiency of systemic interventions as well as potential of unintended consequences.\n* ''Interpersonal'': Interpersonal competence is the ability to motivate, enable, and facilitate collaborative and participatory sustainability research and problem solving. This capacity includes advanced skills in communicating, deliberating and negotiating, collaborating, [[Glossary|leadership]], pluralistic and trans-cultural thinking and empathy.",
        "summary": "In 2011, Wiek et al. identified five key competencies for students of Sustainability Science: Systems Thinking, Anticipatory, Normative, Strategic, and Interpersonal. These competencies involve understanding complex systems, developing future scenarios, applying sustainability values, implementing strategies for sustainable futures, and facilitating collaborative research and problem solving.",
        "question": "What are the five key competencies for students of Sustainability Science identified by Wiek et al. in 2011?",
        "ground_truths": "The five key competencies for students of Sustainability Science identified by Wiek et al. in 2011 are Systems Thinking, Anticipatory, Normative, Strategic, and Interpersonal."
    },
    {
        "contexts": "At the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and contexts-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science: This showcases that this knowledge - and more importantly - the perspective from a philosophy-of-science viewpoint is only starting to emerge, and much more work will be needed until our methodological canon and the knowledge we want to produce enable us to solve the problems we are facing, but also to create these solution in ways that are closer to a mode how we want to create these solutions. We may well be able to solve certain things, and to produce knowledge that can be seen as solutions. I would however argue, that it also matters how we create these solutions and how we create knowledge. Only if people are empowered and society and science work seamlessly together - with ethical restrictions and guidelines in place, of course - will we not only produce the knowledge needed, but we also produce it in a way how we should as scientists. Science is often disconnected and even arrogant, and building an educational system that is reflexive and interconnected will be maybe the largest challenge we face. This is why we give you these criteria here, because I think that we need to consider what further design criteria can be in order to enhance and diversify our conceptual thinking about the scientific methodological canon. Plurality on scientific methods will necessarily mean to evolve, and in this age of interconnectedness, our journey is only beginning.",
        "summary": "Sustainability Science aims to develop practical solutions to existing problems through cooperative research with societal actors. Caniglia et al. suggest three types of knowledge to be developed and incorporated by Sustainability Science. The perspective from a philosophy-of-science viewpoint is emerging, and more work is needed to solve problems and create solutions. It's important how solutions and knowledge are created, and it's necessary for society and science to work together. The challenge is to build an educational system that is reflexive and interconnected. Further design criteria are needed to enhance and diversify the scientific methodological canon.",
        "question": "What is the main goal of Sustainability Science and what are the challenges it faces?",
        "ground_truths": "The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected."
    },
    {
        "contexts": "Scientific methods can engage with non-scientific actors on diverse levels, depending on the extent of their involvement in the process of scientific inquiry. Interaction with stakeholder may be especially relevant in transdisciplinary research. Here, we refer to four levels of interaction: Information: Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation. Consultation: A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process. Collaboration: Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue. Empowerment: The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.",
        "summary": "Scientific methods can engage with non-scientific actors at different levels, which is especially relevant in transdisciplinary research. The four levels of interaction are information, consultation, collaboration, and empowerment. Information is the most common form of science-society cooperation, where stakeholders are informed about scientific insights. Consultation involves a one-directional information flow from stakeholders to academia. Collaboration involves stakeholders cooperating with academia to solve a distinct issue. Empowerment is the highest form of involvement, where stakeholders are given authority to solve problems themselves.",
        "question": "What are the four levels of interaction between scientific methods and non-scientific actors?",
        "ground_truths": "The four levels of interaction between scientific methods and non-scientific actors are information, consultation, collaboration, and empowerment."
    },
    {
        "contexts": "Design Thinking is a creative process in product or service design with a user-centered perspective. It helps empathise with others, understand their perspectives and develop solutions to their problems. This way, it prevents you from creating a solution when there is no problem. Disclaimer: Design Thinking does not necessarily help you to compromise and balance different interests on the same resource e.g. clean air. It is an iterative process of brainstorming, persona building, prototyping and testing. A Design Thinking team should not be larger than five individuals, but several teams can work on the same problem at once. The length of the process is expandable (up to a week), but may also be squeezed into a 3-hour workshop",
        "summary": "Design Thinking is a user-centered creative process used in product or service design. It involves empathising with others, understanding their perspectives, and developing solutions to their problems. It's an iterative process of brainstorming, persona building, prototyping, and testing, typically conducted by a team of no more than five individuals.",
        "question": "What is the process of Design Thinking?",
        "ground_truths": "Design Thinking is a user-centered creative process used in product or service design. It involves empathising with others, understanding their perspectives, and developing solutions to their problems. It's an iterative process of brainstorming, persona building, prototyping, and testing, typically conducted by a team of no more than five individuals."
    },
    {
        "contexts": "First, you have to decide under which conditions you want to host a Design Thinking session. What is the topic or the problem you want to work on? Which participants, perspectives and skills do you need to go through the process? It is recommendable to not work in larger groups than five, if you are more participants, you can split up in smaller teams and find out with what kind of solutions you come up with. Define a workshop facilitator who guides the process, and create an agenda. Find a room and organize the material and resources that you need (paper, pen, post-its, whiteboard etc.)",
        "summary": "To host a Design Thinking session, you need to decide on the problem or topic, the participants, perspectives and skills needed, and the conditions under which the session will be held. It's recommended to work in groups of no more than five. A workshop facilitator should be defined, an agenda created, and necessary materials and resources organized.",
        "question": "How do you prepare for a Design Thinking session?",
        "ground_truths": "To host a Design Thinking session, you need to decide on the problem or topic, the participants, perspectives and skills needed, and the conditions under which the session will be held. It's recommended to work in groups of no more than five. A workshop facilitator should be defined, an agenda created, and necessary materials and resources organized."
    },
    {
        "contexts": "In the first phase of the process, you try to change your perspective from your own to the user that you want to focus on. Define your target/ user group and research about their background. You can make use of persona building to visualize your target group. A look into demographics can be of help to understand the specific situation of your target group. Maybe you'll also want to have a look at the SINUS group approach. Do all you can to understand the life that your target group is living. Your goal is to be able to think and feel like the person or group of persons you want to empathize with.",
        "summary": "The first phase of Design Thinking involves changing your perspective to that of the user. This involves defining the target user group, researching their background, and using persona building to visualize them. The goal is to understand the life of the target group and to be able to empathize with them.",
        "question": "What is the first phase of the Design Thinking process?",
        "ground_truths": "The first phase of Design Thinking involves changing your perspective to that of the user. This involves defining the target user group, researching their background, and using persona building to visualize them. The goal is to understand the life of the target group and to be able to empathize with them."
    },
    {
        "contexts": "The rule of \"yes-and\" means, that you should try to build up on each others ideas. Even though you have the impression that the suggestion by your team member is not suitable, add something that makes it fitting. Not everything that is brought up in the process will be the next big thing. Try to not judge ideas and collect everything at first. You can throw out unnecessary stuff later. As it happens fast that you get carried away by the brainstorm-thunder, try to get back to the problem description multiple times and check whether your solution fits to the problem and target group. This helps you to stay on track and adjust if necessary. You might have the impression that your idea is better than the other one. Try to focus on one idea with the whole group and do not start to do your own thing. Use the attention of the whole group to progress. Depending on your preference, you can draw an idea, make a mindmap, write everything on post-its or use a written discussion - the most important aspect is:  write it down. Therefore, you will have a written documentation later and can get back to ideas that were brought up hours ago.",
        "summary": "In design thinking, it's important to build on each other's ideas, even if they seem unsuitable at first. Ideas should be collected without judgement, and the problem description should be revisited frequently to ensure the solution fits. The group should focus on one idea and work together, documenting their process visually for future reference.",
        "question": "What are some key principles to follow in design thinking?",
        "ground_truths": "Key principles in design thinking include building on each other's ideas, not judging ideas initially, frequently revisiting the problem description, focusing on one idea as a group, and documenting the process visually."
    },
    {
        "contexts": "Imagine you want to find solutions for different stakeholders on land use in the city of L\u00fcneburg. At first, you would need to define the different needs of the stakeholder and cluster them into need groups, e.g. all stakeholders that are interested in climate change mitigation aspects. Then, you would assign one team per need group and let them dive into the process. The process steps are still the same for all of the groups. After having finished the process, you can compare the different solutions. One group might have come up with a participation format of how to include a group that is not yet involved in a decision process. Another group might have developed a ranking scheme for decision criteria and so on. Now, you can see whether there are similarities between the different solutions and how they can be combined, transferred to another need group.",
        "summary": "In a hypothetical scenario of finding solutions for land use in L\u00fcneburg, stakeholders' needs are first defined and grouped. Each group is assigned a team to work on the process. After the process, the different solutions are compared to see if they can be combined or transferred to another group. Some solutions might include a participation format or a ranking scheme for decision criteria.",
        "question": "How can design thinking be applied to solve land use issues in a city?",
        "ground_truths": "Design thinking can be applied to solve land use issues by first defining and grouping stakeholders' needs. Each group is then assigned a team to work on the process. After the process, the different solutions are compared to see if they can be combined or transferred to another group. Some solutions might include a participation format or a ranking scheme for decision criteria."
    },
    {
        "contexts": "In statistical hypothesis testing, the [https://www.youtube.com/watch?v=-MKT3yLDkqk p-value] or probability value is the probability of obtaining the test results as extreme as the actual results, assuming that the null hypothesis is correct. In essence, the p-value is the percentage probability the observed results occurred by chance. Probability is one of the most important concepts in modern statistics. The question whether a relation between two variables is purely by chance, or following a pattern with a certain probability is the basis of all probability statistics (surprise!). In the case of linear relation, another quantification is of central relevance, namely the question how much variance is explained by the model. Between these two numbers -the amount of variance explained by a linear model, and the fact that two variables are not randomly related- are related at least to some amount.",
        "summary": "The p-value in statistical hypothesis testing is the probability of obtaining test results as extreme as the actual results, assuming the null hypothesis is correct. It represents the chance that the observed results occurred randomly. The relationship between two variables and the variance explained by a model are key concepts in probability statistics.",
        "question": "What does the p-value represent in statistical hypothesis testing?",
        "ground_truths": "The p-value in statistical hypothesis testing is the probability of obtaining test results as extreme as the actual results, assuming the null hypothesis is correct. It represents the chance that the observed results occurred randomly."
    },
    {
        "contexts": "In Fisher's example of a [https://www.youtube.com/watch?v=lgs7d5saFFc lady tasting tea], a lady claimed to be able to tell whether tea or milk was added first to a cup. Fisher gave her 8 cups, 4 of each variety, in random order. One could then ask what the probability was for her getting the specific number of cups she identified correct, but just by chance. Using the combination formula, where n (total of cups) = 8 and k (cups chosen) = 4, there are 8!/4!(8-4)! = 70 possible combinations. In this scenario, the lady would have a 1.42% chance of correctly guessing the contents of 4 out of 8 cups. If the lady is able to consistently identify the contents of the cups, one could say her results are statistically significant.",
        "summary": "In Fisher's tea tasting experiment, a lady claimed to discern whether tea or milk was added first. Given 8 cups, 4 of each variety, the probability of her correctly identifying the contents of 4 cups by chance was calculated using the combination formula, resulting in a 1.42% chance. If she consistently identifies the contents correctly, her results are statistically significant.",
        "question": "What was the probability of the lady correctly identifying the contents of 4 out of 8 cups by chance in Fisher's tea tasting experiment?",
        "ground_truths": "The probability of the lady correctly identifying the contents of 4 out of 8 cups by chance in Fisher's tea tasting experiment was 1.42%."
    },
    {
        "contexts": "In statistical design theory, blocking is the practice of separating experimental units into similar, separate groups (i.e. \"blocks\"). A blocking group allows for greater accuracy in the results achieved by removing previously unaccounted for variables. A prime example is blocking an experiment based on male or female sex. Within each block, multiple treatments can be administered to each experimental unit. A minimum of two treatments are necessary, one of which is the control where, in most cased, \"nothing\" or a placebo is the treatment. Replicates of the treatments are then made to ensure results are statistically significant and not due to random chance.",
        "summary": "Blocking in statistical design theory involves separating experimental units into similar groups or 'blocks' to increase result accuracy by eliminating unaccounted variables. An example is blocking an experiment based on sex. Each block can receive multiple treatments, including a control. Replicates ensure results are statistically significant and not random.",
        "question": "What is the purpose of blocking in statistical design theory?",
        "ground_truths": "Blocking in statistical design theory involves separating experimental units into similar groups or 'blocks' to increase result accuracy by eliminating unaccounted variables."
    },
    {
        "contexts": "The initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. We have to recognise that modern science is a system that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.",
        "summary": "The history of science, starting from ancient times, has led to the development of scientific methods. However, modern science is flawed as it provides a singular worldview, built on oppression and inequalities. It lacks the necessary link between empirical and ethical consequences. Therefore, critical theory and ethics are essential for researchers.",
        "question": "Why are critical theory and ethics important in modern science?",
        "ground_truths": "Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences."
    },
    {
        "contexts": "Science is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research.",
        "summary": "While scientific knowledge is growing, it is often isolated and doesn't interact with society. Scientific disciplines need to focus more on society and reintegrate humanities to consider the ethical conduct and consequences of research.",
        "question": "How can scientific disciplines improve their interaction with society?",
        "ground_truths": "Scientific disciplines can improve their interaction with society by orientating their wider focus and daily interaction more strongly towards society and reintegrating the humanities to consider the ethical conduct and consequences of their research."
    },
    {
        "contexts": "The rise of empiricism and many other developments of society created critical theory, which questioned the scientific paradigm, the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.",
        "summary": "The rise of empiricism led to the creation of critical theory, questioning the scientific paradigm and societal norms. This led to a new form of scientific practice that can be normative and transformative.",
        "question": "What impact did the rise of empiricism have on scientific practice?",
        "ground_truths": "The rise of empiricism led to the creation of critical theory, which questioned the scientific paradigm and societal norms, paving the way for a new, normative and transformative form of scientific practice."
    },
    {
        "contexts": "The testing of a hypothesis was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs.",
        "summary": "The testing of a hypothesis and the importance of observation, as proclaimed by Francis Bacon, were breakthroughs in scientific thinking. These led to repeated observation under manipulated conditions and systematic methodological designs.",
        "question": "What were the breakthroughs in scientific thinking?",
        "ground_truths": "The breakthroughs in scientific thinking were the testing of a hypothesis and the importance of observation, as proclaimed by Francis Bacon, which led to systematic methodological designs."
    },
    {
        "contexts": "Building on the criteria from David Hume, we define causality through temporal links (\"if this - then this\"), as well as through similarities and dissimilarities. If A and B cause C, then there must be some characteristic that makes A and B similar, and this similarity causes C. If A causes C, but B does not cause C, then there must be a dissimilarity between A and B. Causal links can be clearly defined, and it is our responsibility as scientists to build on this understanding, and understand its limitations.",
        "summary": "Causality is defined through temporal links and similarities or dissimilarities between factors. If two factors cause an outcome, they must share a characteristic that leads to the outcome. If one factor causes an outcome but another does not, they must differ in some way. It's crucial for scientists to understand and acknowledge the limitations of causal links.",
        "question": "How is causality defined and what is the role of scientists in understanding it?",
        "ground_truths": "Causality is defined through temporal links and similarities or dissimilarities. Scientists have the responsibility to understand and acknowledge the limitations of causal links."
    },
    {
        "contexts": "Scientific methods partly dealt with the necessary approaches to enable a flourishing yet often abusive trade, work out solutions for mechanisation, and develop modern agriculture based on systematic inquiry. Modern medicine - or better medical research - is one example of a framing of knowledge production on and around scientifc method. Equally did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently did science often act morally wrong, or failed to offer the deep leverage points that could drive transformational change. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.",
        "summary": "Scientific methods have been used to enable trade, mechanisation, and modern agriculture, with medical research being a prime example. Society has also driven scientific inquiry, often funding it as a means to an end. However, this has sometimes led to moral failings or missed opportunities for transformational change. Critical views of science, particularly empirical approaches, have emerged from society and philosophy.",
        "question": "What are some applications of scientific methods and how has society influenced scientific inquiry?",
        "ground_truths": "Scientific methods have been applied in trade, mechanisation, and modern agriculture, with medical research being a key example. Society has influenced scientific inquiry by driving demand for solutions and providing funding, although this has sometimes led to moral failings or missed opportunities for transformational change."
    },
    {
        "contexts": "System thinking gained prominence during the last century, allowing to take interactions and interdependencies into account when investigating a system. To this end, a \u2018system\u2019 stands for something that is in the focus of the investigation, such as a catchment area, a business enterprise, a social institution, or a population of wasps. Such systems show signs of complexity, which means that the interactions of smaller entities in the system may be unpredictable or chaotic, which makes systems more than the sum of their parts. Under this definition, solving problems within a system is often the greatest challenge: while the system dynamics may be understandable, the solutions for an emerging problem are not instantly available. Consequently, solutions for complex problems emerge, and this demands a new line of thinking about systems.",
        "summary": "System thinking, which considers interactions and interdependencies, became prominent in the last century. A 'system' could be anything under investigation, such as a business or a population of wasps. These systems are complex, with unpredictable interactions making them more than the sum of their parts. Solving problems within these systems is challenging, requiring a new way of thinking as solutions emerge.",
        "question": "What is system thinking and what challenges does it present?",
        "ground_truths": "System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps. The complexity and unpredictability of these systems present challenges in problem-solving, requiring a new way of thinking as solutions emerge."
    },
    {
        "contexts": "The increasing availability of data offers many new possibilities, and the emergence of new ways of getting data through the internet poses not only opportunities, but also - among others - ethical challenges. All the while, ever more diversity of data becomes available to people, leading to an even larger wealth of qualitative data. Again, this poses many ethical questions, and imposes an all new agenda onto many methodological approaches, including data security, picture rights, normative interpretations and even culture wars.",
        "summary": "The growing availability of data, particularly through the internet, presents new opportunities but also ethical challenges. The increasing diversity of data leads to a wealth of qualitative data, raising ethical questions and impacting methodological approaches, including data security, picture rights, normative interpretations, and culture wars.",
        "question": "What opportunities and challenges does the increasing availability of data present?",
        "ground_truths": "The increasing availability of data presents opportunities for new insights but also poses ethical challenges. These challenges impact methodological approaches and include issues related to data security, picture rights, normative interpretations, and culture wars."
    },
    {
        "contexts": "Methods are often characterized by a specific language, which is why a lot of time needs to be invested into understanding each other. In addition, experts in one method are often deeply invested in their specific focus, which is why interdisciplinary collaboration is mainly built on trust. Methods are not like cooking recipes that anyone with the right recipe can unlock. Instead, methods evolve, just as the harmonisation of methods evolves. A mixed methods approach is thus not like a mere recipe, but should be approached as something new, and only time will tell how we may become more experienced and systematic in the combination of different methods.",
        "summary": "Methods often have a specific language, requiring time to understand. Experts are usually deeply invested in their method, making interdisciplinary collaboration reliant on trust. Methods aren't static like recipes; they evolve, as does the harmonisation of methods. A mixed methods approach should be seen as something new, with time revealing how to best combine different methods.",
        "question": "What are the challenges and considerations in using and combining different methods?",
        "ground_truths": "Methods often require time to understand due to their specific language, and interdisciplinary collaboration relies on trust due to experts' deep investment in their methods. Methods aren't static and evolve over time, and a mixed methods approach should be seen as something new, with time revealing how to best combine different methods."
    },
    {
        "contexts": "For a long time, scientific disciplines existed alongside each other and separated from the rest of society, using the world as an object of inquiry and gathering data all the while. However, the state of our world does not allow for such separation anymore. We need a new collaboration within science and between science and society in order to find solutions to urgent challenges.",
        "summary": "Scientific disciplines were once isolated from society, but the current state of the world requires a new collaboration between science and society to address urgent issues.",
        "question": "Why is there a need for a new collaboration between science and society?",
        "ground_truths": "The current state of the world requires a new collaboration between science and society to address urgent issues."
    },
    {
        "contexts": "Transdisciplinary research is built on the premise of scientific and non-scientific actors from diverse backgrounds jointly framing, understanding and solving societally relevant problems. Transdisciplinarity is reflexive, participatory and holistic and able to develop mutually beneficial approaches to complex and normative issues.",
        "summary": "Transdisciplinary research involves diverse actors collaboratively addressing societal problems, using a reflexive, participatory, and holistic approach to develop beneficial solutions to complex issues.",
        "question": "What is the premise of transdisciplinary research?",
        "ground_truths": "Transdisciplinary research involves diverse actors collaboratively addressing societal problems, using a reflexive, participatory, and holistic approach."
    },
    {
        "contexts": "Despite the rather recent emergence of transdisciplinary research, a lot has been developed in terms of appropriate research modes, methods and surroundings. Examples of this are Visioning, Scenario Planning and Living Labs. More is yet to come, but the foundations have been set.",
        "summary": "Transdisciplinary research, though recent, has developed various research modes and methods such as Visioning, Scenario Planning, and Living Labs, setting the foundation for future advancements.",
        "question": "What are some examples of methods developed in transdisciplinary research?",
        "ground_truths": "Some methods developed in transdisciplinary research include Visioning, Scenario Planning, and Living Labs."
    },
    {
        "contexts": "This article will provide you with ideas helpful to preparing and carrying out digital workshops or online sessions in general. It is especially meant to prevent \"online fatigue\" and share best practices that make online communication less error-prone and exhausting.",
        "summary": "The article provides useful ideas for preparing and conducting digital workshops or online sessions, with a focus on preventing online fatigue and sharing best practices for efficient online communication.",
        "question": "What is the main purpose of the article?",
        "ground_truths": "The main purpose of the article is to provide useful ideas for preparing and conducting digital workshops or online sessions, and to share best practices for efficient online communication."
    },
    {
        "contexts": "Just as in the analogue world, you should stick to the basics that make meetings effective and organized. We think it is even more important in the digital realm: Have a clear agenda and goals for the meeting. Time yourself. You can use tools to support that, e.g. Pomodoro Tracker. Clarify roles (Who is moderating? Who is writing protocol?)",
        "summary": "In digital meetings, it's crucial to adhere to the basics that make meetings effective: having a clear agenda and goals, timing yourself with tools like Pomodoro Tracker, and clarifying roles.",
        "question": "What are the basic principles for effective digital meetings?",
        "ground_truths": "The basic principles for effective digital meetings include having a clear agenda and goals, timing yourself with tools like Pomodoro Tracker, and clarifying roles."
    },
    {
        "contexts": "Working with digital tools can be great, but it can also be exclusive if people lack the experience or technical requirements to participate. If you are using tools, ensure that everyone has access and knows how to use it.",
        "summary": "While digital tools can be beneficial, they can also exclude those lacking experience or technical requirements. Therefore, it's important to ensure everyone has access and knows how to use these tools.",
        "question": "What should be considered when using digital tools in online workshops?",
        "ground_truths": "When using digital tools in online workshops, it's important to ensure that everyone has access and knows how to use these tools."
    },
    {
        "contexts": "Break-Out-Sessions are a great way to dissolve your group into smaller discussion groups, work out a specific question or problem and then come together to discuss in the larger group. Unfortunately, this is, as far as we know, restricted to the software zoom at the moment.",
        "summary": "Break-Out-Sessions, which allow a group to split into smaller discussion groups and then reconvene, are a useful tool but currently only available on the Zoom software.",
        "question": "What is the purpose of Break-Out-Sessions in digital workshops and which software supports this feature?",
        "ground_truths": "The purpose of Break-Out-Sessions in digital workshops is to allow a group to split into smaller discussion groups and then reconvene. Currently, this feature is only available on the Zoom software."
    },
    {
        "contexts": "Retaining results is one of the most important yet overlooked parts of workshops that can get lost in the heat of the moment. Here's some suggestions on how to not go down that road: Make sure you have a slot in the agenda for reflection and wrap-up. Have a common file share or workspace to which all participants have access. Do the equivalent of a photo protocoll in a PowerPoint (or something similar). Leave some room for open questions. Remember that it is important to give this at least a few minutes of time as questions have to start building up. You can play music in the background if you can't stand the silence. Do a poll at the end: What did people learn? What did you get out of the event? Don't forget to brush up the results and provide them to your participants!",
        "summary": "Retaining results is crucial in workshops. Suggestions include having a reflection slot in the agenda, a common file share for all participants, a photo protocol in PowerPoint, room for open questions, a poll at the end, and brushing up the results for participants.",
        "question": "What are some suggestions for retaining results in workshops?",
        "ground_truths": "Some suggestions for retaining results in workshops include having a reflection slot in the agenda, a common file share for all participants, a photo protocol in PowerPoint, room for open questions, a poll at the end, and brushing up the results for participants."
    },
    {
        "contexts": "Disney Method is a fairly simple (group) brainstorming technique that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.",
        "summary": "The Disney Method is a brainstorming technique that involves applying different perspectives to a topic. It involves generating ideas, envisioning their implementation, and reflecting on their feasibility. It can be used for idea generation, problem-solving, strategy development, and decision-making. Walt Disney, who invented the method, used it to view a movie from the perspectives of a director, audience member, and producer.",
        "question": "What is the Disney Method and how is it used?",
        "ground_truths": "The Disney Method is a brainstorming technique that involves applying different perspectives to a topic. It is used for generating ideas, envisioning their implementation, and reflecting on their feasibility. It was invented by Walt Disney who used it to view a movie from the perspectives of a director, audience member, and producer."
    },
    {
        "contexts": "The Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. The Dreamers try to come up with new ideas, are creative and imaginative and do not set themselves any limits. The Realists think about what needs to be done to implement the ideas, are practical and realistic. The Critics look at the idea objectively and try to identify crucial mistakes, are critical, but constructive. Each role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics. Each phase should be approx. 20 minutes long, and each phase is equivalently important.",
        "summary": "The Disney method involves a group of people assuming three roles: Dreamers, Realists, and Critics. Dreamers generate ideas without limits, Realists consider practical implementation, and Critics objectively identify potential mistakes. Each role has a specific area in a room, and the process moves from Dreamers to Realists to Critics, with each phase lasting about 20 minutes.",
        "question": "What are the roles in the Disney Method and how does the process work?",
        "ground_truths": "In the Disney Method, a group of people take on the roles of Dreamers, Realists, and Critics. Dreamers generate ideas without limits, Realists consider how to implement these ideas practically, and Critics objectively identify potential mistakes. The process moves from Dreamers to Realists to Critics, with each phase lasting about 20 minutes."
    },
    {
        "contexts": "A dummy variable is a variable that is created in regression analysis to represent a given qualitative variable through a quantitative one, which takes one of two values: zero or one. Typically we use quantitative variables for linear regression model equations. These can be a specific size of an object, age of an individual, population size, etc. But sometimes the predictor variables can be qualitative. Those are variables that do not have a numeric value associated with them, e.g. gender, country of origin, marital status. Since machine learning algorithms including regression rely on numeric values, these qualitative values have to be converted.",
        "summary": "Dummy variables are used in regression analysis to represent qualitative variables as quantitative ones, taking values of zero or one. They are used when predictor variables are qualitative, such as gender or marital status, as machine learning algorithms require numeric values.",
        "question": "What is the purpose of a dummy variable in regression analysis?",
        "ground_truths": "A dummy variable is used in regression analysis to represent a qualitative variable as a quantitative one, allowing machine learning algorithms to process these variables as they require numeric values."
    },
    {
        "contexts": "If the qualitative variable, which is also known as a factor, has only two levels, then integrating it into a regression model is very simple: we need to use dummy variables. For example, our variable can describe if a given individual smokes or does not. If a qualitative variable has more than two levels, a single dummy variable cannot represent all possible values. In this case we create additional dummy variables. The general rule that applies here, is the following: If you have k unique terms, you use k - 1 dummy variables to represent.",
        "summary": "Dummy variables are used in regression models to represent qualitative variables with two levels, such as whether an individual smokes or not. If a qualitative variable has more than two levels, additional dummy variables are created, following the rule of using k - 1 dummy variables for k unique terms.",
        "question": "How are dummy variables used to represent qualitative variables with more than two levels?",
        "ground_truths": "If a qualitative variable has more than two levels, additional dummy variables are created, following the rule of using k - 1 dummy variables for k unique terms."
    },
    {
        "contexts": "Converting a single column of values into multiple columns of binary values, or dummy variables, is also known as one-hot-encoding. Not all machines know how to convert qualitative variables into dummy variables automatically, so it is important to know different methods how to do it yourself. We will look at different ways to code it with the help of both, R and Python programming languages.",
        "summary": "The process of converting a single column of values into multiple columns of binary values, or dummy variables, is known as one-hot-encoding. As not all machines can automatically convert qualitative variables into dummy variables, it's important to know how to do this manually using programming languages like R and Python.",
        "question": "What is the process of converting a single column of values into multiple columns of binary values known as?",
        "ground_truths": "The process of converting a single column of values into multiple columns of binary values is known as one-hot-encoding."
    },
    {
        "contexts": "The wikipedia article provides a detailed guide on how to create dummy variables in R and Python, and how to use them in a linear regression model. The example uses a dataset with variables such as Gender_Code, Region, Income, and Spending. The Gender_Code and Region variables are categorical and are converted into dummy variables. The linear regression model is then built to predict Income based on Spending, Gender_Code_Male, and Region_Urban. The fitted regression line is given by the equation: Income = 14.06584 + 0.02634*(Spending) + 0.83435*(Gender_Code_Male) + 22.78781*(Region_Urban). This equation can be used to estimate the income of an individual based on their spending, gender, and region.",
        "summary": "The article explains how to create dummy variables in R and Python for categorical variables and use them in a linear regression model. An example dataset with variables Gender_Code, Region, Income, and Spending is used. The categorical variables are converted into dummy variables and a linear regression model is built to predict Income. The resulting regression equation can be used to estimate an individual's income based on their spending, gender, and region.",
        "question": "How can dummy variables be used in a linear regression model to predict income?",
        "ground_truths": "Dummy variables can be created for categorical variables such as Gender_Code and Region. These dummy variables can then be used in a linear regression model to predict Income. The resulting regression equation, which includes coefficients for Spending, Gender_Code_Male, and Region_Urban, can be used to estimate an individual's income based on these variables."
    },
    {
        "contexts": "The Elevator Pitch is a precise and short (30-120 second) presentation format. Its name originates from the attempt of salesmen to enthuse potential customers and investors during the duration of an elevator ride - which seldomly took longer than 60 seconds. However, it can be used to 'sell' basically anything - an idea, a product, a company, or one's self. This does not have to happen in a seller-customer-relationship, but can also be of help when you want to make an impression on job interviewers or new acquaintances. It also applies to research, when an idea has to be 'sold' to potential supporters or to colleagues.",
        "summary": "An Elevator Pitch is a brief presentation, typically 30-120 seconds long, used to spark interest in an idea, product, company, or person. It's not limited to sales situations, but can also be used in job interviews, networking, or research.",
        "question": "What is an Elevator Pitch and when can it be used?",
        "ground_truths": "An Elevator Pitch is a short presentation used to generate interest in an idea, product, company, or person. It can be used in various situations, not just in sales, but also in job interviews, networking, or research."
    },
    {
        "contexts": "An Elevator Pitch is not done spontaneously. Instead, it should be well prepared and rehearsed so that it can be acted out whenever necessary. The structure of the Elevator Pitch may follow the AIDA scheme: Attention: raise attention, for example with an innovative introduction, a question, a controversial statement or a joke. Interest: raise Interest by highlighting your professional experiences, the product's qualities or the idea's USP. What about your offer is of interest to the listener? Desire: Make the listener want what you have to offer. Illustrate the benefits of them hiring you or supporting your idea. Action: A good Elevator Pitch should yield results, so you have to build up to these. Sketch the next steps, include a Call-To-Action and make sure that there is a follow-up conversation to the pitch.",
        "summary": "An Elevator Pitch should be well-prepared and rehearsed, not spontaneous. It often follows the AIDA scheme: Attention, Interest, Desire, and Action. The goal is to grab attention, generate interest, create desire for your offer, and prompt action.",
        "question": "How should an Elevator Pitch be prepared and what structure does it often follow?",
        "ground_truths": "An Elevator Pitch should be well-prepared and rehearsed. It often follows the AIDA scheme, which includes grabbing attention, generating interest, creating desire for the offer, and prompting action."
    },
    {
        "contexts": "(Active) Empathetic listening is a skill that helps to understand a dialogue partner on a deeper level. It strengthens the bond between conversation partners, soothes the work atmosphere during stressful phases, prevents the reproduction of injustice, and enriches creative thinking. This ability can become especially useful during qualitative interviews, customer/client/patient conversations, and conversations between co-workers, e.g. (trans)disciplinary researchers, or co-workers in any other field of work. It is best performed without time pressure.",
        "summary": "Empathetic listening is a skill that deepens understanding between conversation partners, improves the work atmosphere, prevents injustice, and enhances creative thinking. It's particularly useful in interviews, customer interactions, and co-worker discussions, especially without time pressure.",
        "question": "What are the benefits and applications of empathetic listening?",
        "ground_truths": "Empathetic listening strengthens the bond between conversation partners, soothes the work atmosphere during stressful phases, prevents the reproduction of injustice, and enriches creative thinking. It is especially useful during qualitative interviews, customer/client/patient conversations, and conversations between co-workers."
    },
    {
        "contexts": "It all starts with the setting. It is best to create a quiet and comfortable atmosphere, although this may no longer be necessary with rising experience with the skill. A comfortable atmosphere enhances the concentration on what is being said. Also, visual distractions, fidgeting or eating, and drinking should be held to a minimum. A setting free of distraction also includes taking enough time for the conversation, so that nothing must be rushed through and consequently enough patience to not only sit out the meeting but actively participate as the listener.",
        "summary": "The practice of empathetic listening begins with a quiet, comfortable setting free of distractions. This setting enhances concentration and allows for enough time and patience to actively participate in the conversation.",
        "question": "How should the setting be prepared for effective empathetic listening?",
        "ground_truths": "For effective empathetic listening, a quiet and comfortable atmosphere should be created, visual distractions, fidgeting or eating, and drinking should be minimized, and enough time should be taken for the conversation."
    },
    {
        "contexts": "However, your positioning towards the speaker is fundamental. This includes for one your actual pose and your body language, which should be open, genuine, and neither too neutral nor too emotionally charged with your own emotions. This also includes your actual positioning as a person. First, take yourself back. It is very important to be selfless and genuinely interested in the other. This creates the key element of what makes somebody listen to - and not only hear - others. To enhance your interest and show it to your dialogue partner, try to think of open-ended questions. You can also paraphrase the speaker\u2019s perspective to remember and internalize better what has been said.",
        "summary": "Your positioning towards the speaker is crucial in empathetic listening. This includes your body language, which should be open and genuine, and your personal positioning, which should be selfless and genuinely interested in the other person. Using open-ended questions and paraphrasing the speaker\u2019s perspective can enhance understanding.",
        "question": "What are the key elements of positioning in empathetic listening?",
        "ground_truths": "The key elements of positioning in empathetic listening include open and genuine body language, being selfless and genuinely interested in the other person, using open-ended questions, and paraphrasing the speaker\u2019s perspective."
    },
    {
        "contexts": "This way, it becomes easier to actively imagine the other one\u2019s experiences, ideas, and emotions. Since emotionally charged topics might come up, being emotionally available and tolerant is another key competence. This must exclude interruption of the speaker and may exclude criticism if the speaker does not precisely ask for it. Also, filling the silence with your own words instead of giving time for the speaker to think thoroughly and continue the talk is not desirable. At the end of the conversation, you may want to ask for a follow-up conversation, especially if time unexpectedly runs short or the topic was complex or emotionally challenging. Remember the well-being of the speaker is of the utmost importance. If no follow-up conversation is wished for, do not pressure the speaker for one.",
        "summary": "Empathetic listening involves actively imagining the speaker's experiences, ideas, and emotions. Being emotionally available and tolerant is key, as is avoiding interruptions or unnecessary criticism. Allowing for silence and considering a follow-up conversation are also important, with the speaker's well-being as a priority.",
        "question": "What are some important considerations during the process of empathetic listening?",
        "ground_truths": "During empathetic listening, it's important to actively imagine the speaker's experiences, ideas, and emotions, be emotionally available and tolerant, avoid interruptions or unnecessary criticism, allow for silence, consider a follow-up conversation if needed, and prioritize the speaker's well-being."
    },
    {
        "contexts": "Following Sidgwicks \"Methods of ethics\", ethics can be defined as ''the world how it ought to be''. Derek Parfit argued that if philosophy be a mountain, Western philosophy climbs it from three sides: The first side is Utilitarianism, which is widely preoccupied with the question how we can evaluate the outcome of an action. The most ethical choice would be the action that creates the most good for the largest amount of people. The second side is reason, which can be understood as the human capability to reflect what one is ought to do. Kant said much to this end, and Parfit associated it to the individual, or better, the reasonable individual. The third side of the mountain is the social contract, which states that a range of moral obligations are agreed upon in societies, a thought that was strongly developed by Locke. Parfit associated this even wider, referring to the whole society in his triple theory.",
        "summary": "Ethics, as defined by Sidgwicks, is how the world ought to be. Derek Parfit proposed that Western philosophy approaches this from three angles: Utilitarianism, reason, and the social contract. Utilitarianism focuses on the outcome of an action, aiming for the greatest good for the most people. Reason is the human ability to reflect on what one should do, associated with the individual. The social contract refers to agreed moral obligations in societies, associated with the whole society.",
        "question": "What are the three sides of philosophy as proposed by Derek Parfit?",
        "ground_truths": "The three sides of philosophy as proposed by Derek Parfit are Utilitarianism, reason, and the social contract."
    },
    {
        "contexts": "Personally, I think ethics matters deeply for statistics. Let me try to convince you. Looking at the epistemology of statistics, we learned that much of modern civilisation is built on statistical approaches, such as the design and analysis of Experiments or the correlation of two continuous variables. Statistics propelled much of the exponential growth in our society, and Statistics is responsible for many of the problems we currently face through our unsustainable behaviour. After all, Statistics was willing to utilize means and accomplish goals that led us more into the direction of a more unsustainable path. Many would argue now that if statistics were a weapon, itself would not kill. Instead, it would be the human hand that uses it. This is insofar true as statistics would not exist without us, just as weapons were forged by us. However, I would argue that statistics are still deeply normative, as they are associated with our culture, society, social strata, economies and so much more. This is why we should embrace a critical perspective on statistics. Much in our society depends on statistics, and many decisions are taken because of statistics. As we have learned, some of these statistics might be problematic or even wrong, and consequently, this can render the decisions wrong as well. More strangely, our statistics can be correct, but as we have learned, statistics can even then contribute to our downfall, for instance when they contribute to a process that leads to unsustainable production processes. We may calculate something correctly, but the result can be morally wrong. Ideally, our statistics would always be correct, and the moral implications that follow out of our actions that are informed by statistics are also right.",
        "summary": "Ethics plays a significant role in statistics, which has greatly influenced modern civilization and its growth. However, statistics have also contributed to many current problems due to unsustainable practices. While some argue that statistics, like weapons, are neutral tools used by humans, others believe that statistics are deeply normative, reflecting our culture, society, and economy. Therefore, a critical perspective on statistics is necessary, as they can lead to incorrect or morally wrong decisions, even when calculated correctly.",
        "question": "Why is a critical perspective on statistics necessary?",
        "ground_truths": "A critical perspective on statistics is necessary because they can lead to incorrect or morally wrong decisions, even when calculated correctly."
    },
    {
        "contexts": "However, statistics is more often than not seen as something that is not normative, and some people consider statistics to create objective knowledge. This is rooted in the deep traditions and norms of the disciplines where statistics are an established methodological approach, and in the history and theory of science that governs our research. Many scientists are regrettably still positivists, and often consider the knowledge they create to be objective. More often than not, this is not a conscious choice, but the combination of unreflected teachers in some education system in general. Today, obvious moral dilemmas and ambiguities are generally part of complex ethical pre-checks in many study designs, for which medicine provides the gold standard. Here, preventing blunders was established early on, and is now part of the canon of many disciplines, with medicine leading the way. Such problems often deal with questions of sample size, randomisation and the question when a successful treatment should be given to all participants. These are indirect reflections on validity and plausibility within the study design, acknowledging that failures or flaws in these elements may lead to biased or even plain wrong results of the study.",
        "summary": "Statistics is often viewed as non-normative and a source of objective knowledge, a belief rooted in the traditions and norms of disciplines where statistics are used, and in the history and theory of science. Many scientists, often unconsciously, still hold positivist views, considering their knowledge to be objective. Today, ethical pre-checks, including considerations of sample size and randomisation, are part of many study designs, with medicine setting the standard. These checks reflect on the validity and plausibility of the study design, acknowledging that flaws can lead to biased or incorrect results.",
        "question": "What are some considerations in the ethical pre-checks in study designs?",
        "ground_truths": "Some considerations in the ethical pre-checks in study designs include questions of sample size, randomisation and when a successful treatment should be given to all participants."
    },
    {
        "contexts": "When looking at the relation between ethics and statistics from the perspective of statistics, there are several items which can help us understand their interactions. First and foremost, the concept of validity can be considered to be highly normative. Two extreme lines of thinking come into mind: One technical, orientating on mathematical formulas; and the other line of thinking, being informed actually by the content. Both are normative, but of course there is a large difference between a model being correct in terms of a statistical validation, and a model approximating a reality that is valid. Empirical research makes compromises by looking at pieces of the puzzle of reality. Following critical realism, we may be able to unlock the strata of everything we can observe (the \u2018empirical\u2019, as compared to the \u2018real\u2019 and the \u2018actual\u2019), but ultimately this will always be a subjective perspective. Hence empirical research will always have to compromise as we choose versions of reality in our pursuit for knowledge.",
        "summary": "The relationship between ethics and statistics involves the concept of validity, which is highly normative. There are two main lines of thinking: one technical, based on mathematical formulas, and the other informed by the content. Both are normative, but there's a significant difference between a statistically valid model and a model that approximates a valid reality. Empirical research, which is always subjective, makes compromises in its pursuit of knowledge.",
        "question": "What are the two main lines of thinking in the relationship between ethics and statistics?",
        "ground_truths": "The two main lines of thinking in the relationship between ethics and statistics are one that is technical, based on mathematical formulas, and another that is informed by the content."
    },
    {
        "contexts": "If there is such a thing as general truth, you may find it in philosophy and probably thus also in ethics. Interestingly, philosophy originally served to generate criteria for validity in empirical research. However, knowledge in philosophy itself will never be empirical, at least not if it equally fulfils the criterion of being generally applicable. Through empirical research, we can observe and understand and thus generate knowledge, while philosophy is about the higher norms, principles and mechanism that transcend the empirical. Statistics is able to generate such empirical knowledge, and philosophy originally served to generate criteria for validity of this empirical process. Philosophy and empirical branches of science were vividly interacting in the Antique, they started to separate with the Enlightenment. The three concepts that (Western?) philosophy is preoccupied with - utilitarianism, reason and social contract - dissolved into scientific disciplines as economics, psychology, social science or political science.",
        "summary": "Philosophy, which originally served to generate criteria for validity in empirical research, is about higher norms, principles, and mechanisms that transcend the empirical. While empirical research generates knowledge through observation and understanding, philosophy itself will never be empirical. The interaction between philosophy and empirical branches of science, which began in the Antique, started to separate with the Enlightenment. The three concepts that Western philosophy is preoccupied with - utilitarianism, reason, and social contract - have dissolved into scientific disciplines.",
        "question": "What are the three concepts that Western philosophy is preoccupied with?",
        "ground_truths": "The three concepts that Western philosophy is preoccupied with are utilitarianism, reason, and social contract."
    },
    {
        "contexts": "Statistics did however indirectly contribute to altering our moral values. Take utilitarianism. It depends deeply on our moral values how we calculate utility, and hence how we manage our systems through such calculations. Statistics in itself could be argued to be non-normative if it dealt with totally arbitrary data that has no connection to any reality. However, as soon as statistics deals with data of any version of reality, we make Statistics normative by looking at these versions of reality. It is our subjective views that make statistics no longer objective. One might argue that physics would be an exception, and it probably is. It is more than clear however that social sciences, economics, ecology and psychology offer subjective epistemological knowledge, at least from a viewpoint of critical realism. The numbers are clear, yet our view of them is not. Consider for instance that on the one hand, world hunger decreased over the last decades and more people get education or medical help than ever before. At the same time, other measures propose that inequality increases, for instance income inequality.",
        "summary": "Statistics indirectly contributes to altering our moral values, such as utilitarianism, which depends on how we calculate utility. While statistics could be non-normative when dealing with arbitrary data, it becomes normative when dealing with data of any version of reality. This is due to our subjective views, which make statistics no longer objective. Social sciences, economics, ecology, and psychology offer subjective epistemological knowledge. Despite clear numbers, our view of them is not always clear, as seen in the contrasting measures of decreasing world hunger and increasing income inequality.",
        "question": "How does statistics indirectly contribute to altering our moral values?",
        "ground_truths": "Statistics indirectly contributes to altering our moral values by how we calculate utility, which is deeply dependent on our moral values. When statistics deals with data of any version of reality, it becomes normative, influenced by our subjective views."
    },
    {
        "contexts": "Another example is the question of [[Non-equilibrium dynamics|non-linearity]]. Preferring non-linear relationships to linear relationships is something that has become more important especially recently due to the wish to have a higher predictive power, to understand non-linear shifts, or for other reasons. Bending models - mechanically speaking - into a situation that allows to increase the model fit (that is, how much the model can predict) comes with the price that we sacrifice any possibility of [[Causality|causal]] understanding, since the zig-zag relations of non-linear models are often harder to match with our theories. While of course there are some examples of rapid shifts in systems, this has next to nothing to do with [[Glossary|non-linearity]] that is assumed in many modern statistical analysis schemes and the predictive algorithms whose data power over us governs wide parts of our societies. Many disciplines gravitated towards non-linear statistics in the last decades, sacrificing explainable relationships for an ever increasing model fit. Hence the clear demarcation between [[:Category:Deductive|deductive]] and [[:Category:Inductive|inductive]] research became blurry, adding an epistemological/ontological burden on research. In many papers, the question of plausibility and validity has been deeply affected by this issue, and I would argue that this casts a shadow on the reputation of science. Philosophy alone cannot solve this problem, since it looks at the general side of reality. Only by teaming up with empirical research, we may arrive at measures of validity that are not on a high horse, but also can be connected to the real world. To this end, I would currently trust critical realism the most, yet time will tell where theory of science will lead us in the future.",
        "summary": "The preference for non-linear relationships in statistical models has increased due to their higher predictive power. However, this comes at the cost of sacrificing causal understanding, as non-linear models are harder to match with theories. This shift towards non-linearity has blurred the line between deductive and inductive research, adding an epistemological burden on research. This issue has affected the plausibility and validity of many papers, casting a shadow on the reputation of science. Philosophy alone cannot solve this problem, it needs to team up with empirical research to arrive at valid measures.",
        "question": "Why has the shift towards non-linear statistical models affected the reputation of science?",
        "ground_truths": "The shift towards non-linear statistical models has affected the reputation of science because it sacrifices causal understanding for higher predictive power, blurs the line between deductive and inductive research, and adds an epistemological burden on research. This has affected the plausibility and validity of many papers."
    },
    {
        "contexts": "Another obvious example regarding the normativity of statistical analysis are constructs. [[To Rule And To Measure|Constructs]] are deeply normative, and often associated to existing norms within a discipline, and may even transcend whole world views. The Likert Scale in psychology is such an example. The obvious benefit of that scale is that it may unlock perceptions form a really diverse array of people, which is a positive aspect of such a scaling. Hence it became part of the established body of knowledge, and much experience is available on the benefits and drawbacks of this scaling, yet it is a normative choice whether we use it or not. Often, scales are even part of the tacit signature of cultural and social norms, and these indirectly influence the science that is conducted in this setting. An example would be the continuous Arabian numbering that dominates much of Western thinking, and that many of us grow into these days. Philosophy and especially cultural studies have engaged in such questions increasingly in the last decades, often focusing on isolated constructs or underlying fundamental aspects such as racism, colonialism and privilege. It will be challenging to link these important and timely concepts directly to the flaws of empirical research. These days, luckily, I recognize a greater awareness among empirical researchers to avoid errors implemented through such constructs. Much needs to be done, but I conceive reason for hope as well. Personally, I believe that constructs will always be associated to personal identity and thus pose a problem for a general acceptance. Time will tell if and how this gap can be bridged.",
        "summary": "Constructs in statistical analysis are deeply normative and often associated with existing norms within a discipline. The Likert Scale in psychology is an example of such a construct. These scales are part of the tacit signature of cultural and social norms, influencing the science conducted in this setting. Philosophy and cultural studies have increasingly engaged in questions about these constructs, focusing on aspects such as racism, colonialism, and privilege. There is a growing awareness among researchers to avoid errors implemented through such constructs, but much work still needs to be done.",
        "question": "How do constructs influence the science conducted in a particular setting?",
        "ground_truths": "Constructs influence the science conducted in a particular setting as they are deeply normative and often associated with existing norms within a discipline. These scales are part of the tacit signature of cultural and social norms, indirectly influencing the science conducted in this setting."
    },
    {
        "contexts": "Science often claims to create objective knowledge. Famously, Karl Popper and others stated that this claim is false - or should at least be questioned. Still, it is not only perceived by many people that science is de facto objective, but more importantly, many scientists often claim that the knowledge they produce is objective. Large parts of academia like to spread an air of superiority and arrogance, which is one of many explanations for the science-society-gap. When scientists should talk about potential facts, they often just talk about \u2018facts\u2019. When they could highlight our inability as scientists to grasp the true nature of reality, they instead present their version of reality to be the best or maybe even the only version of reality. Of course, this is not only their fault, since society increasingly demands easy explanations. A philosopher would be probably deeply disturbed if they understood how statistics are applied today, but even more disturbed if this philosopher saw how statistics are being interpreted and marketed. Precision and clarity in communicating results, as well as information on bias and caveats, are missing more often that we should hope for.",
        "summary": "Science often claims to create objective knowledge, a claim that has been questioned by philosophers like Karl Popper. Many scientists claim that their knowledge is objective, contributing to a perceived superiority and arrogance in academia. This contributes to the science-society gap. Scientists often present their version of reality as the best or only version, which can be problematic. The application and interpretation of statistics today would likely disturb a philosopher, as precision, clarity, and information on bias and caveats are often missing.",
        "question": "Why would a philosopher be disturbed by the current application and interpretation of statistics?",
        "ground_truths": "A philosopher would be disturbed by the current application and interpretation of statistics because precision, clarity, and information on bias and caveats are often missing in the communication of results."
    },
    {
        "contexts": "There are many facets that could be highlighted under such a provocative heading. Since Larry Laudan, it has become clear that the assumption that developments of science which are initiated by scientists are reasonable, is a myth at best. Take the example of one dominating paradigm in science right now: Publish or perish. This paradigm highlights the current culture dominating most parts of science. If you do not produce a large amount of peer-reviewed publications, your career will not continue. This created quite a demand on statistics as well, and the urge to arrive at significant results that probably created frequent violations of Occam's razor (that is, things should be as complex as necessary, but as simple as possible). The reproducibility crisis in psychology is one example of these developments, yet all other disciplines building on statistics struggle to this end, too. Another problem is the fact that with this ever-increasing demand for scientific innovation, models evolve, and it is hard to catch up. Thus, instead of having robust and parsimonious models, there are more and more unsuitable and overcomplicated models.",
        "summary": "The 'publish or perish' paradigm in science has led to a high demand for significant statistical results, often leading to violations of Occam's razor. This has resulted in a reproducibility crisis in psychology and other disciplines that rely on statistics. The constant demand for scientific innovation has led to the evolution of models, making it difficult to keep up and resulting in increasingly unsuitable and overcomplicated models.",
        "question": "What impact has the 'publish or perish' paradigm had on the use of statistics in science?",
        "ground_truths": "The 'publish or perish' paradigm has led to a high demand for significant statistical results, often leading to violations of Occam's razor. This has resulted in a reproducibility crisis in psychology and other disciplines that rely on statistics. The constant demand for scientific innovation has led to the evolution of models, making it difficult to keep up and resulting in increasingly unsuitable and overcomplicated models."
    },
    {
        "contexts": "One could say that utilitarianism created the single largest job boost for the profession of statisticians. From predicting economic development, to calculating engineering problems, to finding the right lever to tilt elections, statistics are dominating in almost all aspects of modern society. Calculating the maximisation of utility is one of the large drivers of change in our globalised economy. But it does not stop there. Followers on social media and reply time to messages are two of many factors how to measure success in life these day, often draining their direct human interactions in the process, and leading to distraction or even torment. Philosophy is deeply engaged in discussing these conditions and dynamics, yet statistics needs to embed these topics, which are strongly related to ethics, more into their curriculum. If we become mercenaries for people with questionable goals, then we follow a long string of people that maximised utility for better or worse. History teaches us of the horrors that were conducted in the process of utility maximisation, and we need to end this string of willing aids to illegitimate goals. Instead, statistics should not only be about numbers, but also about the fact that these numbers have meaning. Numbers are not abstract representations of the world, but can have different meanings for different people. Hence numbers can add information that is missing, and can serve as a starting point for an often necessary deeper reflection.",
        "summary": "Utilitarianism has greatly increased the demand for statisticians, who are now involved in various aspects of modern society, from economic predictions to social media analytics. However, this focus on utility maximisation has led to ethical concerns, with statistics being used to achieve questionable goals. It is argued that statistics should not just be about numbers, but also about their meaning, and that this should be incorporated into the curriculum.",
        "question": "What ethical concerns are raised by the use of statistics in utilitarianism?",
        "ground_truths": "The focus on utility maximisation has led to ethical concerns, with statistics being used to achieve questionable goals. It is argued that statistics should not just be about numbers, but also about their meaning, and that this should be incorporated into the curriculum."
    },
    {
        "contexts": "Another problem that emerges out of statistics as seen from a perspective of ethics is the inability to accept different versions of reality. Statistics often arrives at a specific construction of reality, which is the confirmed or rejected. After all, most applications and interpretations in statistics are still from a standpoint of positivism. At best, such a version of reality becomes then more and more refined, until it becomes ultimately the accepted reality. Today, more people would agree that versions of reality are mere snapshots and are probable to change in the future. Linking our empirical snapshots or reality with ethical concepts I possible but challenging. Hedonism would be one example where statistics can serve as a blunt and unreflected tool, with the hedonist being the center of the universe. However, personal identity is increasingly questioned, and may play a different role in the future than it does today. We have to acknowledge that statistics is about epistemological knowledge, while ethics can be about ontological truths. Statistics may thus build a refined version of reality, while ethics can claim and alter between different realities that allow for the reflection of higher concepts or principles that transcend subjective realities and are thus non-empirical in a sense that these principles may be able to integrate all empirical realities. While this is one of the most daring endeavors in philosophy that emerged over the last decades, it builds on the premise that ethics can start lines of thinking outside of empirical realities. This freedom in ethics i.e., through Thought Experiments, is not possible in statistics. In Statistics, the empirical nature requires a high workload and time effort, which creates a penalty that ultimately makes statistics less able to compromise or integrate different versions of ethics. Future considerations need to clearly indicate what the limitations of statistics are, and how this problem of linking to other lines of thinking i.e. ethicscan be solved, even if such other worldviews violate statistical results or assumptions that characterize empirical realities. In other words, you can consider your subjective perspective as your epistemological reality, and I would argue that experience in statistics can enable you to develop a potentially better version of your epistemological reality. This may even change your ontological reality, as it can interact with your ontological principles, i.e. your moral views and ethical standpoints.",
        "summary": "Statistics often constructs a specific version of reality, which is either confirmed or rejected. This reality becomes more refined until it is accepted. However, linking these empirical realities with ethical concepts is challenging. Statistics is about epistemological knowledge, while ethics can be about ontological truths. The empirical nature of statistics makes it less able to compromise or integrate different versions of ethics. Future considerations need to indicate the limitations of statistics and how to link it with other lines of thinking such as ethics.",
        "question": "Why is it challenging to link empirical realities with ethical concepts?",
        "ground_truths": "It is challenging to link empirical realities with ethical concepts because statistics, which is about epistemological knowledge, is less able to compromise or integrate different versions of ethics due to its empirical nature."
    },
    {
        "contexts": "A famous thought experiment from Derek Parfit is about 50 people dying of thirst in the desert. Imagine you could give them one glass of water, leaving a few drops for each person. This would not prevent them from dying, but it would still make a difference. You tried to help, and some would even argue, that if these people die, they still die with the knowledge that somebody tried to help them. Hence your actions may matter even if they are ultimately futile, yet classical ethics do not manage to account for such imperceptible consequences of our actions. Equally would a utilitarian statistician tell you that calculating the number of drops for each person is not only hard to measure, but would probably anyway evaporate before the people could sip it.",
        "summary": "Derek Parfit's thought experiment involves 50 people dying of thirst in the desert and the futile attempt to help them with one glass of water. This highlights the imperceptible consequences of our actions that classical ethics fail to account for. A utilitarian statistician would argue that calculating the number of water drops for each person is difficult and probably futile as the water would evaporate before the people could drink it.",
        "question": "What does Derek Parfit's thought experiment highlight about classical ethics?",
        "ground_truths": "Derek Parfit's thought experiment highlights that classical ethics fail to account for the imperceptible consequences of our actions."
    },
    {
        "contexts": "One last example of the relation between ethics and statistics is the problem of inaction. What if you could save 5 people, but you would know that somebody else then dies through the action that saves the other five. Many people prefer not to act. To them, an inaction is ethically more ok than the actual act, which would make them responsible for the death of one person, even if they saved five times as many people. This is also related to knowledge and certainty. Much knowledge exists that should lead people to act, but they prefer to not act at all. Obviously, this is a great challenge, and while psychology investigates this knowledge-action gap, I propose that it will still be here to stay for an unfortunately long time. If people were able to emotionally connect to information derived from numbers, just as the example above, and would be able to act reasonable, much harm could be avoided or at least minimised. This consequentialism is widely missing to date, albeit much of our constructed systems are based on quite similar rules. For instance, many people today do in fact understand that their actions have a negative impact on the climate, but nevertheless continue with these actions. Hence it is not only an inaction between knowledge and action, but also a gap between constructed systems and individuals. There is much to explore here, and as Martha Nussbaum rightly concluded in the Monarchy of Fear, even under dire political circumstances, hope really is both a choice and a practical habit. Hence it is not only utilitarianism that links statistics to ethics, but these questions also links to the social contract, and how we act reasonably or unreasonably. Many experimental modifications of the Trolley experiment were investigated by Psychology, and these led to many interpretations on why people act unreasonably. One main result is that humans may tend to minimize the harm they do to others, even if this may create more harm to people on an indirect level. Pushing a lever is a different thing from pushing a person.",
        "summary": "The relationship between ethics and statistics is exemplified by the problem of inaction, where people often choose not to act even when they could save lives, due to the ethical implications of causing harm to others. This is linked to the gap between knowledge and action, and the lack of emotional connection to statistical information. Despite understanding the negative impacts of their actions, many people continue to act in ways that harm the environment. This gap extends to the disconnect between constructed systems and individuals. The Trolley experiment in psychology has explored why people act unreasonably, suggesting that humans may try to minimize direct harm, even if it causes more indirect harm.",
        "question": "What is the relationship between ethics, statistics, and the problem of inaction?",
        "ground_truths": "The relationship between ethics and statistics is exemplified by the problem of inaction, where people often choose not to act even when they could save lives, due to the ethical implications of causing harm to others. This is linked to the gap between knowledge and action, and the lack of emotional connection to statistical information. Despite understanding the negative impacts of their actions, many people continue to act in ways that harm the environment. This gap extends to the disconnect between constructed systems and individuals."
    },
    {
        "contexts": "The Feynman lectures provide the most famous textbook volumes on physics. Richard Feynman and his coauthors compiled these books in the 1960, yet to this day, many consider these volumes to be the most concise and understandable introduction to physics. Granted, Feynman's work is brilliant. At the same time, however, this fact also means that in the 1960s the majority of the knowledge necessary for an introduction in physics was already available. While we may consider that much happened ever since, students can still use these textbooks today. Something similar can be claimed for basic statistics, although it should be noted that physics is a scientific discipline, while statistics is a scientific method. Some disciplines use different methods, and many disciplines use statistics as a method. However, the final word on statistics has not been said, as the differences between probability statistics and Bayesian statistics are not yet deeply explored. Especially Bayesian statistics may provide a vital link to experiments, yet this has been hardly explored to date. Critical spirits may say that the teaching of statistics is often the most normal part of any sort of normal science study program, and this reputation led to statistics more often than not being characterized by a lack of excitement for students. We should not only know and teach the basics, but also engage in the discussion what these all may mean.",
        "summary": "The Feynman lectures, compiled in the 1960s, are still considered a concise introduction to physics, indicating that much of the necessary knowledge was already available then. Similarly, basic statistics, a scientific method used across disciplines, has foundational knowledge that remains relevant. However, the field of statistics is not fully explored, particularly the differences between probability and Bayesian statistics. The teaching of statistics is often seen as mundane, but there is a need to engage in discussions about its implications.",
        "question": "What is the significance of the Feynman lectures and how does it relate to the field of statistics?",
        "ground_truths": "The Feynman lectures, compiled in the 1960s, are still considered a concise introduction to physics, indicating that much of the necessary knowledge was already available then. Similarly, basic statistics, a scientific method used across disciplines, has foundational knowledge that remains relevant. However, the field of statistics is not fully explored, particularly the differences between probability and Bayesian statistics."
    },
    {
        "contexts": "Philosophy can be thought together well with statistics. Unfortunately until today they are not very much associated with each other. Something surprisingly similar can be said about philosophy. While much of the basic principles are known, these are hardly connected. Philosophy hence works ever deeper on specifics, but most of its contributors move away from a unified line of thinking. This is what makes Derek Parfits work stand out, since he tried to connect the different dots, and it will be the work of the next decades to come to build on his work, and improve it if necessary. While philosophy and statistics both face a struggle to align different lines of thinking, it is even more concerning how little these two are aligned with each other. Statistics make use of philosophy at some rare occasions, for instance when it comes to the ethical dilemmas of negatively affecting people that are part of a study. While these links are vital for the development of specific aspects of statistics, the link between moral philosophy and statistics has hardly been explored so far. In order to enable statistics to contribute to the question how we ought to act, a systematic interaction is needed. I propose that exploring possible links is a first step, and we start to investigate how such connections work. The next step would be the proposal of a systematic conceptualisation of these different connections. This conceptualisation would then need to be explored and amended, and this will be very hard work, since both statistics and moral philosophy are scattered due to a lack of a unified theory, and hardly anyone is versatile in both fields.. This makes a systematic exploration of such a unifying link even more difficult to explore is the question who would actually do that. Within most parts of the current educational system, students learn either empirical research and statistics, or the deep conceptual understanding of philosophy. Only when we enable more researcher to approach from both sides - empirical and conceptual - will we become increasingly able to bridge the gap between these two worlds.",
        "summary": "Philosophy and statistics, despite their potential for synergy, are rarely associated. Philosophy, while having known basic principles, lacks a unified line of thinking, a gap that Derek Parfit attempted to bridge. Similarly, statistics occasionally uses philosophy, particularly in ethical dilemmas, but the link between moral philosophy and statistics is underexplored. To enable statistics to contribute to moral questions, a systematic interaction is needed, starting with exploring possible links and proposing a systematic conceptualisation. However, this is challenging due to the lack of a unified theory in both fields and the educational system's focus on either empirical research and statistics or philosophy.",
        "question": "Why is the integration of philosophy and statistics challenging and how can it be addressed?",
        "ground_truths": "The integration of philosophy and statistics is challenging due to the lack of a unified theory in both fields and the educational system's focus on either empirical research and statistics or philosophy. To address this, a systematic interaction is needed, starting with exploring possible links and proposing a systematic conceptualisation."
    },
    {
        "contexts": "Most links between statistics and ethics are tacit, sparse and more often than not singular. Overall, the link is difficult to establish because much of ethics are about concepts and thus holistic ways of thinking, while statistics engages in the empirical, which is after all subjective. I would however argue that it is still possible to link these two, and to illustrate this I use the example of Occam\u2019s razor. Remember: \u201cEverything needs to be as simple as possible, and as complex as necessary\u201d. I would propose now that Occam\u2019s razor is not really a principle - like justice or altruism - but a heuristic. Heuristics can help us guide our actions, or better, the way",
        "summary": "The connection between statistics and ethics is often implicit and not easily defined, as ethics involves holistic concepts while statistics is empirical and subjective. However, the author argues that a link can be established through the example of Occam\u2019s razor, which is proposed to be a heuristic rather than a principle, guiding our actions.",
        "question": "How does the author propose to link statistics and ethics?",
        "ground_truths": "The author proposes to link statistics and ethics through the example of Occam\u2019s razor, which is considered a heuristic that can guide our actions."
    },
    {
        "contexts": "The Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.",
        "summary": "The Feynman Method is a learning technique that emphasizes explaining a topic to someone as the best way to understand it. It promotes understanding over memorization, making it easier to apply new knowledge to different situations.",
        "question": "What is the main principle of the Feynman Method?",
        "ground_truths": "The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it."
    },
    {
        "contexts": "The Feynman method is a simple, circular process: Select the topic you want to learn more about. Find someone to talk to. Make notes. Have a look at your notes and try to find more information. Now explain the topic again. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!",
        "summary": "The Feynman method involves choosing a topic, explaining it to someone, taking notes, seeking more information, and explaining the topic again. This process is repeated until the topic can be confidently explained to anyone.",
        "question": "How does the Feynman method work?",
        "ground_truths": "The Feynman method works by choosing a topic, explaining it to someone, taking notes, seeking more information, and explaining the topic again. This process is repeated until the topic can be confidently explained to anyone."
    },
    {
        "contexts": "With a rise in knowledge, it became apparent that the controlled setting of a laboratory was not enough. First in astronomy, but then also in agriculture and other fields the notion became apparent that our reproducible settings may sometimes be hard to achieve. Observations can be unreliable, and error of measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter. The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions.",
        "summary": "As knowledge increased, it became clear that laboratory settings were insufficient for certain types of research, particularly in fields like astronomy and agriculture. The variability of nature and the need for more food due to population growth led to the development of field experiments.",
        "question": "Why did the development of field experiments become necessary?",
        "ground_truths": "The development of field experiments became necessary due to the limitations of laboratory settings, the variability of nature, and the increasing demand for food due to population growth."
    },
    {
        "contexts": "The analyis of variance was actually inspired widely by field experiments, as Fisher was working with crop data which proved as a valuable playground to develop and test his statistical approaches. It is remarkable that the p-value was actually chosen by him and did not follow any wider thinking within a community. Probably not since Linne or Darwin did a whole century of researchers commit their merits to a principle derived by one person, as scientific success and whole careers are build on results measured by the p-value. It should however be noted that the work needed to generate enough replicates to measure probability with Fishers measure of 0.05 was actually severe within crop experiments when compared to lab settings.",
        "summary": "Field experiments greatly influenced the analysis of variance, with Fisher's work on crop data leading to the development of the p-value. The effort required to generate enough replicates for a significant p-value was much greater in field experiments than in lab settings.",
        "question": "How did field experiments influence the analysis of variance and the use of the p-value?",
        "ground_truths": "Field experiments influenced the analysis of variance and the use of the p-value through Fisher's work on crop data, which led to the development of the p-value and highlighted the effort required to generate enough replicates for a significant p-value in field experiments."
    },
    {
        "contexts": "Field experiments try to compensate for gradients that infer variance within a sample design by increasing the number of samples. However, in order to uncouple the effects of underlying variance, randomisation is central. Take the example of an agricultural field that has more sandy soils on one end, and more loamy soils on the other end. If now all samples of one treatment would be on the loamy end of the field, and all samples of another treatment would be at the sandy end of the field, then the treatment effect would contain an artefact of the soil gradient. Randomisation in fields is hence essential to maximize independence of the treatment effect from the underlying variance.",
        "summary": "Field experiments use increased sample sizes and randomisation to compensate for variance within a sample design. For example, in an agricultural field with different soil types, randomisation ensures that the treatment effect is independent of the underlying variance.",
        "question": "Why is randomisation important in field experiments?",
        "ground_truths": "Randomisation is important in field experiments to ensure that the treatment effect is independent of the underlying variance, such as differences in soil type in an agricultural field."
    },
    {
        "contexts": "In order to tame the variance of the real world, blocking was a plausible approach. By basically introducing agricultural fields as blocks, the variance from individual blocks can be tamed. This was one of the breakthroughs, as the question of what we want to know i.e. hypothesis testing, was statistically uncoupled from the question what we do not want to know, i.e. the variance inferred from individual blocks. Consequently, the samples and treatment combinations need to be randomised within the different blocks, or can be alternatively replicated within these blocks. This has become established as a standard approach in the designing of experiments, often for rather pragmatic reasons. For instance are agricultural fields often showing local characteristics in terms of soil and microclimate, and these should be tamed by the clear designation of blocks and enough blocks in total within the experiment. The last point is central when thinking in terms of variance, since it would naturally be very hard to think in terms of variance regarding e.g. only two blocks. A higher number of blocks allow to better tame the block effect. This underlines the effort that often needs to go into designing experiments, since a sufficient number of blocks would basically mean that the effort can be multiplied by the number of blocks that are part of the design. Ten blocks means ten times as much work, and maybe with the result that there is no variance among the blocks overall.",
        "summary": "Blocking is a method used to control the variance in field experiments by introducing agricultural fields as blocks. This allows for the variance from individual blocks to be tamed and the hypothesis testing to be statistically uncoupled from the unwanted variance. The samples and treatment combinations are then randomized within these blocks. This method requires a significant amount of effort in the design of the experiment, as the number of blocks can greatly increase the workload.",
        "question": "What is the purpose of blocking in field experiments?",
        "ground_truths": "The purpose of blocking in field experiments is to control the variance by introducing agricultural fields as blocks. This allows for the variance from individual blocks to be tamed and the hypothesis testing to be statistically uncoupled from the unwanted variance."
    },
    {
        "contexts": "Within field experiments, one factor is often nested within another factor. The principle of nestedness works generally like the principle of Russian dolls: Smaller ones are encapsulated within larger ones. For instance can a block be seen as the largest Russian doll, and the treatments are then nested in the block, meaning each treatment is encapsulated within each block. This allows for a testing where the variance of the block effect can be minimised, and the variance of the treatment levels can be statistically compared. Quite often the variance across different levels of nestedness is a relevant information in itself, meaning for instance how much variance is explained by a different factor. Especially spatially nested designs can have such a hierarchical structure, such as neighbourhoods within cities, streets within neighbourhoods and houses in streets. The nested structure would in this case be Cities/neighbourhoods/streets/houses. Just as with blocks, a nested structure demands a clear designing of an experiment, and greatly increase the sample size. Hence such a design should be implemented after much reflection, based on experience, and ideally by consultation with experts both in statistics as well as the given system.",
        "summary": "In field experiments, one factor is often nested within another, similar to Russian dolls. For example, a block can be seen as the largest doll, with treatments nested within each block. This allows for the variance of the block effect to be minimized and the variance of the treatment levels to be statistically compared. Nested designs, which can have a hierarchical structure, require careful design and can greatly increase the sample size.",
        "question": "What is the concept of nestedness in field experiments?",
        "ground_truths": "The concept of nestedness in field experiments refers to one factor being nested within another, similar to Russian dolls. For example, a block can be seen as the largest doll, with treatments nested within each block. This allows for the variance of the block effect to be minimized and the variance of the treatment levels to be statistically compared."
    },
    {
        "contexts": "The analysis of field experiments demands great care, since this is mostly a deductive approach where equal emphasis is put on what we understand, and what we do not understand. Alternatively, we could highlight that rejection of the hypothesis is the most vital step of any experiment. In statistical terms the question of explained vs. unexplained variance is essential. The first step is however checking the p-value. Which treatments are significant, and which ones are not? When it comes to two-way ANOVAs, we may need to reduce the model to obtain a minimum adequate model. This basically equals a reduction of the full model into the most parsimonious version, following Occam's razor. While some researchers tend to report the full model, with all non-significant treatments and treatment combinations, I think this is wrong. If we reduce the model, the p-values change. This can make the difference between a treatment that is significant, and a non-significant model. Therefore, model reduction is being advised for. For the sake of simplicity, this can be done by first reducing the highest level interactions that are not significant. However, the single effects always need to be included, as the single effects are demanded if interactions of a treatment are part of the model, even if these single effects are not significant.",
        "summary": "The analysis of field experiments requires careful consideration, with a focus on understanding what is known and unknown. The first step is to check the p-value to determine which treatments are significant. In two-way ANOVAs, it may be necessary to reduce the model to a minimum adequate model, following Occam's razor. This involves reducing the highest level interactions that are not significant, while still including the single effects, even if they are not significant.",
        "question": "What is the first step in the analysis of field experiments?",
        "ground_truths": "The first step in the analysis of field experiments is to check the p-value to determine which treatments are significant."
    },
    {
        "contexts": "Within ANOVA designs, the question whether a variable is a fixed or a random factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To this end, it is helpful to consult existing studies to differentiate between random and fixed factors. Current medical trials may consider many variables, and have to take even more random factors into account. Testing the impact of random factors on the raw data is often a first step when looking at initial data, yet this does not help if it is a purely deductive design. In this case, simplified pre-tests are often a first step to make initial attempts to understand the system and also check whether variables - both fixed or random - are feasible and can be utilised in the respective design. Initial pre-tests at such smaller scales are a typical approach in medical research, yet other branches of research reject them as being too unsystematic. Fisher himself championed small sample designs, and we would encourage pre-tests in field experiments if at all possible. Later flaws and errors in the design can be prevented, although form a statistical standpoint the value of such pre-tests may be limited at best.",
        "summary": "In ANOVA designs, determining whether a variable is a fixed or random factor can be challenging. Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed. Pre-tests are often used to understand the system and check the feasibility of variables.",
        "question": "What is the difference between fixed and random factors in ANOVA designs?",
        "ground_truths": "Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed."
    },
    {
        "contexts": "Acknowledging unexplained variance was a breakthrough in modern science, as we should acknowledge that understanding a phenomena fairly well now is better than understanding something perfectly, but never. In a sense was the statistical developing of uncertainty reflected in philosophical theory, as Karl Popper highlighted the imperfection of experiments in testing or falsification of theories. Understanding the limitations of the scientific endeavour thus became an important baseline, and the scientific experiments tried partly to take this into account through recognising the limitations of the result. What is however often confused is whether the theory is basically imperfect - hence the results are invalid or implausible - or whether the experiment was conducted in an imperfect sense, making the results unreliable. The imperfection to understand the difference between flaws in theory and flaws in conducting the experiment is a continuous challenge of modern science. When looking at unexplained variance, we always have to consider that our knowledge can be limited through theory and empirical conduct, and these two flaws are not clearly separated. Consequently, unexplained variance remains a blank in our knowledge, and should always be highlighted as such. As much as it is important to acknowledge what we know, it is at least equally important to highlight what we do not know.",
        "summary": "Recognizing unexplained variance was a significant advancement in science, emphasizing that partial understanding is better than no understanding. Karl Popper highlighted the imperfections of experiments in testing theories. It's often confused whether the theory is flawed or the experiment is conducted imperfectly. The challenge is to understand the difference between theoretical and experimental flaws. Unexplained variance, a gap in our knowledge, should always be highlighted.",
        "question": "Why is acknowledging unexplained variance important in scientific research?",
        "ground_truths": "Recognizing unexplained variance was a significant advancement in science, emphasizing that partial understanding is better than no understanding. It's often confused whether the theory is flawed or the experiment is conducted imperfectly. The challenge is to understand the difference between theoretical and experimental flaws. Unexplained variance, a gap in our knowledge, should always be highlighted."
    },
    {
        "contexts": "Interpreting results from field experiments demands experience. First of all, we shall interpret the p-value, and check which treatments and interactions are significant. Here, many researchers argue that we should report the full model, yet I would disagree. P-values in ANOVA summaries differ between the so called full models -which include all predictors- and minimum adequate models -which thrive to be the most parsimonious models. Model reduction is essential, as the changing p-values may make a difference between models that are reporting true results, or flawed probabilities that vaporize once the non-significant terms are subsequently reduced. Therefore, one by one we need to minimize the model in its complexity, and reduce the model until it only contains significant interaction terms as well as the maybe even non-significant single terms, which we have to include if the interaction is significant. This will give us a clear idea which treatments have a significant effect on the dependent variable. Second, when expecting model results we should interpret the sum of squares, thereby evaluating how much of the respective treatment is explain the effect of the dependent variable. While this is partly related to the p-value, it is also important to note how much variance is explained by potential block factors. In addition, it is also important to notice how much remains unexplained in total, as this residual variance indicates how much we do not understand using this experimental approach. This is extremely related to the specific contexts, and we need to be aware that knowledge of previous studies may aid us in understanding the value of our contribution. Lastly, we need to take further flaws into our considerations when interpreting results from field experiments. Are there extreme outliers. How do the residuals look like? Is any treatment level showing signs of an uneven distribution or gaps? Do the results seem to be representative? We need to be very critical of our own results, and always consider that the results reflect only a part of reality.",
        "summary": "Interpreting field experiment results requires experience. We should interpret the p-value and check which treatments are significant. P-values in ANOVA summaries differ between full models and minimum adequate models. Model reduction is crucial to avoid flawed probabilities. We should also interpret the sum of squares to evaluate the effect of the treatment on the dependent variable. It's important to note how much variance is explained by potential block factors and how much remains unexplained. We should also consider any flaws, outliers, uneven distribution or gaps in the results.",
        "question": "What are the key considerations when interpreting results from field experiments?",
        "ground_truths": "We should interpret the p-value and check which treatments are significant. Model reduction is crucial to avoid flawed probabilities. We should also interpret the sum of squares to evaluate the effect of the treatment on the dependent variable. It's important to note how much variance is explained by potential block factors and how much remains unexplained. We should also consider any flaws, outliers, uneven distribution or gaps in the results."
    },
    {
        "contexts": "Field experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for agriculture and other fields of production to thrive, but then also did medicine, psychology, ecology and even economics use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate.",
        "summary": "Field experiments revolutionized many scientific fields, including agriculture, medicine, psychology, ecology, and economics, by allowing for systematic testing of hypotheses. This led to more specific and detailed knowledge, such as the development of various antibiotics. However, this also resulted in unforeseen consequences like antibiotic resistance.",
        "question": "What impact did field experiments have on various scientific fields?",
        "ground_truths": "Field experiments revolutionized many scientific fields by allowing for systematic testing of hypotheses, leading to more specific and detailed knowledge. However, they also led to unforeseen consequences like antibiotic resistance."
    },
    {
        "contexts": "There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased.",
        "summary": "The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This can be attributed to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.",
        "question": "What is the replication crisis and how does it affect modern research?",
        "ground_truths": "The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models."
    },
    {
        "contexts": "Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam\u2019s razor also when designing a study.",
        "summary": "Preregistration of study designs, common in psychology and medicine, can prevent manipulation and improve replication. However, as research designs become more complex, especially in field experiments, the possibility of replication decreases due to unaccounted factors. This complexity sacrifices robustness, and clear documentation is one way to address these issues.",
        "question": "How can preregistration and clear documentation help address the replication crisis in modern research?",
        "ground_truths": "Preregistration of study designs can prevent manipulation and improve replication, while clear documentation can help address the issues introduced by the increasing complexity of research designs."
    },
    {
        "contexts": "A Fishbowl is a medium-length dialogue format for large groups, lead by a moderator. Four to five people discuss a question or topic while being surrounded by a typically much bigger audience, which is enabled to participate in the discussion by taking one of the chairs in the discussion group. The Fish Bowl format can be used for teaching (e.g. group discussions in seminars), for workshops (e.g. in work environments, events, organisational activities), or for stage discussions (e.g. on conferences). Depending on the contexts, the length of a fishbowl discussion can range from 15-20 minute sessions to more than an hour, with 30-45 minutes being rather typical.",
        "summary": "Fishbowl is a dialogue format for large groups, moderated by a leader. A small group discusses a topic while a larger audience can join the discussion by taking a chair in the group. It can be used in teaching, workshops, or stage discussions, with sessions typically lasting 30-45 minutes.",
        "question": "What is the Fishbowl format and how is it used?",
        "ground_truths": "The Fishbowl is a dialogue format for large groups, moderated by a leader. It involves a small group of people discussing a topic, while a larger audience can participate by taking a chair in the group. It can be used in various settings such as teaching, workshops, or stage discussions. The length of a Fishbowl discussion can vary, but typically lasts between 30-45 minutes."
    },
    {
        "contexts": "In a typical Fishbowl, four to five chairs are arranged in a circle, with a larger number of chairs forming another circle around them. In a variation, only two chairs may be placed for a face-to-face conversation. Participants for the discussions, including one moderator, occupy the chairs, with the audience filling the surrounding chairs. This arrangement (see visualisation) gives the Fishbowl its name. The discussants should be rather heterogenous to allow for an interesting and potentially controversial discussion.",
        "summary": "In a Fishbowl, 4-5 chairs are arranged in a circle, surrounded by more chairs for the audience. A variation may have only two chairs for a one-on-one conversation. Participants, including a moderator, occupy the chairs, with a diverse group of discussants for a lively discussion.",
        "question": "How is a Fishbowl discussion physically arranged?",
        "ground_truths": "A Fishbowl discussion is physically arranged by placing four to five chairs in a circle, with a larger number of chairs forming another circle around them. In a variation, only two chairs may be placed for a face-to-face conversation. Participants for the discussions, including one moderator, occupy the chairs, with the audience filling the surrounding chairs. The discussants should be rather heterogenous to allow for an interesting and potentially controversial discussion."
    },
    {
        "contexts": "There are two different forms of the Fishbowl: The open Fishbowl, where one chair is left empty and any audience member may, at any time of the discussion, enter the discussion circle, take the chair and thus force one of the discussants to leave his/her own chair and go back into the audience. Instead of leaving one chair unoccupied, the audience members may also tap one participant on the shoulder to indicate them to leave. The closed fishbowl, where an audience member may only enter the stage when one of the discussants leaves after a pre-determined amount of time.",
        "summary": "There are two forms of Fishbowl: the open Fishbowl, where an audience member can join the discussion at any time, forcing a discussant to leave, and the closed Fishbowl, where an audience member can only join when a discussant leaves after a set time.",
        "question": "What are the two forms of Fishbowl discussions?",
        "ground_truths": "The two forms of Fishbowl discussions are the open Fishbowl, where an audience member can join the discussion at any time, forcing a discussant to leave, and the closed Fishbowl, where an audience member can only join when a discussant leaves after a set time."
    },
    {
        "contexts": "A challenge with the Fishbowl format is that shy people are often not joining the discussion as much as people who feel like they have a lot to say. In a classroom setting, this may be solved both by splitting the students into groups and thus limiting the size of the audience, as well as through time limits for each discussant so that all students have to participate at some point.",
        "summary": "The Fishbowl format can be challenging for shy people who may not join the discussion. In a classroom, this can be addressed by splitting students into groups, limiting audience size, and setting time limits for each discussant to ensure participation.",
        "question": "What is a challenge with the Fishbowl format and how can it be addressed in a classroom setting?",
        "ground_truths": "A challenge with the Fishbowl format is that shy people often do not join the discussion as much as those who have a lot to say. In a classroom setting, this can be addressed by splitting the students into groups to limit the size of the audience, and setting time limits for each discussant to ensure that all students participate at some point."
    },
    {
        "contexts": "The flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. Flashlight rounds can be initiated by the team leader or a team member. Everyone is asked to share their opinion in a short 2-3 sentence statement. During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended.",
        "summary": "The flashlight method is a tool used in group discussions to gauge participants' views on a specific topic or their feelings at a given moment. Initiated by a team leader or member, everyone shares a brief statement of their opinion. During this round, only clarification questions are permitted, with further issues discussed after the round.",
        "question": "What is the purpose and process of the flashlight method in group discussions?",
        "ground_truths": "The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards."
    },
    {
        "contexts": "The flashlight can be used as a starting round or energizer in between. The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.",
        "summary": "The flashlight method can be used to start a discussion or as an energizer. It's important for the team leader to consider timing and setting. The method is fast and efficient, allowing everyone to express their views without interruption, benefiting quieter participants. However, it can make shy participants uncomfortable. Understanding group dynamics is crucial for successful flashlight rounds.",
        "question": "What are the considerations and potential benefits and drawbacks of the flashlight method?",
        "ground_truths": "The flashlight method can be used as a starting point or energizer in discussions. The team leader needs to consider the timing, usefulness, and setting for the method. It is quick and efficient, allowing everyone to voice their opinion without interruption, which can benefit quieter participants. However, it can also make shy participants uncomfortable. Understanding the group dynamics is key to successful flashlight rounds."
    },
    {
        "contexts": "The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes. Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.",
        "summary": "Keeping statements short can distract participants as they formulate their responses. To avoid this, present the question and allow thinking time. Flashlight rounds can work with large groups, but they can become lengthy and inefficient. Breaking into smaller groups and synthesizing the results is a potential solution.",
        "question": "What are some strategies to improve the efficiency of the flashlight method in group discussions?",
        "ground_truths": "To improve the efficiency of the flashlight method, it can be helpful to present the question and allow participants some time to think before they respond. This can prevent them from being distracted while formulating their responses. For large groups, the rounds can become lengthy and inefficient, so breaking into smaller groups and synthesizing the results afterwards can be a good strategy."
    },
    {
        "contexts": "A Flipped Classroom is a teaching concept that flips the classical structure of in-class vs out-of-class. Teaching material is provided for the students (e.g. digitally) to engage with at home, while the in-class sessions presuppose knowledge of the material and are then used for interactive discussions and exercises.",
        "summary": "Flipped Classroom is a teaching method where the traditional class structure is reversed. Students engage with teaching materials at home, and use class time for interactive discussions and exercises.",
        "question": "What is the concept of a Flipped Classroom?",
        "ground_truths": "Flipped Classroom is a teaching method where students engage with teaching materials at home, and use class time for interactive discussions and exercises."
    },
    {
        "contexts": "There is no unified conceptual approach to the *Flipped Classroom* as the concept is rather new. However, there are general elements that all approaches share. Based on these, the flipped classroom elements can be categorized two-fold: structurally and didactically. On a structural level, the flipped classroom can be split into in-class and out-of-class activities. The ''out-of-class'' activities provide the students with a range of learning material, such as pre-recorded lectures, online tests, reading material, homework exercises, additional videos, podcasts etc. The students receive this material, preferably digitally via a learning platform, and can generally engage with the material whenever they like. However, the instructor may expect the students to know the content of the material when the in-class session starts, and may check this knowledge at the beginning of the session. In presence (= ''in-class''), the students are not faced with more frontal learning, but instead, these sessions are structured around active learning exercises. These may include group discussions, group exercises, Q&A sessions or interactive tests (for example via Mentimeter), challenges, panel discussions etc. The students should not receive (much) new content, but rather consolidate their knowledge in these sessions and actively engage with the topics in their peer groups.",
        "summary": "The Flipped Classroom concept, though new, has general elements shared by all approaches. It can be categorized structurally and didactically. Out-of-class activities involve students engaging with various learning materials at their own pace. In-class activities focus on active learning exercises, where students consolidate their knowledge and engage with the topics in peer groups.",
        "question": "How is the Flipped Classroom concept structured?",
        "ground_truths": "The Flipped Classroom concept is structured into out-of-class and in-class activities. Out-of-class activities involve students engaging with various learning materials at their own pace. In-class activities focus on active learning exercises, where students consolidate their knowledge and engage with the topics in peer groups."
    },
    {
        "contexts": "On a didactic level, the teacher (professor) assumes a different role in Flipped Classrooms compared to traditional lectures: (s)he is no longer only an information provider, but rather becomes a learning guide to support active learning. To this end, the Flipped Classroom intends to put an emphasis on student-centered learning theories such as active learning, peer-assisted learning and collaborative learning (Ak\u00e7ay\u0131r & Ak\u00e7ay\u0131r, 2018, p. 335). This changes the atmosphere of the sessions, and allows for the students to engage with the content of the lecture from more diverse perspectives, which may support different learning types.",
        "summary": "In Flipped Classrooms, the teacher's role changes from being an information provider to a learning guide, supporting active learning. This approach emphasizes student-centered learning theories, changing the session's atmosphere and allowing students to engage with the lecture content from diverse perspectives.",
        "question": "What is the role of a teacher in a Flipped Classroom?",
        "ground_truths": "In a Flipped Classroom, the teacher's role changes from being an information provider to a learning guide, supporting active learning."
    },
    {
        "contexts": "Ak\u00e7ay\u0131r & Ak\u00e7ay\u0131r (2018) provide a range of hypothesized advantages and disadvantages which they base on a systematic literature review. These include improved learning outcomes, higher student satisfaction, enjoyment and engagement, better student-student and student-instructor interaction and more efficient use of class time. On the other hand, challenges include higher workloads for both students and instructors, a subsequent lack of preparedness for the in-class sessions and technical issues. Since Flipped Classrooms are a rather young concept, there is not much evidence on its effectiveness. Still, the existing literature indicates positive outcomes compared to traditional lecturing styles. It may be noted that the benefits of the concept may vary depending on the topic of the class, as well as other factors such as class size. Just as any other learning format are flipped classroom settings contexts dependent, and need to be adapted to the respective settings.",
        "summary": "Flipped Classrooms have potential advantages like improved learning outcomes and higher student satisfaction, but also challenges like higher workloads and technical issues. The effectiveness of this approach is still being studied, but initial findings show positive outcomes. The benefits may vary depending on factors like class size and topic.",
        "question": "What are the potential advantages and challenges of Flipped Classrooms?",
        "ground_truths": "Potential advantages of Flipped Classrooms include improved learning outcomes and higher student satisfaction, while challenges include higher workloads and technical issues. The effectiveness of this approach is still being studied."
    },
    {
        "contexts": "The first use of Focus Groups can be dated back to market research the 1920s and later to the 1950s when scientists of the Columbia University studied the effect that the governments\u2019 war propaganda on the radio had on citizens (1, 7). In the latter, the scientists had initially started out with a quantified approach where the participants pressed red and green buttons depending on how they felt about the radio reports they heard, until one researcher of the team, Robert Merton, proposed to instead follow a qualitative technique that he had developed for this project. More precisely, the participants would be interviewed as a group to discover their \u201csubjective reactions\u201d (Bloor et al. 2001, p. 2) to the radio clips. Together with a team of scientists, Merton devised \u201ca fairly standardized set of procedures for these interviews\u201d (Bloor et al. 2001, p. 2) and published them together with Patricia Kendall in the American Journal of Sociology under the title \u201cThe focused interview\u201d in 1946 (1).",
        "summary": "Focus Groups originated in the 1920s for market research and were later used in the 1950s by Columbia University scientists to study the impact of war propaganda on citizens. Initially, a quantitative approach was used, but researcher Robert Merton proposed a qualitative technique where participants were interviewed as a group to understand their subjective reactions. Merton and his team developed a standardized set of procedures for these interviews, which were published in 1946.",
        "question": "Who proposed the qualitative technique for focus groups and what was the purpose?",
        "ground_truths": "Robert Merton proposed the qualitative technique for focus groups to understand the subjective reactions of participants to war propaganda."
    },
    {
        "contexts": "Seeing the successful application of Focus Groups by private marketing companies, organizations of the public sector started to adopt the method to e.g. examine the impact of state-run campaigns. However, the use of Focus Groups differs between the public and the private sector. This divergence is mostly a consequence of costs being kept low in the private sector and relates to a less rigorous analysis of the data then in the public sector as well as academia (1).",
        "summary": "After observing the success of Focus Groups in private marketing companies, public sector organizations began using the method, for example, to assess the impact of state-run campaigns. However, the use of Focus Groups varies between the public and private sectors, mainly due to cost considerations and the level of data analysis required.",
        "question": "How does the use of Focus Groups differ between the public and private sectors?",
        "ground_truths": "The use of Focus Groups differs between the public and private sectors mainly due to cost considerations and the level of data analysis required."
    },
    {
        "contexts": "Focus Groups are a specific type of group discussion held in a rather informal setting (3, 5). The focus in Focus Groups refers to a specific topic or a set of topics being discussed, or an activity being undertaken by the participants during the focus group session (3, 4). Focus Groups clearly differ from group interviews, as the moderator does not just aim to gather answers from the participants but to achieve group interaction (5, 7). It is not just of interest what participants say, but also how they say it, and how they react to what others say. The objective lies also in understanding \u201cthe meanings and norms which underlie those group answers\u201d (Bloor et al., p. 43) and to \u201cdraw out the cognitive structures which previously have been unarticulated\u201d (Kitzinger, p. 106).",
        "summary": "Focus Groups are a type of group discussion in an informal setting where a specific topic or set of topics is discussed or an activity is undertaken. Unlike group interviews, the goal is not just to collect answers but to facilitate group interaction. The interest lies not only in what participants say but also how they say it and their reactions to others. The aim is to understand the meanings and norms underlying the group's responses and to reveal previously unarticulated cognitive structures.",
        "question": "What is the main difference between Focus Groups and group interviews?",
        "ground_truths": "The main difference between Focus Groups and group interviews is that the goal of Focus Groups is not just to collect answers but to facilitate group interaction and understand the meanings and norms underlying the group's responses."
    },
    {
        "contexts": "The data gathered during Focus Groups \u2013 i.e. audiotapes, notes on non-verbal data, and items recalled \u2013 can be analyzed with a Content Analysis (4, 5). However, several aspects unique to Focus Group data should be additionally considered, e.g. regarding the relative contribution of single participants to the discussion, social factors affecting the degree of participation, the group and temporal contexts of statements and opinions as well as their potential alteration throughout the discussion as well as measures of dominance (e.g. the number of interventions and words spoken by different participants and their impact on the course of the discussion) (5).",
        "summary": "Focus Groups gather data such as audiotapes, notes, and recalled items, which can be analyzed with Content Analysis. However, unique aspects of Focus Group data, like the contribution of participants, social factors, contexts of statements, and measures of dominance, should also be considered.",
        "question": "What are some unique aspects to consider when analyzing Focus Group data?",
        "ground_truths": "Unique aspects to consider when analyzing Focus Group data include the relative contribution of single participants, social factors affecting participation, the group and temporal contexts of statements and opinions, and measures of dominance such as the number of interventions and words spoken by different participants."
    },
    {
        "contexts": "The Focus Group size, i.e. the number of participants per Focus Group, should allow for enough information and viewpoints on the given topic while at the same time ensuring a comfortable atmosphere and enough opportunities for everyone to express themselves (4, 6). Around four to twelve participants can serve as a guide for the Focus Group size (4, 6). Over-recruiting the Focus Groups to factor in participants who do not show up is recommended (4). As opposed to the Focus Group size, the sample size is the number of Focus Groups conducted to answer the research question (2). The number of Focus Groups should ideally be based on the point of data or theoretical saturation (4, 2). However, financial and time constraints often become the cornerstone of decision-making in this regard (2).",
        "summary": "The size of a Focus Group should allow for diverse viewpoints and a comfortable atmosphere, typically consisting of four to twelve participants. Over-recruiting is recommended to account for no-shows. The sample size refers to the number of Focus Groups conducted, ideally based on data or theoretical saturation, but often influenced by financial and time constraints.",
        "question": "What factors influence the size and number of Focus Groups in a study?",
        "ground_truths": "The size and number of Focus Groups in a study are influenced by the need for diverse viewpoints and a comfortable atmosphere, the potential for participant no-shows, the point of data or theoretical saturation, and financial and time constraints."
    },
    {
        "contexts": "As group interaction in Focus Groups is crucial, the role of the researcher is rather peripheral and consists of the moderation and facilitation of the group discussion. What counts are the group dynamics and the interpersonal relationships between the participants (5). The facilitator should thus \u201cencourage people to engage with one another\u201d (Kitzinger, p. 106) and to animate the discussion (5). This is usually achieved by asking open-ended questions (5). Further, group exercises can be included into the Focus Group session. Besides encouraging interaction among the participants, those exercises are meant to draw the participants' attention away from the facilitator and towards the other participants and topic of interest, and to make them feel comfortable in the group. Further, such exercises can evoke physical reactions (such as twitching when a card is put into the \u2018wrong\u2019 category) which may hint towards approval or disagreement within the group and can then be explored further (3).",
        "summary": "In Focus Groups, the researcher's role is to moderate and facilitate discussion, with emphasis on group dynamics and interpersonal relationships. The facilitator encourages engagement through open-ended questions and group exercises, which also divert attention from the facilitator to the participants and topic. These exercises can also elicit physical reactions indicating approval or disagreement.",
        "question": "What is the role of the facilitator in a Focus Group?",
        "ground_truths": "The facilitator in a Focus Group moderates and facilitates discussion, encourages participant engagement through open-ended questions and group exercises, and uses these exercises to divert attention to the participants and topic and to elicit physical reactions indicating approval or disagreement."
    },
    {
        "contexts": "Focus Groups allow to explore people\u2019s views in a social contexts: \u201cWe learn about the 'meaning' of AIDS, (or sex, or health or food or cigarettes) through talking with and observing other people, through conversations at home or at work; and we act (or fail to act) on that knowledge in a social contexts. When researchers want to explore people's understandings, or to influence them, it makes sense to employ methods which actively encourage the examination of these social processes in action.\u201d (Kitzinger, p. 117). As S\u00e4yn\u00e4joki et al. (2014, p.6625) highlight: \"Focus groups are particularly useful in studies where the researcher seeks to uncover attitudes, perceptions and beliefs.\"",
        "summary": "Focus Groups allow exploration of people's views in a social contexts, learning about various topics through conversation and observation. They are useful in studies aiming to uncover attitudes, perceptions, and beliefs.",
        "question": "Why are Focus Groups useful in research?",
        "ground_truths": "Focus Groups are useful in research because they allow the exploration of people's views in a social contexts and are particularly effective in studies aiming to uncover attitudes, perceptions, and beliefs."
    },
    {
        "contexts": "A challenge one needs to be aware of when conducting and analyzing Focus Groups is the censorship of certain \u2013 e.g. minority or marginalized \u2013 viewpoints, which can arise from the group composition (3). As Parker and Tritter (2006, p. 31) note: \u201cAt the collective level, what often emerges from a focus group discussion is a number of positions or views that capture the majority of the participants\u2019 standpoints. Focus group discussions rarely generate consensus but they do tend to create a number of views which different proportions of the group support.\"",
        "summary": "A challenge in conducting and analyzing Focus Groups is the potential censorship of certain viewpoints, such as those of minority or marginalized groups, due to group composition. Focus group discussions often reflect the majority of participants' standpoints rather than generating consensus.",
        "question": "What is a challenge in conducting and analyzing Focus Groups?",
        "ground_truths": "A challenge in conducting and analyzing Focus Groups is the potential censorship of certain viewpoints, such as those of minority or marginalized groups, due to group composition, and the tendency of discussions to reflect the majority of participants' standpoints rather than generating consensus."
    },
    {
        "contexts": "S\u00e4yn\u00e4joki et al (2014) used Focus Groups to investigate how Finnish urban planners perceive the potential positive impact of their field on environmental sustainability. Focus Groups were chosen since the focus of the study was not solely on what people think but on how they articulate, rationalise, challenge each other's views (p.6625). The researchers invited 32 urban planning specialists to a workshop, including individuals from fourteen Finnish cities, the Finnish environment ministry, two architectural firms, four consulting companies, one of Finland\u2019s largest energy companies, a market leading construction company, the Green Building Council Finland and the Finnish Association of Building Owners and Construction Clients (RAKLI) (p.6626).",
        "summary": "A study by S\u00e4yn\u00e4joki et al (2014) used Focus Groups to explore Finnish urban planners' views on the positive impact of their field on environmental sustainability. The study involved 32 urban planning specialists from various sectors.",
        "question": "Who conducted the study using Focus Groups to understand the perception of Finnish urban planners on environmental sustainability?",
        "ground_truths": "S\u00e4yn\u00e4joki et al (2014) conducted the study using Focus Groups to understand the perception of Finnish urban planners on environmental sustainability."
    },
    {
        "contexts": "The researchers asked this group three questions as a prompt to the subsequent focus group sessions: Why is environmental sustainability assessed in urban planning? How does environmental assessment steer decision-making in urban planning? What is the role of urban planning in environmental sustainability? Then, the 32 participants were divided into three groups of 11, 11 and 10 participants, respectively, with one moderating researcher each.",
        "summary": "The researchers posed three questions related to environmental sustainability and urban planning to the focus group participants. The 32 participants were then divided into three groups, each moderated by a researcher.",
        "question": "What were the three questions posed to the focus group participants in the study?",
        "ground_truths": "The three questions posed to the focus group participants in the study were: Why is environmental sustainability assessed in urban planning? How does environmental assessment steer decision-making in urban planning? What is the role of urban planning in environmental sustainability?"
    },
    {
        "contexts": "The analysis of the data, i.e. the transcribed recordings and notes, was done thematically in accordance with Guest et al. (2012). This means that not every statement was assessed, but that the researchers focused on on identifying and describing themes in the data (p.6626). For this, all parts of the transcripts were marked with codes representing the five guiding questions. All data for each code was grouped, and each group of data was then categorized more specifically until conclusions could be drawn (p.6628). The researchers' notes as well as the recording were used to contextualize the data.",
        "summary": "The data from the focus groups was analyzed thematically, with the researchers identifying and describing themes rather than assessing every statement. The transcripts were coded based on the guiding questions, and the data was grouped and categorized until conclusions could be drawn.",
        "question": "How was the data from the focus groups analyzed in the study?",
        "ground_truths": "The data from the focus groups was analyzed thematically. The researchers identified and described themes rather than assessing every statement. The transcripts were coded based on the guiding questions, and the data was grouped and categorized until conclusions could be drawn."
    },
    {
        "contexts": "The researchers conclude that land use planners are not by themselves able to deploy the full potential power of urban planning to impact environmental sustainability (p.6640), and depict a range of suggestions to better support land use planners in doing so. Overall, the focus group approach allowed for the researchers not only to gather diverse perspectives on the topic of interest, but also to perceive the underlying motivations and frustrations of the stakeholders and contextualize their statements accordingly.",
        "summary": "The researchers concluded that land use planners alone cannot fully utilize urban planning to impact environmental sustainability. They suggested ways to better support planners and found the focus group approach helpful in understanding diverse perspectives and underlying motivations.",
        "question": "What was the conclusion of the researchers in the study?",
        "ground_truths": "The researchers concluded that land use planners alone cannot fully utilize urban planning to impact environmental sustainability. They suggested ways to better support planners."
    },
    {
        "contexts": "We all stand on the shoulders of giants. Any given research question is based on previous research. No research is an island. Whatever we do as researchers, we have to start with what was done before. Reading is the most essential skill of any person new to research. Balance is key to this end. If you read everything there is on a specific topic, then you are in for a long ride. Otherwise, if you miss something important that has been published before, you basically reinvent the wheel, which is a waste of your times and the time of your readers. Hence, make sure that you get the main approaches that have been attempted before, and the main knowledge that has been gained. You will eventually have to make a cut at some point, otherwise you would basically read everything there is, since almost everything is connected. Hence try to focus on what is specific for your topic and for the area you focus on, thereby gaining some insights about the respective contexts. contexts knowledge matters to this end, because only if you know the contexts of the specific topic, it allows you to aim with the right ratio of focus and distance. Why both? If you look too close, your knowledge is too singular, too specific, or just beyond the point, or any point at all. If it is too broad, it may be generic, trivial, and thus again beyond any point whatsoever once more. Ideally, you work in a specific system. This may be a group of people, and institution, or any other constructed entity. This allows you to add specificity to your research topic. You aim at creating knowledge about this system, people or entity. Hence your framing can be about a specific topic, an entity or institution, but also about a theory. What matters to this end is to have the right angle. Did you ever try to look at a painting from the side? Basically, this does not work, because the framing will block the view, and you do not see the picture. In framing your research, this is basically the same. You need to have the same angle, not from the side, but up front. Within research, we often look at a topic or problem through a certain theory. While testing a theory is restricted to deductive research, regarding a research question our framing and viewpoint is more open minded. It is thus not restricted to the yes/no categories of a hypothesis, but instead allows us to ask a broader question that allows us to create contextual and novel knowledge.",
        "summary": "Research is built on previous work, and reading is crucial to understand the contexts and main approaches of a topic. It's important to balance the breadth and depth of reading to avoid reinventing the wheel or missing key information. The framing of a research question should be specific and from the right angle, often through a certain theory. This framing is not restricted to yes/no hypotheses, but allows for broader, contextual questions.",
        "question": "Why is it important to balance the breadth and depth of reading when framing a research question?",
        "ground_truths": "Balancing the breadth and depth of reading when framing a research question is important to avoid missing key information or reinventing the wheel. It helps to understand the contexts and main approaches of a topic, and to frame the research question from the right angle."
    },
    {
        "contexts": "Our research question may start with a \"How\", thereby examining explanations of dynamics or patterns. \"Where\" questions will typically try to spatially locate phenomena, while \"when\" questions examine dimensions of temporality. \"Why\" questions try often to go deeper into reasonings and examine patterns that may in one extreme be pretty foundational, yet can in other extreme cases be borderline trivial. \"What\" question are one last example of research question that can either be rather procedural or again end up being vague. Some research question may avoid such question formats altogether, which is for instance true for much of the realms of descriptive research. Yet another example is critical research, which may not be occupied with questions at all, but instead offer a critical reflection or specific viewpoint. What should however be clear is that research questions can be at least partly answered, or we may conclude that based on the current design and data the research question cannot be answered. This is perfectly ok, and a part of scientific research. However, research is often biased towards positive or even potentially exciting research, while research rarely report that they could not find anything out at all, and the their initial research question remains unanswered.",
        "summary": "Research questions can start with 'how', 'where', 'when', 'why', or 'what', each examining different aspects of a topic. Some research avoids these formats, like descriptive or critical research. Research questions should be answerable, at least partly, but it's also acceptable if they can't be answered based on the current design and data. There's often a bias towards positive or exciting research, while unanswered questions are rarely reported.",
        "question": "What are the different types of research questions and what do they examine?",
        "ground_truths": "Research questions can start with 'how', which examines explanations of dynamics or patterns; 'where', which tries to spatially locate phenomena; 'when', which examines dimensions of temporality; 'why', which often goes deeper into reasonings and examines patterns; and 'what', which can be procedural or vague. Some research avoids these formats, like descriptive or critical research."
    },
    {
        "contexts": "Testability and rejection is actually a key criteria that should differentiate knowledge gained through science from mere opinions, because we can have all sorts of opinions, but scientific empirical knowledge can be tested and can also be falsified, refined or changed if it turns out to be wrong. Hence wrong facts can be debunked, while it is really hard to debunk a conspiracy theory that is based on mere opinions. In addition to the process of testing or examining within the framework of research it is possible and indeed perfectly normal that our knowledge evolves; this is actually what science is all about. Consequently, the question we try to examine should represent a scientific process that can be handled by us. We cannot come up with a research question that is perfect in terms of everything but the fact that it is impossible to conduct it.",
        "summary": "Scientific knowledge, unlike opinions, can be tested, falsified, and refined, allowing for the evolution of our understanding. Therefore, a research question should represent a scientific process that is feasible to conduct.",
        "question": "Why is testability a key criteria in differentiating scientific knowledge from opinions?",
        "ground_truths": "Testability is a key criteria in differentiating scientific knowledge from opinions because scientific knowledge can be tested, falsified, and refined, allowing for the evolution of our understanding."
    },
    {
        "contexts": "Our research question does not need to be  a breakthrough that changes the whole world, but more often than not it is a stepping stone in the bigger picture that is our united research working to unravel. Our framing is a piece of what we call reality, and building on a respective theory or theoretical foundation may help us to focus our research and create knowledge that is specific and not generic. Equally can a contextual focus or specification help within a research question to understand what we work about, or where our sample entity is located or rooted in. All this is part of our framing. Naturally, the framing also needs to be clear. If your wording or different parts of the frame are too many, then other researchers will not be able to follow. Hence we need to make sure to use fewer words instead of too many, and each word should be chosen carefully.",
        "summary": "Research questions often serve as stepping stones in the broader research landscape, contributing to our understanding of reality. Clear framing, grounded in theory and contexts, is crucial for creating specific knowledge and ensuring other researchers can follow.",
        "question": "How does the framing of a research question contribute to the creation of specific knowledge?",
        "ground_truths": "The framing of a research question, grounded in theory and contexts, helps to focus the research and create knowledge that is specific and not generic."
    },
    {
        "contexts": "Taken together, we may indeed ask questions in our research questions, but these should be specifically frames, state the respective contexts, root deeply in previous research and knowledge, be neither generic nor hyper-specific, need to be feasible, and allow for a scientific conduct that can be documented, reproduced, or both. Specific theories allow for refined viewpoints, and empirical research may benefit from stating where it is conducted. The most difficult challenge in framing a research question is then to decide what to include and what to exclude. Researchers make choices, and need to focus. Creating a research question demands to keep the most important information, and omit all that is not central to the initial question. It takes practice to get good at framing research questions. Be patient, and keep going.",
        "summary": "Research questions should be framed specifically, rooted in previous research, feasible, and allow for scientific conduct that can be documented or reproduced. The challenge lies in deciding what to include and exclude, requiring focus and practice.",
        "question": "What are the key considerations in framing a research question?",
        "ground_truths": "Key considerations in framing a research question include ensuring it is specifically framed, rooted in previous research, feasible, and allows for scientific conduct that can be documented or reproduced. It also involves deciding what information to include and exclude."
    },
    {
        "contexts": "Functions (or \u201cMethods\u201d in some other programming languages) are reusable blocks of code that do a specific thing and can be reused multiple times in a program. It is good for task breakdown and code readability. We have already used several functions in the tutorials above, such as the \u201cprint()\u201d function. These functions are default functions readily available in Python. However, it might be handy sometimes to develop your own functions for specific occasions. First, we will look at pre-defined functions and then develop our own user-defined functions.",
        "summary": "Functions, also known as methods in some languages, are reusable code blocks that perform a specific task. They enhance code readability and task breakdown. Python has default functions like 'print()', but you can also create your own functions.",
        "question": "What are functions in Python and why are they useful?",
        "ground_truths": "Functions in Python are reusable blocks of code that perform a specific task. They are useful for task breakdown and code readability. Python has default functions, but users can also create their own functions for specific tasks."
    },
    {
        "contexts": "Understanding all the pre-defined functions in python is a journey of a lifetime. However, through our exercises, you will know and get used to the most common functions that exist. We can start with the 3 examples that most of you have seen before: print(), sum(), len(). These are predefined functions to print, add elements in a list, and show the length of a list respectively.",
        "summary": "Python has numerous pre-defined functions. Through practice, you can become familiar with the most common ones like print(), sum(), and len(), which print, add list elements, and show list length respectively.",
        "question": "What are some examples of pre-defined functions in Python?",
        "ground_truths": "Some examples of pre-defined functions in Python are print(), sum(), and len(). These functions are used to print, add elements in a list, and show the length of a list respectively."
    },
    {
        "contexts": "An example function looks as follows: def greet(name): print('Hi, how are you,', name) return. The function begins with 'def', and then the function name. Inside parentheses is what we call argument (or input parameters). To finish the first line of code, a colon (:) is needed. The body of a function is the \u201cinside\u201d of a function. Lastly, using the \u201creturn\u201d statement will end your function.",
        "summary": "A function in Python starts with 'def' followed by the function name and parentheses containing arguments. The function body is indented and ends with a 'return' statement.",
        "question": "What is the basic structure of a function in Python?",
        "ground_truths": "A function in Python starts with the keyword 'def', followed by the function name and parentheses that contain the arguments. The body of the function is indented and the function ends with a 'return' statement."
    },
    {
        "contexts": "It is possible to set a default value for arguments in a function. For example, in the function is_mood_good(good_mood=True), the argument good_mood is set to True by default.",
        "summary": "In Python, you can set default values for function arguments. For instance, in the function is_mood_good(good_mood=True), good_mood defaults to True.",
        "question": "Can you set default values for arguments in a Python function?",
        "ground_truths": "Yes, you can set default values for arguments in a Python function. An example is the function is_mood_good(good_mood=True), where good_mood is set to True by default."
    },
    {
        "contexts": "We can create a variable in a function. That variable can only \u201clive\u201d inside that function. On the opposite, if we create a variable outside a function, it can \u201clive\u201d both inside and outside the function. The advantage of local variables is that they cannot be altered by any other commands in your script and make it easier to oversee.",
        "summary": "Variables created inside a function, known as local variables, only exist within that function. Variables created outside a function exist both inside and outside the function. Local variables are advantageous as they can't be altered by other script commands.",
        "question": "What is the difference between a local variable and a variable created outside a function in Python?",
        "ground_truths": "A local variable in Python is created inside a function and only exists within that function. A variable created outside a function exists both inside and outside the function. The advantage of local variables is that they cannot be altered by other commands in the script."
    },
    {
        "contexts": "This Wiki article aims at giving a first and basic introduction to the topic of gender. Our intention is to offer some helpful tools and advice on how gender is important to consider in everyday life at university. We hope that students and teachers find helpful information and suggestions of how to act more aware regarding gender. As this is meant to be a gender first aid kit we start this article by offering some practical advice on how to use gender neutral language and different pronouns as well as reflect on one\u2019s own talking behaviour. After that we will describe the history of the concept of gender and thereby highlighting to you the most important information on the topic from our point of view. In the end, we will point out the relation of institutions such as universities with gender and explain why it is one important factor to consider while studying or working in an institution.",
        "summary": "This article provides an introduction to gender, offering tools and advice for considering gender in university life. It includes practical advice on using gender-neutral language and pronouns, and reflects on the history of the concept of gender. It also discusses the relationship between institutions like universities and gender, explaining its importance in studying or working in an institution.",
        "question": "What is the main purpose of this article?",
        "ground_truths": "The main purpose of this article is to provide an introduction to the concept of gender, offer practical advice on using gender-neutral language and pronouns, and explain the importance of considering gender in university life and institutions."
    },
    {
        "contexts": "This section will provide a brief historical outline of how the concept of '''sex''', and later '''gender''', came into being from the 18th century onward until today. This is by no means a complete historical account but rather tries to give an introduction to the topic. The further links we provide might be a good starting point to deepen your knowledge if interested.  We will mainly focus on the Enlightenment period in which natural sciences such as anthropology and biology emerged to explain the development of the sex binary and the differentiation of humans in races. After that, there will be a short summary of the four waves of feminism focusing on each\u2019s main claims and developments regarding sex and gender. Furthermore, this historical section refers to the German/European/Western history of sex and gender and cannot be generalized to other societies, cultures and regions of the world.",
        "summary": "This section outlines the historical development of the concepts of sex and gender from the 18th century to today, focusing on the Enlightenment period when natural sciences emerged to explain the sex binary and human racial differentiation. It also summarizes the four waves of feminism and their main claims and developments regarding sex and gender. This history is specific to German/European/Western societies and cannot be generalized to other cultures and regions.",
        "question": "What is the main focus of the historical outline provided in this section?",
        "ground_truths": "The main focus of the historical outline in this section is the development of the concepts of sex and gender from the 18th century to today, with a particular focus on the Enlightenment period and the four waves of feminism. It is specific to German/European/Western societies."
    },
    {
        "contexts": "Society in the 18th century was widely built upon a hierarchical system rooted in gender and race, which constructed social positions based on gender and racial differences. This separation of society into women and men and white and non-white people emerged during the Enlightenment which was an intellectual movement in the 18th century that strived for objective answers based on reason. Thus, the previous focus of society on religion and culture was tried to be replaced by reason. Based on the idea that all people are equal by nature, social, civil and universal human rights slowly evolved. An early example of these developments were the demands of European women for more rights, thereby questioning long-established social arrangements with respect to women\u2019s rights and duties. This is now considered the first wave of feminism.",
        "summary": "In the 18th century, society was structured on a hierarchy based on gender and race. This division emerged during the Enlightenment, an intellectual movement that sought rational answers and aimed to replace religious and cultural focus with reason. The idea of natural equality led to the evolution of human rights, with early developments including European women demanding more rights, marking the first wave of feminism.",
        "question": "What societal changes emerged during the Enlightenment period in the 18th century?",
        "ground_truths": "During the Enlightenment period in the 18th century, society began to be structured on a hierarchy based on gender and race. The Enlightenment sought to replace the focus on religion and culture with reason. This period also saw the evolution of human rights, with early developments including European women demanding more rights, which marked the first wave of feminism."
    },
    {
        "contexts": "The beard as a feeble indicator for superiority was already mentioned, yet over the next decades and centuries, further explanations to justify social inequalities were investigated. Criteria of inclusion and exclusion of certain people also applied to academic communities, creating a closed ingroup. Knowledge production thus consolidated itself as an exclusive white-male club, yet deprived itself of other perspectives, whose ideas and experiences were not considered valid. The white-male science of the time became restrictive in their theoretical and empirical focus. The division of people into the categories of male and female, white and non-white and the simultaneous establishment of a superiority of white males, argued through characteristics possessed only by them, led to the exclusion of non-white people  from science (Schiebinger 1993).",
        "summary": "Over time, social inequalities were justified through various explanations, leading to the exclusion of certain people from academic communities. This resulted in knowledge production becoming an exclusive white-male club, ignoring other perspectives. The science of the time, dominated by white males, became restrictive and led to the exclusion of non-white people.",
        "question": "How did the exclusion of certain groups affect knowledge production in the past?",
        "ground_truths": "The exclusion of certain groups, particularly non-white people, led to knowledge production becoming an exclusive white-male club, ignoring other perspectives and experiences. This resulted in a restrictive focus in science and academia."
    },
    {
        "contexts": "Since Colonisation was a multi-faceted process, main actors were not just statesmen who executed their orders from their European home countries. It was also missionaries who enforced a process of Christianisation in the colonies. Christian missionaries fulfilled the task of education on colonised land. The school often functioned as the church also was a place where the culture should be transformed to align with European values. Especially the family system was targeted with a focus on polygamy. \u201cFor the missionaries, having multiple wives was not only primitive but against God's law: polygamy was adultery, pure and simple\u201d (Oyewumi 1997: 137).",
        "summary": "Colonisation was a complex process involving not only statesmen but also missionaries who enforced Christianisation in the colonies. These missionaries were responsible for education, transforming local cultures to align with European values, particularly targeting the family system and polygamy.",
        "question": "What role did missionaries play in the process of colonisation?",
        "ground_truths": "Missionaries played a significant role in the process of colonisation. They enforced Christianisation in the colonies, were responsible for education, and worked to transform local cultures to align with European values. They particularly targeted the family system and polygamy."
    },
    {
        "contexts": "In the 18th century, the concept of sex was created by natural scientists who applied self-selected differentiating characteristics on the basis of biological determinsm. After centuries of reflection on biases, systematic preconceptions, and oppression, parts of the scientific community came to the conclusion that neither sex nor gender are objective truths. Today, both sex and gender are considered socially constructed. What we have tried to show with this historical outline is that even supposedly scientifically produced knowledge is not objectively true but is permeated by social and cultural norms of the time. Scientific knowledge was and is used to justify policies and practices but one has to keep in mind that people have and still suffer because of these.",
        "summary": "In the 18th century, the concept of sex was created by natural scientists based on biological determinism. However, after centuries of reflection on biases and oppression, it was concluded that neither sex nor gender are objective truths but are socially constructed. This highlights that scientific knowledge is influenced by social and cultural norms and has been used to justify policies and practices that have caused suffering.",
        "question": "How has the understanding of sex and gender evolved over time?",
        "ground_truths": "The understanding of sex and gender has evolved significantly over time. In the 18th century, the concept of sex was created based on biological determinism. However, after centuries of reflection on biases and oppression, it was concluded that neither sex nor gender are objective truths but are socially constructed."
    },
    {
        "contexts": "Being baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended Frequentist statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians. The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised mixed effect models. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.",
        "summary": "John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to overcome the limitations of normal distribution regressions. They extended Frequentist statistics to include other distributions like Poisson and Binomial, enabling more accurate statistical analyses. This innovation has been widely adopted in fields like insurance, econometrics, and ecology, and has paved the way for more complex models. Today, GLMs are a standard tool for advanced statisticians.",
        "question": "Who developed the Generalized Linear Models and why?",
        "ground_truths": "John Nelder and Robert Wedderburn developed Generalized Linear Models in the 1960s to overcome the limitations of normal distribution regressions."
    },
    {
        "contexts": "Generalized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often deductive methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.",
        "summary": "Generalized Linear Models (GLMs) are statistical analyses that can handle specific sampling designs and calculate dependent variables that can be count data, binary data, or proportions. They can calculate relations between continuous variables where the dependent variable deviates from the normal distribution. GLMs can be calculated with common statistical software like R and SPSS, making them a crucial tool for advanced statisticians.",
        "question": "What types of data can Generalized Linear Models handle and calculate?",
        "ground_truths": "Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions."
    },
    {
        "contexts": "Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world. Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly outside of designed studies. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. In addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong.",
        "summary": "Generalized Linear Models (GLMs) allow for powerful calculations in datasets that do not follow the normal distribution, making them more applicable to real-world dynamics. They represent a breakthrough in analyzing skewed or imperfect datasets, whether due to the nature of the data or sampling flaws. GLMs enable more precise investigations than normal distribution analyses, and are often used in fields like insurance and ecology where simple regression approaches may be flawed or incorrect.",
        "question": "Why are Generalized Linear Models considered a breakthrough in statistical analysis?",
        "ground_truths": "Generalized Linear Models are considered a breakthrough because they allow for powerful calculations in datasets that do not follow the normal distribution, making them more applicable to real-world dynamics and enabling more precise investigations than normal distribution analyses."
    },
    {
        "contexts": "While Geographical Information Systems are typically associated with digital systems, the systematic creation of knowledge through the analysis of spatial data can be dated back long before the invention of modern computers. The first spatial analysis was focussed on the spread of Cholera in Paris (1832, \"Rapport sur la marche et les effets du chol\u00e9ra dans Paris et le d\u00e9partement de la Seine. Ann\u00e9e 1832\") and London (1854). While the map of Paris showed Cholera cases on a scale of the districts, John Snow showed in London the cases of citizens that died of Cholera as dots. This allowed for a clear representation of the spread of the disease. Since cases were clustered around a well, John Snow could infer that the water sources is the main spread vector of the disease. This is insofar remarkable, as is clearly highlights how knowledge could be derived inductively without any understanding of the deeper mechanics of Cholera, or let alone even the knowledge of bacteria.",
        "summary": "Geographical Information Systems (GIS) have been used for spatial data analysis long before modern computers. The first spatial analysis was done on the spread of Cholera in Paris and London in the 19th century. John Snow in London used dots to represent Cholera cases, showing a clear pattern of disease spread around a well, thus inferring that water was the main vector of the disease.",
        "question": "What was the first known use of spatial analysis in disease tracking?",
        "ground_truths": "The first known use of spatial analysis in disease tracking was the study of the spread of Cholera in Paris and London in the 19th century. John Snow in London used a form of Geographical Information Systems to represent Cholera cases as dots on a map, showing a clear pattern of disease spread around a well."
    },
    {
        "contexts": "At this time, topographic maps were already perfected as part of the Nation States creating inventories and understandings of their territories, and also of their colonies and their wealth. Topographic maps were increasingly able to depict different layers of information, as maps were printed in different colours that all represented different kinds of information, such as vegetation, thematic information, water, and more. With the rise of the computer age, this information was implemented into computers, and Canada created the first digital Geographical Information System in the 1960. This highlights how innovation of Geographical Information Systems was strongly driven by application, since such systems are of direct benefit in planning and governance. The main driver of innovation in early GIS development was however Howard T. Fisher from Harvard, who - with his team - developed important cornerstones that still serve as a basis for GIS, including different data formats and the general architecture of GIS systems.",
        "summary": "Topographic maps, which depicted various layers of information, were perfected by Nation States for inventory and understanding of their territories. With the advent of computers, this information was digitized, leading to the creation of the first digital Geographical Information System in Canada in the 1960s. Howard T. Fisher from Harvard was a key figure in early GIS development, contributing to the creation of different data formats and the general architecture of GIS systems.",
        "question": "Who was a key figure in the early development of Geographical Information Systems?",
        "ground_truths": "Howard T. Fisher from Harvard was a key figure in the early development of Geographical Information Systems. He and his team developed important cornerstones that still serve as a basis for GIS, including different data formats and the general architecture of GIS systems."
    },
    {
        "contexts": "First commercial systems started to thrive in the 1970s and 1980s, yet only with the rise of personal computers and the Internet, Geographical Information Systems unleashed their full potential. While simple vector formats were already widely used in the 1990s and even before, the development of a faster bandwidth and computer power proved essential to foster the exchange and usage of satellite data. While much of this data (NOAA, LANDSAT) can be traced back to the 1970s, it was the computer age that made this data widely available to the common user. Thanks to the 'Freedom of Information Act', much of the US-based data was quickly becoming available to the whole world, for instance through the University of Maryland and the NASA. The development of GPS systems that originated in military systems allowed for a solution to spatial referencing and location across the whole globe.",
        "summary": "Commercial GIS systems began to flourish in the 1970s and 1980s, but their full potential was realized with the advent of personal computers and the Internet. The development of faster bandwidth and computer power in the 1990s facilitated the exchange and use of satellite data, making it widely available to the common user. The 'Freedom of Information Act' made much of the US-based data globally accessible, and the development of GPS systems provided a solution for global spatial referencing and location.",
        "question": "What developments in the 1990s facilitated the widespread use of Geographical Information Systems?",
        "ground_truths": "The development of faster bandwidth and increased computer power in the 1990s facilitated the widespread use of Geographical Information Systems. These advancements allowed for the exchange and use of satellite data, making it widely available to the common user. Additionally, the 'Freedom of Information Act' made much of the US-based data globally accessible."
    },
    {
        "contexts": "In the 1960s, empirical social science was stuck between a rock and a hard place. The critique of positivism was at its peak, all the while the recognition of social science as an empirical science was dominated by a lack of suitable methodological approaches. Classical surveys and other methodological approaches were criticised by the anti-positivists, and the lack of methods led to a problematic view of on social science form other disciplines. This problem can be strongly associated to the dominating reign of deductive approaches during this time, which was a suitable line of thinking in psychology, physics, agricultural research, biology or other fields, yet did not only pose an epistemological shortcoming, but was in addition not suitable for parts of science and knowledge production that built on inductive reasoning. In this contexts of the history and theory of science, the proposal of Grounded Theory can be seen as nothing short of a revolution.",
        "summary": "During the 1960s, empirical social science faced criticism and lacked suitable methodological approaches. The dominance of deductive approaches, suitable for fields like psychology and physics, was not suitable for areas of science that relied on inductive reasoning. The introduction of Grounded Theory was revolutionary in this contexts.",
        "question": "Why was the introduction of Grounded Theory considered revolutionary in the 1960s?",
        "ground_truths": "The introduction of Grounded Theory was considered revolutionary because it provided a suitable methodological approach for areas of science that relied on inductive reasoning, which was lacking during the 1960s."
    },
    {
        "contexts": "Following the increasing focus on the individual, Grounded Theory can be seen as a main stepping stone for a science that is interactive with non-scientists, and even can be interactive even within itself. From the perspective of philosophy of science, this is a clear commitment to the fact that there are branches of knowledge that do not build on theories, but instead derive theories from observation. This concluded a pendulum swing from Bacon and the inductive approach to the positivists and back, enabling a more pluralistic and diverse science that could approach new frontiers. What is more important, this approach paved the road for a thinking in science that was less absolute and pretending to be objective, but instead acknowledged that scientists look at parts of the picture, which are highly important, but should not be seen as objective facts.",
        "summary": "Grounded Theory, with its focus on the individual, is a stepping stone for a more interactive science. It acknowledges that some branches of knowledge derive theories from observation, not pre-existing theories. This approach allows for a more pluralistic science and acknowledges that scientists' perspectives are not absolute or objective.",
        "question": "How does Grounded Theory contribute to a more pluralistic and diverse science?",
        "ground_truths": "Grounded Theory contributes to a more pluralistic and diverse science by focusing on the individual and acknowledging that some branches of knowledge derive theories from observation, not pre-existing theories. This approach allows for a more interactive science and acknowledges that scientists' perspectives are not absolute or objective."
    },
    {
        "contexts": "Grounded Theory is a systematic approach to derive hypotheses and theories that are open and more general, starting with research questions that may lead to the development of theories, often through qualitative analysis and coding. Researchers utilising the Grounded Theory approach thus typically formulate broad research questions that are widely unaimed at specifics, but instead try to observe and generate new insights. This is often done in a systematic sense through coding of information, thereby allowing for code items that are similar or at least allow for certain similarities to be identified and to be grouped in clusters. In the next step, such clusters of code items allow for the conceptualisation or formulation of a theory.",
        "summary": "Grounded Theory is a systematic approach to develop open and general hypotheses and theories, often through qualitative analysis and coding. Researchers using this approach formulate broad research questions and observe to generate new insights. Similar code items are grouped into clusters, which then allow for the formulation of a theory.",
        "question": "How does the Grounded Theory approach lead to the formulation of a theory?",
        "ground_truths": "The Grounded Theory approach leads to the formulation of a theory by starting with broad research questions and observing to generate new insights. This is done systematically through coding of information, where similar code items are grouped into clusters. These clusters then allow for the conceptualisation or formulation of a theory."
    },
    {
        "contexts": "The general idea of Concept Mapping was developed by Joseph Novak and colleagues at Cornell University in the 1970s (Brown). The idea was to structure and visualize knowledge about a topic in an educational contexts. Concept Mapping is typically still used in education until today. In 1989, William M.K. Trochim, also from Cornell University, expanded on this idea and introduced Group Concept Mapping as a structured methodological approach (Trochim 1989). He originally presented it as a way to create conceptual frameworks for planning and evaluation purposes. At the same time, the results of a Group Concept Mapping process provide insights into specific topics that are of strong interest to scientific research, which is why the method may be seen by some researchers as both a research method and a planning and evaluation tool (cf. Dare & Nowicki 2019, p.3). Thematically, the method can find application in any field and is often used in health and social work (see for example McCaffrey et al. 2019, Trochim et al. 1994, Trochim et al. 2003)",
        "summary": "Concept Mapping was developed by Joseph Novak and colleagues at Cornell University in the 1970s to structure and visualize knowledge in an educational contexts. In 1989, William M.K. Trochim expanded this idea and introduced Group Concept Mapping as a structured methodological approach for creating conceptual frameworks for planning and evaluation purposes. The method is often used in health and social work and can be seen as both a research method and a planning and evaluation tool.",
        "question": "Who developed the idea of Group Concept Mapping and what is its purpose?",
        "ground_truths": "William M.K. Trochim developed the idea of Group Concept Mapping in 1989. Its purpose is to create conceptual frameworks for planning and evaluation purposes, and it can also be used as a research method."
    },
    {
        "contexts": "Group Concept Mapping is a structured, linear group-based approach of gathering conceptual elements, arranging and rating them, and creating visual representations. The end result is typically a map or similar form of visual representation, which displays the ideas and how they relate to each other. This visualisation, as suggested by Trochim 1989, can serve as a baseline for a subsequent planning or evaluation process. An important feature is the focus on the group of actors who create the concept map on their own, supported by the methodological process and a facilitator. The end result is based on their own contributions, represents their own understanding of the issue at hand, and can thus more easily be used by them for the following processes.",
        "summary": "Group Concept Mapping is a structured approach of gathering, arranging, and rating conceptual elements, and creating visual representations. The end result is a map that displays the ideas and their relationships, serving as a baseline for planning or evaluation. The process is facilitated but the end result is based on the group's own contributions and understanding.",
        "question": "What is the process and purpose of Group Concept Mapping?",
        "ground_truths": "The process of Group Concept Mapping involves gathering, arranging, and rating conceptual elements, and creating visual representations. Its purpose is to serve as a baseline for planning or evaluation, with the end result based on the group's own contributions and understanding."
    },
    {
        "contexts": "Concept Maps refer to visual representations of conceptual ideas, as introduced by Novak and colleagues in the 1970s (Trochim 1989). Information (words or short sentences) is put into boxes or circles, which are connected in a network-like structure through arrows. This is in a way comparable to a mindmap. However, Mindmaps do not serve the purpose of presenting a final, comprehensive overview over a specific focus on a topic, but rather an unstructured, spontaneous collection of general related ideas around one key term. Both processes - Concept Maps and Mindmaps - can be done by a single person or in a group. For example, Brown (2003) shows how group-based creation of a concept map can enhance biology teaching.",
        "summary": "Concept Maps are visual representations of conceptual ideas, with information put into boxes or circles and connected through arrows. This is similar to a mindmap, but unlike mindmaps, Concept Maps aim to present a final, comprehensive overview of a specific topic. Both Concept Maps and mindmaps can be created by an individual or a group.",
        "question": "What are Concept Maps and how do they differ from mindmaps?",
        "ground_truths": "Concept Maps are visual representations of conceptual ideas, with information put into boxes or circles and connected through arrows. They differ from mindmaps in that they aim to present a final, comprehensive overview of a specific topic, while mindmaps are an unstructured, spontaneous collection of general related ideas around one key term."
    },
    {
        "contexts": "Group Concept Mapping typically consists of six linear steps (see figure): Preparation, Statement Generation, Structuring, Representation, Interpretation, Utilization. Below, we will elaborate on each step based on the original methodological design by Trochim (1989), and present the process as done in the exemplary study by McCaffrey et al. (2019), who attempted to generate a conceptual model of good health care from the perspective of patients.",
        "summary": "Group Concept Mapping consists of six steps: Preparation, Statement Generation, Structuring, Representation, Interpretation, Utilization. Each step is elaborated based on Trochim's original methodological design, and the process is presented as done in a study by McCaffrey et al., who attempted to create a conceptual model of good health care.",
        "question": "What are the six steps of Group Concept Mapping?",
        "ground_truths": "The six steps of Group Concept Mapping are Preparation, Statement Generation, Structuring, Representation, Interpretation, and Utilization."
    },
    {
        "contexts": "Missing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55). In this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.",
        "summary": "Missing values in datasets can occur due to various reasons like inaccurate data collection, exclusion of irrelevant data, or respondents' inability to provide answers. This text explores the types and distributions of missing values and their implications for data analysis.",
        "question": "What are some reasons for missing values in datasets?",
        "ground_truths": "Missing values in datasets can occur due to inaccurate data collection, exclusion of irrelevant data, or respondents' inability to provide answers."
    },
    {
        "contexts": "There are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149). Unit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).",
        "summary": "The two main types of missing values are unit nonresponse and item nonresponse. Item nonresponse happens when an individual can't answer a specific question, while unit nonresponse occurs when an entire unit, like a household or business, can't provide answers.",
        "question": "What are the two main types of missing values?",
        "ground_truths": "The two main types of missing values are unit nonresponse and item nonresponse."
    },
    {
        "contexts": "The distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).",
        "summary": "Missing values in a dataset can be distributed randomly or non-randomly, affecting the analysis and conclusions. The three common distributions are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR).",
        "question": "What are the three common distributions of missing values in a dataset?",
        "ground_truths": "The three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR)."
    },
    {
        "contexts": "Missing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).",
        "summary": "Missing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables and do not follow any pattern or trend. They are completely random and do not contain any necessary information.",
        "question": "What is the characteristic of missing values that are missing completely at random (MCAR)?",
        "ground_truths": "Missing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables and do not follow any pattern or trend. They are completely random and do not contain any necessary information."
    },
    {
        "contexts": "Missing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55).",
        "summary": "Missing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but might be related to other variables in the dataset. They are not completely random, but not systematically related to the true value of the missing values either.",
        "question": "What is the characteristic of missing values that are missing at random (MAR)?",
        "ground_truths": "Missing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but might be related to other variables in the dataset. They are not completely random, but not systematically related to the true value of the missing values either."
    },
    {
        "contexts": "Missing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).",
        "summary": "Missing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. The missing data are not random and are influenced by some underlying factor, which can lead to biased results if not properly accounted for.",
        "question": "What is the characteristic of missing values that are missing not at random (MNAR)?",
        "ground_truths": "Missing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. The missing data are not random and are influenced by some underlying factor, which can lead to biased results if not properly accounted for."
    },
    {
        "contexts": "A heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation. Heatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.",
        "summary": "A heatmap is a data visualization tool where numerical values are replaced with colors, making it easier to understand the data. Each column represents a feature and each row an observation.",
        "question": "What is a heatmap and why is it useful?",
        "ground_truths": "A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution."
    },
    {
        "contexts": "The principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen. It is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.",
        "summary": "Colors in a heatmap are assigned based on the ranking of values from highest to lowest, segregated into bins. For small datasets, colors might be assigned based on the values themselves. Normalization of data is often required to bring all values to a common scale.",
        "question": "How are colors assigned in a heatmap and why is normalization of data important?",
        "ground_truths": "Colors in a heatmap are assigned based on the ranking of values, segregated into bins. For small datasets, colors might be assigned based on the values themselves. Normalization of data is important to bring all values to a common scale without distorting differences in the ranges of values."
    },
    {
        "contexts": "In the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let\u2019s look at the feature \u201cNumber of Carburetors\u201d. We can see that Maserati Bora has the darkest color, hence it has the largest number of carburetors, followed by Ferrari Dino, which has the second-largest number of carburetors. While other models such as Fiat X1-9 or Toyota have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.",
        "summary": "In a heatmap, darker colors usually represent higher values and lighter colors represent lower values. For example, in the 'Number of Carburetors' feature, Maserati Bora has the darkest color, indicating the highest number of carburetors.",
        "question": "How are the colors in a heatmap interpreted?",
        "ground_truths": "In a heatmap, darker colors usually represent higher values and lighter colors represent lower values."
    },
    {
        "contexts": "The choice of color for the heatmap is one of the most important aspects of creating an understandable and nice-looking representation of the data. If you do not specify the color (as in the example above) then the default color palette will be applied. However, you can use the argument col and choose from a wide variety of palettes for coloring your heatmap.",
        "summary": "The choice of color is crucial in creating a heatmap. If not specified, the default color palette is used. However, one can choose from a variety of color palettes using the 'col' argument.",
        "question": "How can one choose the color palette for a heatmap?",
        "ground_truths": "One can choose the color palette for a heatmap using the 'col' argument. If not specified, the default color palette is used."
    },
    {
        "contexts": "The roots of Hermeneutics reach back to antiquity. The word 'Hermeneutics' derives from the Greek word ''hermeneuein'', which basically means \u201cto explain\u201d. The messenger-god Hermes brought messages from the gods to the humans and explained their meaning to them (4). There has been a highly developed practice of interpretation in Greek antiquity, aiming at diverse interpretanda like oracles, dreams, myths, philosophical and poetical works, but also laws and contracts. The beginning of ancient Hermeneutics as a more systematic activity, however, goes back to the exegesis of the Homeric epics.",
        "summary": "Hermeneutics, originating from the Greek word 'hermeneuein' meaning 'to explain', has roots in antiquity. It began as a systematic activity with the interpretation of Homeric epics and was used to interpret various forms of communication such as oracles, dreams, myths, and laws.",
        "question": "What is the origin and early use of Hermeneutics?",
        "ground_truths": "Hermeneutics originated from the Greek word 'hermeneuein' and was used in antiquity to interpret various forms of communication such as oracles, dreams, myths, and laws. It began as a systematic activity with the interpretation of Homeric epics."
    },
    {
        "contexts": "Hermeneutics was further developed in the Middle Ages: \"In its earliest modern forms, hermeneutics developed primarily as a discipline for the analysis of biblical texts. It represented a body of accepted principles and practices for interpreting an author's intended (and inspired) meaning, as well as providing the proper means of investigating a text's socio-historical contexts.\" (5). Then, \"Hermeneutics took on a new and widening importance in the early modern period with the application of philological techniques for the validation and authentication of classical texts. With the Reformation, hermeneutic approaches were further deployed in intense exegetial exercises devoted to the resolution of contested meanings in scriptural texts.\" (4) The ambition of early modern hermeneuts was to assign correct meanings to a text and generate 'truth'. These aspects remained the focus of Hermeneutics until the period of Enlightenment.",
        "summary": "During the Middle Ages, Hermeneutics evolved as a discipline for analyzing biblical texts, interpreting author's intentions, and investigating socio-historical contexts. It gained importance in the early modern period with the use of philological techniques for validating classical texts. The main goal was to assign correct meanings to a text and generate 'truth', a focus that remained until the Enlightenment.",
        "question": "How did Hermeneutics evolve during the Middle Ages and the early modern period?",
        "ground_truths": "During the Middle Ages, Hermeneutics evolved as a discipline for analyzing biblical texts, interpreting author's intentions, and investigating socio-historical contexts. In the early modern period, it gained importance with the use of philological techniques for validating classical texts. The main goal was to assign correct meanings to a text and generate 'truth', a focus that remained until the Enlightenment."
    },
    {
        "contexts": "During the last decades and centuries, Hermeneutics has emerged into a field of philosophy, whereas its origins lie rather in a methodological approach. \"It has recently emerged as a central topic in the philosophy of the social sciences, the philosophy of art and language and in literary criticism - even though its modern origin points back to the early ninteenth century.\" (2) As a method, Hermeneutics is nowadays mostly used in the Social Sciences and Humanities, including theology, law, psychology, philosophy and history, with diverging methodological characteristics.",
        "summary": "Hermeneutics, originally a methodological approach, has evolved into a field of philosophy. It has become a central topic in the philosophy of social sciences, art, language, and literary criticism. Currently, it is primarily used in Social Sciences and Humanities, including theology, law, psychology, philosophy, and history, with varying methodological characteristics.",
        "question": "How has Hermeneutics evolved in recent decades and centuries, and where is it primarily used today?",
        "ground_truths": "Originally a methodological approach, Hermeneutics has evolved into a field of philosophy and has become a central topic in the philosophy of social sciences, art, language, and literary criticism. Today, it is primarily used in Social Sciences and Humanities, including theology, law, psychology, philosophy, and history, with varying methodological characteristics."
    },
    {
        "contexts": "With the rise of language, and much later with the invention of writing, we witnessed the dawn of human civilisation. Communicating experience and knowledge is considered to be one of the most pivotal steps that led to the formation of societies. Once we moved from being hunters and gatherers into larger and more complex cultures that lived in cities, a surplus in food production allowed for some privileged people to preoccupy themselves with other goals than the mere safeguarding of their daily survival. This led to the blossoming of early cultures across the globe, many of which with tremendous developments in terms of agriculture, engineering and architecture. The rise of urban cultures East and West led to a new line of inquiry and ultimately also a new line of thinking, most notably in civilizations such as the Vedic, the Zhou period, early Persian culture, and - often most recognised in the West - Ancient Greece.",
        "summary": "The rise of language and writing marked the dawn of human civilization, leading to the formation of societies. As societies evolved from hunters and gatherers to complex cultures living in cities, a surplus in food production allowed some privileged individuals to focus on other goals. This led to the development of early cultures globally, with significant advancements in agriculture, engineering, and architecture. The rise of urban cultures in the East and West led to new lines of inquiry and thinking, particularly in civilizations such as the Vedic, the Zhou period, early Persian culture, and Ancient Greece.",
        "question": "What led to the development of early cultures globally?",
        "ground_truths": "The rise of language and writing, the formation of societies, and a surplus in food production which allowed some privileged individuals to focus on other goals led to the development of early cultures globally."
    },
    {
        "contexts": "Greece is often in the focus because we consider it the birthplace of modern democracy, despite only a small privileged elite actually being considered citizens. Though these privileged were only few, we owe Aristotle, Socrates and Plato and many others the foundation of Western Philosophy. Empirical inquiry did not weigh down thinking yet, hence much of the thinking of Greek philosophy was lofty but free from the burden of real world inquiry. There were connections between philosophical thinking and the real world - you might remember Pythagoras from school - yet much later philosophy and the rest of science would be vastly disconnected. Early accounts such as the scriputures of Herodot give testimony of the history of this age, and represent one of the earliest accounts of a systematic description of geography and culture.",
        "summary": "Greece is considered the birthplace of modern democracy, despite only a small privileged elite being considered citizens. We owe Aristotle, Socrates, Plato, and many others the foundation of Western Philosophy. Greek philosophy was lofty and free from the burden of empirical inquiry. There were connections between philosophical thinking and the real world, but philosophy and science would later become vastly disconnected. Early accounts, such as the scriptures of Herodot, provide a systematic description of geography and culture during this age.",
        "question": "Why is Greece considered important in the history of Western Philosophy?",
        "ground_truths": "Greece is considered important in the history of Western Philosophy because it is where Aristotle, Socrates, Plato, and many others laid the foundation of Western Philosophy."
    },
    {
        "contexts": "Eastern cultures often had a comparably early development of philosophies, with Confucius' work as well as the Vedas and the Pali canon as testimony of the continuous cultural influence that is comparable to Greek philosophy in the West. Equally did many Eastern empires use methods such as census and survey as measures of governance, and many other notable approaches that would later contribute to the formation of scientific methods. Law was an early testimony of the necessity of rules and norms in human societies. Consequently, Legal Research can be seen as one of the earliest forms of inquiry that translated systematic inquiry and analysis directly to the real world.",
        "summary": "Eastern cultures had an early development of philosophies, with works like Confucius', the Vedas, and the Pali canon showing continuous cultural influence comparable to Greek philosophy in the West. Eastern empires used methods such as census and survey for governance, contributing to the formation of scientific methods. Law was an early testimony of the necessity of rules and norms in societies, making Legal Research one of the earliest forms of systematic inquiry and analysis applied to the real world.",
        "question": "How did Eastern cultures contribute to the formation of scientific methods?",
        "ground_truths": "Eastern cultures contributed to the formation of scientific methods by using methods such as census and survey for governance, which would later contribute to the formation of scientific methods."
    },
    {
        "contexts": "Between the ancients and the medieval times there a somewhat blurry gap, with the fall of the Roman Empire in the West, the expansion of the Mongol empire in the East, and the occupation of much of India by Islam as notable elements of change. All this triggered not only an enormous cultural change, but more importantly an increasing exchange, leading to schools of thoughts that became increasingly connected through writing, and also often in thinking. Logic is an example of a branch of philosophy that linked the ancients (Plato, Confucius, Buddha) with the philosophy of the enlightenment. This was an important precondition in medieval times for a systematic line of thinking, triggering or further developing pragmatic, analytic and sceptic approaches, among others. Logic as a part of philosophy provided the basis for a clear set of terms and rhetoric within inquiry, and would later become even more important with regards to rationality. The early need for a clear wording was thus deeply rooted in philosophy, highlighting the importance of this domain until today.",
        "summary": "The period between the ancients and the medieval times saw significant changes, including the fall of the Roman Empire, the expansion of the Mongol empire, and the occupation of much of India by Islam. These changes triggered a cultural shift and an increase in exchange, leading to schools of thought that became increasingly connected through writing and thinking. Logic, a branch of philosophy, linked the ancients with the philosophy of the enlightenment, providing a basis for a clear set of terms and rhetoric within inquiry. This need for clear wording was deeply rooted in philosophy, highlighting its importance until today.",
        "question": "What role did logic play in the development of philosophy?",
        "ground_truths": "Logic, a branch of philosophy, linked the ancients with the philosophy of the enlightenment, providing a basis for a clear set of terms and rhetoric within inquiry. This need for clear wording was deeply rooted in philosophy, highlighting its importance until today."
    },
    {
        "contexts": "Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer Alhazen (a.k.a. Ibn al-Haytham). Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate experimental conditions in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of scientific disciplines, which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.",
        "summary": "The approach of controlled testing by Arabian mathematician and astronomer Alhazen brought us closer to the concrete application of scientific methods. Alhazen, emerging from the mathematics of antiquity and combining this with the rising investigation of physics, was the first to systematically manipulate experimental conditions, paving the way for the scientific method. Alhazen, a polymath, highlights the rise of knowledge that enabled such characters, but was still far from the true formation of the diverse canon of scientific disciplines. Alhazen is one of many who contributed to the rise of science in the Islamic world during medieval times, seen as a cradle of Western science and a continuity from the ancients, when much of the Greek and Roman heritage was lost in Europe.",
        "question": "How did Alhazen contribute to the development of scientific methods?",
        "ground_truths": "Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method."
    },
    {
        "contexts": "Originating in the long-term perspective of forest management, many cultures had recognized sustainability in its original meaning since a long time. With the rise of colonialism and the industrialisation, the impact of human societies onto the environment propelled into previously unseen dimensions, and the negative effects of pollution were an early indicator that modern factories and coal plants were not only drivers of economic growth, but equally harming livelihoods and the environment. First accounts of the environmental damage were localised, and Carson\u2018s Silent Spring is an early account of a systematic yet widely descriptive account of the negative impact that human activities had on the environment. This localised perspective was quickly altered onto a global scale through the Club of Rome. Their discourse around Climate Change was a first step from a problem-orientated perspective (Reports 1-3) to a solution-oriented perspective. The Millennium Ecosystem Assessments were equally hinting at the shift from describing the system towards a more transformative agenda by the follow-up institution IBPES.",
        "summary": "Sustainability, originally recognized in forest management, gained importance with the rise of industrialization and its environmental impact. Early accounts like Carson's Silent Spring highlighted the negative effects of human activities. The Club of Rome shifted the focus from local to global, and from problem-oriented to solution-oriented. The Millennium Ecosystem Assessments further emphasized this shift towards a transformative agenda.",
        "question": "What was the role of the Club of Rome in the evolution of sustainability science?",
        "ground_truths": "The Club of Rome shifted the focus from local to global, and from problem-oriented to solution-oriented in sustainability science."
    },
    {
        "contexts": "The first or at least earliest relevant recognition of the global scale and the combination with inequality came from Karl Marx. While Marx did not elaborate on a methodological approach, his work is a foundation that opened a new empirical dimension. Economic data gained prominence out of the global trade rooted in colonialism, with the Dutch double book-keeping serving as a initial basis for a rising amount of money and numbers. Maps and surveys led to a global spatial understanding, and an early combination of economic purpose and spatial mapping was deployed in forestry, which integrated a long term perspective with a resource focus. Another contribution that slowly emerged as a necessary reaction to the industrialisation was the observation of pollution. An early prominent example was the Great Smog of London, which killed thousands due to specific weather conditions that trapped the pollution over the city.",
        "summary": "Karl Marx's work was the first to recognize the global scale of inequality. Economic data, rooted in colonialism, gained prominence, with Dutch double book-keeping as a basis. Maps and surveys led to a global spatial understanding, with forestry integrating economic purpose and spatial mapping. The observation of pollution emerged as a reaction to industrialization, with the Great Smog of London as a prominent example.",
        "question": "How did the observation of pollution emerge in the contexts of sustainability science?",
        "ground_truths": "The observation of pollution emerged as a necessary reaction to industrialization, with the Great Smog of London as a prominent example."
    },
    {
        "contexts": "As data plays an increasingly important role in our day-to-day life not only as researchers but also as individuals, the issue regarding data privacy and security has become more sensitive. As such, governing bodies (of countries or union of countries) have been taking more active steps to address this issue. Specifically in EU (as of November 2019) GDPR mandates that in case of personal data only the dataset that is immediately usable is stored, and only for the duration of time that the data is used for. There are further intricacies that are important to have in mind. As such, keeping yourself aware of the data privacy and security laws of the area where you and/or your stakeholders operate in is very important.",
        "summary": "Data privacy and security are increasingly important, with governing bodies taking active steps to address this. In the EU, GDPR mandates that only immediately usable personal data is stored, and only for as long as it's used. Awareness of data privacy and security laws in your area is crucial.",
        "question": "What does the GDPR mandate regarding data storage?",
        "ground_truths": "GDPR mandates that in case of personal data only the dataset that is immediately usable is stored, and only for the duration of time that the data is used for."
    },
    {
        "contexts": "Some organizations have a strict policy that determines how long the data should be stored for. These policies generally already account for the current legal landscape and hence make deciding how long to store data for quite straightforward \u2014 you store the data for as long as the organization mandates that you store them for.",
        "summary": "Organizations often have strict policies determining data storage duration. These policies usually account for legal requirements, making the decision straightforward.",
        "question": "How do organizational policies influence data storage duration?",
        "ground_truths": "Organizations often have strict policies that determine how long the data should be stored for. These policies generally already account for the current legal landscape."
    },
    {
        "contexts": "Some researches are more sensitive than others. For examples, consider that you are performing research in medicine industry where you work with patients' data. In this case, the data you have access to, what you do with that data, and the next steps your organization takes based on your work are all incredibly sensitive. Compare that to another situation where you are an engineer that wants to learn some process and are work with simulated data. Here too, your work is important and carries real consequences in the future. However, one of the biggest differences in the two aforementioned contexts is the data involved. You would naturally have to treat data on peoples' health issues and behaviors more importantly than you would a simulated data.",
        "summary": "The sensitivity of research varies. For instance, medical research involving patient data is highly sensitive compared to engineering research with simulated data. The nature of the data involved significantly influences how it should be treated.",
        "question": "How does the nature of research influence data treatment?",
        "ground_truths": "The sensitivity of research varies. For instance, medical research involving patient data is highly sensitive compared to engineering research with simulated data."
    },
    {
        "contexts": "If you can ensure that you can keep the data you use for your research private and secure for however long you have to store the data for, then adhering to the organizational policy (if available) or the 10-years-long heuristic (if no other guidelines are available) are fine. However, if you lack skills or resources to ensure data privacy and security, then you should reconsider long-term storage of data. Either you have to use services that ensure data security for you, or you will have to purge the data as soon as its purpose has been fulfilled (which is not ideal).",
        "summary": "If you can keep your research data private and secure, adhering to organizational policy or a 10-year heuristic is acceptable. However, if you lack the skills or resources for data privacy and security, reconsider long-term storage or use services that ensure data security.",
        "question": "What should be considered when deciding on long-term data storage?",
        "ground_truths": "If you lack the skills or resources to ensure data privacy and security, you should reconsider long-term storage of data."
    },
    {
        "contexts": "First of all, it is difficult to know whether you really want to become a PhD student. Back in the day, I did not want to do a PhD. I was basically done with University. Then my future supervisors gave me a call, with the words: \"This is a call from fate\". I feel he was totally on point, spot on. For me, doing a PhD was serendipity. Not everybody is always totally clear whether doing a PhD is the right way. Others are clear about it already while doing their Bachelor, and always claim they want to do a PhD. Consequently they do not need to ponder much about it. What is however tricky is the group of people who do not really know what to do in general, and just do a PhD, because, why not? While it is save to say that everybody needs to find their way in doing a PhD, it is difficult if you start a PhD because you do not have any alternatives. Do not misunderstand me: many of the best PhDs will come out of this group of people who lack alternatives. However, some will realise halfway that maybe a PhD is not the main goal in their life. This is perfectly alright. However, ideally you should make up your mind as early as possible. I wish I could offer some suggestion on who should ideally opt for doing a PhD, yet I feel that academia is built on diversity, and hence there is no universal recipe for a PhD, let alone for a successful PhD student, that is.",
        "summary": "Deciding to pursue a PhD can be challenging. Some people know early on that they want to do a PhD, while others stumble upon it by chance. However, there are also those who pursue a PhD because they don't have other options. While some of the best PhDs come from this group, others may realize halfway that a PhD is not their main goal. It's important to make up your mind as early as possible, but there's no universal recipe for a successful PhD student.",
        "question": "Why is it important to make up your mind early about pursuing a PhD?",
        "ground_truths": "It's important to make up your mind early about pursuing a PhD because some people may realize halfway through that a PhD is not their main goal. This can lead to wasted time and resources."
    },
    {
        "contexts": "What is however relevant for your decision to do a PhD is that you get funding. If you apply for a project position, you will have funding, but there is also a clear expectation of what you are supposed to be working on. Quite often, you are part of a bigger team, and this demands not only a will to cooperate, but also a clear responsibility and reliability towards your other fellows. In my experience, there are quite some positions out there in exciting projects, but the collaboration in such projects is then part of your PhD, and you will constantly have to negotiate between your goals, and the goals of the team. While this sounds easy in theory, it is often a hole in which many work hours are buried under a pile of emotions. On the other hand, you may be the only PhD in a team working on a specific topic. There are for instance quite some scholarships available, and while I never understood the decisions about which PhDs get funding, and which are not funded, these scholarships have at least some sort of a stochastic chance to gain you funding. These scholarships are more independent, which then becomes a problem down the road. Many people believe that academia is about being part of a larger team. While this is in some sense true, because academics can work in teams, this exchange in teams will not take away the necessity that it is you who needs to shoulder the bulk of the workload. This workload is at the heart of any PhD, even if you want to contribute to a larger narrative, you will have to mostly work alone. This is something that almost all PhDs struggle with: to accept that it is them who will have to do the work, and no one else. As a supervisor, I am willing to go all the way with you, but I cannot go the way for you. You have to learn to walk this path alone, a supervisor is just for the general direction. A PhD is ultimately about learning to overcome yourself, not for others to help you overcome yourself. Others - your fellow students and supervisors - may be catalystic, but the learning is all yours. Embrace this early on. Everybody struggles with it, so I felt it is only fair if I tell you this early on.",
        "summary": "Funding is a crucial factor in deciding to do a PhD. If you apply for a project position, you'll have funding but also clear expectations and responsibilities. You may be part of a team or work alone on a specific topic. Scholarships offer a chance for funding but also require independence. A PhD involves a lot of individual work, even if you're part of a team. It's about learning to overcome yourself, with others serving as catalysts. This is a struggle for most PhD students.",
        "question": "What is the role of a supervisor in a PhD journey?",
        "ground_truths": "A supervisor in a PhD journey is there to provide general direction. They can support and guide the student, but they cannot do the work for the student. The student has to learn to walk the path alone."
    },
    {
        "contexts": "However, there are some character traits that I observed in the past that created successful cases. First and foremost, I think that previous PhDs I know were often thrilled and excited about their research. This showed in many different appearances, but overall, many of these folks were basically quite driven in terms of their focus. You know it when you see it as an external observer. However, you do not know it if you are one of those driven people themselves. Instead, most potential PhD students are full of doubts, focus on their wannabe limitations, and are often never sure whether they are on the right track. Therefore, it is often a good indicator if successful researchers consider it a good choice for you to pursue a PhD, and are willing to support you. A PhD is more about what you could become compared to what you are. Learning is relative. Taken together, if an experienced researcher is willing to support you, and you feel you want to commit the next years of your life to work deeply on one topic, and mostly for yourself, then you may be the right person for a PhD. This brings us to the next point.",
        "summary": "Successful PhD students often exhibit certain traits, such as excitement about their research and a driven focus. However, many potential PhD students are full of doubts and unsure if they're on the right track. If successful researchers support your decision to pursue a PhD and you're willing to commit to deep work on a single topic, you may be the right person for a PhD.",
        "question": "What are some traits of successful PhD students?",
        "ground_truths": "Successful PhD students are often thrilled and excited about their research and are quite driven in terms of their focus. They are also willing to commit to deep work on a single topic."
    },
    {
        "contexts": "Iconology is a research method originating in the field of art history and was founded by the German art historian Erwin Panofsky in cooperation with Aby Warburg in the 1930s (6, 3). To this day it is one of the groundworks for all visual analysis done in art, media, theatre, and performance analysis (4, 8). Contemporarily, it is also used as an analysis method in research fields like architecture, literary studies and even politics (4, 8). This entry focuses on the basics of the traditional Iconology. However, <artwork> and <artist> can also be replaced by any other object and practitioner and can be especially effective in experimental, inter-, and transdisciplinary research.",
        "summary": "Iconology, a research method in art history, was founded by Erwin Panofsky and Aby Warburg in the 1930s. It is fundamental to visual analysis in art, media, theatre, and performance, and is also used in architecture, literary studies, and politics. The method can be applied to any object or practitioner, making it effective in various research fields.",
        "question": "Who founded the research method of Iconology and in what fields is it used?",
        "ground_truths": "The research method of Iconology was founded by Erwin Panofsky and Aby Warburg. It is used in fields such as art history, media, theatre, performance, architecture, literary studies, and politics."
    },
    {
        "contexts": "Generally, Iconology works as a three-step model through which different questions are formulated and the according insights are put together subsequently in a complex manner (10). The three steps are: the pre-iconographic step, the step of interpretation (or iconography) and the iconological synthesis (10). Panofsky advised to not consider the steps as absolutely separated, because the verbal articulation of the artwork can only really be usefully read in its complete meaning after the analysis and interpretation (7).",
        "summary": "Iconology is a three-step model consisting of the pre-iconographic step, the step of interpretation, and the iconological synthesis. These steps are not separate, as the full meaning of the artwork is understood only after the complete analysis and interpretation.",
        "question": "What are the three steps involved in the Iconology research method?",
        "ground_truths": "The three steps involved in the Iconology research method are the pre-iconographic step, the step of interpretation, and the iconological synthesis."
    },
    {
        "contexts": "The pre-iconographic step involves the analysis of all formal aspects of the artwork (1a) and the articulation of the emotional reception of the work (1b) (10). Usually, the formal aspects are described upfront (1, 10). However, sometimes the emotional reception is formulated first and positioned ahead of the rest of the analysis as a \u201cpercept\u201d (zu Deutsch: \u201cPerzept\u201d), because it involves the researcher\u2019s subjectivity and is therefore a non-scientific insight which some writers and readers like to separate from the scientific text. Nonetheless, the percept is an important part of the iconological analysis and should not be completely excluded, because it can situate the analysis historically and culturally and lead to fruitful insights for the analysis itself and for foreign or future audiences.",
        "summary": "The pre-iconographic step in Iconology involves analyzing the formal aspects of the artwork and articulating the emotional reception. While the formal aspects are usually described first, the emotional reception, or 'percept', is sometimes positioned at the beginning due to its subjective nature. Despite being non-scientific, the percept is crucial for situating the analysis historically and culturally.",
        "question": "What does the pre-iconographic step in Iconology involve and why is the 'percept' important?",
        "ground_truths": "The pre-iconographic step in Iconology involves analyzing the formal aspects of the artwork and articulating the emotional reception. The 'percept' is important as it situates the analysis historically and culturally, despite its subjective and non-scientific nature."
    },
    {
        "contexts": "The easiest way to represent count information are basically barplots. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation. Correlation plots ('xyplots') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a regression line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.",
        "summary": "Barplots are a simple way to represent count information, with the ability to show either absolute numbers or proportions. They can be simplistic or more advanced depending on the levels of information they contain. Correlation plots, or 'xyplots', are another common statistical graphic, often used to represent correlations. They can also include a regression line to show effect strengths and variance, and can be enhanced with additional information through point size or color.",
        "question": "What are the basic forms of data visualisation in statistics?",
        "ground_truths": "The basic forms of data visualisation in statistics are barplots and correlation plots. Barplots represent count information and can show either absolute numbers or proportions. Correlation plots represent correlations and can include a regression line to show effect strengths and variance."
    },
    {
        "contexts": "Boxplots are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant. A histogram is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.",
        "summary": "Boxplots and histograms are two other important statistical figures. Boxplots show the variance of continuous data across different factor levels and are a solid representation of the Analysis of Variance. Histograms, on the other hand, display data using bars of different heights to group numbers into ranges, providing information about the distribution of data, its median, and skewness.",
        "question": "What are boxplots and histograms used for in data visualisation?",
        "ground_truths": "Boxplots are used to show the variance of continuous data across different factor levels and are a solid representation of the Analysis of Variance. Histograms are used to display data using bars of different heights to group numbers into ranges, providing information about the distribution of data, its median, and skewness."
    },
    {
        "contexts": "Multivariate data can be principally shown by three ways of graphical representation: ordination plots, cluster diagrams or network plots. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.",
        "summary": "Multivariate data can be represented graphically in three ways: ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data and are more graphical than statistical.",
        "question": "How can multivariate data be graphically represented?",
        "ground_truths": "Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data."
    },
    {
        "contexts": "The first thing to note is that exam success hardly depends on the few weeks before the exam itself, but rather on your whole semester. Starting to learn early and consistently will, in the long run, save you a lot of time and stress and will also improve your exam success. Part of this is having a more or less clear overview on what is going to be covered or assessed and what it is you want to learn. This should also include a clear idea on what is not interesting or too important, either for yourself or the exam. Try to get an overview of topics and contents via the syllabus or ask the teacher / lecturer for it. Another part is always being more or less up to date with lectures or seminars. It doesn't take more than 30 minutes after a lecture for you to forget most of what you just heard. If at all possible, take 10 to 15 minutes after each session to work with your notes, reformulate them and throw them into your knowledge base (whatever that is). A good idea is also to directly transfer them to a flashcard tool such as Anki (see below).",
        "summary": "Exam success depends on consistent learning throughout the semester, not just the weeks before the exam. It's important to have a clear overview of what will be covered and what you want to learn, including what is not too important. Stay up to date with lectures and seminars, and review your notes shortly after each session to retain the information. Using a flashcard tool like Anki can be helpful.",
        "question": "How can one improve their chances of exam success?",
        "ground_truths": "One can improve their chances of exam success by starting to learn early and consistently throughout the semester, having a clear overview of what will be covered and what they want to learn, staying up to date with lectures and seminars, and reviewing their notes shortly after each session to retain the information."
    },
    {
        "contexts": "There are several learning techniques that have been scientifically proven to yield better results than what is commonly used by many students and learners. Try to stick to these as you learn! Spaced-Repetition One important concept is spaced repetition. This means that you should repeat content in increasingly longer intervals to ensure long-term retention. So, concretely, if you've just learned something, repeat it first on the next day, then after 3 days, after 7 days, after two weeks, and so on. Repetition intervals should depend on how well you were able to remember something on a given day (e.g. when you had lots of trouble after 7 days, maybe throw in another repetition 2 days later). The aforementioned overview of learning content can be combined with this, i.e. you can make a table with all the content for a given course and track when you repeated which topic to help you keep an overview.",
        "summary": "Several learning techniques can yield better results than common methods. One such technique is spaced repetition, which involves repeating content at increasingly longer intervals to ensure long-term retention. The intervals should depend on how well you remembered the content. You can also make a table to track when you repeated each topic.",
        "question": "What is the concept of spaced repetition in learning?",
        "ground_truths": "Spaced repetition in learning is a technique where you repeat content at increasingly longer intervals to ensure long-term retention. The intervals should depend on how well you remembered the content."
    },
    {
        "contexts": "Active Recall means recapitulating everything you know on a topic or question you are learning without checking your notes or sources. There is a significant learning edge to this over re-reading notes or trying to immediately fill in your gaps. Try to remember everything you know for as hard as possible until you are certain that you will not remember anything else. Only then should you check back with your notes to see if you have forgotten anything. This will then subsequently show you on which gaps you need to focus more. Useful tools can be using MindMaps or SpiderDiagrams, but also writing down bullet points, full texts or talking to a friend about it. If you happen to have an interim outage of friends, it is absolutely fine to talk to your chair or any other attentive listener that can't escape.",
        "summary": "Active Recall involves trying to remember everything you know about a topic without checking your notes. This method is more effective than re-reading notes or immediately filling in gaps. After trying to remember everything, you can check your notes to see what you've forgotten. Tools like MindMaps, SpiderDiagrams, bullet points, full texts, or discussing with a friend can be useful.",
        "question": "What is the method of Active Recall in learning?",
        "ground_truths": "Active Recall in learning is a method where you try to remember everything you know about a topic without checking your notes. After trying to remember everything, you can check your notes to see what you've forgotten. Tools like MindMaps, SpiderDiagrams, bullet points, full texts, or discussing with a friend can be useful."
    },
    {
        "contexts": "The field of Law has a long-standing history. \"Roman legal doctrine developed since the second century before Christ, and reached a very high level as from the third century after Christ. Its rediscovery and renewed study in Bologna in the eleventh century was the start for the creation of universities.\" (van Hoecke 2011, p.1). The emergence of Law and its eventual institutionalisation as a scientific discipline was fundamentally based on the reading and interpretation of (Roman) legal documents (2). \"During the whole of the Middle-Ages, legal doctrine was highly thought of and considered as a 'scientific discipline', as in those times 'authoritative interpretation', not 'empirical research', was the main criterion for the scientific status of a discipline.\" (van Hoecke 2011, p.1). The discipline subsequently spread from the first universities in Italy throughout Europe (2). In this time, legal work was mostly restricted to each national contexts due to the differences in the respective legal systems (2). This early approach to Law as a scientific discipline represents the so-called 'doctrinal approach' which is still the most common understanding of legal work (see What the method does).",
        "summary": "Law has a long history, with Roman legal doctrine developing since the second century before Christ and reaching a high level by the third century after Christ. Its rediscovery and study in Bologna in the eleventh century led to the creation of universities. The emergence of Law as a scientific discipline was based on the reading and interpretation of legal documents. During the Middle Ages, legal doctrine was considered a scientific discipline, with authoritative interpretation being the main criterion for scientific status. The discipline spread from the first universities in Italy throughout Europe, with legal work mostly restricted to each national contexts due to differences in legal systems. This early approach to Law is known as the 'doctrinal approach'.",
        "question": "What is the 'doctrinal approach' in the contexts of legal research?",
        "ground_truths": "The 'doctrinal approach' in legal research refers to the early approach to Law as a scientific discipline, which was based on the reading and interpretation of legal documents. This approach was the most common understanding of legal work during the Middle Ages."
    },
    {
        "contexts": "However, a second approach developed from the 17th Century on, but mainly from the 19th Century until today. With the rise of positivism - which implies that the only valid knowledge arises from scientific inquiry and falsification of scientific theory -, a new understanding of science emerged. Here, the use of empirical data, the testing of hypotheses and the development of theories, independent from geographical limitations, became the new scientific ideal. In consequence, Law as a scientific discipline changed (van Hoecke 2011, p.1). The research focus in Law partly shifted, both in terms of research questions and methodology: \"[i]n the 1960s and 1970s, legal realists and socio-legal scholars started the law and society movement, and pointed to the importance of understanding the gap between 'law in books' and 'law in action', and the operation of law in society. They were interested in examining the legal system in terms of whether legal reform brings about beneficial social effects and protects the interests of the public. Similarly, in the 1980s critical legal studies integrated ideas and methods found in disciplines such as sociology, anthropology and literary theory.\" (McConville & Chui 2007, p.5). With Law being a discipline between the Sciences and the Humanities, this new movement borrowed heavily from the Social Sciences in terms of methodology. New adjacent disciplines emerged, including legal sociology, legal psychology and law and economics, among others (2, 5). This 'Law in contexts' - also called 'socio-legal' - approach includes new topics and methodological directions with a stronger focus on empirical research and theory building and has been universally taken up in academic institutions.",
        "summary": "A second approach to legal research developed from the 17th Century onwards, mainly from the 19th Century until today. This approach was influenced by the rise of positivism, which posits that valid knowledge comes from scientific inquiry and falsification of scientific theory. This led to a shift in the focus of legal research, with an emphasis on empirical data, hypothesis testing, and theory development. In the 1960s and 1970s, legal realists and socio-legal scholars started the law and society movement, focusing on the gap between 'law in books' and 'law in action', and the operation of law in society. This 'socio-legal' approach, which includes new topics and methodological directions with a focus on empirical research and theory building, has been universally adopted in academic institutions.",
        "question": "What is the 'socio-legal' approach in legal research?",
        "ground_truths": "The 'socio-legal' approach in legal research is a method that emerged in the 1960s and 1970s, focusing on the gap between 'law in books' and 'law in action', and the operation of law in society. This approach includes new topics and methodological directions with a focus on empirical research and theory building, and has been universally adopted in academic institutions."
    },
    {
        "contexts": "From this contextualized approach to Law, research emerged on the relationship between Law and Society and questions of power relations, more specificially on aspects such as gender, social class, ethnicity and religion (5). ''''Socio-legal research' may be seen as an alternative or a supplement, but not necessarily as a substitute to traditional legal analysis''' (2, 5). The openness towards a contextualization of Law acknowledges the fact that Law is influenced by thoughts and discourses from other disciplines such as sociology, economics, political science, history and psychology (4, 7). In addition, \"understanding of the modern patent law, environmental law, and information technology law presupposes adequate study of biotechnology, ecology, and information technology.\" (4, p.18). New approaches to and combinations of methods have emerged and continue to develop. The common 'doctrinal' approach to legal research, i.e. collecting, reading and interpreting (legal) texts, is increasingly scrutinized as the primary methodological approach (2, 3). According to critics, legal publications and teaching in academic institutions do not sufficiently reflect upon the methodological approaches of traditional legal research (2, 5). This critique stems from a comparison to other established disciplines in the (Social) Sciences which have been using a diverse set of defined methodological approaches for decades (2). It has also emerged in view of fundamental changes in academia in the last decades, notably in terms of methodological advancements and trends of internationalisation (2).",
        "summary": "From the contextualized approach to Law, research emerged on the relationship between Law and Society and aspects such as gender, social class, ethnicity and religion. Socio-legal research can be seen as an alternative or supplement to traditional legal analysis. This approach acknowledges that Law is influenced by other disciplines such as sociology, economics, political science, history and psychology. Understanding modern patent law, environmental law, and information technology law requires study of biotechnology, ecology, and information technology. New approaches and methods continue to develop, and the traditional 'doctrinal' approach to legal research is increasingly scrutinized. Critics argue that legal publications and teaching do not sufficiently reflect upon the methodological approaches of traditional legal research, a critique stemming from comparisons to other established disciplines in the Social Sciences and changes in academia.",
        "question": "What are the criticisms of the traditional 'doctrinal' approach to legal research?",
        "ground_truths": "The traditional 'doctrinal' approach to legal research is criticized for its lack of reflection upon its methodological approaches. Critics argue that legal publications and teaching do not sufficiently reflect upon these approaches, a critique that stems from comparisons to other established disciplines in the Social Sciences that have been using a diverse set of defined methodological approaches for decades. This criticism has also emerged in view of fundamental changes in academia in recent decades, notably in terms of methodological advancements and trends of internationalisation."
    },
    {
        "contexts": "Lego can be a tool in a Design Thinking approach to prototype research ideas, haptically test setups and explore team settings and interactions. The strength of Lego Serious Play lies in group settings, where Lego can serve as a boundary object to allow team members to interact, allowing for a reflexive interaction. Lego is playful, many remember it from childhood, and it can trigger people to challenge their underlying assumptions. Through the haptic interaction people are enabled to interact more than if they would be just talking. Originating in management, Lego Serious Play can be used in any sort of research setting, and is suitable for disciplinary design settings yet particularly helpful in transdisciplinary research modes. The structure of Lego can help structure the research. Lego can be thus be used as a catalyst for a team to develop research together.",
        "summary": "Lego Serious Play is a tool used in Design Thinking to prototype research ideas, test setups and explore team interactions. It serves as a boundary object in group settings, enabling reflexive interaction and challenging underlying assumptions. It originated in management but can be used in any research setting, particularly in transdisciplinary research modes. The structure of Lego can help structure the research, acting as a catalyst for team development.",
        "question": "How can Lego Serious Play be used in research settings?",
        "ground_truths": "Lego Serious Play can be used in research settings to prototype ideas, test setups and explore team interactions. It serves as a boundary object in group settings, enabling reflexive interaction and challenging underlying assumptions. It is particularly helpful in transdisciplinary research modes and can help structure the research."
    },
    {
        "contexts": "Lego Serious Play can be seen more as a way and not a goal. The goal can be bluntly put to 1) facilitate people to brainstorm in a smaller (3-15) group setting about research 2) integrate knowledge in a room in a non-hierarchical and enabling space 3) create research designs, workshop settings or the architecture of larger research projects. The goal to this end is not so much the final Lego model, but more the way that leads to it.",
        "summary": "Lego Serious Play is more of a method than a goal. It is used to facilitate brainstorming in small groups, integrate knowledge in a non-hierarchical space, and create research designs or workshop settings. The final Lego model is not the main goal, but rather the process leading to it.",
        "question": "What is the main purpose of Lego Serious Play?",
        "ground_truths": "The main purpose of Lego Serious Play is to facilitate brainstorming in small groups, integrate knowledge in a non-hierarchical space, and create research designs or workshop settings. The final Lego model is not the main goal, but rather the process leading to it."
    },
    {
        "contexts": "First of all, you need some Lego. Personally, I think the Lego Serious Play sets are not really necessary. Instead, any collection of Lego will do, as long as you do not have a Lego expert who will be missing this specific piece from his childhood. Find a large table or a clean ground space (blanket?) where everybody can sit comfortably for a long time. Put the Lego in the center, and make it clear, that everybody can and should be equally engaged. Ideally, there is a moderator who only intervenes in times of crisis or may engage people by asking specific questions. The process is still often led by hierarchies, yet this should ideally be avoided.",
        "summary": "To start with Lego Serious Play, you need some Lego, but not necessarily the specific sets. Any collection will do. Find a large table or clean ground space where everyone can sit comfortably. The Lego should be placed in the center, with everyone equally engaged. Ideally, a moderator should be present to intervene in times of crisis or to engage people with specific questions. Hierarchies should be avoided.",
        "question": "How should one set up a Lego Serious Play session?",
        "ground_truths": "To set up a Lego Serious Play session, you need some Lego, a large table or clean ground space where everyone can sit comfortably. The Lego should be placed in the center, with everyone equally engaged. Ideally, a moderator should be present to intervene in times of crisis or to engage people with specific questions. Hierarchies should be avoided."
    },
    {
        "contexts": "'Theory' is not so much the first level as it is the general idea of the following terms. Theory is defined as a 'systematic ideational structure of broad scope, conceived by the human imagination, that encompasses a family of empirical (experiential) laws regarding regularities existing in objects and events, both observed and posited. A scientific theory is a structure suggested by these laws and is devised to explain them in a scientifically rational manner.' (Britannica). Wikipedia understands it as 'an explanation of an aspect of the natural world that can be repeatedly tested and verified in accordance with the scientific method, using accepted protocols of observation, measurement, and evaluation of results.' Theories can be tested either through experiments or through principles of abductive reasoning. An example of a theory would be the theory of evolution, which has been tested empirically and abductively so often that we came to call it accepted knowledge on how this part of the world works. In theory, however, this theory could be rejected, and a recent epigenetic renaissance in Lamarkism proves that such theories are continuously explored.",
        "summary": "A 'Theory' is a systematic ideational structure that encompasses a family of empirical laws regarding regularities in objects and events. It is an explanation of an aspect of the natural world that can be repeatedly tested and verified using the scientific method. An example is the theory of evolution, which has been tested so often that it is accepted knowledge.",
        "question": "What is a theory and how is it tested?",
        "ground_truths": "A 'Theory' is a systematic ideational structure that encompasses a family of empirical laws regarding regularities in objects and events. It is tested either through experiments or through principles of abductive reasoning."
    },
    {
        "contexts": "Most commonly, 'Concepts' are understood as mental representations of the world. Wikipedia defines them as 'abstract ideas or general notions that occur in the mind, in speech, or in thought. They are understood to be the fundamental building blocks of thoughts and beliefs'. According to Merriam-Webster, a concept is 'an abstract or generic idea generalized from particular instances'. Concepts are the highest unit of theoretical thought. Our mind develops concepts to make sense of more specific elements of the world, such as the concept of 'nations' that helps subsume different groups of people and enables us to speak about these on a more generalized level. Other examples are Reason, Logic or Justice, which are abstract ideas of how a variety of elements from the world may be aligned, and related to each other.",
        "summary": "'Concepts' are mental representations of the world, abstract ideas or general notions that occur in the mind. They are the fundamental building blocks of thoughts and beliefs, and are the highest unit of theoretical thought. Examples include 'nations', Reason, Logic, and Justice.",
        "question": "What are concepts and how do they function in theoretical thought?",
        "ground_truths": "'Concepts' are mental representations of the world, abstract ideas or general notions that occur in the mind. They are the fundamental building blocks of thoughts and beliefs, and are the highest unit of theoretical thought."
    },
    {
        "contexts": "In scientific discourse, the term 'Paradigm' was coined by Kuhn in the 1960s. Kuhn described scientific paradigms as 'universally recognized scientific achievements that, for a time, provide model problems and solutions for a community of practitioners'. A paradigm includes the concepts and theories, laws and thought patterns, generalizations and assumptions, and basically guidelines and standards on how to conduct research in the respective scientific school or discipline (Merriam-Webster, Wikipedia). Kuhn called the successive transition from one paradigm to the next the maturation process of science. You may have heard the term 'Paradigmenwechsel' in German public discourse - it means that everthing that was assumed about a field of knowledge is questioned and re-developed.",
        "summary": "The term 'Paradigm' was coined by Kuhn in the 1960s to describe universally recognized scientific achievements that provide model problems and solutions for a community of practitioners. A paradigm includes concepts, theories, laws, thought patterns, generalizations, assumptions, and guidelines on how to conduct research. The transition from one paradigm to the next is called the maturation process of science.",
        "question": "What is a paradigm and how does it influence scientific research?",
        "ground_truths": "A 'Paradigm' is a universally recognized scientific achievement that provides model problems and solutions for a community of practitioners. It includes concepts, theories, laws, thought patterns, generalizations, assumptions, and guidelines on how to conduct research."
    },
    {
        "contexts": "A 'Framework' is a real or conceptual basic structure that supports or guides practical applications (Merriam-Webster, whatis.com). Frameworks pave the road to connect paradigms to the real world. The [https://www.un.org/sustainabledevelopment/sustainable-development-goals/ 17 Sustainable Development Goals] are an example, or the [http://www.dpi.inpe.br/gilberto/cursos/cst-317/papers/ostrom_sustainability.pdf Ostrom framework]. Frameworks imply, explore or test causality between their single elements or have the explicit focus to make paradigms applicable in planning and management. Hence frameworks link science to the real world.",
        "summary": "A 'Framework' is a basic structure that supports or guides practical applications, connecting paradigms to the real world. Examples include the 17 Sustainable Development Goals and the Ostrom framework. Frameworks explore or test causality between their elements and make paradigms applicable in planning and management.",
        "question": "What is a framework and how does it connect paradigms to the real world?",
        "ground_truths": "A 'Framework' is a basic structure that supports or guides practical applications, connecting paradigms to the real world. It explores or tests causality between their elements and makes paradigms applicable in planning and management."
    },
    {
        "contexts": "The '4th level' are Cases. Cases are simply the bread and butter of empirical work, allowing us to apply frameworks. On the '5th level' you have actors. Actors are diverse, may be together part of institutions, and thus typically have identities, knowledge - ideally - and -finally - they act. No actor is an island. Actors are connected and interacting through actor-networks. Lastly, actors are non-static, they are changeable in space and time.",
        "summary": "The '4th level' of theory are Cases, which are essential for empirical work and the application of frameworks. The '5th level' are Actors, who are diverse, part of institutions, have identities and knowledge, and act. Actors are connected through networks and are changeable in space and time.",
        "question": "What are the 4th and 5th levels of theory and how do they function?",
        "ground_truths": "The '4th level' of theory are Cases, which are essential for empirical work and the application of frameworks. The '5th level' are Actors, who are diverse, part of institutions, have identities and knowledge, and act. Actors are connected through networks and are changeable in space and time."
    },
    {
        "contexts": "Life Cycle Analysis, or Life Cycle Assessment (LCA), dates back to the late 1960s/early 1970s when the first (partial) LCAs were conducted: \"The scope of these studies was initially limited to energy analyses but was later broadened to encompass resource requirements, emission loadings and generated waste. LCA studies in this period mainly focused on packaging alternatives.\" (Guin\u00e9e 2011, p.9; 5) Here, a study done for the Coca-Cola company set the foundation for subsequent life cycle analyses (4). Yet, the method was not unified at all, but existed as a broad variety of theoretical frameworks and methodological approaches. This led to a lack of comparability between the results and hindered the method from becoming an accepted analytical tool until the 1980s (1).",
        "summary": "Life Cycle Analysis (LCA) began in the late 1960s and early 1970s, initially focusing on energy analyses before expanding to include resource requirements, emissions, and waste. The method was not standardized, leading to a lack of comparability between results and preventing it from becoming an accepted analytical tool until the 1980s.",
        "question": "When did Life Cycle Analysis (LCA) first begin and what were its initial focuses?",
        "ground_truths": "Life Cycle Analysis (LCA) began in the late 1960s and early 1970s, initially focusing on energy analyses before expanding to include resource requirements, emissions, and waste."
    },
    {
        "contexts": "During the 1980s, the Green Movement in Europe strengthened public interest in the method (4). In the 1990s, its image and usefulness improved through the engagement of SETAC (Society of Environmental Toxicology and Chemistry) and ISO (International Organization for Standardization) in the harmonization and standardization of LCAs. At this time, the method also first appeared in scientific publications (1).",
        "summary": "In the 1980s, the Green Movement in Europe increased public interest in LCA. The method's image and usefulness improved in the 1990s through the involvement of SETAC and ISO in standardizing LCAs, and it also began appearing in scientific publications.",
        "question": "What events in the 1980s and 1990s increased the popularity and usefulness of Life Cycle Analysis?",
        "ground_truths": "The Green Movement in Europe in the 1980s increased public interest in LCA. In the 1990s, the involvement of SETAC and ISO in standardizing LCAs improved its image and usefulness, and it also began appearing in scientific publications."
    },
    {
        "contexts": "Since the year 2000, LCAs have been increasingly adopted by policy actors (such as the EU, US and UN). Alongside a publicly increasing role of life-cycle-thinking, more diverse forms of the LCA have been been developed that differ in terms of their system boundaries, allocation methods and the considered impacts (1).",
        "summary": "Since 2000, LCAs have been increasingly used by policy actors like the EU, US, and UN. As life-cycle-thinking has become more prominent, more diverse forms of LCA have been developed, differing in system boundaries, allocation methods, and considered impacts.",
        "question": "How has the use and development of Life Cycle Analysis evolved since the year 2000?",
        "ground_truths": "Since 2000, LCAs have been increasingly used by policy actors like the EU, US, and UN. As life-cycle-thinking has become more prominent, more diverse forms of LCA have been developed, differing in system boundaries, allocation methods, and considered impacts."
    },
    {
        "contexts": "More recently, the Life Cycle Sustainability Assessment (LCSA) has been suggested to better connect LCAs with the sustainability discourse. The LCSA broadens the scope of the LCA both in terms of criteria involved (from purely environmental to economic, environmental and social impacts) and regional scope (from the product to the industry) while deepening the perspective on the elements involved in the Life Cycle (1, 2, 3). While this approach is still in development, it may be seen as one/the future of the Life Cycle approach (see Outlook).",
        "summary": "Recently, the Life Cycle Sustainability Assessment (LCSA) has been proposed to better integrate LCAs with sustainability discussions. LCSA expands the scope of LCA in terms of criteria and regional scope, while providing a deeper perspective on the elements involved in the Life Cycle. This approach, still in development, could be the future of the Life Cycle approach.",
        "question": "What is the Life Cycle Sustainability Assessment (LCSA) and how does it expand on the traditional Life Cycle Analysis?",
        "ground_truths": "The Life Cycle Sustainability Assessment (LCSA) is a recent proposal to better integrate LCAs with sustainability discussions. LCSA expands the scope of LCA in terms of criteria and regional scope, while providing a deeper perspective on the elements involved in the Life Cycle."
    },
    {
        "contexts": "The name of the scale comes from the American psychologist Rensis Likert. In 1932, Likert developed a 7-point agreement scale out of interest in measuring people\u2019s opinions and attitudes. His initial scale is very similar to the ones used today.",
        "summary": "American psychologist Rensis Likert developed a 7-point agreement scale in 1932 to measure people's opinions and attitudes. This scale is still widely used today.",
        "question": "Who developed the Likert Scale and when?",
        "ground_truths": "The Likert Scale was developed by American psychologist Rensis Likert in 1932."
    },
    {
        "contexts": "The most common Likert Scale asks for attitude on 5 different levels: Strongly Agree, Agree, Neutral/Undecided, Disagree, and Strongly Disagree. Other options include 3 or 7 answer options. Odd numbers are commonly used to include a neutral or undecided option in the middle. If no such neutral option is added, we call the scale a forced-choice method, because respondents are forced to decide if they are leaning more against the agree or disagree side of the scale. Sometimes, opt-out responses are also used, such as \u201cI don\u2019t know\u201d or \u201cNot applicable\u201d. Those are then placed outside the scale.",
        "summary": "The Likert Scale commonly uses 5 levels of attitude: Strongly Agree, Agree, Neutral/Undecided, Disagree, and Strongly Disagree. Other scales may use 3 or 7 options. If no neutral option is provided, it's called a forced-choice method. Opt-out responses like \u201cI don\u2019t know\u201d or \u201cNot applicable\u201d are sometimes used.",
        "question": "What are the common levels of attitude in a Likert Scale?",
        "ground_truths": "The common levels of attitude in a Likert Scale are Strongly Agree, Agree, Neutral/Undecided, Disagree, and Strongly Disagree."
    },
    {
        "contexts": "Even though one may find that analyzing Likert Scales looks fairly easy and straight forward, there is a huge debate on how to analyze these scales correctly. The biggest point of conflict in the discussion is whether the scale can be considered ordinal or interval. While Likert Scales technically are ordinal data, because there clearly is a grading: \"Strongly Agree\" is graded higher than \"Agree\", strictly speaking, the assumption of equal distance between categories is not valid. That is, the distance between \u201cAgree\u201d and \u201cNeutral\u201d theoretically is not the same as the distance between \u201cNeutral\u201d and \u201cNot Agree\u201d. That is why, we would strictly not consider Likert Scales as interval scaled, where each distance should be equal.",
        "summary": "Analyzing Likert Scales is a subject of debate, particularly whether they are ordinal or interval. While they are technically ordinal, the assumption of equal distance between categories is not valid. For instance, the distance between \u201cAgree\u201d and \u201cNeutral\u201d is not the same as between \u201cNeutral\u201d and \u201cNot Agree\u201d.",
        "question": "Why is there a debate on whether Likert Scales are ordinal or interval?",
        "ground_truths": "The debate exists because while Likert Scales are technically ordinal, the assumption of equal distance between categories, a characteristic of interval scales, is not valid."
    },
    {
        "contexts": "Statements in a survey are often differently framed. While some are framed positively, for instance, the participant may be asked for its attitude to the statement \u201cI am satisfied with what I learned this semester\u201d, others are framed negatively, like \u201cI am not satisfied with the course offer this semester\u201d. The different framing is used to check for respondents who do not read the question and just click the same answer every time. In a statistical analysis, it may be useful to recode the responses, so that each response is going in the same logical direction when 0 is \"Strongly Disagree\", 1 is \"Disagree\", and so on.",
        "summary": "Survey statements can be framed positively or negatively to check for respondents who do not read the question. In statistical analysis, it may be useful to recode the responses so that each response is going in the same logical direction.",
        "question": "Why are survey statements framed both positively and negatively?",
        "ground_truths": "Survey statements are framed both positively and negatively to check for respondents who do not read the question and just click the same answer every time."
    },
    {
        "contexts": "Statistics provide a specific viewpoint with which you can look at the world, and it can help you understand parts of what you're seeing. If more people understood statistics and the insights we can gain through it, these people would become more empowered to make their decisions based on their own analysis and insight. However, we do not only need to recognise the benefits that can be gained through statistics, but we also need to be aware of the limitations of statistics. After all, it is a view through numbers and models, and can therefore only offer parts of the complete picture. In addition, we have to accept that statistics are also riddled by diverse biases, which we also have to take into our focus and conscious recognition as much as possible.",
        "summary": "Statistics offer a unique perspective on the world, aiding in understanding and decision-making. However, it's important to recognize its limitations as it only provides a partial view through numbers and models. Additionally, statistics are often subject to various biases.",
        "question": "Why is it important to understand the limitations of statistics?",
        "ground_truths": "Understanding the limitations of statistics is important because it only provides a partial view of the world through numbers and models and is often subject to various biases."
    },
    {
        "contexts": "The question of the limitations of statistics in the current societal debates - and the possibilities and opportunities which statistics may offer for the great transformation - are in the focus of the last part of this entry. Why are many societal debates so disconnected from available results derived through statistics? And how can we improve the literacy regarding statistics within society? Lastly, we want to examine with a critical eye how the limitations of statistics may contribute to some of the problems we face in society today, and how we may ultimately overcome these.",
        "summary": "The limitations of statistics in societal debates and their potential for transformation are discussed. The disconnection between societal debates and statistical results, the need for improved statistical literacy, and the role of statistical limitations in societal problems are examined.",
        "question": "How can the limitations of statistics contribute to societal problems?",
        "ground_truths": "The limitations of statistics can contribute to societal problems by causing a disconnection between societal debates and available statistical results, and by influencing the level of statistical literacy within society."
    },
    {
        "contexts": "First, and many would argue most importantly, are the questions of plausibility and validity. Are the questions we are testing plausible, and are the predictors we use to access these models valid? We often test constructs: for instance, GDP is commonly used as a proxy for many other things. While this had some surely some merits in the past, it also draws an increasing amount of criticism. Another example would be the Intelligence Quotient, which may tell you something about people, but more often than not is misleading and judgmental.",
        "summary": "The plausibility and validity of the questions and predictors used in statistical models are crucial. Constructs like GDP and IQ are often tested, but these proxies have limitations and can be misleading.",
        "question": "Why is the validity of predictors in statistical models important?",
        "ground_truths": "The validity of predictors in statistical models is important because they can often be misleading and judgmental, as seen in the use of GDP and IQ as proxies."
    },
    {
        "contexts": "Both the Living Lab and the Real-World Laboratory approach are rather new research areas, emerging from the beginning of the 21st century. The Living Lab (LL) started outside of academia, revolving around testing new technologies in home-like constructed environments in order to promote entrepreneurial innovation (1). In 2006, the European Network of Living Labs (EnoLL) was founded by the Finnish government, focusing on information and communication technologies and institutionalizing the concept (6, 9). The concept is therefore common predominantly in Europe but has also expanded to other geographical contexts during the last years (see Further Information). Living Labs have also emerged as a transdisciplinary scientific method, its applications going beyond product and service development for the sake of business: Living Labs have also been used in the fields of, among others, eco-design (4), IT (1), urban development (2), energy or mobility for the purpose of sustainable development.",
        "summary": "Living Labs and Real-World Laboratories are new research areas that emerged in the 21st century. Living Labs started outside academia, testing new technologies in home-like environments to promote innovation. In 2006, the Finnish government founded the European Network of Living Labs, focusing on ICT and institutionalizing the concept. The concept is common in Europe and has expanded globally. Living Labs have become a transdisciplinary scientific method, used in fields like eco-design, IT, urban development, energy, and mobility for sustainable development.",
        "question": "What is the origin and application of the Living Labs concept?",
        "ground_truths": "The Living Labs concept originated outside academia and was institutionalized by the Finnish government in 2006 through the European Network of Living Labs. It is used to test new technologies in home-like environments to promote innovation. The concept is common in Europe and has expanded globally. It has become a transdisciplinary scientific method, used in fields like eco-design, IT, urban development, energy, and mobility for sustainable development."
    },
    {
        "contexts": "The concept of Real-World Laboratories ('Reallabore') emerged around the same time, but was a scientific field from the beginning. It is most prevalent in German-speaking research communities that focus on sustainability and transdisciplinary transformative research (10). A broad set of RWLs have been set up in and supported by the German state of Baden-W\u00fcrttemberg since 2015 (14).",
        "summary": "The concept of Real-World Laboratories, also known as 'Reallabore', emerged around the same time as Living Labs, but was a scientific field from the start. It is most common in German-speaking research communities focusing on sustainability and transdisciplinary transformative research. Many RWLs have been established and supported by the German state of Baden-W\u00fcrttemberg since 2015.",
        "question": "What is the origin and focus of Real-World Laboratories?",
        "ground_truths": "Real-World Laboratories, also known as 'Reallabore', emerged around the same time as Living Labs and was a scientific field from the start. It is most common in German-speaking research communities focusing on sustainability and transdisciplinary transformative research. Many RWLs have been established and supported by the German state of Baden-W\u00fcrttemberg since 2015."
    },
    {
        "contexts": "Living Labs and Real-World Laboratories are research, innovation and learning environments. They share a variety of traits, but differ in crucial elements. Their characteristics will be described below. The Living Lab has often been defined as both a methodology and the environment in which this methodology is being applied (1, 3, 6, 9). Generally, LLs revolve around the testing and implementation of new, marketable and standardized products and services that provide social or technological innovation (10). They experiment in settings with limited participation with these innovations and attempt to create generalizable insights. They do not necessarily contribute to transformative change.",
        "summary": "Living Labs and Real-World Laboratories are environments for research, innovation, and learning. They share many traits but also have key differences. Living Labs are both a methodology and the environment where this methodology is applied. They focus on testing and implementing new, marketable, and standardized products and services that provide social or technological innovation. They experiment in settings with limited participation and aim to create generalizable insights, but do not necessarily contribute to transformative change.",
        "question": "What are the characteristics and focus of Living Labs?",
        "ground_truths": "Living Labs are both a methodology and the environment where this methodology is applied. They focus on testing and implementing new, marketable, and standardized products and services that provide social or technological innovation. They experiment in settings with limited participation and aim to create generalizable insights, but do not necessarily contribute to transformative change."
    },
    {
        "contexts": "Real-World Laboratories, too, exhibit a broad and not clearly defined research format. They are generally understood as research environments that complement transdisciplinary and transformation-oriented sustainability research by offering a real-world environment for experimentation and reflexive learning. RwLs attempt to close the gap between research and practice by combining scientific research with contributions to societal change in order to solve societally relevant problems (12, 13). This may help solve scientific problems, but also practical issues and fulfill educational purposes (13). RwLs are both interdisciplinary and transdisciplinary in that they strongly favor the involvement of various stakeholders from different scientific and non-scientific backgrounds that are relevant to the problem at hand. RwLs and Living Labs share the characteristics of stakeholder involvement and real-world experimentation, but focus more on the process of researching, learning and testing of new structures and transformative processes rather than on the implementation of a specific innovation.",
        "summary": "Real-World Laboratories have a broad and not clearly defined research format. They are research environments that complement transdisciplinary and transformation-oriented sustainability research by offering a real-world environment for experimentation and learning. They aim to bridge the gap between research and practice by combining scientific research with societal change to solve relevant problems. This can help solve scientific and practical issues and fulfill educational purposes. RwLs are both interdisciplinary and transdisciplinary, involving various stakeholders from different backgrounds. They share characteristics with Living Labs, but focus more on researching, learning, and testing new structures and transformative processes rather than implementing a specific innovation.",
        "question": "What are the characteristics and focus of Real-World Laboratories?",
        "ground_truths": "Real-World Laboratories have a broad and not clearly defined research format. They are research environments that complement transdisciplinary and transformation-oriented sustainability research by offering a real-world environment for experimentation and learning. They aim to bridge the gap between research and practice by combining scientific research with societal change to solve relevant problems. This can help solve scientific and practical issues and fulfill educational purposes. RwLs are both interdisciplinary and transdisciplinary, involving various stakeholders from different backgrounds. They focus more on researching, learning, and testing new structures and transformative processes rather than implementing a specific innovation."
    },
    {
        "contexts": "Core characteristics of Real-World Laboratories are that they contribute to transformation by experimenting with potential solutions and support transitions by providing evidence for the robustness of solutions. They deploy transdisciplinarity as the core research mode in order to integrate scientific and societal knowledge, related to a real-world problem. They establish a culture of sustainability around the laboratory, stabilize the cooperation between the actors and empower the involved practitioners. They therefore have a strong normative and ethical component and pursue to contribute to the common good. They include experiments as a core research method which they provide concrete settings for, and which stakeholders are actively involved in (co-design and co-production).",
        "summary": "Real-World Laboratories (RWLs) are characterized by their contribution to transformation through experimentation and evidence-based solutions. They use transdisciplinarity to integrate scientific and societal knowledge, establish a culture of sustainability, and have a strong normative and ethical component. They involve stakeholders in the experimental process.",
        "question": "What are the core characteristics of Real-World Laboratories?",
        "ground_truths": "Real-World Laboratories contribute to transformation by experimenting with potential solutions, support transitions by providing evidence for the robustness of solutions, deploy transdisciplinarity as the core research mode to integrate scientific and societal knowledge, establish a culture of sustainability, have a strong normative and ethical component, and involve stakeholders in the experimental process."
    },
    {
        "contexts": "Apart from LL and RWL, there are further related approaches which should be mentioned: (Urban) Transition Labs are based on transition management and focus on developing new ideas, practices and structures to support transition processes. Guiding principles and future visions for the transition process are developed and translated into experiments through backcasting. They focus neither on transdisciplinary research processes nor on specific socio-technical innovations, but revolve around the conceptualization of broader socio-technical change, including social learning and empowerment processes.",
        "summary": "In addition to Living Labs and Real-World Laboratories, there are other related approaches such as Urban Transition Labs. These labs focus on developing new ideas and structures to support transition processes, guided by principles and future visions. They do not focus on transdisciplinary research or specific socio-technical innovations, but on broader socio-technical change.",
        "question": "What is the focus of Urban Transition Labs?",
        "ground_truths": "Urban Transition Labs focus on developing new ideas, practices and structures to support transition processes. They revolve around the conceptualization of broader socio-technical change, including social learning and empowerment processes."
    },
    {
        "contexts": "Living Labs. The central element of the Living Lab - the co-involvement of 'users' at all stages of the development process - constitutes a distinctiveness of the method and provides specific advantages to the process. Instead of designing the product around the alleged needs of the users, the ideas and knowledge contributed by the users enable the developer to more properly design their product / service according to the user's needs and desires. The contextual knowledge gained in the real-world environment further facilitates the design process of the respective product or service. This way, the risk of failure is decreased while the result better fits the user and supports sustainability, e.g. by reducing the rebound effect in eco-design.",
        "summary": "The key element of Living Labs is the involvement of users at all stages of development, which provides distinct advantages. User ideas and knowledge enable developers to design products or services that better meet user needs. This approach reduces the risk of failure and supports sustainability by better fitting the user and reducing the rebound effect in eco-design.",
        "question": "What is the central element of the Living Lab approach and how does it benefit the development process?",
        "ground_truths": "The central element of the Living Lab approach is the co-involvement of users at all stages of the development process. This enables developers to design products or services that better meet user needs, reduces the risk of failure, and supports sustainability by better fitting the user and reducing the rebound effect in eco-design."
    },
    {
        "contexts": "As with all transdisciplinary research methods, Living Labs and Real-World Laboratories have a highly normative component that relates to collaborating with real-world actors (see the [[Transdisciplinarity|TD entry]]). For the Living Lab, this is the case since it revolves around creating products and services that provide maximum suitability to the needs, desires and ideas of individuals. Living Labs are situated in the middle of the user involvement spectrum ranging from the user as the main creator to the user as a passive subject whose insights are introduced into the innovation process (3, 6). For RwLs, the transformative purpose raises questions of which kind of future scenario is desired by whom: \"RwLs [Real-World Laboratories] become immersed in political and normative issues that science traditionally attempts to avoid. Correspondingly researchers take on new roles in addition to what is traditionally seen as research (i.e., producing knowledge), including acting as facilitators of the process, knowledge brokers, and change agents\" (Sch\u00e4pke et al. p.87). The selection of the participating individuals and stakeholders is a normative endeavour. An important notion in this regard is the question of an Open vs Closed format of user involvement - i.e. is everyone allowed to participate or only selected users and stakeholders? A closed format allows for more focused and in-depth feedback but requires the capacity to select individuals and limit access to the research environment. An open format, on the other hand, is simpler to implement and provides more diversity, but requires the capacity to filter results and manage the greater number of users (6). Both approaches serve as empowering environments for the individuals involved (1, 8, 10, 13). They can have educational impacts and enable citizens to participate in processes that influence their daily lives.",
        "summary": "Living Labs and Real-World Laboratories are transdisciplinary research methods with a normative component, involving real-world actors. Living Labs focus on creating products and services that meet individual needs, while Real-World Laboratories explore future scenarios and involve researchers in roles beyond traditional research. The selection of participants is a normative task, with open and closed formats offering different benefits and challenges. Both approaches empower individuals and can have educational impacts.",
        "question": "What are the roles of researchers in Real-World Laboratories?",
        "ground_truths": "In Real-World Laboratories, researchers take on new roles in addition to what is traditionally seen as research, including acting as facilitators of the process, knowledge brokers, and change agents."
    },
    {
        "contexts": "Being a rather young methodological approach, the Living Lab still lacks clear definitions and supporting concepts as well as robust and homogenous methodologies, methods and tools (1, 4, 9). The majority of theoretical publications include thoughts on how to locate the method within participatory design approaches, or descriptions of individual projects. Few peer-reviewed studies exist that provide an instructional, clear illustration of the method's elements (6, 9). This impedes its more wide-spread use of the approach but may also be seen as flexibility, displayed by the various applications and adaptations of the approach in an increasing amount of diverse contexts (5, see Further Information). Real-World Laboratories are yet to be established in the realm of the other comparable approaches. They are not \"(...) unique or completely new in what they pursue and in the ways they proceed. They are part of a larger development: the emergence of a family of transdisciplinary and experimental approaches to transformative research. (...) Current trends in [[Glossary|funding]] programs and research collaborations provide space to further explore the potential of experimental approaches in transformative research. Long-term evaluation and comparisons will show which approaches or combinations are most promising, in terms of real-world sustainability transformation and acceleration in given contexts. (...) [T]he diverse emerging approaches may complement each other, rather than compete for being the \u201cbest\u201d approach.\" RwLs \"(...) have a lot to learn from the other approaches. (...) Their open approach may provide a suitable [[Glossary|framework]] for combining different components of experimental transformative research. Thus, RwLs might contribute to building bridges between different styles of transformative research, the design and implementation of experiments, as well as the evaluation fo processes of learning and reflexivity.\" (Sch\u00e4pke et al. 2018, p.95)",
        "summary": "The Living Lab approach is still young and lacks clear definitions and robust methodologies, which hinders its widespread use but also allows for flexibility. Real-World Laboratories are part of a larger development of transdisciplinary and experimental approaches to transformative research. They are not unique but have potential to contribute to this field, with current trends in funding and research collaborations allowing for further exploration. Long-term evaluation will reveal the most promising approaches.",
        "question": "What are the challenges and potential benefits of the Living Lab approach?",
        "ground_truths": "The Living Lab approach, being a young methodological approach, lacks clear definitions, supporting concepts, and robust methodologies. This impedes its more widespread use. However, this can also be seen as a source of flexibility, as it allows for various applications and adaptations in diverse contexts."
    },
    {
        "contexts": "Traditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you need to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and/or the nature of the outputs is well understood. However, in situations where the inputs can be noisy or the outputs are expected to be different in each case, you cannot hand-craft the rules that account for every type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: Machine Learning.",
        "summary": "In traditional computer science and mathematics, a rule or function is used to process inputs and generate outputs. This works well when inputs and outputs are well understood. However, when inputs are noisy or outputs vary, it's not feasible to create rules for every possible scenario. In these cases, Machine Learning can be used.",
        "question": "What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?",
        "ground_truths": "Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions."
    },
    {
        "contexts": "The core idea behind Machine Learning is that instead of being required to hand-craft all the rules that take inputs and provide outputs in a fairly accurate manner, you can train the machine to learn the rules based on the inputs and outputs that you provide. The trained models have their foundations in the fields of mathematics and computer science. As computers of various types have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available.",
        "summary": "Machine Learning involves training machines to learn rules based on provided inputs and outputs, rather than manually crafting these rules. This approach is grounded in mathematics and computer science. With the ubiquity of computers and the availability of data, Machine Learning models can be trained more easily.",
        "question": "How does the process of Machine Learning differ from traditional rule-based systems?",
        "ground_truths": "Machine Learning involves training machines to learn rules based on provided inputs and outputs, rather than manually crafting these rules."
    },
    {
        "contexts": "The term Machine Learning does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.",
        "summary": "Machine Learning is not a single method, but a term encompassing three main groups of methods: supervised learning, unsupervised learning, and reinforcement learning.",
        "question": "What are the three main groups of methods that fall under the term Machine Learning?",
        "ground_truths": "The three main groups of methods that fall under the term Machine Learning are supervised learning, unsupervised learning, and reinforcement learning."
    },
    {
        "contexts": "This family of Machine Learning methods rely on input-output pairs to learn rules. This means that the data that you have to provide can be represented as (X, y) pairs where X = (x_1, x_2, ..., x_n) is the input data (in the form of vectors or matrices) and y = (y_1, y_2, ..., y_n) is the output (in the form of vectors with numbers or categories that correspond to each input), also called true label.",
        "summary": "Supervised learning, a type of Machine Learning, uses input-output pairs to learn rules. The data provided is represented as (X, y) pairs, where X is the input data and y is the output, also known as the true label.",
        "question": "What kind of data is used in supervised learning methods in Machine Learning?",
        "ground_truths": "Supervised learning methods in Machine Learning use input-output pairs, represented as (X, y) pairs, where X is the input data and y is the output, also known as the true label."
    },
    {
        "contexts": "In ''regression learning'', the objective is to predict a particular value when certain input data is given to the algorithm. An example of a regression learning task is predicting the price of a house when certain features of the house (eg. PLZ/ZIP code, no. of bedrooms, no. of bathrooms, garage size, energy raging, etc.) are given as an input to the Machine Learning algorithm that has been trained for this specific task. In ''classification learning'', the objective is to predict which ''class'' (or ''category'') a given observation/sample falls under given the characteristics of the observation. There are 2 specific classification learning tasks: ''binary classification'' and ''multiclass classification''. As their names suggest, in binary classification, a given observation falls under one of the two classes (eg. classifying whether an email is spam or not), and in multiclass classification, a given observation falls under one of many classes (eg. in ''digit recognition'' task, the class for a picture of a given hand-written digit can range from 0 to 9; there are 10 classes).",
        "summary": "Regression learning aims to predict a specific value based on input data, such as predicting a house's price based on its features. Classification learning, on the other hand, aims to categorize an observation into a class or category. There are two types of classification learning: binary, where an observation falls under one of two classes, and multiclass, where an observation can fall under multiple classes.",
        "question": "What are the two types of classification learning tasks in machine learning?",
        "ground_truths": "The two types of classification learning tasks in machine learning are binary classification and multiclass classification."
    },
    {
        "contexts": "Whereas supervised learning is based on pairs of input and output data, unsupervised learning algorithms function based only on the inputs: <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> and are rather used for recognizing patterns than for predicting specific value or class. Some of the methods that are categorized under unsupervised learning are ''[https://en.wikipedia.org/wiki/Principal_component_analysis Principal Component Analysis]'', ''[[Clustering_Methods|Clustering Methods]]'', ''[https://en.wikipedia.org/wiki/Collaborative_filtering Collaborative Filtering]'', ''[https://en.wikipedia.org/wiki/Hidden_Markov_model Hidden Markov Models]'', ''[https://brilliant.org/wiki/gaussian-mixture-model/ Gaussian Mixture Models]'', etc.",
        "summary": "Unsupervised learning algorithms operate solely on inputs, rather than pairs of input and output data, and are used more for pattern recognition than prediction. Methods under unsupervised learning include Principal Component Analysis, Clustering Methods, Collaborative Filtering, Hidden Markov Models, and Gaussian Mixture Models.",
        "question": "What is the main use of unsupervised learning algorithms in machine learning?",
        "ground_truths": "The main use of unsupervised learning algorithms in machine learning is for pattern recognition."
    },
    {
        "contexts": "This Machine Learning approach, also called ''offline learning'', is the most common approach to Machine Learning. In this approach, a Machine Learning model is built from the entire dataset in one go. The main disadvantage of this approach is that depending on the computational infrastructure being used, the data might not fit into the memory and/or the training process can take a long time. Additionally, models based on batch learning need to be retrained on a semi-regular basis with new training examples in order for them to keep performing well.",
        "summary": "Batch learning, also known as offline learning, is a common machine learning approach where a model is built using the entire dataset at once. However, this approach can be computationally intensive and may require frequent retraining with new examples to maintain performance.",
        "question": "What is a major disadvantage of the batch learning approach in machine learning?",
        "ground_truths": "A major disadvantage of the batch learning approach in machine learning is that it can be computationally intensive and may require frequent retraining with new examples to maintain performance."
    },
    {
        "contexts": "Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.) In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models. Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability. Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6]. Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks. Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.",
        "summary": "Machine learning techniques excel in various tasks, often outperforming humans and rule-based models, especially in noisy situations. However, they suffer from a lack of interpretability and explainability, and face a reproducibility crisis. They are often criticized as a 'brute force' approach and require large datasets and significant computational resources, which can be costly.",
        "question": "What are some of the challenges faced by machine learning techniques?",
        "ground_truths": "Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources."
    },
    {
        "contexts": "To standardize pond netting to enable comparisons between different sites or different times at the same site, two points in the water are marked with canes and the net is moved in between the poles for a certain number of times at a constant speed. After netting, the net is emptied into a white tray such as a photographic developing tray. The net should be washed very carefully to remove all the specimens. Some specimens such as nymphs often strongly cling to the net. For tow netting, a net is pulled behind a driving boat to estimate the number of zooplankton in open water. While driving the boat, it is important to keep the opening of the net perpendicular to the water surface to keep the opening of the net at the same size throughout each tow. To measure the distance of towing, it is helpful to add a flowmeter to the net. After towing, the content of the collecting bottle is emptied into a container. To wash off all the zooplankton from the inside of the net, water is splashed from the outside. It can take a considerable amount of time to transfer all the invertebrates from the net into the container. The sample is preserved with 95% ethanol.",
        "summary": "Pond netting is standardized by marking two points in the water and moving the net between them at a constant speed. After netting, the net is emptied and carefully washed to remove all specimens. Tow netting involves pulling a net behind a boat to estimate the number of zooplankton in open water. The net's opening should be kept perpendicular to the water surface and a flowmeter can be used to measure the towing distance. After towing, the net's contents are emptied into a container and the net is washed from the outside.",
        "question": "How is the process of pond netting and tow netting conducted?",
        "ground_truths": "Pond netting is conducted by marking two points in the water and moving the net between them at a constant speed. After netting, the net is emptied and carefully washed to remove all specimens. Tow netting involves pulling a net behind a boat to estimate the number of zooplankton in open water. The net's opening should be kept perpendicular to the water surface and a flowmeter can be used to measure the towing distance. After towing, the net's contents are emptied into a container and the net is washed from the outside."
    },
    {
        "contexts": "Pond nets can be used for quick surveys of the species that occur in a pond or stream. Within only three minutes of netting, you can find 62% of families and 50% of species that you can catch during 18 minutes of netting (Furse et al. 1981). However, this method is hard to standardize as the net-hauls always need to have the same length and the same speed and these are hard to keep up. This means that this method can be used to get an overview on the number of species, but not to compare species richness and abundances between different sites or points in time. Tow nets can be used in deep, open and weed-free waters. They are suitable for large areas of water, especially for watching zooplankton as this is often distributed in patches. They require more work as at least two people are needed, of which one steers the boat and the other one looks after the net. A major problem is to drive the boat with constant speed to standardize the method. Moreover, turbulence caused by the boat might disturb the position of the net. The nets can become clogged by sediment and phytoplankton, so that they lose their catching efficiency.",
        "summary": "Pond nets are used for quick surveys of species in a pond or stream, but are hard to standardize for comparing species richness and abundances between different sites or times. Tow nets are used in deep, open waters and are suitable for large areas, especially for observing zooplankton. However, they require more work, need at least two people, and can become clogged by sediment and phytoplankton, reducing their efficiency.",
        "question": "What are the challenges of using pond nets and tow nets?",
        "ground_truths": "The challenges of using pond nets include difficulty in standardizing the method for comparing species richness and abundances between different sites or times. The challenges of using tow nets include the need for more work, at least two people, and the possibility of the nets becoming clogged by sediment and phytoplankton, reducing their efficiency."
    },
    {
        "contexts": "Kick sampling is a method that is applied to collect invertebrates from fast-flowing streams, especially from riffles, to assess the water quality. Therefore, a net is held on the ground of the river with its opening facing towards the water flow direction. The substrate is disturbed by kicking and the invertebrates are dislodged and collected in the net a short distance downstream (Figure 2). The standardized version of this method consists of sampling all the micro-habitats within a stretch of a stream by kick-sampling for a total of three minutes by using a 0.9-mm-mesh pond net. Invertebrates are stored in a white tray. The sample can be preserved with 95% ethanol. After determination of the taxa, individual scores are calculated that are based on the tolerance towards oxygen depletion of each taxon. These scores are summed up to produce a biotic score for this section of the river. This score gives an estimate on the oxygen content of the river stretch and therefore, of its water quality.",
        "summary": "Kick sampling is used to collect invertebrates from fast-flowing streams to assess water quality. A net is held on the river ground with its opening facing the water flow direction, and the substrate is disturbed by kicking. The invertebrates are collected in the net downstream. The method involves sampling all micro-habitats within a stream stretch for three minutes. The invertebrates are stored in a white tray and the sample can be preserved with ethanol. Individual scores based on each taxon's tolerance to oxygen depletion are calculated and summed up to estimate the river's oxygen content and water quality.",
        "question": "How is kick sampling conducted and how does it help in assessing water quality?",
        "ground_truths": "Kick sampling is conducted by holding a net on the river ground with its opening facing the water flow direction, and the substrate is disturbed by kicking. The invertebrates are collected in the net downstream. The method involves sampling all micro-habitats within a stream stretch for three minutes. The invertebrates are stored in a white tray and the sample can be preserved with ethanol. It helps in assessing water quality by calculating individual scores based on each taxon's tolerance to oxygen depletion. These scores are summed up to estimate the river's oxygen content and water quality."
    },
    {
        "contexts": "A methodological design is one of the three central stages of any scientific endeavor that builds on a methodological approach. While analysis and interpretation are conducted after the empirical data has been gathered, designing a study is what hardwires the type of knowledge that is being gathered, and thus creates a path dependency on the overall scientific output, for better or worse. It is next to impossible to reduce the essence of all the diversity of different approaches to methodological designs into a text, yet there are some common challenges scientists face, and also mistakes that are made frequently to this day. Hence nothing written here can serve as one size fits all solution, yet it may serve as a starting point or safeguarding in order to get most empirical research on track. Let us start with the basics.",
        "summary": "Methodological design is a crucial stage in scientific research, determining the type of knowledge gathered and influencing the overall scientific output. Despite the diversity of approaches, common challenges and mistakes persist. This text can't provide a universal solution, but can guide and safeguard empirical research.",
        "question": "Why is methodological design important in scientific research?",
        "ground_truths": "Methodological design is important in scientific research because it determines the type of knowledge that is being gathered and influences the overall scientific output. It also serves as a guide and safeguard for empirical research."
    },
    {
        "contexts": "Any given research builds on what happened so far. In order to create a tangible and tested design, you need to read the literature within the specific branch of science. Read the highly cited papers, and the ones that match your specific contexts most precisely, and build on a ration from there. While many people start with textbooks, this should only be done by absolute beginners. Methodological textbooks are usually too static and constructed to do justice to the specific contexts of your research, yet they may be a good starting point. The examples in textbooks are however often rather lifeless and hard to connect to what you actually want to do.",
        "summary": "Research builds on existing knowledge. To create a solid design, one must read relevant literature, especially highly cited papers. Textbooks, while useful for beginners, are often too rigid and their examples may not align with the specific contexts of your research.",
        "question": "How can one create a solid research design?",
        "ground_truths": "To create a solid research design, one must build on existing knowledge by reading relevant literature, especially highly cited papers. While textbooks can be a good starting point, they are often too rigid and their examples may not align with the specific contexts of the research."
    },
    {
        "contexts": "We create scientific designs in order to make sure that our gathering of data ultimately works and produces the knowledge we potentially want to analyze and interpret. Hence, we create our design based on past experience of research and researchers. We need to be realistic concerning the potential problems we may face. There are often aspects within research that are prone to errors or unforeseen circumstances and a good research design creates a certain failsafe and guard-rails against unforeseen circumstances. A low rollback in an interview campaign or moulded samples in a soil chemistry lab are a known example where your research faces challenges that a good design should fortify you against.",
        "summary": "Scientific designs ensure effective data gathering and knowledge production. They are based on past research experiences and anticipate potential problems. Good research designs provide safeguards against errors and unforeseen circumstances, such as low response rates in interviews or contaminated samples in labs.",
        "question": "What is the purpose of a scientific design in research?",
        "ground_truths": "The purpose of a scientific design in research is to ensure effective data gathering and knowledge production. It is based on past research experiences and provides safeguards against potential problems, errors, and unforeseen circumstances."
    },
    {
        "contexts": "Statistician Karl Pearson had a strong focus on openly integrating knowledge into overarching results, and in this spirit he created one of the first systematic Meta-Analyses by integrating results from several comparable studies on typhoid into one of the first Meta-Analyses in 1904 (1, 2). It was more than 35 years later that within psychology the next relevant Meta-Analysis was published: the book-length \"Extrasensory Perception After Sixty Years\", authored by Pratt et al. in 1940, summarizing experimental results from 1882 to 1939 (1). Despite some few studies being compiled over the next decades (most based on clinical research), more sophisticated statistical analysis tools emerging after WW2 ultimately sealed the breakthrough of the method. In the 1970s, Gene V. Glass - together with many other statisticians - developed what he called the 'analysis of the analyses' (2). By integrating statistical results into a meta-analytical frame, it became possible to combine results from the at the time already rising number of publications. Another key development was the availability of modern computers, which allowed for the conducting pf the more sophisticated analyses, along with more studies becoming available online with the rise of the Internet. Meta-analysis thus represents a method where the demand for knowledge of a different order became possible because the general data basis increased, and advanced analyses were made possible by developments in statistics and the wider availability of technology. Today, the method is widely established within all branches of normal science, and generates knowledge way beyond individual studies, yet new challenges emerge concerning which studies can be integrated, and how.",
        "summary": "Karl Pearson created one of the first Meta-Analyses in 1904, integrating results from several typhoid studies. The next significant Meta-Analysis was published in 1940 within psychology. The method's breakthrough came after WW2 with the emergence of more sophisticated statistical analysis tools. In the 1970s, Gene V. Glass developed the 'analysis of the analyses', allowing for the combination of results from a growing number of publications. The availability of modern computers and the rise of the Internet facilitated more sophisticated analyses and the integration of more studies. Today, Meta-Analysis is widely used in all branches of science, but challenges remain regarding the integration of studies.",
        "question": "Who developed the 'analysis of the analyses' method in the 1970s?",
        "ground_truths": "Gene V. Glass developed the 'analysis of the analyses' method in the 1970s."
    },
    {
        "contexts": "Regarding methodological design criteria, it is important to note that meta-analyses are deductive, as they try to integrate and test results from designed studies. While these results are clearly, quantitative, it is important to notice that there is a trend of so called qualitative meta-analysis, which we will not include here. The spatial scale depends on the scale of the studies that are integrated here. While hence a meta-analysis can be global in its focus, more often than not do the studies that a meta-analysis is based on an individual scale, such as in medicine and psychology. It is a matter of debate whether meta-analyses are a snapshot of the current state of the art, or a look into the past. To this end, it is important to recognize that the development or changes over time can be implemented into the analysis, and thus be a relevant part of the hypothesis.",
        "summary": "Meta-analyses are deductive, integrating and testing results from designed studies. There is a trend of qualitative meta-analysis, but this discussion focuses on quantitative ones. The spatial scale of a meta-analysis depends on the scale of the integrated studies, often individual scale in fields like medicine and psychology. It's debated whether meta-analyses represent the current state of the art or a look into the past, but changes over time can be incorporated into the analysis.",
        "question": "What is the spatial scale of a meta-analysis often based on?",
        "ground_truths": "The spatial scale of a meta-analysis is often based on the scale of the integrated studies, often individual scale in fields like medicine and psychology."
    },
    {
        "contexts": "The Meta-Analysis is a well established method that revolves around the systematic identification of relevant studies, integration into a meta-analytical design, and interpretation based on established norms. This process is most established within medical research and psychological science, with extensive protocols being available, while in other scientific disciplines the procedures are more diverse. Meta-Analyses most commonly take place in a Systematic Literature Review, allowing for a quantitative analysis of the results of the gathered publications. The Meta-Analysis is thus not the same as a Systematic Literature Review, but the quantitative process of integrating data within such a process.",
        "summary": "Meta-Analysis is a method involving systematic identification of relevant studies, integration into a meta-analytical design, and interpretation based on norms. It's most established in medical research and psychological science, with extensive protocols available. Meta-Analyses often occur in a Systematic Literature Review, allowing for quantitative analysis of gathered publications. However, Meta-Analysis is not the same as a Systematic Literature Review, but the quantitative process within it.",
        "question": "In which fields is the process of Meta-Analysis most established?",
        "ground_truths": "The process of Meta-Analysis is most established within medical research and psychological science."
    },
    {
        "contexts": "Meta-Analyses are at least as strong (and weak) as the multiple studies that they are based on. Great care needs to be taken when identifying studies that are suitable for a meta-analytical approach, since all studies should be as comparable as possible in terms of their study design. In other words, unexplained variance between studies because of factors that are not taken into account should be avoided. The analysis of data within a Meta-Analysis demands a clear utilization of both the available data and the usage of a proper statistical approach. Many scientific disciplines have established standards that are robust, but can also be rigid. In the long run, a greater exchange of knowledge on the available approaches would be helpful.",
        "summary": "The strength of Meta-Analyses depends on the studies they are based on. Care must be taken to identify suitable studies for a meta-analytical approach, ensuring they are as comparable as possible. Unexplained variance between studies should be avoided. The analysis of data within a Meta-Analysis requires clear utilization of available data and a proper statistical approach. Many disciplines have established robust but rigid standards, and a greater exchange of knowledge on available approaches would be beneficial.",
        "question": "What should be avoided when identifying studies for a meta-analytical approach?",
        "ground_truths": "Unexplained variance between studies because of factors that are not taken into account should be avoided when identifying studies for a meta-analytical approach."
    },
    {
        "contexts": "We define ''Scientific Methods'' as follows: * First and foremost, scientific methods produce knowledge. * Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. * From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. * Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature.",
        "summary": "Scientific methods are defined as reproducible, learnable, and documentable approaches that produce knowledge and help gather, analyze, and interpret data. They can be differentiated into different schools of thinking and have finer differentiations or specifications. Despite the complexity, it's possible to gain an overview of the methodological canon of science with effort and time.",
        "question": "What are the characteristics of scientific methods?",
        "ground_truths": "Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications."
    },
    {
        "contexts": "'''This Wiki describes each presented method in terms of''' * its historical and disciplinary background, * its characteristics and how the method actually works, * its strengths and challenges, * normative implications of the method, * the potential future and open questions for the method, * exemplary studies that deploy the method, * as well as key publications and further readings. Also, each scientific method that is described on this Wiki is categorized according to the Wiki's underlying [[Design Criteria of Methods]].<br/> '''This means that each method fulfills one or more categories of each of the following criteria:''' <br/> * [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]] * [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]] * Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]] * Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]] You can click on each category for more information and all the entries that belong to this category.",
        "summary": "The Wiki describes each scientific method in terms of its background, characteristics, strengths, challenges, implications, future prospects, exemplary studies, and key publications. Each method is also categorized according to the Wiki's design criteria, which includes quantitative or qualitative, inductive or deductive, and various spatial and temporal scales.",
        "question": "How does the Wiki describe and categorize each scientific method?",
        "ground_truths": "The Wiki describes each scientific method in terms of its background, characteristics, strengths, challenges, implications, future prospects, exemplary studies, and key publications. Each method is also categorized according to the Wiki's design criteria, which includes quantitative or qualitative, inductive or deductive, and various spatial and temporal scales."
    },
    {
        "contexts": "You'll want to setup everything nice and clean in the beginning. Nothing fancy, but so that it satisfies all the formal requirements and doesn't hurt your eyes. Set up the font size and styles, line spacing, page margins, front page information and your basic structure (abstract, intro, main parts, conclusions, references and reference format, indices, glossary, ...). Check 3 times and then with your advisor if your basic setup satisfies all the formal requirements. If you use a citation software, set that one up as well and make sure everything works to your liking.",
        "summary": "Start by setting up your document to meet all formal requirements. This includes font size and styles, line spacing, page margins, front page information, and basic structure. Check with your advisor to ensure your setup is correct. If you use citation software, set it up to your preference.",
        "question": "What are the initial steps in setting up a Microsoft Word document for academic writing?",
        "ground_truths": "The initial steps in setting up a Microsoft Word document for academic writing include setting up the font size and styles, line spacing, page margins, front page information, and basic structure. It's also important to check with an advisor to ensure the setup meets all formal requirements. If citation software is used, it should also be set up to the user's preference."
    },
    {
        "contexts": "This is the part where you basically don't want to have to care about the software you're using. Write plain text. If you have to format, use styles (see instructions below), and don't ever play around with picture or table positioning. Anything fancy you do here will make you despair when revising and reformatting later.",
        "summary": "During the writing phase, focus on writing plain text and avoid playing with picture or table positioning. If formatting is necessary, use styles. Any complex formatting done at this stage may cause issues during revision and reformatting.",
        "question": "What should be the focus during the writing phase in Microsoft Word for academic writing?",
        "ground_truths": "During the writing phase in Microsoft Word for academic writing, the focus should be on writing plain text and avoiding complex formatting or positioning of pictures or tables. If formatting is necessary, styles should be used."
    },
    {
        "contexts": "First, do not, under any circumstances, manually format any piece of text, table or picture in your document. Always, always use Word Styles (\"Formatvorlagen\"). There are many reasons why it is a good idea, but the main one is that you are guaranteed to miss some font style change that happened due to you copy/pasting stuff in and you will spend roughly 3 weeks in your document to try and spot where your font style changed. Use styles instead, and adjust those when necessary.",
        "summary": "Avoid manually formatting any text, table, or picture in your document. Instead, always use Word Styles. This is because manual formatting can lead to missed font style changes, especially when copy/pasting, which can be time-consuming to correct. Adjust styles as necessary.",
        "question": "Why should you avoid manual formatting in Microsoft Word for academic writing?",
        "ground_truths": "Manual formatting should be avoided in Microsoft Word for academic writing because it can lead to missed font style changes, especially when copy/pasting. This can be time-consuming to correct. Instead, Word Styles should be used and adjusted as necessary."
    },
    {
        "contexts": "Mindfulness is a concept with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative assumptions that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the contexts of many diverse but often religious schools of thinking.",
        "summary": "Mindfulness is a multifaceted concept that aims to clear the mind and focus on the present moment, free from normative assumptions. It is a practice with a long history, predating Buddhism, and is often associated with various schools of thought and religious practices.",
        "question": "What is the main goal of practicing mindfulness?",
        "ground_truths": "The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions."
    },
    {
        "contexts": "Since the goal of mindfulness is basically having 'no mind', it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free.",
        "summary": "While the ultimate goal of mindfulness is to achieve a state of 'no mind', it is often used to help balance feelings of anxiety, stress, and unhappiness. It is a key component of Buddhist practice and has been shown to have positive effects on various psychological challenges.",
        "question": "What are some benefits of practicing mindfulness?",
        "ground_truths": "Some benefits of practicing mindfulness include balancing feelings of anxiety, stress, and unhappiness, and it can have positive effects on various psychological challenges."
    },
    {
        "contexts": "The easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen \u2013 sitting meditation \u2013 is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.",
        "summary": "The simplest form of mindfulness involves sitting upright and focusing on one's breath. This practice, along with others like Zazen, or sitting meditation, are key components of Buddhist and other Eastern monastic traditions. These practices often involve exploring the origins of our thoughts and emotions, or our interconnectedness with others.",
        "question": "What is the simplest form of mindfulness practice?",
        "ground_truths": "The simplest form of mindfulness practice involves sitting upright and focusing on one's breath."
    },
    {
        "contexts": "During the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.",
        "summary": "In recent decades, mindfulness has gained popularity in the Western world, leading to the development of various approaches and tools, including apps like Headspace. However, it's important to note that these modern approaches often deviate significantly from the original principles of mindfulness.",
        "question": "How has the practice of mindfulness evolved in the Western world?",
        "ground_truths": "In the Western world, the practice of mindfulness has evolved to include various approaches and tools, including apps like Headspace, but these often deviate significantly from the original principles of mindfulness."
    },
    {
        "contexts": "Mindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around \u2013 often referred to as the monkey mind \u2013, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at this article). Just like sports, mindfulness exercises are a practice where one gets better over time.",
        "summary": "While mindfulness has been marketed as a self-optimisation tool, it is more accurately a practice for achieving a calm mind. It involves recognizing and calming the 'monkey mind', a term for the mind's tendency to constantly wander. Like any skill, mindfulness improves with practice.",
        "question": "What is the 'monkey mind' in the contexts of mindfulness?",
        "ground_truths": "In the contexts of mindfulness, the 'monkey mind' refers to the mind's tendency to constantly wander."
    },
    {
        "contexts": "If you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions \u2013 just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!",
        "summary": "To start a mindfulness practice, one could use apps like Headspace, engage in breathing exercises, journaling, body scans, or yoga. Mindfulness can also be practiced during everyday activities or through writing a gratitude journal. Regular mindfulness practice can help relieve stress and fear, increase feelings of peace and gratitude, and improve focus and calmness.",
        "question": "How can one start a mindfulness practice?",
        "ground_truths": "One can start a mindfulness practice by using apps like Headspace, engaging in breathing exercises, journaling, body scans, or yoga, or by practicing mindfulness during everyday activities or through writing a gratitude journal."
    },
    {
        "contexts": "Mindmapping is a tool for the visual organisation of information, showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports communication and creativity. Visualise information in an intuitive structure for a good overview of key elements of a topic. Better communicate and structure information for individual and team work.",
        "summary": "Mindmapping is a visual tool for organizing information around a central topic. It helps provide an overview of a topic and supports communication and creativity. It is used to visualize information in an intuitive structure and to improve communication and structure in individual and team work.",
        "question": "What is the purpose of mindmapping?",
        "ground_truths": "The purpose of mindmapping is to visually organize information around a central topic, provide an overview of a topic, and support communication and creativity. It is also used to improve communication and structure in individual and team work."
    },
    {
        "contexts": "Although this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.",
        "summary": "The concept of a Mindmap, which mimics the brain's way of structuring information, is not fully backed by research. However, studies have shown that this method is effective for memorizing and understanding information. The Mindmap was developed and popularized in the 1970s by Tony Buzan.",
        "question": "Who created the Mindmap and when?",
        "ground_truths": "The Mindmap was created and propagated in the 1970s by Tony Buzan."
    },
    {
        "contexts": "A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more. It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a brainstorming process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.",
        "summary": "A Mindmap allows for the visual organization of various types of information, providing a quick overview of all relevant items on a topic. It aids in tracking important elements and can support communication of information to others. It can also be useful in brainstorming processes due to its intuitive approach to arranging information.",
        "question": "How can a Mindmap be useful in a brainstorming process?",
        "ground_truths": "A Mindmap can be useful in a brainstorming process as it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information."
    },
    {
        "contexts": "The central topic is written into the center of the visualisation (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city. The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. Elements may be subordinate to other elements, indicating nestedness of the information. Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.",
        "summary": "In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it like districts in a city. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.",
        "question": "How is information arranged in a Mindmap?",
        "ground_truths": "In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance."
    },
    {
        "contexts": "Mixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.",
        "summary": "Mixed Effect Models, a continuation of Fisher's work, incorporate both fixed and random factors to minimize the impact of unwanted information on statistical results. Fisher's experiments aimed to control variance within experiments, while acknowledging the influence of underlying factors. Charles Roy Henderson further developed this by creating calculations for linear unbiased estimates, allowing for the testing of complex hypotheses and analysis of complex datasets. This versatility has led to the widespread use of Mixed Effect Models in various disciplines.",
        "question": "Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?",
        "ground_truths": "Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models."
    },
    {
        "contexts": "Mixed Effect Models are - mechanically speaking - one step further with regards to the combination of regression analysis and Analysis of Variance, as they can combine the strengths of both these approaches: Mixed Effects Models are able to incorporate both categorical and/or continuous independent variables. Since Mixed Effect Models were further developed into Generalized Linear Mixed Effect Models, they are also able to incorporate different distributions concerning the dependent variable. Typically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.",
        "summary": "Mixed Effect Models combine regression analysis and Analysis of Variance, incorporating both categorical and continuous independent variables. They have evolved into Generalized Linear Mixed Effect Models, which can handle different distributions of the dependent variable. Their diverse algorithmic foundations, such as maximum likelihood and restricted maximum likelihood, provide more statistical power than standard regressions. Their main strength lies in their ability to incorporate random effects, making them highly versatile in univariate statistics.",
        "question": "What is the main strength of Mixed Effect Models in statistical analysis?",
        "ground_truths": "The main strength of Mixed Effect Models is their ability to incorporate random effects."
    },
    {
        "contexts": "The biggest strength of Mixed Effect Models is how versatile they are. There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done better by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know.",
        "summary": "Mixed Effect Models are highly versatile, capable of handling almost any part of univariate or advanced statistics more effectively than other models. They outperform Analysis of Variance in statistical power and Regressions in handling complex datasets, and their ability to plan and understand random variance brings science closer to distinguishing between what we want to know and what we do not.",
        "question": "How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?",
        "ground_truths": "Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets."
    },
    {
        "contexts": "\"Mixed Methods refers in the broadest sense to the combination of elements of a qualitative and a quantitative research approach within one investigation or several investigations related to each other. The combination can refer to the underlying scientific theoretical position and the research question, to the methods of data collection or analysis, or to the procedures of interpretation and quality assurance\" (Schreier & Odag, p.263, definition based on Johnson, Onwuegbuzie & Turner 2007, p.123). Mixed methods have become almost like a standard reply to any given problem that researchers define to be outside of the domain of normal science. These days, mixed methods are almost like a mantra, a testimony of beliefs, a confirmation of openness and the recognition of diversity. One could speak of a confusion concerning mixed methods, where researchers speak about ontology (how we make sense of what we know about the world), when they should actually speak about epistemology (what we know about the world and how we create this knowledge); the talk of mixed methods is drifting into a category mistake that then becomes a categorical mistake. Mixed methods are one of the moon shots of modern science, they are proclaimed, envisioned and continuously highlighted, but the question is now: ''How do we get to the moon of mixed methods?''",
        "summary": "Mixed Methods is a combination of qualitative and quantitative research approaches used in one or several related investigations. It has become a standard response to problems outside the domain of normal science, often seen as a mantra or a testament to openness and diversity. However, there is confusion about mixed methods, with researchers often confusing ontology and epistemology. Despite being a focus of modern science, the practical application of mixed methods remains a challenge.",
        "question": "What is the concept of Mixed Methods in research?",
        "ground_truths": "Mixed Methods is a combination of qualitative and quantitative research approaches used in one or several related investigations. It has become a standard response to problems outside the domain of normal science, often seen as a mantra or a testament to openness and diversity."
    },
    {
        "contexts": "The first set of challenges when trying to bring mixed methods into reality are epistemological problems. Among the most profane yet widely unsolved questions is the integration of different data formats. While we can code data into qualitative and quantitative information into tables, this can hardly do justice to the diversity of knowledge within science. Even the integration of data within tables poses many unsolved problems. For instance, different methods can have not only a different understanding but also diverging indications of validity and plausibility. While these are pivotal in the realms of quantitative data but also in logic, validity and plausibility may be totally wrong criteria regarding much knowledge from qualitative domains where contexts and transferability - or the lack thereof - are more relevant. Some forms of knowledge are important to many, while other forms of knowledge are important to few. Yet, these forms of knowledge are still important, and science often ignores diversity more strongly than we should.",
        "summary": "Implementing mixed methods presents epistemological challenges, particularly in integrating different data formats. Coding data into qualitative and quantitative information in tables doesn't fully capture the diversity of knowledge in science. Different methods can have varying understandings and indications of validity and plausibility, which may not be appropriate for all types of knowledge. The importance of diverse forms of knowledge is often overlooked in science.",
        "question": "What are the challenges in implementing mixed methods in research?",
        "ground_truths": "Implementing mixed methods presents epistemological challenges, particularly in integrating different data formats. Coding data into qualitative and quantitative information in tables doesn't fully capture the diversity of knowledge in science. Different methods can have varying understandings and indications of validity and plausibility, which may not be appropriate for all types of knowledge."
    },
    {
        "contexts": "In order to enable successful integration and bridge the epistemological challenges with the ontological problems, facilitation becomes a key part of mixed method research. This demands first and foremost the willingness to be critical about one's own positionality and reflection concerning the critical positioning in the [[History of Methods|history of science]]. Otherwise, the limitations of the knowledge in the respective area of science is often ignored, leading to a wrong recognition of these limitations. '''Many conflicts between different disciplines and their methodological epistemologies are nothing but an unreflected rejection of the unknown.''' Consequently, it takes time and effort to not only locate yourself within the canon of knowledge production, but to also value and appreciate other forms of knowledge. A simple division into a ''better or worse'' highlights the shortcomings of our understanding concerning the [[Normativity of Methods|normativity of methods]].",
        "summary": "Facilitation is crucial in mixed method research to bridge epistemological and ontological challenges. This requires critical reflection on one's position in the history of science and an understanding of the limitations of knowledge in one's field. Conflicts between disciplines often stem from an unreflected rejection of the unknown. It's important to appreciate diverse forms of knowledge and avoid simplistic divisions of 'better' or 'worse'.",
        "question": "Why is facilitation important in mixed method research?",
        "ground_truths": "Facilitation is crucial in mixed method research to bridge epistemological and ontological challenges. This requires critical reflection on one's position in the history of science and an understanding of the limitations of knowledge in one's field. It's important to appreciate diverse forms of knowledge and avoid simplistic divisions of 'better' or 'worse'."
    },
    {
        "contexts": "The project focuses on the Trans Mongolian Railroad Corridor (TMR) between Ulaanbaatar and Erenhot, where fenced railroad - a consequence of a growing mining sector - creates a barrier for nomands and their herds, leads to habitat fragmentation for the movement of wildlife, and imposes a danger to both. This conflict between infrastructure, local populations and animals is in the center of this research proposal. Two research proposals are presented here, which both revolve around the idea of identifying potential spots for railway crossings for wildlife and nomads. Landsat GIS data, as well as wildlife movement GIS data from previous studies, are proposed be analyzed to examine topographical preconditions for railway crossings, as well as vegetation changes and gazelle migration patterns to identify relevant spots for local wildlife. In addition, data from semi-structured interviews with ten nomad herders shall be gathered and analyzed to assess if, and where, they would be interested in also using railway crossings in accordance with their traditional and personal requirements. In combination, the gathered data shall help identify potential spots for railway crossings, which can be installed subsequently.",
        "summary": "The research focuses on the Trans Mongolian Railroad Corridor, where the fenced railroad creates a barrier for nomads and wildlife. The study proposes to identify potential spots for railway crossings for wildlife and nomads using Landsat GIS data and wildlife movement GIS data. Additionally, semi-structured interviews with nomad herders will be conducted to assess their interest in using railway crossings.",
        "question": "What methods are proposed to identify potential spots for railway crossings for wildlife and nomads in the Trans Mongolian Railroad Corridor?",
        "ground_truths": "The study proposes to use Landsat GIS data and wildlife movement GIS data from previous studies to examine topographical preconditions for railway crossings, vegetation changes and gazelle migration patterns. Additionally, semi-structured interviews with ten nomad herders will be conducted to assess their interest in using railway crossings."
    },
    {
        "contexts": "This research proposal revolves around the problem of providing affordable, yet healthy and sustainable food at university cafeterias in Germany and the US. The suggested idea here is to, first, conduct a questionnaire with 60 first semester university students at two universities, each, based on which the participants will be grouped according to their financial resources, which are of interest to this study. Then, students take part in four six-week long project phases, in which they are asked to document in a digital diary their food consumption and review their experience with one of three rotating interventions, as well as their initial behavioral patterns. The interventions either include meal vouchers for vegan or vegetarian meals at dining halls; educational sessions on health, sustainability and affordable nutrition as well as cooking classes; and engagement with a newly set up greenhouse. After going through all four phases, participants are asked to rank all interventions in a questionnaire. All diary and questionnaire data is analyzed qualitative and quantitatively, and suitable recommendations are developed for both universities based on the insights.",
        "summary": "The research proposal focuses on providing affordable, healthy, and sustainable food at university cafeterias in Germany and the US. A questionnaire will be conducted with 60 first semester students at two universities, followed by four six-week long project phases where students document their food consumption and review their experience with one of three interventions. After all phases, participants rank the interventions and the data is analyzed to develop recommendations.",
        "question": "What are the three interventions proposed in the research to provide affordable, healthy, and sustainable food at university cafeterias?",
        "ground_truths": "The three interventions proposed in the research include meal vouchers for vegan or vegetarian meals at dining halls, educational sessions on health, sustainability and affordable nutrition as well as cooking classes, and engagement with a newly set up greenhouse."
    },
    {
        "contexts": "This research proposal investigates the potential of volunteer tree planting in the Sierras Grandes in Argentina. To do so, it is proposed to establish a local science center in order to co-develop a future scenario with local stakeholders and subsequently ensure its long-term and inclusive implementation. In a first step, a literature review and ethnographic observation of local actors are conducted to get an initial overview on the study area and field. Then, in semi-structured interviews, relevant stakeholders shall be identified, which shall be invited to four scenario planning workshop. Here, with a focus on inclusive and collaborative development of the region, four scenarios are constructed, one of which is selected as the baseline for future activitites. First measures, with a focus on volunteer tree planting, shall be implemented and evaluated as a baseline for future plans.",
        "summary": "The research proposal explores the potential of volunteer tree planting in the Sierras Grandes in Argentina. It proposes to establish a local science center to co-develop a future scenario with local stakeholders. A literature review and ethnographic observation are conducted initially, followed by semi-structured interviews with stakeholders. Four scenarios are constructed in a workshop, one of which is selected for future activities focusing on volunteer tree planting.",
        "question": "What is the main focus of the research proposal in the Sierras Grandes in Argentina?",
        "ground_truths": "The main focus of the research proposal in the Sierras Grandes in Argentina is to explore the potential of volunteer tree planting and to co-develop a future scenario with local stakeholders."
    },
    {
        "contexts": "The diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.",
        "summary": "Model reduction, comparison, and simplification in univariate statistics are often stuck between specific schools of thought and lack transparency and reproducibility. This contributes to crises in various scientific fields struggling to produce reproducible results and transferable knowledge. The main challenge is the diverse approaches and numerous control measures and parameters. The general tendency of model reduction is to negotiate between complexity and simplicity, with Occam's razor at the core of this philosophy.",
        "question": "What is the main challenge in the field of model reduction in univariate statistics?",
        "ground_truths": "The main challenge in the field of model reduction in univariate statistics is the diverse approaches available and the almost uncountable control measures and parameters."
    },
    {
        "contexts": "Regarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches.",
        "summary": "Model reduction is understood as a form of model simplification, making model comparison a means to an end. The maximum model, which includes all variables and their combinations, has several problems. It can be difficult to interpret due to the number of variables and statistical results, and it may not provide pragmatic information for decision-making. Additionally, statistical analysis based on probabilities has a flawed view on maximum models, as probabilities change with model reduction.",
        "question": "What are the problems associated with the maximum model in model reduction?",
        "ground_truths": "The problems associated with the maximum model in model reduction are that variables may be redundant, the model can be difficult to interpret due to the number of variables and statistical results, and statistical analysis based on probabilities has a flawed view on maximum models, as probabilities change with model reduction."
    },
    {
        "contexts": "The most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.",
        "summary": "A blunt approach to model reduction is a stepwise procedure, which boils down a model until only significant variables remain. This can be done through backward selection, starting from a maximum model, or forward selection, starting with one predictor and adding more. However, these procedures are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. Therefore, they should be avoided or seen as non-smart brute force tools.",
        "question": "Why should stepwise procedures in model reduction be avoided?",
        "ground_truths": "Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way."
    },
    {
        "contexts": "Instead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values.",
        "summary": "Data should be initially inspected for statistical distributions and prevalences, checking for outliers, skewed distributions, and other potential issues. Qualitative data should be checked for sufficient sample size across all factor levels, and missing values should be replaced or excluded. Redundancies should be filtered out, with redundant variables containing more missing values being excluded in favor of those with similar explanatory power but fewer missing values.",
        "question": "What should be the initial steps in inspecting data for model reduction?",
        "ground_truths": "The initial steps in inspecting data for model reduction should be checking the data for statistical distributions and prevalences, outliers, skewed distributions, and other potential issues. Qualitative data should be checked for sufficient sample size across all factor levels, and missing values should be replaced or excluded. Redundancies should be filtered out."
    },
    {
        "contexts": "The simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented.",
        "summary": "The simplest approach to identify redundancies is through correlations, specifically Pearson correlation, which doesn't require a normal distribution. Variables with a correlation coefficient above 0.9 are considered redundant, and those between 0.7 and 0.9 are suspicious. A more integrated approach is ordinations, with principal component analysis being the main tool for continuous variables. This reduces the main variances of datasets to artificial axes or dimensions, allowing for the filtering of large datasets into fewer artificial axes while maintaining much of the information.",
        "question": "What are the methods to identify redundancies in data for model reduction?",
        "ground_truths": "The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables."
    },
    {
        "contexts": "In this article, we will go through the whole process of running a multiple regression in Python. This includes the choice of the relevant packages, inspecting and cleaning data, creating descriptive data, writing the multiple regression model, and interpreting its results. Generally, a multiple regression analysis can be understood as a model that identifies a relationship between one dependent variable and several independent variables, while taking the effects of all independent variables into account. The prediction you receive regarding the change in the dependent variable by one unit increase of a dependent variable is assumed to be linear. That means that no matter how high the dependent or independent variable is, the change is always the same. An example of multiple regression analysis is an analysis of how the number of hours spent for domestic unpaid work is influenced by gender, the number of children in the household, the number of paid hours worked, marital status, and educational level. The dependent variable is the hours of domestic unpaid work, and all the other variables are independent variables.",
        "summary": "This article explains the process of running a multiple regression in Python, which includes selecting relevant packages, inspecting and cleaning data, creating descriptive data, writing the regression model, and interpreting results. Multiple regression is a model that identifies a relationship between one dependent variable and several independent variables, assuming a linear relationship. An example is given where the dependent variable is the hours of domestic unpaid work, influenced by several independent variables.",
        "question": "What is the process of running a multiple regression in Python?",
        "ground_truths": "The process of running a multiple regression in Python includes selecting relevant packages, inspecting and cleaning data, creating descriptive data, writing the regression model, and interpreting results."
    },
    {
        "contexts": "Firstly, we have to import all relevant packages for analysis and visualization. import pandas as pd ## needed for organizing our data import matplotlib.pyplot as plt ## needed for data visualization import seaborn as sns ## needed for data visualization import statsmodels.api as sm ##needed to run the multiple regression import scipy.stats ## for checking your multiple regression model from scipy.stats import jarque_bera ##for checking your multiple regression model import statsmodels.api as sm ## for checking your multiple regression model from statsmodels.stats.diagnostic import het_breuschpagan ## for checking your multiple regression model from statsmodels.stats.outliers_influence import variance_inflation_factor ##needed to test for multicollinearity",
        "summary": "The first step in running a multiple regression in Python is to import all necessary packages for data organization, visualization, running the regression, and checking the model. These packages include pandas, matplotlib, seaborn, statsmodels, scipy.stats, jarque_bera, het_breuschpagan, and variance_inflation_factor.",
        "question": "What are the necessary packages to import for running a multiple regression in Python?",
        "ground_truths": "The necessary packages to import for running a multiple regression in Python are pandas, matplotlib, seaborn, statsmodels, scipy.stats, jarque_bera, het_breuschpagan, and variance_inflation_factor."
    },
    {
        "contexts": "In the next step, we load our dataset and print some general information about it, such as column names, shape, number of NAs, and the datatypes of the column. df = pd.read_csv('C:/Users/Dell Latitude/Downloads/archive/adm_data.csv')## make sure you fill in the correct working directory to let python know where the data is. I have here made an exemplary path. print(df.columns) ###shows the columns print(df.shape)##tells you the number of rows and columns (first rows, then columns) print(df.isnull().sum()) ## tells you the number of missing values per column print(df.dtypes)## tells you the data types print(df.head)## you can see here the first five and the last five rows to check the structure of the data",
        "summary": "The next step involves loading the dataset and printing general information about it, such as column names, shape, number of NAs, and the datatypes of the column. This is done using pandas functions like read_csv, columns, shape, isnull, dtypes, and head.",
        "question": "What information is printed about the dataset in the next step of running a multiple regression in Python?",
        "ground_truths": "In the next step of running a multiple regression in Python, general information about the dataset is printed, such as column names, shape, number of NAs, and the datatypes of the column."
    },
    {
        "contexts": "We can see that in this case, the dataset does not contain any NA values ('not available', values that are missing), which is why we can skip the step of dropping or filling in the missing values. In the next step, we are going to do a first visualization of our data. To keep it simple, we are going to leave out the categorical data (LOR, SOP & University Ranking). However, to use categorical data in the case of a regression, the variables could be coded into dummy variables containing 1 & 0's. You can find a wiki entry on this topic, here: https://sustainabilitymethods.org/index.php/Dummy_variables",
        "summary": "The dataset does not contain any NA values, so the step of dropping or filling in missing values can be skipped. The next step is to visualize the data, leaving out the categorical data. However, categorical data can be used in regression by coding them into dummy variables containing 1 & 0's.",
        "question": "How can categorical data be used in a regression?",
        "ground_truths": "Categorical data can be used in a regression by coding them into dummy variables containing 1 & 0's."
    },
    {
        "contexts": "Storytelling has been a way for humankind to express, convey, form and make sense of their reality for thousands of years (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). 'Storytelling' is defined as the distinct tonality, format and presentation in which a story is told. The term 'narrative' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term 'narrative' has been used used predominantly as a synonym for 'story' in academia for decades (Barrett & Stauffer 2009).",
        "summary": "Storytelling, a method humans have used to understand their reality for millennia, is characterized by its unique tone, format, and presentation. The term 'narrative' encompasses both the story and the act of storytelling, although it has often been used interchangeably with 'story' in academic circles.",
        "question": "What does the term 'narrative' encompass in the contexts of storytelling?",
        "ground_truths": "The term 'narrative' encompasses both the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling."
    },
    {
        "contexts": "Psychologist Jerome Bruner introduced the notion of 'narrative' as being one of two forms of distinct modes of thinking in 1984 - the other being the 'logico-scientific' mode (Barrett & Stauffer 2009). While the latter is (...) more concerned with establishing universal truth conditions (Barrett & Stauffer 2009, p.9), the 'narrative' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that 'narratives' are a central form of human learning about - and sense-making of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.",
        "summary": "In 1984, psychologist Jerome Bruner proposed 'narrative' as one of two distinct modes of thinking, the other being the 'logico-scientific' mode. The 'narrative' mode represents the broad human experience of reality, leading to the idea that narratives are central to human learning and understanding of the world. This recognition led to the establishment of narrative psychology.",
        "question": "What are the two distinct modes of thinking introduced by Jerome Bruner?",
        "ground_truths": "The two distinct modes of thinking introduced by Jerome Bruner are 'narrative' and 'logico-scientific' mode."
    },
    {
        "contexts": "As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).",
        "summary": "Narrative Research, often simply called 'narrative', is a relatively new scientific method. It has evolved into a mode of scientific inquiry in various social science disciplines, paralleling the rise of qualitative research in the latter half of the 20th century. Narrative Research, which is based on the understanding of 'narrative' as both a form of story and a form of meaning-making, is now widely used across many disciplines, particularly in educational research.",
        "question": "What is the role of 'narrative' in Narrative Research?",
        "ground_truths": "'Narrative' in Narrative Research is understood as both a form of story and a form of meaning-making of the human experience."
    },
    {
        "contexts": "Narrative Research -  or 'Narrative Inquiry' - is shaped by and focussing on a conceptual understanding of 'narratives' (Barrett & Stauffer 2009, p.15). Here, 'narratives' are seen as a format of communication that people apply to make sense of their life experiences. Communities, social groups, and subcultures tell stories with words and meanings that are specific to their experience and way of life. The lexicon of a social group constitutes its perspective on the world, and it is assumed that narrations preserve particular perspectives in a more genuine form (Jovchelovitch & Bauer 2000, p.2). Narratives are therefore not merely forms of representing a chain of events, but a way of making sense of what is happening. Through the telling of a story, people link events in meaning. The elements that are conveyed in the story, and the way these are conveyed, indicates how the story-teller - and/or their social surrounding - sees the world. They are a form of putting reality into cultural and individual perspective. Also, narratives are never final but change over time as new events arise and perspectives develop (Jovchelovitch & Bauer 2000, Webster & Mertova 2007, Squire et al. 2014, Moen 2006).",
        "summary": "Narrative Research or 'Narrative Inquiry' is based on a conceptual understanding of 'narratives' as a form of communication used by individuals and communities to make sense of their experiences. Narratives are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.",
        "question": "How are 'narratives' used in Narrative Research?",
        "ground_truths": "'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve."
    },
    {
        "contexts": "Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000). Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research \"(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates\". Further, \"Narrative inquiry attempts to capture the 'whole story', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important 'intervening' stages\" (ibid, p.3), with the latter being the contexts and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).",
        "summary": "Narratives have inherent structure and narrative inquiry is not non-structured but gains structure by itself. Research methods that view narratives as mere data format neglect an important feature of narratives: they require going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions. Narrative inquiry attempts to capture the 'whole story', whereas other methods often omit the important 'intervening' stages.",
        "question": "What is the unique feature of narrative research that other research methods often neglect?",
        "ground_truths": "The unique feature of narrative research that other research methods often neglect is that it requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions. It attempts to capture the 'whole story', whereas other methods often omit the important 'intervening' stages."
    },
    {
        "contexts": "The insights gained through narratives are subjective to the narrator, which implies advantages and challenges. Compared to an 'objective' description of, e.g. a chain of events, the narration provides insights about the individual's interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: \"Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived.\" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the 'real' world. Squire et al. (2014) refer to this distinction as different forms of 'truth' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical 'truth'. They should try \"(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather.\" Moen (2006, p.63), by comparison, explains that \"(...) there is no static and everlasting truth\", anyway.",
        "summary": "Narrative insights are subjective to the narrator, providing insights about the individual's interpretation and experience of events, which may be inaccessible elsewhere. However, this subjective representation may be distorted and differ from the 'real' world. Researchers may be interested in either representations of the physical world or of social realities. Researchers should engage with the subjective narrative, then compare it to further information on the physical 'truth'.",
        "question": "What are the advantages and challenges of the subjectivity of narratives in narrative research?",
        "ground_truths": "The advantages of the subjectivity of narratives in narrative research are that they provide insights about the individual's interpretation and experience of events, which may be inaccessible elsewhere. The challenges are that this subjective representation may be distorted and differ from the 'real' world."
    },
    {
        "contexts": "This [[Bias and Critical Thinking|conflict between different 'truths']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, 'usefulness' and 'persuasiveness' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: \"Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable' (...) Narrative research does not claim to represent the exact 'truth', but rather aims for 'verisimilitude'\". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a 'trustworthy' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one's research results (see Moen 2006, p.64). Also, researchers \"(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts\" (Polkinghorne 2007, p.484).",
        "summary": "The conflict between different 'truths' has consequences for the quality criteria for Narrative Inquiry. 'Usefulness' and 'persuasiveness' of the created narratives have been suggested as quality criteria. Narrative research does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable'. A 'trustworthy' set of field notes and Interview data may serve as a measure of quality.",
        "question": "What are the quality criteria for Narrative Inquiry?",
        "ground_truths": "The quality criteria for Narrative Inquiry include 'usefulness' and 'persuasiveness' of the created narratives. Narrative research does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable'. A 'trustworthy' set of field notes and Interview data may also serve as a measure of quality."
    },
    {
        "contexts": "Further challenges may arise during the active collaboration of the researcher in the field. For example, \"(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher\" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]].",
        "summary": "Challenges may arise during the active collaboration of the researcher in the field, such as the researcher and the research subjects interpreting specific events in different ways or the research subjects questioning the interpretive authority of the researcher.",
        "question": "What challenges may arise during the active collaboration of the researcher in the field in narrative research?",
        "ground_truths": "Challenges that may arise during the active collaboration of the researcher in the field in narrative research include the researcher and the research subjects interpreting specific events in different ways or the research subjects questioning the interpretive authority of the researcher."
    },
    {
        "contexts": "Frequentist statistics widely builds on equilibrium dynamics. Generally, equilibrium dynamics are systems that are assumed to have reached a steady state, and whose variability is either stable or understood. The last difference is already of pivotal importance and marks a pronounced difference in how models evolved over the last decades. Regarding stable dynamics, steady state systems have been known for long in physics, and their dynamics have equally long been explored by mathematics. However, especially the developments in physics during the 20th Century made it clear that many phenomena in the natural world do not follow steady state dynamics, and this thought slowly inched its way into other fields beyond physics.",
        "summary": "Frequentist statistics are largely based on equilibrium dynamics, which are systems that have reached a steady state with stable or understood variability. This concept has been long known in physics and explored by mathematics. However, 20th-century developments in physics revealed that many natural phenomena do not follow steady state dynamics, leading to a shift in focus in other fields beyond physics.",
        "question": "What is the basis of frequentist statistics and how has its understanding evolved over time?",
        "ground_truths": "Frequentist statistics are largely based on equilibrium dynamics, which are systems that have reached a steady state with stable or understood variability. However, 20th-century developments in physics revealed that many natural phenomena do not follow steady state dynamics, leading to a shift in focus in other fields beyond physics."
    },
    {
        "contexts": "Knowledge about complex or chaotic systems slowly inched its way into other branches of science as well, with cotton prices being a prominent example of patterns that could be better explained by chaos theory, at least partly. There seemed to be unexplained variance in such numbers that could not be tamed by the statistical approaches available before. From our viewpoint today, this is quite understandable. Cotton is an industry widely susceptible to fluctuations by water availability and pests, often triggering catastrophic losses in the production of cotton that were as hard to explain as long-term weather conditions. Cotton pests and weather fluctuations can be said to follow patterns that are at least partly comparable, and this is where chaotic dynamics - also known as 'non-equilibrium dynamics' - become a valuable approach.",
        "summary": "Understanding of complex or chaotic systems has gradually permeated other scientific fields, with cotton prices being a notable example of patterns better explained by chaos theory. The cotton industry, prone to fluctuations due to water availability and pests, often experiences catastrophic losses that were as difficult to explain as long-term weather conditions. These patterns are partly comparable and can be better understood through chaotic dynamics or 'non-equilibrium dynamics'.",
        "question": "How has the understanding of complex or chaotic systems influenced other scientific fields, and what is an example of this?",
        "ground_truths": "Understanding of complex or chaotic systems has gradually permeated other scientific fields, with cotton prices being a notable example of patterns better explained by chaos theory. The cotton industry, prone to fluctuations due to water availability and pests, often experiences catastrophic losses that were as difficult to explain as long-term weather conditions. These patterns are partly comparable and can be better understood through chaotic dynamics or 'non-equilibrium dynamics'."
    },
    {
        "contexts": "Over the last decades, many types of statistical models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with Generalised Linear Models. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer causality when explaining the patterns that are being modeled.",
        "summary": "In recent decades, various statistical models have been developed to handle non-linear dynamics, with Generalized Additive Models (GAM) being a notable example. Developed by Trevor Hastie and Robert Tibshirani, these models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.",
        "question": "What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?",
        "ground_truths": "Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns."
    },
    {
        "contexts": "ORM is a programming technique for mapping objects to relational databases. There is a significant difference between object-oriented and relational databases. Relational databases are organized in rows and columns where each row has a unique key, and each column has a unique name. This way, you have a very structured database. An object-oriented database relies on objects which contain data and methods. For example, an object of Leuphana University might contain data about the staff employed, the total number of students, the number of courses offered, yearly budget spending, etc. But it can also contain methods, such as a formula to calculate the average monthly spending on chemistry laboratory equipment. These two datatypes therefore follow different logics and are not fully compatible, even though information from one database sometimes needs to be accessible from the other.",
        "summary": "Object Relational Mapping (ORM) is a technique that maps objects to relational databases. Relational databases are structured with rows and columns, each with unique keys and names. On the other hand, object-oriented databases rely on objects that contain data and methods. These two types of databases follow different logics and are not fully compatible.",
        "question": "What is the difference between object-oriented and relational databases?",
        "ground_truths": "Relational databases are organized in rows and columns where each row has a unique key, and each column has a unique name, making it very structured. Object-oriented databases, however, rely on objects which contain data and methods. These two types of databases follow different logics and are not fully compatible."
    },
    {
        "contexts": "ORM creates an intermediate layer between the developer and the database, converting the data into object entities in Python, which shields the differences between different databases while making it very easy for the developer to manipulate the data in the database to use the advanced object-oriented features. Many components in Python provide ORM support, each with a slightly different application field, but the theoretical principles of database manipulation are the same. The following describes some of the more well-known ORM frameworks for Python databases.",
        "summary": "ORM acts as a middle layer between the developer and the database, converting data into Python object entities. This shields the differences between databases and simplifies data manipulation for the developer. Many Python components provide ORM support, with slightly different application fields. Several well-known ORM frameworks for Python databases are described.",
        "question": "What is the role of ORM in Python?",
        "ground_truths": "ORM creates an intermediate layer between the developer and the database, converting the data into object entities in Python, which shields the differences between different databases while making it very easy for the developer to manipulate the data in the database to use the advanced object-oriented features."
    },
    {
        "contexts": "Database details are shielded from developers so that developers do not have to deal with SQL statements, which improves development efficiency. Facilitates database migration. SQL-based data access layers often spend much time debugging SQL statements when changing databases because of the nuanced differences in SQL statements between each database. ORM provides a SQL-independent interface, and the ORM engine handles the differences between databases, so no code changes are required when migrating databases. Applying techniques such as cache optimization can sometimes improve the efficiency of database operations.",
        "summary": "ORM shields database details from developers, improving development efficiency and facilitating database migration. It provides a SQL-independent interface, handling the differences between databases, so no code changes are required when migrating databases. Techniques like cache optimization can improve database operation efficiency.",
        "question": "How does ORM improve development efficiency and facilitate database migration?",
        "ground_truths": "ORM shields database details from developers so they do not have to deal with SQL statements, which improves development efficiency. It also facilitates database migration by providing a SQL-independent interface and handling the differences between databases, so no code changes are required when migrating databases."
    },
    {
        "contexts": "The following sections introduce the basic syntax of ORM in Python, using SQLAlchemy as an example. SQLAlchemy ORM presents a method of associating user-defined Python classes with database tables and instances of those classes (objects) with rows in their corresponding tables. An application can be built using ORM alone.",
        "summary": "The basic syntax of ORM in Python is introduced using SQLAlchemy as an example. SQLAlchemy ORM allows user-defined Python classes to be associated with database tables and instances of those classes with rows in their corresponding tables. An application can be built using only ORM.",
        "question": "How does SQLAlchemy ORM associate Python classes with database tables?",
        "ground_truths": "SQLAlchemy ORM presents a method of associating user-defined Python classes with database tables and instances of those classes (objects) with rows in their corresponding tables."
    },
    {
        "contexts": "Interview methodology is perhaps the oldest of all the social science methodologies. Asking interview participants a series of informal questions to obtain knowledge has been a common practice among anthropologists and sociologists since the inception of their disciplines. Within sociology, the early-20th-century urban ethnographers of the Chicago School did much to prompt interest in the method. Barney Glaser and Anselm Strauss are central figures regarding this method-and interviews and other methods in general- ever since they developed the Grounded Theory approach to qualitative data analysis in the 1960s. Their 1967 landmark book paved the road for the integration of methodologically conducted interviews into sociological research. While Glser & Strauss built on several previously explored approaches, they widely triggered this branch of science, and set off a change considering the role that interviews play ever since in research, creating an altogether new arena in science.",
        "summary": "Interview methodology is a long-standing social science methodology, with its roots in anthropology and sociology. The Chicago School's early-20th-century urban ethnographers sparked interest in this method. Barney Glaser and Anselm Strauss, who developed the Grounded Theory approach to qualitative data analysis in the 1960s, are key figures in this field. Their work led to the integration of methodologically conducted interviews into sociological research and transformed the role of interviews in research.",
        "question": "Who are the central figures in the development of interview methodology in social science?",
        "ground_truths": "Barney Glaser and Anselm Strauss are the central figures in the development of interview methodology in social science."
    },
    {
        "contexts": "Interviews are a form of data gathering. They can be conducted in direct conversation with individuals or groups. This is also possible online or via the phone. It is possible for more than one researcher to conduct the interview together. The necessary equipment includes notes regarding topics of interest that help ask relevant questions, a recording device and paper & pencil or a computer for note taking during the interview.",
        "summary": "Interviews are a method of data collection that can be conducted in person, online, or over the phone, and can involve one or more researchers. The necessary tools include notes on relevant topics, a recording device, and materials for note-taking.",
        "question": "What are the necessary tools for conducting an interview?",
        "ground_truths": "The necessary tools for conducting an interview include notes on relevant topics, a recording device, and materials for note-taking."
    },
    {
        "contexts": "Conducting the interview For an open interview - similar to the Semi-structured Interview - the researcher may prepare a set of questions ahead of the interview. These questions may result from the study design and research questions, but also from literature or prior studies on the respective research topic and specific characteristics of the interviewee. However, different from the Semi-structured Interview, these questions are not strictly pre-formulated but rather represent general topics of interest to the researcher. Before the start of the interview, the interviewer lets the interviewee sign a Voluntary Consent Form, guaranteeing voluntary participation in the research. The interviewer should provide Full Disclosure (information about the main goal of the research and further use of the information), Confidentiality of the Data as well as the right of the interviewees to review the results before publication and to withdraw from the interview without any need for an explanation.",
        "summary": "In an open interview, the researcher prepares a set of questions based on the study design, research questions, literature, prior studies, and the interviewee's characteristics. Unlike a Semi-structured Interview, these questions are not strictly pre-formulated but represent general topics of interest. The interviewer must obtain the interviewee's voluntary consent, provide full disclosure about the research, ensure data confidentiality, and allow the interviewee to review the results before publication and withdraw without explanation.",
        "question": "What are the key differences between an open interview and a semi-structured interview?",
        "ground_truths": "The key differences between an open interview and a semi-structured interview are that in an open interview, the questions are not strictly pre-formulated but represent general topics of interest to the researcher."
    },
    {
        "contexts": "Compared to the more restricted standardized format of surveys, the qualitative interview allows for an open collection and investigation of self-interpretations and situational meanings on the part of the interviewees. This way, theories from psychology and sociology can more easily be empirically explored (3), while also keeping the option open to work independently from initial theories. Open interviews may reproduce existing power structures less strongly, allowing for more open spaces where more information is shared. This highlights the importance of the overall settings, and a clarification of the respective roles, as well as the general research protocol, including ethical concerns. At the same time, due to this focus on the subjective position of the interviewee with regards to his/her/their feelings, attitudes and interpretations, the results gathered in qualitative interviews represent the perspective and contexts of an individual and hence differ in many aspects from data gathered by quantitative surveys (2), often offering what many consider to be a deeper perspective.",
        "summary": "Open interviews allow for a more open collection and investigation of self-interpretations and situational meanings, making it easier to explore theories from psychology and sociology. They may reproduce existing power structures less strongly and allow for more information sharing. The results gathered represent the individual's perspective and contexts, differing from data gathered by quantitative surveys.",
        "question": "How do open interviews differ from quantitative surveys?",
        "ground_truths": "Open interviews allow for a more open collection and investigation of self-interpretations and situational meanings, representing the individual's perspective and contexts, which differs from data gathered by quantitative surveys."
    },
    {
        "contexts": "Language barriers impose problems in understanding the interviewee's statements during the interview and in the transcription process, making it often necessary to hire native speakers, since due to the qualitative nature and the sensitive interview setting a deep understanding of the respective cultural and social contexts is desirable. Pitfalls during the interview: It is crucial that the interviewer is well-acquainted with the conceptual approach and design of the research, which is crucial to maintain the general flow of the interview setting (8). This process imposes high demands on the interviewer. The interviewer must remain attentive and flexible throughout the interview in order to make sure all relevant aspects of the interview guide are profoundly answered. The quality and richness of the data depend on the proficiency in this process (1). In addition, the interviewer must make sure not to impose bias on the interviewee. Therefore, he/she/they should not ask closed yes/no-questions or offer answers for the interviewee to choose from. This open form can be lengthy, at time, which demands a lot of patience form the interviewer. Answers should not be commented or confirmed. The questions should not be judgemental, unexpected or incomprehensible to the interviewee (3, 5).  It is thus recommendable that the interview be rehearsed (as much as possible in view of the open structure) and the interview guide be tested before the first real interview so as to ensure the quality of the interview conduction.",
        "summary": "Language barriers can cause problems in understanding the interviewee's statements, necessitating the hiring of native speakers. The interviewer must be well-acquainted with the research design, remain attentive and flexible, and avoid imposing bias. The open form of the interview can be lengthy and requires patience. It is recommended that the interview be rehearsed and the guide tested to ensure quality.",
        "question": "What are some challenges and recommendations for conducting open interviews?",
        "ground_truths": "Challenges include language barriers and the need for the interviewer to be well-acquainted with the research design, remain attentive and flexible, and avoid imposing bias. Recommendations include rehearsing the interview and testing the guide to ensure quality."
    },
    {
        "contexts": "The amount and depth of data that is gathered in long interviews prohibits a big sample size in terms of number of different interviews. This affects the feasible sample size. Also, the more extensive the interview is, the longer takes the transcription process. Especially longer interviews cannot be replicated endlessly, as opposed to the huge number results possible with standardized surveys. Open interviews therefore tend to rather small samples (1, 2).",
        "summary": "The amount of data gathered in long interviews prohibits a large sample size and affects the feasible sample size. The more extensive the interview, the longer the transcription process. Unlike standardized surveys, longer interviews cannot be replicated endlessly, leading to smaller samples in open interviews.",
        "question": "Why do open interviews tend to have smaller sample sizes?",
        "ground_truths": "Open interviews tend to have smaller sample sizes because the amount of data gathered in long interviews prohibits a large sample size, the transcription process is lengthy, and unlike standardized surveys, longer interviews cannot be replicated endlessly."
    },
    {
        "contexts": "Ordination techniques evolved already more than a century ago in mathematics, and allowed fro a reduction of information that makes these analysis approaches timely up until today. Ordination techniques rely strongly on a profound knowledge of the underlying data format of the respective dataset that is being analysed. Since ordination allow for both inductive and deductive analysis, they often pose a risk for beginners, who typically get confused by the diversity of approaches and the answers these analysis may provide. This conduction is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques. What is more, ordinations are often deeply entangled in disciplinary cultures, with some approaches such as factor analysis being almost exclusive to some disciplines, and other approaches such as principal component analysis being utilised in quite diverse ways within different disciplines.",
        "summary": "Ordination techniques, which have been around for over a century, are used in mathematics to reduce information. They require a deep understanding of the data format being analyzed and can be used for both inductive and deductive analysis. However, they can be confusing for beginners due to the variety of approaches and results they can provide. Ordinations are often intertwined with disciplinary cultures, with some techniques being exclusive to certain disciplines.",
        "question": "What are some challenges beginners might face when using ordination techniques?",
        "ground_truths": "Beginners might get confused by the diversity of approaches and the answers these analysis may provide. This confusion is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques."
    },
    {
        "contexts": "Ordination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches.",
        "summary": "Ordinations are a key component of pattern recognition and are used in many disciplines, including data science. The choice of analysis depends on the data format, whether it's continuous or categorical. It's also important to consider whether the ordination is used as a starting point to inspect the data or as an endpoint in the analysis. Ordinations are useful for skimming through data and revealing results that might not be obtained through other methods.",
        "question": "What factors should be considered when choosing an ordination technique for data analysis?",
        "ground_truths": "The choice of analysis depends on the data format, whether it's continuous or categorical. It's also important to consider whether the ordination is used as a starting point to inspect the data or as an endpoint in the analysis."
    },
    {
        "contexts": "What is an outlier? An outlier is a data point that is significantly different from the other data points in a dataset. Its value is mostly unexpected and deviates a lot from the mean of the dataset. The task of outlier detection is to detect these points. However, one has to consider the contexts and the dataset when selecting a specific method for detecting outliers. Some examples of outliers include: 1. In a dataset of housing prices in a city, a data point representing a house that is significantly more or less expensive than the others could be an outlier. 2. In a dataset of exam scores for a class, a student who scores significantly higher or lower than the rest of the class could be an outlier. 3. In a dataset of consecutive GPS coordinates of sailing activities, coordinates with high deviation from the data points beforehand could be an outlier.",
        "summary": "An outlier is a data point that significantly deviates from the mean of a dataset. Outlier detection involves identifying these points. The contexts and dataset must be considered when choosing a method for detecting outliers. Examples of outliers include a house price significantly different from others in a city dataset, a student's score significantly different from the rest in a class dataset, and GPS coordinates with high deviation in a sailing activity dataset.",
        "question": "What is an outlier and how is it detected in a dataset?",
        "ground_truths": "An outlier is a data point that significantly deviates from the mean of a dataset. Outlier detection involves identifying these points. The contexts and dataset must be considered when choosing a method for detecting outliers."
    },
    {
        "contexts": "Why is it important to detect outliers? Outlier detection is important because it helps to identify unusual or anomalous data points that may not fit with the rest of the data and therefore will have an impact on further analysis of the data. For instance, a linear regression model would be immensely affected by outliers and lose its ability to generalize well over the dataset.",
        "summary": "Detecting outliers is crucial as it helps identify unusual data points that may not fit with the rest of the data, impacting further data analysis. For example, outliers can significantly affect a linear regression model's ability to generalize well over the dataset.",
        "question": "Why is outlier detection important in data analysis?",
        "ground_truths": "Detecting outliers is crucial as it helps identify unusual data points that may not fit with the rest of the data, impacting further data analysis. For example, outliers can significantly affect a linear regression model's ability to generalize well over the dataset."
    },
    {
        "contexts": "What are reasons for outliers? There are many possible reasons why outliers may occur in a dataset. Some common reasons include: 1. Measurement errors: Outliers can occur due to errors in the measurement process, such as incorrect readings or faulty equipment. 2. Data entry errors: Outliers can also occur due to errors in data entry, such as transposing numbers or mistyping data. 3. Natural variations: In some cases, outliers may occur naturally as a result of natural variations in the data. For example, extreme weather events or rare occurrences may result in data points that are significantly different from the rest of the data. 4. Fraud: Outliers can also occur as a result of fraudulent activity, such as attempts to manipulate data or hide certain transactions. 5. Sampling errors: In some cases, outliers may occur due to sampling errors, such as a sample that is not representative of the population being studied.",
        "summary": "Outliers in a dataset can occur due to various reasons including measurement errors, data entry errors, natural variations in the data, fraudulent activity, and sampling errors. These can result from incorrect readings, faulty equipment, transposing numbers, mistyping data, extreme weather events, rare occurrences, attempts to manipulate data, hide certain transactions, or a sample that is not representative of the population being studied.",
        "question": "What are some common reasons for the occurrence of outliers in a dataset?",
        "ground_truths": "Outliers in a dataset can occur due to various reasons including measurement errors, data entry errors, natural variations in the data, fraudulent activity, and sampling errors."
    },
    {
        "contexts": "In this section different methods for detecting outliers in datasets are presented using Python code. To demonstrate the methodology of different approaches, a dataset containing accelerometer data (captured during cycling activity) is used. More files like that can be found under the following Kaggle dataset: [https://www.kaggle.com/datasets/nilshmeier/bike-underground-detection Kaggle Dataset]. In the first place, the dataset is loaded and limited to the first 10 seconds to make the visualizations of the outlier detection results better understandable.",
        "summary": "Different methods for detecting outliers in datasets are demonstrated using Python code and a dataset containing accelerometer data from cycling activity. The dataset is loaded and limited to the first 10 seconds for better visualization of the outlier detection results.",
        "question": "How are different methods for detecting outliers demonstrated?",
        "ground_truths": "Different methods for detecting outliers in datasets are demonstrated using Python code and a dataset containing accelerometer data from cycling activity. The dataset is loaded and limited to the first 10 seconds for better visualization of the outlier detection results."
    },
    {
        "contexts": "It is perfectly normal to feel some kind of nervousness or excitement before an important exam or presentation. This will actually help you to stay focused during the test because of higher adrenalin levels in your body. However, there are cases when the feeling of fear and worrying thoughts are so strong that they inhibit a persons ability to concentrate properly and create strong physical symptoms. These can include headaches, an upset stomach, feelings of fear and dread, sweating, shortness of breath, overthinking, black outs and more. Test anxiety most often occurs shortly before or during an exam or similar test situations. In severe cases it can negatively affect a students overall social life or emotional development, and - of course - the academic career. Therefore it is important to know how to handle feelings of anxiety that may occur in your life as a student at university.",
        "summary": "Nervousness before an exam is normal and can help focus due to increased adrenaline. However, intense fear and worry can inhibit concentration and cause physical symptoms like headaches, upset stomach, and more. Test anxiety usually occurs before or during an exam and can negatively impact a student's social life, emotional development, and academic career.",
        "question": "What are the effects of severe test anxiety on a student?",
        "ground_truths": "Severe test anxiety can inhibit a student's ability to concentrate, cause physical symptoms, and negatively impact their social life, emotional development, and academic career."
    },
    {
        "contexts": "There are several strategies that can help you stay calm before or during an exam and you can practice them! This will help you to get out of the vicious cycle exam anxiety can create: Fear of failure and physical discomfort during an exam might lead to lower marks which can convince the student of his self-attributed inability to master the subject. In turn this can create even more anxiety. Here is what you can do to create positive change: For some it might already help to pay more attention to their overall lifestyle. Getting enough sleep, proper nutrition, and regular exercise strengthen the body's resources and ability to deal with stressful situations, which is not only beneficial for exams but for life in general. Additional stress-management practices like meditation, mindfulness, breath work and study routines will enable you to actively calm your mind and body in your day to day life. Using this knowledge before a test can help you get back into a quiet, focused, and calm state.",
        "summary": "Strategies to stay calm before or during an exam can break the cycle of exam anxiety. Fear of failure and discomfort during an exam can lead to lower marks and more anxiety. Paying attention to lifestyle, getting enough sleep, proper nutrition, and regular exercise can help manage stress. Practices like meditation, mindfulness, breath work, and study routines can help calm the mind and body.",
        "question": "What strategies can help manage exam anxiety?",
        "ground_truths": "Strategies to manage exam anxiety include paying attention to lifestyle, getting enough sleep, proper nutrition, regular exercise, and stress-management practices like meditation, mindfulness, breath work, and study routines."
    },
    {
        "contexts": "In short: Partial Correlation is a method to measure the degree of association between two variables, while controlling for the impact, a third variable has on both. When looking at correlations, they can often be misleading and must be treated with care. For example, this website provides a list of strongly significant correlations that do not seem to have a meaningful and direct connection. This undermines that correlation is not necessarily related to causality. On the one hand, this may be due to the principle of null hypothesis testing (see p-value). But in the case of correlation, strong significances can often be caused by a relationship to a common third variable and are therefore biased.",
        "summary": "Partial Correlation is a technique used to measure the relationship between two variables while controlling for the effect of a third variable. Correlations can be misleading and are not necessarily related to causality. Strong correlations can often be due to a relationship to a common third variable, making them biased.",
        "question": "What is the purpose of using Partial Correlation?",
        "ground_truths": "The purpose of using Partial Correlation is to measure the degree of association between two variables, while controlling for the impact a third variable has on both."
    },
    {
        "contexts": "To use partial correlation and gain valid results, some assumptions must be fulfilled. No worries if these assumptions are violated! There is often still a work-around that can be done to bypass them. Both variables and the third variable to control for (also known as control variable or covariate) must be measured on a continuous scale. There needs to be a linear relationship between all three variables. This can be inspected by plotting every possible pair of variables. There should be no significant outliers. Partial correlation is sensitive to outliers, because they have a large effect on the regression and correlation coefficients. Determining if a point is really a significant outlier, is most likely based on personal experience and can therefore be ambiguous. All variables should be approximately normally distributed.",
        "summary": "For valid results from partial correlation, certain assumptions must be met. The variables must be measured on a continuous scale, have a linear relationship, and be approximately normally distributed. The method is sensitive to outliers, which can significantly affect the regression and correlation coefficients.",
        "question": "What are the assumptions that need to be met for using partial correlation?",
        "ground_truths": "The assumptions that need to be met for using partial correlation are that both variables and the third variable to control for must be measured on a continuous scale, there needs to be a linear relationship between all three variables, there should be no significant outliers, and all variables should be approximately normally distributed."
    },
    {
        "contexts": "To check if a connection between two variables is caused by a common third variable, we test whether the relationship still exists after we have accounted for the influence of the third variable. This can be achieved by calculating two simple linear regressions to predict each of the two variables based on the expected third variable. Thus, we account for the impact the third variable has on the variance of each of our two variables. That way we can now consider the left-over (unexplained) variance - namely the residuals of the regressions. To better understand, why we are doing this, let\u2002s look at an individual data point of the regressions. If this point exceeds the predictions for both regressions equally, then there seems to be a remaining connection between both variables. So, when looking back at the broad picture of every data point, this means that if the residuals of both regressions are unrelated, then the relationship of the two variables could be explained by the third variable.",
        "summary": "To determine if a relationship between two variables is due to a third variable, we calculate two linear regressions to predict each variable based on the third. We then consider the residuals of the regressions. If the residuals are unrelated, the relationship between the two variables could be explained by the third variable.",
        "question": "How do we determine if a relationship between two variables is due to a third variable?",
        "ground_truths": "We determine if a relationship between two variables is due to a third variable by calculating two linear regressions to predict each of the two variables based on the expected third variable. If the residuals of both regressions are unrelated, then the relationship of the two variables could be explained by the third variable."
    },
    {
        "contexts": "When thinking about partial correlation, one might notice that the basic idea is similar to a multiple linear regression. Even though a correlation is bidirectional while a regression aims at explaining one variable by another, in both approaches we try to identify the true and unbiased relationship of two variables. So when should we use which method? In general, partial correlation is a building step for multiple linear regression. Accordingly, linear regression has a broader range of applications and is therefore in most cases the method to choose. Using a linear regression, one can model interaction terms and it is also possible to loosen some assumptions by using a generalized linear model. Nevertheless, one advantage, partial correlation has over linear regression, is that it delivers an estimate for the relationship which is easy to interpret. The linear regression outputs weights that depend on the unit of measurement, making it hard to compare them to other models. As a work-around, one could use standardized beta coefficients. Yet those coefficients are harder to interpret since they do not necessarily have an upper bound. Partial correlation coefficients, on the other hand, are defined to range from -1 to 1.",
        "summary": "Partial correlation and multiple linear regression are similar in that they both aim to identify the true relationship between two variables. However, linear regression is generally preferred due to its broader range of applications. One advantage of partial correlation is that it provides an easy-to-interpret estimate of the relationship, with coefficients ranging from -1 to 1.",
        "question": "What is the advantage of using partial correlation over linear regression?",
        "ground_truths": "The advantage of using partial correlation over linear regression is that it delivers an estimate for the relationship which is easy to interpret, with coefficients defined to range from -1 to 1."
    },
    {
        "contexts": "Persona Building is a tool that helps better understand the target group when designing a product or service. In Persona Building, a specific fictional user is imagined with his or her experiences, needs and characteristics, and the prospective product or service can be developed more specifically for this persona. This is mostly done in business for marketing purposes, where the personas are potential customers or people to pitch an idea to. It can nevertheless also be helpful for any design process that aims to fulfill the needs of a specific audience (see [[Design Thinking]])",
        "summary": "Persona Building is a method used to understand the target audience when creating a product or service. It involves creating a fictional user with specific experiences, needs, and characteristics. This tool is often used in business for marketing purposes, but can also be useful in any design process.",
        "question": "What is the purpose of Persona Building?",
        "ground_truths": "The purpose of Persona Building is to better understand the target group when designing a product or service. It involves creating a fictional user with specific experiences, needs, and characteristics. This tool is often used in business for marketing purposes."
    },
    {
        "contexts": "Persona Building starts with the '''gathering of data''' about the target groups. This can be [[Glossary|data]] that already exists, such as large-scale surveys, information on potential user groups such as the [https://www.sinus-institut.de/sinus-loesungen/sinus-milieus-deutschland/ SINUS milieus], or observations made in terms of the audiences of comparable products and services. It can also emerge from knowledge and experiences within the team, or be original data gathered for the specific design purpose, if the resources are available for that.",
        "summary": "The process of Persona Building begins with collecting data about the target groups. This data can come from existing sources like surveys, information on potential user groups, or observations of similar products and services. It can also come from the team's knowledge and experiences, or be original data collected specifically for the design purpose.",
        "question": "How does the process of Persona Building start?",
        "ground_truths": "The process of Persona Building starts with the gathering of data about the target groups. This data can come from existing sources like surveys, information on potential user groups, or observations of similar products and services. It can also come from the team's knowledge and experiences, or be original data collected specifically for the design purpose."
    },
    {
        "contexts": "Each created persona should possibly include a '''drawing or picture''' of the imagined person and his or her '''name'''. This way, the persona becomes more vivid and recognizable within the project. Furthermore, the persona should be described in terms of his or her '''characteristics''': his or her background, hobbies and job, lifestyle, family situation, needs, problems, preferences and desires. This helps understand how and why the person would be interested in the to-be-designed product or service, and how it can be adapted and improved to better fit the persona's circumstances. The designers should try to understand their offer from the persona's perspective.",
        "summary": "Each persona should ideally include a drawing or picture and a name to make it more vivid and recognizable. The persona should also be described in terms of their characteristics, such as background, hobbies, job, lifestyle, family situation, needs, problems, preferences, and desires. This helps designers understand how and why the persona would be interested in the product or service, and how it can be adapted to better fit the persona's circumstances.",
        "question": "What should each created persona include and why?",
        "ground_truths": "Each created persona should include a drawing or picture and a name to make it more vivid and recognizable. The persona should also be described in terms of their characteristics, such as background, hobbies, job, lifestyle, family situation, needs, problems, preferences, and desires. This helps designers understand how and why the persona would be interested in the product or service, and how it can be adapted to better fit the persona's circumstances."
    },
    {
        "contexts": "Lastly, the persona should be '''implemented into the design process'''. From this point on, all team members should know about the persona and speak in terms of his or her specific needs and actions instead of speaking about broad needs of a potential target audience. The persona can be thrown into imagined scenarios and situations to understand how he or she would react. This way, the design process becomes more user-focused. Also, all team members, which may come from different disciplines or departments, now have a common reference point to speak about.",
        "summary": "The persona should be integrated into the design process. All team members should be familiar with the persona and focus on their specific needs and actions rather than the general needs of the target audience. The persona can be used in hypothetical scenarios to understand their reactions, making the design process more user-focused and providing a common reference point for all team members.",
        "question": "How should the persona be used in the design process?",
        "ground_truths": "The persona should be integrated into the design process. All team members should be familiar with the persona and focus on their specific needs and actions. The persona can be used in hypothetical scenarios to understand their reactions, making the design process more user-focused and providing a common reference point for all team members."
    },
    {
        "contexts": "Let us develop an example. Imagine you are attempting to develop a solution for smart mobility in the city. You may be working in the city planning department, or in a start-up that tries to develop new technologically or socially innovative solutions. You do know what the general service should be - something that makes cycling more attractive in cities -, but you are unsure of the specificities. '''Now Persona Building comes into play.''' You develop a handful of Personas based on the target audience you want to provide with new opportunities for a change in their mobility behaviour. So, first of all, we think of Lisa. Lisa is 30 years old and has a six year old daughter, Nele. Lisa works at an insurance firm in a middle-sized town and rides the bike to work. She likes cycling, also with her daughter, but the city only has bikelanes for some parts of her commute and into town. So, Lisa is frustrated about the stressful interactions with cars, as well as concerned about her own and her daughter's safety. Lisa would be ready to pay some money for a good solution, but she doesn't want to give up cycling.",
        "summary": "For example, if you're trying to develop a solution for smart city mobility, you might use Persona Building. You create personas based on your target audience, such as Lisa, a 30-year-old mother who works at an insurance firm and cycles to work. She likes cycling but is frustrated with the lack of bike lanes and concerned about safety. She would be willing to pay for a good solution but doesn't want to give up cycling.",
        "question": "How can Persona Building be applied in developing a solution for smart city mobility?",
        "ground_truths": "In developing a solution for smart city mobility, Persona Building can be applied by creating personas based on the target audience. For example, a persona named Lisa can be created who is a 30-year-old mother who works at an insurance firm and cycles to work. She likes cycling but is frustrated with the lack of bike lanes and concerned about safety. She would be willing to pay for a good solution but doesn't want to give up cycling."
    },
    {
        "contexts": "A probability distribution summarizes the probabilities for the values of a random variable. The general properties/features that define a distribution include the four mathematical moments: Expected value E(X), Variance Var(X), Skewness, and Kurtosis. Each random variable has its own probability distribution that may have a similar shape to other variables, however, the structure will differ based on whether the variable is discrete or continuous, since probability distributions of continuous variables have an infinite number of values and probability functions of discrete variables assign a probability to each concrete data point.",
        "summary": "Probability distributions represent the probabilities of values of a random variable. They are defined by four mathematical moments: Expected value, Variance, Skewness, and Kurtosis. The structure of a distribution varies depending on whether the variable is discrete or continuous.",
        "question": "What are the four mathematical moments that define a probability distribution?",
        "ground_truths": "The four mathematical moments that define a probability distribution are Expected value E(X), Variance Var(X), Skewness, and Kurtosis."
    },
    {
        "contexts": "Poisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. Poisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.",
        "summary": "Poisson Distribution is a discrete probability distribution. It can be used when data is counts of events, the events are random and independent, and the mean number of events in a specific time frame is constant and known.",
        "question": "What are the three conditions under which Poisson Distribution can be used?",
        "ground_truths": "Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known."
    },
    {
        "contexts": "The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time. Introducing lambda, \u03bb, as a rate parameter (events/time)*T into the equation gives us. So, \u03bb is basically the expected number of event occurrences in an interval, a function of number of events and the time interval. Changing \u03bb means changing the probability of event occurrence in one interval.",
        "summary": "The Poisson distribution probability mass function (pmf) calculates the probability of observing a certain number of events in a given time period. The rate parameter, \u03bb, represents the expected number of event occurrences in an interval, and changing \u03bb alters the probability of event occurrence in that interval.",
        "question": "What does the rate parameter, \u03bb, represent in the Poisson distribution probability mass function?",
        "ground_truths": "In the Poisson distribution probability mass function, the rate parameter, \u03bb, represents the expected number of event occurrences in an interval."
    },
    {
        "contexts": "Using Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.",
        "summary": "The Pomodoro technique is useful for maintaining focus on tasks, minimizing distractions, and combating procrastination.",
        "question": "Why is the Pomodoro technique beneficial?",
        "ground_truths": "The Pomodoro technique is beneficial because it helps maintain focus on tasks, minimize distractions, and combat procrastination."
    },
    {
        "contexts": "Pomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. Pomodoro is very simple. All you need is work to be done and a timer. There are six steps in the technique: Decide on the task to be done. Set the pomodoro timer (traditionally to 25 minutes = 1 Pomodoro). Work on the task. End work when the timer rings (Optionally: put a checkmark on a piece of paper). If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2. After four pomodoros, take a longer break (15\u201330 minutes), reset your checkmark count to zero, then start again at step 1.",
        "summary": "The Pomodoro technique is a simple method for self-organization and productivity that involves working in timed intervals, taking short breaks after each interval, and taking a longer break after four intervals.",
        "question": "How does the Pomodoro technique work?",
        "ground_truths": "The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again."
    },
    {
        "contexts": "Poster, usually a large sheet of paper with a designed combination of text, graphs, color, etc., is one of the cost-effective means used to convey information to a large audience. There are different types of poster for various settings, such as advertising, event, politics, for motivational and scientific uses and many more. You may encounter the need to create a poster for a presentation at University or at a Conference. Informative posters are oftentimes commercial ones of a product or services hung in front of stores, whereas formative ones are to raise awareness or speak for a good cause.",
        "summary": "Posters are cost-effective tools for conveying information to a large audience. They come in various types for different settings like advertising, events, politics, motivational and scientific uses. They can be informative, promoting products or services, or formative, raising awareness for a cause.",
        "question": "What are the different uses of posters?",
        "ground_truths": "Posters are used in various settings such as advertising, events, politics, motivational and scientific uses. They can be used to promote products or services, or to raise awareness for a cause."
    },
    {
        "contexts": "The target audience should be at the center when designing a poster, as not only the content itself but other elements in the poster can be tailor to best reach its recipients. An example compares two different posters both advertising for a fundraising event. The poster on the left for the Pride Color Run event looks vibrant, lively and is targeted to a younger demographic. The right poster is made for a charity dinner and has a more structured and professional layout, which normally attracts the older, established audience. Many differences in color scheme, typography, layout, styling and image use can be spotted between these two posters of the same cause, which emphasizes the importance of identifying who a poster should speak to and how to effectively draw their attention.",
        "summary": "When designing a poster, the target audience should be the focus. The content and other elements of the poster should be tailored to best reach its recipients. For example, a vibrant, lively poster may attract a younger demographic, while a more structured, professional layout may appeal to an older, established audience.",
        "question": "Why is the target audience important in poster design?",
        "ground_truths": "The target audience is important in poster design because the content and other elements of the poster should be tailored to best reach its recipients. A poster should effectively draw the attention of its intended audience."
    },
    {
        "contexts": "Each color creates a unique kind of energy that could reflect the poster\u2019s message, may it be bold, subtle, agony or ecstasy. Choosing the right color scheme will set a tone for your statement to shine, help drawing attention from the audience from afar, create a subliminal association between then and the topic even before they start reading the text. The actual color shade is also very important. It may come to you as a surprise, but not only gray has (more than) fifty shade. The point is, it is important to take this into account when creating a color palette for your poster. Different shades combination will create different effects even though the rough colors stay them same.",
        "summary": "Color choice in a poster can reflect its message and draw attention from the audience. The right color scheme can set a tone for the statement and create a subliminal association with the topic. The actual shade of the color is also important as different shades can create different effects.",
        "question": "How does color choice impact a poster's design?",
        "ground_truths": "Color choice in a poster can reflect its message, draw attention from the audience, set a tone for the statement, and create a subliminal association with the topic. The actual shade of the color is also important as different shades can create different effects."
    },
    {
        "contexts": "Typography is an art itself. There is so much more you can convey with the use of fonts and arrangement of texts, sometimes incorporating illustrations and images. It is an integral part to the poster as a final product, and if done right, makes a unique lasting visual impact to the audience.",
        "summary": "Typography, the use of fonts and arrangement of texts, is an art that can convey more than just words. It is an integral part of a poster and, if done right, can make a unique lasting visual impact on the audience.",
        "question": "What role does typography play in poster design?",
        "ground_truths": "Typography, the use of fonts and arrangement of texts, is an integral part of a poster. It can convey more than just words and, if done right, can make a unique lasting visual impact on the audience."
    },
    {
        "contexts": "A lot of times: less is more. With a poster you only have limited space, so deciding what\u2019s making it there and what not is crucial. Negative space, or white space, is the area of the layout that is purposefully left empty. It serves as a breathing room for all other elements on the poster. As mentioned at the beginning, your message has to stand out. But if everything on the poster is so crowded in technicolor, your message becomes a leaf in the forest. That\u2019s why it is important to selectively pick out some crucial elements and make them eye-catching, while creating a background with plenty of room for your message to pop.",
        "summary": "In poster design, less is often more. With limited space, it's crucial to decide what to include and what to leave out. Negative space, or white space, is purposefully left empty to give other elements room to breathe. It's important to make crucial elements eye-catching and allow the message to stand out.",
        "question": "Why is negative space important in poster design?",
        "ground_truths": "Negative space, or white space, is important in poster design because it gives other elements room to breathe. It allows the message to stand out and makes crucial elements eye-catching."
    },
    {
        "contexts": "Knowing the market value of an overnight stay in an Airbnb home is of high interest for both, renters (or hosts) and guests. As a guest one might be a bit overwhelmed going through all the offers Airbnb nowadays contains (approximately 51.000 for the city of New York as of November 2018). Knowing exactly what drives prices of Airbnb listings could make the whole search process more enjoyable. Setting the right price on the other hand is a crucial tasks for the hosts, as they want neither, their listing not being booked, nor to rent it out too cheap. To tackle these problems, this report aims at identifying the determinants of prices for Airbnb listings. More precisely, the following questions will be answered throughout the analysis: How do hosts promote their listing? How are expensive/cheap listings being promoted? What areas are the most expensive ones? Does this correlate with the location score? Are there any seasonal effects on the price? What are the overall most important determinants for the price?",
        "summary": "Understanding the market value of Airbnb accommodations is important for both hosts and guests. This report aims to identify the factors that determine Airbnb prices, including how listings are promoted, the most expensive areas, correlation with location score, seasonal effects, and the most significant price determinants.",
        "question": "What are the key factors that determine the price of Airbnb accommodations?",
        "ground_truths": "The key factors that determine the price of Airbnb accommodations include how the listings are promoted, the location of the property, the location score, seasonal effects, and other significant determinants."
    },
    {
        "contexts": "Airbnb itself does only provide a very limited insight into their data, but a separate group named Inside Airbnb scrapes and compiles publicly available information about many cities' listings from the Airbnb website. For this report, data for the city of New York from the year 2018 was been analysed. The dataset can be accessed through this link. Otherwise, more up-to-date datasets from the same website can be used for this analysis, making sure the features described and used for it appear in the tables. The current dataset comprises of two tables: listings and calendar. The listings table contains the unique listings offered on Airbnb for New York at a particular time particular time. It has been scraped on the third and fourth of November, 2018. The data consist of 50968 entries (unique listings) and 96 features. The single identifier for a listing is the id. Interesting features are, among others, the location (zipcode) of the listing, it's overall score on reviews (review_scores_rating) and the type of accommodation (room_type).",
        "summary": "Airbnb provides limited data, but a group called Inside Airbnb compiles publicly available information about listings. This report analyzed 2018 data for New York City, available through a link. The dataset includes two tables: listings and calendar. The listings table, scraped in November 2018, contains 50968 unique listings and 96 features, including location, overall review score, and accommodation type.",
        "question": "What information is contained in the 'listings' table of the Airbnb dataset?",
        "ground_truths": "The 'listings' table of the Airbnb dataset contains unique listings offered on Airbnb for New York at a particular time. It includes 50968 entries and 96 features, including the location (zipcode) of the listing, its overall score on reviews (review_scores_rating), and the type of accommodation (room_type)."
    },
    {
        "contexts": "Principal Component Analyses are helpful when you have a lot of different data samples with a variety of variables. For example, the following dataset which contains different nutrient measurements in various pizzas from different pizza brands. How can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.",
        "summary": "Principal Component Analysis (PCA) is useful when dealing with large datasets with many variables, such as nutrient measurements in pizzas from different brands. It helps to represent this high-dimensional data in a concise and understandable way, overcoming the 'curse of dimensionality'.",
        "question": "What is the role of Principal Component Analysis in handling large datasets with many variables?",
        "ground_truths": "Principal Component Analysis is used to represent large datasets with many variables in a concise and understandable way."
    },
    {
        "contexts": "This term was coined by Richard R. Bellman, an American applied mathematician. As the number of features / dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. Principle Component Analysis helps to alleviate this problem.",
        "summary": "The 'curse of dimensionality', a term coined by Richard R. Bellman, refers to the challenges of dealing with high-dimensional data in machine learning. As the number of features increases, data points become sparse and distant, making learning more difficult and error-prone. This also complicates data visualization and interpretation, and increases computational costs. Principal Component Analysis is a solution to this problem.",
        "question": "What is the 'curse of dimensionality' and how does Principal Component Analysis help to address it?",
        "ground_truths": "The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation. Principal Component Analysis helps to address this problem by reducing the dimensionality of the data."
    },
    {
        "contexts": "Principle Component Analysis is one of the foundational methods to combat the curse of dimensionality. It is an unsupervised learning algorithm whose goals is to reduce the dimensionality of the data, condensing its entirety down to a low number of dimensions (also called principle components, usually two or three). Although it comes with a cost of losing some information, it makes data visualization much easier, improves the space and time complexity required for machine learning algorithms tremendously, and allows for more intuitive intepretation of these models. PCA can also be categorized a feature extraction techniques, since it creates these principle components - new and more relevant features - from the original ones.",
        "summary": "Principal Component Analysis (PCA) is a foundational method to combat the curse of dimensionality. It is an unsupervised learning algorithm that reduces the dimensionality of data to a few principle components. Despite some information loss, it simplifies data visualization, improves computational efficiency, and allows for more intuitive interpretation of models. PCA can also be seen as a feature extraction technique, creating new, more relevant features from the original ones.",
        "question": "What are the benefits and drawbacks of using Principal Component Analysis?",
        "ground_truths": "The benefits of using Principal Component Analysis include simplifying data visualization, improving computational efficiency, and allowing for more intuitive interpretation of models. It can also be seen as a feature extraction technique. The drawback is that it may result in some loss of information."
    },
    {
        "contexts": "Oftentimes the features in the data are measured on different scales. This step makes sure that all features contribute equally to the analysis. Otherwise, variables with large range will trump thoses with smaller range (for example: a time variable that ranges between 0ms and 1000ms with dominate over a distance variable that ranges between 0m and 10m). Each variable can be scaled by subtracting its mean and dividing by the standard deviation (this is the same as calculating the z-score, and in the end, all variables with have the same mean 0 and standard deviation of 1).",
        "summary": "In data analysis, features often have different scales. To ensure all features contribute equally, they are standardized by subtracting their mean and dividing by their standard deviation. This prevents variables with larger ranges from dominating those with smaller ranges.",
        "question": "Why is it necessary to standardize features in data analysis?",
        "ground_truths": "Standardizing features in data analysis is necessary to ensure all features contribute equally to the analysis and prevent variables with larger ranges from dominating those with smaller ranges."
    },
    {
        "contexts": "The covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties: The size of the matrix is equal to the number of features in the data. The main diagonal on the matrix contains the variances of each initial variables. The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2). The covariance matrix gives you a summary of the relationship among the initial variables. A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa). A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa).",
        "summary": "The covariance matrix is a square matrix that represents the covariance of pairs of original features. Its size equals the number of features, its main diagonal contains the variances of each variable, and it is symmetric. It summarizes the relationships among variables, with positive values indicating direct proportionality and negative values indicating inverse proportionality.",
        "question": "What is a covariance matrix and what does it represent in data analysis?",
        "ground_truths": "A covariance matrix is a square matrix that represents the covariance of pairs of original features. It summarizes the relationships among variables in data analysis, with positive values indicating direct proportionality and negative values indicating inverse proportionality."
    },
    {
        "contexts": "The Q methodology was developed in 1935 at the University of London by William Stephenson in an attempt to challenge the logic of \u201ctesting\u201d that was and is predominant in psychology. However, in Stephenson`s opinion, psychology had not yet reached a \u201csophisticated theoretical status\u201d (such as e.g. physics had) which would have enabled a deductive mode of theory-testing. Therefore, he proclaimed a more inductive approach to derive new discoveries before moving on to testing theories (Watts and Stenner 2005). Therefore, he developed the Q methodology as a qualitative and inductive method to identify groups of people who share the same points of view on a certain topic (Stephenson 1935; Watts & Stenner 2005). In the beginning, Q methodology was only used in psychology, but recently it is used in various research communities, e.g. nutrition research (see Dennis & Goldberg 1996), biodiversity management (see Hamadou et al. 2016) as well as research on the perceptions of ecosystem services (see Baral et al. 2017; Milcu et al. 2014).",
        "summary": "The Q methodology, developed in 1935 by William Stephenson at the University of London, was an attempt to challenge the prevalent logic of 'testing' in psychology. Stephenson believed that psychology had not yet reached a 'sophisticated theoretical status' and thus advocated for an inductive approach to derive new discoveries before testing theories. The Q methodology is a qualitative and inductive method used to identify groups of people with similar viewpoints on a specific topic. Initially used only in psychology, it is now used in various research fields such as nutrition research, biodiversity management, and research on perceptions of ecosystem services.",
        "question": "Who developed the Q methodology and what is its purpose?",
        "ground_truths": "The Q methodology was developed by William Stephenson at the University of London in 1935. Its purpose is to identify groups of people who share the same points of view on a certain topic."
    },
    {
        "contexts": "The Q methodology is applied to identify different perspectives on a topic that occur in a population. This is done by identifying groups of people who sort a pool of items in a comparable way (Watts and Stenner 2005). However, this method does not allow generalised statements about the whole population the participants are from. Instead, Q methodology only allows us to identify the main perspectives or perceptions that can be found in this population. The process of the Q methodology follows the standard process of research. First, the data is gathered by letting participants sort a number of items called \u201cQ items\u201d (e.g. cards with statements or pictures) according to their preference from high to low. Second, data is analysed by correlating the resulting configurations, called \u201cQ sorts\u201d, with each other and performing a factor analysis. Third, the results of the factor analysis are interpreted.",
        "summary": "The Q methodology is used to identify varying perspectives on a topic within a population by identifying groups of people who sort a pool of items similarly. This method does not allow for generalized statements about the entire population, but rather identifies the main perspectives or perceptions within it. The process involves gathering data by having participants sort 'Q items' according to their preference, analyzing the data by correlating the resulting 'Q sorts' and performing a factor analysis, and interpreting the results of the factor analysis.",
        "question": "How is the Q methodology applied and what is its process?",
        "ground_truths": "The Q methodology is applied by identifying groups of people who sort a pool of items in a similar way to identify different perspectives on a topic within a population. The process involves gathering data by having participants sort 'Q items' according to their preference, analyzing the data by correlating the resulting 'Q sorts' and performing a factor analysis, and interpreting the results of the factor analysis."
    },
    {
        "contexts": "The Q set is compiled of the Q items that the participants have to sort. The Q items can be statements that the participants agree or disagree with to a certain extent or pictures that they like more or less. The Q items can be everything that can be brought into an order of preference. In early studies, Q methodology was even conducted with bottled fragrances (Stephenson 1936). The Q set is generated based on the research question. This means that the research question needs to be clear before generating the Q set. The research question should be straightforward and easily understandable for the participants. The Q set is supposed to enable the participants to answer the research question. Therefore, the Q items need to be as heterogeneous as possible to cover a broadly representative range of perspectives. An ideal Q set comprises between 40 and 80 items (Stainton Rogers 1995). The Q items need to be selected very carefully to find items that do not overlap, but at the same time no perspective should be missing. Therefore, the selection process takes the most time and effort of all the steps of the Q methodology. It is suggested to first create an overly large set of Q items and then reduce this number by doing pilot tests.",
        "summary": "The Q set, which is compiled of Q items that participants sort, can be statements or pictures that participants agree or disagree with to varying degrees. The Q set is generated based on the research question, which should be clear and straightforward. The Q items should be as diverse as possible to cover a wide range of perspectives. An ideal Q set contains between 40 and 80 items. The selection process, which involves careful selection of items that do not overlap and ensuring no perspective is missing, is the most time-consuming and effort-intensive part of the Q methodology. It is recommended to initially create a large set of Q items and then reduce this number through pilot tests.",
        "question": "What is a Q set and how is it generated in the Q methodology?",
        "ground_truths": "A Q set is compiled of Q items that participants have to sort, which can be statements or pictures that participants agree or disagree with to varying degrees. It is generated based on the research question, which should be clear and straightforward. The Q items should be as diverse as possible to cover a wide range of perspectives, and an ideal Q set contains between 40 and 80 items. The selection process involves careful selection of items that do not overlap and ensuring no perspective is missing."
    },
    {
        "contexts": "As a rule of thumb, t 40 to 60 participants  in a Q methodological study are proposed to be an appropriate sample size (Stainton Rogers 1995). However, Q studies can also be carried out with fewer participants. Being a qualitative method, the number of participants is less important than the constitution of the participant group. To get a wide range of perspectives, it might make sense to strategically sample participants from different backgrounds. For instance, it makes sense to include experts who might have a pivotal opinion on a topic. The optimal relation between Q items and participants is described as 2:1 for classic Q studies (Watts & Stenner 2012, cited after Burkhardt 2017).",
        "summary": "In a Q methodological study, 40 to 60 participants are typically considered an appropriate sample size. However, fewer participants can also be used. The number of participants is less significant than the composition of the participant group in this qualitative method. To capture a wide range of perspectives, participants may be strategically sampled from different backgrounds, including experts with pivotal opinions on the topic. The optimal ratio of Q items to participants is 2:1 for classic Q studies.",
        "question": "What is the typical sample size in a Q methodological study and what is the optimal ratio of Q items to participants?",
        "ground_truths": "In a Q methodological study, 40 to 60 participants are typically considered an appropriate sample size. The optimal ratio of Q items to participants is 2:1 for classic Q studies."
    },
    {
        "contexts": "In classic Q studies, the distribution matrix follows a normal distribution from -6 to +6. In this case, Q items that are ranked lowest by participants are placed at -6 whereas items that are highly preferred by participants are placed at +6. Items that are ranked at 0 would be the ones that the participants feel indifferent towards. Just as in a normal distribution, the extreme values -6 and +6 can be assigned the lowest number of items and the middle value 0 can be assigned  the most (Figure 1).",
        "summary": "In classic Q studies, the distribution matrix follows a normal distribution from -6 to +6. Q items ranked lowest by participants are placed at -6, while items highly preferred are placed at +6. Items ranked at 0 are those that participants feel indifferent towards. Like in a normal distribution, the extreme values -6 and +6 are assigned the fewest items, and the middle value 0 is assigned the most.",
        "question": "How is the distribution matrix arranged in classic Q studies?",
        "ground_truths": "In classic Q studies, the distribution matrix follows a normal distribution from -6 to +6. Q items ranked lowest by participants are placed at -6, while items highly preferred are placed at +6. Items ranked at 0 are those that participants feel indifferent towards. The extreme values -6 and +6 are assigned the fewest items, and the middle value 0 is assigned the most."
    },
    {
        "contexts": "Building on the general recognition that current methods are not enough to approximate knowledge towards the solution needed for the wicked problems which we face, the questions arises how we can meaningfully question the ''status quo'' in methods, and do this in a way that moves us forward. Too many enthusiastic questions on the ''status quo'' in the realms of [[Glossary|scientific methods]] have been proposed without a clear understanding what is actually the problem, or better, which knowledge we are lacking. The current state of many lines of thinking urged us to move out of the dimensions of a normal science in the sense of Kuhn. However, calling out a revolution does not mean that all problems are automatically solved. On the contrary: concerning many aspects, we are still in a state of explicitly not knowing how to solve the wicked problems we face.",
        "summary": "The current scientific methods are insufficient to solve the complex problems we face today. There is a need to question the status quo in methods, but this should be done with a clear understanding of the problem and the knowledge we lack. Despite calls for a revolution in science, many problems remain unsolved and we are still in a state of not knowing how to address them.",
        "question": "Why is there a need to question the status quo in scientific methods?",
        "ground_truths": "There is a need to question the status quo in scientific methods because the current methods are insufficient to solve the complex problems we face today. Despite calls for a revolution in science, many problems remain unsolved and we are still in a state of not knowing how to address them."
    },
    {
        "contexts": "Climate change is a complex phenomenon: at first, the realisation that human-induced climate change is happening was slowly emerging out of diverse scientific data sources and research approaches. While today, the majority of society has accepted that human-induced climate change is real, and that it is our responsibility to react and counteract, there is no general agreement on a best way to do so. Instead, there are calls for a globally orchestrated policy-response, which takes place are on a totally different scale compared to local adaptation. Most importantly, how can we convince citizens in countries which lead in terms of a negative contribution to climate change for decades to not only change their behaviour, but actually to contribute to reversing its catastrophic effects? This is the current frontier in research, and many promising suggestions and strategies are currently being investigated.",
        "summary": "Human-induced climate change is a complex issue that emerged from various scientific data and research. While it's widely accepted and there's a call for global response, there's no consensus on the best approach. The challenge lies in convincing citizens in high-contributing countries to change their behavior and contribute to reversing the effects. This is a current research focus with many promising strategies being explored.",
        "question": "What is the current challenge in addressing climate change?",
        "ground_truths": "The current challenge in addressing climate change lies in convincing citizens in high-contributing countries to change their behavior and contribute to reversing the effects. This is a current research focus with many promising strategies being explored."
    },
    {
        "contexts": "Research about normative challenges - as well as research about joined problem framing between actors and researchers - has been on the rise. More and more studies engage in the new contract between science and society. However, the roles and [[Glossary|power]] relations of different actors within a system are deeply contextual, and so far the knowledge of such studies did not yet saturate into a more general understanding on how such study setting can be approached. While blueprints already exist and there is a growing understanding of relevant concepts, such as social learning, actor participation is still something that did not find its way into a broad diversity of textbooks, and available approaches are far from unified.",
        "summary": "Research on normative challenges and joint problem framing has increased, engaging in a new contract between science and society. However, the roles and power relations of actors are deeply contextual and understanding of such studies is not yet widespread. Despite existing blueprints and growing understanding of concepts like social learning, actor participation is not widely incorporated in textbooks and approaches are not unified.",
        "question": "What is the current state of research on normative challenges and actor participation?",
        "ground_truths": "The current state of research on normative challenges and actor participation is that it has increased and is engaging in a new contract between science and society. However, the roles and power relations of actors are deeply contextual and understanding of such studies is not yet widespread. Despite existing blueprints and growing understanding of concepts like social learning, actor participation is not widely incorporated in textbooks and approaches are not unified."
    },
    {
        "contexts": "The question how we shift our consumption towards being more sustainable is another thriving debate within sustainability science and beyond. While there is research focusing on global trade and its inequalities, there is equally research on individual behaviour and the motivations of consumers. Understanding behavior - and even more so - driving behaviour change in terms of sustainable consumption is to date a diverse field, with methodological roots in psychology, social science and many other domains. On the other hand, global supply chains and trade arrangements are part of totally different fields in sciences and these two scales are hardly matched. There is a clear gap between research focusing on supply and research focussing on demand.",
        "summary": "The shift towards sustainable consumption is a major debate in sustainability science. Research focuses on global trade inequalities and individual consumer behavior, with roots in psychology, social science, and other fields. However, there's a clear gap between research on supply (global supply chains and trade arrangements) and demand (consumer behavior).",
        "question": "What is the main challenge in research on sustainable consumption?",
        "ground_truths": "The main challenge in research on sustainable consumption is the clear gap between research on supply (global supply chains and trade arrangements) and demand (consumer behavior)."
    },
    {
        "contexts": "Regression analyses and assessment of correlation between different variables are common approaches when working with data in many different fields. Albeit it does not provide detailed insights into the mechanisms underlying the relationship of the variables investigated, it is a good first analysis to get an overview of what is going on. Correlation describes a linear relationship between at least two variables. It ranges between -1 and 1, which are called perfect positive and perfect negative correlation respectively. For example, a correlation between income (in Euros) and education (in years of schooling) of 0.5 indicates that there is a positive linear relationship between an increase in education and income. The most common approach to assessing corelation is via regression analysis. In regression analysis, a relationship between one dependent variable and at least on independent variable is assessed. With the help of the regression analysis, the effect of a change in the independent variable(s) by one unit on the dependent variable is modelled. This model therefore tries to find the linear curve that is best representing the actual relationship between the dependent and the independent variable(s). Returning to the example of income and education, we can set income to be the dependent variable and education as the independent variable. We can also add other variables affecting income, such as average wages in the country of the workplace (GDP), or relevant working experiences (in months of experience). With the regression analysis, we can then assess the change in income with a one unit increase of education and working experience, and an increase in GDP when having an international dataset. One of the most used estimators for assessing correlation is the ordinary least squares estimator (OLS). The OLS aims to produce a linear curve whose values minimize the sum of the squared values of the dependent variable and the predicted values from the regression equation. The OLS estimator relies on a number of assumptions that cannot be discussed in detail, but you can find them [https://towardsdatascience.com/assumptions-in-ols-regression-why-do-they-matter-9501c800787dXX here].",
        "summary": "Regression analysis and correlation assessment are common data analysis methods. Correlation, ranging from -1 to 1, describes a linear relationship between two variables. Regression analysis assesses the relationship between a dependent variable and one or more independent variables, modeling the effect of a unit change in the independent variables on the dependent variable. The ordinary least squares estimator (OLS) is a common tool for assessing correlation, aiming to produce a linear curve that minimizes the sum of the squared values of the dependent variable and the predicted values from the regression equation.",
        "question": "What is the purpose of the ordinary least squares estimator in regression analysis?",
        "ground_truths": "The ordinary least squares estimator (OLS) is used in regression analysis to produce a linear curve that minimizes the sum of the squared values of the dependent variable and the predicted values from the regression equation."
    },
    {
        "contexts": "The following parameters are used: id = final digit of the student matriculation number group = name of the learning group sex = gender in binary categories (m = male, f = female) quanti = number of solved exercises points = total score achieved from the exercises exam = total score achieved from the final written exam (Students must have to participate in the final written exam. If not, they will be considered as fail) passed = dummy whether the person has passed the class or not. Firstly, the aim for analyzing the dataset is to figure out the performance scored among the learning groups and gender for the solved questions, exercises and written exams. Secondly, we want to figure out the correlation between variables and most importantly to figure out heteroscedastic and homoscedastic dispersion, since the OLS is only apt when homoscedasticity is the case.",
        "summary": "The dataset includes parameters such as student id, learning group, gender, number of solved exercises, total score from exercises, total score from final exam, and pass status. The analysis aims to understand performance across learning groups and genders, and to identify correlations between variables. It also seeks to determine heteroscedastic and homoscedastic dispersion, as OLS is only suitable when homoscedasticity is present.",
        "question": "Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?",
        "ground_truths": "Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present."
    },
    {
        "contexts": "The question whether two continuous variables are linked initially emerged with the rise of data from astronomical observation. Initial theoretical foundations were laid by Gauss and Legendre, yet many relevant developments also happened much later. At their core, the basics of regressions revolved around the importance of the normal distribution. While Yule and Pearson were more rigid in recognising the foundational importance of the normal distribution of the data, Fisher argued that only the response variable needs to follow this distribution. This highlights yet another feud between the two early key innovators in statistics - Fisher and Pearson - who seem to be only able to agree to disagree on each other. The regression analysis was rooted famously in an observation by Galton called regression towards the mean - which proclaims that within most statistical samples, an outlier point is more likely than not followed by a data point that is closer to the mean. This proves to be a natural law for many dynamics that can be observed, underlining the foundational importance of the normal distribution, and how it translates into our understanding of patterns in the world.",
        "summary": "The concept of regression analysis, which tests the relationship between two continuous variables, originated from astronomical data. Theoretical foundations were established by Gauss and Legendre, with further developments occurring later. The method is based on the importance of the normal distribution, a point of contention between statisticians Fisher and Pearson. The principle of regression towards the mean, observed by Galton, states that outliers in statistical samples are typically followed by data points closer to the mean.",
        "question": "What is the principle of regression towards the mean?",
        "ground_truths": "The principle of regression towards the mean, observed by Galton, states that outliers in statistical samples are typically followed by data points closer to the mean."
    },
    {
        "contexts": "Regressions rose to worldwide recognition through econometrics, which used the increasing wealth of data from nation states and other systems to find relations within market dynamics and others patterns associated to economics. Equally, the regression was increasingly applied in medicine, engineering and many other fields of science. The 20th century became a time ruled by numbers, and the regression was one of its most important methods. Today, it is commonplace in all branches of science that utilise quantitative data to analyze data through regressions, including economics, social science, ecology, engineering, medicine, psychology, and many other branches of science. Almost all statistical software packages allow for the analysis of regressions, the most common software solutions are R, SPSS, Matlab and Python. Thanks to the computer revolutions, most regressions are easy and fast in their computation, and with the rising availability of more and more data, the regression became the most abundantly used simple statistical model that exists to date.",
        "summary": "Regression analysis gained global recognition through econometrics, using data from nation states to find economic patterns. It was also applied in medicine, engineering, and other scientific fields. Today, it's a common method in all branches of science that use quantitative data. Almost all statistical software packages, including R, SPSS, Matlab and Python, allow for regression analysis. With the rise of computing power and data availability, regression has become the most widely used simple statistical model.",
        "question": "What fields commonly use regression analysis?",
        "ground_truths": "Regression analysis is commonly used in all branches of science that use quantitative data, including economics, social science, ecology, engineering, medicine, psychology, and many other branches of science."
    },
    {
        "contexts": "Regressions statistically test the dependence of one continuous variable with another continuous variable. Building on a calculation that resolves around least squares, regression analysis can test whether a relation between to continuous variables is positive or negative, how strong the relation is, and whether the relation is significantly different from chance, i.e. following a non-random pattern. This is an important difference to Correlations, which only revolve around the relation between variables without assuming - or testing for - a causal link. Thus, identifying regressions can help us infer predictions about future developments of a relation. Within a regression analysis, a dependent variable is explained by an independent variable, both of which are continuous. At the heart of any regression analysis is the optimisation of a regression line that minimises the distance of the line to all individual points. In other words, the least square calculation maximizes the way how the regression line can integrate the sum of the squares of all individual data points to the regression line. The line can thus indicate a negative or positive relation by a negative or positive estimate, which is the value that indicates how much the y value increases if the x value increases.",
        "summary": "Regression analysis statistically tests the relationship between two continuous variables, determining whether the relationship is positive or negative, its strength, and whether it's significantly different from chance. This differs from correlations, which don't assume or test for a causal link. Regression analysis involves a dependent and an independent variable, both continuous. The method optimises a regression line to minimise the distance to all data points, indicating a negative or positive relationship through the estimate.",
        "question": "What is the difference between regression analysis and correlations?",
        "ground_truths": "The difference between regression analysis and correlations is that regression analysis tests the relationship between two continuous variables, determining whether the relationship is positive or negative, its strength, and whether it's significantly different from chance, while correlations don't assume or test for a causal link."
    },
    {
        "contexts": "The sum of the squares of the distance of all points to the regression line allows to calculate an r squared value. It indicates how strong the relation between the x and the y variable is. This value can range from 0 to 1, with 0 indicating no relation at all, and 1 indicating a perfect relation. There are many diverse suggestions of what constitutes a strong or a weak regression, and this depends strongly on the contexts. Lastly, the non-randomness of the relation is indicated by the p-value, which shows whether the relation of the two continuous variables is random or not. If the p-value is below 0,05 (typically), we call the relation significant. If there is a significant relation between the dependent and the independent variable, then new additional data is supposed to follow the same relation (see 'Prediction' bevlow). There are diverse ideas whether the two variables should follow a normal distribution, but it is commonly assumed that if the residuals - which is the deviation of the data points from a perfect relation - should follow a normal distribution. In other words, the error that is revealed through your understanding of the observed pattern follows a statistical normal distribution. Any non-normally distributed pattern might reveal flaws in sampling, a lack of additional variables, confounding factors, or other profound problems that limit the value of your analysis.",
        "summary": "The sum of the squares of the distances of all points to the regression line is used to calculate the r squared value, indicating the strength of the relationship between the variables. This value ranges from 0 (no relation) to 1 (perfect relation). The p-value indicates the non-randomness of the relation, with a value below 0.05 typically considered significant. It's commonly assumed that the residuals, or deviations from a perfect relation, should follow a normal distribution. Non-normally distributed patterns may indicate issues with the analysis.",
        "question": "What does the r squared value in regression analysis indicate?",
        "ground_truths": "The r squared value in regression analysis indicates the strength of the relationship between the variables."
    },
    {
        "contexts": "When conducting research, being reflexive and documenting your own progress can be very helpful. If you procrastinate, then this is for you. So basically, it is for everybody.  As an active researcher, it is not only important to track your progress, you also have document it, and ideally reflect about it. This can be of specific importance for early career scientists who face their first large challenge, like writing a thesis or publishing a paper.",
        "summary": "Documenting and reflecting on your research progress is crucial, especially for early career scientists facing significant challenges like thesis writing or paper publishing.",
        "question": "Why is documenting and reflecting on research progress important?",
        "ground_truths": "Documenting and reflecting on research progress is important as it helps in tracking progress and is particularly beneficial for early career scientists who are facing significant challenges like writing a thesis or publishing a paper."
    },
    {
        "contexts": "Getting started is the hardest point of any research diary, or better, getting started and sticking to it. You need to design a time and ideally even a place where you want to document your research and reflect about it. Bullet journaling has introduced fancy litte A5 journals into the life of many people, and such a book could be a good start. What is most important is that no setting is ideal for everybody, you have to find your own setting. Some write in the morning before the day gets started, some write at a fixed time in the office, others use the calm in the evening, and even other write whenever ad wherever they feel like it. We have to be aware that writing a research diary should be a committed goal if we decide to do it, and it needs to be a habit. Naturally, hair changes need time, thus starting small may pay off if it is nevertheless continuously. While a diary may help us to write down how we perceive reality, a research diary helps us to write down how we perceive research, and how we perceive ourself in research.",
        "summary": "Starting a research diary can be challenging, but it's important to find a routine that works for you. Whether it's writing in the morning, evening, or whenever you feel like it, the key is to make it a habit. A research diary helps document how you perceive your research and yourself in it.",
        "question": "How can one effectively start and maintain a research diary?",
        "ground_truths": "To effectively start and maintain a research diary, one needs to find a routine that works for them, whether it's writing in the morning, evening, or whenever they feel like it. The key is to make it a habit and to use the diary to document how they perceive their research and themselves in it."
    },
    {
        "contexts": "Research is often a very emotional process, which is not surprising. Whenever we spend a lot of time on something, we are more often than not deeply emotionally invested. To this end, it can be quite helpful to see how already are quite relaxed about parts of your research -say failure- about things that some months ago made you an emotional wreck. Looking back at the process has something liberating for many, as it shows that we may after all gain some process, if only at a forward inching scale.For other, a diary may be a good source to learn to focus. What is the main dish of your research, and what are side dishes? Research is a continuous endevour, and documenting this continuously can be indeed very insightful.",
        "summary": "Research is an emotional process, and a research diary can help manage these emotions. It can provide perspective on past failures and successes, and help focus on the main aspects of the research. Continuous documentation can provide valuable insights.",
        "question": "How can a research diary assist in the emotional process of research?",
        "ground_truths": "A research diary can assist in the emotional process of research by providing perspective on past failures and successes, helping to focus on the main aspects of the research, and offering valuable insights through continuous documentation."
    },
    {
        "contexts": "A sample is a selection of individuals from a larger population that researchers investigate instead of investigating the whole population, which is not possible for a range of reasons. Sampling is a crucial step in research: \"[o]ther than selecting a research topic and appropriate research design, no other research task is more fundamental to creating credible research than obtaining an adequate sample.\" (Marshall et al. 2013, p.11). This is also true for Interview methdology: the creation of a good sample directly affects the validity of the research results (7). How many people you ask your questions, hand out your questionnaire to, or invite to your Focus Groups directly influences the broadth and depth of the created insights. Inviting too few people might not deliver sufficient material to answer your research questions. An overly large sample, then again, might be superfluous, i.e. increase the costs and organisational effort of your project without providing further insights. A sample of the study population that is not representative of the full breadth of possible viewpoints and experiences within the whole population will definitely influence your results, and non-representative samples can happen no matter the sample size, but are less of a problem for bigger samples than for small ones. Thus, approaching the question of sampling can generally be guided by Ockham's Razor - you need a sample that is as big as necessary, but as small as possible. However, this is often easier to say than to apply.",
        "summary": "Sampling involves selecting individuals from a larger population for research. It's a crucial step in research and affects the validity of the results. The size of the sample influences the breadth and depth of insights. Too small a sample might not provide enough data, while too large a sample might be unnecessary and costly. Non-representative samples can skew results, but this is less of a problem with larger samples. The ideal sample size is as big as necessary, but as small as possible.",
        "question": "Why is sampling an important step in research?",
        "ground_truths": "Sampling is important in research because it affects the validity of the results. The size of the sample influences the breadth and depth of insights. Non-representative samples can skew results, but this is less of a problem with larger samples."
    },
    {
        "contexts": "In a first step, the 'sample universe' - also known as 'study population' or 'target population' - needs to be defined. To do so, the researcher develops inclusion and / or exclusion criteria to guide the selection of all individuals that are theoretically of interest to answer the research question (see Figure). The inclusion and exclusion criteria that are applied will determine the characteristics of the sample universe. This universe can thus become very homogeneous as well as very heterogeneous (see Table). These criteria are mostly guided by theory, in accordance to the research purpose, questions, and methodological design, and potentially with regards to other work that has been done before. Further, organisational elements already play a role here, like geographical access, or the level of access that the researcher has to the population. Within this population, the sample will be drawn, which will help answer the research questions.",
        "summary": "The first step in sampling is defining the 'sample universe' or 'target population'. This is done by developing inclusion and exclusion criteria based on the research question. These criteria determine the characteristics of the sample universe, which can be very homogeneous or heterogeneous. The criteria are guided by theory, research purpose, questions, and methodological design. Organisational elements like geographical access also play a role. The sample is then drawn from this population.",
        "question": "How is the 'sample universe' or 'target population' defined in research?",
        "ground_truths": "The 'sample universe' or 'target population' is defined by developing inclusion and exclusion criteria based on the research question. These criteria determine the characteristics of the sample universe, which can be very homogeneous or heterogeneous. The criteria are guided by theory, research purpose, questions, and methodological design. Organisational elements like geographical access also play a role."
    },
    {
        "contexts": "Before drawing the sample from the whole population within the sampling universe, the next step is to clarify the amount of individuals that are planned to be interviewed. This differs between Interview formats.",
        "summary": "Before drawing the sample, the researcher needs to determine the number of individuals to be interviewed. This varies depending on the interview format.",
        "question": "What needs to be determined before drawing the sample from the population?",
        "ground_truths": "Before drawing the sample, the researcher needs to determine the number of individuals to be interviewed."
    },
    {
        "contexts": "Sankey diagrams show the flows, where a width of each flow is proportional to the quantity represented. The flows are called links. Links connect entities (called nodes) and may converge or diverge. Sankey diagrams help to understand a many-to-many mapping between two domains or multiple paths of data through a set of states. Since it is possible to see the considered entities / nodes and links between them, one can say, that there is an information about the structure of the defined system. The source node is the node where the flow originates. The target node is the node where the flow ends. The nodes are usually represented as rectangles with a label.",
        "summary": "Sankey diagrams represent flows, with the width of each flow proportional to the quantity it represents. These flows, or links, connect entities known as nodes. The diagrams help understand many-to-many mappings between two domains or multiple data paths. The source node is where the flow starts, and the target node is where it ends.",
        "question": "What do Sankey diagrams represent and how are they structured?",
        "ground_truths": "Sankey diagrams represent flows, with each flow's width proportional to the quantity it represents. These flows, known as links, connect entities called nodes. The diagrams help in understanding many-to-many mappings between two domains or multiple data paths. The flow starts at the source node and ends at the target node."
    },
    {
        "contexts": "Sankey diagram is named after Irish engineer Matthew H. Sankey, who created a diagram of steam engine efficiency, that used arrows having widths proportional to heat loss. The illustration is dated to 1898. In the 20th century the Austrian mechanical engineer Alois Riedler began to apply flow charts to analyze the power and the energy losses of passenger cars. Also some government departments used it for financial goals, focusing on material and energy efficiency in the beginning of the 20th century.",
        "summary": "The Sankey diagram, named after Irish engineer Matthew H. Sankey, was first used to represent steam engine efficiency in 1898. In the 20th century, Austrian engineer Alois Riedler and some government departments began using it to analyze power and energy losses and for financial goals.",
        "question": "Who invented the Sankey diagram and what were its early applications?",
        "ground_truths": "The Sankey diagram was invented by Irish engineer Matthew H. Sankey and was first used to represent steam engine efficiency in 1898. In the 20th century, it was used by Austrian engineer Alois Riedler to analyze power and energy losses in passenger cars, and by some government departments for financial goals."
    },
    {
        "contexts": "There are many ways to use Sankey diagram. It can show data, energy, capacity, materials, costs, social and biological data (population, migration) and so on. Spheres, such as energy, facility, supply chain management, business, marketing analysis, apply these diagrams constantly. The use case examples can be found [https://www.sankey-diagrams.com/ here]. Sankey diagram can be perceived intuitively. There is no standard notation of how the diagram should look, therefore, diverse options exist. The viewer may pay attention on the largest flow width, linked entities or notice the losses of the definite process.",
        "summary": "Sankey diagrams can represent various types of data, including energy, capacity, costs, and social and biological data. They are widely used in fields like energy, supply chain management, and business analysis. The diagrams are intuitive and flexible, with no standard notation, allowing viewers to focus on different aspects such as the largest flow or linked entities.",
        "question": "What types of data can Sankey diagrams represent and in what fields are they commonly used?",
        "ground_truths": "Sankey diagrams can represent a variety of data types, including energy, capacity, costs, and social and biological data. They are commonly used in fields such as energy, supply chain management, and business analysis."
    },
    {
        "contexts": "The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the Delphi Method. Later, during the 1960s, scenarios were \"(...) extensively used for social forecasting, public policy analysis and decision making\" in the US. (Amer et al. 2013, p.24).",
        "summary": "Scenario planning originated from the Manhattan Project in the 1940s, where physicists used computer simulations to estimate the impact of the atomic bomb. This method evolved into three strands: computer simulations, game theory, and military planning, including the Delphi Method developed by the RAND corporation. By the 1960s, scenarios were widely used in the US for social forecasting, public policy analysis, and decision making.",
        "question": "When and where did the use of scenarios as a tool for structured thinking about the future originate?",
        "ground_truths": "The use of scenarios as a tool for structured thinking about the future originated from the Manhattan Project in the early 1940s."
    },
    {
        "contexts": "Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell. At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2).",
        "summary": "Scenario Planning was significantly advanced by Shell in response to increasing forecasting errors since the early 1970s. Shell introduced the \"Unified Planning Machinery\" to prepare for potential market shocks, effectively inventing Scenario Planning. This system allowed Shell to anticipate future events, helping them manage the 1973 and 1981 oil crises. By 1982, over half of the Fortune 500 companies had adopted Scenario Planning.",
        "question": "How did Shell contribute to the advancement of Scenario Planning?",
        "ground_truths": "Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."
    },
    {
        "contexts": "Today, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to Visioning & Backcasting, it has also found its way into research. For instance, researchers in transdisciplinary sustainability science gather stakeholders' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.",
        "summary": "Scenario Planning is still a crucial tool for corporate planning due to growing complexity and uncertainty in business environments. It has also been incorporated into research, particularly in transdisciplinary sustainability science, where stakeholders' expertise is gathered to contemplate desirable and undesirable future states. This method aids in advising and supporting planning for companies, NGOs, cities, and even nations.",
        "question": "How is Scenario Planning used in today's corporate planning and research?",
        "ground_truths": "Scenario Planning is used in today's corporate planning as a tool to navigate increasing complexity and uncertainty in business environments. In research, particularly in transdisciplinary sustainability science, it is used to gather stakeholders' expertise to contemplate desirable and undesirable future states, aiding in advising and supporting planning for companies, NGOs, cities, and nations."
    },
    {
        "contexts": "Scenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called \"scenarios\". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them \"(...) in coherent script-like or narrative fashion.\" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all \"fundamentally different\" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios \"(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties.\" (Schoemaker 1993, p.196).",
        "summary": "Scenario Planning involves systematically developing descriptions of multiple plausible futures, known as \"scenarios\". These scenarios, which can be contradictory and irreconcilable, are presented in a narrative fashion rather than as statistical predictions. The aim is not to predict the most likely future, but to understand the range of possible developments and their interactions, thereby better understanding future uncertainties.",
        "question": "What is the main purpose of Scenario Planning?",
        "ground_truths": "The main purpose of Scenario Planning is not to predict the most likely future, but to understand the range of possible developments and their interactions, thereby better understanding future uncertainties."
    },
    {
        "contexts": "There is no ''one'' procedure for Scenario Planning (5). A commonly cited approach by Schoemaker (2, 3) includes the following steps: Definition of time frame, scope, decision variables and major actors of the issue in question. Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes. Elimination of implausible or impossible futures and, based on the themes that emerged from these, the creation of new scenarios. This process is repeatedly done until internally consistent scenarios are found. The number of scenarios developed depends on the scope and purpose of the planning process (1, 5).",
        "summary": "There is no single procedure for Scenario Planning. A common approach involves defining the time frame, scope, decision variables, and major actors; identifying current trends and how they influence decision variables; constructing extreme future states for all variables; assessing these extremes for consistency and plausibility; eliminating implausible futures and creating new scenarios based on emerging themes. This process is repeated until consistent scenarios are found. The number of scenarios depends on the scope and purpose of the planning.",
        "question": "What are the steps involved in a commonly cited approach to Scenario Planning?",
        "ground_truths": "The steps involved in a commonly cited approach to Scenario Planning include defining the time frame, scope, decision variables, and major actors; identifying current trends and how they influence decision variables; constructing extreme future states for all variables; assessing these extremes for consistency and plausibility; eliminating implausible futures and creating new scenarios based on emerging themes. This process is repeated until consistent scenarios are found."
    },
    {
        "contexts": "From early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar.",
        "summary": "Scientific paradigms have been influential in societal development since early times, with high cultures of antiquity remembered for their scientific advancements. Early science often had a practical focus, like metallurgy, or was tied to the metaphysical, like astronomy. Astronomy, for instance, was used for navigation and influenced belief systems, as seen in the star of Bethlehem and the Mayan calendar.",
        "question": "How did early scientific paradigms influence society and belief systems?",
        "ground_truths": "Early scientific paradigms, such as metallurgy and astronomy, influenced society by having practical applications like navigation and by shaping belief systems, as seen in the star of Bethlehem and the Mayan calendar."
    },
    {
        "contexts": "While the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples.",
        "summary": "The interaction between scientific methods and societal paradigms is a key focus, challenging the notion that science is disconnected from society. While not applicable to all scientific branches, some are more intertwined with society, as illustrated by three examples.",
        "question": "How do scientific methods interact with societal paradigms?",
        "ground_truths": "Scientific methods interact with societal paradigms in various ways, and while some branches of science may seem disconnected from society, others are more intertwined and have a direct impact on societal paradigms."
    },
    {
        "contexts": "Since the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world.",
        "summary": "The Enlightenment marked the solidification of many scientific disciplines, which often interacted with societal paradigms. Scientific disciplines focus on parts of reality, which are then encapsulated in theories and translated into societal paradigms. This process has led to category mistakes, where theories explaining one part of the world are applied to other parts.",
        "question": "What role did the Enlightenment play in the interaction between scientific developments and societal paradigms?",
        "ground_truths": "The Enlightenment marked the solidification of many scientific disciplines, which often interacted with societal paradigms. This period saw scientific theories, which focus on specific parts of reality, being translated into societal paradigms, sometimes leading to category mistakes."
    },
    {
        "contexts": "Interview methodology is perhaps the oldest of all the social science methodologies. Asking Interview participants a series of informal questions to obtain knowledge has been a common practice among anthropologists and sociologists since the inception of their disciplines. Within sociology, the early-20th-century urban ethnographers of the Chicago School did much to prompt interest in the method. Barney Glaser and Anselm Strauss are crucial figures in this regard, developing the Grounded Theory approach to qualitative data analysis during the 1960s. With their 1967 landmark book (see Key Publications) they paved the road towards an integration of methodologically building on Interviews, thereby creating a whole new area of research in sociology and beyond. Building on their proposal, yet also before, several publications contributed to a proper understanding of Interview methodology and how to best conduct Interviews (see Key Publications). Today, qualitative Interviews are used mostly in gender studies, social and political studies as well as ethnographics.",
        "summary": "Interview methodology, one of the oldest social science methodologies, involves asking participants informal questions to gain knowledge. This practice has been common among anthropologists and sociologists since their disciplines began. Barney Glaser and Anselm Strauss developed the Grounded Theory approach to qualitative data analysis in the 1960s, creating a new area of research in sociology and beyond. Today, qualitative interviews are primarily used in gender studies, social and political studies, and ethnographics.",
        "question": "Who are the key figures in the development of the Grounded Theory approach to qualitative data analysis?",
        "ground_truths": "Barney Glaser and Anselm Strauss are the key figures in the development of the Grounded Theory approach to qualitative data analysis."
    },
    {
        "contexts": "Interviews are a form of data gathering. They can be conducted in direct conversation with individuals or groups. This is also possible online or via the phone. It is possible for more than one researcher to conduct the Interview together. The necessary equipment includes an Interview guide that helps ask relevant questions, a recording device and paper & pencil or a computer for note taking during the Interview. Semi-structured Interviews can be used for many research purposes, including specialized forms. Among the latter, more common versions include the expert Interview where the Interviewee is a person of special knowledge on the topic, the biographical Interview that serves to investigate a life-history, the clinical Interview that helps diagnose illnesses, and the dilemma-Interview that revolves around moral judgements.",
        "summary": "Interviews are a method of data collection that can be conducted in person, online, or over the phone, and can involve more than one researcher. Necessary equipment includes an interview guide, a recording device, and note-taking materials. Semi-structured interviews can be used for various research purposes, including expert interviews, biographical interviews, clinical interviews, and dilemma-interviews.",
        "question": "What are some of the specialized forms of semi-structured interviews?",
        "ground_truths": "Specialized forms of semi-structured interviews include expert interviews, biographical interviews, clinical interviews, and dilemma-interviews."
    },
    {
        "contexts": "In a semi-structured Interview, the researcher asks one or more individuals open-ended questions. These questions are based on and guided by an Interview guide that is developed prior to the conduction of the Interview. The Interview guide is based on the research intent and questions, and can be informed by existing literature: the researchers think of the information they need to gather from the Interviews to answer the research questions, and develop the Interview guide accordingly. The Interview guide thus contains keywords or issues that need to be covered during the Interview in order to answer the research questions. However, in this form of Interviews, there is still room for new topics, or focal points to existing questions, emerging during the Interview itself. The guide, therefore, ought to be as open as possible, as structuring as necessary.",
        "summary": "In a semi-structured interview, researchers ask open-ended questions based on an interview guide developed before the interview. The guide is informed by the research intent and questions, and existing literature. It contains keywords or issues that need to be addressed during the interview. However, there is room for new topics or focal points to emerge during the interview.",
        "question": "What is the purpose of an interview guide in a semi-structured interview?",
        "ground_truths": "The interview guide in a semi-structured interview is used to guide the researcher's questions. It is based on the research intent and questions, and contains keywords or issues that need to be addressed during the interview."
    },
    {
        "contexts": "\"Gaming has roots in systems analysis, operations research and decision sciences. The earliest use of gaming in support of decision making are in war games (...). They have a long history and originated as devices for planning military operations (...). The use of gaming in a political-military-security contexts was subsequently transferred to a non-military contexts, hence the interest in gaming simulation by not only computer scientists and game designers, but also decision makers, public policy makers, engineers and scientists.\" (Savic et al. 2016, p.457). Serious Games are a form of gaming and thus located within the field of simulation and gaming research, alongside other aspects such as computer simulations, agent-based modeling, role-plays and virtual reality (2). \"The notion of serious gaming was introduced by Abt (1970), who established how simulation games could be used for education, decision making and for public policy making.\" (Savic et al. 2016, p. 457, emphasis added). Since Abt's primary work, the method has found its way into several fields, including business, corporate, healthcare, government, military (among these the RAND corporation who invented the [[Delphi]] Method) and science (1, 2, 8). However, its primary usage is for education, training and conflict resolution (1, 5).",
        "summary": "Gaming, which originated in systems analysis, operations research, and decision sciences, was initially used in war games for decision making and planning military operations. It later found application in non-military contexts, attracting interest from various professionals. Serious Games, a form of gaming, was introduced by Abt in 1970, who demonstrated its use in education, decision making, and public policy making. It has since been adopted in various fields, but is primarily used for education, training, and conflict resolution.",
        "question": "Who introduced the concept of Serious Games and what are its primary uses?",
        "ground_truths": "The concept of Serious Games was introduced by Abt in 1970. Its primary uses are in education, training, and conflict resolution."
    },
    {
        "contexts": "\"[A game] is almost impossible to define, but we recognize one when we see it.\" writes Crookall (2), citing the philosopher Wittgenstein. This statement highlights that there is a wide, unanimous range of definitions for the term 'game', and subsequently for the term 'Serious Gaming'. However, most of the existing definitions agree upon the aspect that \"(...) they are games used for purposes other than mere entertainment.\" (Savic et al. 2016, p.457). Different from most popular video games, entertainment is not the primary goal of a (serious) game, but rather a means to an end (4, 5, 8). A Serious Game pursues a specific purpose that lies predominantly in training, educating and achieving behavioral change in the players (4, 5). Specific goals can be, among others, raising attention, challenging prejudgments, communicating political statements or merely offering information, all of which shall impact the players' lives beyond the gaming experience itself (4, 5). For decision makers and planners as well as scientists, Serious Games can be helpful tools since they \"(...) provide a means of identifying and evaluating the consequences of alternative plans and policies.\" (Mitgutsch & Alvarado 2012, p.11).",
        "summary": "Defining a game, particularly a Serious Game, is challenging, but it is generally agreed that they are used for purposes beyond entertainment. Unlike popular video games, the primary goal of a Serious Game is not entertainment, but training, education, and achieving behavioral change in players. They can serve various specific goals, such as raising awareness, challenging prejudices, communicating political statements, or providing information. Serious Games can be useful tools for decision makers, planners, and scientists as they help identify and evaluate the consequences of alternative plans and policies.",
        "question": "What is the primary goal of a Serious Game and how can it be useful for decision makers, planners, and scientists?",
        "ground_truths": "The primary goal of a Serious Game is training, education, and achieving behavioral change in players. For decision makers, planners, and scientists, Serious Games can be useful as they provide a means of identifying and evaluating the consequences of alternative plans and policies."
    },
    {
        "contexts": "Games differ from simulations. While a simulation is a \"simplified, dynamic and accurate model of reality\" (Coovert et al. 2017, p.5 citing Adroitly), a game is a \"ficticious or artificial situation in which the player is expected to perform\" (ibid). The Serious Game may be seen as a combination of both forms: the simulation model no longer focuses on system analysis, but instead serves a learning purpose, adds the player as an actor amongst the other stakeholders, allows for interactive engagement with the model and is used in several different groups (3).",
        "summary": "Games and simulations are different. A simulation is a simplified, dynamic, and accurate model of reality, while a game is a fictitious or artificial situation where the player is expected to perform. A Serious Game combines both, using the simulation model not for system analysis, but for learning purposes, adding the player as an actor, allowing interactive engagement with the model, and being used in various groups.",
        "question": "How does a Serious Game combine the elements of a game and a simulation?",
        "ground_truths": "A Serious Game combines the elements of a game and a simulation by using the simulation model not for system analysis, but for learning purposes. It adds the player as an actor, allows interactive engagement with the model, and is used in various groups."
    },
    {
        "contexts": "In general, positive impacts of games include analytical and spatial skills, strategic planning skills, better orientation in three-dimensional spaces, psychomotor skills, learning capabilities and more (8). When applied as a tool for planning and education, further advantages include: Serious Gaming, compared to purely technical simulations, better acknowledges socio-technical challenges of complex systems (1). Games are cheap and motivating forms of education and training and allow the players to receive feedback on their actions. They enable the evaluators to easily assess the players' performances and provide more creativity for the game designers without having the associated risks of implementing the simulated decisions in the real world (4).",
        "summary": "Games have positive impacts such as enhancing analytical and spatial skills, strategic planning, and learning capabilities. Serious Gaming, in particular, acknowledges socio-technical challenges of complex systems better than purely technical simulations. They are cost-effective educational tools that provide feedback to players and allow for creativity in design without real-world risks.",
        "question": "What are the benefits of using games, particularly Serious Gaming, in education and training?",
        "ground_truths": "Games enhance analytical and spatial skills, strategic planning, and learning capabilities. Serious Gaming acknowledges socio-technical challenges of complex systems better than purely technical simulations. They are cost-effective, provide feedback to players, and allow for creativity in design without real-world risks."
    },
    {
        "contexts": "Serious Gaming may be built upon other methodological approaches, such as Geographical Information Systems or forms of System Analysis, including Social Network Analysis or Causal Loop Diagrams. Serious Gaming may be used as a transdisciplinary research method in stakeholder workshops. Thus, all notions of normativity in transdisciplinary research may apply, such as conflicting roles and challenges in problem framing. The information and framing that are part of the game serve a specific purpose, which may represent unanimous positions within the sustainability community, but may as well inflict opposing and harmful moral positions.",
        "summary": "Serious Gaming can incorporate various methodological approaches like Geographical Information Systems and System Analysis. It can be used as a transdisciplinary research method in stakeholder workshops, with all the normative aspects of such research. The game's information and framing serve a specific purpose, which can represent different positions within the sustainability community.",
        "question": "How can Serious Gaming be utilized in research and what are its normative aspects?",
        "ground_truths": "Serious Gaming can incorporate various methodological approaches like Geographical Information Systems and System Analysis. It can be used as a transdisciplinary research method in stakeholder workshops. The game's information and framing serve a specific purpose, which can represent different positions within the sustainability community."
    },
    {
        "contexts": "As a scientific method, Serious Gaming has existed for 50 years now, but there is still a lack of a homogeneous usage of the term, with common alternatives being 'digital learning games', 'game-based learning' or 'edutainment games', among others (2, 8). However, as Crookall (2) hopes, this ambiguity should not halt the further development of the method. He claims that the field of simulation and gaming may be moving towards being a full scientific discipline rather than only a set of methodological approaches (2). While the usage of Serious Games is currently widespread across different disciplines, a suggested name for the discipline-to-be is \"Game Science\" (5).",
        "summary": "Serious Gaming, a scientific method for 50 years, lacks a homogeneous term, with alternatives like 'digital learning games' and 'game-based learning'. Despite this ambiguity, the field may be moving towards a full scientific discipline, potentially named 'Game Science', and is widely used across different disciplines.",
        "question": "What is the current state and potential future of Serious Gaming as a scientific discipline?",
        "ground_truths": "Serious Gaming has been a scientific method for 50 years but lacks a homogeneous term. Despite this, the field may be moving towards becoming a full scientific discipline, potentially named 'Game Science', and is widely used across different disciplines."
    },
    {
        "contexts": "The next important test is the Wilcoxon rank sum test. This test is also a paired test. What is most relevant here is that not the real numbers are introduced into the calculation, but instead these numbers are transformed into ranks. In other words, you get rid of the question about normal distribution and instead reduce your real numbers to an order of numbers. This can come in handy when you have very skewed distribution - so a exceptionally non-normal distribution - or large gaps in your data. The test will tell you if the means of two samples differ significantly (i.e. p-value below 0,05) by using ranks.",
        "summary": "The Wilcoxon rank sum test is a paired test that uses ranks instead of real numbers in its calculations. This is useful when dealing with skewed or non-normal distributions, or large gaps in data. The test determines if the means of two samples differ significantly.",
        "question": "What is the purpose of the Wilcoxon rank sum test?",
        "ground_truths": "The Wilcoxon rank sum test is used to determine if the means of two samples differ significantly by using ranks instead of real numbers."
    },
    {
        "contexts": "The f-test allows you to compare the ''variance'' of two samples. Variance is calculated by taking the average of squared deviations from the mean and tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean. If the p-value of the f-test is lower than 0,05, the variances differ significantly. Important: for the f-Test, the data of the samples has to be normally distributed.",
        "summary": "The f-test is used to compare the variance of two samples, which is the average of squared deviations from the mean. If the p-value of the f-test is below 0.05, the variances differ significantly. The data for the f-test must be normally distributed.",
        "question": "What does the f-test compare in two samples?",
        "ground_truths": "The f-test compares the variance of two samples."
    },
    {
        "contexts": "Simple tests are not abundantly applied these days in scientific research, and often seem outdated. Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion. However, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics.",
        "summary": "Simple tests are not commonly used in modern scientific research due to the complexity of scientific designs and datasets. However, they are robust and often the most straightforward approach. They also serve as a foundation for more complex methods and have contributed to the development of frequentist statistics.",
        "question": "Why are simple tests not commonly used in modern scientific research?",
        "ground_truths": "Simple tests are not commonly used in modern scientific research because scientific designs and datasets have become more complex."
    },
    {
        "contexts": "One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. [W]e should focus instead on the emergent consequences of the interaction of individual actions. (Marin & Wellman 2010, p.6)",
        "summary": "German philosopher and sociologist Georg Simmel was a pioneer of Network Analysis around 1900. He emphasized the significance of social relations in understanding social systems, arguing against viewing society as a collection of independent individuals. Instead, he proposed focusing on the emergent outcomes of individual interactions.",
        "question": "Who was one of the originators of Network Analysis and what was his perspective on understanding social systems?",
        "ground_truths": "One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel. He believed in the importance of social relations when understanding social systems, rather than focusing on individual units."
    },
    {
        "contexts": "Romanian-American psychosociologist Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s with his - and his collaborator Helen Jennings' - 'sociometry', which served as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction. (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupied in their social networks (see Diagram).",
        "summary": "In the 1930s, Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings significantly influenced Social Network Analysis with their 'sociometry', a method to conceptualize the structures of small groups through friendship patterns and informal interaction. Their work was inspired by a case of runaways at the Hudson School for Girls in New York, hypothesizing that the girls' positions in their social networks influenced their decision to run away.",
        "question": "Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?",
        "ground_truths": "Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks."
    },
    {
        "contexts": "Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm. (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter. Social Network Analysis is based on the idea that social life is created primarily and most importantly by relations and the patterns formed by these relations. Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations. (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as entitites, actors, vertices or agents and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are tied to one another through socially meaningful relations (Prell et al. 2009, p.503), which can be collaborations, friendships, trade ties, web links, citations, resource flows, information flows or any other possible connection (Marin & Wellman 2010, p.2).",
        "summary": "Social network analysis is a perspective or paradigm, not a theory or methodology. It encompasses various methodological approaches and is based on the concept that social life is primarily formed by relations and their patterns. Social networks are defined as a set of nodes, or network members, tied by one or more types of relations. These nodes, often individuals or organizations, are connected through socially meaningful relations such as collaborations, friendships, trade ties, and more.",
        "question": "What is the basis of Social Network Analysis and how are social networks defined in this contexts?",
        "ground_truths": "Social Network Analysis is a perspective or paradigm that is based on the idea that social life is primarily created by relations and the patterns formed by these relations. Social networks are defined as a set of nodes (or network members) that are tied by one or more types of relations."
    },
    {
        "contexts": "While there is definitely value in writing manually, and it certainly has the nostalgic charme of writing a letter to your grandmother, we must admit that almost all communication is digital today. Throughout your (non-)scientific career in the 21st Century, you will write on a computer. A lot. And while voice recognition and text-to-speech are getting better by the day, we will have to deal with pushing letters on a keyboard for quite some time. Might as well learn to do it quickly.",
        "summary": "In the digital age, most communication is done through typing on a computer. Despite advancements in voice recognition and text-to-speech, typing remains a crucial skill.",
        "question": "Why is typing a crucial skill in the digital age?",
        "ground_truths": "Most communication is done through typing on a computer, and despite advancements in voice recognition and text-to-speech, we will still have to deal with pushing letters on a keyboard for quite some time."
    },
    {
        "contexts": "There is an endless range of websites that allow you to learn ten-finger-typing. Often, these offer writing challenges, providing you with texts you need to type and ranking your results (speed, accuracy) compared to others. While this can help getting better, you should start off with the basics to really get to know the keyboard. Fortunately, many websites also offer tutorials to learn the first steps. You should be able to recreate the keyboard from your mind and know the position of every letter before really attempting to increase your speed. Otherwise, you will make so many mistakes that correcting them slows you down, equalizing your speed gains. Only later will your muscle memory take over, and you may as well what a keyboard looks like, while still being able to type blindly.",
        "summary": "Numerous websites offer ten-finger-typing lessons and challenges. It's important to start with the basics and know the keyboard layout before attempting to increase speed. Muscle memory will eventually allow for blind typing.",
        "question": "What is the recommended approach to learning touch typing?",
        "ground_truths": "Start with the basics and know the keyboard layout before attempting to increase speed. Muscle memory will eventually allow for blind typing."
    },
    {
        "contexts": "There are different kinds of keyboards, and different people have different preferences. The most common type is the membrane keyboard, which you probably have in front of you right now. It's cheap and thus the standard, and it mostly works well. There are differences here, too - some keyboards are rather flat, some are rather bulky, and it's mostly a matter of taste which you prefer. However, there are also mechanical keyboards. Often referred to as gaming keyboards, they provide a better feedback and feel when pressing them, and often have a better built quality. However, they also produce more noise, which is why they are rather seldomly used in typical work environments. You might still want to consider acquiring one if you type a lot.",
        "summary": "There are various types of keyboards, including the common membrane keyboard and the mechanical keyboard, often used for gaming. Each type has its pros and cons, and preference depends on individual taste and usage.",
        "question": "What are the differences between membrane and mechanical keyboards?",
        "ground_truths": "Membrane keyboards are cheap and standard, while mechanical keyboards provide better feedback and feel when pressing, have better build quality but produce more noise."
    },
    {
        "contexts": "Practice makes perfect. Don't be too harsh with yourself. Take breaks while typing, and even more so, while practicing and learning to type. Look out of the window now and then, your eyes will be thankful. Maintain a healthy posture at the desk.",
        "summary": "Practicing typing requires patience and regular breaks. It's also important to rest your eyes and maintain a good posture.",
        "question": "What are some tips for practicing typing?",
        "ground_truths": "Don't be too harsh with yourself, take breaks, rest your eyes and maintain a good posture."
    },
    {
        "contexts": "A Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. The Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total. A Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total. A data series is a set of data represented as a line in a Stacked Area Plot.",
        "summary": "A Stacked Area Plot is a visualization tool that displays quantitative values for multiple data series. It shows the evolution of a numeric variable for several groups in a dataset, with each group displayed on top of the other. This plot is used to track the total value of the data series and understand the breakdown of that total into different data series.",
        "question": "What is the purpose of a Stacked Area Plot?",
        "ground_truths": "A Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series."
    },
    {
        "contexts": "Stacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.",
        "summary": "While Stacked Area Plots are useful for studying the evolution of the whole data series and the relative proportions of each data series, they have limitations and are not suitable for studying the evolution of individual data series.",
        "question": "What are the limitations of Stacked Area Plots?",
        "ground_truths": "Stacked Area Plots are not suitable for studying the evolution of individual data series."
    },
    {
        "contexts": "R uses the function <syntaxhighlight lang=\"R\" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots. The function <syntaxhighlight lang=\"R\" inline>geom_area()</syntaxhighlight> has the following syntax: Syntax: <syntaxhighlight lang=\"R\" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight> Parameters: Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot. x: This parameter contains numerical value of variable for x axis in Stacked Area Plot. y: This parameter contains numerical value of variables for y axis in Stacked Area Plot. fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.",
        "summary": "In R, the function geom_area() is used to create Stacked Area Plots. The syntax for this function includes parameters for the dataset, x and y variables, and a group variable for the fill parameter.",
        "question": "How is a Stacked Area Plot created in R?",
        "ground_truths": "A Stacked Area Plot is created in R using the function geom_area(). The syntax for this function includes parameters for the dataset, x and y variables, and a group variable for the fill parameter."
    },
    {
        "contexts": "Sustainability Research is a research field that aims at tackling societal problems by developing systemic understandings of these problems and oftentimes collaboratively solving them through transdisciplinary research designs. In this contexts, the concept of stakeholders is a crucial component. Stakeholders are people that are directly affected by sustainability problems and can contribute to their solution. Mapping-out these stakeholders is a common entry-point to gaining an understanding of the system and problem at hand, and is further important to gather an overview of the relevant stakeholders in this contexts.",
        "summary": "Sustainability Research is a field that addresses societal issues by developing systemic understandings and collaboratively solving them through transdisciplinary research. Stakeholders, who are directly affected by these problems and can contribute to their solution, are a key component. Mapping these stakeholders is a common starting point to understand the system and problem, and to identify relevant stakeholders.",
        "question": "What is the role of stakeholders in Sustainability Research?",
        "ground_truths": "Stakeholders are people that are directly affected by sustainability problems and can contribute to their solution. Mapping these stakeholders is a common starting point to understand the system and problem, and to identify relevant stakeholders."
    },
    {
        "contexts": "'Stakeholders' are individuals, groups of individuals or institutions that have an interest in, or are affected by, decision-making related to a specific issue or problem. The stakeholders' interest in the respective decision-making may stem from being affected by the problem itself and thus the consequences of potential solutions to it, or from the intention of influencing the decision for any other reason. The relevant stakeholders in transdisciplinary projects are typically non-academic actors, such as NGOs, citizens, politicians, economic actors. However, academic actors may also be identified as stakeholders. Stakeholders can be distinguished more broadly, e.g. when everyone in a region is a potential stakeholder, or more narrowly, depending on the issue.",
        "summary": "Stakeholders are individuals, groups, or institutions with an interest in or affected by decision-making related to a specific issue. Their interest may come from being affected by the problem or the intention to influence the decision. In transdisciplinary projects, stakeholders are typically non-academic actors, but can also include academic actors. The scope of stakeholders can be broad or narrow, depending on the issue.",
        "question": "Who can be considered as stakeholders in transdisciplinary projects?",
        "ground_truths": "In transdisciplinary projects, stakeholders are typically non-academic actors, such as NGOs, citizens, politicians, economic actors. However, academic actors may also be identified as stakeholders."
    },
    {
        "contexts": "'Mapping' refers to the visual representation and structured arrangement of the identified actors in preparation to a transdisciplinary research project. Such a 'map' can take various forms. One approach is to categorize all relevant stakeholders according to their position towards the researched problem, or towards solutions to this problem. For example, in the Conviva project in Brazil, stakeholders were mapped in a 2x2 matrix according to their support or resistance towards nature conservation efforts.",
        "summary": "'Mapping' is the visual representation and structured arrangement of identified actors in a transdisciplinary research project. The 'map' can take different forms, such as categorizing stakeholders according to their position towards the problem or its solutions. For instance, in the Conviva project, stakeholders were mapped based on their support or resistance to nature conservation efforts.",
        "question": "What does 'mapping' refer to in the contexts of a transdisciplinary research project?",
        "ground_truths": "'Mapping' refers to the visual representation and structured arrangement of the identified actors in preparation to a transdisciplinary research project."
    },
    {
        "contexts": "Through the process of stakeholder mapping, the research team is enabled to identify all relevant actors for the given issue. This helps plan next steps, i.e. the conduction of interviews to gather further insights, or provides an overview on which individuals to invite to workshop formats or further methods of transdisciplinary research. This way, all important actors are involved in the process, which is important for the success of the envisioned solutions. A Leventon et al. (2016) highlight: 'Researchers in natural resource management consistently find that stakeholders should be included in solution-finding in order to facilitate negotiation and mutual learning; reduce conflict; and increase support and actor buy-in for decisions made.' Apart from these advantages, stakeholder mapping also ensures that all relevant knowledge and perspectives are included and no key aspects of the topic at hand are neglected. Especially the perspectives of marginalised groups are important to this end.",
        "summary": "Stakeholder mapping enables the research team to identify all relevant actors for the issue, helping to plan next steps like conducting interviews or inviting individuals to workshops. This ensures all important actors are involved, which is crucial for the success of the solutions. Stakeholder mapping also ensures all relevant knowledge and perspectives are included, especially those of marginalized groups.",
        "question": "Why is stakeholder mapping important in the process of transdisciplinary research?",
        "ground_truths": "Stakeholder mapping enables the research team to identify all relevant actors for the issue, helping to plan next steps like conducting interviews or inviting individuals to workshops. This ensures all important actors are involved, which is crucial for the success of the solutions. Stakeholder mapping also ensures all relevant knowledge and perspectives are included, especially those of marginalized groups."
    },
    {
        "contexts": "Much in modern science is framed around statistics, for better or worse. Due to the arrogance of \"the scientific method\" being labeled based on deductive approaches, and the fact that much of the early methodological approaches were biased to and dominated by quantitative approaches. This changed partly with the rise or better increase of qualitative methods during the last decades. We should realise to this end, that the development of the methodological canon is not independent but interconnected with the societal paradigm. Hence the abundance, development and diversity of the methodological canon is in a continuous feedback loop with changes in society, but also driven from changes in society.",
        "summary": "Modern science is largely based on statistics, which were initially dominated by quantitative approaches. However, the rise of qualitative methods has led to a more diverse methodological canon, which is interconnected with societal changes.",
        "question": "How has the methodological canon in science evolved over time?",
        "ground_truths": "The methodological canon in science has evolved from being dominated by quantitative approaches to incorporating more qualitative methods. This evolution is interconnected with societal changes."
    },
    {
        "contexts": "Mixed methods are one step in this evolution. Kuhn spoke of scientific revolutions, which sounds appealing to many. As much as I like the underlying principle, I think that mixed methods are more of a scientific evolution that is slowly creeping in. The scientific canon that formed during the enlightenment and that was forged by the industrialisation and a cornerstone of modernity. The problems that arose out of this in modern science were slowly inching in between the two world wars, while methodology and especially statistics not only bloomed in full blossom, but contributed their part to the catastrophe.",
        "summary": "Mixed methods represent an evolution in scientific methodology, which has been influenced by historical events such as the Enlightenment and the two world wars. These methods have slowly become more prevalent in modern science.",
        "question": "What role do mixed methods play in the evolution of scientific methodology?",
        "ground_truths": "Mixed methods represent a step in the evolution of scientific methodology, becoming more prevalent in modern science."
    },
    {
        "contexts": "From a systematic standpoint we can now determine at least three developments: 1) Methods that were genuinely new, 2) methods that were used in a novel contexts, and 3) methods that were combined with other methods. Let us embed statistics into this line of thinking. Much of the general line of thinking in statistics in the last phase of modernity, i.e. before the two world wars. While in terms of more advanced statistics of course much was developed later and is still being developed, but some of the large breakthroughs in terms of the general line of thinking were rather early.",
        "summary": "There are three key developments in scientific methodology: the creation of new methods, the application of existing methods in new contexts, and the combination of different methods. Statistics, in particular, saw significant advancements before the two world wars.",
        "question": "What are the three key developments in scientific methodology?",
        "ground_truths": "The three key developments in scientific methodology are the creation of new methods, the application of existing methods in new contexts, and the combination of different methods."
    },
    {
        "contexts": "Not least because of the dominating position that statistics held in modern science, and also because of the lack of other forms of knowledge becoming more and more apparent, qualitative methods were increasingly developed and utilised after the second world war. I keep this dating so vague, not because of my firm belief that much was a rather continuous evolvement, but mostly because of the end of modernity. With the enlightenment divinely ending, we entered a new age where science became more open, and methods became more interchangeable within different branches of science.",
        "summary": "The dominance of statistics in modern science and the increasing need for other forms of knowledge led to the development and use of qualitative methods after the second world war. This marked a new era where scientific methods became more interchangeable across different disciplines.",
        "question": "Why did the use of qualitative methods in science increase after the second world war?",
        "ground_truths": "The use of qualitative methods in science increased after the second world war due to the dominance of statistics and the increasing need for other forms of knowledge."
    },
    {
        "contexts": "As an aspiring scientist, it is important to follow the current discussions in science. Staying on top of research is an essential skill that will enhance your science game significantly. Because that's the gist: There is more to science than searching for terms in Google Scholar. Henrik did an entire video on how he does research and stays up to date - saying I wasn't inspired by his take on the topic would be a lie. So let's get started, fellow scientist. In the game of science, ultimately, you want to: identify key papers that are relevant for your research and read the papers that cite these key papers, identify key authors in your field and follow whatever they publish next, identify key journals in your field that have a high rating (so are being cited frequently) and therefore publish the most important news in your field.",
        "summary": "For scientists, staying updated with current research is crucial. This involves identifying key papers, authors, and journals in your field. Key papers are those that are relevant and frequently cited, key authors are those who are influential in your field, and key journals are those with high ratings and frequently publish important news.",
        "question": "What are the key elements a scientist should focus on to stay updated with current research?",
        "ground_truths": "A scientist should focus on identifying key papers, authors, and journals in their field to stay updated with current research."
    },
    {
        "contexts": "You have just started out with your study program and want to understand research as such. You may have recently found out that journals exist and that journal ratings are a thing. This stage is all about you gaining knowledge about research and also discovering what interests you most. On this level, you could follow: The journals Nature and Science are very basic newspapers of science. They tend to have rather sensational titles, feature mostly short papers but also report the main news. Follow them to get the latest of the scientific world.",
        "summary": "When starting a study program, understanding research is important. This involves gaining knowledge about research and discovering your interests. At this stage, following basic science journals like Nature and Science can help you stay updated with the latest scientific news.",
        "question": "What should a beginner in a study program do to understand research and stay updated with the latest scientific news?",
        "ground_truths": "A beginner in a study program should gain knowledge about research, discover their interests, and follow basic science journals like Nature and Science to understand research and stay updated with the latest scientific news."
    },
    {
        "contexts": "You've understood the game of science and have written your first few papers for your degree program. This means you levelled up! Now it's time to follow some general resources in your field. Ask your professors for these key resources or - since we did that already for you - take some from the lists below:",
        "summary": "Once you've written your first few papers and understood the science game, it's time to level up by following general resources in your field. You can ask your professors for these resources or refer to the provided list.",
        "question": "What should a student do after writing their first few papers for their degree program?",
        "ground_truths": "After writing their first few papers, a student should level up by following general resources in their field."
    },
    {
        "contexts": "SQL is the standard database language for relational databases. It is used for retrieving and manipulating data. When analyzing data, SQL is great to retrieve and organize data. It is used by many different institutions, such as hospitals and large companies. Basically anywhere, where data needs to be analyzed! To use SQL in Python, you need to choose a package. We will have a look at the different options later. For now, we will focus on how SQL works.",
        "summary": "SQL is a standard language for managing data in relational databases. It's used in various institutions for data analysis. To use SQL in Python, a specific package is required.",
        "question": "What is the purpose of SQL in data management?",
        "ground_truths": "SQL is used for retrieving and manipulating data in relational databases. It is great for data retrieval and organization, and is used in various institutions for data analysis."
    },
    {
        "contexts": "A relational database is a collection of data elements having specified relationships following the relational model. This model for data organization was developed by E. F. Codd in 1970 and pursues the goal of making data from large databases accessible without knowledge about its structure and location in the database. The relational model allows data to be organized into tables. Each table contains one type of record which is described by a set of attributes. In the tables, each attribute is represented by a column whereas each record is represented by a row. Relationships between the records of different tables are established using primary and foreign keys. Primary keys are unique identifiers for each row of a table. Foreign keys contain the primary key of another table and thus link the rows of the tables (Codd 1970).",
        "summary": "Relational databases, developed by E. F. Codd in 1970, are collections of data elements with specified relationships. Data is organized into tables with each table containing one type of record described by attributes. Relationships between records are established using primary and foreign keys.",
        "question": "How are relationships between records established in a relational database?",
        "ground_truths": "Relationships between the records of different tables are established using primary and foreign keys. Primary keys are unique identifiers for each row of a table. Foreign keys contain the primary key of another table and thus link the rows of the tables."
    },
    {
        "contexts": "The SELECT statement is used to retrieve data from a relational database. You can either specify a list of columns that should be selected from a table or select all columns using a \u201c*\u201d. Furthermore, the source table has to be defined using the FROM clause. Instead of the SELECT statement the SELECT DISTINCT statement can be used in order to show only unique rows in the output table.",
        "summary": "The SELECT statement is used to retrieve data from a database. It can specify a list of columns to be selected from a table or select all columns using a \u201c*\u201d. The SELECT DISTINCT statement shows only unique rows in the output table.",
        "question": "What is the purpose of the SELECT statement in SQL?",
        "ground_truths": "The SELECT statement is used to retrieve data from a relational database. It can specify a list of columns that should be selected from a table or select all columns."
    },
    {
        "contexts": "Data can be filtered by using the WHERE clause. It is followed by a boolean function that outputs \u201cTrue\u201d for all records which should be included and \u201cFalse\u201d for those which should be excluded. Such Boolean functions usually contain comparisons of attributes with fixed values or other attributes. Comparisons are expressed using comparison operators (table 1). Missing values can be identified by the IS NULL operator returning \u201cTrue\u201d for all records having a null value in the data field of interest. IS NOT NULL is the negation of this operator. Expressions can be linked with the logical operators AND and OR. AND returns \"True\" if both linked expressions are true; OR returns \"True\" if one of those is.",
        "summary": "The WHERE clause in SQL is used to filter data. It uses a boolean function that outputs \u201cTrue\u201d for records to be included and \u201cFalse\u201d for those to be excluded. Comparisons are expressed using comparison operators. The IS NULL operator identifies missing values, and expressions can be linked with the logical operators AND and OR.",
        "question": "How does the WHERE clause in SQL filter data?",
        "ground_truths": "The WHERE clause in SQL filters data by using a boolean function that outputs \u201cTrue\u201d for all records which should be included and \u201cFalse\u201d for those which should be excluded. It uses comparison operators for comparisons of attributes with fixed values or other attributes. The IS NULL operator is used to identify missing values."
    },
    {
        "contexts": "The rows of the output table can be sorted using the ORDER BY keyword. The keyword is followed by the columns by which the output should be sorted. Thereby, the order of the columns determines the priority with which they are considered. The keywords ASC and DESC after a column determine whether the values are put in ascending or descending order.",
        "summary": "The ORDER BY keyword in SQL is used to sort the rows of the output table. The order of the columns determines the sorting priority. The keywords ASC and DESC determine whether the values are sorted in ascending or descending order.",
        "question": "How does the ORDER BY keyword function in SQL?",
        "ground_truths": "The ORDER BY keyword in SQL is used to sort the rows of the output table. The order of the columns determines the sorting priority. The keywords ASC and DESC after a column determine whether the values are put in ascending or descending order."
    },
    {
        "contexts": "Studying is about learning. Learning will be the main focus of your whole day, and sometimes even beyond the day. Learning to learn is a key goal that schools still miss out on big time, even if the situation has already greatly improved. I only learned to learn properly at university in a structured and self-motivated way. Balancing your learning efforts against your motivational levels is something that you develop and learn, and different people learn differently, which is a trivial detail but important to consider given the pressure that many people perceive. Being in academia means tinkering with the skills you need as an academic, often your whole life, even if you do not remain in academia.",
        "summary": "Studying is primarily about learning, which should be the main focus of a student's day. The ability to learn effectively is a crucial skill that is often overlooked in schools. It is important to balance learning efforts with motivation, and understand that different people learn in different ways. Being in academia involves continuously developing the necessary skills, regardless of whether one stays in academia or not.",
        "question": "What is the main focus of studying and how does it relate to being in academia?",
        "ground_truths": "The main focus of studying is learning. Being in academia involves continuously developing the necessary learning skills, regardless of whether one stays in academia or not."
    },
    {
        "contexts": "Academia is first and foremost about reading. Reading a lot is inevitable, and in addition you need to learn to cross-read large chunks of text and fast. Quite often it is not about getting every bit of the text, but instead the bigger picture. This is not some ability that you are being born with, but it will take years to evolve. Still, reading you must, and a lot, and all the time. The reading assignments of an average study program should amount to several ten thousands of pages, and since you have other interests that are also transported in written texts, it is important to develop strategies how to read.",
        "summary": "Academia primarily involves a lot of reading, including the ability to quickly skim through large amounts of text to understand the bigger picture. This skill takes years to develop. The average study program involves reading tens of thousands of pages, so it's crucial to develop effective reading strategies.",
        "question": "What is the primary activity in academia and why is it important?",
        "ground_truths": "The primary activity in academia is reading. It is important because it allows students to understand the bigger picture of their study program and it involves reading tens of thousands of pages."
    },
    {
        "contexts": "Science is about collaboration. Many people have a misconception about this. Collaboration does not mean that you work together non-stop, are all exited standing in front of whiteboards and sitting at round tables, cheering each other on. Instead much of the time spent in group work is about planning together, working solitarily, and then bringing the different bits and pieces together. You still will have to do most of the work alone, otherwise it is going to be a huge time drag. It is an important skill to do brainstorming in a group, and a whiteboard can indeed go a long way. Group work is however often about dragging people along, and can be even about working against people. Compromising is a great skill, but how do you deal with unequal knowledge and experience in a group is often altogether different; an unbalanced work load can be the most destructive force in any group work. Therefore, it is important to find strategies how to cope with all of these challenges, which again takes practice.",
        "summary": "Science involves collaboration, which doesn't mean constant group work, but rather planning together, working individually, and then combining the results. Most of the work is done individually to avoid wasting time. Brainstorming in a group is a valuable skill, as is compromise. Dealing with unequal knowledge and experience in a group can be challenging, and an unbalanced workload can be destructive. Therefore, it's crucial to develop strategies to cope with these challenges.",
        "question": "What does collaboration in science involve and why is it important?",
        "ground_truths": "Collaboration in science involves planning together, working individually, and then combining the results. It is important because it helps to avoid wasting time, encourages brainstorming and compromise, and helps to deal with challenges such as unequal knowledge and experience in a group."
    },
    {
        "contexts": "Integrating your thoughts quickly into a written text is at the heart of academia. If we want to learn to communicate our thoughts to others, we need to learn to write. Many people claim they are not good writers, yet I would counter argue that they are not good at developing their writing skills. The first way to become a good writer is to become a good reader. Being inspired by others can help you to consciously grasp why some sentences are better than others, and how you may borrow from the writers able to produce better sentences. It is surprising how well a professional educator on writing can empower you to this end, and at Leuphana we are fortunate enough to have the writing center that is excellent in teaching these insights. There are also some really good books on learning to write, and I would argue that among the more known books on writing, it is almost impossible to make any mistakes. Just get one that speaks to you or was recommended by somebody, and put time into it.",
        "summary": "Quickly integrating thoughts into written text is crucial in academia. To communicate effectively, one must learn to write. Many people who believe they are not good writers simply haven't developed their writing skills. Becoming a good reader is the first step to becoming a good writer. Inspiration from others can help improve writing, and professional educators can be very helpful. There are many good books on writing that can also be beneficial.",
        "question": "What is crucial in academia and how can one improve in it?",
        "ground_truths": "Quickly integrating thoughts into written text is crucial in academia. One can improve by learning to write, becoming a good reader, drawing inspiration from others, seeking help from professional educators, and reading good books on writing."
    },
    {
        "contexts": "With the rise of empirical knowledge in the Enlightenment arose the possibility to synthesize knowledge from different studies into an overview work. A Treatise on the Scurvy - A Critical and Chronological View of What has been Published on the Subject by James Lind is seen as the first systematic review, highlighting the importance of knowledge integration. Another important origin of research synthesis can be traced to the work of 17th Century astronomers who combined data sets from different studies to ammend their own observations. Systematic literature reviews gained a vital tool through the work of Karl Pearson, whose work on statistics allowed to compile the results from several datasets into an overview. His 1904 publication - in which he combined 11 studies on typhoid vaccines and highlighted irregularities in the results - can be considered the first Meta-Analysis. Meta-Analyses were subsequently applied more commonly during the 20th Century, for example in agriculture.",
        "summary": "The concept of systematic literature review originated during the Enlightenment, with James Lind's work on scurvy considered the first systematic review. The method was further developed by 17th Century astronomers and Karl Pearson, who used statistics to compile results from multiple datasets. Pearson's 1904 publication, which combined 11 studies on typhoid vaccines, is considered the first Meta-Analysis.",
        "question": "Who is considered to have conducted the first systematic literature review?",
        "ground_truths": "James Lind is considered to have conducted the first systematic literature review."
    },
    {
        "contexts": "After the Second World War, US social scientists began to recognize the need to review the rising amount of research data while considering how to reduce bias and enhance reproducibility of systematic reviews. This also led to the increasing recognition of qualitative elements. In the 1970s, statistician Gene Glass and colleagues proclaimed Meta-Analyses as a valid procedure for synthesising studies which helped to consolidate the approach. However, Systematic Literature Reviews were long viewed as second-class studies within Academia, since they did not yield primary data. This changed during the last decades, partly due to increasing interest in scientific evidence on diverse topics on the part of public policy makers, practitioners and the general public.",
        "summary": "Post World War II, US social scientists recognized the need for systematic reviews to manage the growing volume of research data, reduce bias and improve reproducibility. The 1970s saw the validation of Meta-Analyses as a method for synthesizing studies. Despite initial skepticism within academia, the importance of systematic literature reviews has grown in recent decades due to increased interest in scientific evidence from policy makers, practitioners and the public.",
        "question": "Why did the importance of systematic literature reviews grow in recent decades?",
        "ground_truths": "The importance of systematic literature reviews grew in recent decades due to increased interest in scientific evidence from policy makers, practitioners and the public."
    },
    {
        "contexts": "A Systematic Literature Review is, in short, a reproducible process in which scientific publications that contain information, ideas, data and evidence on a specific topic are gathered; studies that fulfill a previously defined level of quality are selected; and their results and insights are summarized and evaluated. For the researcher, the results from this process provide insights into the current state of research and highlight relevant new directions for (their) further research. The term 'systematic' refers to the fact that the process is structured to minimize bias and maximimize reproducibility. Being 'systematic' means being reproducible and goes along with an a priori specified, dedicated research design and an explicit documentation of the steps taken during the research.",
        "summary": "A Systematic Literature Review is a reproducible process where scientific publications on a specific topic are gathered, selected based on a predefined quality level, and their results and insights are summarized and evaluated. This process provides insights into the current state of research and highlights new directions for further research. The term 'systematic' refers to the structured, bias-minimizing, and reproducible nature of the process.",
        "question": "What does the term 'systematic' refer to in a Systematic Literature Review?",
        "ground_truths": "The term 'systematic' in a Systematic Literature Review refers to the structured, bias-minimizing, and reproducible nature of the process."
    },
    {
        "contexts": "A Systematic Literature Review follows a set of steps that is similar to any scientific research process: 1) Planning: The research is designed by formulating the question and scope of the review (here, the different forms of the review - see above - are of relevance). This planning can be both inductive as well as deductive, depending on the focus of the review. For example, the researcher may be interested in how the literature defines Sustainable Agriculture, and thus choose to search for literature that apply and preferably define this concept.",
        "summary": "A Systematic Literature Review follows a scientific research process, starting with planning where the research question and scope of the review are defined. This planning can be inductive or deductive, depending on the focus of the review. For instance, a researcher interested in how the literature defines Sustainable Agriculture would search for literature that applies and defines this concept.",
        "question": "What is the first step in conducting a Systematic Literature Review?",
        "ground_truths": "The first step in conducting a Systematic Literature Review is planning, where the research question and scope of the review are defined."
    },
    {
        "contexts": "Finally, the results of the Systematic Literature Review are compiled into a structured paper. A sample structure for the review as a preface to an original study may look like this (Rowley & Slack 2004, p.38): Basic Definitions of key terms, Why is the subject of interest?, What research has already been undertaken on the topic, and is there any research on aspects of the topic that this research might investigate? A clear summary of the results and new research opportunities that emerge from the literature review. Quotations may be used to underline specific findings from the review. In addition, the literature gathered during the review should be listed. Many reviews consists a combination of information on the specific topic, the conceptual foundation, methodological approaches, and relevant scales that are associated to the available literature.",
        "summary": "The results of a Systematic Literature Review are organized into a structured paper. This includes definitions of key terms, the reason for interest in the subject, previous research on the topic, a summary of results, and potential new research opportunities. The literature used in the review is also listed, and the review often combines information on the topic, conceptual foundation, methodologies, and relevant scales.",
        "question": "What are the key components of a Systematic Literature Review paper?",
        "ground_truths": "A Systematic Literature Review paper includes definitions of key terms, the reason for interest in the subject, previous research on the topic, a summary of results, potential new research opportunities, and a list of the literature used in the review. It often combines information on the topic, conceptual foundation, methodologies, and relevant scales."
    },
    {
        "contexts": "A literature review is a helpful starting point for any research: it provides a structured overview of the current knowledge and open questions in the respective field and may lead to new research questions that had not been obvious to the researcher before. The strength here is the systematic nature of the literature review: by not only reading randomly through literature, but instead following a systematic and reproducible approach, the researcher generates a more objective overview of the literature. Of course, the selected literature may still be biased and heavily depends on the research and selection criteria, but if this process is properly reasoned and documented, the bias may be reduced compared to a non-systematic review.",
        "summary": "A literature review is a useful starting point for research, providing a structured overview of current knowledge and open questions in a field. The systematic nature of the review allows for a more objective overview of the literature. Although the selected literature may be biased, a systematic and reproducible approach can reduce this bias.",
        "question": "Why is a systematic literature review a useful starting point for research?",
        "ground_truths": "A systematic literature review is a useful starting point for research because it provides a structured overview of current knowledge and open questions in a field. Its systematic and reproducible approach allows for a more objective overview of the literature and can reduce bias."
    },
    {
        "contexts": "The Systematic Literature Review is strongly connected to further methods. As explained before, this form of secondary research is often included as a groundwork in original primary studies to help justify the research topic, design and methodology. Indeed, the concluding paragraphs of the literature review should lead seamlessly to research propositions and methodologies. The analysis of the gathered studies, as mentioned before, can take place in a quantitative way based on a variety of quantitative approaches. Most notably, the Meta-Analysis is of relevance here.",
        "summary": "The Systematic Literature Review is linked to other methods and is often used as groundwork in original primary studies to justify the research topic, design, and methodology. The review's conclusions should lead to research propositions and methodologies. The analysis of the studies can be quantitative, with Meta-Analysis being particularly relevant.",
        "question": "How does a Systematic Literature Review contribute to original primary studies?",
        "ground_truths": "A Systematic Literature Review contributes to original primary studies by providing groundwork to help justify the research topic, design, and methodology. Its conclusions should lead to research propositions and methodologies."
    },
    {
        "contexts": "I perceive the norm of writing in a third person as a scientist as a clear indicator of the arrogance of science that is deeply rooted in the fallacy of positivism. By writing in the third person, many scientists claim they convey objective information that can be reproduced by anyone, while the first person humanises our work. I think that by clearly indicating something as our personal opinion or to be based on our personal experience we give the information more value, and not less. This is rooted in me defining myself as a critical realist. I think there is higher value information in ethics that help us make sense of the world and find meaning, and these are ontologically objective assumptions. However, when it comes to epistemological knowledge - and this is what sciences is mostly preoccupied with - we only have subjective information at our disposal.",
        "summary": "The author disagrees with the norm of writing in the third person in scientific writing, viewing it as a sign of arrogance and a fallacy of positivism. They argue that using the first person humanises the work and adds value by clearly indicating personal opinion or experience. They identify as a critical realist and believe that while ethics provide valuable, ontologically objective information, epistemological knowledge is inherently subjective.",
        "question": "Why does the author disagree with the norm of writing in the third person in scientific writing?",
        "ground_truths": "The author believes that writing in the third person in scientific writing is a sign of arrogance and a fallacy of positivism. They argue that using the first person humanises the work and adds value by clearly indicating personal opinion or experience."
    },
    {
        "contexts": "Within this epistemological knowledge, it is possible to clearly differentiate between activated observations that I made, and observations that are shared by the scientific community. For the latter, I would clearly use the third person. By doing this, I clearly acknowledge that our knowledge may change in the future, but there is a scientific canon that currently agrees on specific knowledge. An example of such knowledge would be the continuous development of life through evolution. Our knowledge about the mechanisms which we understand that are behind these processes still continuously change (consider for instance the impact of epigenetic on our lives). Still, we can widely agree that there are processes such as natural selection and mutations that drive the adaptation and further development of living organisms.",
        "summary": "The author differentiates between personal observations and those shared by the scientific community, using the third person for the latter. They acknowledge that knowledge may change, but there is currently agreed upon knowledge, such as the continuous development of life through evolution. Despite ongoing changes in understanding the mechanisms behind these processes, there is wide agreement on processes like natural selection and mutations.",
        "question": "How does the author differentiate between personal observations and those shared by the scientific community?",
        "ground_truths": "The author differentiates between personal observations and those shared by the scientific community by using the first person for personal observations and the third person for shared observations."
    },
    {
        "contexts": "The use of third person is to me a great destroyer of scientific discourse since it makes the way we conduct discourse less precise. I think the scientific community made a grave mistake when it shifted to the third voice almost exclusively, since this took away any possibility to clearly indicate whether you speak of widely accepted knowledge, knowledge accepted by a specific community, or your own experience. The last sentences are all written in the first person, because I wrote down my opinion. It is \"I\" who has this opinion, and while I am sure and even vaguely know that I am not the only person that has this opinion, the norms of the scientific community widely reject my opinion. I have no precise knowledge about other scientists who share my opinion. Hence I verbalised a new thought, and this deserves the first person. It is I who has this thought, and I offer this thought to the scientific community as a basis for discourse.",
        "summary": "The author believes that the use of third person in scientific discourse is detrimental as it reduces precision. They argue that the shift to third person has removed the ability to clearly indicate the source of knowledge, whether it's widely accepted, accepted by a specific community, or personal. They use the first person to express their opinion, acknowledging that while others may share it, it is not widely accepted in the scientific community.",
        "question": "Why does the author believe the use of third person in scientific discourse is detrimental?",
        "ground_truths": "The author believes that the use of third person in scientific discourse is detrimental as it reduces precision and removes the ability to clearly indicate the source of knowledge, whether it's widely accepted, accepted by a specific community, or personal."
    },
    {
        "contexts": "To summarise, this wiki will use three different perspectives in terms of statements. The widest most general agreement includes knowledge that is accepted by the majority of scientists - written in the third person. The second category is knowledge by a scientific sub community - \"we\". The third - innermost - category is knowledge that is proclaimed by me as a curator of this Wiki, on this Wiki, marked by \"I\". All three categories are normative, since the knowledge within all three categories may change.",
        "summary": "The wiki will use three perspectives: knowledge accepted by the majority of scientists (third person), knowledge by a scientific sub community (\"we\"), and knowledge proclaimed by the author (\"I\"). All categories are normative and subject to change.",
        "question": "What are the three perspectives used in the wiki?",
        "ground_truths": "The three perspectives used in the wiki are knowledge accepted by the majority of scientists (third person), knowledge by a scientific sub community (\"we\"), and knowledge proclaimed by the author (\"I\")."
    },
    {
        "contexts": "Academia is equally an opportunity and a challenge, and many times we cannot even say which is which. Some of its elements date back into the ages, and on the other end is academia defined by creating knowledge that evolves, pushing our insights and the way we act based upon them forward, ideally radically changing the world for the better. Academia originated in a resource surplus of early societies, which could suddenly afford to have citizens that concentrated on nothing but knowledge with all its facets. Over time, academia became subsequently more focussed, and scientific disciplines emerged out of schools of thinking. Today, there is a dendritic network of disciplines that divides into ever smaller units with strong identities, which are often defined by their focal topics, underlying concepts and theories, and the associated canon of specific methods.",
        "summary": "Academia, a blend of opportunity and challenge, is a system that creates evolving knowledge, ideally for the betterment of the world. Originating from resource-rich early societies, it has become more focused over time, with scientific disciplines emerging from schools of thought. Today, academia is a network of disciplines, each with its own focus, concepts, theories, and methods.",
        "question": "What is the origin and evolution of academia?",
        "ground_truths": "Academia originated from resource-rich early societies and has evolved over time, becoming more focused with scientific disciplines emerging from schools of thought. Today, it is a network of disciplines, each with its own focus, concepts, theories, and methods."
    },
    {
        "contexts": "While all countries to this day build on a hierarchical system where professors are the highest caste, this system has lately been drastically changed in many countries. Germany is at the forefront of a movement where you have an overall higher number of full professors, but in comparison less assistant professors and lecturers. While in the past much of the teaching was long-term shouldered by these mid-level academia workers, most of these increasingly vanished today, and much of the remaining positions are limited to a maximum of six years. This limited time frame is often one of the main breaking points in modern academia. You have 6 years for your PhD, and 6 years as a postdoc. If you do not have a permanent contract after 12 years, you are legally out.",
        "summary": "Academic systems are hierarchical, with professors at the top. However, changes have occurred, with Germany leading a movement towards more full professors and fewer assistant professors and lecturers. Many academic positions are now limited to six years, with a total of 12 years for a PhD and postdoc combined. Without a permanent contract after this period, one must leave academia.",
        "question": "What changes have occurred in the hierarchical structure of academia?",
        "ground_truths": "Changes in the hierarchical structure of academia include a movement towards more full professors and fewer assistant professors and lecturers, led by Germany. Additionally, many academic positions are now limited to six years, with a total of 12 years for a PhD and postdoc combined. Without a permanent contract after this period, one must leave academia."
    },
    {
        "contexts": "There are mainly two ways that can help prospering young academics to find their way into a successful career: A solid peer-network and one or few mentors. Without a mentor, life in academia and more importantly life into academia is almost impossible. May it be the medieval apprenticeship in the style of \"The Sword in the Stone\", shall it be the Kung Fu disciple just as in the \"36th chamber of the Shaolin\", or may it be \"Mulan\". Having someone see the raw germ and unleashing its full potential is a collaborative act of learning, and current professors can thus become one of the best bets that future professors will be better. One of my first mentors did actually always say that he only accepts disciples that are better than he is.",
        "summary": "Two key elements for young academics to succeed are a solid peer network and mentorship. Without a mentor, navigating academia is almost impossible. Mentors help to unlock the full potential of their mentees, fostering a collaborative learning environment. This mentorship can help shape better future professors.",
        "question": "What are the key elements for young academics to succeed in their career?",
        "ground_truths": "The key elements for young academics to succeed in their career are a solid peer network and mentorship. Mentors help to unlock the full potential of their mentees, fostering a collaborative learning environment."
    },
    {
        "contexts": "We drown in meetings. When the end of humanity comes, it will probably happen in a meeting. We humans are creatures that thrive through communication and collaboration, and meeting can - at least in theory - be prime places to identify goals, clear obstacles, and energise a team towards a joint goal. Instead, however, many of us cement ourselves into an endless track of useless meetings that are dull, aimless and numbing. Here, I will outline my perception why it came to this, and propose my attempts how to make it out of this conundrum.",
        "summary": "Meetings are a common part of human communication and collaboration, and can be useful for identifying goals and energising a team. However, many people find themselves stuck in a cycle of pointless, dull meetings. The author plans to explain why this happens and propose solutions.",
        "question": "Why do some people find themselves stuck in unproductive meetings?",
        "ground_truths": "Many people find themselves stuck in unproductive meetings because they lack clear goals and direction, leading to dull and aimless discussions."
    },
    {
        "contexts": "1) Meetings without an agenda or established norms. Meetings should either have a clear agenda or follow an established norm. Ideally, the agenda is either sent around before, or there is a central repository that contains the items to be discussed. Ideally, an agenda needs to be shared early enough to allow for a proper preparation. Within my role as a dean we established as a team a central document where everybody can add agenda items. This is to me really helpful as it is both empowering and also helps participants to prepare themselves for the meeting.",
        "summary": "Meetings should have a clear agenda or established norms. The agenda should be shared in advance to allow for preparation. In the author's role as a dean, a central document was created where everyone can add agenda items, which was found to be empowering and helpful for preparation.",
        "question": "What is the importance of having a clear agenda in a meeting?",
        "ground_truths": "Having a clear agenda in a meeting is important as it allows for proper preparation and helps participants to understand what will be discussed, making the meeting more productive and empowering."
    },
    {
        "contexts": "2) Unprepared Meetings. Unprepared meetings are not only due to a lack of an agenda, but also because participants did not clarify their role and manage their own expectations. If I decide to join a meeting I would ideally want to either trust the person leading the meeting, or alternatively prepare my own agenda. The latter is a strategy with which I actually made some good experience in the past. Writing your own agenda if an actual agenda is missing is a good move to ultimately turn a dull meeting into something that at least contributes to your own goals or to the parts of the institution you represent.",
        "summary": "Unprepared meetings can be due to a lack of agenda or unclear roles and expectations. The author suggests trusting the meeting leader or preparing your own agenda if one is not provided, as this can turn an unproductive meeting into a useful one.",
        "question": "How can one make an unprepared meeting productive?",
        "ground_truths": "An unprepared meeting can be made productive by either trusting the meeting leader or preparing your own agenda, which can help align the meeting with your own goals or the goals of the institution you represent."
    },
    {
        "contexts": "3) Wrong composition of people. Any given meeting will gain its dynamics out of the composition of participants. To me, the composition of team members in a meeting is so relevant since group identity is built over time. Resilience is built out of diversity, and it is quite important to enable people to have their say in a given meeting. Though some people are missing at some times - or may have colliding obligations - I came to the conclusion that in the long run, those people participating in meetings are the ones who build and propel the team spirit further.",
        "summary": "The composition of participants in a meeting is crucial as it influences the dynamics and group identity. Diversity builds resilience and it's important for everyone to have a say. Even if some people are absent at times, those who participate regularly are the ones who build and drive the team spirit.",
        "question": "Why is the composition of participants important in a meeting?",
        "ground_truths": "The composition of participants is important in a meeting because it influences the dynamics and group identity. It also builds resilience and allows everyone to have a say, which in turn builds and drives the team spirit."
    },
    {
        "contexts": "4) Lack of facilitation. Facilitation can be defined as a designed support structure by one or several people to foster constructive integration within a specific setting. Facilitation is currently somewhat of a holy grail in academia and beyond, and rightly so, as we were widely missing out on this dimension beforehand. Facilitation done right can be a form of art, and facilitation can be prove that a path can surely be more important than the actual goals. One could write a whole book on all aspects of facilitation, yet since it is experience based, the most important advice is to encourage people to try it out for themselves.",
        "summary": "Facilitation, or providing a support structure to foster constructive integration, is crucial in meetings. It's highly valued in academia and other fields, and when done correctly, can be more important than the actual goals. It's recommended for people to try facilitating themselves to gain experience.",
        "question": "What is the role of facilitation in meetings?",
        "ground_truths": "The role of facilitation in meetings is to provide a support structure that fosters constructive integration. When done correctly, facilitation can be more important than the actual goals of the meeting."
    },
    {
        "contexts": "6) Lack of balance. Lack of balance can manifest in many ways within meetings. Often, one or few people totally dominate a meeting. This can be ok if the goals are still achieved, and other participants do not feel disempowered. In my experience, it can however be quite a negative experience if a substantial amount of talking time is monopolised by one person. While this can be an indicator of some underlying problems in the team, it makes the meeting often a frustrating experience. Like commitment and facilitation, this is something where you need to build an identity as a group and openly reflect upon such a disbalance with the respective person afterwards.",
        "summary": "A lack of balance in meetings can occur when one or a few people dominate the discussion. This can be negative if it monopolises talking time and disempowers others. It may indicate underlying team issues and can make the meeting frustrating. It's important to build a group identity and address such imbalances openly.",
        "question": "What problems can arise from a lack of balance in meetings?",
        "ground_truths": "A lack of balance in meetings can lead to one or a few people dominating the discussion, which can disempower others, monopolise talking time, and make the meeting frustrating. It may also indicate underlying team issues."
    },
    {
        "contexts": "In statistics, today, there is a tendency to use more [[Agency, Complexity and Emergence|complex]] models as a sells pitch, as well as too simple models due to lack of experience. In the long run however, knowledge and experience may spread, and people may use the most appropriate models. I think a starting point for this will be pursuing different models at the same time, not in an ensemble way where the model results are put together and averaged, but in a reflexive way. Imagine I see a weather development that may have ramifications for the nomads in an area, being a foreshadow of a drought that may affect their livestock. I could not use [[Regression Analysis|simple linear models]], which would not help me to grapple such an extreme event, but would allow me to see some long-term trend statistics and get some explanation of what might happen in terms of the big picture. Then I might focus with a finer grain on the actual weather data right now, comparing climate and weather in a nested way, using non-linear statistics to focus on the actual prediction. Next, I could ask myself what happens if I take a fresh look, using a Bayesian approach to allow myself to only grapple the current data with real time updates. All these models unravel different aspects of the same phenomena, yet today -more often than not - statistics decide for one way of analysis, or combine all ways (e.g. ensemble models) as an average. Yet I propose that much knowledge is like an onion, and [[Statistics and mixed methods|different statistical models]] can help us to peel of the different layers.",
        "summary": "There is a current trend in statistics to use overly complex or overly simple models. However, the future may see a shift towards using a variety of models simultaneously to understand different aspects of the same phenomena. This could involve using different statistical methods to analyze different aspects of a situation, such as long-term trends and immediate data.",
        "question": "How might the use of statistical models evolve in the future?",
        "ground_truths": "In the future, people may use a variety of statistical models simultaneously to understand different aspects of the same phenomena, rather than relying on a single model or averaging the results of multiple models."
    },
    {
        "contexts": "Much of the knowledge that science produces is driven by scientific demand, and this can be a good thing. However, our growth-driven economy shows the catastrophic shortcoming of a line of thinking that focuses on one aspect of knowledge, all the while actively rejecting or denying other parts of knowledge or reality. Empirical knowledge depends on the questions we ask. A manager of an organisation that attempts to maximise their profits may or may not ask the right questions for their goal, as focussing on growth may be the main reason for the organisation's demise or prosperity in the future. To the manager, focusing on growth seems plausible, so they pursue it. If it is reasonable, they might misjudge. Plausibility is closely linked by definition with people being reasonable, and plausible questions have some probability of being worthwhile our investigation. Following Larry Laudan, however, it is clear that scientists of the past were not always reasonable. Larry Laudan thought that many events and discoveries in the history of science were not as rational or reasonable as we think these scientific discoveries were. Future changes such as a more tightly-knit exchange culture in science or more pronounced ethical checks of research proposals may foster more plausible research questions, but this may not the right way to more plausible research questions. Instead, society may shift in the future, and people may just not come to the conclusion to investigate questions that are implausible.",
        "summary": "Scientific knowledge is often driven by demand and can be limited by a focus on a single aspect of knowledge. The questions we ask shape our understanding, and these questions are often influenced by what seems plausible or reasonable at the time. However, what seems plausible may not always lead to the most beneficial outcomes. The future may see a shift in the types of questions being asked, influenced by changes in society and scientific culture.",
        "question": "Why is the nature of the questions we ask important in scientific research?",
        "ground_truths": "The questions we ask in scientific research shape our understanding and knowledge. If we focus on what seems plausible or reasonable, we may miss out on other important aspects of knowledge."
    },
    {
        "contexts": "Statistics was born out of the urge to understand and control system dynamics, partly with a purely non-materialistic focus such as in Astronomy - a motor of early statistics - or with a total focus on materialistic gain, as was the case in early econometrics. With the rise of the Internet, our inflict of data increased at an exponential pace - following Moore's law - triggering both positive but also devastating effect on the global dynamics we witness today. Access to information has become on of the most important privilege people have today, and much has been gained, but much was also lost in translation. Examples such as information spreading from activists showcase what can be gained, yet terrorists and insurgents are coordinating themselves equally in the digital age. Data security is stronger shifting into the focus of societal debate, but we are far away from real data security, and in many aspects seemingly only a step away from a Black Mirror episode. All the while is is clear that we have no idea of which [[To Rule And To Measure|types of data]] may be available in the future. Movement patterns of elderly risk patients can predict a risk of a heart attack weeks before it would actually happen. Research can trigger a tilt towards fairer income distribution, or support the notion of a universal basic income. Detection systems of weather, oceans and the upper crust of Earth may prevent countless losses of life in in case of disasters. However, all these examples are still connected to our reality today, and we have to assume that future people will utilise data in ways that will be hard to imagine for us today.",
        "summary": "Statistics originated from the desire to understand and control systems, with applications ranging from astronomy to economics. The rise of the Internet has led to an exponential increase in data, which has both positive and negative effects. Access to information is now a crucial privilege, but data security remains a concern. The future may see new types of data and novel ways of using it, with potential applications in health, income distribution, disaster prevention, and more.",
        "question": "What are some potential future applications of statistics and data?",
        "ground_truths": "Future applications of statistics and data could include predicting health risks, influencing income distribution policies, and improving disaster prevention systems."
    },
    {
        "contexts": "Within this module, we focused on learning simple statistics. Understanding basic descriptive statistics allows us to calculate averages, sums and many other measures that help us grasp the essentials about certain data. Data formats are essential in order to understand the diverse forms that data can take. We learned that all data is constructed, which becomes most apparent when looking at indicators which can tell some story, yet without deeper knowledge about the construction - i.e., the contexts of an indicator - it is hard to grasp. Once you get a hold of the diverse data formats, you can see then how data can represent different distributions.",
        "summary": "The module focused on basic descriptive statistics and understanding different data formats. It emphasized that all data is constructed and can represent different distributions.",
        "question": "What are the key learnings from the module on simple statistics?",
        "ground_truths": "The key learnings from the module are understanding basic descriptive statistics, the importance of data formats, and the concept that all data is constructed and can represent different distributions."
    },
    {
        "contexts": "The most simple tests can test for counts of groups within two variables (chi-square test), comparisons of two distributions (f-test) and the comparison of the mean values of two samples of a variables (t-test). Other tests avoid the question of statistical distribution by breaking the data into ranks, which are however less often applied (Wilcoxon test). A breakthrough in statistics was the development of the correlation, which allows to test whether two continuous datasets are meaningfully related. If one of the variables increases, the other variable increases as well, which would be a positive correlation. If one variable increases, and the other one decreases, we speak of a negative correlation.",
        "summary": "Simple statistical tests include chi-square, f-test, and t-test. The concept of correlation, which tests if two datasets are related, was a major breakthrough in statistics.",
        "question": "What are some examples of simple statistical tests and what was a major breakthrough in statistics?",
        "ground_truths": "Some examples of simple statistical tests are chi-square, f-test, and t-test. The development of the concept of correlation was a major breakthrough in statistics."
    },
    {
        "contexts": "While regressions were originally designed to test for clear hypotheses, these models are today utilised under diverse contexts, even in inductive research, thereby creating tensions when it comes to the interpretation of the model results. A significant regression does not necessarily indicate a causal relation. This is a matter of the normativity of the respective branch within science, and ultimately, also a question of philosophy of science. This is comparable to the analysis of variance (ANOVA), which unleashed the potential to conduct experiments, starting in agricultural research, yet quickly finding its way into psychology, biology, medicine and many other areas in science. The ANOVA allows to compare several groups in terms of their mean values, and even to test for interaction between different independent variables.",
        "summary": "Regressions, originally designed for hypothesis testing, are now used in diverse contexts, creating interpretation challenges. ANOVA, which allows comparison of group means and tests for interaction between variables, has found wide application in various scientific fields.",
        "question": "What are the uses and challenges of regression models and ANOVA in scientific research?",
        "ground_truths": "Regression models are used in diverse contexts including inductive research, but their interpretation can be challenging as a significant regression does not necessarily indicate a causal relation. ANOVA is used to compare several groups in terms of their mean values and to test for interaction between different independent variables, and has found wide application in various scientific fields."
    },
    {
        "contexts": "First, the setting. Perfecting patience in R-coding demands the right surrounding. Some people cannot code everywhere, others can code everywhere. I think this is actually inaccurate; the latter are simply better in adapting, and would do their prime work in a well defined setting as well, but this is just my own experience. I thought I am very adaptive, but learned that I benefit from a defined setting as well.",
        "summary": "The right environment is crucial for perfecting patience in R-coding. Some people can code anywhere, while others need a specific setting. The author believes that those who can code anywhere are simply better at adapting, but also benefit from a defined setting.",
        "question": "Why is the right environment important in R-coding?",
        "ground_truths": "The right environment is important in R-coding because it helps in perfecting patience and focus. Some people can adapt and code anywhere, but they also benefit from a well-defined setting."
    },
    {
        "contexts": "You need to code regularly, and a lot. Hence it is good to design a space where you can sit easily for hours to an end, and focus on your coding. You do not need the best laptop, but ideally one that is a tool that you feel comfortably with. There is the old Mac vs. a PC rant, and I guess this will never be solved. However, it is good if you can afford a computer that works well for you. Some people use a mouse and a keyboard, which can be beneficial. However, I advice you to use as few mouse clicks as possible. Try to rely on keyboard shortcuts instead, which are the easiest bits of code that you should learn first. Switching between windows, copy & paste, tab setting and shortcuts are the rudimentaries you need to master. In my experience, a second screen can get you a long way. For students, a Padlet can be a welcome extension of the screen, as it allows for more mobility. Many people can code everywhere, and this is an advantage. Therefore, it is so important to get a Laptop, and to have a a good backpack to allow for mobility and relocation.",
        "summary": "Regular coding requires a comfortable space and a good computer. It's not about Mac vs PC, but about having a tool you're comfortable with. Using keyboard shortcuts instead of a mouse can be beneficial. A second screen or a Padlet can enhance productivity. Mobility is also important, hence the need for a laptop and a good backpack.",
        "question": "What are some essential tools and conditions for effective R-coding?",
        "ground_truths": "Essential tools and conditions for effective R-coding include a comfortable space, a good computer that the coder is comfortable with, mastery of keyboard shortcuts, and possibly a second screen or a Padlet for enhanced productivity. Mobility is also important, hence the need for a laptop and a good backpack."
    },
    {
        "contexts": "When you learn programming, there is a high likelihood that you stay in one place for at least some years. If your budget permits, how about making a sedentary commitment: A second screen can make a real difference when programming, and using a screen as a main device and the laptop screen as a secondary screen can also be more comfortable on your eves and neck, as the position of the screen can be more upward. Some people are also faster on a keyboard, and it can be adjusted to different typing poses. Lastly, while you should minimise the use of a mouse or touchpad by all means, I prefer a touchpad, mostly because it is more agile in Finalcut. Overall, I think the specs of the computer matter less than people think they do. In my experience, you either do not wait at all for a calculation, or you wait very very long - if not too long - anyway. In this case, you anyway need to find an alternative server to calculate your stuff. In R, this hardly ever happens, at least while learning to code. If something calculates really long, in most cases you made a mistake. Only large data or endless loops can stall the average calculations. Bayesian stuff may pose a problem, and larger data. Hence try to avoid the latter, and brace for the Bayesian coffee break.",
        "summary": "Learning programming often involves staying in one place for years. A second screen can enhance comfort and productivity. Keyboard speed and comfort are important, and the use of a mouse or touchpad should be minimized. Computer specs are less important than thought, as long calculations usually indicate a mistake or the need for an alternative server. Large data or endless loops can stall calculations, and Bayesian calculations may require patience.",
        "question": "What are some considerations for setting up a productive programming environment?",
        "ground_truths": "Setting up a productive programming environment involves considerations such as having a second screen for comfort and productivity, mastering keyboard use and minimizing mouse or touchpad use, and understanding that computer specs are less important than often thought. It's also important to understand that long calculations usually indicate a mistake or the need for an alternative server, and that large data or endless loops can stall calculations."
    },
    {
        "contexts": "The more you know about the knowledge of the old, the more you will be able to derive from your data. While it is true that no methods can solve everything, all methods can solve more than nothing. Being experienced in scientific methods is of utter importance to get the best initial understanding of any given dataset. Statistics stand out to this end, as they are the basis of most approaches related to numbers. However, one has to recognise that there is an almost uncountable amount of statistical approaches. Start with the most important ones, as they will help you to cover most of the way. Take correlations. I think it can be done to understand the simple correlation within a few hours, maybe days. You will get the general mechanics, the formula, the deeper mathematics behind it. You can even square this with the textbook preconditions, the interpretation, maybe even the limitations. You are all set. However this will help you as much as practicing a punch alone and without an experienced teacher when you suddenly find yourself in a real fight. You may be able to throw the punch the way you practised it, but your opponent will not stand in the spot you practised to hit. Reality is messy, you need to be agile and adaptive. R coding is like kung fu, you need a lot of practice, but you also need to get into peer-to-peer practice, and you need an experienced teacher, and learn from the Ancient Ones. Just as every attack of an opponent is different, every dataset is different. However when you are versatile, you can find you goal no matter what. As the Hagakure says: Taking an enemy on the battlefield is like a hawk taking a bird. Even though one enters into the midst of thousands of them, it gives no attention to any bird other than the one it has first marked. Finding patterns in data is exactly like this. Once you become experienced and versatile, this is how you will find patterns in data.",
        "summary": "Understanding the old knowledge and being experienced in scientific methods is crucial for data analysis. Statistics are the basis of most approaches, but there are countless statistical methods. Learning correlations is a good start, but real-world data analysis requires agility and adaptability, much like kung fu. Every dataset is different, but with versatility, you can find your goal. Finding patterns in data is like a hawk marking its prey among thousands.",
        "question": "Why is versatility important in data analysis?",
        "ground_truths": "Versatility is important in data analysis because every dataset is different. Being versatile allows one to adapt to different datasets and find the goal or pattern within the data."
    },
    {
        "contexts": "Non-duality in data science relates to the difference between predictive power and explanatory power. Any given dataset can be seen from these two perspectives, and equally from none of these perspectives. This is what non-duality of data science is all about. You need to learn to see both in data, and also none. Predictive power and explanatory power are one and the same, and they are not. As the ancients said, it is bad if one thing becomes two. The same is true for data analysis. Many guide their analysis through predictive power, and they become obsessed by the desire to have what they call the best model. Thoughts on the goodness of fit circulate in their heads like wild dragons, and they never manage to see anything but the best model they can possibly achieve, and hence they fail. Many compromises have been made by people to find the best model, and sadly, the best model may never be found. As the Ancient Ones said: all models are wrong, some models are useful. One should never forget this.",
        "summary": "Non-duality in data science refers to the balance between predictive and explanatory power. Both perspectives are important and should be considered in data analysis. However, obsession with finding the best model can lead to failure, as all models are inherently flawed. The key is to find useful models, not perfect ones.",
        "question": "What is the concept of non-duality in data science?",
        "ground_truths": "Non-duality in data science refers to the balance between predictive and explanatory power. It emphasizes the importance of considering both perspectives in data analysis and warns against the obsession with finding the perfect model, reminding that all models are flawed and the goal should be to find useful ones."
    },
    {
        "contexts": "The Thought Experiment may well be the oldest scientific method. The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the ''What if'' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early enlightenment. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the ''Origin of Species'' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term \"Thought Experiment\", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.",
        "summary": "Thought Experiments, possibly the oldest scientific method, were crucial in human evolution, allowing early thinkers to explore potential futures. They bridge philosophy and natural sciences, with examples found in Ancient Greece, Buddhism, and Hinduism. Aristotle, Galileo, and Newton used Thought Experiments to reflect on the natural world. Charles Darwin's 'Origin of Species' was a Thought Experiment, later confirmed by DNA but challenged by epigenetics. The term was coined by Mach, and Lichtenberg systematized the method. In the 20th century, ethical Thought Experiments became prevalent, with Derek Parfit using them in ethical arguments.",
        "question": "Who are some early thinkers known to have used Thought Experiments?",
        "ground_truths": "Some early thinkers known to have used Thought Experiments include Aristotle, Galileo, Newton, and Charles Darwin."
    },
    {
        "contexts": "Thought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.",
        "summary": "Thought Experiments are philosophical methods that systematically ask \"What if\" questions, challenging our world assumptions. They are normative and transformative, questioning our understanding of the world. The term \"experiment\" is somewhat misleading as there is no definitive right or wrong answer, making them a form of open discourse. While some Thought Experiments may imply a presumed answer, many are open-ended, with potential answers reflecting the respondent's norms and moral constructs.",
        "question": "What is the purpose of Thought Experiments?",
        "ground_truths": "The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it."
    },
    {
        "contexts": "Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world. Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schr\u00f6dinger's Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences.",
        "summary": "Normative Thought Experiments differ from those centered on the natural world, such as dropping a feather and a stone from a high building, or Schr\u00f6dinger's Cat, which illustrates quantum states and uncertainty. These experiments demonstrate the method's flexibility. However, interpretations of Thought Experiments can be normative, as definitive proof is often elusive, and they are based on subjective realisations and inferences.",
        "question": "What is an example of a Thought Experiment centered on the natural world?",
        "ground_truths": "An example of a Thought Experiment centered on the natural world is dropping a feather and a stone from a high building, or Schr\u00f6dinger's Cat, which illustrates quantum states and uncertainty."
    },
    {
        "contexts": "We humans are known to be a mere glitch in the long cosmic developments that lay already behind us. Much time has already passed, probably billions of years, and we as humankind have only been here for a few thousand or a few hundred thousand years, depending on how you define us. Yet within this comparably short or even negligible period, we have become an entity that - according to Derek Parfit - starts to understand the universe. In the long run, this may be our most remarkable achievement, and we already understood as much as that we will probably not be there forever. This has led to many debates about the reasons for our being, and our role in the universe, and these debates will likely not end anytime soon. There exist diverse flavours to make sense of our presence, and while I invite everyone to participate in this great abyss of a debate, I am sure that it will be difficult to come up with a conclusion that we all share. Following Derek Parfit, we may agree that our future could be much better than our past, and it would be worthwhile in this spirit to go on, and to contribute to a future of humankind that could be so much better than pur past. Nihilism, existentialism and many other -isms are telling us to not buy into this optimistic rhetoric, yet personally, I would disagree.",
        "summary": "Humans have existed for a relatively short period in the cosmic timeline, yet have made significant strides in understanding the universe. This understanding has sparked debates about our existence and role in the universe. Despite differing views, there is a shared belief that our future could be better than our past, despite the pessimistic views of nihilism and existentialism.",
        "question": "What is the general consensus about the future of humankind according to Derek Parfit?",
        "ground_truths": "According to Derek Parfit, there is a shared belief that our future could be better than our past."
    },
    {
        "contexts": "Temporal autocorrelation. This principle defines that we humans value everything that occurs in the close past or future to be more relevant than occurrences in the distant future or past. This is even independent of the likelihood whether future events will actually happen. As an example, imagine that you want to have a new computer every few years, and you can pay 5\u20ac to have a 50% chance to get a new computer tomorrow. If you actually needed one, you would surely do that. However, even adjusted for inflation, many people would not pay the same 5\u20ac to have a 50 % chance to win the latest computer in 10 years. What difference does it make? It is the latest tech in any case. Temporal discounting is one of the main ways how people act unreasonably. This even extends well beyond the point where we are dead already, although this also plays a role. Our unreasonable inabilities to transcend temporal discounting extend likewise into the past. The strongest argument to this end is how many people insist that they are worse off today when compared to the past - in the 'good old days'. The mistake that is typically made is that some aspects in the past were better, while many other aspects were worse. People hence have a tendency to value the time period closer to them different from the time periods more distant. For the past, only the good things are remembered, and for the present, the negative sides are also acknowledged. It widely depends on other circumstances whether this unreasonable evaluation is better or worse than a more objective view on each time period. In any case, the sheer difference between how people evaluate distant time periods or events compared to closer ones is one indirect motivation for science to put this knowledge into perspective.",
        "summary": "Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future. This is evident in temporal discounting, where people act unreasonably by valuing the present more than the future or past. This bias extends to how people perceive their current situation compared to the past, often idealizing the past and overlooking its negative aspects.",
        "question": "What is temporal autocorrelation and how does it affect human behavior?",
        "ground_truths": "Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future. This affects human behavior in the form of temporal discounting, where people act unreasonably by valuing the present more than the future or past."
    },
    {
        "contexts": "Historical research as well as other branches of science that investigate data or information from the past allow us to put our current knowledge, contexts and situation into the contexts of past periods or events. This is widely driven by the historical sources that are at our disposal. Palaenonthology, which is the science that deals with the oldest records, is a good example how single artefacts and findings can be embedded into a quantitative timeline via carbon dating. Be measuring the decay of radionucleids in the objects, it is today possible to get a rather good tracking of the age of fossils. However, before the establishment of this hard scientific method, palaeontologists and geologists relied widely on the contexts of the sediments in which the fragments were found. The sedimental history hence become an early dating method, and many other indicators such as thin layers of global catastrophes through meteors allow to pinpoint the temporal origins with an often high precision. One prominent example is the layer at the Cratecous-Tertiary line, when an Earth-shattering meteor not only extinguished the dinosaurs, but also created a thin sedimental layer that allows to date this event in time with often remarkable precision. Other important methods in dating distant developments can be hidden in the molecules and systematics of many organisms, since sophisticated DNA analysis often allow to generate a holistic developmental history of organisms if suitable material for DNA analysis is found. To this end, preserved record in moorlands is often a source of material dating back 10 thousands of years, creating a database of the development of life itself.",
        "summary": "Historical research and other scientific fields use data from the past to contextualize our current knowledge. Techniques like carbon dating in palaeontology allow us to track the age of fossils and other artefacts. Before this method, scientists relied on sedimental history as a dating method. Other methods include analyzing molecules and systematics of organisms through DNA analysis, with preserved records in moorlands providing material dating back thousands of years.",
        "question": "What are some methods used in historical research and other scientific fields to date past events and developments?",
        "ground_truths": "Some methods used in historical research and other scientific fields to date past events and developments include carbon dating, sedimental history, and DNA analysis."
    },
    {
        "contexts": "With the rise of photography an altogether different kind of record was created, and the development of modern cartography had an equally dramatic influence on the density of knowledge that became available. The detailed scientific examination of the diverse sources since the development of the printing press by Johannes Gutenberg was already a a severe development leading to an exponential growth of the printed word. The development of printed images and multiplication of pictures that came later was an altogether different media unleashed, leading to a totally different world. Once these image started moving and even talking, human civilisation created an exponentially growing record of the world, of which the Internet is the latest development. From a methodological standpoint, this diversity of media triggered two relevant developments: An ever-increasing differentiation of empirical analyiss, and severe philosophical consequences of this brave new world.",
        "summary": "The rise of photography and modern cartography dramatically increased the density of available knowledge. The development of the printing press led to an exponential growth of the printed word, and the later development of printed images created a different media landscape. The Internet is the latest development in this exponentially growing record of the world. This diversity of media has led to an increase in empirical analysis and philosophical consequences.",
        "question": "What were the impacts of the development of photography, modern cartography, and the printing press?",
        "ground_truths": "The development of photography, modern cartography, and the printing press dramatically increased the density of available knowledge, led to an exponential growth of the printed word, created a different media landscape, and resulted in an increase in empirical analysis and philosophical consequences."
    },
    {
        "contexts": "Derek Parfit concluded that our future may be wonderful, and that we cannot make the ethical decision whether future people should exist at all. In other words, we have no moral grounds to end human history. The fact that humans can think about the future is one of the most defining elements about our species. Compare us to the chipmunk. The chipmunk may store nuts, and forget about most of these. Birds migrate in anticipation of the seasons changing. Whales may follow their food. It is probably only us who have a abstract understanding about different futures, and can adapt our actions based on this knowledge.",
        "summary": "Derek Parfit believes that our future may be wonderful and that we have no ethical grounds to decide whether future people should exist. The ability to think about the future is a defining characteristic of humans, setting us apart from other species who act based on instinct rather than abstract understanding of different futures.",
        "question": "What sets humans apart from other species in terms of thinking about the future?",
        "ground_truths": "Humans are set apart from other species by their ability to think about the future and adapt their actions based on an abstract understanding of different futures."
    },
    {
        "contexts": "While the oracles and mysticisms of the ancients were often complicated yet not systematical in their methodological approaches, this changed with modern agriculture. Human civilisation got domesticated by its crops, and depended on their harvest. The demise of the Maya may be an early testimony of crop failures, and especially in Asia and Europe, records of the central role of the harvest within the seasonal calendar have been preserved for centuries. The ripening of the wine harvest is a record often known since several centuries, and deviances from the known averages often led to catastrophic famine and migrations. To prevent such catastrophes, societies began to index and plan their agricultural practice into the future, and the rise of numbers - with examples from basically all early cultures - testify how this allowed for the thriving of many diverse cultures.",
        "summary": "The methodological approaches of ancient oracles and mysticisms were complicated but not systematic, a trend that changed with modern agriculture. Human civilization became dependent on crop harvests, with the demise of the Maya possibly being an early example of crop failure. Records of harvests have been preserved for centuries, and deviations from the average often led to famine and migration. To prevent such disasters, societies began to plan their agricultural practices for the future, leading to the thriving of diverse cultures.",
        "question": "How did the development of modern agriculture change human civilization?",
        "ground_truths": "The development of modern agriculture made human civilization dependent on crop harvests. To prevent disasters like famine and migration, societies began to plan their agricultural practices for the future, leading to the thriving of diverse cultures."
    },
    {
        "contexts": "Time series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run. In this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from Kaggle (Download date: 20.12.2022).",
        "summary": "Time series data is a sequence of data points collected at regular intervals, often used in fields like engineering, finance, or economics to analyze trends and patterns. It is typically numeric data, such as stock prices or sensory data from a fitness tracker. This article uses a household energy consumption dataset from Kaggle as an example.",
        "question": "What is time series data and where is it often used?",
        "ground_truths": "Time series data is a sequence of data points collected at regular intervals. It is often used in fields like engineering, finance, or economics to analyze trends and patterns."
    },
    {
        "contexts": "Often, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component. The latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal.",
        "summary": "Time series data often contains long-term trends, seasonality, and a residual component. These factors should be considered when analyzing the data. Depending on the goal, trends and seasonality might be important, but sometimes it's necessary to focus on the residual component, as in a 2020 project that aimed to classify the type of surface while cycling.",
        "question": "What components does time series data often contain and how might they be used in analysis?",
        "ground_truths": "Time series data often contains long-term trends, seasonality in the form of periodic variations, and a residual component. These factors should be taken into account when analyzing the data. Depending on the domain and goal, trends, and seasonality might be of interest, but sometimes, the most important information is contained in the residual component."
    },
    {
        "contexts": "The first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).",
        "summary": "The first step in dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots visualize the time domain, while scatter plots inspect the frequency domain obtained by a fast Fourier transformation, which can transform the data into different frequencies of usage and how often they occurred.",
        "question": "What is the first step in dealing with time series data and what are the uses of line plots and scatter plots?",
        "ground_truths": "The first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation."
    },
    {
        "contexts": "It is tempting to record yourself in front of the camera, talking over slides. While this may be useful for many lectures, sometimes, it might be easier for your students to understand and engage with your content when you record your lecture as a conversation with a relative, a colleague, or someone from your household to which you explain the lecture's content. Don't stage this - if you listener does not understand your lecture, you students probably won't, either. Let your listener ask questions, and explain things in more detail, if necessary. Include interactive elements in your (pre-recorded) lectures, such as small tasks or questions for the students, which you come back to during or after the lecture, or in a Zoom session.",
        "summary": "Instead of simply talking over slides, consider recording your lecture as a conversation with someone else. This can make the content more engaging and understandable for students. Include interactive elements in your lectures, like tasks or questions, to further engage students.",
        "question": "How can lecturers make their digital lectures more engaging and understandable for students?",
        "ground_truths": "Lecturers can make their digital lectures more engaging and understandable for students by recording their lecture as a conversation with someone else and including interactive elements in their lectures, like tasks or questions."
    },
    {
        "contexts": "In Zoom calls, use Mentimeter to facilitate participation for your students. You can create quizzes or ask for thoughts and let students rank items, but also show them slides or videos. You can also host Q&A sessions via Mentimeter, where students can upvote questions, which brings order into Q&A. However, a Q& A may best be supported by letting students raise their hands in the Zoom call so they can also speak and listen to fellow students, and not just read, type and click.",
        "summary": "Use Mentimeter in Zoom calls to facilitate student participation. You can create quizzes, ask for thoughts, show slides or videos, and host Q&A sessions. Let students raise their hands in the Zoom call for a more interactive Q&A session.",
        "question": "What tools can be used in Zoom calls to facilitate student participation and interaction?",
        "ground_truths": "Mentimeter can be used in Zoom calls to facilitate student participation and interaction. It allows for the creation of quizzes, asking for thoughts, showing slides or videos, and hosting Q&A sessions. Letting students raise their hands in the Zoom call can also enhance interaction."
    },
    {
        "contexts": "From time to time, give participants the opportunity to briefly (e.g. 5-10 mins.) talk with you one-on-one. You can offer an open Discord channel for this, or a Zoom room, in which you are approachable during pre-determined timeslots. Here, let students and team members join you individually. This fosters a more personal relationship and gives everyone the possibility for the type of interaction that is more spontaneous and less guarded (and therefore more personal). Give it the atmosphere similar to an exchange during a coffee break, while walking down a hallway, or similar situations to break the behaviour patterns that are typically prevalent during a class or a formal meeting.",
        "summary": "Offer opportunities for one-on-one conversations with participants via an open Discord channel or a Zoom room. This fosters a more personal relationship and allows for more spontaneous and less guarded interaction. Aim for an atmosphere similar to a casual coffee break or hallway chat.",
        "question": "How can lecturers foster a more personal relationship and spontaneous interaction with students in a digital setting?",
        "ground_truths": "Lecturers can foster a more personal relationship and spontaneous interaction with students in a digital setting by offering opportunities for one-on-one conversations via an open Discord channel or a Zoom room, aiming for an atmosphere similar to a casual coffee break or hallway chat."
    },
    {
        "contexts": "Throughout history, humans have developed myriads of tools and units that help measure, categorize, compare, analyze and understand things, processes and people - in short, the world. We have measured space and time, sound and light, the movement of atoms and molecules, and there is no end in sight. You use measurements every day - stepping onto the weighing scale in the morning (don't do that!), calculating the time you need to get to university, the amount of sugar you need for the cake, or the money you spent last weekend. You may even (unconsciously) reflect on the amount of fun different potential activities might be, or think about your level of satisfaction with your last meal. Some units of measurement are very strongly connected to the physical world, such as measuring weights or lengths, despite the existence different units to do so. Others are very much socially constructed, such as the Human Development Index (HDI), which puts a number to the whole quality of life. And then, there is a wide range in between, like the Beaufort scale which does measure something physical (wind), but is quite an arbitrary form of doing so.",
        "summary": "Humans have developed numerous tools and units to measure and understand the world, including space, time, sound, light, and atomic movement. These measurements are used daily in various aspects of life. Some units, like weight and length, are strongly tied to the physical world, while others, like the Human Development Index, are socially constructed. There are also units like the Beaufort scale that fall somewhere in between.",
        "question": "What are some examples of units of measurement that humans have developed?",
        "ground_truths": "Some examples of units of measurement that humans have developed include those for measuring weight and length, the Human Development Index for measuring quality of life, and the Beaufort scale for measuring wind."
    },
    {
        "contexts": "The same can be said for tools that enable the translation of physical or social phenomena into such units: some, like a ruler, are very much related to physical properties, while others, such as IQ tests, are based on so many assumptions about how intelligence should or could be measured, that there is no denying strong normative implications. There is no judgement to be made here, but one should be aware of the fact that units of measurement may imply specific assumptions about the world and how much we can know about it. There are things we can measure, things we can measure but are not necessarily like that, and things we cannot measure at all.",
        "summary": "Tools that translate physical or social phenomena into units of measurement can be based on physical properties, like a ruler, or on assumptions, like IQ tests. These units may imply specific assumptions about the world and our knowledge of it. There are things we can measure accurately, things we can measure with uncertainty, and things we cannot measure at all.",
        "question": "What are the implications of using tools that translate physical or social phenomena into units of measurement?",
        "ground_truths": "The implications of using tools that translate physical or social phenomena into units of measurement are that these units may imply specific assumptions about the world and our knowledge of it. There are things we can measure accurately, things we can measure with uncertainty, and things we cannot measure at all."
    },
    {
        "contexts": "Celsius, Fahrenheit and Kelvin represent the most common temperature scales used today. While Celsius is used in most places in the world, the USA and associated territories still use the Fahrenheit scale. Kelvin is based directly on Celsius and is most commonly used by scientists to communicate their results (e.g. when speaking about a temperature increase of 2K), but not really used in everyday practice in any country. In Celsius, water freezes at (~) 0\u00b0C and boils at (~) 100\u00b0C. -273,15\u00b0C represents the lowest possible temperature a gas can (theoretically) reach, and can be translated into 0 Kelvin. Kelvin is thus always 273,15 higher than Celsius. The unit of Fahrenheit is based on a thermometer using a mixture of water, ice and ammonium chloride - the minimum temperature that could be reached with this mixture was set as 0\u00b0F. Fahrenheit can be translated into Celsius as follows: \u00b0F = (\u00b0C)x(9/5) + 32",
        "summary": "Celsius, Fahrenheit, and Kelvin are the most common temperature scales. Celsius is used globally, Fahrenheit in the USA and associated territories, and Kelvin by scientists. Kelvin is based on Celsius and is always 273.15 higher. Fahrenheit is based on a thermometer using a mixture of water, ice, and ammonium chloride, with the minimum temperature set as 0\u00b0F.",
        "question": "What are the most common temperature scales and where are they used?",
        "ground_truths": "The most common temperature scales are Celsius, Fahrenheit, and Kelvin. Celsius is used in most places in the world, the USA and associated territories use the Fahrenheit scale, and Kelvin is most commonly used by scientists."
    },
    {
        "contexts": "The Richter Scale is used to measure the strength of Earthquakes. It was developed in 1935 by US American seismologists Richter and Gutenberg. It is based on the logarithm of the amplitude of the largest seismic wave of an earthquake event. Each increase of one unit on the scale signifies a tenfold increase in the magnitude of an earthquake, and a 31 times higher amount of energy released. While today's technologies can measure seismic waves below what was possible with the Richter scale, and are better to measure very strong earthquakes, the Richter scale is still often used, especially in the media.",
        "summary": "The Richter Scale, developed in 1935 by seismologists Richter and Gutenberg, measures the strength of earthquakes. It's based on the logarithm of the amplitude of the largest seismic wave, with each unit increase signifying a tenfold increase in magnitude and a 31 times higher energy release. Despite advancements in technology, the Richter scale is still widely used, particularly in the media.",
        "question": "What does the Richter Scale measure and how does it work?",
        "ground_truths": "The Richter Scale measures the strength of earthquakes. It works based on the logarithm of the amplitude of the largest seismic wave of an earthquake event, with each unit increase signifying a tenfold increase in the magnitude of an earthquake and a 31 times higher amount of energy released."
    },
    {
        "contexts": "Transcription is not a scientific method in itself, but an important step between the conduction of Interviews and the Coding process and subsequent analysis. Transcription entails the process of writing down what has been said in an Interview based on video or audio recordings. Transcription has become common practice in qualitative research (1). The transcription process is an analytical and interpretative act, which influences how the transcript represents what has actually been said or done by the interviewee(s) (1, 3). A well-conceived transcription process will support the analysis, and a bad transcription may lead to an omission or distortion of important data.",
        "summary": "Transcription, the process of converting audio or video recordings of interviews into written text, is a crucial step in qualitative research. It's an analytical and interpretative act that can influence how the interviewee's words are represented. The quality of transcription can significantly impact the subsequent analysis.",
        "question": "What is the role and importance of transcription in qualitative research?",
        "ground_truths": "Transcription is a crucial step in qualitative research, serving as an analytical and interpretative act that converts audio or video recordings of interviews into written text. The quality of transcription can significantly influence how the interviewee's words are represented and can impact the subsequent analysis."
    },
    {
        "contexts": "In a denaturalized approach, the recorded speech is written down word for word. Denaturalized transcription revolves mostly around the informational content of the interviewee's speech. Denaturalism suggests that within speech are meanings and perceptions that construct our reality (Oliver et al. 2005, p.1274). This approach is more about what is said, than how it is said, and is mostly relevant to research interested in how people conceive and communicate their world, for example in Grounded Theory research (although there are also exceptions here) (Oliver et al. 2005, p.1278).",
        "summary": "Denaturalized transcription involves writing down recorded speech word for word, focusing on the informational content of the speech. This approach is more concerned with what is said rather than how it is said, making it relevant for research interested in understanding how people perceive and communicate their reality.",
        "question": "What is denaturalized transcription and when is it most relevant?",
        "ground_truths": "Denaturalized transcription is the process of writing down recorded speech word for word, focusing primarily on the informational content of the speech. It is most relevant in research that aims to understand how people perceive and communicate their reality, such as in Grounded Theory research."
    },
    {
        "contexts": "Naturalized transcription is as detailed as possible, including stutters, pauses and other idiosyncratic elements of speech. It attempts to provide more details and a more 'realistic' representation of the interviewee's speech. The idea is to reduce a loss of information, be more 'objective' and true to the Interviewee(s), and thus impose less assumptions through the researcher. Naturalized Transcription is, for example, of interest when a conversation between individuals is recorded (in a group interview, or a Focus Group), and the way in which these individuals interact with each other is of interest (overlapping talk, turn-taking etc.) (Oliver et al. 2005, p.1275.) Grammatical or spelling mistakes are not corrected in the transcript for naturalistic transcription (1). Also, verbal cues that support the spoken word may elicit more insights for the researchers, such as dialects, increasing or decreasing volumes and specific emphases for individual words, or pauses.",
        "summary": "Naturalized transcription is a detailed process that includes all elements of speech, such as stutters and pauses, aiming to provide a realistic representation of the interviewee's speech. It seeks to reduce information loss and be more objective. This approach is useful when the interaction between individuals is of interest, such as in group interviews or focus groups. Grammatical or spelling mistakes are not corrected, and verbal cues like dialects and volume changes are noted.",
        "question": "What is naturalized transcription and in what situations is it most useful?",
        "ground_truths": "Naturalized transcription is a detailed transcription process that includes all elements of speech and aims to provide a realistic representation of the interviewee's speech. It is most useful in situations where the interaction between individuals is of interest, such as in group interviews or focus groups. It also takes into account verbal cues like dialects and volume changes."
    },
    {
        "contexts": "No matter the approach to transcription, it is important to acknowledge that any transcript is a re-construction of the Interview done by the researcher. As Skakauskaite (2012, p.24) highlights: Transcribing (...) is a form of analysis that is shaped by the researchers' examined and unexamined theories and assumptions, ideological and ethical stances, relationships with participants, and the research communities of which one is a member. A researcher that bases his or her analysis on Interview transcripts needs to acknowledge this role he or she imposes on the data in the subsequent analysis and interpretation. A lack of reflexivity in this regard may distort the research results, and impede interpretation especially in-between different research communities. Oliver et al. (2005) therefore suggests researchers to reflect upon the chosen approach and which challenges emerge based on it.",
        "summary": "Regardless of the transcription approach, it's crucial to recognize that any transcript is a researcher's re-construction of the interview. Transcription is a form of analysis influenced by the researcher's theories, assumptions, and relationships with participants. Researchers must acknowledge their role in shaping the data and reflect on their chosen approach and its challenges to avoid distorting research results.",
        "question": "Why is reflexivity important in the transcription process?",
        "ground_truths": "Reflexivity is important in the transcription process because it helps researchers acknowledge their role in shaping the data and the influence of their theories, assumptions, and relationships with participants on the transcription. Reflecting on their chosen approach and its challenges can help avoid distorting research results."
    },
    {
        "contexts": "Oliver et al. (2005) list a range of challenges in the translation of recorded speech to text. These include technical issues, such as bad recording quality. Further, transcribers may mis-interpret pronunciation, slang or accents, and write down incorrect words. Also, involuntary vocalizations, such as coughing or laughing, may provide additional information to supplement speech, but also not be of meaning, which may be hard to decide. Lastly, non-verbal cues such as waving or gesticulations may add information to the spoken word, but are only analyzable if recorded on video and/or noted down during the interview, and subsequently added to the transcript.",
        "summary": "Transcription faces several challenges, including technical issues like poor recording quality, misinterpretation of pronunciation, slang, or accents, and deciding the relevance of involuntary vocalizations like coughing or laughing. Non-verbal cues can add information but are only analyzable if recorded on video or noted during the interview.",
        "question": "What are some challenges faced in the transcription process?",
        "ground_truths": "Some challenges faced in the transcription process include technical issues like poor recording quality, misinterpretation of pronunciation, slang, or accents, and deciding the relevance of involuntary vocalizations like coughing or laughing. Non-verbal cues can also pose a challenge as they can add information but are only analyzable if recorded on video or noted during the interview."
    },
    {
        "contexts": "To understand transdisciplinarity, it should first be distinguished from multidisciplinarity and interdisciplinarity. This is especially relevant when comparing German-speaking and English-speaking resources: the US-American understanding of transdisciplinarity is rather comparable to interdisciplinarity, while the German-speaking discourse is based on the following distinctions. Multidisciplinarity revolves around the cooperation between different academic disciplines for one research endeavour. The researchers study the same topic in a parallel structure and all have their distinct goals and research questions. They share knowledge and compare their findings and may also combine their results in one big report, but the disciplinary boundaries generally remain intact. Interdisciplinarity is a research mode that deploys a higher level of cooperation between researchers from different, often rather unrelated disciplines. Disciplinary viewpoints are crossed and integrated to develop new perspectives in order to create new knowledge or re-assess existing, disciplinary knowledge. As an example, political scientists and ecologists may come together to jointly study a eco-political issue. Transdisciplinarity, then, can be seen as the next higher step. Transdisciplinarity is a reflexive, integrative, method-driven scientific principle aiming at the solution or transition of societal problems and concurrently of related scientific problems by differentiating and integrating knowledge from various scientific and societal bodies of knowledge.",
        "summary": "Transdisciplinarity, multidisciplinarity, and interdisciplinarity are distinct research approaches. Multidisciplinarity involves different academic disciplines working on the same topic with distinct goals, sharing and comparing knowledge, but maintaining disciplinary boundaries. Interdisciplinarity involves higher cooperation between unrelated disciplines, crossing and integrating viewpoints to create new knowledge. Transdisciplinarity is a step further, integrating knowledge from various scientific and societal bodies to solve societal and scientific problems.",
        "question": "What distinguishes transdisciplinarity from multidisciplinarity and interdisciplinarity?",
        "ground_truths": "Transdisciplinarity is a reflexive, integrative, method-driven scientific principle that aims at the solution or transition of societal problems and concurrently of related scientific problems by differentiating and integrating knowledge from various scientific and societal bodies of knowledge. This is different from multidisciplinarity, which involves different academic disciplines working on the same topic with distinct goals, sharing and comparing knowledge, but maintaining disciplinary boundaries, and interdisciplinarity, which involves higher cooperation between unrelated disciplines, crossing and integrating viewpoints to create new knowledge."
    },
    {
        "contexts": "Throughout the last centuries and decades, the differentiation and institutionalisation of scientific disciplines allowed academia to develop and deepen specified conceptual and methodological expertise and create distinct language and topics that have enabled these disciplines to provide plenty of insightful knowledge and guidance for society. However, there has been an increasing recognition that disciplinary viewpoints and approaches may no longer be sufficient to solve prevalent challenges that span across several scientific and societal spheres. Examples for this are ecological threats such as biodiversity loss or socio-technological challenges such as digitalisation which demand the creation of new theoretical and empirical scientific insights, but also action-oriented solutions that can be applied in policy, business, education, culture and such. With his landmark 1994 publication, Michael Gibbons introduced the term Mode 2 research which highlights this emerging form of scientific inquiry that is characterised by closer interaction between scientific, technological and industrial modes of knowledge production, by the weakening of disciplinary and institutional boundaries, by the emergence of more or less transient clusters of experts, often grouped around large projects of various kinds, and by the broadening of the criteria of quality control and by enhanced social accountability.",
        "summary": "The institutionalisation of scientific disciplines has allowed academia to develop specific expertise and provide insightful knowledge for society. However, disciplinary approaches may not be sufficient to solve challenges that span across scientific and societal spheres, such as biodiversity loss or digitalisation. Michael Gibbons' Mode 2 research highlights an emerging form of scientific inquiry characterised by closer interaction between scientific, technological and industrial modes of knowledge production, weakening of disciplinary boundaries, and enhanced social accountability.",
        "question": "What is the concept of Mode 2 research introduced by Michael Gibbons?",
        "ground_truths": "Mode 2 research, introduced by Michael Gibbons, is an emerging form of scientific inquiry that is characterised by closer interaction between scientific, technological and industrial modes of knowledge production, by the weakening of disciplinary and institutional boundaries, by the emergence of more or less transient clusters of experts, often grouped around large projects of various kinds, and by the broadening of the criteria of quality control and by enhanced social accountability."
    },
    {
        "contexts": "Example: I want to get the \u201cAT\u201d from \u201cCAT\u201d\n\n<syntaxhighlight lang=\"Python\" line>\npet = \u201cCAT\u201d\nprint( pet[1:3] ) \n</syntaxhighlight>\nHere again, you learn how Python counts. When using the slice operator, the lower bound (in this case 1) is included and the upper bound (in this case three) is not. In words this means: From the variable \"pet\", print all indexes from the index 1 (second element) to three (fourth element), excluding three.",
        "summary": "The Python slice operator includes the lower bound and excludes the upper bound. For example, to get 'AT' from 'CAT', you would use the slice operator from index 1 to 3, excluding 3.",
        "question": "How does the slice operator in Python work?",
        "ground_truths": "The Python slice operator includes the lower bound and excludes the upper bound. For example, to get 'AT' from 'CAT', you would use the slice operator from index 1 to 3, excluding 3."
    },
    {
        "contexts": "Kim has three apple trees. From each tree, they can harvest 20 apples. They plant a new tree every year but can only harvest apples one year after it has been planted. Kim has 5 housemates who each want two apples every year and three family members who want 3 apples a year. To pay her rent they need to sell 53 apples a year. Are the apple trees enough to pay the rent and give their friends the apples? If no, when is this the case?",
        "summary": "Kim has three apple trees, each yielding 20 apples per year. She plants a new tree annually, but it only starts producing after a year. She has 5 housemates and 3 family members who need apples, and she needs to sell 53 apples to pay rent. The question is whether her apple trees can meet these demands.",
        "question": "Can Kim's apple trees meet her needs and obligations?",
        "ground_truths": "The article does not provide a definitive answer, but it implies that calculations would need to be made based on the number of trees, their yield, and the demands of Kim's housemates, family, and rent."
    },
    {
        "contexts": "In the beginning of the 20th Century, ethnographers made use of early video technology for so-called 'film studies' that had a focus on human behavior and conduct, interaction and communication (Jan\u00edk et al. 2009, p.7; 1). Yet, video- or audio-taping was also often used merely to help produce more precise transcriptions of speech in qualitative research and to identify and describe the speaking individuals (4). During the second half of the 20th Century, however, the rising availability and fidelity of video technology led to an increase of its usage and to advancements of the methodology. Being able to videotape events, such as classroom sessions, offered researchers new, rich possibilities of qualitative analysis compared to solely relying on taking notes and conducting interviews (1). At the same time, this deep insight limited in turn the potential sample size in the respective studies (4). So while purely qualitative approaches - focusing on single or small numbers of classes - had become a common form of educational analysis by the 1990s, they did not allow for generalizations that were necessary for changes in educational policies.",
        "summary": "Early 20th century ethnographers used video technology for 'film studies' focusing on human behavior and interaction. Video and audio-taping were used to produce precise transcriptions of speech in qualitative research. The rise in availability and quality of video technology in the second half of the 20th century led to increased usage and methodological advancements. Videotaping events like classroom sessions offered new possibilities for qualitative analysis, but limited the potential sample size.",
        "question": "How did the rise in availability and quality of video technology impact research methods in the 20th century?",
        "ground_truths": "The rise in availability and quality of video technology in the second half of the 20th century led to increased usage and advancements in research methodology. It allowed researchers to videotape events like classroom sessions, offering new possibilities for qualitative analysis. However, this deep insight also limited the potential sample size."
    },
    {
        "contexts": "As a consequence, quantitative approaches in form of Video Surveys were introduced at the end of the 1980s (1, 4). The most notable research endeavor in this regard was the TIMSS Videotape Classroom Study in the 1990s (see Key Publications). This study relied on 'Video Surveys', which include bigger case numbers and which were made possible through improvements in multimedia computer technology (1, 4). They combine the potential for statistical analysis and thus more generalizable findings while maintaining the validity provided by the video technology. Due to the technological improvements during the last decades, video-based research was subsequently applied more often (1, 3, 6). Today, video-based methodologies are used mainly in educational research, often in the field of cross-national comparisons, but also in psychology and sociology (1, 4).",
        "summary": "Quantitative approaches like Video Surveys were introduced in the late 1980s, with the TIMSS Videotape Classroom Study being a notable example. Video Surveys, enabled by advancements in multimedia technology, allowed for larger case numbers and combined statistical analysis with the validity of video technology. Video-based research has since been applied more frequently, especially in educational research, cross-national comparisons, psychology, and sociology.",
        "question": "What are some applications of video-based research today?",
        "ground_truths": "Today, video-based methodologies are used mainly in educational research, often in the field of cross-national comparisons, but also in psychology and sociology."
    },
    {
        "contexts": "Video-based research refers to (...) research of social or educational reality based on analysis of video recordings (Jan\u00edk et al. 2009, p.7). Individual events, such as classroom sessions or cultural ceremonies, are recorded on video. The generated video footage then serves as the basis for the analysis of the respective situation. Video-based research is thus a method that includes both data gathering and data analysis (2). The video data is coded in a process similar to Content Analysis: the events happening in the video and the statements made by, for example, teachers and students are coded according to theory-led, pre-determined or inductively and iteratively developed categories (4). Often, additional data is adduced to provide contextual information (see Challenges).",
        "summary": "Video-based research involves the analysis of social or educational reality based on video recordings of individual events like classroom sessions or cultural ceremonies. The video footage serves as the basis for analysis, making video-based research a method that includes both data gathering and analysis. The video data is coded in a process similar to Content Analysis, with events and statements coded according to pre-determined or inductively developed categories. Additional data is often used to provide contexts.",
        "question": "What is the process involved in video-based research?",
        "ground_truths": "Video-based research involves recording individual events like classroom sessions or cultural ceremonies on video. The generated video footage then serves as the basis for the analysis of the respective situation. This method includes both data gathering and data analysis. The video data is coded in a process similar to Content Analysis, with events and statements coded according to pre-determined or inductively developed categories. Additional data is often used to provide contexts."
    },
    {
        "contexts": "The biggest strength of the method stems from the nature of the data. Video material can be slowed down, stopped, rewound, re-watched and stored for a long time. This makes video-based research (...) a tool for social scientists to observe phenomena that are too complex to be noticed by the naked eye. (Jan\u00edk et al. 2009, p.7). The researchers are not limited to what they were able to note down during the event itself, but can assess everything that happened as often as they like (1, 4, 6). At the same time, this complexity can be reduced to a specific aspect of interest for a first analysis, after which the researchers can come back to the data at any later point for further inquiries (2). Long-time comparisons are also possible if similarly taped videos are produced over a span of time. Several researchers - potentially from different disciplinary backgrounds - can code and analyze the material at once, and regularly discuss their insights and exchange perspectives which can lead to the emergence of new ideas and analytic categories (1, 3, 4, 6). This increases inter-coder reliability without having to determine a narrow focus of the research prior to the data collection (3, 4). In addition, the data format facilitates the communication of results since exemplary scenes or images can be taken from the video (4).",
        "summary": "The strength of video-based research lies in the nature of the data. Video material can be manipulated and stored for long periods, allowing social scientists to observe complex phenomena. Researchers can assess everything that happened in the video as often as they like, focus on specific aspects for initial analysis, and return to the data for further inquiries. Long-term comparisons are possible, and multiple researchers can code and analyze the material simultaneously, leading to the emergence of new ideas and analytic categories. The data format also facilitates the communication of results.",
        "question": "What are the strengths of video-based research?",
        "ground_truths": "The strengths of video-based research include the ability to manipulate and store video material for long periods, allowing for the observation of complex phenomena. Researchers can assess everything that happened in the video as often as they like, focus on specific aspects for initial analysis, and return to the data for further inquiries. Long-term comparisons are possible, and multiple researchers can code and analyze the material simultaneously, leading to the emergence of new ideas and analytic categories. The data format also facilitates the communication of results."
    },
    {
        "contexts": "In short: In a Visioning process, one or more desirable future states are developed in a panel of scientific and non-scientific stakeholders. In the process of Backcasting, potential pathways and necessary policies or measures to achieve these future states are developed.",
        "summary": "Visioning involves developing desirable future states with a panel of stakeholders. Backcasting then develops potential pathways and policies to achieve these states.",
        "question": "What are the processes of Visioning and Backcasting?",
        "ground_truths": "In Visioning, one or more desirable future states are developed with a panel of stakeholders. In Backcasting, potential pathways and necessary policies or measures to achieve these future states are developed."
    },
    {
        "contexts": "Visioning and Backcasting are historically connected and each of them cannot be thought without the other. Backcasting emerged earlier than Visioning during the 1970s and first in the field of energy planning, growing out of discontent with regular energy forecasting that was based on trend extrapolation, an assumed ongoing increase in the energy demand and a disregard for renewable energy technologies and energy conservation. Subsequently, Backcasting was furthered in this field in the USA, Canada and Sweden. The topics approached through Backcasting shifted towards the field of sustainability after the Our Common Future report in 1987.",
        "summary": "Visioning and Backcasting are historically linked. Backcasting, which emerged in the 1970s in energy planning, was a response to dissatisfaction with regular energy forecasting. It was further developed in the USA, Canada, and Sweden, and shifted towards sustainability after the 1987 Our Common Future report.",
        "question": "What is the historical connection between Visioning and Backcasting?",
        "ground_truths": "Visioning and Backcasting are historically connected. Backcasting emerged before Visioning in the 1970s in the field of energy planning, and was further developed in the USA, Canada, and Sweden. The focus of Backcasting shifted towards sustainability after the 1987 Our Common Future report."
    },
    {
        "contexts": "Modern Visioning approaches emerged later during the 1980s and 1990s with the incorporation of System Thinking and participatory engagement. Since its emergence and due to a rising role of participatory approaches, different versions of Visioning have been developed, including future workshops, community visioning, sustainability solution spaces, future search conference, visioneering and others. Today, Visioning is used most prominently within planning and planning research where it helps guide investments, politics and action programs.",
        "summary": "Modern Visioning approaches, which emerged in the 1980s and 1990s, incorporated System Thinking and participatory engagement. Various versions of Visioning have been developed and it is now primarily used in planning and planning research to guide investments, politics, and action programs.",
        "question": "How have modern Visioning approaches evolved and where are they primarily used?",
        "ground_truths": "Modern Visioning approaches emerged in the 1980s and 1990s with the incorporation of System Thinking and participatory engagement. Various versions of Visioning have been developed since its emergence. Today, Visioning is primarily used in planning and planning research to guide investments, politics, and action programs."
    },
    {
        "contexts": "In a Visioning process, multidisciplinary stakeholders are brought together for a workshop to collect ideas and finally formulate a joint vision as an answer to a previously asked question. A vision provides a key reference point for developing strategies to transition from the current state to a desirable future state, actively avoiding undesirable developments. This vision can take the form of qualitative or quantitative goals and targets. For example, such a vision could be a society based entirely on renewable resources or a technological process that causes minimum environmental impact.",
        "summary": "In a Visioning process, stakeholders from various disciplines gather to formulate a joint vision in response to a question. This vision, which can be qualitative or quantitative, serves as a reference for developing strategies to transition from the current to a desirable future state.",
        "question": "What is the purpose of a vision in a Visioning process?",
        "ground_truths": "In a Visioning process, a vision serves as a key reference point for developing strategies to transition from the current state to a desirable future state, actively avoiding undesirable developments."
    },
    {
        "contexts": "Visioning combines data gathering, data analysis & interpretation: In preparation, sound knowledge of the issues at hand is mandatory which may be developed by analyzing and interpreting existent data on the current state of a particular system. Based on this, a vision for said system is created, generating new qualitative and/or quantitative data. This process can be structured in four steps according to Wiek & Iwaniec (2014, p.504). Framing the Visioning process, Creating initial vision material (vision pool), Decomposing and analyzing this material, and finally Revising and recomposing the vision. The Visioning process should continuously be reflected upon and revised iteratively.",
        "summary": "Visioning involves data gathering, analysis, and interpretation. It requires understanding of the issues, which is developed by analyzing existing data on the current state of a system. A vision is then created for the system, generating new data. The process includes framing the Visioning process, creating initial vision material, analyzing this material, and revising the vision.",
        "question": "What are the steps involved in the Visioning process?",
        "ground_truths": "The Visioning process involves four steps: Framing the Visioning process, Creating initial vision material (vision pool), Decomposing and analyzing this material, and finally Revising and recomposing the vision."
    },
    {
        "contexts": "The world and environment are in a critical state as there are certain sustainability challenges, such as biodiversity loss, global warming, limited resources, and increased inequalities. From this, the need to react to them arises, both locally and globally. Developing certain sustainability competencies (skills, abilities) can be a start to learn how to do so. Wiek et al. (1) have sketched out five competencies that should be considered for academic program development. These are as follows: systems thinking, anticipatory competence, normative competence, strategic competence, and interpersonal/collaborative competence. Based on these, Caniglia et al. (2) have worked out a method, aiming at building some of the aforementioned competencies, which is called ''Walking exercise''. The method combines mental mapping and exploratory walking.",
        "summary": "Due to sustainability challenges like biodiversity loss and global warming, there's a need to develop sustainability competencies. Wiek et al. have identified five key competencies: systems thinking, anticipatory competence, normative competence, strategic competence, and interpersonal/collaborative competence. Caniglia et al. developed a method called 'Walking exercise' to build these competencies, combining mental mapping and exploratory walking.",
        "question": "What are the five key sustainability competencies identified by Wiek et al.?",
        "ground_truths": "The five key sustainability competencies identified by Wiek et al. are systems thinking, anticipatory competence, normative competence, strategic competence, and interpersonal/collaborative competence."
    },
    {
        "contexts": "Mental mapping stems from the field of behavioural geography and was especially coined by Kevin Lynch and his work \u201cThe Image of the City\u201d (3). It captures how people perceive their urban environment. Practically speaking, a person\u2019s image of a city is their mental map of it. The map usually entails the following characteristics: paths: routes along which people move throughout the city; edges: boundaries and breaks in continuity; districts: areas characterized by common characteristics; nodes: strategic focus points for orientation like squares and junctions; landmarks: external points of orientation, usually an easily identifiable physical object in the urban landscape.",
        "summary": "Mental mapping, a concept from behavioural geography, captures how people perceive their urban environment. A person's mental map of a city includes paths, edges, districts, nodes, and landmarks.",
        "question": "What are the five characteristics usually included in a person's mental map of a city?",
        "ground_truths": "The five characteristics usually included in a person's mental map of a city are paths, edges, districts, nodes, and landmarks."
    },
    {
        "contexts": "The ''walking exercise'' is a bottom-up, student-centered, and experience-based method in higher education settings to develop sustainability competencies in local contexts. It is meant for students with no or little previous knowledge in sustainability science, for example first-semester students in the environmental sciences realm and spans over one semester. The goal is to actively engage with sustainability problems in one\u2019s surroundings from the beginning on and thereby understand concepts, principles, methods of sustainability and think about solution options.",
        "summary": "The 'walking exercise' is a method used in higher education to develop sustainability competencies. It's designed for students with little or no previous knowledge in sustainability science and aims to engage students with sustainability problems in their surroundings.",
        "question": "What is the goal of the 'walking exercise' method in higher education?",
        "ground_truths": "The goal of the 'walking exercise' method in higher education is to actively engage students with sustainability problems in their surroundings and help them understand concepts, principles, and methods of sustainability and think about solution options."
    },
    {
        "contexts": "Essential for this is the development of sustainability competencies, especially systems thinking, normative, and collaborative competencies as named by Wiek et al. (2011). Systems thinking means the ability to analyze complex systems and problems across different domains (society, economy, environment) and scales (local to global) in order to engage with and tackle them. Normative competencies, or \u201cvalue-focused thinking\u201d, stands for the evaluation of sustainability through different sustainability principles and the ability to discuss and apply values, habits, perceptions and experiences. It is tightly connected with ethics and touches upon reflecting on one\u2019s own position as well. Being able to motivate people and facilitate group processes using non-violent and empathetic communications, as well as actively listening, is the essence of collaborative competencies. As it describes practices between people, it is called interpersonal competence also.",
        "summary": "The development of sustainability competencies is essential, including systems thinking, normative, and collaborative competencies. Systems thinking involves analyzing complex systems across different domains and scales. Normative competencies involve evaluating sustainability through different principles and discussing and applying values. Collaborative competencies involve motivating people and facilitating group processes using non-violent and empathetic communications.",
        "question": "What are the three key sustainability competencies essential for the 'walking exercise' method?",
        "ground_truths": "The three key sustainability competencies essential for the 'walking exercise' method are systems thinking, normative, and collaborative competencies."
    },
    {
        "contexts": "First we install BeuatifulSoup which we need for the web scraping, and time, to track the response time. We have also set the start time in the beginning. Then, we create an Html parser to parse the Html document. Parsing means that the Html code is transformed in a way that we can use it in Python. We then fetch the Html elements of interest using 'CSS selectors' which run in the backend of the select() function. By passing the 'td.titleColumn' argument, we ask the selector to fetch the td elements with the class 'titleColumn'. This data is stored in 'movies'.",
        "summary": "We install BeautifulSoup for web scraping and time to track response time. We create an Html parser to transform the Html code for Python use. We fetch Html elements using 'CSS selectors' and the 'td.titleColumn' argument to fetch td elements with the 'titleColumn' class.",
        "question": "What is the purpose of using BeautifulSoup and CSS selectors in web scraping?",
        "ground_truths": "BeautifulSoup is used for web scraping and time is used to track response time. CSS selectors are used to fetch Html elements of interest."
    },
    {
        "contexts": "We have already seen that the information related to the ratings is present in a different element altogether. So, we use a CSS selector here with a different keystring 'td.ratingColumn.imdbRating strong'. This asks the selector to fetch those td elements which have the classes 'ratingColumn' and 'imdbRating'. Then, we ask the selector to fetch the 'strong' element within each of the chosen td elements (see the Html code in the last figure). Finally, we would like to fetch the content of the 'title' attribute for each selected strong element. This data is stored in 'ratings'.",
        "summary": "Information related to ratings is in a different element. We use a CSS selector with 'td.ratingColumn.imdbRating strong' to fetch td elements with 'ratingColumn' and 'imdbRating' classes. We fetch the 'strong' element within each td element and the content of the 'title' attribute for each strong element.",
        "question": "How is the information related to ratings fetched in web scraping?",
        "ground_truths": "The information related to ratings is fetched using a CSS selector with 'td.ratingColumn.imdbRating strong' to fetch td elements with 'ratingColumn' and 'imdbRating' classes. The 'strong' element within each td element and the content of the 'title' attribute for each strong element is also fetched."
    },
    {
        "contexts": "We now iterate over 'movies' and 'ratings' objects to fetch the individual strings and extract the required data per movie. The following explains what exactly is happening in each iteration: Since each string in 'ratings' object is formatted as '(rating) based on (count) user ratings', we split the string of the rating column at each space character and pick the first and fourth elements of the split string to get the rating and the number of user ratings. Also, we format the number of user ratings by removing ',' in the string (the 2nd- 3rd line in the loop) Rank details are at the very beginning of the text in each movie string (2nd-3rd last line in the loop). Crew details are present in the title attribute of the anchor tag of the string and the movie title is embedded between the anchor tags. Thus, we use the relevant code to extract these details Release year is present between the span tags and is formatted as (year). Hence we fetch the content from the span tags and select the substring after removing the first and last elements of the original string.",
        "summary": "We iterate over 'movies' and 'ratings' to fetch strings and extract data per movie. Each string in 'ratings' is split at each space character to get the rating and number of user ratings. Rank details are at the beginning of each movie string. Crew details are in the title attribute of the anchor tag and the movie title is between the anchor tags. Release year is between the span tags.",
        "question": "How is the data per movie extracted in web scraping?",
        "ground_truths": "Data per movie is extracted by iterating over 'movies' and 'ratings' to fetch strings. Each string in 'ratings' is split at each space character to get the rating and number of user ratings. Rank details are at the beginning of each movie string. Crew details are in the title attribute of the anchor tag and the movie title is between the anchor tags. Release year is between the span tags."
    },
    {
        "contexts": "We now look at how to store the data prepared thus far. This is rather simple in Python as we simply store the data in a pandas DataFrame, with the appropriate column names provided, and then convert that to a .csv file. import pandas as pd df = pd.DataFrame(movie_data, columns = ['rank', 'title', 'crew', 'release_year', 'rating', 'num_user_ratings']) df.to_csv('movie_data.csv', index=False) We have successfully stored our data in a file titled 'movie_data.csv'.",
        "summary": "The data is stored in a pandas DataFrame with appropriate column names and then converted to a .csv file. The data is successfully stored in a file titled 'movie_data.csv'.",
        "question": "How is the scraped data stored in Python?",
        "ground_truths": "The scraped data is stored in a pandas DataFrame with appropriate column names and then converted to a .csv file."
    },
    {
        "contexts": "I consider statistics to be very powerful. Statistics became established over the last decades as one of the most important and abundantly used methods in modern science. Due to the development of modern computers and the manifestation of smartphones in the individual's daily life, there is a wealth of information available today. This triggered an almost co-evolutionary pattern where statistics evolved further in tandem with the increase of available data. While initially statistics focused on the calculation in astronomy and was widely preoccupied with testing hypotheses, this has vastly changed today.",
        "summary": "Statistics has become a powerful tool in modern science, evolving alongside the development of technology and the increase of available data. Its focus has shifted from astronomy and hypothesis testing to a broader range of applications.",
        "question": "How has the role of statistics evolved in modern science?",
        "ground_truths": "The role of statistics in modern science has evolved from being focused on astronomy and hypothesis testing to having a broader range of applications, thanks to the development of technology and the increase of available data."
    },
    {
        "contexts": "We should aim to maximise our understandings based on statistics if it is possible, but we also need to be aware of the mere fact that statistics will not provide us with all the answers. Most statisticians are well aware of that, but science went a little bit on the wrong track when results derived from statistical analysis were communicated. The confidence of the statistical results is overall very clear when it comes to numbers, although these numbers mean different things to different people. Furthermore, numbers have contexts in certain settings, and this makes it highly implausible that the same number will always mean the same under any given circumstances.",
        "summary": "While statistics can enhance our understanding, it's important to recognize that they don't provide all the answers. The interpretation of statistical results can vary among individuals and contexts, which can lead to misunderstandings.",
        "question": "Why is it important to be cautious when interpreting statistical results?",
        "ground_truths": "It's important to be cautious when interpreting statistical results because they don't provide all the answers and their interpretation can vary among individuals and contexts."
    },
    {
        "contexts": "Humans are constructed in a sense where the distant future, the distant past, but also distant places are unreasonably less important to us then they should be. On the other hand, are things close to us unreasonably more important for most people (Parfit, 2013, pp. 56\u201357). Statistics do not have that floor. They are of course biased through the people that conduct them. However, the predictive power of statistics may enable us to do more guided actions. If more people would be able to find structures and patterns in data, they could take more informed decisions.",
        "summary": "Humans tend to prioritize the immediate over the distant, which can lead to biased decisions. Statistics, while not immune to bias, can help guide actions by revealing patterns in data, enabling more informed decisions.",
        "question": "How can statistics help in making more informed decisions?",
        "ground_truths": "Statistics can help in making more informed decisions by revealing patterns in data, which can guide actions and counteract the human tendency to prioritize the immediate over the distant."
    },
    {
        "contexts": "Also, statistical analysis is made in specific software tools such as R, SPSS or Stata, or even programming languages such as Python or C++. Learning to apply these software tools and languages takes time. If you are versatile in statistical analysis, you are rewarded, since many people will need your expertise. There is more data than experienced statistical analysts in the world. But more importantly, there are more open questions than answers in the world, and some of these answers can be generated by statistical analysis.",
        "summary": "Statistical analysis is conducted using specific software tools and programming languages, which require time to learn. The demand for expertise in statistical analysis is high due to the abundance of data and open questions that can be answered through statistical analysis.",
        "question": "Why is expertise in statistical analysis valuable?",
        "ground_truths": "Expertise in statistical analysis is valuable because there is a high demand for it due to the abundance of data and open questions that can be answered through statistical analysis."
    },
    {
        "contexts": "While software for analysing statistical data was in the past widely restricted to experts, the computer revolution even offers open source solutions available to many people. Hence the proportion of people that are able to code, do statistical analysis, and understand the presentation of statistical results is dramatically increasing. If everybody would be able to interpret correlations and box plots, and would have clear understanding of statistical significance, we would be able to talk of progress.",
        "summary": "The computer revolution has made statistical analysis software more accessible, increasing the number of people who can code, perform statistical analysis, and understand statistical results. If everyone could interpret statistical data and understand statistical significance, it would signify progress.",
        "question": "How has the computer revolution impacted the field of statistical analysis?",
        "ground_truths": "The computer revolution has made statistical analysis software more accessible, leading to an increase in the number of people who can code, perform statistical analysis, and understand statistical results."
    },
    {
        "contexts": "\"The map is not the territory\" basically means that maps are a generalisation and therefore reduce the detailed richness/the richness of details of the territory. It took me very long to get this sentence. While it simply suggests that reality is different from the representation in a map, I think the sentence puzzled me because in my head it is kind of the other way around. The territory is not the map. This is how a statistician would probably approach this matter. Statistics are about generalisation, and so are maps. If you would have a map that would contain every detail of the reality how you perceive it, it would not only be a gigantic map, but it would be completely useless for orientation in the territory.",
        "summary": "\"The map is not the territory\" implies that maps, like statistics, are generalisations and do not capture every detail of reality. A map containing every detail would be too large and unhelpful for navigation.",
        "question": "What is the similarity between maps and statistics according to the phrase \"The map is not the territory\"?",
        "ground_truths": "Both maps and statistics are generalisations and do not capture every detail of reality."
    },
    {
        "contexts": "The World Caf\u00e9 is a method for facilitating discussions in big groups. With many participants, discussion rounds tend to be sprawling, slow and dominated by strong speakers. If you want to facilitate a discussion that is more effective, energetic, and inclusive, the World Caf\u00e9 is a helpful technique. It divides participants into moderated subgroups, who then wander together through a parcours of stations with different questions, all the while the atmosphere is relaxed and casual like in a caf\u00e9.",
        "summary": "The World Caf\u00e9 is a technique for facilitating effective and inclusive discussions in large groups. It involves dividing participants into moderated subgroups that move through a series of stations with different questions, creating a relaxed and casual atmosphere.",
        "question": "What is the World Caf\u00e9 method and how does it work?",
        "ground_truths": "The World Caf\u00e9 is a method for facilitating discussions in large groups. It works by dividing participants into moderated subgroups that move through a series of stations with different questions, in a relaxed and casual atmosphere."
    },
    {
        "contexts": "Depending on group size, room capacities and questions you want to discuss, different stations are set up (can be tables, boards, flipcharts etc.) with a moderator who will introduce the question and lead the discussion. The participants will be divided into as many subgroups as there are stations. Each subgroup will visit every station. The moderator welcomes the subgroup participants and introduces the question. Within a given time slot, the subgroups will discuss the question and write down their ideas and insights, before they then wander to the next station. The moderators remain with their station and welcome the next group. They present the question plus a broad overview of the insights of the former group and deepen the discussion with the new group. After the parcours has been completed by all subgroups, the moderators present the collective discussion results of each question to the full group.",
        "summary": "In the World Caf\u00e9 method, stations are set up based on group size, room capacity, and discussion topics. Participants are divided into subgroups that rotate through each station, discussing a question and recording their ideas. Moderators stay at their station, presenting the question and previous group's insights to each new group. After all subgroups have visited each station, the collective results are presented to the full group.",
        "question": "How is the World Caf\u00e9 method implemented in a discussion?",
        "ground_truths": "The World Caf\u00e9 method is implemented by setting up stations based on group size, room capacity, and discussion topics. Participants are divided into subgroups that rotate through each station, discussing a question and recording their ideas. Moderators stay at their station, presenting the question and previous group's insights to each new group. After all subgroups have visited each station, the collective results are presented to the full group."
    },
    {
        "contexts": "Before you write an article for publication in a journal, there are a few basic things that are good to think about. Content: You need to know what it is that you are planning to write about. Once you know what you want to write about, there is still the question of how to best present the content. Many papers with solid content could have been published in much more highly ranked journals if they were presented slightly differently. The issue in such cases is not so much the main content as such, but how it is prioritized, structured, at which length and level of depth it is communicated, how it is embedded in current debates, and how it is framed.",
        "summary": "Before writing a journal article, consider what you want to write about and how to present it. The presentation of the content, including its structure, depth, and framing, can affect the paper's acceptance in high-ranking journals.",
        "question": "What factors should be considered when preparing to write a journal article?",
        "ground_truths": "When preparing to write a journal article, one should consider the content they want to write about and how to best present it. The presentation of the content, including its prioritization, structure, depth, and how it is embedded in current debates and framed, can influence its acceptance in high-ranking journals."
    },
    {
        "contexts": "Your target journal: You need to understand your target journal \u2013 what will appeal to the readers, to the editors, and what kinds of particular policies does the journal have. There are papers that are very good but are rejected because they are submitted to the \u2018wrong\u2019 journal. Moreover, there are brilliant papers that are accepted in a good journal \u2013 but then nobody reads them! That can happen if the paper is a poor fit relative to what the journal overall is about. Study your journals a bit, flick through recent tables of contents, find out something about the editors, read how the journal presents itself in its \u2018scope\u2019 section, and so on. You will find that the same content might fit quite different journals, depending on how it is structured and presented.",
        "summary": "Understanding your target journal, its readers, editors, and policies is crucial. Good papers can be rejected or unread if they don't fit the journal's scope. Researching the journal, its contents, editors, and presentation can help tailor your paper to fit different journals.",
        "question": "Why is understanding the target journal important when writing a journal article?",
        "ground_truths": "Understanding the target journal is important when writing a journal article because it helps in tailoring the content to appeal to the readers and editors, and align with the journal's policies. Good papers can be rejected or remain unread if they do not fit the journal's scope. Therefore, researching the journal, its contents, editors, and presentation can help in structuring and presenting the paper in a way that fits different journals."
    },
    {
        "contexts": "For a given quality of science, how the paper is framed probably makes the biggest difference for where it is published, and how well read and cited it will be. Framing, essentially, is how you fit your paper into the \u2018bigger picture\u2019, or if you prefer a less neutral term, it is how you sell your paper. My general advice is that you choose the largest plausible frame for your paper that is reasonable. In other words, to make sure you\u2019re widely read it makes sense to find a big hook off which to hang your paper \u2013 but you have to be careful not to oversell your work. This is subjective.",
        "summary": "The framing of a paper significantly impacts its publication and readership. Framing is how the paper fits into the larger contexts or 'bigger picture'. It's advisable to choose a broad, plausible frame for your paper to ensure wider readership, but avoid overselling your work.",
        "question": "What is the role of framing in the publication and readership of a journal article?",
        "ground_truths": "Framing plays a significant role in the publication and readership of a journal article. It is how the paper fits into the larger contexts or 'bigger picture'. Choosing a broad, plausible frame for the paper can ensure wider readership, but it's important to avoid overselling the work."
    },
    {
        "contexts": "The last sentence of a paragraph or section has to fully wrap up the content. Make sure that your thought is not left hanging, only 90 % complete. You need to fully finish your thought so the reader has no doubt about your intended \u2018so what\u2019. So, when you think your paragraph is finished, ask yourself: \u2018So what?\u2019 If you just need to read out the last sentence again, you included the so-what. If, however, you need to come up with a new, additional sentence, it shows you hadn\u2019t quite finished your thought. This little exercise won\u2019t always work, but it may help you to test yourself whether your take-home message would actually get through to the reader.",
        "summary": "The final sentence of a paragraph should completely encapsulate the content, leaving no doubt about the intended message. If you need to add an additional sentence, it indicates that the thought was not fully finished. This technique may not always work, but it can help test if your main message is clear.",
        "question": "Why is it important to fully encapsulate the content in the last sentence of a paragraph?",
        "ground_truths": "It's important to fully encapsulate the content in the last sentence of a paragraph to ensure that the reader has no doubt about the intended message."
    },
    {
        "contexts": "Direct sentences are easy to understand, whereas indirect ones are difficult to understand. This was an example of a very direct sentence. It has a few properties: The most important notion comes first and tells you what the sentence is about. It has a simple structure, in this case two parallel phrases with identical grammar. It uses simple words and avoids jargon. I used \u2018direct\u2019, because it is a short and simple word. And to contrast it with something else, I used \u2018indirect\u2019. I could also have used \u2018convoluted\u2019 or some other word, but the simplest word is the one that creates the sharpest, clearest, black-and-white contrast to the other one. Note that the era of big words is finished, at least in scientific writing! The simpler the better. You don\u2019t come across as smart if you use (unnecessarily) big words, but as overly complicated. It uses unambiguous words. For example, I used \u2018whereas\u2019, which has only one meaning \u2013 I avoided \u2018while\u2019 because this has a temporal meaning as well as a contrasting meaning. Similarly, I used \u2018difficult\u2019 and not \u2018hard\u2019, because hard has multiple meanings. It is short! Many short sentences are much easier to follow than few long ones.",
        "summary": "Direct sentences, which are easy to understand, have certain properties: the most important idea comes first, they have a simple structure, use simple and unambiguous words, and are short. Using unnecessarily big words can make the sentence overly complicated. Short sentences are easier to follow than long ones.",
        "question": "What are the properties of a direct sentence?",
        "ground_truths": "A direct sentence has the most important idea first, a simple structure, uses simple and unambiguous words, and is short."
    },
    {
        "contexts": "The use of passive voice was common in academic writing in the past. For example, an ecologist might have reported \u201cAll animals heard or seen were recorded by two experienced observers\u201d. Today, this sentence would probably read \u201cWe counted all animals heard or seen\u201d. Although writing in active voice is now preferred by many journals (some have it in their style guide!), this is not to be mixed up with necessarily using the first person. In the example above, \u2018we\u2019 is the first person plural, and so in this case, indeed, the active voice wording also uses a first person perspective. But take a typical sentence from a results section, such as \u201cCompanies used a variety of strategies to market their green credentials\u201d. This is an active voice sentence (in third person plural), and there is absolutely no need to somehow try to turn it into a first-person sentence. For example, adding \u201cWe found that\u2026\u201d at the beginning of sentences is not necessary (though you will find that some authors now do just that) \u2013 in fact, it makes the sentence longer and unnecessarily complicated.",
        "summary": "Passive voice was commonly used in past academic writing, but active voice is now preferred by many journals. However, this doesn't mean that the first person should always be used. For instance, a sentence in the results section can be in active voice without using the first person, and adding 'We found that...' at the beginning of sentences is unnecessary and can make the sentence overly complicated.",
        "question": "What is the current preference in academic writing regarding the use of voice and why?",
        "ground_truths": "The current preference in academic writing is to use active voice, as it is clearer and more direct. However, this doesn't mean that the first person should always be used, as it can make the sentence unnecessarily complicated."
    },
    {
        "contexts": "With the completion of an outline you take one first step of what your research could be all about. One should never forget that research is an evolving and iterative process, though. Still, writing an outline makes you settle for what you want to focus on in this moment, and more importantly, also allows your supervisors as well as your peers to give you structured feedback. This is why any research project should start with the landmark of writing an outline. Different branches in science have different focal points and norms that an outline is build upon. Here, we present an approach that tries to do justice to the diversity of approaches that are out there, yet it is always advisable to ask your supervisors for modification if need be.",
        "summary": "An outline is the first step in research, helping to focus and receive structured feedback. It's an evolving process, with different branches of science having different norms. It's advisable to ask supervisors for modifications if needed.",
        "question": "Why is writing an outline considered an important step in the research process?",
        "ground_truths": "Writing an outline is important in the research process because it helps to focus the research, allows for structured feedback from supervisors and peers, and accommodates the evolving nature of research. It also acknowledges the different norms across various branches of science."
    },
    {
        "contexts": "All research starts with a  title. Personally, I read hundreds of titles of research papers each month, and only a small portion are appealing to my specific focus and interest to invite me to read further. Titles are the door-opener for most researchers, which is why titles should be simple and to the point. If a title is too long it will lack clarity and crispiness. If a title is too short, it will surely not give away enough information needed to know what the research is all about. Often people try to make a fancy title that is supposed to be witty or funny and contains some sort of wordplay or inside joke. Avoid this. You may go for such a title later once the research is done and the paper is written, yet such titles need to be earned. Hence especially in an outline it is best for a title that is walking the fine line of giving away enough information but not too much.",
        "summary": "Research begins with a title, which should be simple and to the point. Titles that are too long lack clarity, while those that are too short don't provide enough information. Avoid fancy or witty titles, especially in an outline.",
        "question": "What are the characteristics of a good research title?",
        "ground_truths": "A good research title should be simple and to the point. It should not be too long as it may lack clarity, and not too short as it may not provide enough information. Fancy or witty titles should be avoided, especially in an outline."
    },
    {
        "contexts": "The topical focus and its background are often what drives people. Most researchers are very exited about their topics, and it is valuable to have something that can fuel your energy while you work on your thesis. There will be ups and downs surely, yet it is still good to focus on something that not only drives you, but also current research. Timely topics are ideally embedded into a longer development that led to the emergence of the topic. You do not want to work on a topic where thousands and thousands of paper were already published, but ideally you should also not work on something that is so new that there are no shoulders to stand on. Researchers do really stand on the shoulders of giants, and despite the thrive to make innovative and timely research, we should never forget that we are part of a bigger movement. Within this movement, we will add one tiny step. A friend once said that we are drops in a wave - a fitting allegory when looking how research evolves. Still, our contribution matters, and may move the research landscape forward. What it does however more importantly is that it moves us forward. A thesis is first and foremost a prove that you can conduct research, and that you are thus able to contribute to the scientific community. Because of this, the topic is less important than most people think it is, because research is a transformational experience for the research. Do not misunderstand me, all topics of past people I supervised were really important to me. What is however even more important is that they learned to overcome themselves, and slay the dragon that is their thesis.",
        "summary": "The topic and its background often drive researchers. It's important to choose a topic that fuels your energy and contributes to current research. Avoid topics with too many or too few existing papers. Remember, research is a transformational experience and your contribution matters.",
        "question": "What factors should be considered when choosing a research topic?",
        "ground_truths": "When choosing a research topic, it's important to consider your interest and energy towards the topic, its relevance to current research, and the existing body of work on the topic. It's advisable to avoid topics with too many or too few existing papers. The transformational experience of conducting research and the contribution it makes to the scientific community are also important factors."
    },
    {
        "contexts": "The next point is a very delicate one: Research questions vs. hypotheses. Let us start with the more simple point. How much of those should you have? It is really difficult to answer one question. This is something that philosophers may do, but most research demands more questions that build on each other. See it from a structural point of view, answering one question pre-structures your introduction into one big blob of text. That is certainly not convenient. 2-5 questions seem ideal, because -let's face it- it is really hard to remember more than 5 things intuitively. We then tend to forget, and it also has the drawback to make your research seem \u00fcber-structured. Hence it can be a good heuristic to have 2-5 research questions or hypotheses. Now let us move to the more troubling part, which is the question whether it is research questions or hypotheses. From a philosophy of science standpoint, the two should be mutually exclusive. While hypotheses are surely deductive and demand a clear confirmation or rejection, research questions are somewhat more open and hence inductive. Thus while the latter has never been clear cut, it can be helpful to draw such a clear line. Some folks may now raise the streetwise question whether we should not all be thinking abductively anyway? In a sense this is where all science moves anyway, because we need both inductive and deductive approaches, and ideally combines the benefits of both in the long run. Yet this is mainly a questions of both experience and temporal grain. Working abductively means that you can clearly remark between hypotheses and research questions, because the first demand a very clear structure, while the latter are way more open. There is an underlying relation that hypotheses tend to be more quantitative, while research questions are often more qualitative. Still, this is a mere correlation and not a really causal relation, but it confuses a great many people. Chasing whether to write hypotheses or research questions is in addition often a question of the tradition of the respective scientific discipline. Many folks in natural science are still proud of their razor sharp hypotheses, and other fellow within socials science lean clearly towards research questions. Ideally, find out what your supervisors demand, which is the most simple heuristic to this end. Still, this point underlines that from a philosophy of science standpoint the silo-mentality of disciplines has its reason, but does not always make sense. What is most important for you is that an adductive procedure may be most desirable, yet only in the long run. It is part of the tradition of many sciences the postulate, test, and adjust. This works more on a time-scale of years or decades, but thus demands a temporal grain of a longer research agenda. Hence an abductive is not suitable for a shorter project such as a thesis.",
        "summary": "Research requires 2-5 questions or hypotheses, with the latter being more structured and deductive, and the former more open and inductive. The choice between them often depends on the scientific discipline and the demands of the supervisors. An abductive approach, combining both inductive and deductive methods, is desirable in the long run, but not suitable for shorter projects.",
        "question": "What is the difference between research questions and hypotheses in scientific research?",
        "ground_truths": "Research questions are more open and inductive, while hypotheses are more structured and deductive. The choice between them often depends on the scientific discipline and the demands of the supervisors."
    },
    {
        "contexts": "The next thing you want to focus on is the study area. This does not necessarily need to be a concrete space, but can also be something of a less concrete system. In a systematic review or critical content analysis this can be a branch of the literature. Within an image analysis this can be a set of paintings. In an ethnographic study this can be a specific group of people. Within a lab experiments it can be a completely artificially designed study, such as a set of planting pots that are irrigated, shaded and endure different temperature settings. This the study area is the specific system or setting that is being investigated. Again, there is a certain tradition that deductive studies are more tamed or have a deeper control or understanding of their study area, while inductive studies are more open minded and less clear cut when it comes the the pre-study understanding of the study area. Yet do not be fooled, inductive studies can be very clear when it comes to looking at something specific, these studies do it just with a different kind of open-mindedness. What is the benefit for you is to be clear in defining where you want to work in, may it be inductive or deductive. It is helpful to have a clear definition, because otherwise you will be either overworking yourself or have a sample set that is too small. The study area should ideally be chosen to make it very clear how your research represents some dynamics, pattern or mechanisms that can either serve as a basis to approximate reproducible knowledge, or at least knowledge where the path towards it can be clearly documented. Hence the study area is something that allows you to make your research specific and thus tamed. Your choice remarks a start as well as an end. Ideally, it should be exciting and represent a clear knowledge gap, but in addition it also demands to represent a bigger picture that is well represented by the chosen system. If you want to work on small business dynamics you need to work with organization that can be seen as representing the overall dynamics. If you want to make a survey, you certainly do not want to focus on outliers if you try to represent a bigger group. Ideally you build on the experience or already established researchers to learn to make the right choice concerning your study area.",
        "summary": "The study area, which can be a concrete space or a less concrete system, is the specific setting being investigated. Deductive studies often have a deeper understanding of their study area, while inductive studies are more open-minded. The study area should be clearly defined to avoid overworking or having a small sample set, and should ideally represent a clear knowledge gap and a bigger picture.",
        "question": "What is the importance of defining the study area in research?",
        "ground_truths": "Defining the study area is important to avoid overworking or having a small sample set. It should ideally represent a clear knowledge gap and a bigger picture, and it allows the research to be specific and tamed."
    },
    {
        "contexts": "\"Yes, and...\"-thinking is a technique used in improvisation. It originates from and is most commonly known in improvisational theater, but it can also be applied in any other contexts where new ideas shall be developed. Spontaneous improvisation building on \"Yes, and...\" can improve brainstorming processes and encourage team members to better cooperate and listen to each other in the process of idea generation. You may call it a proper philosophy on how to work and communicate with each other.",
        "summary": "\"Yes, and...\" is a technique from improvisational theater that can be used in any contexts where new ideas are needed. It can enhance brainstorming and foster better cooperation and communication among team members.",
        "question": "What is the \"Yes, and...\" technique and how can it be used?",
        "ground_truths": "\"Yes, and...\" is a technique used in improvisation, originating from improvisational theater. It can be applied in any contexts where new ideas need to be developed, improving brainstorming processes and encouraging better cooperation and communication among team members."
    },
    {
        "contexts": "There are two parts to \"Yes, and...\": the \"Yes!\" and the \"and...\". The \"Yes\" means not to immediately judge other people's ideas and decline them if they do not seem suitable, but rather be positive about them. The \"and...\"-element encourages the addition of new information and thoughts which can lead to more complex ideas. \"Yes, and...\" does not necessarily mean that every new sentence has to begin with the words \"Yes, and\". Instead, it is about the mental attitude of accepting whatever other people offer and continuing or expanding upon their line of thought. It means going with the flow instead of blocking ideas and losing the dynamics of an ongoing discussion. \"Yes, and\" is about losing one's ego and fostering positivity in a group.",
        "summary": "\"Yes, and...\" consists of two parts: \"Yes!\" and \"and...\". \"Yes\" is about being open to others' ideas, while \"and...\" encourages adding new information to develop more complex ideas. It's not about starting every sentence with \"Yes, and\", but about accepting and building on others' thoughts, fostering positivity in a group.",
        "question": "What are the two parts of the \"Yes, and...\" technique and what do they mean?",
        "ground_truths": "The two parts of the \"Yes, and...\" technique are \"Yes!\" and \"and...\". \"Yes\" means not to immediately judge or decline others' ideas, but to be positive about them. The \"and...\" part encourages the addition of new information and thoughts, leading to more complex ideas. It's about accepting and expanding on others' thoughts, fostering positivity in a group."
    }

]


