{
    "mediawiki": {
        "@xmlns": "http://www.mediawiki.org/xml/export-0.10/",
        "@xmlns:xsi": "http://www.w3.org/2001/XMLSchema-instance",
        "@xsi:schemaLocation": "http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd",
        "@version": "0.10",
        "@xml:lang": "en",
        "siteinfo": {
            "sitename": "Sustainability Methods",
            "dbname": "sustevfz_mw19527",
            "base": "https://sustainabilitymethods.org/index.php/Main_Page",
            "generator": "MediaWiki 1.33.0",
            "case": "first-letter",
            "namespaces": {
                "namespace": [
                    {
                        "@key": "-2",
                        "@case": "first-letter",
                        "#text": "Media"
                    },
                    {
                        "@key": "-1",
                        "@case": "first-letter",
                        "#text": "Special"
                    },
                    {
                        "@key": "0",
                        "@case": "first-letter"
                    },
                    {
                        "@key": "1",
                        "@case": "first-letter",
                        "#text": "Talk"
                    },
                    {
                        "@key": "2",
                        "@case": "first-letter",
                        "#text": "User"
                    },
                    {
                        "@key": "3",
                        "@case": "first-letter",
                        "#text": "User talk"
                    },
                    {
                        "@key": "4",
                        "@case": "first-letter",
                        "#text": "Sustainability Methods"
                    },
                    {
                        "@key": "5",
                        "@case": "first-letter",
                        "#text": "Sustainability Methods talk"
                    },
                    {
                        "@key": "6",
                        "@case": "first-letter",
                        "#text": "File"
                    },
                    {
                        "@key": "7",
                        "@case": "first-letter",
                        "#text": "File talk"
                    },
                    {
                        "@key": "8",
                        "@case": "first-letter",
                        "#text": "MediaWiki"
                    },
                    {
                        "@key": "9",
                        "@case": "first-letter",
                        "#text": "MediaWiki talk"
                    },
                    {
                        "@key": "10",
                        "@case": "first-letter",
                        "#text": "Template"
                    },
                    {
                        "@key": "11",
                        "@case": "first-letter",
                        "#text": "Template talk"
                    },
                    {
                        "@key": "12",
                        "@case": "first-letter",
                        "#text": "Help"
                    },
                    {
                        "@key": "13",
                        "@case": "first-letter",
                        "#text": "Help talk"
                    },
                    {
                        "@key": "14",
                        "@case": "first-letter",
                        "#text": "Category"
                    },
                    {
                        "@key": "15",
                        "@case": "first-letter",
                        "#text": "Category talk"
                    }
                ]
            }
        },
        "page": [
            {
                "title": "A/B Testing",
                "ns": "0",
                "id": "1130",
                "revision": {
                    "id": "7230",
                    "timestamp": "2023-06-23T16:28:39Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This entry introduces you to the concept of A/B testing.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9295",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==A/B Testing in a nutshell==\nA/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.\n\n===Goal Definition===\nThe goal identifies problems or optimization potential to improve the software product. For example, in the case of a website, the goal could be to increase newsletter subscriptions or improve the conversion rate through changing parts of the website.\n\n===Hypotheses Formulation===\nTo determine if a particular change is better than the default version, a two-sample hypothesis test is conducted to determine if there are statistically significant differences between the two samples (version A and B). This involves stating the null hypothesis and the alternative hypothesis.\n\nFrom the perspective of an A/B test, the null hypothesis states that there is no difference between the control and treatment group, while the alternative hypothesis states that there is a difference between the two groups which is influenced by a non-random cause.\n\nIn most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n'''For example:'''\n\n\"To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.\"\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\n===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.\n\nThe sample size is determined by the percentage of users included in the test variants (control and treatment) and the duration of the experiment. As the experiment runs for a longer period of time, more visitors are exposed to the variants, resulting in an increase in the sample size. Because many external factors vary over time, it is important to randomize over time by running the control and treatment variants simultaneously at a fixed percentage throughout the experiment. Thereby the goal is to obtain adequate statistical power, where the statistical power of an experiment is the probability of detecting a particular effect if it exists. In practice, one can assign any percentages to the control and treatment, but 50% gives the experiment maximum statistical power.\n\nFurthermore, it is important to analyze only the subset of the population/users that were potentially affected. For example, in an A/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.\n\n==Execution Phase==\nThe execution phase involves implementing the study design, collecting data, and monitoring the study to ensure it is conducted according to the plan. During this phase, users are randomly assigned to the control or treatment group ensuring that the study is conducted in a controlled and unbiased manner.\n\n==Evaluation Phase==\nThe evaluation phase involves analyzing the data collected during the study and interpreting the results. This phase is crucial for determining the statistical significance of the results and drawing valid conclusions about whether there was a statistical significant difference between the treatment group and the control group. One commonly used method is calculating the [[Designing_studies#P-value|p-value]] of the statistical test, or by using [https://www.youtube.com/watch?v=9TDjifpGj-k Bayes' theorem] calculating the probability that the treatment had a positive effect based on the observed data and the prior beliefs about the treatment.\n\nDepending on the type of data being collected different [[Simple Statistical Tests|statistical tests]] should be considered. For example, when dealing with discrete metrics such as click-through rate, the [https://mathworld.wolfram.com/FishersExactTest.html Fisher exact test] can be used to calculate the exact p-value, while the [https://www.youtube.com/watch?v=7_cs1YlZoug chi-squared test] may be more appropriate for larger sample sizes.\n\nIn the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon rank sum test] may be more appropriate.\n\n==Advantages and Limitations of A/B Testing==\n'''Advantages'''\nA/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.\n\n'''Limitations'''\nDespite its advantages, A/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented.\n\n==Key Publications==\nKohavi, Ron, and Roger Longbotham. \u201cOnline Controlled Experiments and A/B Testing.\u201d Encyclopedia of Machine Learning and Data Mining, 2017, 922\u201329. https://doi.org/10.1007/978-1-4899-7687-1_891Add to Citavi project by DOI.\n\nKoning, Rembrand, Sharique Hasan, and Aaron Chatterji. \u201cExperimentation and Start-up Performance: Evidence from A/B Testing.\u201d Management Science 68, no. 9 (September 2022): 6434\u201353. https://doi.org/10.1287/mnsc.2021.4209Add to Citavi project by DOI.\n\nSiroker, Dan, and Pete Koomen. A / B Testing: The Most Powerful Way to Turn Clicks Into Customers. 1st ed. Wiley, 2015.\n\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Malte Bartels. Edited by Milan Maushart"
                    },
                    "sha1": "flyirjxi1bhmsftxsnsddy011ybrvk0"
                }
            },
            {
                "title": "ANOVA",
                "ns": "0",
                "id": "721",
                "revision": {
                    "id": "6748",
                    "parentid": "6747",
                    "timestamp": "2022-08-02T06:35:08Z",
                    "contributor": {
                        "username": "Annrau",
                        "id": "128"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15675",
                        "#text": "'''Note:''' This entry introduces the Analysis of Variance. For more on Experiments, in which ANOVAs are typically conducted, please refer to the enries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n\n[[File:ConceptANOVA.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[ANOVA]]**]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' The Analysis of Variance is a statistical method that allows to test differences of the mean values of groups within a sample.\n\n\n== Background ==\n[[File:SCOPUS ANOVA.png|400px|thumb|right|'''SCOPUS hits per year for ANOVA until 2019.''' Search terms: 'ANOVA' in Title, Abstract, Keywords. Source: own.]]\nWith a rise in knowledge during the [[History of Methods|Enlightenment]], it became apparent that the controlled setting of a [[Experiments|laboratory]] were not enough for experiments, as it became obvious that more knowledge was there to be discovered in the [[Field experiments|real world]]. First in astronomy, but then also in agriculture and other fields, the notion became apparent that our reproducible settings may sometimes be hard to achieve within real world settings. Observations can be unreliable, and errors in measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter (Rucci & Tweney 1980). The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct experiments under field conditions. \n\nConsequently, building on the previous development of the [[Simple_Statistical_Tests#One_sample_t-test|t-test]], Fisher proposed the Analysis of Variance, abbreviated as ANOVA (Rucci & Tweney 1980). '''It allowed for the comparison of variables from experimental settings, comparing how a [[Data_formats#Continuous_data|continuous]] variable fared under different experimental settings.''' Doing experiments in the laboratory reached its limits, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory. It also became questionable whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a dramatic effect on their design, conduct and outcome. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. Fisher and his ANOVA became the forefront of a new development of scientifically designed experiments, which allowed for a systematic [[Experiments and Hypothesis Testing|testing of hypotheses]] under field conditions, taming variance through replicates. '''The power of repeated samples allowed to account for the variance under field conditions, and thus compare differences in [[Descriptive_statistics|mean]] values between different treatments.''' For instance, it became possible to compare different levels of fertiliser to optimise plant growth. \n\nEstablishing the field experiment became thus a step in the scientific development, but also in the industrial capabilities associated to it. Science contributed directly to the efficiency of production, for better or worse. Equally, the systematic experiments translated into other domains of science, such as psychology and medicine. \n\n\n== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments.\n\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...(graph based on Crawley 2007, p. 451)]]\n\nSingle factor analysis that are also called '[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\nThe data that is of relevance to ANOVAs can be ideally visualised in [[Introduction_to_statistical_figures#Boxplot|boxplots,]] which allows for an initial visualisation of the data distribution, since the classical ANOVA builds on the [[Regression Analysis|regression model]], and thus demands data that is [[Data_distribution#The_normal_distribution|normally distributed]]. '''If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference.''' When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIn addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs. \n\n\n== Strengths & Challenges ==\nThe ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.\n\nIn addition, the ANOVA is equally limited as the regression, as both build on the [[Data_distribution#The_normal_distribution|normal distribution]]. Extensions of the ANOVA translated its analytical approach into the logic of [[Generalized Linear Models|generalised linear models]], enabling the implementation of other distributions as well. What unites all different approaches is the demand that the ANOVA has in terms of data, and with increasing complexity, the demands increase when it comes to the sample sizes. Within experimental settings, this can be quite demanding, which is why the ANOVA only allows to test very constructed settings of the world. All categories that are implemented as predictors in an ANOVA design represent a constructed worldview, which can be very robust, but is always a compromise. The ANOVA thus tries to approximate causality by creating more rigid designs. However, we have to acknowledge that experimental designs are always compromises, and more knowledge may become available later. Within clinical trials - most of which have an ANOVA design at their heart - great care is taken into account in terms of robustness and documentation, and clinical trial stages are built on increasing sample sizes to minimise the harm on humans in these experiments.\n\n'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis.\n\n\n== Normativity ==\nDesigning an ANOVA-based design demands experience, and knowledge of the previous literature. The deductive approach of an ANOVA is thus typically embedded into an incremental development in the literature. ANOVA-based designs are therefore more often than not part of the continuous development in normal science. However, especially since the millennium, other more advanced approaches gained momentum, such as mixed effect models, information theoretical approaches, and structural equation models. The rigid root of the normal distribution and the basis of p-values is increasingly recognised as rigid if not outright flawed, and model reduction in more complex ANOVA designs is far from coherent between different branches of sciences. Some areas of science reject p-driven statistics altogether, while other branches of science are still publishing full models without any model reduction whatsoever. In addition, the ANOVA is today also often used to analyse inductive datasets, which is technically ok, but can infer several problems from a statistical standpoint, as well as based on a critical perspective rooted in a coherent theory of science. \n\nHence the ANOVA became a swiss army knive for group comparison for a continuous variable, and whenever different category levels need to be compared across a dependent variable, the ANOVA is being pursued. Whether there is an [[Causality|actual question of dependency]] is often ignored, let alone model assumptions and necessary preconditions. '''Science evolved, and with it, our questions became ever more complex, as are the problems that we face in the world, or that want to test.''' [[Agency, Complexity and Emergence#Complexity|Complexity]] reigns, and simple designs are more often than not questioned. The ANOVA remains as one of the two fundamental models of deductive statistics, with [[Regression Analysis|regression]] being the other important line of thinking. As soon as rigid questions of dependence were conveniently ignored, statisticians - or the researchers that applied statistics - basically dug the grave for these rigid yet robust approaches. There are still many cases where the ANOVA represents the most parsimonious and even adequate model. However, as long as positivist scientists share a room with a view in their ivory tower and fail to clearly indicate the limitations of their ANOVA-based designs, they undermine their [[Glossary|credibility]], and with it the [[Glossary|trust]] between science and society. The ANOVA is a testimony of how much statsicsi can serve society, for better or worse. The ANOVA may serve as a sound approximations of knowledge, yet at its worst it speaks of the arrogance of researchers who imply [[Causality|causality]] into mere patterns that can and will change once more knowledge becomes available.\n\n== Outlook ==\nThe Analysis of Variance was one of the most relevant contributions of statistics to the developments of the 20th century. By allowing for the systematic testing of hypotheses, not only did a whole line of thinking of the theory of science evolve, but whole disciplines were literally emerging. Lately, frequentist statistics was increasingly critizised for its reliance on p-values. Also, the reproducibility crisis highlights the limitations of ANOVA-based designs, which are often not reproducible. Psychological research faces this challenge for instance by pre-registering studies, indicating their statistical approach before approaching the data, and other branches of science are also attempting to do more justice to the limitations of the knowledge of experiments. In addition, new ways of experimentation of science evolve, introducing a systematic approach to case studies and solution oriented approaches. This may open a more systematic approach to inductive experiments, making documentation a key process in the creation of a canonised knowledge. Scientific experiments were at the forefront of developments that are seen more critically regarding their limitations. Taking more complexities into account, ANOVAs become a basis for more advanced statistics, and they can indeed serve as a robust basis if the limitations are clearly indicated, and the ANOVA designs add to parts of a larger picture of knowledge. \n\n\n== Key Publications ==\n'''Theoretical'''\n\nCrawley, M. J. (2007). The R book. John Wiley & Sons.\n\n== References ==\nCrawley, M. J. (2007). The R book. John Wiley & Sons.\nRucci, A. J., & Tweney, R. D. (1980). Analysis of variance and the\" second discipline\" of scientific psychology: A historical account. Psychological Bulletin, 87(1), 166.\n\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "05jccp6pitjh4a6z802xdubd16hoobb"
                }
            },
            {
                "title": "A matter of probability",
                "ns": "0",
                "id": "153",
                "revision": {
                    "id": "7034",
                    "parentid": "7023",
                    "timestamp": "2023-04-18T12:47:53Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8944",
                        "#text": "[[File:Cube-1655118 1280.jpg|thumb|The most common example explaining probability is rolling the dice.]]\n'''[https://www.youtube.com/watch?v=uzkc-qNVoOk Probability] indicates the likelihood whether something will occur or not.''' Typically, probabilities are represented by a number between zero and one, where one indicates the hundred percent probability that an event may occur, while zero indicates an impossibility of this event to occur. \n__NOTOC__\n[https://www.britannica.com/science/probability/Risks-expectations-and-fair-contracts The concept of probability goes way back] to Arabian mathematicians and was initially strongly associated with cryptography. With rising recognition of preconditions that need to be met in order to discuss probability, [[Glossary|concepts]] such as evidence, validity, and transferability were associated with probabilistic thinking. Probability plays also a role when it came to games, most importantly rolling dice. With the rise of the Enlightenment many mathematical underpinnings of probability were explored, most notably by the mathematician Jacob Bernoulli.\n\n'''Gauss presented a real breakthrough, due to the discovery of the normal distribution.''' It allowed the feasible approach to link sample size of observations with an understanding of the likelihood how plausible these observations were. Again building on Sir Francis Bacon, the theory of probability reached its final breakthrough once it was applied in statistical hypothesis testing. It is important to notice that this would throw modern statistics into an understanding through the lens of so-called [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ frequentist statistics]. This line of thinking dominates up until today, and is widely built on repeated samples to understand the distribution of probabilities across a phenomenon. \n\n[[File:Bildschirmfoto 2020-04-15 um 17.18.08.png|thumb|Another simple example for calculating probability which you have probably also discussed in school is flipping a coin. Here there are only two options: head or tail.]]\n\n'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while [https://365datascience.com/bayesian-vs-frequentist-approach/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this theory explored deeply, and builds the foundation of branches in data science and machine learning. The two approaches are also often coined as objectivists for frequentist probability fellows, and subjectivists for folllowers of [https://www.youtube.com/watch?v=9TDjifpGj-k Bayes theorem]. \n\nAnother perspective on the two approaches can be built around the question whether we design studies - or whether we base our analysis on the data we just have. This debate is the basis for the deeply entrenched conflicts you have in statistics up until today, and was already the basis for the conflicts between Pearson and Fisher. From an epistemological perspective, this can be associated with the question of inductive or [[Glossary|deductive reasoning]], although not many statisticians might not be too keen to explore this relation deeply, since they are often stuck in either deductive or inductive thinking, but not both. \n\n'''While probability today can be seen as one of the core foundations of statistical testing, probability as such is increasingly criticised.''' It would exceed this chapter to discuss this in depth, but let me just highlight that without understanding probability, much of the scientific literature building on quantitative methods is hard to understand. What is important to notice, is that probability has trouble considering [https://sustainabilitymethods.org/index.php/Why_statistics_matters#Occam.27s_razor Occam's razor]. This is related to the fact that probability can deal well with the chance of an event to a occur, but it widely ignores the complexity that can influence such a likeliness. Modern statistics explore this thought further but let us just realise here: without learning probability we would have trouble reading the contemporary scientific literature.\n\n[[File:Bildschirmfoto 2020-04-17 um 15.40.02.png|500px|thumb|The GINI coefficient is a good example for a measure which compares the income distribution of different countries.]]\n\nThe probability can be best explained with the [https://www.stat.colostate.edu/~vollmer/stat307pdfs/LN4_2017.pdf normal distribution]. The normal distribution basically tells us through probability how a certain value will add to an array of values. Take the example of the height of people, or more specifically people who define themselves as males. Within a given population or country, these have an average height. This means in other words, that you have the highest chance to have this height when you are part of this population. You have a slightly lower chance to have a slightly smaller or larger height compared to the average height. And you have a very small chance to be much smaller or much taller compared to the average. In other words, your probability is small to be very tall or very small. Hence the distribution of height follows a normal distribution, and this normal distribution can be broken down into probabilities. In addition, such a distribution can have a variance, and these variances can be compared to other variances by using a so called [https://www.youtube.com/watch?v=FlIiYdHHpwU f test]. Take the example of height of people who define themselves as males. Now take the people who define themselves as females from the same population and compare just these two groups. You may realise that in most larger populations these two are comparable. This is quite relevant when you want to compare the income distribution between different countries. Many countries have [http://www.oecd.org/statistics/compare-your-income.htm different average incomes], but the distribution across the average as well as the very poor and the filthy rich can still be compared. In order to do this, the [http://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r f-test] is quite helpful. \n\n<syntaxhighlight lang=\"R\" line>\n\n#Let us perform a F test in R\n#therefore we load the dataset 'women'\n\ndatasets::women\nwomen_data<-women\n\n#we want to compare the variances of height and weight for American women aged 30-39\n#first we have to test for the normality of our samples\n\n#q-q plots\nqqnorm(women_data$height)\nqqline(women_data$height)\nqqnorm(women_data$weight)\nqqline(women_data$weight)\n#both are normally distributed\n\n\n#F-Test (Test for Equality of Variance)\n# H0 : Ratio of variance is equal to 1\n# H1 : Ratio of variance is NOT equal to 1\n\nvar.test(women_data$height,women_data$weight)\n#since p-value is low we reject H0\n\n</syntaxhighlight>\n\n='''External Links'''=\n== '''Websites'''==\n[https://seeing-theory.brown.edu/basic-probability/index.html Seeing Theory:] A great visual introduction to probability that you should definitely check out!\n\n=='''Articles'''==\n[https://www.britannica.com/science/probability/Risks-expectations-and-fair-contracts History of Probability]: An Overview\n\n[https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist vs. Bayesian Approaches in Statistics]: A comparison\n\n[https://365datascience.com/bayesian-vs-frequentist-approach/ Bayesian Statistics]: An example from the wizarding world\n\n[https://www.stat.colostate.edu/~vollmer/stat307pdfs/LN4_2017.pdf Probability and the Normal Distribution]: A detailed presentation\n\n[http://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r F test]: An example in R\n\n[http://www.oecd.org/statistics/compare-your-income.htm Compare your income]: A tool by the OECD\n\n=='''Videos'''==\n[https://www.youtube.com/watch?v=uzkc-qNVoOk Probability]: An Introduction\n\n[https://www.youtube.com/watch?v=9TDjifpGj-k Bayes Theorem]: An explanation\n\n[https://www.youtube.com/watch?v=FlIiYdHHpwU F test]: An example calculation\n\n----\n[[Category: Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "rc986eczv7g187y8u3ubwbmkn5kvypm"
                }
            },
            {
                "title": "Agency, Complexity and Emergence",
                "ns": "0",
                "id": "438",
                "revision": {
                    "id": "6792",
                    "parentid": "6336",
                    "timestamp": "2022-10-21T13:44:33Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "24108",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Agency, Complexity and Emergence (German)]]\n\n'''In short:''' This entry discusses three central elements of systems, and how we can investigate them methodologically.\n\nAgency, Complexity and Emergence are three concepts that can be understood as lenses through which we can see results that originate in theoretical considerations, as well as information we access through empirical research. Since such [[Glossary|concepts]] are binding the two realms of scientific inquiry - theory and practice - I call the three concepts of agency, complexity and emergence \u201eboundary objects\u201c. They are good examples of ideas that are most relevant within the philosophy of science. In addition, they strongly determine our fundamental and normative view of the world as well as the consequences of the knowledge we gain from the world. If I have for instance the world view that no free will exists, then this will also be a fundamental influence on how I interpret results from empirical inquiry of the actions of people. '''Many would agree that people have the potential to have free will, just as all milk can be turned into butter.''' There are other concepts besides agency, complexity and emergence that are worth pursuing form the perspective of philosophy of science, but for the sake of simplicity and priority, [[Big problems for later|we will only deal]] with these three boundary object here. Let us start with an attempt to show the diversity of definitions of these concepts.\n\n\n== Agency ==\nThe most simple definition of agency that is widely considered to be most relevant from a methodological standpoint is that agency is defined as ''the capacity of an individual to act intentionally with the assumption of a causal outcome based on this action''. However, within different discourses and disciplines there is a wide variety of approaches that consider broader or more narrow definitions of agency. There is a first disagreement whether an agent should have a mental state that is able to anticipate the outcome of the action, and such diverse realms as neuroscience and philosophy disagree on that point already. For instance, reflexes are instincts, and it is very difficult to track down whether such actions are intentional and contain assumptions about the outcome. Therefore, broader definitions of agency include the action-based adaptation of an agent to his, her or its environment, which includes also much of the animal kingdom. This broad definition of agency will be ignored here, although it should be stated that the actions of some higher animals indicate intention and mental states that may be able to anticipate. These are however difficult to be investigated from a methodological standpoint, although much can be expected from this direction in the future. \n[[File:Tiger.jpg.jpg|350px|thumb|right|'''Animals react to their environment, but do they also act intentionally?''' Source: pixabay]]\n\n'''What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency.''' Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy's law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of agency. There is thus a metaphysical problem when we try to differentiate the agent, their actions, and the consequences of their actions. One could claim that this can be solved by focussing on the consequences of the actions of agents alone. However, this consequentialist view is partly a theoretical consideration, as this view can create many interesting experiments, but does not really help us to solve the problem of unintentional acts per se. Still, consequentialism and a focus on mere actions is also relevant because it allows to step away from a single agent and focus instead on the interactions within the agency of several actors, which is a good start for the next concept.\n\n\n== Complexity ==\nMany phenomena in nature are simple, and follow simple rules. Many other phenomena are not simple, but instead can be defined as \"complex\". In order to negotiate between the two there is the fundamental law of Occam's razor, which defines that all things are as simple as possible, and as complex as necessary. Complex system theory strongly developed over the last decades, and created ripples into all empirical science and beyond. '''What is however problematic is that no unified and universally accepted definition of complexity exists, which is quite ironic.''' It seems that complexity in itself is complex. Here, I will therefore focus on some of the main arguments and characteristics that are relevant from a methodological standpoint. First of all, I will restrict all considerations to 'complex systems'. To this end, I simply define [[System Thinking & Causal Loop Diagrams|\u201esystems\u201c]] as any number of individuals or elements that interact. Complex Systems, then, are systems that are composed of many components which may interact with each other in various ways and which are therefore difficult to model. Specific properties include [[Glossary|non-linearity]], emergence, adaptation and [[Glossary|feedback loops]]. From a methodological standpoint we should be able to observe these interactions, while from a philosophical standpoint we should be able to reflect upon them. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n[[File:System.jpg.jpg|350px|thumb|left|'''Systems can become quite large, and their interaction may lead to complex dynamics.''' Source: pixabay]]\nSuch interactions can be simply adaptive, which is a key characteristic of many organisms and systems, but we may not be able to anticipate such adaptations. Through interactions, networks may form that can be observed, yet not be anticipated. Interactions can create feedback mechanisms or actions that reinforce [[Glossary|patterns]], creating what many call 'complex interactions'. I would argue that these interactions become often unpredictable because they exceed our original assumptions about the dynamics that are possible in the given system. The complexity of such dynamics is then rooted in our own imperfect assumptions about such system dynamics. \n\nAnother phenomenon in complex systems is the question of non-linearity. Non-linear behavior is - mechanically speaking - changes that occur strongly and suddenly at the same time, and not gradually and continuously. Non-linear dynamics in systems are often occurring because the dynamics we observe are mediated by another factor. This additional and unknown factor or variable is the original trigger that in interaction with other parts of the system leads to non-linear system dynamics. The third characteristic in complex systems is related to spontaneous order. Spontaneous order relates to \"the spontaneous emergence of order out of seeming chaos\" (wikipedia), i.e. the creation of structures and order where it could not be anticipated. An example is the synchronisation of clapping noises in a crowd. Such spontaneous orders can often not be anticipated, and provide thus a problem when considering agency of systems. A lack of possible anticipation of feedback loops and non-linear behavior can thus lead to a spontaneous order that highlights our own imperfections when understanding systems. Such system states can be called emergent.\n\n\n== Emergence ==\n[[File:The Beatles.jpg.jpg|350px|thumb|right|'''What defined the Beatles was certainly more than the sum of their band members.''' Source: pixabay]]\nIf two or more entities together have a characteristic or behaviour that could not be anticipated only based on their individual parts, then this non-anticipated property is called \"emergent\". '''To me, a good example of emergence are the Beatles.''' The Beatles were much more than just John, Paul, George and Ringo. Another prominent examples is water, which can extinguish fire. Oxygen and hydrogen do quite the opposite with fire, they propel it. These two examples show one of the key problems of emergence: it is quite simple to find examples for it, yet it is very hard to find underlying laws or even lines of thinking that can be used to approximate principles of emergence. Again, emergence is - just as complexity - a fluid if not untamable concept in itself. Empirical researchers are fascinated when they encounter it, yet it cannot be solved in itself, because that is what emergence is all about. While I will [[Big problems for later|leave this to philosophy]], one might argue that some things are not meant to be anticipated. Humans today are unlocking ever more strata of reality which has allowed us to capture many emergent phenomena, such as quantum physics, organic chemistry, behaviour biology and many higher system dynamics. All emergent phenomena build on interconnectedness of the parts that interact and thus jointly emerge. Another relevant criterion is defined as fallacy of division -  parts of a whole may show differing behaviour that the whole. This is again related to interconnectedness, but also to the variance that might exist within a population. Many phenomena of the natural world show patterns of emergence, such as the physics of sand. Chemical reactions, biological interactions and our economical system in itself are all emergent. Yet, if we cannot predict them, how can we empirically approach them?\n\n\n== Methodological implications ==\nConsidering these three concept, from a methodological standpoint we should consequently ask which entities lack agency, which systems are simple, and which interconnected phenomena defy any emergence. Falsification is one of the most important principles of science, and applying this approach to these three concepts can help us approximate the current methodological frontier in science. The word \"current\" is of central importance here, because we have to understand that all three concepts are at least partly normative, because they revolve around our current state of knowledge. Individuals that we deny agency now may be attributed agency by our grid of theory and methods at some point. Systems that seem complex today may be simple in the future. Emergent phenomena today may enable predictability of future interconnections. '''To this end, these three concepts are merely snapshots of our own ignorance'''. Right now we cannot know whether in the very long run all agency will be tracked, all systems will be simple, and all interconnections predictable, which right now only philosophy can examine. I think empirical research right now is incapable to answer how much people in the future may be able to understand, but it is not for us to decide how much will be explained in the future. \n\nWhat is clearer is that ''agency'' widely revolves around complex experimental designs that bridge different realms of science, such as neuroscience and psychology, psychology and microeconomics or behavioural science and zoology. The concept of agency links different fields of inquiry. Besides our diverse theories when it comes to agency, we only start to fully acknowledge the challenges related to its measurement as well as the opportunities to bridge different domains of sciences. From an empirical standpoint, agency has not been systematically accessed, which is why much emphasis is given to some few studies, and existing schools of thought often build on diverse premises. A good example to access agency is for instance in the combination of psychological or behaviour focused experiments with neurological examinations, although even such complicated experimental settings are not free of ambiguous results. Another methodological approach to agency could be in gamification, since such approaches would be able to test behavior of players within a complex environment, including reactions and motivations. The concept of altruism does however illustrate that, for the time being, the explanation of such behaviors may be unattainable. Just like agency, altruism can be divided into evolutionary or metaphysical explanations, roughly speaking. Time will tell if these schools of thought can be bridged. \n\n''Complex system theory'' is increasingly in the focus of many fields of research, which is no surprise. The dynamics of [[Glossary|complex systems]] are an Eldorado for many empirical researchers, and many disciplines are engaged in this arena. From a methodological standpoint, methods such as network analysis or structural equation models and also theoretical work such as the [https://en.wikipedia.org/wiki/Elinor_Ostrom Ostrom framework] are examples of the increasing recognition of complex systems. The Ostrom framework builds a bridge to this end, as it links a conceptual focus on resources with a methodological line of thinking to allow for a structured recognition of system dynamics. Several approaches try to implement the dynamics of complex systems within an analytical [[Glossary|framework]], yet these approaches often suffer from a gap between empirical data and sufficiently complex theory-driven models, since we only start to approach the gap between individual case studies and a broader view of empirical results. Here, the Ostrom framework is recognized as a landmark approach, although we must understand that even this widely popular framework was implemented in surprisingly few empirical studies (Partelow 2019).\n[[File:Ostrom framework.jpg|400px|thumb|left|'''The Ostrom framework allows us to reflect on socio-ecological systems.''' Source: [https://science.sciencemag.org/content/325/5939/419.abstract Ostrom 2009.]]]\n\nTo this end, it is important to realise that the problems that complex systems may encompass can be often diagnosed - which is called system knowledge - however the transformative knowledge necessary to solve these problems is ultimately what is complex. For instance do many people know that we already understood that climate change is real, and what we could do against it. What we could really do to solve the deeper problems - i.e. changing people's behaviour and consumption patterns - is however much more difficult to achieve, and this is what complex system thinking often about. '''Problem diagnosis is often simpler than creating solutions.''' Another challenge in complex systems are temporal dynamics. For instance, meteorological models are built on complex theories yet are also constantly refined based on a diversity of data. These models showcase that we are able to predict the weather for some days, and coarse patterns even for weeks, but there is still a clear limitation of our understanding of the weather when we go further into the future. A recent example of complex system dynamics is the Corona pandemic. While in the early stage of the pandemic our knowledge about the spread of the virus and the associated dynamics grew quickly, effective solutions are a long-term goal. There are clear relations how certain actions lead to certain developments in the pandemic, but the local and regional context can cascade into severe dynamics and differences. In addition, the development of potential solutions - such as a vaccine - is very complex and difficult to achieve. While thus many developments during this pandemic can be understood, it is certainly more complex to create a solution. \n\nTracking down ''emergence'' has become a key focus in many areas of science, but organic chemistry can serve as an example of how much has been done, yet how much is still on the horizon. Many negative effects of chemicals were not anticipated, with prominent examples being FCKW, pesticides and antibiotics. The effect of different medical drugs on people is yet another example, since interactions between different medications are hardly understood at all, as the field only slowly unravels the negative side-effects of interacting medications or treatments. We are far away from understanding the impact that the concepts ''agency'', ''complex systems'' and ''emergence'' have on our knowledge. Yet, we need to diversify our canon of methods in order to approach these concepts from an empirical standpoint. Otherwise we will not be able to unlock new strata of knowledge. This includes the combination of different methods, the utilization of specific methods in a different context, as well as the development of novel methods.\n\nWhat is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https://www.britannica.com/science/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.\n\n\n== The way forward ==\nIf we want to empirically investigate agency, we first and foremost investigate individuals, or actions of entities we consider as non-variable, or consequences of actions of individuals. All this has consequences for the methods we apply, and the questions whether we observe or test premises has in addition further methodologial ramifications. '''I can interview individuals, yet this will hardly allow me to prove agency.''' Because of this, much of our current knowledge of agency is either rooted in widely deductive experimental settings or the testing of very clear hypotheses, or questions of either logic or metaphysics, which are widely associated with philosophy. \n\nThe investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon is based on probability and linear relations. While linear relations allow for a better inference of causal explanations, the long existing yet until recently hardly explored [[Bayesian inference|Bayesian]] statistics are an example that we can inductively learn about systems at a growing pace without being dependent on linearity or normal-distributions. This Bayesian revolution is currently under way, but much of the disciplines relying on statistics did not catch up on this yet. Other methodological approaches will certainly be explored to gain insight into the nuts and bolts of complex systems, yet this is only slowly emerging. \n\nThe whole globe - although not a closed system \u2013 can be seen as a global system, and this is certainly worthwhile pursuing from a methodological standpoint. Still, global dynamics consist of such diverse data, that simply the translational act to bring different data together seems almost impossible right now. While emergence can lead to novel solutions, globalisation and technology have triggered uncountable events of emergence, such as global conflicts, climate change, increase in cancer rates and biodiversity loss. Humankind did certainly not plan these potential endpoints of ourselves, instead they emerged out of unpredictable combinations of our actions, and the data that can represent them. From a methodological standpoint, these events are just as unpredictable as is the effect which two molecules have onto each other and the environment. '''Emergence is a truly cross-scalar phenomenon.''' Consequently, many methodological accounts to countermeasure threats to human societies are correlative if they are empirical. We are far away from any deep understanding of emergence, and what makes phenomena emergent.\n[[File:Climate model.png|400px|thumb|right|'''Climate models are increasingly getting more accurate, but the complexity and emergence of the global climate system may never be fully understood.''' Source: [https://blogs.egu.eu/geolog/2018/09/19/how-to-forecast-the-future-with-climate-models/ European Geosciences Union]]]\n\nBased on the current methodological canon we can say: '''Much is not (yet?) observed, much is not known, and it will be crucial to understand what we will not be able to know, at least for the time being.''' These three different qualities of knowledge clearly highlight that research will have to collaborate - agency, many complex systems and several phenomena that are emergent cannot be sufficiently investigated by one discipline alone. While chemistry may find emergence in chemical reactions, and medicine may find interactions between different drugs, these results become normative as soon as they are applicable in the real world. To this claim, one could make the exception of ethics or broadly speaking philosophy here, which can meaningfully engage with all three domains of knowledge - unobserved, not known and never known - yet may not be able to contribute to all empirical problems. On the other end, everything but philosophy may only be able to go into the un-observed and unknown. We need to combine our methodological approaches to create the knowledge that is needed now. The need for collaboration is a question of responsibility, and only if all disciplines dissolve (or at least redefine their main goal to unite and not differentiate), agency may be fully explored and complex problems may be solved. Many of the solutions we have in our hands right now already would seem like wizardry to our ancestors. It is our responsibility to continue on this path, not as technocrats or positivists that have arrogant pride of their achievement, but as mere contributors to a wider debate that should ultimately encompass all society. \n\nTo say it with Derek Parfit:\n\n\"Some things (...) matter, and there are better and worse way to live. After many thousands of years of responding to reasons in ways that helped them to survive and reproduce, human beings can now respond to other reasons. We are a part of a Universe that is starting to understand itself. And we can partly understand, not only what is in fact true, but also what ought to be true, and what we might be able to make true. What now matters the most is that we avoid ending human history. If there are no rational beings elsewhere, it may depend on us and our successors whether it will all be worth it, because the existence of the Universe will have been on the whole good.\"\n\n== Further Information ==\n* A [https://www.youtube.com/watch?v=16W7c0mb-rE video by Kurzgesagt] that illustrates the concept of Emergence\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "04rzezupkv0a0y0cxkn22slgdua53e8"
                }
            },
            {
                "title": "Agency, Complexity and Emergence (German)",
                "ns": "0",
                "id": "497",
                "revision": {
                    "id": "6787",
                    "parentid": "6337",
                    "timestamp": "2022-10-10T11:57:01Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26803",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Agency, Complexity and Emergence]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag diskutiert drei zentrale Elemente von Systemen, und wie wir sie methodisch untersuchen k\u00f6nnen.\n\nAgency, Komplexit\u00e4t und Emergenz k\u00f6nnen als Linsen verstanden werden, durch die wir sowohl Ergebnisse sehen k\u00f6nnen, die aus theoretischen \u00dcberlegungen stammen, als auch Informationen, auf die wir durch empirische Forschung zugreifen. Da solche Konzepte die beiden Bereiche der wissenschaftlichen Untersuchung - Theorie und Praxis - miteinander verbinden, nenne ich die drei Konzepte Agency, Komplexit\u00e4t und Emergenz \"Grenzobjekte\". Sie sind gute Beispiele f\u00fcr Ideen, die in der Wissenschaftsphilosophie am relevantesten sind. Dar\u00fcber hinaus bestimmen sie in hohem Ma\u00dfe unser grundlegendes und normatives Weltbild sowie die Konsequenzen des Wissens, das wir aus der Welt gewinnen. Wenn ich z.B. die Weltsicht habe, dass es keinen freien Willen gibt, dann wird dies auch einen grundlegenden Einfluss darauf haben, wie ich Ergebnisse aus der empirischen Untersuchung der Handlungen von Menschen interpretiere. '''Ich pers\u00f6nlich glaube, dass die Menschen das Potential haben, einen freien Willen zu haben, so wie alle Milch in Butter verwandelt werden kann'''. Es gibt andere Konzepte als agency, complexity und emergence, die es aus der Perspektive der Wissenschaftstheorie zu verfolgen lohnt, aber der Einfachheit und Priorit\u00e4t halber werden wir uns hier nur mit diesen drei Grenzobjekten befassen. Beginnen wir mit dem Versuch, die Vielfalt der Definitionen dieser Begriffe aufzuzeigen.\n\n\n== Agency ==\nDie einfachste Definition von Agency, die weithin als die methodisch relevanteste angesehen wird, ist \"die F\u00e4higkeit eines Individuums, vors\u00e4tzlich zu handeln, unter der Annahme eines kausalen Ergebnisses, das auf dieser Handlung beruht\". Innerhalb der verschiedenen Diskurse und Disziplinen gibt es jedoch eine Vielzahl von Ans\u00e4tzen, die breitere oder engere Definitionen von Agency in Betracht ziehen. Es gibt eine erste Meinungsverschiedenheit dar\u00fcber, ob eine handlungsbeteiligte Personmeinen Geisteszustand haben sollte, der in der Lage ist, das Ergebnis der Handlung zu antizipieren, und in diesem Punkt sind sich so unterschiedliche Bereiche wie Neurowissenschaft und Philosophie bereits uneinig. Zum Beispiel sind Reflexe Instinkte, und es ist sehr schwierig, herauszufinden, ob solche Handlungen beabsichtigt sind und Annahmen \u00fcber das Ergebnis enthalten. Daher schlie\u00dfen breitere Definitionen von Agency die handlungsbasierte Anpassung eine*r Agent*in an seine*ihre Umgebung ein, was auch einen Gro\u00dfteil des Tierreichs einschlie\u00dft. Diese weit gefasste Definition des Handelns wird hier ignoriert, obwohl festgestellt werden sollte, dass die Handlungen einiger h\u00f6herer Tiere auf Absicht und mentale Zust\u00e4nde hindeuten, die in der Lage sein k\u00f6nnten, vorauszusehen. Diese sind jedoch vom methodischen Standpunkt aus schwer zu untersuchen, obwohl von dieser Richtung in Zukunft viel erwartet werden kann. \n[[File:Tiger.jpg.jpg|350px|thumb|right|'''Tiere reagieren auf ihre Umwelt, aber handeln sie mit einer Absicht?''' Quelle: pixabay]]\n'''Handlungen von Agent*innen m\u00fcssen vors\u00e4tzlich sein, d.h. eine blo\u00dfe Handlung, die als Serendipit\u00e4t angesehen werden kann, ist nicht Teil des Handelns.''' Ebenso sind nicht vorhergesehene Folgen von Handlungen, die auf Kausalketten beruhen, ein Problem hinsichtlich Agency. Agency steht vor einer Herausforderung, wenn es darum geht, entweder Serendipit\u00e4t oder Murphys Gesetz anzuerkennen. Solche gl\u00fccklichen oder ungl\u00fccklichen Handlungen wurden von dem*der Agent*in nicht vorhergesehen und sind daher nicht wirklich in der Definition von Agency enthalten. Es gibt also ein metaphysisches Problem, wenn wir versuchen, den*die Agent*in, seine*ihre Handlungen und die Folgen der Handlungen zu unterscheiden. Man k\u00f6nnte behaupten, dass dieses Problem gel\u00f6st werden kann, wenn man sich allein auf die Folgen der Handlungen der Agent*innen konzentriert. Diese folgerichtige Sichtweise ist jedoch zum Teil eine theoretische \u00dcberlegung, da diese Sichtweise zwar viele interessante Experimente hervorbringen kann, uns aber nicht wirklich hilft, das Problem der unbeabsichtigten Handlungen an sich zu l\u00f6sen. Dennoch ist die Konzentration auf blo\u00dfe Handlungen auch deshalb relevant, weil sie es erlaubt, sich von eine*r einzelne*n Akteur*in zu entfernen und sich stattdessen auf die Interaktionen innerhalb der Agency mehrerer Akteur*innen zu konzentrieren, was ein guter Anfang f\u00fcr das n\u00e4chste Konzept ist.\n\n\n== Komplexit\u00e4t ==\nViele Ph\u00e4nomene in der Natur sind einfach und folgen einfachen Regeln. Viele andere Ph\u00e4nomene sind nicht einfach, sondern k\u00f6nnen stattdessen als \"komplex\" definiert werden. Um zwischen den beiden zu verhandeln, gibt es das Grundgesetz des Ockhamschen Rasiermessers (Occam's Razor), das definiert, dass alle Dinge so einfach wie m\u00f6glich und so komplex wie n\u00f6tig sind. Die komplexe Systemtheorie hat sich in den letzten Jahrzehnten stark entwickelt und Wellen in die gesamte empirische Wissenschaft und dar\u00fcber hinaus geschlagen. '''Problematisch ist jedoch, dass es keine einheitliche und allgemein akzeptierte Definition von Komplexit\u00e4t gibt, was ziemlich ironisch ist.''' Es scheint, dass die Komplexit\u00e4t an sich schon komplex ist. Daher werde ich mich hier auf einige der wichtigsten Argumente und Merkmale konzentrieren, die aus methodologischer Sicht relevant sind. Zun\u00e4chst einmal werde ich alle \u00dcberlegungen auf komplexe Systeme beschr\u00e4nken. Zu diesem Zweck definiere ich [[System Thinking & Causal Loop Diagrams|\"Systeme\"]] einfach als eine beliebige Anzahl von Individuen oder Elementen, die interagieren. Vom methodischen Standpunkt aus sollten wir in der Lage sein, diese Interaktionen zu beobachten, w\u00e4hrend wir vom philosophischen Standpunkt aus in der Lage sein sollten, sie zu reflektieren. \n\n[[File:System.jpg.jpg|350px|thumb|left|'''Systeme k\u00f6nnen recht gro\u00df werden, und ihre Interaktionen weisen mitunter komplexe Dynamiken auf.''' Quelle: pixabay]]\nSolche Interaktionen k\u00f6nnen anpassungsf\u00e4hig sein, was eine Schl\u00fcsseleigenschaft vieler Organismen und Systeme ist, aber wir sind m\u00f6glicherweise nicht in der Lage, solche Anpassungen vorherzusehen. Durch Interaktionen k\u00f6nnen sich Netzwerke bilden, die zwar beobachtet, aber nicht antizipiert werden k\u00f6nnen. Interaktionen k\u00f6nnen R\u00fcckkopplungsmechanismen oder Aktionen hervorrufen, die Muster verst\u00e4rken und das erzeugen, was viele als komplexe Interaktionen bezeichnen. Ich w\u00fcrde argumentieren, dass diese Interaktionen oft unvorhersehbar werden, weil sie unsere urspr\u00fcnglichen Annahmen \u00fcber die Dynamik, die in dem gegebenen System m\u00f6glich ist, \u00fcbersteigen. Die Komplexit\u00e4t einer solchen Dynamik wurzelt dann in unseren eigenen unvollkommenen Annahmen \u00fcber eine solche Systemdynamik. Ein weiteres Ph\u00e4nomen in komplexen Systemen ist die Frage des nichtlinearen Verhaltens; mechanisch gesprochen: Ver\u00e4nderungen, die stark und pl\u00f6tzlich zur gleichen Zeit und nicht allm\u00e4hlich und kontinuierlich auftreten. Nichtlineare Dynamiken in Systemen treten h\u00e4ufig auf, weil die Dynamik, die wir beobachten, durch einen anderen Faktor vermittelt wird. Dieser zus\u00e4tzliche und unbekannte Faktor oder diese unbekannte Variable ist der urspr\u00fcngliche Ausl\u00f6ser, der im Zusammenspiel mit anderen Teilen des Systems zu einer nichtlinearen Systemdynamik f\u00fchrt. Das dritte Merkmal in komplexen Systemen h\u00e4ngt mit der spontanen Ordnung zusammen. Spontane Ordnung ist die pl\u00f6tzliche Entstehung von nicht-erwartbarer Struktur und Ordnung in vermeintlichem Chaos, zum Beispiel die Entstehung synchroner Klatschger\u00e4usche in einem Publikum. Solche spontanen Ordnungen k\u00f6nnen oft nicht vorhergesehen werden und stellen daher ein Problem dar, wenn man die Agilit\u00e4t von Systemen betrachtet. Ein Mangel an m\u00f6glicher Antizipation von R\u00fcckkopplungsschleifen und nichtlinearem Verhalten kann daher zu einer spontanen Ordnung f\u00fchren, die unsere eigenen Unvollkommenheiten im Systemverst\u00e4ndnis hervorhebt. Solche Systemzust\u00e4nde k\u00f6nnen als emergent bezeichnet werden.\n\n\n== Emergenz ==\n[[File:The Beatles.jpg.jpg|350px|thumb|right|'''Die Beatles waren mehr als die Summe ihrer Bandmitglieder.''' Quelle: pixabay]]\nWenn zwei oder mehr Entit\u00e4ten zusammen eine Eigenschaft oder ein Verhalten aufweisen, die bzw. das nur aufgrund ihrer Einzelteile nicht antizipiert werden konnte, dann wird diese nicht antizipierte Eigenschaft als \"emergent\" bezeichnet. '''Ein gutes Beispiel f\u00fcr Emergenz sind f\u00fcr mich die Beatles.''' Die Beatles waren viel mehr als nur John, Paul, George und Ringo. Ein weiteres prominentes Beispiel ist Wasser, das Feuer l\u00f6schen kann. Sauerstoff und Wasserstoff bewirken beim Feuer genau das Gegenteil, sie treiben es an. Diese beiden Beispiele zeigen eines der Schl\u00fcsselprobleme der Emergenz: Es ist recht einfach, Beispiele daf\u00fcr zu finden, aber es ist sehr schwer, zugrundeliegende Gesetze oder auch nur Denkweisen zu finden, die zur Ann\u00e4herung der Prinzipien der Emergenz verwendet werden k\u00f6nnen. Noch einmal: Emergenz ist - ebenso wie Komplexit\u00e4t - ein fl\u00fcssiges, wenn nicht unbez\u00e4hmbares Konzept an sich. Empirische Forscher sind fasziniert, wenn sie ihr begegnen, und doch kann sie in sich selbst nicht gel\u00f6st werden, denn das ist es, worum es bei der Emergenz geht. W\u00e4hrend ich dies [[Big problems for later|der Philosophie \u00fcberlasse]], k\u00f6nnte man argumentieren, dass manche Dinge nicht dazu bestimmt sind, vorhergesagt zu werden. Der Mensch erschlie\u00dft heute immer mehr Strata der Wirklichkeit, was es uns erm\u00f6glicht hat, viele emergente Ph\u00e4nomene zu erfassen, wie z.B. die Quantenphysik, die organische Chemie, die Verhaltensbiologie und viele h\u00f6here Systemdynamiken. Alle auftauchenden Ph\u00e4nomene bauen auf der Vernetzung der Teile auf, die miteinander interagieren und so gemeinsam auftauchen. Ein weiteres relevantes Kriterium wird als Trugschluss der Teilung definiert - Teile eines Ganzen k\u00f6nnen ein anderes Verhalten zeigen als das Ganze. Dies h\u00e4ngt wiederum mit der Verflochtenheit zusammen, aber auch mit der Varianz, die innerhalb einer Population bestehen kann. Viele Ph\u00e4nomene der nat\u00fcrlichen Welt zeigen Muster der Emergenz, wie z.B. die Physik des Sandes. Chemische Reaktionen, biologische Wechselwirkungen und unser Wirtschaftssystem an sich sind alle emergent. Doch wenn wir sie nicht vorhersagen k\u00f6nnen, wie k\u00f6nnen wir uns ihnen empirisch n\u00e4hern?\n\n\n== Methodologische Auswirkungen ==\nWenn wir diese drei Konzepte betrachten, sollten wir vom methodischen Standpunkt aus folglich fragen, welchen Einheiten die Agency fehlt, welche Systeme einfach sind und welche miteinander verbundenen Ph\u00e4nomene sich jeder Emergenz entziehen. Falsifizierung ist eines der wichtigsten Prinzipien der Wissenschaft, und die Anwendung dieses Ansatzes auf diese drei Konzepte kann uns helfen, uns der gegenw\u00e4rtigen methodologischen Grenze in der Wissenschaft anzun\u00e4hern. Das Wort \"aktuell\" ist hier von zentraler Bedeutung, denn wir m\u00fcssen verstehen, dass alle drei Konzepte zumindest teilweise normativ sind, weil sie sich um unseren aktuellen Wissensstand drehen. Individuen, denen wir jetzt die Agency absprechen, k\u00f6nnen durch unser Raster von Theorie und Methoden irgendwann Agency zugeschrieben werden. Systeme, die heute komplex erscheinen, k\u00f6nnen in Zukunft einfach sein. Heute auftauchende Ph\u00e4nomene k\u00f6nnen die Vorhersehbarkeit k\u00fcnftiger Zusammenh\u00e4nge erm\u00f6glichen. Zu diesem Zweck sind diese drei Konzepte lediglich Momentaufnahmen unserer eigenen Unwissenheit. Im Moment k\u00f6nnen wir nicht wissen, ob auf sehr lange Sicht alle Agencies verfolgt werden, alle Systeme einfach und alle Emergenzen vorhersehbar sein werden; das kann im Moment nur die Philosophie untersuchen. Ich denke, dass die empirische Forschung im Augenblick nicht in der Lage ist, zu beantworten, wie viel die Menschen in der Zukunft verstehen k\u00f6nnen, aber es liegt nicht an mir und uns, jetzt zu entscheiden, wie viel in der Zukunft erkl\u00e4rt werden wird. \n\nKlarer ist, dass sich Agency weitgehend um komplexe Versuchsaufbauten dreht, die verschiedene Wissenschaftsbereiche wie Neurowissenschaften und Psychologie, Psychologie und Mikro\u00f6konomie oder Verhaltenswissenschaften und Zoologie verbinden. Abgesehen von unseren unterschiedlichen Theorien zur Handlungsm\u00e4chtigkeit erkennen wir erst jetzt die Herausforderungen im Zusammenhang mit ihrer Messung sowie die M\u00f6glichkeiten zur \u00dcberbr\u00fcckung verschiedener Wissenschaftsbereiche vollst\u00e4ndig an. Vom empirischen Standpunkt aus betrachtet, wurde bisher kein systematischer Zugang zu Wirkungsmechanismen gefunden, weshalb einigen wenigen Studien gro\u00dfe Bedeutung beigemessen wird und bestehende Denkschulen oft auf unterschiedlichen Pr\u00e4missen aufbauen. Ein gutes Beispiel f\u00fcr den Zugang zu Agency ist zum Beispiel die Kombination psychologischer oder verhaltensorientierter Experimente mit neurologischen Untersuchungen, obwohl selbst solche komplizierten Versuchsanordnungen nicht frei von mehrdeutigen Ergebnissen sind. Ein anderer methodischer Ansatz f\u00fcr den Zugang zu Agency k\u00f6nnte in der Gamifizierung liegen, da solche Ans\u00e4tze in der Lage w\u00e4ren, das Verhalten von Spielern innerhalb einer komplexen Umgebung zu testen, einschlie\u00dflich Reaktionen und Motivationen. Das Konzept des Altruismus macht jedoch deutlich, dass die Erkl\u00e4rung solcher Verhaltensweisen vorerst unerreichbar sein k\u00f6nnte. Genau wie die Agency kann Altruismus grob in evolution\u00e4re oder metaphysische Erkl\u00e4rungen unterteilt werden. Die Zeit wird zeigen, ob diese Denkschulen \u00fcberbr\u00fcckt werden k\u00f6nnen. \n\n''Komplexe Systemtheorie'' steht zunehmend im Fokus vieler Forschungsbereiche, was nicht \u00fcberrascht. Die Dynamik komplexer Systeme ist ein Eldorado f\u00fcr viele empirische Forschenden und viele Disziplinen sind auf diesem Gebiet t\u00e4tig. Aus methodischer Sicht sind Methoden wie die Netzwerkanalyse oder Strukturgleichungsmodelle, aber auch theoretische Arbeiten wie das [https://en.wikipedia.org/wiki/Elinor_Ostrom Ostrom-Framework] Beispiele f\u00fcr die zunehmende Anerkennung komplexer Systeme. Das Ostrom-Framework schl\u00e4gt hier eine Br\u00fccke, da es einen konzeptionellen Fokus auf Ressourcen mit einer methodologischen Denkweise verbindet, um eine strukturierte Erkennung der Systemdynamik zu erm\u00f6glichen. Mehrere Ans\u00e4tze versuchen, die Dynamik komplexer Systeme innerhalb eines analytischen Rahmens zu implementieren, doch leiden diese Ans\u00e4tze oft unter einer L\u00fccke zwischen empirischen Daten und hinreichend komplexen theoriegeleiteten Modellen, da wir erst beginnen, uns der L\u00fccke zwischen einzelnen Fallstudien und einer breiteren Sichtweise der empirischen Ergebnisse zu n\u00e4hern. Hier wird das Ostrom-Framework als wegweisender Ansatz anerkannt, obwohl wir verstehen m\u00fcssen, dass selbst dieses weit verbreitete Framework in \u00fcberraschend wenigen empirischen Studien umgesetzt wurde (Partelow 2019).\n[[File:Ostrom framework.jpg|400px|thumb|left|'''Das Ostrom-Framework erm\u00f6glicht eine Reflexion \u00fcber sozio-\u00f6kologische Systeme.''' Quelle: [https://science.sciencemag.org/content/325/5939/419.abstract Ostrom 2009.]]]\n\nEs ist wichtig, sich bewusst zu machen, dass die Probleme, die komplexe Systeme umfassen k\u00f6nnen, oft diagnostiziert werden k\u00f6nnen - was als Systemwissen bezeichnet wird - aber das transformative Wissen, das zur L\u00f6sung dieser Probleme notwendig ist, ist letztlich das, was komplex ist. Zum Beispiel wissen viele Menschen, dass wir bereits verstanden haben, dass der Klimawandel real ist und was wir dagegen tun k\u00f6nnen. Was wir wirklich tun k\u00f6nnten, um die tieferen Probleme zu l\u00f6sen - d.h. das Verhalten und die Konsummuster der Menschen zu \u00e4ndern - ist jedoch viel schwieriger zu erreichen, und genau dar\u00fcber denken komplexe Systeme oft nach. '''Problemdiagnose ist oft einfacher, als L\u00f6sungen zu schaffen.''' Eine weitere Herausforderung in komplexen Systemen ist die zeitliche Dynamik. Beispielsweise bauen meteorologische Modelle auf komplexen Theorien auf, werden aber auch st\u00e4ndig auf der Grundlage einer Vielzahl von Daten verfeinert. Diese Modelle zeigen, dass wir in der Lage sind, das Wetter f\u00fcr einige Tage und grobe Muster sogar f\u00fcr Wochen vorherzusagen, aber es gibt immer noch eine klare Einschr\u00e4nkung unseres Verst\u00e4ndnisses des Wetters, wenn wir weiter in die Zukunft gehen. Ein aktuelles Beispiel f\u00fcr komplexe Systemdynamik ist die Corona-Pandemie. W\u00e4hrend in der Fr\u00fchphase der Pandemie unser Wissen \u00fcber die Ausbreitung des Virus und die damit verbundene Dynamik schnell wuchs, sind wirksame L\u00f6sungen ein langfristiges Ziel. Es besteht ein klarer Zusammenhang, wie bestimmte Aktionen zu bestimmten Entwicklungen der Pandemie f\u00fchren, aber der lokale und regionale Kontext kann sich zu schwerwiegenden Dynamiken und Unterschieden ausweiten. Dar\u00fcber hinaus ist die Entwicklung potenzieller L\u00f6sungen - wie z.B. eines Impfstoffs - sehr komplex und schwer zu erreichen. W\u00e4hrend also viele Entwicklungen w\u00e4hrend dieser Pandemie verstanden werden k\u00f6nnen, ist es sicherlich komplexer, eine L\u00f6sung zu finden. \n\nDas Aufsp\u00fcren von Emergenz ist zu einem Schwerpunkt in vielen Bereichen der Wissenschaft geworden, und die organische Chemie kann als Beispiel daf\u00fcr dienen, wie viel bereits getan wurde, aber wie viel noch in der Zukunft liegt. Viele negative Auswirkungen von Chemikalien wurden nicht vorhergesehen, prominente Beispiele sind FCKW, Pestizide und Antibiotika. Die Wirkung verschiedener Medikamente auf den Menschen ist ein weiteres Beispiel, da die Wechselwirkungen zwischen verschiedenen Medikamenten kaum verstanden werden, da das Feld die negativen Nebenwirkungen von interagierenden Medikamenten oder Behandlungen nur langsam aufdeckt. Wir sind weit davon entfernt, die Auswirkungen zu verstehen, die die Begriffe ''Agency'', ''komplexe Systeme'' und ''Emergenz'' auf unser Wissen haben. Dennoch m\u00fcssen wir unseren Methodenkanon diversifizieren, um uns diesen Konzepten von einem empirischen Standpunkt aus n\u00e4hern zu k\u00f6nnen. Andernfalls werden wir nicht in der Lage sein, neue Wissens-Strata zu erschlie\u00dfen. Dazu geh\u00f6rt die Kombination verschiedener Methoden, die Anwendung spezifischer Methoden in einem anderen Kontext sowie die Entwicklung neuer Methoden.\n\nKlar ist jedoch, dass die drei Konzepte Konsequenzen f\u00fcr unsere Pr\u00e4missen empirischen Wissens haben. Was ist, wenn letztlich nichts verallgemeinerbar ist? Was, wenn alle g\u00fcltigen Argumente nur f\u00fcr eine bestimmte Zeit g\u00fcltig sind? Und was, wenn einige Strata einer wirklich verl\u00e4sslichen Messung f\u00fcr immer entgehen? Wir [[Big problems for later|k\u00f6nnen diese Probleme hier nicht beantworten]], dennoch ist es wichtig zu unterscheiden, was wir wissen, was wir vielleicht wissen k\u00f6nnen und was wir wahrscheinlich nie wissen werden. Das [https://www.britannica.com/science/uncertainty-principle Unsch\u00e4rferelationsprinzip von Heisenberg] in der Quantenmechanik, das sich auf die Position und den Impuls von Teilchen bezieht, veranschaulicht, dass manche Dinge nicht angen\u00e4hert und beobachtet werden oder bekannt sein k\u00f6nnen. Gleiche Behauptungen k\u00f6nnen auch f\u00fcr gr\u00f6\u00dfere Ph\u00e4nomene, wie z.B. die pers\u00f6nliche Identit\u00e4t, aufgestellt werden. Daher k\u00f6nnen Agency, Komplexit\u00e4t und Emergenz zwar Grenzobjekte f\u00fcr Methoden sein, aber sie zeigen auch unsere (derzeitigen) Grenzen auf. \n\n\n== Wie geht es weiter? ==\nWenn wir das Handeln empirisch untersuchen wollen, dann untersuchen wir in erster Linie Individuen oder Handlungen von Entit\u00e4ten, die wir als nicht variabel betrachten, oder Folgen von Handlungen von Individuen. All dies hat Konsequenzen f\u00fcr die Methoden, die wir anwenden, und die Frage, ob wir Pr\u00e4missen beobachten oder testen, hat dar\u00fcber hinaus weitere methodische Verzweigungen. '''Ich kann Einzelpersonen befragen, aber das wird mir kaum erlauben, Agency zu beweisen.''' Aus diesem Grund wurzelt ein Gro\u00dfteil unseres derzeitigen Wissens \u00fcber Agency entweder in weitgehend deduktiven Versuchsanordnungen, in der Pr\u00fcfung sehr klarer Hypothesen oder in Fragen der Logik oder Metaphysik, die weitgehend mit der Philosophie verbunden sind. \n\nDie Erforschung komplexer Systeme hat sich in den letzten Jahrzehnten sowohl aus empirischer als auch aus konzeptueller Perspektive gut entwickelt. Viele Methoden tauchten auf oder wurden sp\u00e4ter angepasst, um Fragen zu beantworten und Beziehungen zu erforschen, und dieser Aufschwung hin zu einem tieferen Verst\u00e4ndnis von Systemen ist zumindest ein wichtiger methodischer Fortschritt bez\u00fcglich Agency. Es sind viele neue Daten verf\u00fcgbar, die oft induktiv erforscht werden. Die Gr\u00f6\u00dfenordnung komplexer Systeme macht eine Intervention mit dem Schwerpunkt auf [[Causality and correlation|Kausalit\u00e4t]] zu einer Herausforderung, daher sind viele untersuchte Beziehungen rein korrelativ. Nehmen wir zum Beispiel Social Media oder \u00f6konomische Str\u00f6me, die korrelativ untersucht werden k\u00f6nnen, doch Kausalit\u00e4t ist eine ganz andere Sache. Dies stellt eine methodologische Herausforderung dar, da viele unserer Fragen zu menschlichen Systemen normativer Natur sind, weshalb viele Forschenden in ihren Untersuchungen von Kausalit\u00e4t ausgehen oder zumindest Beziehungen so diskutieren, als seien diese kausal. Ein weiteres methodologisches Problem im Zusammenhang mit der Kausalit\u00e4t sind nichtlineare Beziehungen, da ein Gro\u00dfteil des statistischen Kanons auf Wahrscheinlichkeit und linearen Beziehungen beruht. W\u00e4hrend lineare Beziehungen eine bessere Ableitung kausaler Erkl\u00e4rungen erm\u00f6glichen, ist die seit langem existierenden, aber bis vor kurzem kaum erforschte [[Bayesian Inference|Bayes'sche]] Statistik ein Beispiel daf\u00fcr, dass wir mit wachsender Geschwindigkeit induktiv etwas \u00fcber Systeme lernen k\u00f6nnen, ohne von Linearit\u00e4t oder Normalverteilungen abh\u00e4ngig zu sein. Diese Bayes'sche Revolution ist derzeit im Gange, aber viele der Disziplinen, die sich auf Statistiken st\u00fctzen, haben dies noch nicht aufgeholt. Sicherlich werden noch andere methodische Ans\u00e4tze erforscht werden, um einen Einblick in das A und O komplexer Systeme zu gewinnen, doch dies zeichnet sich erst langsam ab. \n\nDer ganze Globus - auch wenn er kein geschlossenes System ist - kann als globales System betrachtet werden, und dies ist aus methodischer Sicht sicherlich lohnenswert. Dennoch bestehen globale Dynamiken aus so unterschiedlichen Daten, dass allein der \u00dcbersetzungsakt, verschiedene Daten zusammenzuf\u00fchren, im Moment fast unm\u00f6glich erscheint. W\u00e4hrend das Aufkommen zu neuen L\u00f6sungen f\u00fchren kann, haben Globalisierung und Technologie unz\u00e4hlige Ereignisse von Emergenz ausgel\u00f6st, wie z.B. globale Konflikte, Klimawandel, Anstieg der Krebsraten und Verlust der biologischen Vielfalt. Die Menschheit hat diese potenziellen Endpunkte sicherlich nicht selbst geplant, sondern sie entstanden aus unvorhersehbaren Kombinationen unserer Handlungen und der Daten, die sie repr\u00e4sentieren k\u00f6nnen. Vom methodischen Standpunkt aus gesehen sind diese Ereignisse ebenso unvorhersehbar wie die Auswirkungen, die zwei Molek\u00fcle aufeinander und auf die Umwelt haben. '''Emergenz ist ein wirklich skalen\u00fcbergreifendes Ph\u00e4nomen.''' Folglich sind viele methodologische Darstellungen zur Bek\u00e4mpfung von Bedrohungen f\u00fcr menschliche Gesellschaften korrelativ, wenn sie empirisch sind. Wir sind weit entfernt von einem tiefen Verst\u00e4ndnis der Entstehung von Ph\u00e4nomenen und dessen, was sie entstehen l\u00e4sst.\n[[File:Climate model.png|400px|thumb|right|'''Klimamodelle werden zunehmend genauer, aber die Komplexit\u00e4t und Emergenz des globalen Klimasystems k\u00f6nnte nie ganz verstanden werden. \u00b4''' Quelle: [https://blogs.egu.eu/geolog/2018/09/19/how-to-forecast-the-future-with-climate-models/ European Geosciences Union]]]\n\nAuf der Grundlage des aktuellen Methodenkanons k\u00f6nnen wir sagen: '''Vieles wird (noch?) nicht beobachtet, vieles ist nicht bekannt, und es wird entscheidend sein, zu verstehen, was wir zumindest vorl\u00e4ufig nicht wissen k\u00f6nnen.''' Diese drei unterschiedlichen Qualit\u00e4ten des Wissens machen deutlich, dass die Forschung zusammenarbeiten muss - Agency, viele komplexe Systeme und emergente Ph\u00e4nomene k\u00f6nnen von einer Disziplin allein nicht ausreichend untersucht werden. Auch wenn die Chemie Emergenz bei chemischen Reaktionen und die Medizin Wechselwirkungen zwischen verschiedenen Medikamenten entdecken kann, werden diese Ergebnisse normativ, sobald sie in der realen Welt anwendbar sind. Zu dieser Behauptung k\u00f6nnte man hier eine Ausnahme von der Ethik oder allgemein gesprochen von der Philosophie machen, die sich sinnvoll mit allen drei Wissensbereichen - unbeobachtet, nicht bekannt und nie bekannt - befassen kann, aber m\u00f6glicherweise nicht zu allen empirischen Problemen beitragen kann. Auf der anderen Seite kann alles au\u00dfer der Philosophie m\u00f6glicherweise nur ins Unbeobachtete und Unbekannte gehen. Wir m\u00fcssen unsere methodischen Ans\u00e4tze kombinieren, um das Wissen zu schaffen, das jetzt ben\u00f6tigt wird. Die Notwendigkeit der Zusammenarbeit ist eine Frage der Verantwortung, und nur wenn sich alle Disziplinen aufl\u00f6sen (oder zumindest ihr Hauptziel neu definieren, um zu vereinen und nicht zu differenzieren), kann Agency vollst\u00e4ndig erforscht und k\u00f6nnen komplexe Probleme gel\u00f6st werden. Viele der L\u00f6sungen, die wir jetzt schon in unseren H\u00e4nden halten, k\u00e4men unseren Vorfahren wie Zauberei vor. Es liegt in unserer Verantwortung, diesen Weg weiter zu beschreiten, nicht als Technokrat*innen oder Positivist*innen, die arroganten Stolz auf ihre Errungenschaften haben, sondern als blo\u00dfe Mitwirkende an einer breiteren Debatte, die letztlich die ganze Gesellschaft umfassen sollte. \n\nUm es mit Derek Parfit zu sagen:\n\n\"Einige Dinge (...) sind wichtig, und es gibt einen besseren und einen schlechteren Weg zu leben. Nachdem die Menschen viele tausend Jahre lang auf Ursachen in einer Weise reagiert haben, die ihnen half, zu \u00fcberleben und sich fortzupflanzen, k\u00f6nnen sie nun auf andere Ursachen reagieren. Wir sind Teil eines Universums, das beginnt, sich selbst zu verstehen. Und wir k\u00f6nnen teilweise verstehen, nicht nur, was tats\u00e4chlich wahr ist, sondern auch, was wahr sein sollte und was wir in der Lage sein k\u00f6nnten, wahr zu machen. Was jetzt am wichtigsten ist, ist, dass wir es vermeiden, die Menschheitsgeschichte zu beenden. Wenn es anderswo keine vern\u00fcnftigen Wesen gibt, kann es von uns und unseren Nachfolgern abh\u00e4ngen, ob sich das alles lohnt, denn die Existenz des Universums wird im Gro\u00dfen und Ganzen gut gewesen sein\".\n\n----\n[[Category: Normativity of Methods]]"
                    },
                    "sha1": "shibx0fhdn1okw664gauakjiv5njwto"
                }
            },
            {
                "title": "All Entries",
                "ns": "0",
                "id": "237",
                "revision": {
                    "id": "3655",
                    "parentid": "2789",
                    "timestamp": "2021-01-07T08:04:13Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "533",
                        "#text": "This page lists all entries on this Wiki as an overview.\n\n{| class=\"wikitable\" | + style=\"width: 100%; background-color: white; text-align:center\"\n|-\n! [[Courses]] !! [[Methods]] !! [[Skills & Tools]] !! [[Normativity of Methods]]\n|-\n| <categorytree mode=\"pages\" hideroot=\"on\">Courses</categorytree> || <categorytree mode=\"pages\" hideroot=\"on\">Methods</categorytree>  || <categorytree mode=\"pages\" hideroot=\"on\">Skills_and_Tools</categorytree>  || <categorytree mode=\"pages\" hideroot=\"on\">Normativity_of_Methods</categorytree> \n|-\n|}"
                    },
                    "sha1": "4dir9wbj3t0ihewt0v8lr569p0qk1ni"
                }
            },
            {
                "title": "An initial path towards statistical analysis",
                "ns": "0",
                "id": "567",
                "revision": {
                    "id": "6270",
                    "parentid": "6269",
                    "timestamp": "2021-08-17T08:07:32Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26324",
                        "#text": "Learning statistics takes time, as it is mostly experience that allows us to be able to approach the statistical analysis of any given dataset. '''While we cannot take off of you the burden to gather experience yourself, we developed this interactive page for you to find the best statistical method to analyze your given dataset.''' This can be a start for you to dive deeper into statistical analysis, and helps you better [[Designing studies|design studies]].<br/>\n\n<u>This page revolves around statistical analyses of data that has at least two variables.</u> If you only have one variable, e.g. height data of a dozen trees, or your ratings for five types of cake, you might be interested in simpler forms of data analysis and visualisation. Have a look at [[Descriptive statistics]] and [[Introduction to statistical figures]] to see which approaches might work for your data.\n\n<u>If you have more than one variable, you have come to the perfect place!</u> <br>\nGo through the images step by step, click on the answers that apply to your data, and let the page guide you. <br>\nIf you need help with data visualisation for any of these approaches, please refer to the entry on [[Introduction to statistical figures]].<br>If you are on mobile and/or just want a list of all entries, please refer to the [[Statistics]] overview page.\n----\n'''Start here with your data! This is your first question.'''\n\n<imagemap>Image:Statistics Flowchart - First Step.png|center|650px\npoly 289 5 151 140 289 270 423 137 [[An_initial_path_towards_statistical_analysis|This is where you start!]]\npoly 136 150 0 288 137 418 271 284 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|Multivariate Statistics]]\npoly 441 151 302 289 437 416 571 287 [[An_initial_path_towards_statistical_analysis#Univariate_statistics|Univariate Statistics]]\n</imagemap>\n\n'''How do I know?''' <br/>\n* What does the data show? Does the data logically suggest dependencies - a causal relation - between the variables? Have a look at the entry on [[Causality]] to learn more about causal relations and dependencies.\n\n\n= Univariate statistics =\n'''You are dealing with Univariate Statistics.''' Univariate statistics focuses on the analysis of one dependent variable and can contain multiple independent variables. But what kind of variables do you have?\n<imagemap>Image:Statistics Flowchart - Univariate Statistics.png|650px|center|\npoly 386 5 203 186 385 359 563 182 [[Data formats]]\npoly 180 200 3 380 181 555 359 378 [[An_initial_path_towards_statistical_analysis#At_least_one_categorical_variable|At least one categorical variable]]\npoly 584 202 407 378 584 556 762 379 [[An_initial_path_towards_statistical_analysis#Only_continuous_variables|Only continuous variables]]\n</imagemap>\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric (including continuous) variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is ''categorical''.\n\n\n== At least one categorical variable ==\n'''Your dataset does not only contain continuous data.''' Does it only consist of categorical data, or of categorical and continuous data?\n<imagemap>Image:Statistics Flowchart - Data Formats.png|650px|center|\npoly 288 2 151 139 289 271 424 138 [[Data formats]]\npoly 137 148 0 285 138 417 273 284  [[An_initial_path_towards_statistical_analysis#Only_categorical_data:_Chi_Square_Test|Only categorical data: Chi Square Test]]\npoly 436 151 299 288 437 420 572 287 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data|Categorical and continuous data]]\n</imagemap>\n\n'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\n\n=== Only categorical data: Chi Square Test ===\n'''You should do a Chi Square Test'''.<br/>\nA Chi Square test can be used to test if one variable influenced the other one, or if they occur independently from each other. The key R command here is: <code>chisq.test()</code>. Check the entry on [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi Square Tests]] to learn more.\n\n\n=== Categorical and continuous data ===\n'''Your dataset consists of continuous and categorical data.''' How many levels does your categorical variable have?\n<imagemap>Image:Statistics flowchart - Categorical factor levels.png|650px|center|\npoly 321 3 175 149 325 304 473 152 473 15 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data]]\npoly 149 172 3 318 153 473 301 321 301 321  [[An_initial_path_towards_statistical_analysis#One_or_two_factor_levels: t-test|One or two factor levels: t-test]]\npoly 489 172 343 318 493 473 641 321 641 321 [[An_initial_path_towards_statistical_analysis#More_than_two_factor_levels: ANOVA|More than two factor levels: ANOVA]]\n</imagemap>\n\n'''How do I know?'''\n* A 'factor level' is a category in a categorical variable. For example, when your variable is 'car brands', and you have 'AUDI' and 'TESLA', you have two unique factor levels. \n* Investigate your data using 'levels(categoricaldata)' and count the number of levels it returns. How many different categories does your categorical variable have? If your data is not in the 'factor' format, you can either convert it or use 'unique(yourCategoricalData)' to get a similar result.\n\n\n==== One or two factor levels: t-test ====\n'''With one or two factor levels, you should do a t-test.'''<br/> A one-sample t-test allows for a comparison of a dataset with a specified value. However, if you have two datasets, you should do a two-sample t-test, which allows for a comparison of two different datasets or samples and tells you if the means of the two datasets differ significantly. The key R command for both types is <code>t.test()</code>. Check the entry on the [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]] to learn more.\n\n'''Depending on the variances of your variables, the type of t-test differs.'''\n\n<imagemap>Image:Statistics Flowchart - Equal variances.png|850px|center|\npoly 146 5 0 150 145 290 289 148 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 557 2 408 147 556 286 700 144 [[Simple_Statistical_Tests#f-test|f-Test]]\npoly 392 165 243 310 391 450 535 308 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\npoly 716 160 567 305 715 444 859 302 [[Simple_Statistical_Tests#Most_relevant_simple_tests|t-Test]]\n</imagemap>\n\n'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()</code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()</code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE</code>.\n\n\n==== More than two factor levels: ANOVA ====\n'''Your categorical variable has more than two factor levels: you are heading towards an ANOVA.'''<br/>\n\n'''However, you need to answer one more question''': what about the distribution of your dependent variable?\n<imagemap>Image:Statistics Flowchart - 2+ Factor Levels - normal distribution.png|650px|center|\npoly 291 5 150 140 291 270 423 136 [[Data distribution]]\npoly 141 152 0 287 141 417 273 283 261 270  [[ANOVA]]\npoly 442 152 301 287 442 417 574 283 562 270 [[ANOVA]]\n</imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\nNow, let us have another look at your variables. '''Do you have continuous and categorical independent variables?'''\n\n'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\nIf your answer is NO, you should stick to the ANOVA - more specifically, to the kind of ANOVA that you saw above (based on regression analysis, or based on a generalised linear model). An ANOVA compares the means of more than two groups by extending the restriction of the t-test. An ANOVA is typically visualised using [[Introduction_to_statistical_figures#Boxplot|Boxplots]].</br> The key R command is <code>aov()</code>. Check the entry on the [[ANOVA]] to learn more.\n\nIf your answer is YES, you are heading way below. Click [[An_initial_path_towards_statistical_analysis#Is_there_a_categorical_predictor?|HERE]].\n\n\n== Only continuous variables ==\nSo your data is only continuous.<br/>\nNow, you need to check if there dependencies between the variables.\n<imagemap>Image:Statistics Flowchart - Continuous - Dependencies.png|650px|center|\npoly 383 5 203 181 380 359 559 182 [[Causality]]\npoly 182 205 2 381 179 559 358 382 [[An_initial_path_towards_statistical_analysis#No_dependencies:_Correlations|No dependencies]]\npoly 585 205 405 381 582 559 761 382 [[An_initial_path_towards_statistical_analysis#Clear_dependencies|Clear dependencies]]\n</imagemap>\n\n'''How do I know?'''  \n* Consider the data from a theoretical perspective. Is there a clear direction of the dependency? Does one variable cause the other? Check out the entry on [[Causality]].\n\n\n=== No dependencies: Correlations ===\n'''If there are no dependencies between your variables, you should do a Correlation.'''<br/>\nA correlation test inspects if two variables are related to each other. The direction of the connection (if or which variable influences another) is not set. Correlations are typically visualised using [[Introduction_to_statistical_figures#Scatter_Plot|Scatter Plots]] or [[Introduction_to_statistical_figures#Line_chart|Line Charts]]. Key R commands are <code>plot()</code> to visualise your data, and <code>cor.test()</code> to check for correlations. Check the entry on [[Correlations]] to learn more.\n\n'''The type of correlation that you need to do depends on your data distribution.'''\n\n<imagemap>Image:Statistics Flowchart - Normal Distribution.png|650px|center|\npoly 288 3 154 137 289 269 421 136 [[Data distribution]]\npoly 135 151 1 285 136 417 268 284 268 284  [[Correlations|Pearson]]\npoly 438 152 304 286 439 418 571 285 [[Correlations|Spearman]]\n</imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\n\n=== Clear dependencies ===\n'''Your dependent variable is explained by one at least one independent variable.''' Is the dependent variable normally distributed?\n<imagemap>Image:Statistics Flowchart - Dependent - Normal Distribution.png|650px|center|\npoly 289 2 152 139 290 267 423 13 [[Data distribution]]\npoly 137 151 0 288 138 416 271 281 [[An_initial_path_towards_statistical_analysis#Normally_distributed_dependent_variable:_Linear_Regression|Linear Regression]]\npoly 441 149 304 286 442 414 575 279 [[An_initial_path_towards_statistical_analysis#Not_normally_distributed_dependent_variable|Non-linear distribution of dependent variable]]\n</imagemap>\n\n'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\n\n==== Normally distributed dependent variable: Linear Regression ====\n'''If your dependent variable(s) is/are normally distributed, you should do a Linear Regression.'''<br/>\nA linear regression is a linear approach to modelling the relationship between one (simple regression) or more (multiple regression) independent and a dependent variable. It is basically a correlation with causal connections between the correlated variables. Check the entry on [[Regression Analysis]] to learn more.\n\n'''There may be one exception to a plain linear regression:''' if you have several predictors (= independent variables), there is one more decision to make:\n\n\n===== Is there a categorical predictor? =====\n'''You have several predictors (= independent variables) in your dataset.''' But is (at least) one of them categorical?\n<imagemap>Image:Statistics Flowchart - Categorical predictor.png|650px|center|\npoly 387 1 208 184 385 359 563 183 [[Data formats]]\npoly 180 197 1 380 178 555 356 379 [[An_initial_path_towards_statistical_analysis#ANCOVA|ANCOVA]]\npoly 584 196 405 379 582 554 760 378 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Multiple Regression]]\n</imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. Pay attention to the data format of your independent variable(s). ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\n\n===== ANCOVA =====\n'''If you have at least one categorical predictor, you should do an ANCOVA'''. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more.\n\n\n==== Not normally distributed dependent variable ====\n'''The dependent variable(s) is/are not normally distributed.''' Which kind of distribution does it show, then? For both Binomial and Poisson distributions, your next step is the Generalised Linear Model. However, it is important that you select the proper distribution type in the GLM.\n<imagemap>Image:Statistics Flowchart - Dependent - Distribution type.png|650px|center|\npoly 290 4 154 140 288 269 423 138 [[Data distribution]]\npoly 138 151 2 287 136 416 270 284 271 285 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\npoly 440 152 304 288 438 417 572 285 573 286 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]\n</imagemap>\n\n'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions.\n\n<imagemap>Image:Statistics Flowchart - GLM random variables.png|650px|center|\npoly 289 4 153 141 289 270 422 137 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models]]\npoly 139 151 3 288 139 417 272 284 [[Mixed Effect Models]]\npoly 439 149 303 286 439 415 572 282 [[Generalized Linear Models]]\n</imagemap>\n\n'''How do I know?'''\n* Random variables introduce extra variability to the model. For example, we want to explain the grades of students with the amount of time they spent studying. The only randomness we should get here is the sampling error. But these students study in different universities, and they themselves have different abilities to learn. These elements infer the randomness we would not like to know, and can be good examples of a random variable. If your data shows effects that you cannot influence but which you want to \"rule out\", the answer to this question is 'yes'.\n\n\n= Multivariate statistics =\n'''You are dealing with Multivariate Statistics.''' Multivariate statistics focuses on the analysis of multiple variables at the same time.\n\nWhich kind of analysis do you want to conduct?\n<imagemap>Image:Statistics Flowchart - Clustering, Networks, Ordination.png|center|650px|\npoly 270 4 143 132 271 252 391 126 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics]]\npoly 129 139 2 267 130 387 250 261 [[An_initial_path_towards_statistical_analysis#Ordinations|]]\npoly 407 141 280 269 408 389 528 263 [[An_initial_path_towards_statistical_analysis#Cluster_Analysis|]]\npoly 269 273 142 401 270 521 390 395 [[An_initial_path_towards_statistical_analysis#Network_Analysis|]]\n</imagemap>\n\n'''How do I know?'''\n* In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points.\n* In a Cluster Analysis (or general Classification), you group your data points according to how similar they are, resulting in a tree structure.\n* In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points.\n\n\n== Ordinations ==\n'''You are doing an ordination.''' In an Ordination, you arrange your data alongside underlying gradients in the variables to see which variables most strongly define the data points. Check the entry on [[Ordinations]] MISSING to learn more.\n\nThere is a difference between ordinations for different data types - for abundance data, you use Euclidean distances, and for continuous data, you use Jaccard distances.\n<imagemap>Image:Statistics Flowchart - Ordination.png|650px|center|\npoly 288 6 153 141 289 268 419 137 [[Data formats]]\npoly 136 154 1 289 137 416 267 285 [[Big problems for later|Euclidean distances]]\npoly 439 149 304 284 440 411 570 280 [[Big problems for later|Jaccard distances]]\n</imagemap>\n\n'''How do I know?'''\n* Check the entry on [[Data formats]] to learn more about the different data formats. Abundance data is also referred to as 'discrete' data.\n* Investigate your data using <code>str</code> or <code>summary</code>. Abundance data is referred to as 'integer' in R, i.e. it exists in full numbers, and continuous data is 'numeric' - it has a comma.\n\n\n== Cluster Analysis ==\n'''So you decided for a Cluster Analysis - or Classification in general.''' In this approach, you group your data points according to how similar they are, resulting in a tree structure. Check the entry on [[Clustering Methods]] to learn more.\n\nThere is a difference to be made here, dependent on whether you want to classify the data based on prior knowledge (supervised, Classification) or not (unsupervised, Clustering).\n<imagemap>Image:Statistics Flowchart - Cluster Analysis.png|650px|center|\npoly 290 3 157 138 289 270 421 134 [[Big problems for later|Classification]]\npoly 138 152 5 287 137 419 269 283 [[Clustering Methods]]\npoly 437 153 304 288 436 420 568 284 [[Clustering Methods]]\n</imagemap>\n\n'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between.\n\n'''How do I know what I want?'''\n* Consider your research intent: are you more interested in the role of individual actors, or rather in the network structure as a whole?\n\n<imagemap>Image:Statistics Flowchart - Network Analysis.png|center|650px|\npoly 290 5 155 137 289 270 419 134 [[An_initial_path_towards_statistical_analysis#Network_Analysis]]\npoly 336 364 4 684 340 1000 644 692 632 676 [[Big problems for later|Bipartite]]\npoly 1064 372 732 700 1060 1008 1372 700 [[Big problems for later|Tripartite]]\n</imagemap>\n\n= Some general guidance on the use of statistics =\nWhile it is hard to boil statistics down into some very few important generalities, I try to give you here a final bucket list to consider when applying or reading statistics.\n\n1) '''First of all, is the statistics the right approach to begin with?''' Statistics are quite established in science, and much information is available in a form that allows you to conduct statistics. However, will statistics be able to generate a piece of the puzzle you are looking at? Do you have an underlying theory that can be put into constructs that enable a statistical design? Or do you assume that a rather open research question can be approached through a broad inductive sampling? The publication landscape, experienced researchers as well as pre-test may shed light on the question whether statistics can contribute solving your problem. \n\n2) '''What are the efforts you need to put into the initial data gathering?''' If you decided that statistics would be valuable to be applied, the question then would be, how? To rephrase this statement: How exactly? Your sampling with all its constructs, sample sizes and replicates decides about the fate of everything you going to do later. A flawed dataset or a small or biased sample will lead to failure, or even worse, wrong results. Play it safe in the beginning, do not try to overplay your hand. Slowly edge your way into the application of statistics, and always reflect with others about your sampling strategy. \n\n3) '''The analysis then demands hand-on skills, as implementing tests within a software is something that you learn best through repetition and practice.''' I suggest you to team up with other peers who decide to go deeper into statistical analysis. If you however decide against that, try to find geeks that may help you with your data analysis. Modern research works in teams of complementary people, thus start to think in these dimensions. If you chip in the topical expertise of the effort to do the sampling, other people may be glad about the chance to analyse the data.\n\n4) '''This is also true for the interpretation, which most of all builds on experience.''' This is the point were a supervisor or a PhD student may be able to glance at a result and tell you which points are relevant, and which are negotiable. Empirical research typically produces results where in my experience about 80 % are next to negliable. It takes time to learn the difference between a trivial and an innovative result. Building on knowledge of the literature helps again to this end, but be patient as the interpretation of statistics is a skill that needs to ripen, since context matters. It is not so much about the result itself, but more about the whole context it is embedded in.\n\n5) The last and most important point explores this thought further. '''What are the limitations of your results?''' Where can you see flaws, and how does the multiverse of biases influence your results and interpretation? What are steps to be taken in future research? And what would we change if we could start over and do the whole thing again? All these questions are like ghosts that repeatedly come to haunt a researcher, which is why we need to remember we look at pieces of the puzzle. Acknowledging this is I think very important, as much of the negative connotation statistics often attracts is rooted in a lack of understanding. If people would have the privilege to learn about statistics, they could learn about the power of statistics, as wells its limitations. \n\n'''Never before did more people in the world have the chance to study statistics.''' While of course statistics can only offer a part of the puzzle, I would still dare to say that this is reason for hope. If more people can learn to unlock this knowledge, we might be able to move out of ignorance and more towards knowledge. I think it would be very helpful if in a controversial debate everybody could dig deep into the available information, and make up their own mind, without other people telling them what to believe. Learning about statistics is like learning about anything else, it is lifelong learning. I believe that true masters never achieve mastership, instead they never stop to thrive for it.\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden (concept, text) and Christopher Franz (implementation, linkages)."
                    },
                    "sha1": "1qi7cfkms5ocfcde1p3cp518rr4h51s"
                }
            },
            {
                "title": "Anaconda",
                "ns": "0",
                "id": "1003",
                "revision": {
                    "id": "6906",
                    "parentid": "6905",
                    "timestamp": "2023-01-27T10:19:44Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8062",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE.\n\n== 1. What is Anaconda? ==\n[[File:Anaconda Logo.png|thumb|right|Source: wikimedia.org]]\n\nAnaconda is an open-source Python and R distribution that includes access to different IDEs, such as Spyder and pyCharm, as well as Jupyter Notebook among others. It is aiming at simplifying package management and claims to be the \u201cworld's most popular data science plattform\u201d (1).\n\n== 2. What's included in Anaconda Distribution? ==\n\nIncluded in the basic Anaconda distribution is the Conda package and environment management system as well as the applications Anaconda Navigator, a Powershell terminal, JupyterLab and Jupyter Notebook, as well as the Spyder IDE. These modules are briefly introduced in the following.\n\nConda\n  \u201cConda is an open-source package and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies. It also easily creates, saves, loads, and switches between environments\u201d (2) on a local computer. \nAnaconda Navigator\n  \u201cDesktop application to easily manage integrated applications, packages, and environments\u201d (2) with a graphical user interface instead of using the command line.\nJupyter Notebook\n  \u201cWeb-based, interactive computing notebook environment to edit and run human-readable documents while describing data analysis.\u201d (3)\nJupyterLab\n  \u201cExtensible Environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.\u201d (3)\nSpyder IDE\n  A scientific Integrated Python Development Environment \u201cwith advanced editing, interactive testing, debugging and runtime introspection features.\u201d (3)\n\n== 3. Basic features/properties of Anaconda ==\n\nThe three main features Anaconda claims is that the distribution is \n\n1. \"open-source\", <br>\n2. \"user-friendly\" and <br>\n3. \"trusted\". \n\nThis means that the code is freely accessible and assessable for projects in any field. Moreover, the intuitive platform allows for easy package and environment handling, all while the \u201csecurely hosted packages and artifacts are methodically tested and regularly updated\u201d (2) to avoid conflicts, bugs or scams. \n\n== 4. How to install Anaconda on my machine ==\n\nAnaconda Distribution can be installed free of charge for private use for Windows, macOS and Linux. The newest version currently features Python 3.9.\n\nIts installation process is straight forward: Download the installation file, choose a path and follow the steps. No account is needed. As a student you can choose to sign up for Anaconda Nucleus for \u201cCommunity membership, unlimited environment backups (and) exclusive data science content\u201d (4) for free. \n\nClick [https://www.anaconda.com/products/distribution#Downloads here] to directly download the installer for your operating system. For Windows and MacOS, graphical installers are provided. For Linux, there are only installation scripts that have to be run from the terminal.\n\nFor an in-depth description and links to download Anaconda for older operating systems, you can make use of their extensive [https://docs.anaconda.com/anaconda/install/ documentation].\n\nIf a more minimalistic approach is preferred, Miniconda can be installed. This version just includes conda, Python and necessary dependencies:\n[https://docs.conda.io/en/latest/miniconda.html Install Miniconda]    \n\nIn case of issues during the installation process, please refer to [https://docs.anaconda.com/anaconda/user-guide/troubleshooting/ the help page].\n\n== 5. Using Conda ==\n\nConda is a command line tool used in a terminal to interact with Anaconda. It is a package and environment management software. It can be used to install or update packages, create, save and load environments. To start using conda, open a terminal, type ''conda'' and press enter.\n\nTo open a terminal on windows, press CTRL + R, type ''cmd. exe'' (write this without a space, we're sorry, this is due to Wiki formatting) and press enter. On macOS, open launcher and type ''terminal'' into the search box, clicking the icon when it appears. On Linux, the shortcut Super + T should do the job, otherwise it can be found in the applications menu.\n\n== 6. Using Anaconda Navigator ==\n\nAnaconda Navigator is a desktop application to manage environments and packages via a graphical user interface rather than the conda command line interface. It is automatically installed with Anaconda Distribution version 4.0.0 or higher. In the left column of the Navigator you can switch between the program's four main components. On the '''Learning page''' and '''Community page''' you can access documentation and training related to the Navigator and find out about support forums and social networking activities in the field. The following section describe in more depth the '''Home page''' and the '''Environments page''' that allow you to manage applications and environments.\n\n=== Home page ===\n[[File:Anaconda Homepage.png|thumb|left|Source: https://docs.anaconda.com/navigator/overview/]]\n\nOn the Home page, you can launch and install applications in the active environment by clicking the respective button on one of the application tiles. Additionally, you can use the gear icon in the top-right corner of the tiles to update, remove, or install a specific version of an application. Note that these actions will only apply to the currently-active environment, which you can select with the '''Applications on''' drop-down located on top of the tiles menu.\nTwo important built-in applications are Spyder for writing and executing code and Jupyter Notebook for generating notbooke files. The Anaconda documentation includes a [https://docs.anaconda.com/navigator/overview/#home-page comprehensive list] of all applications that are included by default and instructions on [https://docs.anaconda.com/navigator/tutorials/nav-app/#building-anaconda-navigator-applications how to build your own Navigator applications].\n\n=== Environments page ===\n\n[[File:Anaconda Navigation.png|thumb|right|Source: https://docs.anaconda.com/navigator/overview/]]\n\nThe Environments page, like Conda, allows you to create and activate environments as well as install, update, and remove packages.\n\n==== Create an environment ====\n\nClick on the '''Create''' button at the bottom of the Environments list. A dialog box appears where you can name your environment and select the versions of Python and/or R you want to work with. You can also choose to import an environment from your local drive or Anaconda Nucleus ('''Import''' button) or clone an existing environment ('''Clone''' button). \n\n==== Activate the environment ====\n\nIn the environments list in the left column, click on the arrow button next to the name to activate the environment you want to use. \n\n==== Installing, updating and removing packages ====\n\nBy default, the right table displays all packages that are installed in the active environment. You can use the filter on top of the list to view not installed, updatable, selecte, or all packages instead. Note that you can use the '''Channels''' button to adjust the locations where Navigator searches for packages. See [https://docs.anaconda.com/navigator/tutorials/manage-channels/ Managing channels] for further information on channels. To install a new package, click the '''Apply''' button for an uninstalled package. To update a package, remove a package or install a different package version, click on the checkbox infront of the package name, select the respective action, and click '''Apply'''.  \n\nIf you run into problems or want to explore the functions of Anaconda Navigator further, you can access the [https://docs.anaconda.com/navigator/ documentation] via the '''Help''' button in the top-left corner of the Navigator.\n\n== References ==\n(1) https://www.anaconda.com/<br>\n\n(2) https://www.anaconda.com/products/distribution<br>\n\n(3) https://docs.anaconda.com/navigator/overview/<br>\n\n(4) https://www.anaconda.com/pricing#<br>\n\n(5) https://www.dominodatalab.com/data-science-dictionary/anaconda/<br>"
                    },
                    "sha1": "axs34wpinx8ia3uzzna7vkbig2vgxsn"
                }
            },
            {
                "title": "Ancova",
                "ns": "0",
                "id": "581",
                "revision": {
                    "id": "5814",
                    "parentid": "5768",
                    "timestamp": "2021-06-14T08:07:46Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Definition */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9826",
                        "#text": "'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients\n\n== Definition ==\nAnalysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.\n\n== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.\n\n\n===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset \"anxiety\" taken from the \"datarium\" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is \"What treatment type has the most effect on anxiety level?\"\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"datarium\")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=\"R\" line>\ndata(\"anxiety\", package = \"datarium\")\ndata = anxiety\nstr(data)\n#Getting the general information on data\n\ncor(data$t1, data$t3, method = \"spearman\")\n#Considering the correlation between independent and dependent continious variables in order to see the level of covariance\n</syntaxhighlight>\n\n[[File:Score_by_the_treatment_type.png|250px|frameless|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nlibrary(repr)\noptions(repr.plot.width=4, repr.plot.height=4)\n#regulating the size of the boxplot\n\nboxplot(t3~group,\ndata = data,\nmain = \"Score by the treatment type\",\nxlab = \"Treatment type\",\nylab = \"Post treatment score\",\ncol = \"yellow\",\nborder = \"blue\"\n)\n</syntaxhighlight>\n<br>\nBased on the boxplot we can see that the anxiety mean score is the highest for individuals who have been practicing basal(grp1) type of exercises and the lowest for individuals who have been practicing the high(grp3) type of exercises. These observations are made based on descriptive statistic by not taking into consideration the overall personal ability of each individual to control his/her anxiety level, let us prove it statistically with the help of ANCOVA.\n\n\n===Checking assumptions===\n1. Linearity assumption can be assessed with the help of the regression fitted lines plot for each treatment group. Based on the plot bellow we can visually assess that the relationship between anxiety score before and after treatment is linear.\n[[File:Score_by_the_treatment_type2.png|250px|thumb|left|alt text]]\n<syntaxhighlight lang=\"R\" line>\nplot(x   = data$t1,\n     y   = data$t3,\n     col = data$group,\n     main = \"Score by the treatment type\",\n     pch = 15,\n     xlab = \"anxiety score before the treatment\",\n     ylab = \"anxiety score after the treatment\")\n\nlegend('topleft',\n       legend = levels(data$group),\n       col = 1:3,\n       cex = 1,   \n       pch = 15)\nabline(lm (t3[group == \"grp1\"]~ t1[group == \"grp1\"],\n              data = data))\nabline(lm (t3[group == \"grp2\"]~ t1[group == \"grp2\"],\n              data = data))\nabline(lm (t3[group == \"grp3\"]~ t1[group == \"grp3\"],\n              data = data))\n</syntaxhighlight>\n<br>\n[[File:Residuals_model.png|250px|thumb|right]]\n2. In order to evaluate whether the residuals are unbiased or not and whether the variance of the residuals is equal or not, a plot of residuals vs. dependent variable can be compiled. Based on the plot bellow we can conclude that the variances between the groups are homogeneous(homoscedastic). For interpretation of the plot please refer to [https://sustainabilitymethods.org/index.php/File:Reading_the_residuals.png#file this figure].\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\n\nplot(fitted(model_1),\n     residuals(model_1))\n</syntaxhighlight>\n\n[[File:Histogram_of_residuals(model_1).png|250px|thumb|right]]\n3. Homogeneity of residuals can be examined with the help of the Residual histogram and Shapiro-Wilk test.\n\n<syntaxhighlight lang=\"R\" line>\nhist(residuals(model_1),\n     col=\"yellow\")\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"R\" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is \"bell shaped\" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=\"R\" line>\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nlibrary(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = \"II\")\n</syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=\"R\" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = \"II\")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=\"R\" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = \"II\")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]\n\nAs you can see after adjustment of berfore treatment anxiety score(t1 = covariate) Sum of Squares of Errors decreased from 102.83 to 9.47 meaning that the \"noise\" from covariate was taken under control by making it possible to evaluate the effect of treatment types only.\n\nSo let us recall our main question \"What treatment type has the most effect on anxiety level?\"\n\nAs we can see from the test result above there is a statistically significant difference in after treatment anxiety score between the groups, F(2, 41) = 218.63, p < 0.0001.\n\nThe F-test showed a significant effect somewhere among the groups. However, it did not tell us which pairwise comparisons are significant. This is where post-hoc tests come into play, which will hekp us to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n<syntaxhighlight lang=\"R\" line>\nanova_comp<-aov(data$t3~data$group+data$t1)\nTukeyHSD(anova_comp,'data$group') \n</syntaxhighlight>\n[[File:TukeyPostHoc_ANCOVA.png|300px|frameless|center]]\n\n\nAccording to Tukey Post-Hoc test the mean anxiety score was statistically significantly greater in grp1 compared to the grp2 and grp3, which means that the treatment type 1 has the most effect on anxiety level.\n\n\n==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable \u2013 and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "q6tsa5c44djpt3mwizjgj4ificdzkg6"
                }
            },
            {
                "title": "Anki",
                "ns": "0",
                "id": "306",
                "revision": {
                    "id": "3245",
                    "parentid": "2245",
                    "timestamp": "2020-11-04T10:17:16Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4017",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Anki Logo.png|100px|thumb|right]]\nAnki is a flashcard app for your computer or smartphone. When you want to learn facts or simple concepts in an effective manner, Anki might be your tool to go. For further insights regarding learning, also check our article: [[Learning_for_exams|Learning for exams]]!\n\n== Goals ==\n* Learn effectively with Flashcards\n* Incorporate learning into your daily life\n* Use [https://en.wikipedia.org/wiki/Spaced_repetition spaced repetition] with ease (spaced repetition is just a fancy way of saying that the app will estimate when the optimal point in time for you to repeat a flashcard would be)\n\n== Getting started ==\n[[File:Anki Example.png|300px|thumb|right]]\n[[File:Anki Example Picture.png|200px|thumb]]\nAs this is well documented elsewhere (check the \"Links\" section below), here's only a very brief overview:\n\n* Download the app here: [https://apps.ankiweb.net/#download Download Anki]\n* Create an AnkiWeb account here: [https://ankiweb.net/account/register Register AnkiWeb Account] (this is not mandatory but allows you to backup your flashcards and synchronize them across multiple devices)\n* Login on your smartphone and computer\n* Create your decks and flashcards! \n\nHere's just a few things you can add:\n* Pictures\n* Audio\n* Videos\n* Formulas (via LateX)\n\nAs mentioned, do check out the official \"Getting Started\" documentation here: [https://docs.ankiweb.net/#/getting-started Getting started with Anki]\n\n== Tips & Tricks ==\nWhile using Anki to learn is quite straightforward and intuitive, here's some tips that might be useful to you.\n\n'''Make it a habit to Anki your notes after the lecture'''<br>\nIt is not only useful to recap new knowledge directly after you learned it, but will also save you time and ensure that you don't lose anything on the way. You can also combine this with note-taking methods that emphasize the use of keywords such as the [http://lsc.cornell.edu/study-skills/cornell-note-taking-system/ Cornell Note Taking System].\n\n'''Establish a habit to use Anki'''<br>\nTry to embed using Anki as a daily or otherwise regular habit. It will increase the accuracy of the spaced-repetition algorithm and also - obviously - make you learn more consistently. It might be a good idea to either set yourself a personal goal of doing Anki once a day, or try to use cues for when you want to study with Anki. You can, for example, try to commit checking Anki everytime you wait for a bus or train, or everytime you brush your teeth, or whatever works best for you.\n\n'''Share decks with your fellow students'''<br>\nSome people might not know this, but Anki gives you the possibility to export your own decks and share them with others. While not strictly and directly useful for yourself, you might want to share those with fellow students because sharing is caring. They might return the favor some time you didn't have the time to create one yourself.\n\n== Links & Further reading ==\n=== Links ===\n* [https://apps.ankiweb.net/ Anki Website]\n* [https://docs.ankiweb.net/#/getting-started Official Anki Manual]\n* [https://apps.ankiweb.net/#download Download Anki]\n* [https://leananki.com/how-to-use-anki-tutorial/#What_is_Anki Blog Tutorial]\n\n=== Videos ===\n* [https://www.youtube.com/watch?v=5urUZUWoTLo Derek Banas - How I Learn Everything (Full Anki Tutorial)] \n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Software]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "e3npe61ynfgp0t09an70itokekdp0dq"
                }
            },
            {
                "title": "Apply, Lapply and Tapply",
                "ns": "0",
                "id": "661",
                "revision": {
                    "id": "4325",
                    "parentid": "4324",
                    "timestamp": "2021-03-03T11:25:42Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14685",
                        "#text": "'''In short:''' The functions of the ''apply()''-family are useful tools to go through slices of your data and repetitively perform operations on them. In more sophisticated terms, these functions offer a concise and convenient way of implementing the SPLIT-APPLY-COMBINE-strategy of data analysis, i.e. splitting up some data into smaller pieces, applying a function to each piece and combining the results.\n\n= Why use apply()-functions? =\nOne main advantage of the ''apply()''-functions is that they let you iterate over your data '''without having to use regular loops'''. While loops in R are '''not''' necessarily much slower than the apply()-collection, their syntax is more complicated and redundant. The various apply()-functions offer a way of accessing specific elements of data in a convenient and simplified way. \n\nMore precisely, in contrast to regular loops, the apply()-functions take a '''top-down approach''' syntactically, making them easier to read and understand. Where loops start from the smallest element (`for (i in x) ...`), with the actual operation possibly nested deep within the loop, apply()-functions call the overlying structures first (matrices, lists, ...) before immediately *applying* a function that automatically runs over each element of the called object.\n\nConcerning speed, vectorized functions like, for example, ''colmeans()'' still perform faster than variations of ''_apply(x, mean)''. The great benefit of the ''apply()''-collection is their versatility, as you can pass any function, predefined or user-defined, on to them.\n\nThere exists a whole bunch of ''apply()''-functions, namely ''apply(''), ''lapply()'', ''sapply()'', ''vapply()'', ''tapply()'', ''rapply()'', ''mapply()''. Which one you use, depends mainly on the type of your input, and on the output you want to get. This article will focus mainly on ''apply()'', ''lapply()'' and ''tapply()''.\n \n= apply() =\nThe function ''apply()'' lets you iterate over the rows and columns of a matrix and run a function on them, returning a vector, list or array as the output.\n\nIt has the following structure, taking three arguments:\n\n'''apply(X, MARGIN, FUN, ...)'''\n\nX is the input and has to be a matrix or an array, while FUN can be any function that should be applied on the data. Be sure to always put in the function's name '''without''' parentheses.\n\nMARGIN specifies if the function should be run on each of the columns, rows or cells of the matrix. With MARGIN = 1, you iterate over the rows, with MARGIN = 2, you iterate over the rows, and with MARGIN = 1:2, you iterate over each cell.\n\nLet's use the ''apply()''-function to analyze an actual dataset, in this case, USPersonalExpenditure.\n\n<syntaxhighlight lang=\"R\" line>\nUSPersonalExpenditure\n\n## Output:\n##\n##                      1940   1945  1950 1955  1960\n## Food and Tobacco    22.200 44.500 59.60 73.2 86.80\n## Household Operation 10.500 15.500 29.00 36.5 46.20\n## Medical and Health   3.530  5.760  9.71 14.0 21.10\n## Personal Care        1.040  1.980  2.45  3.4  5.40\n## Private Education    0.341  0.974  1.80  2.6  3.64\n\n</syntaxhighlight>\n\nLet's say we want to find out the total expenditures for each year. We set MARGIN to 2 to iterate over the columns and apply the function \"sum\" to each of them.\n\n<syntaxhighlight lang=\"R\" line>\napply(USPersonalExpenditure, MARGIN = 2, FUN = sum)\n\n## Output:\n##\n##    1940    1945    1950    1955    1960 \n##  37.611  68.714 102.560 129.700 163.140 \n\n</syntaxhighlight>\n\nLikewise, we can look at the rows and compute the mean. Notice that you don't need to write down the names of the arguments if you give them in the default order.\n\n<syntaxhighlight lang=\"R\" line>\napply(USPersonalExpenditure, 1, mean)\n\n## Output:\n##\n##   Food and Tobacco   Household Operation    Medical and Health         Personal Care \n##             57.260                27.540                10.820                 2.854 \n##  Private Education \n##              1.871 \n</syntaxhighlight>\n\nStoring the result in a variable and looking at its class, we can see that in this case, apply() returns a (named) numeric vector.\n\n<syntaxhighlight lang=\"R\" line>\nmean_Exp <- apply(USPersonalExpenditure, 1, mean)\nclass(mean_Exp)\n\n## Output:\n## [1] \"numeric\"\n\n</syntaxhighlight>\n\nIn case you want to perform an action on each cell of the matrix, this is also possible. Let's say you want to convert the dollar values in USPersonalExpenditures to another currency, i.e. multiply each value by some exchange rate. Since ''apply()'' accepts any function, not just predefined ones, this can be easily done by writing a short anonymous function that takes each value as an argument and computes the new value. Actually, '''inserting anonymous functions''' into ''apply()''-structures are a major feature of the collection as a whole.\n\n<syntaxhighlight lang=\"R\" line>\napply(USPersonalExpenditure, 1:2, function(x) {x * 0.82})\n\n## Output:\n##\n##                         1940     1945    1950   1955    1960\n## Food and Tobacco    18.20400 36.49000 48.8720 60.024 71.1760\n## Household Operation  8.61000 12.71000 23.7800 29.930 37.8840\n## Medical and Health   2.89460  4.72320  7.9622 11.480 17.3020\n## Personal Care        0.85280  1.62360  2.0090  2.788  4.4280\n## Private Education    0.27962  0.79868  1.4760  2.132  2.9848\n\n</syntaxhighlight>\n\nAs you can see, ''apply()'' returns another matrix this time instead of a vector. You might also use ''apply()'' with dataframes. Be aware, though, that the data types of the dataframe must be the same, as ''apply()'' will otherwise convert them to one single type. Therefore, either exclude problematic columns first or use another function.\n\n= lapply() =\n\n''lapply()'' is quite similar in its use to ''apply()'', but whereas the latter iterates over elements of a matrix, ''lapply()'' iterates over elements of a list. The returned object is also always a list.\n\n'''`lapply(X, FUN, ...)`'''\n\nwhere X is the input list and FUN the applied function.\n\n<syntaxhighlight lang=\"R\" line>\nlist_characters <- list(\"Steve\", \"Mary\", 2, 4, \"Lisa\")\nlapply(list_characters, is.character)\n\n## Output:\n##\n## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] FALSE\n## \n## [[4]]\n## [1] FALSE\n## \n## [[5]]\n## [1] TRUE\n</syntaxhighlight>\n\nAgain, you can write your own function that lapply() should take as an argument.\n\n<syntaxhighlight lang=\"R\" line>\nlist_numbers <- list(24, 12, 9, 9, 22, 24, 7)\ndecode_func <- function(x) {letters[length(letters) - x + 1]}\ndecode <- lapply(list_numbers, decode_func)\nunlist(decode)\n\n## Output:\n## [1] \"c\" \"o\" \"r\" \"r\" \"e\" \"c\" \"t\"\n</syntaxhighlight>\n\nYou can very well use ''lapply()'' with vectors, dataframes or matrices, but it will always transform them into lists. In the example, the columns of the dataframe '''infert''' are turned into elements of a list:\n\n<syntaxhighlight lang=\"R\" line>\nlapply(infert, class)\n\n## Output:\n## \n## $education\n## [1] \"factor\"\n## \n## $age\n## [1] \"numeric\"\n## \n## $parity\n## [1] \"numeric\"\n## \n## $induced\n## [1] \"numeric\"\n## \n## $case\n## [1] \"numeric\"\n## \n## $spontaneous\n## [1] \"numeric\"\n## \n## $stratum\n## [1] \"integer\"\n## \n## $pooled.stratum\n## [1] \"numeric\"\n\n</syntaxhighlight>\n\n''lapply()'' can be particularly useful when you want to run several operations on top of each other for parts of your data. This method is way more convenient than using nested for-loops.\n\n== sapply() ==\nIn many situations, working with list data can be inconvenient. One solution to this problem would be to use ''sapply()'', which will always try to simplify the result of ''lapply()'' into either a vector or a matrix.\n\n<syntaxhighlight lang=\"R\" line>\nsapply(infert, class)\n\n## Output:\n## \n##      education              age         parity        induced           case    spontaneous \n##       \"factor\"        \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\" \n##        stratum   pooled.stratum \n##      \"integer\"        \"numeric\" \n</syntaxhighlight>\n\nAs the output values above (the elements of the returned list) are 1-dimensional, ''sapply()'' returns a vector. If on the other hand the results are n-dimensional, ''sapply()'' returns a matrix:\n\n<syntaxhighlight lang=\"R\" line>\nsapply(faithful, quantile)\n\n## Output:\n## \n##      eruptions waiting\n## 0%     1.60000      43\n## 25%    2.16275      58\n## 50%    4.00000      76\n## 75%    4.45425      82\n## 100%   5.10000      96\n\n</syntaxhighlight>\n\nShould the output values have different dimensions, ''sapply()'' cannot simplify. In the example below, the $education column is a 3-dimensional vector, whereas the others are 6-dimensional \u2013 therefore, ''sapply()'' returns a list and therefore works just like ''lapply('') does.\n\n<syntaxhighlight lang=\"R\" line>\nsapply(infert, summary)\n\n## Output:\n## \n## $education\n##  0-5yrs 6-11yrs 12+ yrs \n##      12     120     116 \n## \n## $age\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   21.00   28.00   31.00   31.50   35.25   44.00 \n## \n## $parity\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.000   1.000   2.000   2.093   3.000   6.000 \n## \n## $induced\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.5726  1.0000  2.0000 \n## \n## $case\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.3347  1.0000  1.0000 \n## \n## $spontaneous\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.5766  1.0000  2.0000 \n## \n## $stratum\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   21.00   42.00   41.87   62.25   83.00 \n## \n## $pooled.stratum\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   19.00   36.00   33.58   48.25   63.00 \n\n</syntaxhighlight>\n\n== vapply() ==\n\nWhile ''sapply()'' is a convenient function to quickly get handy results, one cannot be sure which type of object it will return. This may pose a problem if you have long, complex code. To tackle that issue, the function ''vapply()'' lets you specify the output. Thus, you can make sure to avoid errors and make debugging much easier. It may be viewed as '''safer version of sapply().'''\n\nConsequently, ''vapply()'' takes three arguments\n\n'''vapply(X, FUN, FUN.VALUE, ...)'''\n\nwhere FUN.VALUE represents the output type.\n\n<syntaxhighlight lang=\"R\" line>\nvapply(infert, class, character(1))\n\n## Output:\n## \n##      education            age         parity        induced           case    spontaneous \n##       \"factor\"      \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\" \n##       stratum   pooled.stratum \n##      \"integer\"        \"numeric\" \n\n</syntaxhighlight>\n\nWhile vapply accepts the output values above, since each is a character vector of length one as specified, it gives a error message below, because not all the output values are a numeric vector of length 6.\n\n<syntaxhighlight lang=\"R\" line>\n\nvapply(infert, summary, numeric(6))\n## Output:\n## \n## Error in vapply(infert, summary, numeric(6)) : values must be length 6,\n##  but FUN(X[[1]]) result is length 3\n\n</syntaxhighlight>\n\n= tapply() =\n\nOften, you do not only want to look at individual columns or rows but to separate your data further into categories before going into analysis. '''With ''tapply()'', you can first split your data into groups''', and only then apply a function to each of the elements.\n\n''tapply()'' takes at least three arguments\n\n'''tapply(X, INDEX, FUN, ...)'''\n\nwhere INDEX represents the grouping variable which splits the output into as many groups as the grouping variable has components. Naturally, this makes most sense with factor variables.\n\nIn the following example, the variable Petal.Length is grouped by the Species variable; subsequently, the function ''summary()'' is applied on each group.\n\n<syntaxhighlight lang=\"R\" line>\ntapply(iris$Petal.Length, iris$Species, mean)\n\n## Output:\n## \n##     setosa versicolor  virginica \n##      1.462      4.260      5.552 \n\n</syntaxhighlight>\n\nIt is possible to group your data by several categorical variables, if you provide a list of variables to the INDEX-argument. In the example below, using the *UCBAdmission* dataset, ''tapply()'' provides a matrix showing the total student applications by gender and department.\n\n<syntaxhighlight lang=\"R\" line>\nadmit_df <- data.frame(UCBAdmissions)\ntapply(admit_df$Freq, list(admit_df$Gender, admit_df$Dept), sum)\n\n## Output:\n##         A   B   C   D   E   F\n## Male   825 560 325 417 191 373\n## Female 108  25 593 375 393 341\n\n</syntaxhighlight>\n\nAgain, you can write your own function for ''tapply()'', for example if you want to check for the ratio of admissions for each department:\n\n<syntaxhighlight lang=\"R\" line>\ntapply(admit_df$Freq, list(admit_df$Gender, admit_df$Dept), function(x) {x[1]/sum(x)})\n\n## Output:\n##                A         B         C         D         E          F\n## Male   0.6206061 0.6303571 0.3692308 0.3309353 0.2774869 0.05898123\n## Female 0.8240741 0.6800000 0.3406408 0.3493333 0.2391858 0.07038123\n\n</syntaxhighlight>\n\nYou can easily combine several ''apply()''-functions to get the information you are interested in. In the example below, the admission rates are first grouped by ''tapply()'', and then summarized by ''apply()'' to do a statistical analysis of male and female admission rates.\n\n<syntaxhighlight lang=\"R\" line>\nratio <- tapply(admit_df$Freq, list(admit_df$Gender, admit_df$Dept), function(x) {x[1]/sum(x)})\napply(ratio, 1, summary)\n\n## Output:\n## \n##               Male     Female\n## Min.    0.05898123 0.07038123\n## 1st Qu. 0.29084900 0.26454952\n## Median  0.35008301 0.34498707\n## Mean    0.38126623 0.41726920\n## 3rd Qu. 0.55776224 0.59733333\n## Max.    0.63035714 0.82407407\n\n</syntaxhighlight>\n\n= When is it better to use actual loops? =\nThe functions of the ''apply()''-familiy are built to loop over static data. They will process each data point independently of any others and only once. Thus, the ''apply()''-functions '''will not work for recursive tasks'''. Therefore, in cases where the output depends on the previous output and input, loops are the way to go.\n\nSimilarly, ''apply()''-functions '''cannot replace while-loops''', when it is not defined at which point the looping has to stop.\n\n= Tidyverse vs. Base R =\n\nThe ''map_()''-functions of the '''purrr'''-package present an alternative for iteration tasks on data while the '''dplyr'''-package of the Tidyverse offers a plethora of functions for data manipulation such as ''select()'', ''group_by()'' or ''mutate()''. Some users like the consistency and speed of the Tidyverse functions more, while others prefer the stability and sometimes more concise code of Base R. In the end, it is matter of taste.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "rpwjder5lt4jc8pmlogjh347445bi9m"
                }
            },
            {
                "title": "Bachelor Statistics Lecture",
                "ns": "0",
                "id": "114",
                "revision": {
                    "id": "3667",
                    "parentid": "2802",
                    "timestamp": "2021-01-11T13:21:02Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Day 10 - Bias */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2611",
                        "#text": "== Overview of the lecture structure ==\nThis course provides an introduction to statistics on a bachelor level.\n\n=== Day 1 - [[Why_statistics_matters|Why statistics matters]] ===\n<br>\n[[Why_statistics_matters#The_Power_of_Statistics|The Power of Statistics]]\n<br>\n[[Why_statistics_matters#Statistics_as_a_part_of_science|Statistics as a part of science]]\n<br>\n[[Why statistics matters#A very short history of statistics|A very short history of statistics]]\n<br>\n[[Why_statistics_matters#Key_concepts_of_statistics|Key concepts of statistics]]\n\n=== Day 2 - [[Data formats]] and [[Data_formats#Descriptive_statistics|descriptive stats]] ===\n<br>\n[[Data_formats#Continuous_data data|Continuous data]]\n<br>\n[[Data_formats#Ordinal_data|Ordinal data]]\n<br>\n[[Data_formats#Descriptive_statistics|Descriptive statistics]]\n\n=== Day 3 -  [[Data_distribution|Data distribution and Probability]] ===\n<br>\n[[Data_distribution#The_normal_distribution|Normal distribution]]\n<br>\n[[Data_distribution#Non-normal distributions|Non-normal distributions]]\n<br>\n[[Data_distribution#A matter of probability|A matter of Probability]]\n\n=== Day 4 - [[Hypothesis_building|Hypothesis building and simple tests]] ===\n<br>\n[[Hypothesis_building|Hypothesis testing]]\n<br>\n[[Hypothesis_building#Validity |Validity]]\n<br>\n[[Hypothesis_building#Simple tests |Simple tests]]\n\n=== Day 5 - [[Correlations|Correlations]] ===\n<br>\n[[Correlations|Correlations on a shoestring]]\n<br>\n[[Correlations#Reading_correlation_plots|Reading correlation plots]]\n<br>\n[[Correlations#Correlative_relations|Correlative relations]]\n\n=== Day 6 - Regression ===\n<br>\n[[Causality|Causality]]\n<br>\n[[Causality#Residuals|Residuals]]\n<br>\n[[Causality#Significance_in_regressions|Significance in regressions]]\n<br>\n[[Causality#Is_the_world_linear.3F|Is the world linear?]]\n<br>\n[[Causality#Prediction|Prediction]]\n<br>\n\n=== Day 7 - Design 1 - Simple Anova OR the lab experiment ===\n<br>\n[[Experiments#The_laboratory_experiment|The laboratory experiment]]\n<br>\n[[Experiments#How_do_I_compare_more_than_two_groups_.3F|How do I compare more than two groups?]]\n<br>\n[[Designing_studies|Designing studies]]\n\n=== Day 8 - Design 2 - [[Field experiments|Field experiments]] ===\n\n=== Day 9 - [[Case studies and Natural experiments|Case studies and natural experiments]] ===\n\n=== Day 10 - Bias ===\n<br>\n[[Bias in statistics]]\n\n=== Day 11 - [[Statistics and mixed methods|Statistics and mixed methods]] ===\n\n=== Day 12 - [[A word on ethics|A word on ethics]] ===\n\n=== Day 13 - [https://sustainabilitymethods.org/index.php/The_big_recap The Big recap] ===\n\n----\n[[Category: Courses]]"
                    },
                    "sha1": "fk3vtvz6zbppq9vvm8bltiodmy6abvm"
                }
            },
            {
                "title": "Back of the envelope statistics",
                "ns": "0",
                "id": "625",
                "revision": {
                    "id": "6005",
                    "parentid": "6004",
                    "timestamp": "2021-07-08T12:06:36Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11399",
                        "#text": "'''In short:''' Back of the envelope statistics revolve around rough, initial calculations that provide a first understanding of quantitative data. In this entry, some approaches are presented.\n\n==Back of the envelope statistics==\n[[File:Bildschirmfoto 2020-04-08 um 11.37.25.png|thumb|left|Back of the envelope calculations give you a first impression about your idea and where it can go to.]]\n\n[https://www.investopedia.com/terms/b/back-of-the-envelope-calculation.asp Back of the envelope calculations] are rough estimates that are made on the small piece of paper, hence the name. They are extremely helpful to get a quick estimate about the basic numbers for a given formula of principle, thus enabling us to get a [https://www.stlouisfed.org/on-the-economy/2020/march/back-envelope-estimates-next-quarters-unemployment-rate quick calculation] that allows us either to check for the plausibility of an assumption, or to derive a simple explanation for a complex issue. '''Back of the envelope statistics can be for instance helpful when you want to get a rough estimate about an idea that can be expressed in numbers.''' Prominent examples include the dominant character coding of the World Wide Web, as well as the development of the laser. Back of the envelope calculations are fantastic within sustainability science, because they can help us illustrate complex issues in a simple form, and they can serve as a guideline for quick planning. Therefore, they can be used within other more complex forms of methodological applications, such as scenario planning. By quickly calculating different scenarios, we can for instance apply a plausibility check and focus our approaches on-the-fly. I encourage you to learn back of the envelope calculations in your [https://www.youtube.com/watch?v=bAU1MLRwh7c everyday life], as many of us already do. A great app for this is [http://tydligapp.com/ \"Tydlig\"], which unfortunately only exists for iOS devices. It is a great example however of how to break numbers down into overall estimates, and make quick on-the-fly calculations.\n<br/>\n<br>\n\n== Some recommendable examples ==\nIn the following, I provide you some simple examples of back of the envelope calculations, which may help you gain some understanding on why these could be valuable.\n\n=== Simple calculations ===\nAdding, subtracting, multiplying and dividing are part of the everyday language of mathematics, and while we all should have learned these in school, most of us are not versatile in applying them regularly. How do we divide the bill by three? What is 20 % of the bill to add as a tip? How many pieces of cake remain? How can I double this recipe? There is ample evidence that this can be intuitive to some, yet most of us struggle. However, I think it is extremely important to regularly practice, and there are apps such as [https://brilliant.org/ \"Brilliant\"] and games like Sudoku to improve our skills with numbers. Only if you learn the basics, you will be able to master the supreme mathematics.\n\n===Probability===\nWe often hear numbers about probability and chances. While many of these are arbitrary, such as the chances of winning the Lottery, there are others probabilities that matter in our day-to-day life. For instance, the chances of catching a COVID-19 infection outside is 19 times lower than inside. One common misconception is that the chances are 0 - they are not, they are just lower, but considerably so! Another misconception is the question of how low your chances of basically anything are in general. Quite often we cannot stop computing a chance calculation, even though our chances are actually very low anyway. Will I win the Lottery? Probably not. Will I be diagnosed with a rare condition? Probably not. Still, we think more about such things than necessary, considering our actual chances. Then again, spending some time on computing the real probability of a certain event happening may actually help us finish the thought once and for all, and act accordingly. The COVID-19 crisis is a good example, where a combination of several modes of action, such as social distancing, wearing masks or washing your hands, can substantially lower your chances of catching the disease. Knowing about this makes a lot of sense. However, it will be next to impossible to lower your chances to 0, unless you would be willing to create harm to others or yourself in an extent that may, in turn, outweigh the low chances of catching the disease. '''Calculating probabilities can be a good exercise to be more clear about your chances.'''\n\nHowever, sometimes it is not the chances or probabilities that you are interested in, but the actual numbers and what they mean, and these two things differ. A chance of 1:100000 seems rather smallish, yet if I tell you that this is the chance of a popular touristic attraction - a scenic cliff - that a tourist falls of the cliff while taking a selfie, then you would agree to create safety measures. '''Numbers count.''' Probabilities or chances are good to put things into perspective, and compare different risks or possibilities. However, thinking about what a certain probability actually means can really make a difference when evaluating said probability. Sometimes, it is worthwhile considering both sides of the coin. Then again, when you are afraid of flying in a plane because the plane might crash, knowing the chances of actually crashing will hardly help you to get over your fear because the image of what it means is too prevalent in your mind. On the other hand, many smokers are aware of the healths risks and seemingly do not care, and might be better off actually contextualizing these risks. I propose that we should more often calculate our chances - in other words, the probability of something - and the actual meaning of being that one in a million. This might give us a more accurate picture of our conditions and circumstances, and help us decide what to do, and how to act.\n\n===Trend statistics===\nAnother prominent example of back of the envelope calculations is knowledge about predictions. '''Unfortunately, many predictions that we make based on rather small samples can be wrong.''' There are prominent examples of recent elections where much was at stake, and it was ambiguous what would come out in the end. The US election in 2016 and 2020 are relevant examples, which showcase how different samples in forecasts lead to changes in the outcome. For instance, in 2020, over the first day after the election, the counting was leaning towards Trump, yet with more and more counts coming in from the larger cities, many states tilted towards Biden. Here, the pre-counts were biased towards a more positive outcome for the Republicans. However, looking at the counties, and checking where most counts were still being counted, gave a clear picture early on towards a shift to Biden. Such trend statistics can hence be rather advanced, and knowledge about the context is crucial. More often than not, this is however not respected. It is almost impossible to give an absolute minimum sample size that is reliable for any kind of calculation, so we should be aware of a given sample's limits and contextualize our decisions accordingly. \n\nThe case statistics of the early COVID-19 infections from Wuhan in Januar of 2020 showed an early picture in terms of the spread of this disease. With this rate of increase, and the rate of mortality evolving with a few weeks lag, the information that patients were spreading the disease before showing symptoms sealed the trend. We learned from this, and from then on, all trend calculations could be probed based on the initial development in Wuhan. Calculations about countermeasures or vaccines later were mere modifications of the same trend function. The numbers were big enough here to make plausible claims. Calculating percentage growth and then extrapolating further up until a maximum system saturation is reached is thus a common tool to understand what might happen. Looking at temporal trend data and understanding the [[Glossary|patterns]] is one of the most essential tools in these times of global change. Become versatile in reading [[Barplots,_Histograms_and_Boxplots#Barplots|bar plots]], and learn to calculate trends both in absolute numbers as well as in percentage growth. If you practice this, many developments will not come as a surprise to you.\n\n===Group differences===\nWhile there are advanced statistical tests to [[Simple Statistical Tests|compare groups]] and calculate differences between individual groups, comparing mean values between groups is the most relevant calculation. While there is for instance a high variance in lactose intolerance across the globe, people from certain regions should avoid milk probably more than people from other groups. Equally, some groups are more prone to specific diseases than others. This may not say much about your individual risk, but is still an initial prior that can translate into a different chance calculation (see above). '''Variance and mean values are not the same.''' We should be aware of this, and ask how things vary between individuals, and how much a mean value can really say about groups of these individuals. Consider the case of the enzyme breaking down alcohol, which is missing in some people of Asian heritage, leading to an overall lower mean value of Alcohol intolerance in this group. Still, this might not help you in a drinking game, since the variance regarding this enzyme is quite high. Another example is the soapy taste some people experience when eating corianthe. While indeed more Europeans than Asians experience a soapy taste, this difference is remarkably small, and our knowledge about this is based on a study that sampled many European participants, and few participants of Asian heritage. Hence, Back of the envelope statistics can not only help you to know certain patterns better, but can equally help you to recognise flaws in other peoples [[Glossary|assumptions]] or calculations. It is indeed quite often the case that research highlight results that may be statistically significant, but when you check out the actual patterns, then these are quite small.\n\n===System dynamics===\nThe last approach I want to highlight are complex system calculations where several calculations are broken down based on various assumptions. A prominent calculation is the carbon footprint of a person. This can be a quite advanced endeavour demanding many calculations of supply chains and circular dynamics, behavioral choices, individual circumstances and external influences. Yet, there are proxies that can allow you to make a rough estimation. The combination of your food choices, travel distance and frequency - including commute habits - as well as heating and electricity usage can lead to severe changes in the carbon footprint of a person, but we can often rely on general estimates to get a first useful insight. '''Approximating such crude measures can often be more transformative than advanced calculation, and we better leave those to the professionals.''' Building your life choices on simple heuristics can be helpful for many people, and Back of the envelope calculations can support such approaches. \n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "qazaewxo7b4vb5h557xokcr4une80yi"
                }
            },
            {
                "title": "Barplots, Histograms and Boxplots",
                "ns": "0",
                "id": "766",
                "revision": {
                    "id": "7169",
                    "parentid": "7033",
                    "timestamp": "2023-05-11T12:24:26Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* Identifying and interpreting histograms */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15641",
                        "#text": "'''Note:''' This entry revolves specifically around Barplots, Histograms and Boxplots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n__TOC__\n<br/>\n'''In short:''' Barplots, histograms and boxplots are commonly used in [https://www.investopedia.com/terms/d/descriptive_statistics.asp (descriptive) statistics] to visualise distribution of [https://365datascience.com/numerical-categorical-data/ numerical data] with two variables (typically the [https://www.statisticshowto.com/explanatory-variable/ response variable] on the X axis and the [https://www.statisticshowto.com/explanatory-variable/ categorical explanatory variable] on the Y axis). \n\n== Barplots ==\n=== Overview ===\nA barplot(also known as 'bar chart' or 'column chart') displays quantitative values for different categories. The chart comprises line marks (bars) \u2013 not rectangular areas \u2013 with the size attribute (length or height) used to represent the quantitative value for each category.\n\n[[File:Bar chart structure.png|350px|thumb|right|Fig.1:This figure shows the structure of a bar chart.]]\n\n=== Plotting in R ===\nWe will plot a basic bar chart based on a dataset built-in to R called <syntaxhighlight lang=\"R\" inline>mtcars</syntaxhighlight>. The data set contains data on specifications of different cars. One such specification is the number of gears a given car's transmission has.  \nWe will first create a summary table that contains the number of cars for a given count of gears. Then, we will use that table to create the plot.\n\nThe table that contains information about the frequency of cars for a given number of gears looks like this:\n\n{| class=\"wikitable\"\n|-\n! gears !! freq\n|-\n| 3|| 15\n|-\n| 4|| 12\n|-\n| 5|| 5\n|-\n| ... || ...\n|}\n[[File:Bar Chart.png|350px|thumb|right|Fig.2: Result in R]]\n\nHere, the data for <syntaxhighlight lang=\"R\" inline>gears</syntaxhighlight> column are categories, and the data for <syntaxhighlight lang=\"R\" inline>freq</syntaxhighlight> columns are numeric.\n\n<syntaxhighlight lang=\"R\" line>\n# get the data\ngears <- table(mtcars$gear)\n\n# Fig.2\n# Plot a basic bar chart with a title and labels\nbarplot(gears,\n        main = \"Frequency of Vehicles of each Gear Type\",   # title of the plot\n        xlab = \"Number of Gears\", ylab = \"Number of Cars\")  # labels of the plot\n</syntaxhighlight>\n\n\nFor more on Barplots, please refer to the entry on [[Stacked Barplots]].\n\n==Histograms==\n===Overview===\nA histogram is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. Histograms reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n[[File:Beaverbodytemperatures.png|350px|thumb|right|Fig.3]]\n=== Plotting in R ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.3\n par(mfrow = c(2,1))\n  hist(beaver2$temp[1:38], main = \"Body temperature of a beaver (in rest)\", xlab = \"Body Temperature in Celcius\", breaks = 5)\n  hist(beaver2$temp[39:100], main = \"Body temperature of a beaver (in move)\", xlab = \"Body Temperature in Celcius\", breaks = 5)\n</syntaxhighlight>\n\nThe two histograms are plotted from the \u201cbeaver2\u201d dataset and illustrate how a beaver\u2019s body temperature changes when it starts moving. Both histograms resemble the bell-curved shape of normal distribution. We can see a change in the beaver\u2019s body temperature from approximately 37 degrees to 38 degrees.\n\n====Identifying and interpreting histograms====\n'''Histograms Vs. Bar charts''' \nHistograms are different than bar charts, and one should not confuse them. A histogram does not have gaps between the bars, but a bar chart does. Histograms have the response variable on the X-axis, and the Y-axis shows the frequency (or the probability density). In contrast, the Y-axis in a bar chart shows the frequency and the X-axis shows the response variable, which however represents nominal data.\n[[File:Bimodalhistrogram.png|400px|thumb|right|Fig.4]]\n\n'''Patterns''' \nHistograms display well how data is distributed. For instance, a the symmetric, unimodal pattern of a histogram represents a normal distribution. Likewise, skewed right and left patterns in histograms display skewness of data - asymmetry of the distribution of data around the mean. See \u201c[https://www.pqsystems.com/qualityadvisor/DataAnalysisTools/interpretation/histogram_shape.php Histogram: Study the Shape]\u201d to learn more about histogram patterns.\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.4\n hist(beaver2$temp, main = \"Bimodal Histogram: Body temperature of a beaver (in rest & in move)\", xlab = \"Body Temperature in Celcius\", breaks = 12)\n</syntaxhighlight>\n\nIf the beaver2 dataset plotted into one histogram, it takes bimodal pattern and represents binomial distribution as there are two means of sample points - the temperature of a beaver in rest and in the move.\n\n'''Number and width of the bars (bins)'''\nHistograms can become confusing depending on how the bin margin is put. As it is said in The [https://www.wiley.com/en-us/The+R+Book%2C+2nd+Edition-p-9781118448960 R Book, p231] - \u201cWide bins produce one picture, narrow bins produce a different picture, unequal bins produce confusion.\u201d Choice of number and width of bins techniques can heavily influence a histogram\u2019s appearance, and choice of bandwidth can heavily influence the appearance of a kernel density estimate. Therefore, it is suggested that the bins stay in the same width and that the number of the bins is selected carefully to best display pattern in data.\n\n==Boxplots==\n=== Overview ===\nThe box plot is a diagram suited for showing the distribution of continuous, univariate data, by visualizing the following six characteristics of a dataset: minimum, first quartile, median, third quartile, maximum and outliers. The compact and space-saving design of box plots allows the comparison of more than one dataset, which is why they have an edge over other diagrams for continuous data, like histograms. A good example for comparative visualisation of both, boxplots and histograms, can be found in the [https://r4ds.had.co.nz/index.html R for Data Science] book, [https://r4ds.had.co.nz/exploratory-data-analysis.html#cat-cont chapter 7.51]:\n[[File:Screenshot 2021-03-29 at 12.42.17.png|700px|frameless|center]]\n\n=== Components of a boxplot ===\nPrerequisite: The values of the dataset should be sorted in ascending order.\n\n'''Minimum:'''\nLowest value of the dataset (outliers excluded)\n<br>\n'''First quartile:'''\nValue which seperates the lower 25% from the upper 75% of the values of the dataset\n<br>\n'''Median:'''\nMiddle value of the dataset\n<br>\n'''Third quartile:'''\nValue which seperates the upper 25% from the lower 75% of the values of the dataset\n<br>\n'''Interquartile range:'''\nRange between first and third quartile\n<br>\n'''Maximum:'''\nHighest value of the dataset (outliers excluded)\n<br>\n'''Whiskers:'''\nLines ranging from minimum to first quartile and from third quartile to maximum\n<br>\n'''Outliers:'''\nAbnormal values represented by circles beyond the whiskers\n\n=== Plotting in R ===\n==== Single box plot ====\n[[File:meanofozoneonrooseveltislandfrommaytosep1973.png|250px|thumb|right|Fig.5]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.5\nboxplot(airquality$Ozone,\n        main = \"Mean of Ozone on Roosevelt Island from May to Sep 1973 \",\n        xlab=\"Ozone\",\n        ylab=\"Parts per Billion\",\n        boxwex = 0.5,     # defines width of box\n        las = 1,          # flips labels on y-axis into horizontal position\n        col=\"red\",        # defines colour of box\n        border = \"black\"  # turns frame and median of box to black\n        )\n</syntaxhighlight>\n\nBy default, box plots are plotted vertically. It can be flipped into a horizontal position, by passing the argument '''horizontal''' and setting it to '''TRUE'''. Furthermore, the box can be equipped with a '''notch''', by passing the argument notch and setting it to '''TRUE'''.\n\n==== Multiple box plots with interaction ====\n[[File:weightsofchicksgivendifferentdietsfor6weeks.png|250px|thumb|right|Fig.6]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.6\nboxplot(chickwts$weight ~ chickwts$feed,     \n        las = 1, \n        main = \"Weights of chicks given different diets for 6 weeks\", \n        xlab = \"Feed\", \n        ylab = \"Weight in grams\",\n        col = \"red\",\n        border = \"black\"\n        )\n#creates interaction between weight and feed column of dataset\n</syntaxhighlight>\n\n\n==== Sorting multiple box plots ====\nMultiple box plots can be sorted according to their characteristics. The following box plot shows the plot from above, sorted in ascending order by the median.\n[[File:sortedweightsofchicksgivendifferentdietsfor6weeks.png|250px|thumb|right|Fig.7]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.7\npar(mfrow = c(1, 1))     \n# sets the plot window to a one by one matrix\nmedians <- reorder(chickwts$feed, chickwts$weight, median)\n# sorts columns feed and weight according to the median  \nboxplot(chickwts$weight ~ medians, las = 1, main = \"Weights of chicks given different diets for 6 weeks\", xlab = \"Feed\")\n# plots interaction of weight and medians    \n</syntaxhighlight>\n\n====Boxplot with notches====\nSimple boxplots are not very useful at illustrating if the median values between the groups of sets of data are significantly different. Therefore, we use boxplot with nothces to show the distribution of data points around the median and to see whether or not the median values are different between the groups. The notches are drawn as a \u2018waist\u2019 on either side of the median and are intended to give a rough impression of significance of the differences between two medians. Boxes in which the notches do not overlap are likely to prove to have significantly different medians under an appropriate statistical test ([https://www.wiley.com/en-us/The+R+Book%2C+2nd+Edition-p-9781118448960 The R Book, p213]). \n\nThe size of the notch increases with the magnitude of the interquartile range and declines with the square root of replication:\n[[File:notchformula1.png|400px|frameless|center]]\n\nwhere \u2018IQR\u2019 is the interquartile range, and \u2018n\u2019 is the replication per sample. Notches are based on assumptions of asymptotic normality of the median and roughly equal sample sizes for two medians being compared and are said to be rather insensitive to the underlying distribution of the samples. The idea is to give roughly a 95% confidence interval for the difference of two medians. ([https://www.wiley.com/en-us/The+R+Book%2C+2nd+Edition-p-9781118448960 The R Book, p213])\n\n[[File:boxplotswithnwithoutnotches.png|400px|thumb|left|Fig.8]]\nHere are the beaver2 data that was used earlier for plotting histograms:\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.8\n par(mfrow = c(1,2))\n  boxplot(beaver2$temp[1:38],beaver2$temp[39:100], names = c(\"in rest\", \"in move\"), ylab = \"Body temperature\", main = \"Boxplot without notches\")\n  boxplot(beaver2$temp[1:38],beaver2$temp[39:100], names = c(\"in rest\", \"in move\"), ylab = \"Body temperature\", notch = T, main = \"Boxplot with notches\")\n</syntaxhighlight>\n\nBecause the boxes do not overlap, it can be assumed that the difference in the median of the two data samples will be highly significant. However, the same cannot be said for the dataset plotted in the boxplots below.\n[[File:Boxplotquartiles.png|400px|thumb|left|Fig.9]]\nBelow example JohnsonJohnson dataset in R contains quarterly earnings (dollars) per Johnson & Johnson shares share between 1960 and 1980.\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.9\nJnJ <- as.data.frame(type.convert(.preformat.ts(JohnsonJohnson)))\nboxplot(JnJ,notch = T)\n</syntaxhighlight>\nWarning message in bxp(list(stats = structure(c(0.61, 1.16, 2.79, 6.93, 14.04, 0.63, : \u201csome notches went outside hinges ('box'): maybe set notch=FALSE\u201d\n\n'''The odd-looking boxplots'''\nThe JohnsonJohnson boxplot also illustrates the notches\u2019 curious behaviour in the boxes of the Qtr1 and Qtr2. The notches extended above 75th percentile and/or below the 25th percentile when the sample sizes are small and/or the within-sample variance is high. The notches act this way to warn of the likely invalidity of the test ([https://www.wiley.com/en-us/The+R+Book%2C+2nd+Edition-p-9781118448960 The R Book, p214]).\n<br>\n<br>\n<br>\n=== Interpretation ===\n* '''Symmetry:'''\nIf a dataset is symmetric, the median is approximately in the middle of the box.\n\n* '''Skewness:'''\nIf a dataset is skewed, the median is closer to one end of the box than to the other. If the median is closer to the lower (or left) end of the box, the data is considered to be right-skewed. If the median is closer to the upper (or right) end of the box, the data is considered to be left-skewed.\nThere are several ways to exactly measure the skewness of data, such as the Bowley-Galton skewness or Pearson's coefficient of skewness. Unfortunately, depending on the characteristics of the dataset, different skewness measures might give different or even contradictory results.\nA hands-on approach for R is included in the further readings below.\n\n* '''Compactness of data sections:'''\nIf one side of the box is larger than the other, it means that the values in this section are spread over a wider range. If it is smaller than the other one, the values in this section are more densely distributed.\n\n*'''Distance and number of outliers:'''\nThe number of outliers and their distance from the rest of the data can be easily read from the diagram.\n\n* '''Significance of difference between two datasets:'''\nIn case of a normal distribution, the significance of the difference between the data of two box plots can be roughly estimated: If the box of one box plot is higher or lower than the median of another box plot, the difference is likely to be significant. Further investigation is recommended.\n\n== Conclusion ==\nWe can conclude that both histogram and boxplots are a great tool in displaying how our values are distributed. As stated in [[Introduction to statistical figures]], boxplots compared to histogram show the variance of continuous data across different factor levels and are a solid graphical representation of the Analysis of Variance. One could see box plots less informative than a histogram or kernel density estimate but they take up less space and are better at comparing distributions between several groups or sets of data.\n\n\n== Further links & reading material ==\n# [https://www.statmethods.net/graphs/boxplot.html Box plot variations]\n# [https://rdrr.io/cran/car/man/Boxplot.html Box plots with point identification]\n# [https://www.r-bloggers.com/2020/11/skewness-and-kurtosis-in-statistics/ Approaches to measuring skewness in R]\n# [https://sites.google.com/site/davidsstatistics/home/notched-box-plots Good description with many graphics]\n===Practice more===\nIf you do not have installed R or if you cannot run in on your computer, you can run R Code online with [https://rdrr.io/snippets/ Snippets] and folow the examples in below resources.\n===[http://www.cookbook-r.com/ Cookbook for R]===\n#[http://www.cookbook-r.com/Graphs/Histogram_and_density_plot/ Histogram and density plot] in base R \n#[http://www.cookbook-r.com/Graphs/Box_plot/ Boxplots] in base R \n#[http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/ Plotting distributions] with ggplot2\n\n----\nThe [[Table of Contributors|authors]] of this entry is Ilkin Bakhtiarov and Henrik von Wehrden.\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "3m0adaanc1hu6y6ln35i34ixhqabjm1"
                }
            },
            {
                "title": "Bayesian Inference",
                "ns": "0",
                "id": "754",
                "revision": {
                    "id": "5817",
                    "parentid": "5488",
                    "timestamp": "2021-06-14T12:28:04Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "22228",
                        "#text": "[[File:ConceptBayesianInference.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Bayesian Inference]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Bayesian Inference is a statistical line of thinking that derives calculations based on distributions derived from the currently available data.\n\n\n== Background == \n[[File:Bayesian Inference.png|400px|thumb|right|'''SCOPUS hits per year for Bayesian Inference until 2020.''' Search terms: 'Bayesian' in Title, Abstract, Keywords. Source: own.]]\n'''The basic principles behind Bayesian methods can be attributed to the probability theorist and philosopher, Thomas Bayes.''' His method was published posthumously by Richard Price in 1763. While at the time, the approach did not gain that much attention, it was also rediscovered and extended upon independently by Pierre Simon Laplace (1). Bayes' name only became associated with the method in the 1900s (3).\n\n'''The family of methods based on the concept of Bayesian analysis has risen the last 50 years''' alongside the increasing computing power and the availability of computers to more people, enabling the technical precondition for these calculation-intense approaches. Today, Bayesian methods are applied in a various and diverse parts of the scientific landscape, and are included in such diverse approaches as image processing, spam filtration, document classification, signal estimation, simulation, etc. (2, 3)\n\n\n== What the method does ==\n'''Bayesian analysis relies on using probability figures as an expression of our beliefs about events.''' Consequently, assigning probability figures to represent our ignorance about events is perfectly valid in Bayesian approach. The probabilities, hence, depend on the current knowledge we have on the event that we are setting our belief on; the initial belief is known is \"prior\", and the probability figure assigned to the prior is called \"prior probability\". Initially, these probabilities are essentially subjective, as these priors are not the properties of a larger sample. However, the probability figure is updated as we receive more data. The final probabilities that we get after applying the Bayesian analysis, called \"posterior probability\", is based on our prior beliefs about the [[Glossary|hypothesis]], and the evidence that we collect:\n\n[[File:Bayesian Inference - Prior and posterior beliefs.png|450px|thumb|center|'''The probability distribution for prior, evidence, and posterior.''']]\n\nSo, how do we update the probability, and hence our belief about the event, as we receive new information? This is achieved using Bayes' Theorem.\n\n==== Bayes' Theorem ====\nBayes theorem provides a formal mechanism for updating our beliefs about an event based on new data. However, we need to establish some definitions before being able to understand and use Bayes' theorem.\n\n'''Conditional Probability''' is a probability based on some background information. If we consider two events A and B, conditional probability can be represented as:\n\n    P(A|B)\n\nThis representation can be read as the probability of event A occurring (or being observed) given that event B occurred (or B was observed). Note that in this representation, the order of A and B matters. Hence P(A|B) and P(B|A) are convey different information (discussed in the coin-toss example below).\n\n'''Joint Probability''', also called \"conjoint probability\", represents the probability of two events being true - i.e. two events occuring - at the same time. If we assume that the events A and B are independent, this can be represented as:\n\n    P(A\\ and\\ B)=P(B\\ and\\ A)= P(A)P(B)\n\nInterestingly, the conjoint probability can also be represented as follows:\n\n    P(A\\ and\\ B) = P(A)P(B|A)\n\n    P(B\\ and\\ A) = P(B)P(A|B)\n\n'''Marginal Probability''' is just the probability for one event of interest (e.g. probability of A regardless of B or probability of B regardless of A) and can be represented as follows. For the probability of event E:\n\n    P(E)\n\nTechnically, these are all the things that we need to be able to piece together the formula that you see when you search for \"Bayes theorem\" online.\n\n    P(A\\ and\\ B) = P(B\\ and\\ A)\n\n''Caution:'' Even though p(A and B) = p(B and A), p(A|B) is not equal to p(B|A).\n\nWe can now replace the two terms on the side with the alternative representation of conjoint probability as shown above. We get:\n\n    P(B)P(A|B)=P(A)P(B|A)\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\n''Note:'' the marginal probability `p(B)` can also be represented as:\n\n    P(B) = P(B|A)*P(A) + P(B|not\\ A)*P(not\\ A)\n\nWe can see all three definitions that were discussed above appearing in the latter two formulae above. Now, let's see how this formulation of Bayes' Theorem can be applied in a simple coin toss example:\n\n\n=== '''Example I: Classic coin toss''' ===\n'''Imagine, you are flipping 2 fair coins.''' The outcome of one of the coins was a Head. Given that you already got a Head, what is the probability of getting another Head?\n\n(This is same as asking: what is the probability of getting 2 Heads given that you know you have at least one Head)\n\n'''Solution:'''\nIf we represent the outcome of Heads by a H and the outcome of Tails by a T, then the possible outcomes of the two coins being tossed simultaneously can be written as: `HH`, `HT`, `TH`, `TT`\n\nThe two events A, and B are:\n\nA = the outcome is 2 heads (also called \"prior probability\")\n\nB = one of the outcomes was a head (also called \"marginal probability\")\n\nSo, essentially, the problem can be stated as: what is the probability of getting 2 heads, given that we've already gotten 1 head? This is the posterior probability, which can be represented mathematically as:\n\n    P(2\\ heads|1\\ head)\n\nIn this case, the prior probability is the probability of getting 2 heads. We can see from our representation above that the prior probability is 1/4 as there is only one outcome where there are 2 Heads. \n\nSimilarly, the likelihood is the probability of getting at least 1 Head given that we get 2 Heads; think of likelihood as being similar to posterior probability, with the two events switched. If we get 2 Heads, then the probability of getting 1 Head is 100%. Hence, the likelihood is 1.\n\nFinally, we need to know what the probability of getting at least 1 Head, the marginal, is. In this example, there are three cases where the outcome is at least 1 Head. Hence, the marginal is 3/4.\n\nWe saw above that the formula for Bayes' Theorem looks like:\n\n    P(A|B) = \\frac{P(B|A)*(A)}{P(B)}\n\nWe can represent the formula above in context of this case with the following:\n\n    P(2\\ heads|1\\ head) = \\frac{P(1\\ head|2\\ heads)* P(2\\ heads)}{P(1\\ head)}\n    = \\frac{1 * \\frac{1}{4}}{\\frac{3}{4}}\n    = \\frac{1}{3}\n\nSince this is a toy example, it is easy to come up with all the probabilities we need. However, in real world, we might not be able to pin down the exact probability and likelihood values as easily as we did here.\n\n==== Using Bayes' Theorem to Update Our Beliefs ====\nWhat we discussed earlier captures one use-case scenario of Bayes' Theorem. However, we already established that Bayes' Theorem allows us to systematically update our beliefs, or hypothesis, about events as we receive new data. This is called the **diachronic interpretation** of Bayes' Theorem.\n\nTo make things a bit easier to understand in this line of thinking, one way of making this formulation a bit easier to understand is by replacing the As and Bs that we have been using until now with **hypothesis (H)**/belief and **data (D)** respectively (however, the As and Bs don't necessarily need to be that):\n\n    P(hypothesis|data) = \\frac{P(hypothesis)*P(data|hypothesis)}{P(data)}\n\nThis formula can be parsed as follows:\n[[File:Bayes Inference - Bayes' Theorem.png|450px|frameless|center]]\n\nLet's go through this formula from the left side to right:\n* Posterior Probability is the probability that we are interested in. We want to establish if our belief is consistent given the data that we have been able to acquire/observe.\n* Prior Probability is the probability of the belief before we see any data. This is usually based on the background information that we have about the nature of distribution that the samples come from, or past events. This can be computed based on historical data sets or be selected by domain experts on a subject matter.\n* Likelihood is the probability of the data under the hypothesis.\n* The Marginal above is a normalizing constant which represents the probability of the data under any hypothesis.\n\n'''There are two key assumptions that are made about hypotheses here:'''\n1. Hypotheses are mutually exclusive - at most one hypothesis can be true.\n2. Hypotheses are collectively exhaustive - assuming any given hypothesis being considered comes from a pool of hypotheses, at least one of the hypotheses in the pool has to be true.\n\n\n=== '''Example II: Monty Hall Problem''' ===\n(This example is largely based on the book \"Think Bayes\" by Allan B. Downey (referenced below))\n\nThe Monty Hall problem is based on the famous show called \"Let's Make a Deal\" where Monty Hall was the host. In one of the games on the show, Monty would present the following problem to the contestant:\n\n'''There are three doors (Door 1, Door 2, and Door 3)'''. Behind two of those doors are worthless items (a goat), and behind one of the door is something valuable (a car). You don't know what is behind which door, but Monty knows.\n\n[[File:Bayesian Inference - Monty Hall.png|450px|frameless|center|Bayesian Inference - Monty Hall]]\n\nYou are asked to choose one of the doors. Then, Monty opens one of the 2 remaining doors. He only ever opens the door with a goat behind it.\n\nLet's say you choose Door 1 - there is no goat behind Door 1, you are lucky. Then Monty will open either Door 2 or Door 3. Whichever door he opens will contain a goat in it. Then you are asked if you want to change your decision and switch to a different door.\n\nThe question that has baffled people over the years is: \"Do you stick to your choice or do you switch?\" While there are many ways to look at this problem, we could view this from a Bayesian perspective, and later verify if this is correct with an experiment.\n\n'''Step 1:''' Let's summarize the problem in a more mathematical way:\n# Initially, we don't know anything so we choose Door 1:\n''Hypothesis (H)'': Door 1 has a car behind it.\n\n# Then, Monty opens a door. This changes our knowledge about the system.\n''Data (D)'': Monty has revealed a door with a goat behind it.\n\n# What is the probability that Door 1 has a car behind it?\n''Problem'':\n\n    P(H|D) = ?\n\n'''Step 2''': Now, let's summarize our knowledge. \nWe use the Bayes' Theorem to solve this problem:\n\n    P(H|D) = \\frac{P(D|H)*P(H)}{P(D)}\n\nThis can be represented as the following:\n\n    P(H|D) = \\frac{P(D|H)*P(H)}{P(D|H)*P(H) + P(D|not\\ H)*P(not\\ H)}\n\nInitial probability of the Hypothesis being true (prior):\n\n    P(H) = 1/3\n\nSimilarly:\n\n    P(not\\ H) = 1 - P(H) = 1 - \\frac{1}{3} = \\frac{2}{3}\n\nSo, what is `P(D|H)`?\nThis is the probability that Monty opens a door with a goat behind it given that the car is behind Door 1 (the door that we initially choose). Since Monty always opens a door with a goat behind it, this probability is 1. So,\n\n    P(D|H) = 1\n\nWhat is `P(D|not H)`?\nThis is the probability that Monty opens a door with a goat behind it given that the goat is not behind Door 1. Again, since Monty always opens a door with a goat behind it anyway, this probability is also 1. So,\n\n    P(D|not\\ H) = 1\n\n'''Step 3''': Now, we have all the information we need to solve this problem the Bayesian way:\n\n    P(H|D) = \\frac{P(D|H)*P(H)}{P(D|H)*P(H) + P(D|not\\ H)*P(not\\ H)}\n    P(H|D) = \\frac{1 * 1/3}{(1*1/3)+(1 * 2/3)}\n    P(H|D) = 1/3\n\n'''Conclusion''': \nSo, the probability that your hypothesis (or rather belief) that the car is behind the door you choose (Door 1) is only `1/3`. Conversely, the probability that the the car is behind a door that you did not choose is `2/3`. \n\nHow can we use this to guide our decision?\nSince the probability that the car is behind a door that we have not chosen `2/3` and Monty has already opened one door with a goat behind it, it would make more sense to switch.\n\nUsing Bayes' Theorem, we could reach this conclusion purely analytically.\n\nIn addition, we can verify the fact that over time the strategy of switching the doors when asked wins 2/3 of the time:\n\n[[File:Bayesian Inference - Monty Hall Results.png|450px|frameless|center|Bayesian Inference - Monty Hall Results]]\n\n==== R Script for the Monty Hall example ====\nIf you want to verify this experimentally, you are encouraged to run the following R script to see the results:\n\n#### Function that Plays the Monty Hall Problem ####\n\nmonty_hall <- function(){\n  \n  # 1. Available doors\n  door_options <- c(1, 2, 3)\n  \n  # 2. The door selected by the player initially.\n  selected_door <- sample(1:3, size=1)\n  \n  # 3. The door which contains a car behind it.\n  winning_door <- sample(1:3, size=1)\n  \n\n  if (selected_door==winning_door){\n    # remove the selected door from door options\n    door_options = setdiff(door_options, selected_door)\n    \n    # The door that Monty opens\n    open_door = sample(door_options, size=1)\n    \n    # The remaining door (player wins if they don't switch)\n    switching_door = setdiff(door_options, open_door)\n  }\n  else{\n    # Remove the selected door from door options\n    door_options = setdiff(door_options, selected_door)\n    \n    # Remove the winning door from door options\n    door_options = setdiff(door_options, winning_door)\n    \n    # Select the open door\n    open_door = door_options\n    \n    # If player switches, they will switch to the winning door\n    switching_door = winning_door\n  }\n  \n  # If player switches and lands to the winning door,\n  #  \"switching strategy\" wins this round\n  if (switching_door == winning_door){\n    switch = 1\n    non_switch = 0\n  }\n  # If player selected the winning door and sticks to it,\n  # \"non-switching strategy\" wins this round \n  else{\n    switch = 0\n    non_switch = 1\n  }\n\n  return(c(\"switch_wins\"=switch, \"non_switch_wins\"=non_switch))\n\n}\n\n#### Investigation on wins and losses over time ####\n\nN = 1000 # No. of experiments\n\n# Vectors used to store rate of wins of each strategy at each experiment step\nswitching_wins_rate <- c()\nnon_switching_wins_rate <- c()\nswitching_wins <- c()\nnon_switching_wins <- c()\n\n# Conduct the experiments\nfor (i in 1:N) {\n  set.seed(i) # change seed for each experiment\n  result <- monty_hall()\n  # store the wins and losses\n  switching_wins <- c(switching_wins, result[1])\n  non_switching_wins <- c(non_switching_wins, result[2])\n  # calculate winning rate based on current and past experiments\n  total <- sum(switching_wins) + sum(non_switching_wins)\n  swr <- sum(switching_wins) / total\n  nswr <- sum(non_switching_wins) / total\n  # store the winning rate of 2 strategies in the vectors\n  # defined above \n  switching_wins_rate <- c(switching_wins_rate, swr)\n  non_switching_wins_rate <- c(non_switching_wins_rate, nswr)\n}\n\n# Plot the result of the experiments\nplot(switching_wins_rate,\n     type=\"l\",\n     col=\"darkgreen\", cex=1.5,\n     ylim=c(0, 1),\n     main=paste0(\"Monty Hall Results (N=\", N, \")\"),\n     ylab=\"Wins Rate\", \n     xlab=\"No. of Experiments\")\nlines(non_switching_wins_rate, col=\"red\", cex=1.5)\nabline(h=2/3, col=\"darkgreen\")\nabline(h=1/3, col=\"red\")\nlegend(\"topright\", \n       legend = c(\"Switch Wins\", \"Do not Switch Wins\"), \n       col = c('darkgreen', \n               \"red\"),\n       lty = c(1, 1),\n       cex = 1)\ntext(y=0.7, N, labels=\"2/3\", col=\"darkgreen\")\ntext(y=0.3, N, labels=\"1/3\", col=\"red\")\n```\n\n== Strengths & Challenges ==\n* Bayesian approaches incorporate prior information into its analysis. This means that any past information one has can be used in a fruitful way.\n* Bayesian approach provides a more intuitive and direct statement of the probability that the hypothesis is true, as opposed to the frequentist approach where the interpretation of p-value is convoluted.\n\n* Even though the concept is intuitive to understand, the mathematical formulation and definitions can be intimidating for beginners.\n* Identifying correct prior distribution can be very difficult in real life problems which are not based on careful experimental design.\n* Solving complex models with Bayesian approach is still computationally expensive.\n\n\n== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin.\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future. \n\n\n== Key Publications ==\n* Bayes, T. 1997. LII. ''An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S''. Phil. Trans. R. Soc. 53 (1763). 370\u2013418.\n* Box, G. E. P., & Tiao, G. C. 1992. ''Bayesian Inference in Statistical Analysis.'' John Wiley & Sons, Inc.\n* de Finetti, B. 2017. ''Theory of Probability''. In A. Mach\u00ed & A. Smith (Eds.). ''Wiley Series in Probability and Statistics.'' John Wiley & Sons, Ltd.\n* Kruschke, J.K., Liddell, T.M. 2018. ''Bayesian data analysis for newcomers.'' Psychon Bull Rev 25. 155\u2013177.\n\n\n== References ==\n(1) Jeffreys, H. 1973. ''Scientific Inference''. Cambridge University Press.<br/>\n(2) Spiegelhalter, D. 2019. ''The Art of Statistics: learning from data''. Penguin UK.<br/>\n(3) Downey, A.B. 2013. ''Think Bayes: Bayesian statistics in Python''. O'Reilly Media, Inc.<br/>\n(4) Donovan, T.M. & Mickey, R.M. 2019. ''Bayesian statistics for beginners: A step-by-step approach.'' Oxford University Press.<br/>\n(5) Kurt, W. 2019. ''Bayesian Statistics the Fun Way: Understanding Statistics and Probability with Star Wars, LEGO, and Rubber Ducks.'' No Starch Press.\n\n\n== Further Information ==\n* [https://library.wur.nl/frontis/bayes/03_o_hagan.pdfBayesian statistics: principles and benefits]\n* [https://www.youtube.com/watch?v=HZGCoVF3YvM 3Blue1Brown: Bayes' Theorem]\n* [https://www.youtube.com/watch?v=SrEmzdOT65s Basic Probability: Joint, Marginal, and Conditional Probability]\n* [https://www.youtube.com/watch?v=9TDjifpGj-k Crash Course Statistics: Examples of Bayes' Theorem being applied.]\n* [https://www.youtube.com/watch?v=TVq2ivVpZgQ Monty Hall Problem: D!NG]\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Prabesh Dhakal and Henrik von Wehrden."
                    },
                    "sha1": "szemmw74d8alllozya4tuwz878aw5ix"
                }
            },
            {
                "title": "Belbin Team Roles",
                "ns": "0",
                "id": "894",
                "revision": {
                    "id": "6449",
                    "parentid": "6448",
                    "timestamp": "2021-11-07T17:45:22Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Links & Further reading */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6597",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== Why & When ==\nThe Belbin Team Inventory is a '''role test for groups'''. The behavioral test developed by Meredith Belbin fosters effective teamwork and is based on personal questions that you answer online. As a result, you receive an evaluation of your individual preferences regarding the following nine team roles: ''plants, monitor evaluators, specialists, resource investigators, co-ordinators, team workers, shapers, implementers and completer-finishers''.\n\n[[File:Team Roles.png|frameless|right|Team Roles]]\n\n'''To form an effective team, diverse roles should be represented.''' Therefore, it is recommended to do the test before forming groups. However, existing groups can also use the test in order to gain a deeper understanding of [[Glossary|competencies]] and responsibilities. The ideal is of course a team in which all nine different roles are distributed. In reality, however, this is often not possible. If there are some roles that are not represented in your group, you may want to talk about what skills you lack and how you would like to compensate them. \n\n'''Let's take a concrete example''' to show how the test allows groups to work more efficiently: Anna's seminar is planning a group work. As preparation, all students do the Belbin test. Anna is assigned to the role called ''plant''. Based on the results of the whole seminar group, teams are put together which have as many different roles as possible. Anna takes on the role of the ''plant'' in her group. Thus, her team members know from the beginning that Anna is a very creative and open minded person and likes to think outside of the box. Therefore she takes over the creative tasks in the group and is available to everyone as a [[Glossary|brainstorming]] partner. \n\n'''There are some things to take care of:''' \nFirst of all, this method involves personal questions. Thus, it is important to make sure that everyone feels comfortable in the process. Second, the Belbin roles only refer to group work and therefore are not to be equated with character traits. And finally, the roles should not be seen as fixed constructs. Team members can also take on different roles at the same time or change roles in the course of group work. The Belbin roles should rather be understood as a basis for an open discussion.\n\nNot a fan of long texts? Check this [https://www.youtube.com/watch?v=hMesDq_rNOw video].\n\n== What ==\nHere is a short '''overview of the different roles''' and a list of keywords for each role (extracted from [https://www.businesscoaching.co.uk/files/belbin_team_role_theories.pdf Peter Mackechnie]). In addition, the nine roles can be assigned to three categories: thought, people and action oriented roles.\n{| class=\"wikitable\" | + style=\"width: 100%; text-align:left\"\n|-\n! Role !! Keywords !! Category\n|-\n| Plants || Independent, unorthodox, imaginative, original, radical, clever, loner, dominant, socially bold, uninhibited, forthright || thought oriented\n|-\n| Monitor Evaluators || Prudent, hard-headed, intelligent, dispassionate, analytic, unemotional, hardly ever wrong, serious minded, shrewd, judge of proposals, uncommitted, immune to enthusiasm, rations interventions to a minimum || thought oriented\n|-\n| Specialists || Calm, logical, detached, uncommunicative, highly competent in his/her field of expertise, interested only in the exercise of that expertise, accurate, conscientious, may be a dogged finisher || thought oriented\n|-\n| Resource Investigators || Communicative, relaxed, sociable, enthusiastic, outgoing, gregarious, versatile and innovative || people oriented\n|-\n| Co-Ordinators || Calm, self-confident, open, good listener, talent spotter,self-disciplined, commands respect, trusting by nature, strong moral commitment, unflappable in face of controversy, naturally enthusiastic, good speaker, thinks positively || people oriented\n|-\n| Team Workers || Sympathetic, understanding, sensitive, aware, sociable, low dominance, trusting, strong interest in human interaction and [[Glossary|communication]], leads from behind || people oriented\n|-\n| Shapers || Dynamic, impulsive, challenging, looks for a pattern, compulsive drive, nervous energy, strong need for achievement, sociable, opportunistic rather than conscientious, tough minded, emotional, fearless and unflinching in face-to-face contact || action oriented\n|-\n| Implementers || Methodical, practical, hard working, reliable, systematic, tough minded tolerant, self controlled, orthodox, strength of character, organizational flair || action oriented\n|-\n| Completer-Finishers || Painstaking, conscientious, follows through, strong attention to detail, relentless, high self-control, appears unflappable but prone to internal anxiety, consistent, capacity for hard and effective work, impatient of slapdash || action oriented\n|-\n|}\n\n== Goals ==\n* Forming diverse teams \n* Distributing responsibilities based on personal prefernces\n* Beeing aware of personal strengths and weaknesses\n* Enabling effective teamwork\n\n== Getting started ==\n# Do the Belbin test online. There is a free questionnaire in pdf-format, which is linked below. Alternatively, you can do the test on the official Belbin homepage for a fee. \n# After you have received your individual team role, discuss and analyse the results in your large group. Based on this you can now form diverse teams.\n# Once you have found your team, it is important to discuss the roles explicitly and to clarify the tasks and responsibilities associated with them.\n\n== Links & Further reading ==\n* Official Belbin homepage https://www.belbin.com/\n* Book: Belbin, Meredith. \"Belbin team roles.\" Book Belbin Team Roles (2004)\n* Belbin team role theories https://www.businesscoaching.co.uk/files/belbin_team_role_theories.pdf\n* Video explaining the Belbin test https://www.youtube.com/watch?v=hMesDq_rNOw\n* Free Belbin test questionnaire https://stevensonsino.com/wp-content/uploads/2019/06/BELBIN-questionnaire.pdf\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Esther Kohlhase."
                    },
                    "sha1": "r2lztusw1bwyeaxwl3bejd8syj1ea7n"
                }
            },
            {
                "title": "Bias and Critical Thinking",
                "ns": "0",
                "id": "446",
                "revision": {
                    "id": "6720",
                    "parentid": "6382",
                    "timestamp": "2022-06-15T20:31:39Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "25318",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Bias and Critical Thinking (German)]]\n\n'''Note:''' This entry revolves more generally around Bias in science. For more thoughts on Bias and its relation to statistics, please refer to the entry on [[Bias in statistics]].\n\n'''In short:''' This entry discusses why science is never objective, and what we can really know.\n\n== What is bias? ==\n\"The very concept of objective truth is fading out of the world.\" - George Orwell \n\nA bias is \u201cthe action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment\u201d (Cambridge Dictionary). In other words, bias clouds our judgment and often action in the sense that we act wrongly. We are all biased, because we are individuals with individual experiences, and are unconnected from other individuals and/or groups, or at least think we are unconnected.\n\nRecognising bias in research is highly relevant, because bias exposes the myth of objectivity of research and enables a better recognition and reflection of our flaws and errors. In addition, one could add that understanding bias in science is relevant beyond the empirical, since bias can also highlight flaws in our perceptions and actions as humans. To this end, '''acknowledging bias is understanding the limitations of oneself.''' Prominent examples are gender bias and racial bias, which are often rooted in our societies, and can be deeply buried in our subconscious. To be critical researchers it is our responsibility to learn about the diverse biases we have, yet it is [[Big problems for later|beyond this text]] to explore the subjective human bias we need to overcome. Just so much about the ethics of bias: many would argue that overcoming our biases requires the ability to learn and question our privileges. Within research we need to recognise that science has been severely and continuously biased against ethnic minorities, women, and many other groups. Institutional and systemic bias are part of the current reality of the system, and we need to do our utmost to change this - there is a need for [[Glossary|debiasing]] science, and our own actions. While it should not go unnoticed that institutions and systems did already change, injustices and inequalities still exist. Most research is conducted in the global north, posing a neo-colonialistic problem that we are far from solving. Much of academia is still far away from having a diverse understanding of people, and systemic and institutional discrimination are parts of our daily reality. We are on the path of a very long journey, and there is much to be done concerning bias in constructed institutions. \n\nAll this being said, let us shift our attention now to bias in empirical research. Here, we show three different perspective in order to enable a more reflexive understanding of bias. The first is the understanding how different forms of biases relate to [[Design Criteria of Methods|design criteria]] of scientific methods. The second is the question which stage in the application of methods - data gathering, data analysis, and interpretation of results - is affected by which bias, and how. Finally, the third approach is to look at the three principal theories of Western philosophy, namely reason, social contract and utilitarianism - to try and dismantle which of the three can be related to which bias. Many methods are influenced by bias, and recognising which bias affects which design criteria, research stage and principal philosophical theory in the application of a method can help to make empirical research more reflexive. \n\n[[File:SchoolOfThought BiasesPoster A3-page-001.jpg|550px|thumb|center|'''An overview on biases'''. Source: [https://yourbias.is/ yourbias.is]]]\n\n== Design criteria ==\nWhile [[:Category:Qualitative|qualitative research]] is often considered prone to many biases, it is also often more reflexive in recognising its limitations. Many qualitative methods are defined by a strong subjective component - i.e. of the researcher - and a clear documentation can thus help to make an existing bias more transparent. Many [[:Category:Quantitative|quantitative approaches]] have a reflexive canon that focuses on specific biases relevant for a specific approach, such as sampling bias or reporting bias. These are often less considered than in qualitative methods, since quantitative methods are still \u2013 falsely - considered to be more objective. This is not true. While one could argue that the goal of reproducibility may lead to a better taming of a bias, this is not necessarily so, as the crisis in psychology clearly shows. Both quantitative and qualitative methods are potentially strongly affected by several cognitive biases, as well as by bias in academia in general, which includes for instance funding bias or the preference of open access articles. While all this is not surprising, it is still all the much harder to solve. \n\nAnother general differentiation can be made between [[:Category:Inductive|inductive]] and [[:Category:Deductive|deductive]] approaches. Many deductive approaches are affected by bias that is associated to sampling. Inductive approaches are more associated to bias during interpretation. Deductive approaches often build around designed experiments, while the strongpoint of inductive approaches is being less bound by methodological designs, which can also make bias more hidden and thus harder to detect. However, this is why qualitative approaches often have an emphasis on a concise documentation.\n\nThe connection between spatial scales and bias is rather straightforward, since the [[:Category:Individual|individual]] focus is related to cognitive bias, while [[:Category:System|system]] scales are more associated to prejudices, bias in academia and statistical bias. \nWhile the impact of temporal bias is less explored, forecast bias is a prominent example when it comes to [[:Category:Future|future predictions,]] and another error is applying our cultural views and values on [[:Category:Past|past]] humans, which has yet to be clearly named as a bias. \nWhat can be clearly said about both spatial and temporal scales is that we are often irrationally biased towards very distant entities - in space or time - and even irrationally more than we should be. We are for instance inclined to reject the importance of a distant future scenario, although it may widely follow the same odds to become a reality than a close future. For example, almost everybody would like to win the lottery tomorrow rather than win the lottery in 20 years, irrespective of your chances to live and see it happen, or the longer time you may spend with your lottery prize for the (longer) time to come. Humans are most peculiar constructed beings, and we are notorious to act irrationally. This is equally true for spatial distance. We may care irrationally more for people that are close to us as compared to people that are very distant, even independent of joined experience (e.g with friends) or joined history (e.g. with family). Again, this infers a bias which we can be aware of, but which has to be named. No doubt the current social developments will increase our capacities to recognise our biases even more, as all these phenomena also affect scientists.\n\n'''The following table categorizes different types of Bias''' as indicated in the [https://en.wikipedia.org/wiki/Bias Wikipedia entry on Bias] according to two levels of the [[Design Criteria of Methods]].\n\n{| class=\"wikitable\" style = \"text-align: center; background-color: white;\"\n|-\n! '''Type of Bias !! Category !! Description !! Relevant to research !! Qualitative !! Quantitative !! Individual !! System'''\n|-\n| style=\"width: 15%\"| Anchoring || style=\"width: 15%\"| Cognitive Bias || style=\"width: 45%\"| Anchoring' one's analysis on the first encountered piece of information (data) as a reference || style=\"width: 5%\"| x || style=\"width: 5%\"| x || style=\"width: 5%\"| x ||style=\"width: 5%\"|  ||style=\"width: 5%\"|  x\n|-\n| Apophenia || Cognitive Bias || The tendency to perceive meaningful patterns within random data || x || x || x || || x\n|-\n| Attribution Bias || Cognitive Bias || Systematic errors based on flawed perception of others' or one's own behavior ||(x) || x || || x || \n|-\n| Confirmation Bias || Cognitive Bias || The tendency to search for and favor information that confirms one's existent beliefs || x || x || x || || x\n|-\n| Dunning Kruger Effect || Cognitive Bias || Lets lets lesser gifted people assume their superiority over others. || (x) || x || || x ||\n|-\n| Framing || Cognitive Bias || The way that individual actors present and construct evidence || x || x || x || x ||\n|-\n| Cultural Bias || Cognitive Bias || Interpreting and judging phenomena by standards inherent to one's own culture || x || x || || x || \n|-\n| Halo / Horn Effect || Cognitive Bias || An observer's overall impression of an entity influences feelings about specifics of that entity's properties || (x) || x || || x ||\n|-\n| IKEA Effect || Cognitive Bias || Attributing a higher value to something one did by oneself || x || x || || x ||\n|-\n| Hindsight Bias || Cognitive Bias || Convincing yourself after an event that you knew it would happen all along || x || x || || x ||\n|-\n| Self-serving Bias || Cognitive Bias || The tendency to credit accomplishment to one's own capacities but failure to outside factors || (x) || x || || x ||\n|-\n| Status Quo Bias || Cognitive Bias || The emotional tendency to perceive any change to the current situation as deterioration || || x || || || x\n|-\n| Bribery || Conflicts of Interest || Being compensated for an influenced behavior or opinion || (x) || x || || x || \n|-\n| Favoritism || Conflicts of Interest || Favoring members of one's in-group over out-group members || || x || || x || x\n|-\n| Lobbying || Conflicts of Interest || The act of influencing actors towards one's own interests || (x) || x || || || x\n|-\n| Self-Regulation Issues || Conflicts of Interest || Inaccuracies occurring through self-versus independent evaluation || || x || x || x || \n|-\n| Shilling || Conflicts of Interest || Pretending to be, but not being; independent; thereby deceiving observing individuals || || || x || || x\n|-\n| Forecast Bias || Statistical Bias || Consistent differences between forecasts and the actual results || x || || x || || x\n|-\n| Observer-expectancy Bias || Statistical Bias || The subconscious influence a researcher's expectations impose on the research || x || x || || x ||\n|-\n| Reporting Bias || Statistical Bias || Selective choice and publication of information, e.g. (un)desirable research results || x || || x || || x\n|-\n| Social Desirability Bias || Statistical Bias || Survey respondents that tend to answer in a supposedly socially acceptable way || x || || x || || x\n|-\n| Selection Bias || Statistical Bias || Unrepresentative Sampling || x || || x || || x\n|-\n| Classism || Prejudices || Attitudes that benefit a specific social class || (x) || x || x || x || x\n|-\n| Lookism || Prejudices || Prejudices based on physical properties, e.g. attractiveness or cultural preferences || (x) || x || x || x || x\n|-\n| Racism || Prejudices || Behavior based on the assumption that there were inferior or superior (human) races || x || x || x || x || x\n|-\n| Sexism || Prejudices || Behavior based on the assumption that one sex or gender (male, mostly) was better than others || x || x || x || x || x\n|-\n| Academic Bias || Biases in Academia || Researchers who let their beliefs and world views shape their research || x || x || x || || x\n|-\n| Experimenter Bias || Biases in Academia || Different experimenters assess subjective criteria differently, or individuals that are observed act differently when watched || x || x || x || || x\n|-\n| Funding Bias || Biases in Academia || The tendency of a scientific study to support the interests of the study's financial sponsor || x || x || x || x || x\n|-\n| Full-text-on-net Bias || Biases in Academia || Favoring open access journals in the references || x || x || || || x\n|-\n| Publication Bias || Biases in Academia || Only publishing what fits in the narrative of the journal, or only results that are significant || x || x || x || || x\n|-\n| Inductive Bias || Other Biases || Machine learning that predicts the future through algorithms based on specific training cases || x || || x || || x\n|-\n| colspan = \"2\" | Other Biases || colspan = \"6\" | Agenda Setting, Gatekeeping, Sensationalism, Educational Bias, Insider Trading, Implicit Bias, Match Fixing, Racial Profiling, Victim Blaming\n|}\n\n== Bias in gathering data, analysing data and interpreting data ==\nThe three steps of the application of a method are clearly worth investigating, as it allows us to dismantle at which stage we may inflict a bias into our application of a method. Gathering data is strongly associated with cognitive bias, yet also to statistical bias and partly even to some bias in academia. Bias associated to sampling can be linked to a subjective perspective as well as to systematic errors rooted in previous results. This can also affect the analysis of data, yet here one has to highlight that quantitative methods are less affected by a bias through analysis than qualitative methods. This is not a normative judgement, and can clearly be counter-measured by a sound documentation of the analytical steps. We should nevertheless not forget that there are even different assumptions about the steps of analysis in such an established field as statistics. Here, different schools of thought constantly clash regarding the optimal approach of analysis, sometimes even with different results. This exemplifies that methodological analysis can be quite normative, underlining the need for a critical perspective. This is also the case in qualitative methods, yet there it strongly depends on the specific methods, as these methods are more diverse. Concerning the interpretation of scientific results, the amount and diversity of biases is clearly the highest, or in other words, worst. While this is related to the cognition bias we have as individuals, it is also related to prejudices, bias in academia and statistical bias. Overall, we need to recognise that some methods are less associated to certain biases because they are more established concerning the norms of their application, while other methods are new and less tested by the academic community. When it comes to bias, there can be at least a weak effect that safety - although not diversity - concerning methods comes in numbers. More and diverse methods may offer new insights on biases, since one method may reveal a bias that another method cannot reveal. '''Methodological plurality may reduce bias.''' For a fully established method the understanding of its bias is often larger, because the number of times it has been applied is larger. This is especially but not always true for the analysis step, and in parts also for some methodological designs concerned with sampling. Clear documentation is however key to make bias more visible among the three stages.\n\n== Bias and philosophy ==\nThe last and by far most complex point is the root theories associated to bias. Reason, social contract and utilitarianism are the three key theories of Western philosophy relevant for empiricism, and all biases can be at least associated to one of these three foundational theories. Many cognitive bias are linked to reason or unreasonable behaviour. Much of bias relates to prejudices and society can be linked to the wide field of social contract. Lastly, some bias is clearly associated with utilitarianism. Surprisingly, utilitarianism is associated to a low amount of bias, yet it should be noted that the problem of causality within economical analysis is still up for debate. Much of economic management is rooted in [[Causality and correlation|correlative]] understandings, which are often mistaken for clear-cut causal relations. Psychology also clearly illustrates that investigating a bias is different from unconsciously inferring a bias into your research. '''Consciousness of bias is the basis for its recognition''': if you are not aware of bias, you cannot take it into account regarding your knowledge production. While it thus seems not directly helpful to associate empirical research and its biases to the three general foundational theories of philosophy - reason, social contract and utilitatrianism -, we should still take this into account, least of all at it leads us to one of the most important developments of the 20th century: Critical Theory.\n\n== Critical Theory and Bias ==\nOut of the growing empiricism of the enlightenment there grew a concern which we came to call Critical Theory. At the heart of critical theory is the focus on critiquing and changing society as a whole, in contrast to only observing or explaining it. Originating in Marx, Critical Theory consists of a clear distancing from previous theories in philosophy - or associated with the social - that try to understand or explain. By embedding society in its historical context (Horkheimer) and by focussing on a continuous and interchanging critique (Benjamin) Critical Theory is a first and bold step towards a more holistic perspective in science. Remembering the Greeks and also some Eastern thinkers, one could say it is the first step back to a holistic thinking. From a methodological perspective, Critical Theory is radical because it seeks to distinguish itself not only from previously existing philosophy, but more importantly from the widely dominating empiricism, and its societal as well as scientific consequences. A Critical Theory should thus be explanatory, practical and normative, and what makes it more challenging, it needs to be all these three things combined (Horkheimer). Through Habermas, Critical Theory got an embedding in democracy, yet with a critical view of what we could understand as globalisation and its complex realities. The reflexive [[Glossary|empowerment]] of the individual is as much as a clear goal as one would expect, also because of the normative link to the political. \n\n'''Critical Theory is thus a vital step towards a wider integration of diverse philosophies,''' but also from a methodological standpoint it is essential since it allowed for the emergence of a true and holistic critique of everything empirical. While this may be valued as an attack, is can also be interpreted as necessary step, since the arrogance and the claim of truth in empiricism can be interpreted not only as a deep danger to methods. Popper does not offer a true solution to positivism, and indeed he was very much hyped by many. His thought that the holy grail of knowledge can ultimately be never truly reached also generates certain problems. He can still be admired because he called for scientists to be radical, while acknowledging that most scientists are not radical. In addition, we could see it from a post-modernist perspective as a necessary step to prevent an influence of empiricism that might pose a threat to and by humankind itself, may it be through nuclear destruction, the unachievable and feeble goal of a growth economy (my wording), the naive and technocratic hoax of the eco modernists (also my wording) or any other [[Glossary|paradigm]] that is short-sighted or naive. In other words, we look at the postmodern. \n\nCritical Theory to this end is now developing to connect to other facets of the discourse, and some may argue that its focus onto the social science can be seen critical in itself, or at least as a normative choice that is clearly anthropocentric, has a problematic relationship with the empirical, and has mixed relations with its diverse offspring that includes gender research, critique of globalisation, and many other normative domains that are increasingly explored today. Building on the three worlds of Popper (the physical world, the mind world, human knowledge), we should note another possibility, that is Critical Realism. Roy Bhaskar proposed three ontological domains (''strata of knowledge''): the real (which is ''everything there is''), the actual (''everything we can grasp''), and the empirical (''everything we can observe''). During the last decade, humankind unlocked ever more strata of knowledge, hence much of the actual became empirical to us. We have to acknowledge that some strata of knowledge are hard to relate, or may even be unrelatable, which has consequences for our methodological understanding of the world. Some methods may unlock some strata of knowledge but not others. Some may be specific, some vague. And some may only unlock new strata based on a novel combinations. What is most relevant to this end is however, that we might look for causal links, but need to be critical that new strata of knowledge may make them obsolete. Consequently, there are no universal laws that we can thrive for, but instead endless strata to explore.\n\nComing back to bias, '''Critical Theory seems as an antidote to bias''', and some may argue Critical Realism even more so, as it combines the criticality with a certain humbleness necessary when exploring the empirical and causal. The explanatory characteristic allowed by Critical Realism might be good enough for the pragmatist, the practical may speak to the modern engagement of science with and for society, and the normative is aware of \u2013 well - all things normative, including the critical. Hence a door was opened to a new mode of science, focussing on the situation and locatedness of research within the world. This was surely a head start with Kant, who opened the globe to the world of methods. There is however a critical link in Habermas, who highlighted the duality of the rational individual on a small scale and the role of global societies as part of the economy (Habermas 1987). This underlines a crucial link to the original three foundational theories in philosophy, albeit in a dramatic and focused interpretation of modernity. Habermas himself was well aware of the tensions between these two approaches \u2013 the critical and the empirical -, yet we owe it to Critical Theory and its continuations that a practical and reflexive knowledge production can be conducted within deeply normative systems such as modern democracies. \n\nLinking to the historical development of methods, we can thus clearly claim that Critical Theory (and Critical Realism) opened a new domain or mode of thinking, and its impact can be widely felt way beyond the social science and philosophy that it affected directly. However, coming back to bias, the answer to an almost universal rejection of empiricism will [[Big problems for later|not be followed here]]. Instead, we need to come back to the three foundational theories of philosophy, and need to acknowledge that reason, social contract and utilitarianism are the foundation of the first empirical disciplines that are at their core normative (e.g. psychology, social and political science, and economics). Since bias can be partly related to these three theories, and consequentially to specific empirical disciplines, we need to recognise that there is an overarching methodological bias. This methodological bias has a signature rooted in specific design criteria, which are in turn related to specific disciplines. Consequently, this methodological bias is a disciplinary bias - even more so, since methods may be shared among scientific disciplines, but most disciplines claim either priority or superiority when it comes to the ownership of a method.\n\nThe disciplinary bias of modern science thus creates a deeply normative methodological bias, which some disciplines may try to take into account yet others clearly not. In other words, the dogmatic selection of methods within disciplines has the potential to create deep flaws in empirical research, and we need to be aware and reflexive about this. '''The largest bias concerning methods is the choice of methods per se.''' A critical perspective is thus not only of relevance from a perspective of societal responsibility, but equally from a view on the empirical. Clear documentation and reproducibility of research are important but limited stepping stones in a critique of the methodological. This cannot replace a critical perspective, but only amends it. Empirical knowledge will only look at parts - or strata according to Roy Bhaskar - of reality, yet philosophy can offer a generalisable perspective or theory, and Critical Theory, Critical Realism as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.\n\n== Further Information ==\n[https://www.thoughtco.com/critical-theory-3026623 Some words on Critical Theory]<br>\n[https://www.newworldencyclopedia.org/entry/Critical_realism#Contemporary_critical_realism A short entry on critical realism]\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "t69aod9bvchce9wyhq0tp2g0cv01rzs"
                }
            },
            {
                "title": "Bias and Critical Thinking (German)",
                "ns": "0",
                "id": "494",
                "revision": {
                    "id": "6786",
                    "parentid": "6381",
                    "timestamp": "2022-10-10T08:57:50Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "29126",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version, can be found here: [[Bias and Critical Thinking]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag er\u00f6rtert, wieso Wissenschaft nie objektiv sein kann, und was wir \u00fcberhaupt wissen k\u00f6nnen.\n\n== Was ist Bias? ==\n\"The very concept of objective truth is fading out of the world.\" - George Orwell \n\nEin Bias ist \"die Handlung, eine bestimmte Person oder Sache in unfairer Weise zu unterst\u00fctzen oder abzulehnen, weil pers\u00f6nliche Meinungen das eigene Urteil beeinflussen k\u00f6nnen\" (Cambridge Dictionary). Mit anderen Worten: Bias tr\u00fcbt unser Urteilsverm\u00f6gen und oft auch unser Handeln in dem Sinne, dass wir falsch handeln. Wir sind alle voreingenommen (= biased), weil wir Individuen mit individuellen Erfahrungen sind und keine Verbindung zu anderen Individuen und/oder Gruppen haben oder zumindest denken, dass wir keine Verbindung zu ihnen haben.\n\nDas Erkennen von Bias in der Forschung ist von hoher Relevanz, weil Bias den Mythos der Objektivit\u00e4t der Forschung entlarvt und eine bessere Erkennung und Reflexion unserer Fehler und Schw\u00e4chen erm\u00f6glicht. Dar\u00fcber hinaus k\u00f6nnte man hinzuf\u00fcgen, dass das Verst\u00e4ndnis von Bias in der Wissenschaft \u00fcber das Empirische hinaus relevant ist, da Bias auch Fehler in unseren Wahrnehmungen und Handlungen als Menschen aufzeigen kann. '''In dieser Hinsicht bedeutet das Anerkennen von Bias, seine eigenen Grenzen zu verstehen'''. Prominente Beispiele f\u00fcr Bias sind geschlechtsspezifische und rassistische Vorurteile, die oft in unseren Gesellschaften verwurzelt sind und tief in unserem Unterbewusstsein vergraben sein k\u00f6nnen. Ich denke, es liegt in unserer Verantwortung, die verschiedenen Biases, die wir haben, kennen zu lernen, und dennoch ist es [[Big problems for later|nicht Inhalt dieses Texts]], die subjektive menschliche Voreingenommenheit zu erforschen, die wir \u00fcberwinden m\u00fcssen. Nur soviel zur Ethik von Bias: Viele w\u00fcrden argumentieren, dass die \u00dcberwindung unserer Biases die F\u00e4higkeit erfordert, zu lernen und unsere Privilegien in Frage zu stellen. In der Forschung m\u00fcssen wir erkennen, dass die Wissenschaft gegen\u00fcber ethnischen Minderheiten, FLINTA*s und vielen anderen Gruppen schwer und kontinuierlich voreingenommen war. Institutioneller und systemischer Bias sind Teil der gegenw\u00e4rtigen Realit\u00e4t des Systems, und ich glaube, dass wir unser M\u00f6glichstes tun m\u00fcssen, um dies zu \u00e4ndern. Es sollte zwar nicht unbemerkt bleiben, dass sich die Institutionen und Systeme bereits ver\u00e4ndert haben, aber Ungerechtigkeiten und Ungleichheiten bestehen nach wie vor. Die meisten Forschungsarbeiten werden im globalen Norden durchgef\u00fchrt und stellen ein neokolonialistisches Problem dar, von dessen L\u00f6sung wir noch weit entfernt sind. Ein Gro\u00dfteil der akademischen Welt ist noch weit davon entfernt, ein vielf\u00e4ltiges Verst\u00e4ndnis von Menschen zu haben, und systemische und institutionelle Diskriminierung sind Teil unserer t\u00e4glichen Realit\u00e4t. Wir sind auf dem Weg eines sehr langen Weges, und es gibt viel zu tun, was Bias in konstruierten Institutionen wie Universit\u00e4ten betrifft. \n\nNach all diesen Vorbemerkungen m\u00f6chte ich nun unsere Aufmerksamkeit auf Biases in der empirischen Forschung lenken. Ich biete drei verschiedene Perspektiven an, um ein reflexiveres Verst\u00e4ndnis von Bias zu erm\u00f6glichen. Die erste ist das Verst\u00e4ndnis, wie verschiedene Formen von Biases mit [[Design Criteria of Methods|Designkriterien]] wissenschaftlicher Methoden zusammenh\u00e4ngen. Die zweite ist die Frage, welche Phase in der Anwendung von Methoden - Datenerhebung, Datenanalyse und Ergebnisinterpretation - von welchem Bias betroffen ist und wie. Der dritte Ansatz schlie\u00dflich besteht darin, die drei Haupttheorien der westlichen Philosophie, n\u00e4mlich die Vernunft, den Sozialvertrag und den Utilitarismus, zu betrachten - um zu versuchen, zu zerlegen, welche der drei Theorien mit welchem Bias in Verbindung gebracht werden k\u00f6nnen. Viele Methoden werden durch Bias beeinflusst, und die Erkenntnis, welcher Bias welche Entwurfskriterien, welches Forschungsstadium und welche philosophische Haupttheorie bei der Anwendung einer Methode beeinflusst, kann dazu beitragen, die empirische Forschung reflexiver zu machen. \n\n[[File:SchoolOfThought BiasesPoster A3-page-001.jpg|550px|thumb|center|'''An overview on biases'''. Source: [https://yourbias.is/ yourbias.is]]]\n\n== Designkriterien ==\nW\u00e4hrend [[:Category:Qualitative|qualitative Forschung]] oft als anf\u00e4llig f\u00fcr viele Biases angesehen wird, ist sie oft auch reflexiver im Erkennen ihrer Grenzen. Viele qualitative Methoden werden durch eine starke subjektive Komponente - d.h. des Forschenden - definiert, und eine klare Dokumentation kann daher dazu beitragen, einen bestehenden Bias transparenter zu machen. Viele [[:Category:Quantitative|quantitative Ans\u00e4tze]] haben einen reflexiven Kanon, der sich auf spezifische Biases konzentriert, die f\u00fcr einen bestimmten Ansatz relevant sind, wie z.B. Stichprobenbiases oder Biases bei der Berichterstattung. Diese werden oft weniger ber\u00fccksichtigt als bei qualitativen Methoden, da quantitative Methoden immer noch - f\u00e4lschlicherweise - als objektiver angesehen werden. '''Dies ist nicht wahr.''' Man k\u00f6nnte zwar argumentieren, dass das Ziel der Reproduzierbarkeit zu einer besseren Z\u00e4hmung eines Bias f\u00fchren k\u00f6nnte, aber dies ist nicht unbedingt der Fall, wie die Krise in der Psychologie deutlich zeigt. Sowohl quantitative als auch qualitative Methoden werden potenziell stark durch verschiedene kognitive Biases sowie durch solche im akademischen Bereich im Allgemeinen beeinflusst, wozu zum Beispiel der Bias bei der Finanzierung oder die Bevorzugung von Open-Access-Artikeln geh\u00f6ren. All dies ist zwar nicht \u00fcberraschend, aber um so schwieriger zu l\u00f6sen. \n\nEine weitere allgemeine Unterscheidung l\u00e4sst sich zwischen [[:Category:Inductive|induktiven]] und [[:Category:Deductive|deduktiven]] Ans\u00e4tzen treffen. Viele deduktive Ans\u00e4tze sind von einem Bias betroffen, die mit der Probenahme verbunden ist. Induktive Ans\u00e4tze sind eher mit einem Bias bei der Interpretation verbunden. Deduktive Ans\u00e4tze bauen oft um geplante Experimente herum, w\u00e4hrend die St\u00e4rke induktiver Ans\u00e4tze darin liegt, dass sie weniger an methodische Designs gebunden sind, was auch dazu f\u00fchren kann, dass Biases verborgener und damit schwerer zu entdecken sind. Aus diesem Grund liegt bei qualitativen Ans\u00e4tzen jedoch h\u00e4ufig der Schwerpunkt auf einer pr\u00e4gnanten Dokumentation.\n\nDer Zusammenhang zwischen r\u00e4umlichen Skalen und Bias ist recht einfach, da der Schwerpunkt [[:Category:Individual|auf dem Individuum]] mit kognitiven Biases zusammenh\u00e4ngt, w\u00e4hrend [[:Category:System|System-]]Skalen eher mit Vorurteilen, Biases im akademischen Bereich und statistischen Biases assoziiert werden. \nW\u00e4hrend die Auswirkungen zeitlicher Biases weniger erforscht sind, ist der prognostische Bias ein prominentes Beispiel, wenn es um [[:Categorie:Future|Zukunftsprognosen]] geht, und ein weiterer Fehler ist die Anwendung unserer kulturellen Ansichten und Werte auf Menschen [[:Category:Past|der Vergangenheit]], was noch nicht eindeutig als Bias benannt wurde.\nWas sowohl \u00fcber r\u00e4umliche als auch \u00fcber zeitliche Skalen klar gesagt werden kann, ist, dass wir oft irrational gegen\u00fcber sehr weit entfernten Entit\u00e4ten - in Raum oder Zeit - und sogar irrational mehr voreingenommen sind, als wir es sein sollten. '''Wir neigen zum Beispiel dazu, die Bedeutung eines fernen Zukunftsszenarios abzulehnen, obwohl es weitgehend den gleichen Chancen folgt, Realit\u00e4t zu werden, wie eine nahe Zukunft.''' Zum Beispiel m\u00f6chte fast jede*r lieber morgen als in 20 Jahren im Lotto gewinnen, unabh\u00e4ngig von den Chancen, die Sie haben, zu leben und es geschehen zu sehen, oder von der l\u00e4ngeren Zeit, die Sie mit Ihrem Lotteriegewinn f\u00fcr die (l\u00e4ngere) kommende Zeit verbringen werden. Der Mensch ist ein h\u00f6chst eigent\u00fcmlich konstruiertes Wesen, und wir sind ber\u00fcchtigt daf\u00fcr, irrational zu handeln. Dies gilt auch f\u00fcr r\u00e4umliche Distanz. Wir m\u00f6gen uns irrational mehr um Menschen k\u00fcmmern, die uns nahe stehen, als um Menschen, die sehr weit entfernt sind, sogar unabh\u00e4ngig von gemeinsamen Erfahrungen (z.B. mit Freunden) oder gemeinsamer Geschichte (z.B. mit der Familie). Auch hieraus l\u00e4sst sich ein Bias ableiten, dessen wir uns bewusst sein k\u00f6nnen, der aber benannt werden muss. Zweifellos werden die aktuellen gesellschaftlichen Entwicklungen unsere F\u00e4higkeit, unsere Biases zu erkennen, noch verst\u00e4rken, da all diese Ph\u00e4nomene auch Wissenschaftler*innen betreffen. \n\n'''Die nachfolgende Tabelle kategorisiert verschiedene Typen von Bias''' gem\u00e4\u00df des englischsprachigen [https://en.wikipedia.org/wiki/Bias Wikipedia-Eintrages \u00fcber Bias], entsprechend zweier [[Design Criteria of Methods (German)|Designkriterien]].\n\n{| class=\"wikitable\" style = \"text-align: center; background-color: white;\"\n|-\n! '''Bias-Typ !! Kategorie !! Beschreibung !! Relevant f\u00fcr Forschung? !! Qualitativ !! Quantitativ !! Individual !! System'''\n|-\n| style=\"width: 15%\"| Ankern || style=\"width: 15%\"| Kognitiver Bias || style=\"width: 45%\"| Die eigene Analyse am ersten angetroffenen Datenpunkt orientieren || style=\"width: 5%\"| x || style=\"width: 5%\"| x || style=\"width: 5%\"| x ||style=\"width: 5%\"|  ||style=\"width: 5%\"|  x\n|-\n| Apoph\u00e4nie || Kognitiver Bias || Die Tendenz, in zuf\u00e4lligen Daten bedeutungsvolle Muster zu erkennen || x || x || x || || x\n|-\n| Attributionsbias || Kognitiver Bias || Systematische Fehler auf Basis einer mangelhaften Wahrnehmung des eigenen oder fremden Verhaltens ||(x) || x || || x || \n|-\n| Best\u00e4tigungsbias || Kognitiver Bias || Die Tendenz, Informationen zu suchen und zu bevorzugen, die die eigene bestehende Meinung best\u00e4tigen || x || x || x || || x\n|-\n| Dunning-Kruger-Effekt || Kognitiver Bias || l\u00e4sst wenig begabte Menschen annehmen, sie seien Anderen \u00fcberlegen || (x) || x || || x ||\n|-\n| Framing || Kognitiver Bias || Die Art und Weise, mit der individuelle Akteur*innen Daten pr\u00e4sentieren und konstruieren || x || x || x || x ||\n|-\n| Kultureller Bias ||Kognitiver Bias || Die Interpretation und Bewertung von Ph\u00e4nomenen gem\u00e4\u00df der eigenen kulturellen Standards || x || x || || x || \n|-\n| Halo / Horn-Effekt || Kognitiver Bias || Der allgemeine Eindruck eines Beobachters beeinflusst dessen Gef\u00fchle \u00fcber spezifische Eigenschaften einer Entit\u00e4t || (x) || x || || x ||\n|-\n| Eigenn\u00fctziger Bias || Kognitiver Bias || Die Tendenz, Erfolge sich selbst und Fehlschl\u00e4ge \u00e4u\u00dferen Faktoren zuzuschreiben || (x) || x || || x ||\n|-\n| R\u00fcckblick-Bias || Kognitiver Bias|| Convincing yourself after an event that you knew it would happen all along || x || x || || x ||\n|-\n| Status Quo-Bias || Kognitiver Bias || Die emotionale Tendenz, jede Ver\u00e4nderung als Verschlechterung wahrzunehmen || || x || || || x\n|-\n| Bestechung || Interessenskonflikte || F\u00fcr eine spezielle Meinung oder ein spezielles Verhalten verg\u00fctet zu werden || (x) || x || || x || \n|-\n| Bevorzugung||  Interessenskonflikte || Mitglieder der eigenen Gruppe gegen\u00fcber Au\u00dfenstehenden zu bevorzugen || || x || || x || x\n|-\n| Lobbyismus ||  Interessenskonflikte || Andere Akteur*innen zugunsten eigener Interessen zu beeinflussen || (x) || x || || || x\n|-\n| Probleme bei Selbstregulierung ||  Interessenskonflikte || Ungenauigkeiten auf Basis von Selbsteinsch\u00e4tzung gegen\u00fcber unabh\u00e4ngiger externer Evaluation || || x || x || x || \n|-\n| Shilling ||  Interessenskonflikte || Vorzugeben, unabh\u00e4ngige*r Beobachter*in zu sein, w\u00e4hrend man dies nicht ist || || || x || || x\n|-\n| Vorhersagen-Bias || Statistischer Bias || Konsistente Unterschiede zwischen Vorhersagen und tats\u00e4chlichen Ergebnissen || x || || x || || x\n|-\n| Erwartungshaltungs-Bias || Statistischer Bias || Der unterbewusste Einfluss, den die Erwartungshaltung der*des Forscher*ins auf die Forschung hat || x || x || || x ||\n|-\n| Reporting Bias || Statistischer Bias || Selektive Wahl und Ver\u00f6ffentlichung von Informationen, z.B. (un)erw\u00fcnschte Forschungsergebnisse || x || || x || || x\n|-\n| Soziale Erw\u00fcnschtheit-Bias || Statistischer Bias || Umfrageteilnehmende, die so antworten, wie sie es f\u00fcr sozial erw\u00fcnscht halten || x || || x || || x\n|-\n| Selektions-Bias || Statistischer Bias || Unrepr\u00e4sentatives Sampling || x || || x || || x\n|-\n| Klassismus || Vorurteile || Haltungen, die einer bestimmten sozialen Klasse zugutekommen || (x) || x || x || x || x\n|-\n| Lookismus || Vorurteile || Vorurteile auf Basis k\u00f6rperlicher Eigenschaften, z.B. Attraktivit\u00e4t oder kulturelle Pr\u00e4ferenz || (x) || x || x || x || x\n|-\n| Rassismus || Vorurteile || Verhalten auf Basis der Annahme, es g\u00e4be \u00fcber- und unterlegene (menschliche) Rassen || x || x || x || x || x\n|-\n| Sexismus || Vorurteile || Verhalten auf Basis der Annahme, dass ein Geschlecht oder eine sexuelle Orientierung (zumeist m\u00e4nnlich, heterosexuell) besser als andere w\u00e4re || x || x || x || x || x\n|-\n| Wissenschaftlicher Bias || Wissenschaftlicher Bias || Wissenschaftler*innen, die ihre Weltanschauungen ihre Forschung beeinflussen lassen || x || x || x || || x\n|-\n| Experimentatoren-Bias || Wissenschaftlicher Bias || Verschiedene Wissenschaftler*innen, die subjektive Kriterien unterschiedlich bewerten, oder Individuen, die sich unter Beobachtung anders verhalten || x || x || x || || x\n|-\n| F\u00f6rderungs-Bias || Wissenschaftlicher Bias || Die Tendenz einer wissenschaftlichen Studie, den finanziellen Sponsor der Studie zu unterst\u00fctzen || x || x || x || x || x\n|-\n| Full-text-on-net Bias || Wissenschaftlicher Bias || Die Bevorzugung von ''open access''-Journals in der Quellliteratur || x || x || || || x\n|-\n| Publikations-Bias || Wissenschaftlicher Bias || Die Ver\u00f6ffentlichung nur dessen, das in die Narrative des Journals passt, oder nur signifikanter Ergebnisse || x || x || x || || x\n|-\n| Induktiver Bias || Andere || ''Machine Learning'', das die Zukunft auf Basis spezifischer F\u00e4lle als Lerngrundlage vorhersagt || x || || x || || x\n|-\n| colspan = \"2\" | Andere || colspan = \"6\" | ''Agenda-Setting'', ''Gatekeeping'', Sensationalismus, Bildungs-Bias, ''Insider Trading'', ''Impliziter Bias, Match Fixing, Racial Profiling, Victim Blaming''\n|}\n\n== Bias in der Sammlung, Analyse und Interpretation von Daten ==\nDie drei Schritte der Anwendung einer Methode sind es eindeutig wert, untersucht zu werden, da sie es uns erm\u00f6glichen, zu dekonstruieren, in welchem Stadium wir einen Bias bei der Anwendung einer Methode hervorrufen k\u00f6nnen. Das Sammeln von Daten ist stark mit kognitiven Biases verbunden, aber auch mit statistischem Bias und teilweise sogar mit einem gewissen Bias im akademischen Bereich. Biases im Zusammenhang mit Stichproben k\u00f6nnen sowohl mit einer subjektiven Perspektive als auch mit systematischen Fehlern, die in fr\u00fcheren Ergebnissen verwurzelt sind, in Verbindung gebracht werden. Dies kann sich auch auf die Analyse von Daten auswirken, doch muss hier hervorgehoben werden, dass quantitative Methoden weniger von Bias durch die Analyse betroffen sind als qualitative Methoden. Dies ist keine normative Beurteilung und kann durch eine fundierte Dokumentation der Analyseschritte eindeutig konterkariert werden. Dennoch sollten wir nicht vergessen, dass es in einem so etablierten Bereich wie der Statistik sogar unterschiedliche Annahmen \u00fcber die Analyseschritte gibt. Hier kollidieren st\u00e4ndig verschiedene Denkschulen \u00fcber den optimalen Ansatz der Analyse, manchmal sogar mit unterschiedlichen Ergebnissen. Dies ist ein Beispiel daf\u00fcr, dass die methodologische Analyse recht normativ sein kann, was die Notwendigkeit einer kritischen Perspektive unterstreicht. Dies ist auch bei qualitativen Methoden der Fall, doch dort h\u00e4ngt es stark von den spezifischen Methoden ab, da diese Methoden vielf\u00e4ltiger sind. Was die Interpretation wissenschaftlicher Ergebnisse anbelangt, so ist das Ausma\u00df und die Vielfalt der Biases eindeutig am h\u00f6chsten, oder mit anderen Worten, am schlimmsten. Dies h\u00e4ngt zwar mit den kognitiven Biases zusammen, die wir als Individuen haben, aber auch mit Vorurteilen, Biases im akademischen Bereich und statistischen Biases. Insgesamt m\u00fcssen wir erkennen, dass einige Methoden weniger mit bestimmten Biases in Verbindung gebracht werden, weil sie hinsichtlich der Normen ihrer Anwendung etablierter sind, w\u00e4hrend andere Methoden neu sind und von der akademischen Gemeinschaft weniger getestet werden. Wenn es um Bias geht, kann es zumindest einen schwachen Effekt haben, dass die Sicherheit - wenn auch nicht die Vielfalt - bei den Methoden zahlenm\u00e4\u00dfig zunimmt. Mehr und vielf\u00e4ltigere Methoden k\u00f6nnen neue Erkenntnisse \u00fcber Bias bieten, da eine Methode einen Bias aufdecken kann, den eine andere Methode nicht aufdecken kann. '''Methodische Pluralit\u00e4t kann Bias reduzieren.''' Bei einer vollst\u00e4ndig etablierten Methode ist das Verst\u00e4ndnis f\u00fcr ihre Biases oft gr\u00f6\u00dfer, weil sie h\u00e4ufiger angewandt wurde. Dies gilt insbesondere, aber nicht immer, f\u00fcr den Analyseschritt und teilweise auch f\u00fcr einige methodische Designs, die sich mit der Probenahme befassen. Eine klare Dokumentation ist jedoch der Schl\u00fcssel, um die Biases zwischen den drei Stufen besser sichtbar zu machen.\n\n== Bias und Philosophie ==\nDer letzte und bei weitem komplexeste Punkt sind die Bias zugrundeliegenden Theorien. Vernunft, Gesellschaftsvertrag und Utilitarismus sind die drei f\u00fcr den Empirismus relevanten Schl\u00fcsseltheorien der westlichen Philosophie, und alle Biases k\u00f6nnen zumindest einer dieser drei Grundlagentheorien zugeordnet werden. Viele kognitive Biases sind mit Vernunft oder unvern\u00fcnftigem Verhalten verbunden. Ein gro\u00dfer Teil der Biases bezieht sich auf Vorurteile, und die Gesellschaft kann mit dem weiten Feld des Gesellschaftsvertrags in Verbindung gebracht werden. Und schlie\u00dflich h\u00e4ngt ein gewisser Bias eindeutig mit Utilitarismus zusammen. \u00dcberraschenderweise wird Utilitarismus mit einem geringen Ma\u00df an Bias in Verbindung gebracht, doch ist anzumerken, dass das Problem der Kausalit\u00e4t innerhalb der wirtschaftlichen Analyse noch immer zur Debatte steht. Ein gro\u00dfer Teil des Wirtschaftsmanagements wurzelt in [[Causality and Correlation|korrelativem]] Verst\u00e4ndnis, das oft mit eindeutigen kausalen Beziehungen verwechselt wird. Die Psychologie zeigt auch deutlich, dass die Untersuchung eines Bias sich von der unbewussten Ableitung eines Bias in die Forschung unterscheidet. '''Das Bewusstsein \u00fcber Bias ist die Grundlage f\u00fcr dessen Anerkennung''': Wenn Sie sich des Bias nicht bewusst sind, k\u00f6nnen Sie ihn bei Ihrer Wissensproduktion nicht ber\u00fccksichtigen. Auch wenn es daher nicht direkt hilfreich erscheint, empirische Forschung und ihre Biases mit den drei allgemeinen Grundlagentheorien der Philosophie - Vernunft, Gesellschaftsvertrag und Utilitarismus - in Verbindung zu bringen, sollten wir dies dennoch ber\u00fccksichtigen, zumindest f\u00fchrt es uns zu einer der wichtigsten Entwicklungen des 20. Jahrhunderts: Die Kritische Theorie.\n\n== Kritische Theorie und Bias ==\nAus dem wachsenden Empirismus der Aufkl\u00e4rung wuchs eine Besorgnis, die wir Kritische Theorie zu nennen begannen. Im Zentrum der Kritischen Theorie steht die Fokussierung auf die Kritik und Ver\u00e4nderung der Gesellschaft als Ganzes, im Gegensatz zu ihrer blo\u00dfen Beobachtung oder Erkl\u00e4rung. Die Kritische Theorie, die ihren Ursprung bei Marx hat, besteht in einer deutlichen Distanzierung von fr\u00fcheren Theorien in der Philosophie - oder mit dem Sozialen verbundenen Theorien -, die zu verstehen oder zu erkl\u00e4ren versuchen. Durch die Einbettung der Gesellschaft in ihren historischen Kontext (Horkheimer) und durch die Konzentration auf eine kontinuierliche und wechselseitige Kritik (Benjamin) ist die Kritische Theorie ein erster und mutiger Schritt hin zu einer ganzheitlicheren Perspektive in der Wissenschaft. Wenn man sich an die griechischen Philosoph*innen und auch an einige Denker*innen aus dem Nord-Afrikanischen, Arabischen und West-Asiatischen Raum erinnert, k\u00f6nnte man sagen, dass dies der erste Schritt zur\u00fcck zu einem ganzheitlichen Denken ist. Aus einer methodologischen Perspektive ist die Kritische Theorie radikal, weil sie sich nicht nur von der bisherigen Philosophie, sondern vor allem vom weit verbreiteten Empirismus und seinen gesellschaftlichen wie wissenschaftlichen Konsequenzen abzugrenzen sucht. Eine Kritische Theorie sollte daher erkl\u00e4rend, praktisch und normativ sein, und was sie herausfordernder macht, sie muss all diese drei Dinge miteinander verbinden (Horkheimer). Durch Habermas erhielt die Kritische Theorie eine Einbettung in die Demokratie, jedoch mit einem kritischen Blick auf das, was wir als Globalisierung und ihre komplexen Realit\u00e4ten verstehen k\u00f6nnten. Das reflexive Empowerment des Individuums ist ein ebenso klares Ziel, wie man es erwarten w\u00fcrde, auch wegen der normativen Verbindung zum Politischen. \n\n'''Kritische Theorie ist somit ein entscheidender Schritt hin zu einer umfassenderen Integration verschiedener Philosophien,''' aber auch aus methodologischer Sicht ist sie von wesentlicher Bedeutung, da sie die Entstehung einer wahren und ganzheitlichen Kritik an allem Empirischen erm\u00f6glichte. Auch wenn dies als Angriff gewertet werden kann, w\u00fcrde ich es als einen notwendigen Schritt werten, da die Arroganz und der Wahrheitsanspruch des Empirismus nicht nur als eine tiefe Gefahr f\u00fcr die Methoden interpretiert werden kann. Popper bietet keine echte L\u00f6sung f\u00fcr den Positivismus, und in der Tat wurde er von vielen sehr gehyped. Sein Gedanke, dass der Heilige Gral des Wissens letztlich nie wirklich erreicht werden kann, erzeugt auch gewisse Probleme. Man kann ihn immer noch bewundern, weil er Wissenschaftler*innen dazu aufrief, radikal zu sein, w\u00e4hrend er einr\u00e4umte, dass die meisten Wissenschaftler*innen nicht radikal sind. Dar\u00fcber hinaus k\u00f6nnten wir ihn aus einer postmodernen Perspektive als einen notwendigen Schritt sehen, um einen Einfluss des Empirismus zu verhindern, der eine Bedrohung f\u00fcr und durch die Menschheit selbst darstellen k\u00f6nnte, sei es durch nukleare Zerst\u00f6rung, das unerreichbare und schwache Ziel einer Wachstumswirtschaft (meine Formulierung), den naiven und technokratischen Schwindel der \u00d6ko-Moderne (ebenfalls meine Formulierung) oder jedes andere Paradigma, das kurzsichtig oder naiv ist. Mit anderen Worten, wir schauen auf die Postmoderne. \n\nDie Kritische Theorie entwickelt sich nun, um eine Verbindung zu anderen Facetten des Diskurses herzustellen, und einige m\u00f6gen argumentieren, dass ihr Fokus auf die Sozialwissenschaft an sich als kritisch angesehen werden kann, oder zumindest als eine normative Wahl, die eindeutig anthropozentrisch ist, ein problematisches Verh\u00e4ltnis zum Empirischen hat und gemischte Beziehungen zu ihren vielf\u00e4ltigen Nachkommen hat, zu denen die Geschlechterforschung, die Globalisierungskritik und viele andere normative Bereiche geh\u00f6ren, die heute zunehmend erforscht werden. Aufbauend auf den drei Welten von Popper (die physische Welt, die Geisteswelt, das menschliche Wissen) sollten wir eine weitere M\u00f6glichkeit beachten, n\u00e4mlich den Kritischen Realismus. Roy Bhaskar schlug drei ontologische Bereiche (''Strata of Knowledge'') vor: das Reale (d.h. \"alles, was es gibt\"), das Tats\u00e4chliche (\"alles, was wir begreifen k\u00f6nnen\") und das Empirische (\"alles, was wir beobachten k\u00f6nnen\"). W\u00e4hrend des letzten Jahrzehnts hat die Menschheit immer mehr Strata erschlossen, daher wurde f\u00fcr uns ein Gro\u00dfteil des Wirklichen empirisch. Wir m\u00fcssen anerkennen, dass einige Strata nur schwer oder gar nicht in Beziehung zueinander gesetzt werden k\u00f6nnen, was Konsequenzen f\u00fcr unser methodisches Verst\u00e4ndnis der Welt hat. Einige Methoden k\u00f6nnen einige Strata erschlie\u00dfen, andere jedoch nicht. Einige m\u00f6gen spezifisch, andere vage sein. Und manche erschlie\u00dfen vielleicht nur neue Schichten auf der Grundlage einer neuartigen Kombination. Am wichtigsten ist jedoch, dass wir nach Kausalzusammenh\u00e4ngen suchen, aber kritisch darauf achten m\u00fcssen, dass neue Strata sie obsolet machen k\u00f6nnen. Folglich gibt es keine universellen Gesetze, nach denen wir streben k\u00f6nnen, sondern stattdessen endlose Schichten, die es zu erforschen gilt.\n\nUm auf Bias zur\u00fcckzukommen: '''Die Kritische Theorie scheint ein Gegenmittel gegen Bias zu sein''', und einige m\u00f6gen den Kritischen Realismus sogar noch mehr verargumentieren, da er die Kritikalit\u00e4t mit einer gewissen Bescheidenheit verbindet, die bei der Erforschung des Empirischen und Kausalen notwendig ist. Das Erkl\u00e4rungsmerkmal, das der Kritische Realismus zul\u00e4sst, mag f\u00fcr Pragmatiker*innen gut genug sein, das Praktische mag f\u00fcr die moderne Auseinandersetzung der Wissenschaft mit und f\u00fcr die Gesellschaft sprechen, und das Normative ist sich - nun ja - aller normativen Dinge, einschlie\u00dflich des Kritischen, bewusst. Damit wurde eine T\u00fcr zu einem neuen Modus der Wissenschaft ge\u00f6ffnet, der sich auf die Situation und Verortung der Forschung innerhalb der Welt konzentriert. Dies war sicherlich ein Vorsprung gegen\u00fcber Kant, der den Globus f\u00fcr die Welt der Methoden \u00f6ffnete. Es gibt jedoch einen kritischen Zusammenhang bei Habermas, der die Dualit\u00e4t - wenn ich mir erlauben darf - des rationalen Individuums im Kleinen und die Rolle der globalen Gesellschaften als Teil der Wirtschaft hervorhob (Habermas 1987). Dies unterstreicht eine entscheidende Verbindung zu den urspr\u00fcnglichen drei grundlegenden Theorien in der Philosophie, wenn auch in einer dramatischen und fokussierten Interpretation der Moderne. Habermas selbst war sich der Spannungen zwischen diesen beiden Ans\u00e4tzen - dem kritischen und dem empirischen - wohl bewusst, dennoch verdanken wir es der Kritischen Theorie und ihren Fortsetzungen, dass eine praktische und reflexive Wissensproduktion in zutiefst normativen Systemen wie den modernen Demokratien durchgef\u00fchrt werden kann. \n\nIn Verbindung mit der historischen Entwicklung der Methoden k\u00f6nnen wir also eindeutig behaupten, dass die Kritische Theorie (und der Kritische Realismus) einen neuen Bereich oder eine neue Denkweise er\u00f6ffnet hat und ihre Auswirkungen weit \u00fcber die Sozialwissenschaft und Philosophie hinaus sp\u00fcrbar sind, die sie direkt betroffen hat. Um jedoch auf Bias zur\u00fcckzukommen: Die Antwort auf eine fast universelle Ablehnung des Empirismus wird [[Big problems for later|hier nicht verfolgt werden]]. Stattdessen m\u00fcssen wir auf die drei grundlegenden Theorien der Philosophie zur\u00fcckkommen und anerkennen, dass Vernunft, Gesellschaftsvertrag und Utilitarismus die Grundlage der ersten empirischen Disziplinen bilden, die in ihrem normativen Kern stehen (Psychologie, Sozial- und Politikwissenschaft und Wirtschaftswissenschaften). Da Biases teilweise auf diese drei Theorien und folglich auf bestimmte empirische Disziplinen zur\u00fcckgef\u00fchrt werden k\u00f6nnen, m\u00fcssen wir anerkennen, dass es einen \u00fcbergreifenden methodologischen Bias gibt. Dieser hat eine Signatur, die in spezifischen Gestaltungskriterien wurzelt, die wiederum mit spezifischen Disziplinen in Verbindung stehen. Folglich handelt es sich bei diesem methodischen Bias um einen disziplin\u00e4ren Bias - umso mehr, da Methoden zwar zwischen den wissenschaftlichen Disziplinen geteilt werden k\u00f6nnen, die meisten Disziplinen aber entweder Priorit\u00e4t oder \u00dcberlegenheit beanspruchen, wenn es um die Eigent\u00fcmer*innenschaft einer Methode geht.\n\nDer disziplin\u00e4re Bias der modernen Wissenschaft schafft somit zutiefst normative methodologische Biases, die einige Disziplinen versuchen m\u00f6gen, zu ber\u00fccksichtigen, andere jedoch eindeutig nicht. Mit anderen Worten: Die dogmatische Auswahl von Methoden innerhalb der Disziplinen hat das Potenzial, tiefe Fehler in der empirischen Forschung zu verursachen, und wir m\u00fcssen uns dessen bewusst sein und dar\u00fcber nachdenken. '''Der gr\u00f6\u00dfte Bias in Bezug auf Methoden ist die Wahl der Methoden an sich.''' Eine kritische Perspektive ist also nicht nur aus der Perspektive der gesellschaftlichen Verantwortung relevant, sondern ebenso aus der Sicht der Empirie. Eine klare Dokumentation und Reproduzierbarkeit der Forschung sind wichtige, aber begrenzte Sprungbretter f\u00fcr eine Kritik der Methodik. Sie kann eine kritische Perspektive nicht ersetzen, sondern nur erg\u00e4nzen. Empirisches Wissen wird nur Teile - oder Schichten nach Roy Bhaskar - der Wirklichkeit betrachten, dennoch kann die Philosophie eine verallgemeinerbare Perspektive oder Theorie anbieten, und Kritische Theorie, Kritischer Realismus sowie andere aktuelle Entwicklungen der Philosophie k\u00f6nnen als ein Aufbl\u00fchen hin zu einer integrierten und ganzheitlichen Wissenschaftsphilosophie gesehen werden, die letztlich an eine \u00fcbergreifende Theorie der Ethik (Parfit) ankn\u00fcpfen kann. Wenn das Empirische und das Kritische uns informieren, dann k\u00f6nnen sowohl eine Wissenschaftsphilosophie als auch eine Ethik uns sagen, wie wir auf der Grundlage unserer Wahrnehmung der Wirklichkeit handeln k\u00f6nnen.\n\n[[Category:Normativity of Methods]]"
                    },
                    "sha1": "js8zy0mi1q96iqyno01sk1lvilb1ut8"
                }
            },
            {
                "title": "Bias in Interviews",
                "ns": "0",
                "id": "799",
                "revision": {
                    "id": "6417",
                    "parentid": "6416",
                    "timestamp": "2021-10-18T08:24:38Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14982",
                        "#text": "'''In short:''' This entry revolves around biases in Interview preparation and conduction. For more general insights on biases, please refer to the entry on [[Bias and Critical Thinking]]. For more in Interview methodology in general, please visit the [[Interviews]] page.\n\n\nA bias is \u201cthe action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment\u201d (Cambridge Dictionary). You might know this already, least from the entry on [[Bias and Critical Thinking]]. '''Researchers are humans, and humans are always biased to some extent - therefore, researchers are biased, if they are aware of it or not.''' This understanding is rooted in our assumption that epistemological knowledge is subjective. It is however better to be aware of one's biases in order to be able to counteract them in one's research. So, in this entry, let us focus on the biases that influence your research that revolves around any form of Interviews. We will have a look at different biases, and for which methods and situations they are especially relevant. For biases influencing the analysis of Interviews - which is the next step - please refer to [[Bias in Content Analysis]] and [[Bias in statistics]].\n\n\n== Biases in Interview Preparation, Literature Research, and Sampling ==\nIn preparation to any form of Interview, you may conduct a systematic literature review or some other form of literature-based theoretical preparation to be able to construct and conduct the Interviews. To this end, you need to make a compromise between the broader literature that may be helpful to gain validity and follow common scientific norms when designing your study, but also at more focused literature in order to have sufficient specificity regarding your interview questions. Here, a '''Reporting / Publication Bias''' will influence your literature search, which refers to the issue that [[Glossary|journals]] and researchers often only report specific types of information, and leave out others. One of the main problem to this end is that most research is biased towards positive or dramatic results, hence you will find few papers describing in detail on how many levels they struggled or failed. In addition, journals often only publish what fits into its narrative, making editors and journals the gatekeepers of science. This is not your fault - but you should be aware of it, factor it into your theoretical assumptions, and ideally try not contribute to these biases in the end by yourself. \n\nFurther, there is often a '''Full-Text-On-Net Bias''', which emerges from the limited access researchers may have to scientific journals and the fact that they thus often favor open access journals in the references. We are quite sure to have this bias here on this Wiki, and you will also struggle with it. Open Access Journals are not necessarily worse than the ones you do not have access to - but by relying on them only, you are missing out  from a large share of available research, and you should try at least to get a hold of papers that are not accessible with one click.\n\nThen, there are biases which is ultimately rooted in you as a researcher. There is the '''Academic Bias''' which highlights that researchers often let their beliefs and world views shape their research. Think about it: which topic are you investigating? Which assumptions are you making, which hypotheses are you stating? Why do you do this research - is it purely because there is a need for it, or do you maybe have a personal opinion or prejudice which you let guide your research design? We are all clouded by our subjective perspectives, but you should question your research approach time after time to unveil such biases. These kinds of biases are especially relevant for rather deductive Interview approaches, such as Surveys and Semi-structured Interviews.\n\nFurther, you might commit a '''Congruence Bias''' if you only test for their hypothesis directly but not for possible alternative hypotheses. There may be other explanations for a specific phenomenon, and by limiting yourself to one perspective, you are limiting your research results and the discourse around a given topic. A similar bias is the '''Confirmation Bias''', where researchers only confirm their expectations again and again by building on the same kind of design, analysis or theories. You are not creating new or even novel knowledge by doing the same kind of research over and over. To find new solutions, you should broaden your theoretical and methodological perspective. These biases are also mostly relevant for deductive designs.\n\nIn the creation of a sample for any kind of Interview, '''Sampling / Selection Bias'''\u00a0is a most common problem. We might only sample data that we are aware of, because we are aware of it, thereby ignoring other data points that lack our recognition. In other words, we are anchoring our very design and sampling already in our previous knowledge. This, again, will likely not create new knowledge. We might also choose an incorrect survey mode (digital, face-to-face) for the intended population, or simply address the wrong population overall. Examples are the exclusion of people who only have a limited Internet access when conducting an online survey or asking teachers instead of their students about an issue that students can provide more accurate information on. Biased samples are \u201cunrepresentative of the intended population and hurt generalizability claims about inferences drawn\u201d (Bhattacherjee, 2012, p.\u00a081) from it.\n\nIt is difficult to create a representative sample for quantitative studies, and to know which characteristics of research subjects (e.g. individuals) are of relevance to ensure representativity. The sampling strategy can heavily influence the gathered data, with convenience sampling potentially inhibiting more diverse insights. Especially for more qualitative Interviews with smaller samples, the selection of Interviewees needs to be well-reasoned and will shape the results.\n\nThe '''non-response bias''' is relevant in Survey research, and refers to the response rate of the targeted sample which \u2013 if too low \u2013 can negatively affect the validity as well as generalizability of the results. Here, it can be important to investigate why the response rate is low in order to assess the impact this might have on the results or to eliminate the causes. (Bhattacherjee, 2012) Besides, the data can be weighted to account for the non-response and approximate \u201csome known characteristics of the population\u201d (Gideon, 2012, p.\u00a032).\n\nWhile conducting an observation as part of our sampling you might have fallen under the\u00a0'''Observer Bias:''' you (dis)favor one or more groups because of one reason or another, thereby influencing your sample and thus your data subconsciously. Then of course, there are '''Bribery, Favoritism, or Lobbying,''' which may influence your research design and your sampling strategy if you engage with them. We hope that it is more relevant for you to be aware of these when reading existing literature, and to question who wrote something under the influence of whom, and why. Lastly, there are systemic problems, which we can all only hope to counteract as much as possible, but which are deeply entrenched into our societal and academic system. These are '''Racism, Sexism, Classism, Lookism,''' and any bias that refers to us (sub)consciously assuming positive or negative traits, or just any preconceived ideas, about specific groups of people. Humans have the unfortunate tendency to create groups as constructs, and our worldview - among many other things - creates a bias in this creation of constructs. This can limit the diversity of our sample, even if we attempt stratified sampling. It is not always easy to detect these biases in your own work, but we urge you to pay attention to them, and consider it as a responsibility as citizens to educate ourselves about these biases, and to reflect on our priviliges. They will also resurface in the interpretation of your data, and you should consider their influence when reading publications from other researchers.\n\nLastly, we should mention the '''Dunning Kruger Effect'''. You might have heard of it - it refers to individuals that overestimate their competence and abilities, and who think that they are the best at something, when they are clearly not. Make no mistake - we are not referring to Interviewees here. Admittedly, there will be Interviewees who claim things that are not supported by any other data, and you should also be aware of this, and use a second source of information where possible. More importantly however, we are talking about you as the researcher. Do not overestimate the strength of your theoretical foundations, hypotheses or sample design. Question the validity and representativity of your sample, and be humble. Otherwise, you might confidently create results that do not bear scrutiny, and it will not make you look good.\n\n\n== Biases in Interview conduction and transcription ==\nSo you are in the Interview situation. You limited the bias in your research design and sample, and you have your survey handed out, or your Interview situation set up. What can go wrong?\n\nFirst, it is important to acknowledge a potential '''Social Desirability Bias.''' This refers to the \u201ctendency among respondents to \u2018spin the truth\u2019 in order to portray themselves in a socially desirable manner\u201d (Bhattacherjee, 2012, p.\u00a081) and has a negative impact on the validity of results. This kind of bias can be better managed in an interview survey than in a questionnaire survey. (Bhattacherjee, 2012) Interviewees may tend to answer in a supposedly socially acceptable way, especially when the research revolves around taboo topics, disputed issues, or any other socially sensitive problems. Akin to this is the '''Observer-Expenctancy Bias''', which is the subconscious influence a researcher's expectations impose on the research. Yes, this is related to the Observer Bias mentioned above. In the Interview situation, it refers to the way that you as the researcher - and Interviewer, or Survey creator - may impact the Interview itself, e.g. by phrasing questions or reacting to certain responses in a specific way. We can also mention '''Framing''' at this point, which is not always a bias in itself, but simply revolves around how questions, data, or topics, are presented by researchers. Framing a specific phenomenon in a negative way might prompt Interviewees to respond rather negatively to it, even if this was not their initial opinion. Observer-Expectancy Bias may also emerge purely from the fact that there is someone listening to the Interviewee. As a result, (s)he may answer differently from what (s)he really thinks, e.g. to impress you, or because (s)he thinks you need specific answers for your research. (S)he might also just not like you, or mistrust you, and respond accordingly. It is not easy to always prevent this. Try to be unbiased with your question design, neutral in your interviewing demeanor, and motivate the interviewee to be as honest as possible. This applies to all kinds of Interview situations.\n\nA challenge one needs to be aware of when conducting and analyzing Focus Groups is the censorship of certain \u2013 e.g. minority or marginalized \u2013 viewpoints, which can arise from the group composition. As Parker and Tritter (2006, p. 31) note: \u201cAt the collective level, what often emerges from a focus group discussion is a number of positions or views that capture the majority of the participants\u2019 standpoints. Focus group discussions rarely generate consensus but they do tend to create a number of views which different proportions of the group support.\" Further, considering the effect that group dynamics have on the viewpoints expressed by the participants is important, as the same people might answer differently in an individual interview. Depending on the focus of the study, either a Focus Group, an individual interview or a combination of both might be appropriate (Kitzinger 1994).\n\nWhen survey participants do not completely and accurately remember events, personal motivations, or behaviors from the past, this is referred to as '''recall bias'''. Their difficulties to recall information that is queried can stem from their \u201cmotivation, memory, and ability to respond\u201d (Bhattacherjee, 2012, p.\u00a082).\n\nThe '''common method bias''' relates to the fallacy that there is a certain \u201ccovariance shared between independent and dependent variables that are measured at the same point in time [\u2026] using the same instrument\u201d (Bhattacherjee, 2012, p.\u00a082). Statistical tests can be used to identify this type of bias. (Bhattacherjee, 2012)\n\nLastly, there is a group of biases revolving around how you perceive your Interviewees. This may influence how you conduct your Interview, and how your transcribe recordings, even before analyzing them further. '''Attribution Bias'''* is about systematic errors based on flawed perception of others' or one's own behavior, for example misinterpreting an individual's behavior. We have the '''Halo / Horn Effect''', which means that an observer's overall impression of an entity influences their feelings about specifics of that entity's properties. If you like an Interviewee, you may interpret their responses differently than if you didn't like them as much, and will ask different kinds of questions therefrom. There are '''Cultural Biases,''' which makes you interpret and judge behavior and phenomena by standards inherent to your own culture. And again, we have '''Sexism, Lookism, Racism, and Classism''', which may lead you to interpreting specific responses in a misguided way due to preconceived ideas about how specific groups of people live or see the world. These influences mostly apply to qualitative face-to-face situations, so mostly to all forms of Open Interviews and Focus Groups.\n\n'''There are certainly more aspects, as this is a complex and emerging topic.''' Not all biases are easy to detect, and it is not always easy to avert them. A first important step is to acknowledge how you and your cultural, institutional, or societal background may influence how you set up and conduct your research. This will help you limit their influence, and there are some additional tips for how to conduct the different types of Interviews, which are mentioned in the respective entries. In your critical reflection of your methodology - which you should always do - you should develop and access knowledge about potential biases, and how much these biases may influence or shape your research. This makes it transparent for readers, helps them evaluate your results, and improves the overall discourse around biases.\n\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|authors]] of this entry are Christopher Franz and Fine B\u00f6ttner."
                    },
                    "sha1": "qjm2z5b3gp1c8gqvd12ejmm055ll9ni"
                }
            },
            {
                "title": "Bias in statistics",
                "ns": "0",
                "id": "440",
                "revision": {
                    "id": "5925",
                    "parentid": "5820",
                    "timestamp": "2021-06-30T15:12:27Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Bias in analyzing data */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19976",
                        "#text": "'''Note:''' This entry revolves around Bias in statistics. For more general thoughts on Bias, please refer to the entry on [[Bias and Critical Thinking]].\n__TOC__\n\n==Defining bias==\n[[File:Cognitive Bias 2.png|450px|thumb|right|'''A world of biases.''' For an amazing overview and the whole graphic in detail, go to '''[https://upload.wikimedia.org/wikipedia/commons/6/65/Cognitive_bias_codex_en.svg Wikipedia]''']]\nA bias is \u201cthe action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment\u201d. In other words, biases cloud our judgment and often action in the sense that we act wrong. [https://www.psychologytoday.com/us/basics/bias We are all biased], because we are individuals with individual experience, and are unconnected from other individuals and/or groups, or at least think we are unconnected.\n\n== Consequences of bias ==\n[[File:Francis Bacon Idols of the Mind.jpg|thumb|left|'''[https://sustainabilitymethods.org/index.php/Why_statistics_matters#The_scientific_method Francis Bacon]''' was also aware of biases, categorised them as the four '''[https://fs.blog/2016/05/francis-bacon-four-idols-mind/ Idols of the Mind]''' (idola mentis).]]\nIf our actions are biased, mistakes happen and the world becomes more unfair or less objective. We are all biased. Therefore, we need to learn of [https://www.youtube.com/watch?v=Rf-fIpB4D50 consciously reflect our biases] if at all possible. Biases are abundantly investigated and discussed both in science and society. However, we  only start to understand the complex mechanism behind the diverse sets of biases, and how to cope with the different biases. In [https://statanalytica.com/blog/bias-in-statistics/ statistics], there are some biases that are more important than others. In addition, statistics are a method that is often used to quantify many biases. Statistics are thus prone to bias, but at the same time can be helpful to investigate biases. This is rooted in the fact that any statistics can only be as good as the person that applies it. If you have a strong bias in your worldview, statistics may amplify this, but statistics can also help you to understand some biases better. \nThe number of biases becomes almost uncountable, highlighting an increasing recognition of where we are biased and how this can be potentially explained. Let us look at the different steps that we have in a study that builds on the use of statistics to focus on the biases that are relevant here. Let us start with an overview on biases.\n\n==Bias in statistical sampling==\nThere are a lot of biases. Within statistics '''sample bias''' and '''[https://data36.com/statistical-bias-types-explained/ selection bias]''' are among the most common problems that occur. Starting with the sampling of data we might have a distorted view of reality through selection bias: we might only sample data because we are aware of it, thereby ignoring other data points that lack our recognition. In other words, we are anchoring our very design and sampling already in our previous knowledge. Another prominent bias features the diversity of groups. We tend to create groups as constructs, and our worldview - among many other things - creates a bias in this creation of constructs. One bias that I just have to mention because I think it's so fantastic is the '''[https://www.britannica.com/science/Dunning-Kruger-effect Dunning Kruger effect]'''. This bias highlights the tendency of some unskilled individuals to overestimate their competence and abilities. Dunning Kruger folks tend to think they are the best when they are actually the worst. Why do I highlight this while talking about statistics? I witnessed quite some people that already destroyed their dataset with their initial sample design, however they consider themselves among the greatest statisticians of all time and advertised their design as the crowning achievement of our civilisation. While conducting an observation as part of our sampling we might have fallen under the '''observer bias''', thereby influencing our data subconsciously for instance by favoring one group in our sampling. We might have already observed a misrepresentation of the total sample through a '''selection bias''', where we favored certain groups in our selection of our sample, and not by how we observed them. Even [[Glossary|hypothesis]] building might be influenced by a '''[https://psychology.wikia.org/wiki/Congruence_bias congruence bias]''', if researchers only test for their hypothesis directly but not for possible alternative hypotheses. To this end you can also add a '''confirmation bias''' for good measure, where researchers only confirm their expectations again and again and again by building on the same statistical design and analysis.\n\n== Bias in analyzing data ==\n[[Glossary|Scientist]]s are notoriously prone to another bias, the '''[https://www.investopedia.com/terms/h/hindsight-bias.asp hindsight bias]''', which highlights the omnipotence of scientists in general, who are quite sure of themselves when it comes to analyzing their data.  Furthermore the '''[https://www.hbs.edu/faculty/Pages/item.aspx?num=41121 IKEA effect]''' might underline how brilliant our analysis is, since we designed the whole study by ourselves. While we conduct an experiment we might have an '''observer expectancy effect''' thereby influencing our study by manipulating the experiment to generate the effects we were expecting after all. The analysis of data is often much like an '''automation bias'''. Most researchers rely on the same statistics again and again. But not to worry: we might have had a selection bias both in our sampling as well as our selective analysis of the data after all, which may be corrected our results without us knowing that thanks to the '''[https://en.wikipedia.org/wiki/Positivity_effect positivity effect]'''. Through framing we might overestimate single samples within our dataset, and often our initial design as well as the final interpretation of our analysis can be explained by such framing, where we rely on single anecdotes or unrepresentative parts of our data.\n\n==Interpretation bias==\n[[File:Semmelweis.jpg|thumb|right|Ignaz Semmelweis, a tragic hero]]\n[https://www.psychologytoday.com/us/blog/science-choice/201504/what-is-confirmation-bias Confirmation bias] is definitely the first bias that comes to mind.\nBut if our results are radical and new they might be ignored or rejected, due to the '''\"[https://en.wikipedia.org/wiki/Ignaz_Semmelweis Semmelweis reflex]\"''', which states that new discoveries are always wrong. Based on our results, we may make a '''forecast bias''', for instance if we use our results to predict beyond the statistical power of our data.\nScientist often conduct a '''self-serving bias''' by interpreting their analysis in a positivistic sense, thereby arriving at a result that supports their grand theory.\nAnd now we come to the [https://amyopp.com/wp-content/uploads/2013/07/Psychology_of_Bias_May_2012.pdf really relevant biases], namely culturalism, sexism, lookism, racism and \u2013 do I have to name more? When interpreting our results we do this through our subjective lense, and our personal identity is a puzzle of our upbringing, our heritage, the group(s) we consider we belong to, our social and cultural norms and many many more factors that influence our interpretation of our data and analysis.\n\n==A world beyond Bias?==\nSo now that we heard some examples about biases, what does this all mean? First of all there is ample research about biases. There is a huge knowledge database from the scientific community that can help us understand where a bias might occur and [https://www.psychologytoday.com/us/blog/in-practice/201508/6-ways-overcome-your-biases-good how it affects us]. Psychology, sociology and cultural studies are to name when it comes to the systematic investigation of biases in people. But there is more. A new forefront of researchers that try to investigate novel, non-violent and reflexive settings where we can try to to minimize our biases. New group interaction formats emerge, and research settings are slowly changing to a bold attempt of a research community that is more aware of their biases. Often these movements are at the outer fringe margins of science, and mix up with new age flavors and self-help driven improvers. Still, it is quite notable that such changes are slowly starting to emerge. Never before was more knowledge available to more people at greater speed. We might want to take it slow these days and focus on what connects us instead of becoming all accelerated but widely biased individuals. \n\nFurthermore, it can of course be interesting to investigate a bias as researchers. Biases can help us as they open a world of normativity in our brains. It is quite fascinating to build on theory e.g. from psychology and focus on one specific bias in order to understand how we can tackle these issues. There are other systematic approaches we can deal with biases. Let us break it down into several steps. First of all there are biases that tinker with our worldview. These biases are relevant as science can build on theories in order to derive hypotheses or generate specific research questions. Therefore it is necessary as scientists to be aware of the fact that we have a limited worldview and that for cultural gender or societal reasons among many other factors our view of the world is biased. This is in itself not problematic as long as we are aware of the fact our scientific knowledge production process is limited to our hemisphere, our culture or specific aspects, like our scientific discipline. Bias can help us to understand our limitations and errors in judgment as scientists better. We may never be best, but we may thrive for it.\n\n==Bias in scientific studies==\nReporting biases in studies that utilise statistics can be roughly sorted into two groups: Biases that are at least indirectly related to statistics, and biases that are widely unrelated to statistics as these are correctly utilised in the study. \nThe first groups is the one we focus on in this text, and the second group is what we basically need to focus on and become hopefully more conscious about during the rest of our life.\n\n[[File:Bildschirmfoto 2020-06-08 um 17.26.32.png|450px|thumb|Gerrymandering is a practice from US politics and a common example for a geographical bias or segregation.]]\nAs part of the first group we can consider that whenever we report everything we made in the study in a transparent way, then even if our result are influenced by some sort of bias, then at least our results should be reproducible or the very least, other researchers may be able to understand that we conducted our study in a way that inferred a bias. This happens all the time, and it is often a matter of experience whether we are aware of a bias. Still, some simple basics can also be taught regarding statistics that may help us to report information to understand a bias, and then there are even some ways to minimise a bias. \n\n[https://catalogofbias.org/biases/wrong-sample-size-bias/ Sample size] as part of the design, as well as sample selection are the first steps to minimise sampling bias. [https://www.khanacademy.org/math/ap-statistics/gathering-data-ap/sampling-methods/v/techniques-for-random-sampling-and-avoiding-bias Stratified designs] may ensure a more representative sampling, as we often tend to make deliberate samples that inflict a certain bias. Being very systematic in your sampling may thus help to tame a sampling bias. Another example of a bias associated to sampling is a [https://diverseeducation.com/article/126527/ geographical bias]. Things that are close to each other are more similar than things that are distant from each other. The same can be true for people regarding some of their characteristics. Segregation is a complex phenomenon that would lead to a biased sample if we want to interview people in a city and then only interview participants from one neighbourhood. [https://www.britannica.com/topic/gerrymandering Gerrymandering] is an example of a utilisation of this knowledge, often to influence election results or conduct other feeble minded [https://www.wnycstudios.org/podcasts/radiolabmoreperfect/episodes/whos-gerry-and-why-he-so-bad-drawing-maps manipulation of administrative borders]. Hence before we sample we need to ask ourselves how we might conduct a sample bias.\n \n[[File:Bildschirmfoto 2020-06-08 um 17.29.33.png|450px|thumb|Open source is used frequently recently but one should also be aware that just because the data is accessible for free it is not free of bias or mistakes.]]\nThe second bias in statistics is the analysis bias. Here, we should be aware that it can be beneficial to contact a statistician in order to inquiry which model would be best for our data. We need to apply the most parsimonious model, yet also should report all results in a most unbiased and thus most reproducible fashion. Quite often one is able to publish details about the analysis in the online appendix of a paper. While this makes the work often more reproducible, I am sorry to report that in the past it also led to some colleagues utilising the data and analysing it only partly or wrong, hence effectively making a scientific misconduct. While this happens rarely, I highlight it for the simple reason to raise awareness that with all the open source hype we currently face, we did not think this through, I believe. Quite often people use data then without being aware of the context, escalating to wrong analysis and even bad science. Establishing standards for open source is a continuous challenge, yet there are good examples out there to highlight that we are on a good way. We need to find reporting schemes that enable reproducibility, which might then enable us detect bias in the future, when more data, better analysis or new concepts might have become available, thereby evolving past results further. Reporting all necessary information on the result of a software or a statistical test or model is one essential step towards this goal. How good is a model fit? How significant is the reuse? How complex is the model? All these values are inevitable to put the results into a broader context of the statistics canon. \n\nSome disciplines tend to add disclaimers to the interruption of results, and in some fields of science it is considered as good scientific practice to write about the [https://wordvice.com/how-to-present-study-limitations-and-alternatives/ limitations of the study]. While this is in theory a good idea, these fields of science devalue their studies by adding such a section generically. I would argue, that if such a section is added every time, it should be added never, since it is totally generic. Everyone with a firm understanding of statistics as well as theory of science knows that studies help to approximate facts, and future insights might lead to different and more often to more precise results. More data might reveal even different patterns, and everyone experienced in statistics knows how much trust can be put into results that are borderline significant, or that are based in small or unrepresentative samples. This should be highlighted if it is really an issues, which I consider to be good scientific practice. If it is highlighted every time, people will not pay the necessary attention to it in the long run. \n[[File:SchoolOfThought BiasesPoster A3-page-001.jpg|550px|thumb|left|An insightful overview was created by [https://yourbias.is/ yourbias.is]]]\nHence I urge you to be transparent in showing your design. Be experienced in the analysis you choose, or consult experts, and evaluate your results in the context of the statistical validity, sampling and analysis. Hardly ever are disciplinary border more tangible as in the reporting of bias. While some fields such as medicine are following very established and conservative -in a good sense- standards, other disciplines are dogmatic, and even other excel through omission of bias reporting. \nOnly if the mechanical details and recognitions of bias within statistics become reported in a reasonable and established fashion we will be able to contribute more to the more complex bias that we face as well. \nRacial bias, gender bias as well as privilege are constant reminders of a vast challenge we all face, and where we did not start to glimpse the tip of the iceberg. Statistics needs to get a grip on bias across different disciplines, otherwise we infer more variance into science, which diminishes the possibility to start engaging in debates into the more serious social, racial, cultural and other biases that we are facing every day. \n\n\n==External Links==\n====Articles====\n[https://www.psychologytoday.com/us/basics/bias What is bias?]: An introduction\n\n[https://link.springer.com/article/10.1007/s11999-010-1538-7 Bias in clinical studies]\n\n[https://upload.wikimedia.org/wikipedia/commons/6/65/Cognitive_bias_codex_en.svg A World of Biases]: For broad overview\n\n[https://fs.blog/2016/05/francis-bacon-four-idols-mind/ The Idols of the Mind]: Were some sort the first categorisation of biases\n\n[https://statanalytica.com/blog/bias-in-statistics/ Bias in Statistics]: An overview\n\n[https://data36.com/statistical-bias-types-explained/ Selection Bias]: An introduction with many examples\n\n[https://www.britannica.com/science/Dunning-Kruger-effect The Dunning-Kruger Effect]: An explanation\n\n[https://psychology.wikia.org/wiki/Congruence_bias Congruence Bias]: An introduction\n\n[https://www.investopedia.com/terms/h/hindsight-bias.asp Hindsight Bias]: Some helpful explanations from the field of economics\n\n[https://www.hbs.edu/faculty/Pages/item.aspx?num=41121 IKEA effect]: A research paper about the boundary conditions, examined in 3 experiments\n\n[https://en.wikipedia.org/wiki/Positivity_effect The Positivity Effect]: A very short definition\n\n[https://www.psychologytoday.com/us/blog/science-choice/201504/what-is-confirmation-bias Confirmation Bias]: A short but insightful article\n\n[https://amyopp.com/wp-content/uploads/2013/07/Psychology_of_Bias_May_2012.pdf The Psychology of Bias]: A really good paper with many real life examples\n\n[https://en.wikipedia.org/wiki/Ignaz_Semmelweis The Semmelweis reflex]: Why the effect has been named after Semmelweis becomes apparent by looking at his biography\n\n[https://www.fs.usda.gov/rmrs/sites/default/files/Kahneman2009_ConditionsforIntuitiveExpertise_AFailureToDisagree.pdf The Conditions of Intuitive Expertise]: This article of Kahneman and Klein investigates whether expert's intuitive estimations are valuable or prone to biases\n\n[https://www.psychologytoday.com/us/blog/in-practice/201508/6-ways-overcome-your-biases-good Overcoming biases]: Some advice to apply in your daily life\n\n[https://catalogofbias.org/biases/wrong-sample-size-bias/ Wrong Sample Size Bias]: Some words on the importance of sample size\n\n[https://diverseeducation.com/article/126527/ Geographical Bias]: An interesting article about bias and college application\n\n[https://www.britannica.com/topic/gerrymandering Gerrymandering]: An example of segregation from the US\n\n[https://wordvice.com/how-to-present-study-limitations-and-alternatives/ Limitations of the Study]: A guideline\n\n====Videos====\n\n[https://www.youtube.com/watch?v=Rf-fIpB4D50 Sampling Methods and Bias within Surveys]: A very interesting video with many vivid examples\n\n[https://www.youtube.com/watch?v=DWj-VHnQWl8 Semmelweisreflex by Fatoni]: Looking at bias from another perspective\n\n[https://www.khanacademy.org/math/ap-statistics/gathering-data-ap/sampling-methods/v/techniques-for-random-sampling-and-avoiding-bias Stratified Sampling]: Techniques for random sampling and avoiding bias\n----\n[[Category: Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "mrp3mlq88gw2n8eie0a68guliqywg8p"
                }
            },
            {
                "title": "Big problems for later",
                "ns": "0",
                "id": "398",
                "revision": {
                    "id": "6729",
                    "parentid": "6536",
                    "timestamp": "2022-07-01T06:38:38Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "52",
                        "#text": "'''this page intentionally left blank'''"
                    },
                    "sha1": "438w7t3p5ds0joeavwoaybx03dvoald"
                }
            },
            {
                "title": "Blog",
                "ns": "0",
                "id": "921",
                "revision": {
                    "id": "6507",
                    "parentid": "6503",
                    "timestamp": "2022-01-21T07:55:32Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "37165",
                        "#text": "'''On this Blog page, we write short updates on the Wiki and provide brief insights into topics relevant to the Wiki.''' <br>\nIn the future, a dedicated Blog feature will be added. For now, you can open each Blog entry by clicking on \"Expand\" on the right.\n\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Some thoughts on practical ethics #5 - Why optimism?\n|-\n|by Henrik von Wehrden\n\nWhile any \"why\" question can easily be dismissed by a simple \"why not?\", times have been hard for many, and a general and individual pessimism is haunting many people these days. Indeed in my surrounding I recognise in many the focus on the general decreasing state of everything, the pending apocalypse, and the end of the world as we know it. I feel to be in a very privileged position indeed, because I will remain a radical optimist until the end. Here are my arguments.\n\nLet us begin with the arguments against pessimism.<br>\n'''Pessimism has never made anything better.''' While this seems like a very easy line of argumentation, it is actually not. Many people argue that pessimism can make people fearful, and such a nudge will help people to recognise to change their behaviour. There is much research that nudges can actually change the behaviour of people. I would dispute this on two terms. How long does it work? What is the prize of a nudge? If a nudge would be catalytic, then it could turn the behaviour of a person completely around. While this is potentially true, there is less evidence of people changing their behaviour completely and long term because of a nudge. This is to some end empirically unfair, because long term research is hard to get funded. Still, I would dispute that there are many nudges that work on many people long term. On the other hand are the negative aspects of nudges only sparsely explored, and this links to the second point. We could agree to utilise nudges if we think that the end justifies the means. Under this assumption, nudges are not a means to an end, but are a prize we choose to pay to achieve a certain end. My main criticism of this would be that nudges make people less free. Nudges can be interpreted as a way to manipulate people, which will ultimately make them less happy. One could attempt to nudge people, and then afterwards reveal the nudge, yet it would be questionable if people would evaluate the act of nudging to be positive. I believe that many people would indeed evaluate the nudging to be not positive. Even worse, what would happen if people would find out by themselves that they were nudged? This would build a growing mistrust within society, and underlines how problematic it would be to upscale the strategy of nudging. Equally bad experience has been made with advertising, showing how people perceive being manipulated by the media. For this reason, the argument that pessimism helps us to \"wake people up\" is not a convincing argument. Many would argue that we do not need to do that, and that the mere \"facts\" are grim enough. There is however a very important difference between presenting information as we perceive it, and presenting it in a pessimistic narrative. I think this difference is extremely important, and many people should try to restrict themselves to the information how they perceive it, and also engage in exchange with others, which may change our perceptions. We manipulate others through pessimistic views. We can equally manipulate others though optimistic worldviews. This brings us to the next point.\n\nNow let us continue with the arguments for pessimism. If we chose to have a pessimistic worldview, then this would be clearly our normative choice. However many people have a pessimistic worldview which is rooted in their origin story or other emotional experiences. '''While it is understandable to have a grim worldview rooted in your biography, it is still no justification for a pessimistic worldview, but a mere explanation.''' After all, pessimistic worldview influence others, and especially people of power have to consider how they want to influence others. If our convincing of the powerful is rooted in fear, then history will not judge kindly on us, because it equally did not do so before. More importantly, the world will surely not become a better place if people are motivated by their fears. While hence pessimism and negative experiences that are part of our identity are well understandable, it is questionable if we need to pass this on to others. There are many positive examples where such endeavours led to a change for the better, yet there are equally negative examples. I believe that if we would start counting if the optimistic or the pessimistic prevail, it is already a lost cause.\n\nOther arguments for pessimism are of more existential nature. Many people have a pessimistic worldview because of all the \"bad\" in the world. I would again argue that this is an emotional argument, and often a a biographical argument. In addition is it clear that people are not bad, but actions are. '''Hence we may judge actions of people, but I would clearly refrain from judging people.''' The world is not a fairytale, where the morals are clear cut. We should never hold negative experience against anyone, but instead try to help people to overcome their negative experiences if at all possible. It will otherwise be difficult to help people overcome their emotional arguments. Derek Parfit argued that the history of humankind could be like an unhappy childhood, yet the overall life might have been -on the whole- worth it. He continued to argue that we cannot know how future people are going to be. This brings us to the next point- arguments for radical optimism.\n\nRadical optimism is just like any form of pessimism or optimism a normative choice. '''Martha Nussbaum rightly stated that hope is a practice, and a choice.''' The question would be then, if we can make this choice, and if we should. It is clear that this normative choice is an effort, and it is unclear who can actively make this choice, and who may merely not have the capability to make that choice. Having a positive outlook in life is nothing people can always choose, but depends on many diverse factors. People who have a generally negative outlook in life may try to seek professionally trained and educated helpers or programs, if they want to seek help. The world of psychiatry has luckily developed more and diverse approaches over the last decades for people in need, yet there is still much to be learned on how to help people with a generally negative outlook in life. Personally, everybody who struggles has my deepest compassion. This is obviously very important, not only for people who struggle, but also for society as a whole. There are other people who may have the capacity within them to gain a general positive outlook. However, shifting our eduction system further and enabling more parts of the educational aspects of society towards enabling a general positive outlook in people is a key challenge for the 21st century. If we would seize this opportunity, the next question would be, why we should make a choice for optimism? We are all interconnected. If one person has a pessimistic outlook, then it may pass on to other people. This argument may not convince everybody, and some people will not have the capacity to react to it. However, an optimistic outlook may equally translate to others, and may also help to create an open and reflective society that is better enabled to give support to people with a pessimistic worldview. \n\n'''How do we keep learning to translate the everyday challenge into a something that does not lead to despair?'''\nBeing happy is a serendipity. Those who can be optimistic can consider themselves lucky, and have a responsibility towards others. This responsibility is a truly societal responsibility, because only if we can all enable everybody to be at peace may we overcome our deeply rooted problems. Our first and foremost priority will be to learn ways to marginalise minorities less, and help humans less privileged than us. We should never forget that optimism can obviously be also part of a privilege. I was privileged enough to see many different cultures, and remember that I saw optimistic and pessimistic world views in rich and poor. Yet this is easier said then lived, and only when all people can grow up with the capability to thrive in this world, and have their rights guarded will we surely and maybe finally be able to make the case for optimism. I for myself thrive towards this lack of inequality, and we have gotten closer over time. When my grandmother was born more than 100 years ago two thirds of all people lived in poverty. When my mother was born after the second world war it had hardly changed. When I was born in the mid-70s it was less than half of the people living on poverty. Today it is about 12%, depending on the threshold and how it is defined. The end of poverty will not mark the beginning of ultimate optimism, yet it is a path towards that. As long as I live I choose to throw all my might in contributing towards a world what is worthwhile our optimism, and this is what I call radical optimism. I cannot find an argument for me against it.\n|}\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Some thoughts on practical ethics #4 - Cultural personal identity\n|-\n|by Henrik von Wehrden\nIt is well understood that we all are defined by our personal identity. The information that personal identity does not ultimately matter will not change that, and not knowing that our individual personal identity will end seems to be the easiest information we all learn to ignore early on in live. Very small children do not have a real personal identity. It seems as if personal identity is something that we discover, develop and/or learn. At a certain age, personal identity is being probed and framed, which is the age when most little children literally become a character. There are several reasons why this will surely not change even in the distant future, because it is part of our evolutionary development. In other words, personal identity is something we first have not, and then we have it. However, several people realise that their personal identity is indeed fleeting, which is not only the insight of the Buddha, but also Derek Parfit. \n\nThere are other people who could even flee into a world of non-identity. There is a prominent example Martin Pistorius. He was trapped for years in his own body, unable to move a muscle, and alter people in his surrounding that after years in a vegetative state, his consciousness had returned. In order to evade the agony of being trapped in his own body, he vanished into a place where \"nothing existed\". Yet while he described this to be a rather dark place, he was also able to vanish into a world of phantasy. Cultural identity is equally such a place of phantasy, because it is not about who a person is, but about who we are as a united group, interacting with each other. Cultural identity would not make sense if you are alone. Cultural identity can be thus seen as a construct that helps us to belong, and create some sort of unity among a group of people. In the past, this unity was often inherited, yet today in a globalised world, there are many cultural groups that are not inherited. The world grew more diverse, and there is a larger recognition of many different facets of cultural identity. '''Culture is what makes us diverse and enables societies to thrive.''' Yet culture cannot be defined as a homogeneous entity, but instead builds on diversity within nested groups. For example may certain traditional houses be built following a localised culture within the construction, but there are often deviances or diversities. This is why art is so central to our lives, because it is \"\"when our senses are at their fullest\" (Ken Robinson). Equally, in a cultural context, art can allow for a strong emotional unity. Many people find at the end of their own personal identity a great consolation that their culture goes on, and hence their contribution to this very culture will be preserved. \n\nCultural identity is therefore highly relevant, not only because of the emotional gratification to belong, but also because cultural identity can thus help people to make more sense of their personal identity, or the lack thereof, i.e. when we feel united. '''If we had no cultural identity, and because we have no personal identity, we would have practically no identity at all.''' This would be clearly a societal problem, because people are ,as was outlined above, not able to live within parts of their development without any form of identity within the foreseeable future. Identity is an important part of our development stages during adolescents, and without such steps people would be lost and confused at this age, and probably also later.\n\nHowever, we shall not forget that some of the worst atrocities in the history of people can be associated to cultural identity. It is however not the exclusion of people that do not belong to an identity group, that is the actual problem. Instead, it is the actions that may arise out of the exclusion of \"others\" from an identity group that is the true problem. '''Consequently, cultural identity should never enable members of a group to take negative actions against other people.''' This is in itself a very difficult assumption, not only because it would be hard to achieve. More importantly would it potentially elevate cultural dimensions onto the status of religion in a secular state. This is not my intention, yet I believe it is important to raise this issue as it would otherwise allow for critics to make the argument to raise concern. After all, culture is also about believes, and can be about values. Most would agree that culture should also not violate legal boundaries, and this matter is a description of many problems that rose in western democracies as part of the cancel culture and culture wars. Since culture builds on values and is set in the real world, it can also be about rights, and often is also about duties.\n\nStarting with the latter, culture is often conserved by duties. These often follow a certain rhythm or are otherwise embedded into the calendar, or may be an action taken under given circumstances. For instance have many cultures certain actions that are taken in case of a solar eclipse. Such cultural actions often give an interpretation or coping mechanism to reality, and we all know many examples that are celebrations. To this end, culture can clearly give meaning to live. Duties within cultural rhymes or habits are thus often a privilege. \nOne of the most controversial points is the relation between culture and rights. While it should be clear that there needs to be a right for culture, which is the case in many countries, it is often less clear how to deal with cultural actions that violate rights or norms. For instance do many traditional cultures to this day catch whales, despite a global recognition of their protection status. Many controversies arise out of such contradictions between local cultures and laws and norms outside of the respective cultural hemisphere. Here, global responsibility should make the difficult negotiation to balance local culture and global responsibility, which is however in many cases a difficult task. Global initiatives such as the IBPES have highlighted the importance of preserving indigenous cultures. Their preservation could be compared to the cold war, when for instance during the Cuba crisis the world was literally a push of a button away from total annihilation. Equally, many indigenous cultures and first nations are one step away from perishing, and many have already perished. Their fate is not different to our potential fate during the cold war, because what is gone will not come back, and as their culture is smaller in extend, it is even more fragile. More research as well as legal and civil action is needed to preserve these cultures, and due to the dramatic situation it is clear that we need to increase our efforts. This thought cannot be more but a mere starting point.\n\n'''To conclude, cultural identity could be seen as a starting point, and not an end in itself.''' Culture is dynamic, interconnected, and ideally flourishing. While many cultures have emerged over the last centuries, many have also emerged, and care needs to be taken to preserve them. More research and action is necessary to embed diverse cultures into the global community. However, as long as people grow up thriving to explore their own identity, and as well as people find meaning in their diverse cultures, cultural identity can be a beacon to belong.\n|}\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Some thoughts on practical ethics #3 - What is better, what is worse, and does this matter?\n|-\n|by Henrik von Wehrden\n\nOne of the most frequent debates I witness in practical ethics is the question whether we can evaluate something to be better or worse. The people I often talk to are actually not exactly part of the academic community focussing on ethics, yet when I mention that some outcome can be best, thereby following Derek Parfit tripple theory, many of these people are baffled. Personally, I am baffled that they are baffled, and will try to unravel here some thoughts about their beliefs.\n\nUtilitarianism claims to aim at the best (overall) outcome, yet our trouble to evaluate exactly what is better or worse is one of the most central mishaps - to me - in Western thinking. To this end I believe we make two main mistakes, that are strangely intertwined and can best be condensed by two big words: '''Epistemology and deontology.''' We make an epistemological mistake by attempting to evaluate consequences of our actions through observational knowledge. The second fallacy we make is based on the error to ethically evaluate our actions against some higher rules, instead of the consequences of our actions, which is, simply put, a deontological mistake. Both problems riddle much of the debate that we have about better or worse, and have divided many in the western world since centuries. More explanations on both mistakes - the epistemological and the deontological - seem to be appropriate.\n\nDeontology focussed on the evaluations of our actions based on the principles or rules these are based on instead of the consequences of our actions. While Bentham as an early advocate of utilitarianism was surely focussing on consequences instead of mere actions and their underlying principles, this problem has within societal debates hardly been resolved. Many religious groups and cults are obsessed with a rule based world up until today, and the current cancel culture and social media wars are a mirror of a similarly rule-obsessed world. These current critical realities and societal debates clearly show why deontology must fail, because as much as we try to act right, it almost aways seems we all fail. Mother Theresa, Ghandi, Einstein, Curie and many other people are long devalued by rule-based criticism, as are it seems all our current leaders and inspirers. It is fair to conclude that people make mistakes, yet while this should be a trivial insight, we have to acknowledge that we should not judge these mistakes if they were made with the aim and knowledge to result in consequences that would not be judged negatively. In other words, I reject the principle of deontology or a rule based evaluation, because is can do nothing but fail from an epistemological perspective. \n\nTake the example of Robin Hood. Living in Sherwood Forest he took from the rich and gave to the poor. '''Who would judge that through a loss of taxes and stolen goods and values the government of Notthingham suffered severe losses?''' Obviously this would be a very bad interpretation of the story. We have to assume that these were the consequences of his actions, yet his rule to take from the rich and give to the poor is an admirable rule, predating Rawls by several centuries. However, we should not forget that Robin Hood broke many other rules while he acted, as do many protagonists in our favourite stories. The world is simply too messy and diverse to allow for an all rule based all-empirical evaluation. \n\nWhen we now take the extreme opposite view to evaluate only the consequences of our actions, then we have to make one important pretext. We can only judge on the intentions of consequences. This seems to have been overall more acceptable in the East, and grew increasingly less acceptable in the West; this is of course a crude generalisation, yet still one important general difference. Take the example where a group of people need to push a button every few hours to prevent a doomsday machine to explode, a story from the TV series Lost. At some point in the story, one of the protagonists decides that it is all a hoax, and that they should stop pressing the button. thereby discontinuing to follow the rule they were given. Next, the doomsday machine exploded, and the sad protagonist saw his mistake. Despite this bad outcome we can still sympathise with his action, because he was given a rule without and explanation or reason. Imagine if the world would be based on bizarre rules that we would never understand. Surely people would rebel against such rules, and rightly so, because we do not live in the times of the gods of old, who could dictate rules to us down from Mount Olympus. '''Instead, modern societies educate us to challenge rules that we do not understand, and the legal system in many democratic nations tries to negotiate exactly this.''' The ever growing canon of legal decisions hence gives testimony on how rules should be interpreted, and many would argue that most laws of most democratically elected governments are often understandable. Yet such political dimensions cannot ultimately be ethical dimensions, and this is part of our ontological fallacy. Ever since Spinoza, Kant and Hegel moved the western world out of the solemnly religious sphere, rules are not anymore god-given, and hence, were increasingly questioned. This devision led to a severe problem, because the centuries since could not clearly answer the question if there are rules to be followed by all people. \n\nIn other words, we widely lost our ontological roots. Now I am far from making a plea for religion here, but simply want to highlight that we seem to have lost any glimpse of ontological truths that we could all agree upon. However, if we cannot agree on anything, then what matters? Critical realism clearly claims that there might be such ontological truths in the world, yet we may never find these principles. While critical realism is still widely restricted to the social dimensions, we can surely widen it to the world as such, and thus state that there are principles we may never observe, but there can be ontological principles that we may as well unravel. These may not be deontological rules, however. Instead it would be much easier that follow principles that do not violate any rules, instead of having our actions simply follow rules. This underlines a different between principles and rules, which is ultimately a matter of scale. The most capital mistake we do to this end is in my opinion to start with our differences, when we should start what unites us. \n\nWe have to conclude that our observational powers as well as the deviance of the real world from our expectations do not always give us sufficient reason for rules we can agree upon. We therefore need to take the intention of our actions into account, and thereby modify our viewpoint. Instead of a rule based world view or an act consequentialists world view we need to settle on an intention based worldview, where no one objects our intentions. Our intentions may follow certain rules, which in many cases cannot be neglected to give some general guideline. If we thus continue to agree to act based on certain rules, we equally need to teach the capacity or allow within a system to deviate from certain rules. This would allow both a reflexive setting as well as a clear documentation of the intentions of our actions, something that may seem hard to imagine for many today, yet may in the future just become a modus operandi towards transparency and evaluative competency. Naturally, we shall not need to write down everything we do, yet focus on these acts that actually have consequences. While this is hard to anticipate now, and we need to be aware even of very small, accumulated and interacting consequences, we shall for now lump sum this as part of the epistemological mist we will need to clear in the future, but not here.\n\nNow let is take the extreme opposite viewpoint, that is assume that there are indeed rules to be followed, and how to find them. Many disagreement about rules are because of cultural values, experiential values or legal values. However the main disagreement on rules are not because of these different types of values, but instead because of the category of values. Many controversies cannot be resolved of cultural differences, yet it should be clear that if we all would have the same culture, the world would be clearly less diverse. Consequently one of the most rules we need to agree upon is to know, reflect and accept the values of other cultures. The alternative of a cultural homogenisation might be a side effect of globalisation, developments in communication et cetera, but will not be considered further here. Instead it is most relevant to honour cultural values of others, even if these values are alien to some of us. There are also examples of wider accepted rules. Humans have already evolved into proclaiming human rights, and these are a commitment of the global community for united values. From a historical viewpoint, it seems that these rights and conventions only emerged recently, and the vast majority of the legal apparatus still operates on national or local levels. This is however well put into perspective when comparing our current situation with the situation about 100 years ago. Humankind evolved clearly since then, and from a standpoint of human rights surely for the better, which is reason for optimism. Nevertheless, more steps need to be taken to allow for a greater implementation of human rights and other global values.\n\n'''The biggest lack to date has probably been in recognising global inequalities.''' First attempts have been made, notably the global sustainability goals of the taxing of global cooperation that try to evade national tax laws. Yet while many global inequalities decreased, some inequalities such as global income disparity, have increased over the last decades. Pessimists claim we shall never overcome these inequalities, but that these inequalities would even increase. This claim does not only contradict past developments, but is also leading nowhere. If we claim that inequalities increase, what would be the aim of this argument? How would humankind need to devolve to become drastically less equal in terms of material resource distribution? It seems highly unlikely that the necessary totalitarian structures would be established, despite all the rumours conspiracy nutheads try to spread. Even if we would make the argument that such grim developments are already underway, action would need to be taken, since mere discussions have clearly less consequences. The global movements of the last years are to this end among the most hopeful initiatives that emerged, and prove the potential of the global community that can transcend diverse cultural values. \n\nThese global initiatives are thus clearly believing in what Derek Parfit called normative truths. There may be indeed some truths that can unite us, because we are able to not only act reasonable, but also responded to reason, underlining the importance of human interconnectedness. Personally, I prefer to live in a world where agency of people can in crease, and we can not only try to unravel what ought to be true, but even may be able to discover what we \"might be able to make true\" (Parfit). The alternative would be that nothing is true. '''Who ever opts for this scenario may become a prepper for the pending anarchistic apocalypse, where nothing matters.''' I hope you have a happy live. \n|}\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Some thoughts on practical ethics #2 - Who am I?\n|-\n|by Henrik von Wehrden\n\nThe 20th Century brought us a thriving of individuality and freedom to many ends, many of which are good. It seems as if we did not reach the heights of our individual personal identity as I write this in 2021 as of yet, and people thrive to explore their identities and find out who they are. From a philosophical standpoint, all this makes no sense, and this is the case since quiet some time, depending who you ask. It was Derek Parfit contribution through his first book \"Reason and persons\" to conclude that all personal identity is - in a nutshell- constructed, and does not make any sense. \n\nSome people may argue about the continuity of memories, we are basically what we remember. Me, I forget many things, and I am sometimes even glad about it. In addition, memory is indeed very fleeting, and more often than not, our memories are plain wrong. When Bob Dylan wrote the first volume of his autobiography, some people pointed out that they remember some stories quite differently. The Master did not care. All of us are often the same. We think we have a memory, but in a nutshell, we made a memory up, or if you want to blame someone, your brain did that. \n\nOur atoms, molecule and cells are constantly changing. '''Hence we cannot be the physical matter that builds us.''' How would you otherwise make sense of Elvis, who - many want to point out to me - yet I can hear his music and see him dancing on video. The King seems very much alive, as is his music. Bodily continuity ends at some point, and since all matter changes constantly all the time, it is indeed hard to defend personal identity via bodily matters.\n\nAre we our cultures, then? Cultural identity is surely something that we could try to settle on. However this is again hard to fix onto a single person, because the times when cultures resolved around one person have slowly come to and end. In ancient egypt the Pharao had the mighty command to build the Pyramids, and while they still remain, all of Egyptians ancient culture is long gone. \n\nThen there is the soul. May it be the Abrahamic religions, yet also Hinduism and also other religions such as some of First nations, the soul is quite central to many beliefs people held, and some still hold. While this is totally up to them, it will be hard to settle on a proof about the existence of a soul. While hence such religions hold the soul as kind of the ultimate claim for personal identity, only in a believers beliefs can this claim hold any value. For everybody else is may not be convincing. The Buddha had a slightly different approach, because causal links do matter in Buddhism, yet the concept of Anatta -non-self- underlines in some lines of thinking any claim of identity or permanence. \n\nMemories, matter, culture or the soul all can thus hold a key to some people concerning personal identity, yet I conclude for myself that if these are not universal answers to the problem of personal identity, then they hold no universal value at all. Do not misunderstand me. Many people get great help in difficult times by knowing that their culture will go on, that they think they will go to heaven, or they will never be forgotten. While all these can be pathways to diminish the sorrows of life, and especially life's end, I agree with Derek Parfit that these forms of personal identity do not matter.\n\n'''Interconnectedness is instead what really matters.''' We are all connected by our actions, and how these may create meaning for other beings. This is not about memories of our actions, or about some afterlife reward system. Instead it is about the general contribution our actions may have, and the consequences that may arise out of this. In other words, this is absolutely not about us as a person, but about life in total. If one of our actions leads to a better outcome overall, then we all get better. While some may see this as a grand overture to altruism, one might simply ask, what the alternative would be? A dog eat dog world, the good old hedonism, or maybe pessimism may lead to -you guessed it- nothing. Material gain will perish, the memory of you will perish, hell -even your soul may perish. Ok, the last one was a cheap shot. \n\nI always think that the funniest scenario would be if everybody would get what they believed in. Imagine how thrilling the memory of you would be in a million years, given that it would never fade. Consider how you feel in heaven or hell after a million years? I could image it would get boring either way. Only on the continuity of culture I could somewhat settle, because cultures evolve. To me, the best scenario would be that I am gone, but that the bundle -as Hume called it- but the impact I had through relations with other remains. The Greek word \"\"trope\"\" comes to mind, which could be understood as \"change\". If we changed things for the better, then we did good. This would as well be in line with the Buddha. What is however I think best about it, is that it is hard to deny that this would have meaning and truth. In other words, who could counter-argue the suggestion that we should leave the world a better place than as we arrived in it, and that this change was at least partly through our actions. \n\nTo conclude, personal identity was a nice construct that emerged as a way towards a greater freedom in societies that were often less free or not free before. Times are different today, as more and more people perceive the capability to become free. As part of this gained freedom, many explore ways towards our own personal identity. Much meaning was lost through that, and we may sometimes have even lost track of the ultimate goal, that is how we can change the world for the better. This is who I would like to be. \n|}\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Some thoughts on practical ethics #1\n|-\n|by Henrik von Wehrden\n\nI believe that the main challenge of the 21st century will be to align our actions with our ethics on why and how we act. The dawn of humans let us emerge from only being driven the need to survive and being guided by emotions, and evolved into a world of communication, culture and cooperation. Hence the dawn of humankind unraveled a world where we had the capability to act beyond our emotions, and consequently discovered reason, logic and goals transcending our personal surrounding. Human civilisations began to thrive, marking attempts in our history to create diverse cultures, and often also differentiating ourselves from our neighbours. '''It was often through these differences that progress was made, yet one should not oversee the conflicts rooted equally in such territorial identities'''. While such conflicts characterised the last century, these times may soon come to pass. The rate of conflicts and casualties has gradually decreased, and armed conflicts become more and more localised. On the other hand did many global challenges emerge calling for responsibility especially among the wealthy, and unity in diversity among all people on this planet. This would call as well for unity in our ethics, because not being united to this end would be a source for continuous and newly emerging conflicts. After all, it is one of the main sources of current conflicts, beside culture and resources. Only culture shall remain a source to honour our differences, yet our ethics-what some would call our moral compass- shall no longer divide us. <br>\n\nThe thought of harmonised and unanimously accepted ethics has long preoccupied philosophers, yet it was especially during the last decades that this ambitious goal gained momentum. Building on these previous writing, basically no thought here is new, or even original. Instead I use this opportunity to order my thoughts when my thinking indeed needs order as a beacon towards the future. Any lapse in clarity, lack of grace or flaw of structure is thus nothing but a reflection of my failure towards an ordered overview towards a united ethics. Writing these texts -which I plan to do for the foreseeable future- anyway is thus a selfish act, but my need towards more clarity and freedom makes it important at least to me.\n|}\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! We now have a blog!\n|-\n|\nWelcome to the Blog page, where we provide short insights into topics relevant to and updates for the Wiki.\n|}"
                    },
                    "sha1": "2q7185cukbmbpg1o6yjsa8ncarlqdtu"
                }
            },
            {
                "title": "Bootstrap Method",
                "ns": "0",
                "id": "598",
                "revision": {
                    "id": "5956",
                    "parentid": "5955",
                    "timestamp": "2021-06-30T20:33:32Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Bootstrapping for confidence interval (using R) */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20809",
                        "#text": "'''In short:''' The bootstrap method is a resampling technique used to estimate statistics on a population by resampling a dataset with replacement.\n\n== Background ==\nThe bootstrap was introduced by Brad Efron in the late 1970s. It is a computer-intensive method for approximating the sampling distribution of any statistic derived from a random sample.\n\nA fundamental problem in statistics is assessing the variability of an estimate derived from sample data.\n\nTo test a [[Glossary|hypothesis]] researchers need to sample from population. For each sample results can be different, therefore to estimate population properties researchers need to take many samples. This set of possible study results represents the sampling distribution. With it one can assess the variability in the real-sample estimate (e.g., attach a margin of error to it), and rigorously address questions.\n\nThe catch is, that it is costly to repeat studies, and the set of possible estimates is never more than hypothetical. The solution to this dilemma, before the widespread availability of fast computing, was to derive the sampling distribution mathematically. This is easy to do for simple estimates such as the sample proportion, but not so easy for more complicated statistics.\n\nFast computing helps to resolve the problem and the main method is Efron\u2019s bootstrap.\n\nIn practice, the bootstrap is a computerbased technique that uses the core concept of random sampling from a set of numbers and thereby estimates the sampling distribution of virtually any statistic computed from the sample. The only way it differs from the hypothetical resampling is that the repeated samples are not drawn from the population, but rather from the sample itself because the population is not accessible.[1]\n\n==What the method does==\n\nBootstrapping is a type of statistical resampling applied to observed data. This method can be used to determine the sampling distribution of a summary statistic (mean, median, and standard deviation) or relationship (correlation and regression coefficient) when these sampling distributions are extremely difficult to obtain analytically.\n\nThere are two main ways in which the bootstrap technique can be applied. In the \"parametric\" method, a knowledge of the data\u2019s distributional form (e.g. Gaussian and exponential) is required. When it is available, such distributional information contributes to greater precision. In the more common \"nonparametric\" version of bootstrapping, no distributional assumption is needed.\n\nBootstrap methods are useful when inference is to be based on a complex procedure for which theoretical results are unavailable or not useful for the sample sizes met in practice, where a standard model is suspect, but it is unclear with what to replace it, or where a \"quick and dirty\" answer is required. They can also be used to verify the usefulness of standard approximations for parametric models, and to improve them if they seem to give inadequate inferences.\n\n===Bootstrap resampling===\n\nBootstrapping involves drawing a series of random samples from the original sample with replacement, a large number of times. The statistic or relationship of interest is then calculated for each of the bootstrap samples. The resulting distribution of the calculated values then provides an estimate of the sampling distribution of the statistic or relationship of interest.\n\nNotice that the replacement allows the same value to be included multiple times in the same sample. This is why you can create many different samples.\n\nThis is the basic algorithm for resampling with replacement:\n\n#Select an observation from the original sample randomly\n#Add this observation to the new sample\n#Repeat steps 1 and 2 till the new sample size is equal to the original.\n\nThe resulting sample of estimations often leads to a Gaussian distribution. And a confidence interval can be calculated to bound the estimator.\n\nFor getting better results, such as that of mean and standard deviation, it is always better to increase the number of repetitions.\n\n'''Why is the resample of the same size as the original sample?'''\n\nThe variation of the statistic will depend on the size of the sample. If we want to approximate this variation we need to use resamples of the same size.\n\n===Bootstrap principle===\nThe bootstrap setup is as follows:\n\n* original sample ( \ud835\udc651 ,  \ud835\udc652 , . . . , \ud835\udc65\ud835\udc5b ) is drawn from a distribution  \ud835\udc39 .\n* '''\ud835\udc62'''  is a statistic computed from the sample.\n* '''\ud835\udc39\u2217'''  is the empirical distribution of the data (the resampling distribution).\n* \ud835\udc65\u22171 ,  \ud835\udc65\u22172 , . . . , \ud835\udc65\u2217\ud835\udc5b  is a resample of the data of the same size as the original sample.\n* '''\ud835\udc62\u2217'''  is the statistic computed from the resample.\n\nThen the bootstrap principle says that:\n\n# \ud835\udc39\u2217  \u2248  \ud835\udc39 .\n# The variation of  \ud835\udc62  is well-approximated by the variation of  \ud835\udc62\u2217.\n\nOur real interest is in point 2: we can approximate the variation of  \ud835\udc62  by that of  \ud835\udc62\u2217 . We will exploit this to estimate the size of confidence intervals. [4]\n\n===Bootstrapping for hypothesis testing (using R)===\n\nBootstrapping may be used for constructing hypothesis tests. It is an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nR programming language is a strong tool for bootstrap implimintation. There is a special package [https://cran.r-project.org/web/packages/boot/boot.pdf Boot] [5] for bootstrapping. For code transparency, in the examples below we will use base R.\n\nUsing R, we will try to reject the null hypothesis that weight of chicks does not differ for casein and meat meal types of feeding.\n\n<syntaxhighlight lang=\"R\" line>\ndiet_df <- chickwts #calling standard dataset \nsummary(diet_df) #exploring the data set\n\n#Output: \n#   weight             feed   \n#Min.   :108.0   casein   :12  \n#1st Qu.:204.5   horsebean:10  \n#Median :258.0   linseed  :12  \n#Mean   :261.3   meatmeal :11  \n#3rd Qu.:323.5   soybean  :14  \n#Max.   :423.0   sunflower:12  \n\nd <- diet_df[diet_df$feed == 'meatmeal' | diet_df$feed == 'casein',] # choosing \"casein\" and \"meatmeal\" for comparison\n\n</syntaxhighlight>\n\nTo understand if two diets actually influence on weight differently we will calculate difference between mean values.\n\n<syntaxhighlight lang=\"R\" line>\nmean_cas <- mean(d$weight[ d$feed == 'casein'])\nmean_meat <- mean(d$weight[ d$feed == 'meatmeal'])\ntest_stat1 <- abs(mean_cas - mean_meat)\nround(test_stat1, 2)\n# Output:\n# 46.67\n</syntaxhighlight>\n\nFor the second estimate we choose difference between medians.\n<syntaxhighlight lang=\"R\" line>\nmedian_cas <- median(d$weight[ d$feed == 'casein'])\nmedian_meat <- median(d$weight[ d$feed == 'meatmeal'])\ntest_stat2 <- abs(median_cas - median_meat)\nround(test_stat2, 2)\n# Output:\n# 79\n</syntaxhighlight>\n\nFor reference we are going to apply to our dataset three classical approaches to hypothesis testing. To compare two means we use t-test:\n\n<syntaxhighlight lang=\"R\" line>\nt.test(d$weight~d$feed, paired = F, var.eq = F) # H0 - means are equal\n#Output:\n#\tWelch Two Sample t-test\n#\n# data:  d$weight by d$feed\n# t = 1.7288, df = 20.799, p-value = 0.09866\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#   -9.504377 102.852861\n# sample estimates:\n#   mean in group casein mean in group meatmeal \n#               323.5833               276.9091 \n</syntaxhighlight>\n\nFor comparison of medians Wilcoxon rank sum test is suitable:\n\n<syntaxhighlight lang=\"R\" line>\nwilcox.test(d$weight~d$feed, paired = F) # H0 - medians are equal\n#Output:\n#\tWilcoxon rank sum exact test\n#\n# data:  d$weight by d$feed\n# W = 94, p-value = 0.09084\n# alternative hypothesis: true location shift is not equal to 0\n\n</syntaxhighlight>\n\nKolmagorov-Smirnov test is helpful to understand whether two groups of observations belong to the same distribution:\n<syntaxhighlight lang=\"R\" line>\nks.test(d$weight[d$feed == 'casein'], d$weight[d$feed == 'meatmeal'], pair = F) # H0 - the same distribution\n#Output:\n#\tTwo-sample Kolmogorov-Smirnov test\n#\n# data:  d$weight[d$feed == \"casein\"] and d$weight[d$feed == \"meatmeal\"]\n# D = 0.40909, p-value = 0.1957\n# alternative hypothesis: two-sided\n</syntaxhighlight>\n\nNow is the time for bootstrapping. For good performance of resampling the number of samples should be big.\n\n<syntaxhighlight lang=\"R\" line>\nn <- length(d$feed) # the sample size\nvariable <- d$weight # the variable we will resample from\nB <- 10000 # the number of bootstrap samples\n\n\nset.seed(112358) # for reproducibility\nBootstrapSamples <- matrix(sample(variable, size = n*B, replace = TRUE), nrow = n, ncol = B) # generation of samples with replacement\n\n\ndim(BootstrapSamples) #check dimention of the matrix (23 observations and 10000 samples)\n\n#Output: 2310000\n\n\n#Initialize the vector to store test statistics\nBoot_test_stat1 <- rep(0, B)\nBoot_test_stat2 <- rep(0, B)\n\n\n#Run through a loop, each time calculating the bootstrap test statistics \nfor (i in 1:B) {\n    Boot_test_stat1[i] <- abs(mean(BootstrapSamples[1:12,i]) - \n                              mean(BootstrapSamples[13:23,i]))\n    Boot_test_stat2[i] <- abs(median(BootstrapSamples[1:12,i]) - \n                              median(BootstrapSamples[13:23,i]))\n}\n\n\n#Lets remind ourselves with the observed test statistics values\nround(test_stat1, 2)\nround(test_stat2, 2)\n\n#Output: \n# 46.67\n# 79\n</syntaxhighlight>\n\nTo understand if the differences between means and medians of two different groups are significant we need to prove that it's highly unlikely that this result is obtained by chance. Here we need a help of p-value.\n\np-value = the number of bootstrap test statistics that are greater than the observed test statistics/ B(the total number of bootstrap test statistics)\n\nLets find p-value for our hypothesis.\n\n<syntaxhighlight lang=\"R\" line>\np_value_means <- mean(Boot_test_stat1 >= test_stat1)\np_value_means\n\n#Output: \n#0.0922\n</syntaxhighlight>\n\nInterpretation: Out of the 10000 bootstrap test statistics calculated, 922 of them had test statistics greater than the observed one. If there is no difference in the mean weights, we would see a test statistic of 46.67 or more by chance roughly 9% of the time\n\n<syntaxhighlight lang=\"R\" line>\np_value_medians <- mean(Boot_test_stat2 >= test_stat2)\np_value_medians\n\n# Output: \n# 0.069\n</syntaxhighlight>\n\n690 of bootstrap median differences are greater or equal than observed one.\n\n'''Conclusion'''\n[[File:Distribution_of_bootstrap_estimates.png|200px|frameless|right]]\nTherefore, classical and bootsrapping hypothesis tests gave close results, that there is no statistically significant difference in weight of chicks with two diets (casein and meatmeal). However p-values are pretty low even with such little sample size. Even if we failed to reject the null hypothesis, we need to test it again with bigger sample size.\n\n<syntaxhighlight lang=\"R\" line>\nplot(density(Boot_test_stat1), xlab = expression(group('|', bar(Yc) - bar(Ym), '|')), main = 'Distribution of bootstrap estimates')\n</syntaxhighlight>\n\n===Bootstrapping for confidence interval (using R)===\nAn important task of statistics is to establish the degree of [[Glossary|trust]] that can be placed in a result based on a limited sample of data.[4]\n\nWe are going to apply bootstrapping to build the confidence interval. Using R, we will build the confidence interval for the difference between weight of chicks with casein and meatmeal types of feeding. Main estimates will be differences in means and medians.\n\n<syntaxhighlight lang=\"R\" line>\nDiff_In_Means <- mean_cas - mean_meat\nround(Diff_In_Means, 2)\n#Output: 46.67\n\nDiff_In_Medians <- (median_cas - median_meat )  #diff in medians\nround(Diff_In_Medians, 2)\n#Output: 79\n</syntaxhighlight>\n\nNow we are going to generate bootstrap samples separately for casein sample and meatmeal sample.\n\n<syntaxhighlight lang=\"R\" line>\nset.seed(13579)   # set a seed for consistency/reproducability\nn.c <- 12  # the number of observations to sample from casein\nn.m <- 11  # the number of observations to sample from meatmeal\nB <- 100000  # the number of bootstrap samples\n\n\n# now, get those bootstrap samples (without loops!)\n# stick each Boot-sample in a column...\nBoot_casein <- matrix(sample(d$weight[d$feed==\"casein\"], size= B*n.c, \n                              replace=TRUE), ncol=B, nrow=n.c)\nBoot_meatmeal <- matrix(sample(d$weight[d$feed==\"meatmeal\"], size= B*n.m, \n                                replace=TRUE), ncol=B, nrow=n.m)\n</syntaxhighlight>\n\nLet's check the distributions of bootstrap sample means.\n\n[[File:Distributions_of_bootstrap_sample_means.png|200px|frameless|left]]\n<syntaxhighlight lang=\"R\" line>\nBoot_test_mean_1 <- apply(Boot_casein, MARGIN = 2,  mean)\nBoot_test_mean_2 <- apply(Boot_meatmeal, MARGIN = 2,  mean)\n\nb <- min(c(Boot_test_mean_1, Boot_test_mean_2))# - 0.001 # Set the minimum for the breakpoints\ne <- max(c(Boot_test_mean_1, Boot_test_mean_2)) # Set the maximum for the breakpoints\nax <- pretty(b:e, n = 110) # Make a neat vector for the breakpoints\n\nhist1 <- hist(Boot_test_mean_1, breaks = ax, plot = FALSE)\nhist2 <- hist(Boot_test_mean_2, breaks = ax, plot = FALSE)\n\nprint(paste('Mean weight casein', round(mean_cas, 1)))\nprint(paste('Mean weight meatmeal', round(mean_meat, 1)))\n\nplot(hist1, col = 2, xlab = 'Weight, g', main = 'Distributions of bootstrap sample means') # Plot 1st histogram using a transparent color\nplot(hist2, col = 4, add = TRUE) # Add 2nd histogram using different color\nlegend(\"topleft\", c('Casein', 'Meatmeal'), col = c(2, 4), lwd = 10)\n\n# Output:\n# [1] \"Mean weight casein 323.6\"\n# [1] \"Mean weight meatmeal 276.9\"\n</syntaxhighlight>\n<br>\nWe can see that bootstrap sample means are normally distributed. Mean of the bootstrap distribution is equal to the mean of original sample.\n\n<syntaxhighlight lang=\"R\" line>\n# check dimentions of matrices(100000 samples, 12 and 11 observations for casein and meatmeal respectively)\ndim(Boot_casein)\ndim(Boot_meatmeal)\n# Output:\n# 12 * 100000\n# 11 * 100000\n</syntaxhighlight>\n\nNext step is calculating differences in means and medians among bootstrap samples of casein and meatmeal.\n\n<syntaxhighlight lang=\"R\" line>\n# calculate the difference in means for each of the bootsamples\nBoot_Diff_In_Means <- colMeans(Boot_casein) - colMeans(Boot_meatmeal)\n# look at the first 10 differences in means\nround(Boot_Diff_In_Means[1:10], 1)\n# Output:\n# 49.5*  48.6*  32.2*  68.6*  46.2*  65.5*  72.2*  29.9*  10.1*  50.1\n\n\n# calculate the difference in medians for each of the bootsamples\nBoot_Diff_In_Medians <- apply(Boot_casein, MARGIN=2, FUN=median) -\n  apply(Boot_meatmeal, MARGIN=2, FUN=median)\n# and, look at the first 10 diff in medians\nBoot_Diff_In_Medians[1:10]\n\n# Output:\n# 27*  39*  0*  87.5*  62*  96*  67*  42.5*  3*  72\n</syntaxhighlight>\n\nCommon approaches to building a bootstrap confidence interval:\n\n* Percentile method\n* Basic method\n* Normal method\n* Bias-Corrected method.\n\nWe will choose the first one as the most intuitive. Percentile method takes the entire set of bootstrap estimates and in order to form a 95 % confidence interval the 2.5th percentile of all the bootstrap estimates or bootstrap distribution used as the lower bound for the interval and 97.5th percentile - as upper bound.\n\n<syntaxhighlight lang=\"R\" line>\n# first, for the difference in MEANS\nquantile(Boot_Diff_In_Means, prob=0.025)\nquantile(Boot_Diff_In_Means, prob=0.975)\n\n# Output:\n# 2.5%: -3.73503787878789\n# 97.5%: 97.0079545454545\n</syntaxhighlight>\n\nWe are 95 % confident that the mean weight for casein diet is between 3.73 g lower up to 97.01 g higher than for meatmeal diet.\n<syntaxhighlight lang=\"R\" line>\n# and then, the difference in MEDIANS\nquantile(Boot_Diff_In_Medians, prob=0.025)\nquantile(Boot_Diff_In_Medians, prob=0.975)\n# Output:\n# 2.5%: -22\n# 97.5%: 116\n</syntaxhighlight>\n\nWe are 95 % confident that the median weight for casein diet is between 22 g lower up to 116 g higher than for meatmeal diet.\n\n'''Conclusion'''\n\nWe can see that both intervals contain 0. From this we can deduce that differences of the means or the medians are not statistically significant.\n\n==Strengths & Challenges==\n\n===Strengths of bootstrapping===\n* Good for checking parametric assumptions.\n* Avoids the costs of taking new samples (estimate a sampling distribution when only one sample is available).\n* Used when parametric assumptions can not be made or are very complicated.\n* Does not require large sample size.\n* Simple method to estimate parameters and standard errors when adequate statistical theory is unavailable.\n* Useful to check stability of results.\n* Works for different statistics \u2014 for example, a mean, median, standard deviation, proportion, regression coefficients and Kendall\u2019s correlation coefficient.\n* Helps to indirectly assess the properties of the distribution underlying the sample data.\n\n===Challenges===\n* Even if bootstrapping is asymptotically consistent, it does not provide general finite-sample guarantees.\n* The result may depend on the representative sample.\n* The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples).\n* Bootstrapping can be time-consuming.\n* There is a limitation of information that can be obtained through resampling even if number of bootstrap samples is large.\n\n===Normativity===\n\nBootstrap methods offer considerable potential for modelling in complex problems, not least because they enable the choice of estimator to be separated from the assumptions under which its properties are to be assessed.\n\nAlthough the bootstrap is sometimes treated as a replacement for \"traditional statistics\". But the bootstrap rests on \"traditional\" ideas, even if their implementation via simulation is not \"traditional\".\n\nThe computation can not replace thought about central issues such as the structure of a problem, the type of answer required, the sampling design and data quality.\n\nMoreover, as with any simulation experiment, it is essential to monitor the output to ensure that no unanticipated complications have arisen and to check that the results make sense. The aim of computing is insight, not numbers.[2]\n\n==Outlook==\nOver the years bootstrap method has seen a tremendous improvement in its accuracy level. Specifically improved computational powers have allowed for larger possible sample sizes used for estimation. As bootstrapping allows to have much better results with less amount of data, the interest for this method rises substantially in research field. Furthermore, bootstrapping is applied in machine learning to assess and improve models. In the age of computers and data driven solutions bootstrapping has good perspectives for spreading and development.\n\n==Key Publications==\n* Efron, B. (1982) The Jackknife, the Bootstrap, and Other Resampling Plans. Philadelphia: Society for Industrial and Applied Mathematics.\n* Efron, B. and Tibshirani, R. J. (1993) An Introduction to the Bootstrap. New York: Chapman and Hall.\n==References==\n# Dennis Boos and Leonard Stefanski. Significance Magazine, December 2010. Efron's Bootstrap.\n# A. C. Davison and Diego Kuonen. Statistical Computing & Statistical Graphics Newsletter. Vol.13 No.1. An Introduction to the Bootstrap with Applications in R\n# Michael Wood. Significance, December 2004. Statistical inference using bootstrap confidence interval.\n# Jeremy Orloff and Jonathan Bloom. Bootstrap confidence intervals. Class 24, 18.05.\n# CRAN documentation. Package \"bootstrap\", June 17, 2019.\n\n==Further Information==\n\nYoutube video. Statquest. [https://www.youtube.com/watch?v=isEcgoCmlO0&ab_channel=StatQuestwithJoshStarmer Bootstrapping main ideas]\n\nYoutube video. MarinStatsLectures. [https://www.youtube.com/watch?v=Om5TMGj9td4&ab_channel=MarinStatsLectures-RProgramming%26Statistics Bootstrap Confidence Interval with R]\n\nYoutube video. [https://www.youtube.com/watch?v=9STZ7MxkNVg&ab_channel=MarinStatsLectures-RProgramming%26Statistics MarinStatsLectures. Bootstrap Hypothesis Testing in Statistics with Example]\n\nArticle. [https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/#:~:text=The%20bootstrap%20method%20is%20a,the%20mean%20or%20standard%20deviation.&text=That%20when%20using%20the%20bootstrap,and%20the%20number%20of%20repeats. A Gentle Introduction to the Bootstrap Method]\n\nArticle. [https://statisticsbyjim.com/hypothesis-testing/bootstrapping/ Jim Frost. Introduction to Bootstrapping in Statistics with an Example]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Andrei Perov."
                    },
                    "sha1": "bgzqncnpoj10tivmy0x9qcaz0bmrqr8"
                }
            },
            {
                "title": "Bubble Plots",
                "ns": "0",
                "id": "933",
                "revision": {
                    "id": "6562",
                    "parentid": "6561",
                    "timestamp": "2022-03-20T20:21:45Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7989",
                        "#text": "'''Note:''' This entry revolves specifically around Bubble plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA Bubble plot is a graphical representation of multivariate data table. One can think of it as an XY scatter plot with two additional variables. X and Y variables are numeric, and two additional variables, either continuous or categorical, can be represented by the bubble colour and bubble size. \n\n==Overview==\nThis wiki entry will elaborate what a bubble plot is, how to implement such a plot and how to customize your own bubble plot.\n\nA bubble plot is able to present up to four variables, without actually being a four dimensional plot. We can first start with trying to plot three variables. For that the input data should be a triplet (Note: the data should be quantitative and non-categorical). One variable is represented by the x-axis, another one by the y-axis and the third by the size of the data points. Therefore the data points differ in size which makes the plot look like an accumulation of bubbles. We will then incorporate the fourth variable as a color later in our example.\n\nA lot of bubble plot examples can be seen online in the Gapminder data tool [https://www.gapminder.org/tools/#$chart-type=bubbles&url=v1 Bubbles]. Check it out, it\u2019s worth it!\n\n==Preliminaries==\nWe will use <syntaxhighlight lang=\"R\" inline>ggplot</syntaxhighlight> to create the bubble plot. In order to use <syntaxhighlight lang=\"R\" inline>ggplot</syntaxhighlight> you need to install the packages <syntaxhighlight lang=\"R\" inline>gapminder</syntaxhighlight> and <syntaxhighlight lang=\"R\" inline>tidyverse</syntaxhighlight> (use the command <syntaxhighlight lang=\"R\" inline>install.packages(\u201cname\u201d)</syntaxhighlight>). Depending on your computer system you may also need to install other dependencies. More information on how to install packages can be found [https://www.datacamp.com/community/tutorials/r-packages-guide?utm_source=adwords_ppc&utm_medium=cpc&utm_campaignid=12492439676&utm_adgroupid=122563405841&utm_device=c&utm_keyword=load%20library%20r&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=504100768473&utm_targetid=kwd-385495694086&utm_loc_interest_ms=&utm_loc_physical_ms=9044405&gclid=Cj0KCQiAkZKNBhDiARIsAPsk0WhoeIpvXklFGyd0Slr1grk4JcrYaQGj6EtVfPVf68Mo1RIL_kbRJQEaApUZEALw_wcB here]. After installing the packages, we need to activate their libraries:\n\n<syntaxhighlight lang=\"R\" line>\nlibrary(tidyverse)\nlibrary(gapminder)\n</syntaxhighlight>\n[[File:mtcarshead.png|500px|thumb|right|Fig.1: First six entries in the mtcars dataset]]\nIf everything is set up you can choose and take a look at your data. I decided to use the <syntaxhighlight lang=\"R\" inline>mtcars</syntaxhighlight> data set, because it is well-known and common to use in examples.\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nhead(mtcars)\n</syntaxhighlight>\n\nFor further information on the variables and what this data set is about run the command <syntaxhighlight lang=\"R\" inline>?mtcars</syntaxhighlight>.\n\n==Code==\nAfter installing and including the <syntaxhighlight lang=\"R\" inline>gapminder</syntaxhighlight> and <syntaxhighlight lang=\"R\" inline>tidyverse</syntaxhighlight> packages we are ready to create the plot. I decided to set the theme via <syntaxhighlight lang=\"R\" inline>theme_set()</syntaxhighlight> of the plot here. The theme is the overall design and background of your plot. An overview of <syntaxhighlight lang=\"R\" inline>ggplot</syntaxhighlight> themes can be found [https://ggplot2.tidyverse.org/reference/ggtheme.html here].\n\n<syntaxhighlight lang=\"R\" line>\n#theme\ntheme_set(theme_linedraw())\n</syntaxhighlight>\n\nA bubble plot can take three variables as the code below shows: two for both of the axis (x- and y-axis) and one for the bubble-size. In order to map the variables to the axis and the size the function <syntaxhighlight lang=\"R\" inline>aes()</syntaxhighlight> is used. The function <syntaxhighlight lang=\"R\" inline>geom_point()</syntaxhighlight> defines the overall type (\u201cpoints\u201d) of the plot. If there is no input to that function (leaving the brackets empty) the plot would just be a scatter plot. The command <syntaxhighlight lang=\"R\" inline>aes(size = variable3)</syntaxhighlight> maps the third variable as the size of points within the function <syntaxhighlight lang=\"R\" inline>geom_point()</syntaxhighlight>. That is all the magic!\n\n[[File:bubbleplotmtcars.png|450px|thumb|right|Fig. 2: Cars' fuel consumption (miles/gallon), their weight (in 1000 lbs) and horsepower visualized with a bubble plot. Dataset: mtcars.]]\n\n<syntaxhighlight lang=\"R\" line>\nbubbleplot <- ggplot(data = mtcars, aes(x = mpg, y = wt)) + #variable 1 and variable 2\n                                                            #(x,y-axis)\n  geom_point(aes(size = hp)) #variable 3 (point size)\n\n#Fig.2\n#print the plot\nprint(bubbleplot)\n</syntaxhighlight>\n\nOf course this plot is missing proper labels. So far <syntaxhighlight lang=\"R\" inline>ggplot</syntaxhighlight> used the column names of the data set to name the axis and the size. The function <syntaxhighlight lang=\"R\" inline>labs()</syntaxhighlight> allows us to customize and add the labels and a title:\n\n[[File:bubplotmtcars.png|450px|thumb|right|Fig. 3: mtcars bubble plot visualization with labels.]]\n\n<syntaxhighlight lang=\"R\" line>\nlabelled_bubbleplot <- ggplot(data = mtcars, aes(x = mpg, y = wt)) +\n\n   geom_point(aes(size = hp)) +\n\n   labs(title = \"Labelled Bubbleplot\", #add labels and title\n        x = \"Fuel economy in mpg\",\n        y = \"Weight in 1000 lbs\",\n        size = \"Power in hp\")\n\n#Fig.3\nprint(labelled_bubbleplot)\n</syntaxhighlight>\n\nNow anyone who does not know the data set can interpret and understand what we plotted.\n\n==Grouping by Colors==\n[[File:colbubpl.png|450px|thumb|right|Fig.4: Cars' fuel consumption (miles/gallon), their weight (in 1000 lbs), horsepower and number of forward gears visualized with a bubble plot. Dataset: mtcars.]]\n[[File:clrplt.png|450px|thumb|right|Fig.5: Overview of all color palettes in the package RColorBrewer]]\nIf you took a look at the Gapminder data tool [https://www.gapminder.org/tools/#$chart-type=bubbles&url=v1 Bubbles], you might have noticed that the bubbles are colored to indicate the world regions. This type of color grouping can be easily implemented within our plot. We just add within the function <syntaxhighlight lang=\"R\" inline>geom_point()</syntaxhighlight> the type <syntaxhighlight lang=\"R\" inline>color</syntaxhighlight> in the function <syntaxhighlight lang=\"R\" inline>aes()</syntaxhighlight>. By this we map another variable, in this case the number of forward gears, to the type <syntaxhighlight lang=\"R\" inline>color</syntaxhighlight>. And last but not least, we can change the color palette with the function <syntaxhighlight lang=\"R\" inline>scale_color_brewer()</syntaxhighlight>, if we do not like the default color palette.\n\n<syntaxhighlight lang=\"R\" line>\ncustomised_bubbleplot <- ggplot(data = mtcars, aes(x = mpg, y = wt)) +\n\n    geom_point(aes(color = as.factor(gear), size = hp)) + #add colors to the bubbles\n                                                          #with respect to gear\n    labs(title = \"Customised Bubbleplot\",\n         x = \"Fuel economy in mpg\",\n         y = \"Weight in 1000 lbs\",\n         size = \"Power in hp\",\n         color = \"Number of forward gears\") +\n\n    scale_color_brewer(palette = \"Set1\") #changing the color palette\n\n#Fig.4\nprint(customised_bubbleplot)\n</syntaxhighlight>\n\nAn overview over all color palettes in the package <syntaxhighlight lang=\"R\" inline>RColorBrewer</syntaxhighlight> can be displayed by running the following code:\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.5\nlibrary(\"RColorBrewer\")\ndisplay.brewer.all(colorblindFriendly = TRUE)\n</syntaxhighlight>\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Kira Herff."
                    },
                    "sha1": "qyr0i59sboaekr3io240fphxjalkfwl"
                }
            },
            {
                "title": "Case studies and Natural experiments",
                "ns": "0",
                "id": "360",
                "revision": {
                    "id": "5947",
                    "parentid": "5892",
                    "timestamp": "2021-06-30T19:28:11Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Experiments in sustainability science */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20926",
                        "#text": "'''Note:''' This entry is a brief introduction to natural experiments. For more details on other types of experiments, please refer to the entries on [[Experiments]], [[Experiments and Hypothesis Testing]] as well as [[Field experiments]].\n__TOC__\n==Natural experiments==\n[[File:Easter Island.jpg|thumb|right|Although this seems to be a little contradictory here, the impossibility of replication is a problem in the case of the Easter Island.]]\nOut of a diverse rooting in discussions about [[Glossary|complexity]], [https://learningforsustainability.net/systems-thinking/ system thinking] and the need to understand specific contexts more deeply, the classic experimental setting did at some point become more and more challenged. What emerged out of the development of [https://sustainabilitymethods.org/index.php/Interactions#The_field_experiment field experiments] was an almost exact opposite trend considering the reduction of complexity. What do we learn from singular cases? How do we deal with cases that are of pronounced importance, yet cannot be replicated? And what can be inferred from the design of such case studies? A famous example from ethnographic studies is the [http://www.eisp.org/818/ Easter Island]. Why did the people there channel much of their resources into building gigantic statues, thereby bringing their society to the brink of collapse? While this is a surely intriguing question, there are no replicates of the Easter Islands. This is at a first glance a very specific and singular problem, yet it is often considered to be an important example on how unsustainable behaviour led to a collapse of a while civilisation. Such settings are referred to as [https://www.britannica.com/science/natural-experiment Natural Experiments]. From a certain perspective, our whole planet is a Natural Experiment, and it is also from a statistical perspective a problem that we do not have any replicates, besides other ramifications and unclarity that derives such single case studies, which are however often increasingly relevant on a smaller scale as well. With a rise in qualitative methods both in diversity and abundance, and an urge for understanding even [[Glossary|complex systems]] and cases, there is clearly a demand for the integration of knowledge from Natural Experiments. '''From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant, plausible and valid.''' To this end, I proclaim the concept of the niche in order to illustrate and conceptualise how single cases can still contribute to the production and canon of knowledge.\n\nFor example the [https://academic.oup.com/rcfs/article/4/2/155/1555737#113865691 financial crisis from 2007], where many [[Glossary|patterns]] where comparable to previous crisis, but other factors were different. Hence this crisis is comparable to many previous factors and patterns regarding some layers of information, but also novel and not transferable regarding other dynamics. We did however understand based on the single case of this financial crisis that certain constructs in our financial systems are corrupt if not broken. The contribution to develop the financial world further is hence undeniable, even so far that many people agree that the changes that were being made are certainly not enough. \n\n'''Another prominent example of a single case or phenomena is the Covid pandemic that emerges further while I am writing these lines.''' While much was learned from previous pandemics, this pandemic is different, evolves different, and creates different ramifications. The impact of our societies and the opportunity to learn from this pandemic is however undeniable. While classical experiments evolve knowledge like pawns in a chess game, moving forward step by step, a crisis such as the Covid pandemic is more like the horse in a chess game, jumping over larger gaps, being less predictable, and certainly harder to master. The evolution of knowledge in an interconnected world often demands a rather singular approach as a starting point. This is especially important in normative sciences, where for instance in conservation biology many researchers approach solutions through singular case studies. Hence the solution orientated agenda of sustainability science emerged to take this into account, and further.\n\nTo this end, [https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 real world experiments] are the latest development in the diversification of the arena of experiments. These types of experiments are currently widely explored in the literature, yet I do not recognise a coherent understanding of what real-world experiments are to date in the available literature. These experiments can however be seen as a continuation of the trend of natural experiments, where a solution orientated agenda tries to generate one or several interventions, the effects of which are tested often within singular cases, but the evaluation criteria are clear before the study was conducted. Most studies to date have defined this with vigour; nevertheless, the development of real-world experiments is only starting to emerge. For more info, please refer to the entry on [[Living Labs & Real World Laboratories]].\n\n[[File:Lueneburg 2030.jpg|800px|thumb|center|'''A wonderful example for a bunch of real world experiments is the project L\u00fcneburg 2030+.''' This map provides an overview of the different experiments.]]\n\n== Ramifications for statistics ==\n====1. Documentation is key.====\nSingle case studies demand a clear documentation, and by clear I do not mean an exhausting documentation. Where many disciplines relying on single case studies often fail is the right balance between the details and the general information, or in other words, getting the Occam\u2019s razor into the documentation. \nIn [https://www.pnas.org/content/108/49/19449.short sustainability science] we did not learn to date which information is vital, and which one trivial. This is vastly different in medicine, where much of the canon of knowledge has been consolidated into a system that allows for a clear presentation of singular case studies. Take for instance the New England Journal of Medicine, one of the most prestigious scientific [[Glossary|journals]] on this planet. Regularily, this journal publishes short case studies that provide either fundamentally new problems or even solutions that are recognised as important new knowledge in the respective field. Medicine is able to and actually demands such a presentation of knowledge, since much of the previous knowledge is compiled into a system that allows for a clear integration of new knowledge. In sustainability science such an established system is still widely lacking. People talk of complexity and the importance of recognising the individual case, yet I would argue that the world of medicine is equally complex. When people think that this is wrong, I think they are wrong. I consider the complexity of a singular individual and the world as such as being comparable, or at least equally complex. \nSustainability science shall thus realize that making cases comparable is the key to coherent knowledge, and ultimately a more systematic approach to new knowledge. Statistics will certainly not be the only contribution to this end, but it is quite certain that it will contribute to a holistic understanding.\n\n[[File:Chess.jpg|thumb|right]]\n\n[https://www.youtube.com/watch?v=ztc7o0NzFrE Chess games] have certain openings. Most opening are known, and while some are famous, others are more obscure. Every chess player knows however the main 4-5 opening moves, since they determine the continuation of the game. Around the 8-12 moves however, something remarkable happens. We leave the range of known chess games, and enter a game that has probably never been played on this planet. It is a completely new game, and there are billions and billions of possibilities. This all then  boils quite often down to fewer and fewer pieces, and if then two players are almost equally good, we might enter the realms of endgames. Endgames can be calculated, and here people that are good in thinking some moves ahead, have a higher chance of winning. However, in between the opening game and the endgame happens something that never happened before, a new game, which many consider to be complex. Here, a friend of mine taught me that this is indeed not complex, but instead builds on experience. He introduced me to the concept of board awareness. Ten thousand of games led in his head to a level of experience where when he would look at a middle game, his brain would almost instinctively interpret the board into hot zones and cold zones. Building on previous experience he would recognise the constellation and could derive actions and possible moves, and especially recognise which areas are prosperous for development through strategic moves, and which ones are endangered and ought to be defended. Experience builds reduction in complexity.\n\n====2. Look for mechanics you know from statistics====\nStatistics offer a certain perspective on the world. It makes a difference whether I count something or construct it into another [https://sustainabilitymethods.org/index.php/Data_formats data format]. Many patterns follow a [https://sustainabilitymethods.org/index.php/Causality#Is_the_world_linear.3F linear trend], yet some few may not. Variables can be grouped. When looking at a single case we can look beyond the complexities by looking at the case through the lens of knowledge we have from statistics. While many people argue that this creates a flawed picture, it might as well offer a more orderly view on the case. When working on a single case, we can look at cause and effect, we may look for predictive variables, and we may consider interactions between these. Many singular cases are rich and diverse, but also structured. We need to peel our way through the outer layer and understand the inner working of what we are looking at. Statistics is not only a method, but also a specific perspective. If we get versatile in statistics, it might help us to see something that we otherwise cannot see. As one of my first professors at University put it: \u201cIf you know more, you can understand more\u201d. Consider statistics as your perspective on a single case. Practice this perspective, learn to see things through this lens.\n\n====3. Search for causality, yet be suspicious====\nWhat if single cases are not complex but simple? Many single case studies are very simple. And even the more complicated, some may have an inner working or a narrative. When looking at single case studies, we ought to remember that there can be a [https://sustainabilitymethods.org/index.php/Causality causal connection] in the world, so there might be some in our case studies as well. Finding similarities between two things that have an effect on something else, may be worthwhile if you consider attracting a diverse set of costumers. Therefore I suggest that the supposably complexity of single cases should not repel us but instead energise and motivate us to find reason and causality within the case. Of course we may not know if there is such a thing as true causality at work, but based on previous cases and careful observation we may still infer new knowledge. Deciding that a case is complex does not help if we cannot say what is complex about it. To this end, complexity became a buzzword that is often used in a sense of something being not understandable. To me, it sometimes feels as if it is part of the zeitgeist to call something complex. Natural experiments should show us the value of single case studies, the value of understanding, and the value of creating new knowledge. I think it is our choice whether we want to seize this opportunity, or not.\n\n====4. Build on the available literature.====\n[[File:Caspar David Friedrich - Der Wanderer \u00fcber dem Nebelmeer.jpg|thumb|right|Although the \"Wander \u00fcber dem Nebelmeer\" does not literally stand on the shoulders of giants, the painting of Caspar David Friedrich is a good visualisation of the metaphor.]]\nNo case is an island, even not cases on islands. While some cases can be novel in their characteristics - take wicked problems for instance -; every case may contain components that are known from previous cases. While wicked problems are new in the combination of building blocks, the building blocks themselves - i.e. parts of the problem - are maybe not new. The more you stand on the shoulders of giants, the further you can see. We need to realize that science is build on failure and iteration. Only by knowing about previous failures but also successes, and only by committing to the canon of knowledge, can we contribute and create new knowledge.\n\n\n==Natural experiments and statistics - a rough guide==\nSeveral layers of information are to be considered when linking statistics to natural experiments. Contextualising natural experiments can be structured into at least 4 steps.\n\n1) '''How is the case connected and embedded in the global context?'''\nWhat are connections to other systems, where are dependencies, and where are gaps and missing links? A lot of information is available on a global scale, and it is worthwhile embedding a case into the global information available. This may allow us to specify the setting of the case, and to allow inference about how the results can be superimposed onto other systems. We hence acknowledge that this contextualisation is place based, and places have boundaries and borders. While it is way beyond this text to discuss this issue, the question how a place is bound can be a source of great confusion. A good rule of thumb from a statistical standpoint can be the idea that a place does only exists if its within variance, regarding central variables, is lower than the variance between the place and its neighbouring places.\n\n2) '''what are imbalances and dispersions within the case, both spatially and temporally?'''\nIn other words, what creates injustices in a place, both intragenerational but also intergenerational. Understanding the roots of injustices can be both as practical as money flows and resource dispersion, yet also as hard to tame as perceived injustices. Hence understanding the cultural and social context of the case's dynamics is often essential. Here statistics can offer ways for a clear documentation through observation, and system analysis and the like are good examples how the complexity of a case can be understood at least partly. \n\n3) '''How are the livelihoods within the case?'''\nWhile many would connect this to the last point, it may be worthwhile to consider this on its own. Livelihoods are what defines groups of people and their possibility to thrive. While this is equally bound to [[Glossary|culture]] and justice, it repeatedly proves that it is much more than the mere sum of the two. Livelihoods are often characterised by a deeply qualitative manifestation, which is where statistics need be to aware of its limitations. While statistics can compile descriptive information on a community and tackle some of the interactions within a community, it has to fail considering other points. This limitation is important to realise.\n\n4) '''what is the intervention of the natural experiment within the case?'''\nHere, a whole world of interventions could be described. Instead, let us just settle on what was the active part of the researcher: systematically compare the state before the intervention with the state after the intervention.\n\n==Experiments in sustainability science==\n[[File:World 3 model.jpg|600px|thumb|right|'''The world 3 model was part of the \"Limits to Growth\" report of the Club of Rome.''' The picture creates a sense of the multitude and complexity of variables that influence a single case study in the real world.]]\nExperiments in sustainability science are characterised by an intervention and the production of empirical evidence. These two key criteria are essential for experiments in sustainability science, which can be in addition also differentiated into a problem focused and a solution orientated focus. Nevertheless, problem orientated studies with full control over all variables (except for the one(s) being investigated) are still clearly timely in sustainability science, as many experiments conducted in ecology prove. Less control over variables is equally important, and again ecology, agricultural science but also experiments in psychology come to mind. Problem orientated focus with no control over variables is at the heart of a joined problem framing of [[Glossary|transdisciplinary]] research in sustainability science, and such an approach also characterised the first Club of Rome report. Solution orientated focus with total or some control over variables is insofar a remarkable step since it marks a shift from the still dominating [[Glossary|paradigm]] of research where facts are only approximated. Since solutions may be compromises but can be achieved, a solution orientated research indicates a shift from more classical lines of thinking. The solution orientated approach with no control over variables marks the frontier of research, since it is clearly long term thinking. Prominent efforts are the IPBES, the sustainable development goal measures, and the current efforts of the rising number of transdisciplinary projects. While these are starting points, I consider these efforts to be a clear sign of hope, not only for out planet, but likewise for the development of science out of its dogmatic slumber.\n\n==Meta-analysis: Integrating cases towards holistic understanding==\nOut of the increasing number of scientific studies with a comparable design, an international school of thinking emerged: [http://meta-evidence.co.uk/difference-systematic-review-meta-analysis/ Meta-analysis]! Within meta-analytical frameworks, great care is taken to combine several studies to investigate their overall outcome. Rooted deeply in medicine and psychology, meta-analysis used the statistical power of studies that show a similar design and setting. Since some studies are larger while others build on smaller samples, meta-analysis are able to take some of these differences into account.\n\nIt is however important to understand the thinking behind meta-analysis, as it helps to create supra-knowledge about certain questions that were enough in the focus so that a number of integrable studies is available. Graphical overviews show summaries of effects, and so called random factors allow for the correction of different sample sizes and other factors. Beyond the disciplines where meta analysis were initially established, this line of thinking found its way into ecology, conservation, agriculture and many other arenas of science that investigate building on deductive designs. To this end, challenges often arise out of field experiments where less variables are being controlled for or are being understood, and hence comparisons may not be valid or at least less plausible. \n\nWith a rising number of case studies, there is currently a trend to integrate more inductively designed studies as well into a meta-analytical scheme of analysis. Due to a more diverse study design and publication culture this is often a difficult and work intense endeavour. Still, it is vital to evolve science into an endeavour where more singular cases can be integrated to create meta-analytical knowledge. This is a task for the current generation, since the mode of science needs to be evolved. The knowledge and understanding of singular cases may be to precious to be lost and now integrated to build a deeper understanding.  For more info, please refer to the [[Meta-Analysis]] entry.\n\n\n==External links==\n[https://learningforsustainability.net/systems-thinking/ System Thinking]: An introduction\n\n[http://www.eisp.org/818/ Ethnographic Studies on the Easter Islands]: An important example of a natural experiment\n\n[https://www.britannica.com/science/natural-experiment Natural Experiments]: An introduction\n\n[https://www.tandfonline.com/doi/full/10.1080/13645579.2018.1488449 Conducting and Evaluating Natural Experiments]: A more detailed article \n\n[https://academic.oup.com/rcfs/article/4/2/155/1555737#113865691 The financial crisis 2009]: A very long & detailed paper\n\n[https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 Real World Experiments in Ecology]: A very interesting paper (VPN needed)\n\n[https://www.pnas.org/content/108/49/19449.short Sustainability Sciences]: A short article about its origins and research agenda\n\n[http://meta-evidence.co.uk/difference-systematic-review-meta-analysis/ Meta-Analysis]: What is the difference to a systematic review?\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "gx3rck3ao4hzwtc4mqe9ewoaegokrmr"
                }
            },
            {
                "title": "Causality",
                "ns": "0",
                "id": "102",
                "revision": {
                    "id": "6866",
                    "parentid": "5898",
                    "timestamp": "2022-12-12T20:50:14Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* The high and low road of causality */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "32655",
                        "#text": "'''Note:''' This entry focuses on Causality in science and statistics. For a brief summary of the relation between Causality and Correlations, please refer to the entry on [[Causality and correlation]].\n\n==Introduction==\n\nCausality. Where to start, how to end? \n\n'''Causality is one of the most misused and misunderstood concepts in statistics.''' All the while it is at the heart of the fact that all statistics are normative. While many things can be causally linked, other things are not causally linked. The problem is that we dearly want certain things to be causally linked, while we want other things not to be causally linked. This confusion has many roots, and spans across such untameable arenas such as faith, psychology, culture, social constructs and so on. Causality can be everything that is good about statistics, and it can be equally everything that is wrong about statistics. To put it in other words, it can be everything that is great about us humans, but it can be equally the root cause of everything that is wrong with us.\n\nWhat is attractive about causality? People search for explanations, and this constant search is probably one of the building blocks of our civilisation. Humans look for reasons to explain phenomena and [[Glossary|patterns]], often with the goal of prediction. If I understood a causal relation, I may be able to know more about the future, cashing in on being either prepared for this future, or at least being able to react properly. \n\nThe problem with causality is that different branches of science as well as different streams of philosophy have different explanations of causality, and there exists an exciting diversity of theories about causality. Let us approach the topic systematically.\n\n\n==== The high and low road of causality====\nLet's take the first extreme case, building on the theory that storks bring the babies. Obviously this is not true. Creating a causal link between these two is clearly a mistake. Now lets take the other extreme case, you fall down a flight of stairs, and in the fall break your leg. There is however some form of causal link between these two actions, that is falling down the stairs caused you to break your leg. However, this already demands a certain level of abstraction, including the acceptance that it was you who did fall down the stairs, you twisting your leg or hitting a stair with enough force, you not being too weak to withstand the impact etc. There is, hence, a very detailed chain of events happening between you starting to lose your balance, and you breaking your leg. Our mind simplifies this into \u201cbecause I fell down the stairs, I broke my leg\u201d. Obviously, we do not blame the person who built the stairs, and we do not blame our parents for bringing us into this world, where we then broke our leg. These things are not causally linked. \n\nBut, imagine now that the construction worker did not construct the stairs the proper way, and that one stair is slightly higher than the other stairs. We now claim that it is the fault of the construction worker. However, how much higher does this one stair need to be so that we blame not ourselves, but the construction worker? Do you get the point? \n\n'''Causality is a construction that is happening in our mind.''' We create an abstract view of the world, and in this abstract view we come of with a version of reality that is simplified enough to explain for instance future events, but it is not too simple, since this would not allow us to explain anything specific or any smaller groups of events.\n\n[[File:860-header-explainer-correlationchart.jpg|500px|thumb|left|'''Correlations can be deceitful'''. Source: [http://www.tylervigen.com/spurious-correlations Spurious Correlations]]] \n\nCausality is hence an abstraction that follows the [[Why_statistics_matters#Occam.27s_razor|''Occam's Razor'']], I propose. And since we all have our own version of Occam's Razor in our head, we often disagree when it comes to causality. I think this is merely related to the fact that everything dissolves under analysis. If we analyse any link between two events then the causal link can also be dissolved, or can become irrelevant. For this reason, a longer discussion was needed to clarify that, ultimately, ''causality is a choice.'' \n\nTake the example of [https://sciencebasedmedicine.org/evidence-in-medicine-correlation-and-causation/ medical studies] where most studies build on a correlative design, testing how, for instance, taking Ibuprofen may help against the common cold. If I have a headache, and I take Ibuprofen, in most cases it may help me. But do I understand how it helps me? I may understand some parts of it, but I do not really understand it on a cellular level. There is again a certain level of abstraction.\n\nWhat is now most relevant for causality is the mere fact that one thing can be explained by another thing. We do not need to understand all the nitty-gritty details of everything, and I have argued above that this would ultimately be very hard on us. Instead, we need to understand whether taking one thing away prevent the other thing to not happen. If I did not walk down the stairs, I would have not broken my leg.\n\nEver since Aristotle and his question \u201dWhat is its nature?\u201d, we are nagged by the nitty-grittiness of true causality or deep causality. I propose that there is a '''high road of causality''', and a '''low road of causality'''. The high road allows us to explain everything on how two things or phenomena are linked. While I reject this high road, many schools of thought consider it to be very relevant. I for myself prefer the low road of causality: May one thing or phenomena be causally linked to another thing or phenomena; if I take one away, will the other not happen? This automatically means that I have to make compromises of how much I understand about the world. \n\nI propose we do not need to understand everything. Our ancestors did not truly understand why walking out in the dark without a torch and a weapon - or better even in a larger group - might result in death in an area with many predators. Just knowing that staying in at night would keep them safe was good enough for them. It is also good enough for me.\n\n====Simple and complex Causality====\n'''Let us try to understand causal relations step by step.''' To this end, we may briefly differentiate two types of causal relations.\n \nStatistical correlations imply causality if one variable A is actively driven by a variable B. If B is taken away or changed, A changes as well or becomes non-existent. This relation is among the most abundantly known relations in statistics, but it has certain problems. First and foremost, two variables may be causally linked but the relation may be weak. [https://www.ncbi.nlm.nih.gov/pubmed/23775705 Zink] may certainly help against the common cold, but it is not guaranteed that Zink will cure us. It is a weak causal correlation. \n\nSecond, a causal relation of variable A and B may interact with a variable C, and further variables D,E,F etc. In this case, many people speak of complex relations. Complex relations can be causal, but they are still complex, and this complexity may confuse people. Lastly, statistical relations may be inflicted by biases, sample size restrictions, and many other challenges statistics face. These challenges are known, increasingly investigated, but often not solvable.\n\n====Structure the chaos: Normativity and plausibility====\n[[File:Black swan.jpg|thumb|right|This is probably one of the most famous examples for a universal theory, that can be disproven by one contradictory case. Are all swans white? It may seem trivial, but the black swan is representative for [https://www.youtube.com/watch?v=XlFywEtLZ9w Karl Popper's Falsificationism], an important principle of scientific work.]]\n\nBuilding on the thought that our mind wanders to find causal relations, and then having the scientific experiment as a powerful tool, scientists started deriving and revising theories that were based on the experimental setups. Sometimes it was the other way around, as many theories were only later proven by observation or scientific experiments. Having causality hence explained by scientific theories creates a combination that led to physical laws, societal paradigms and psychological models, among many other things. \n\nPlausibility started its reign as a key criterion of modern science. Plausibility basically means that relations can only be causal if the relations are not only probable but also [https://justinhohn.typepad.com/blog/2013/01/milton-friedmans-thermostat-analogy.html reasonable].  Statistics takes care of the probability. But it is the human mind that derives reason out of data, making causality a deeply normative act. Counterfactual theories may later disprove our causality, which is why it was raised that we cannot know any truth, but we can approximate it. Our assumptions may still be falsified later.\n \nSo far, so good. It is worth noting that Aristoteles had some interesting metaphysical approaches to causality, as do Buddhists and Hindus. We will ignore these here for the sake of simplicity.\n\n====Hume's Criteria for causality====\n[[File:David Hume.jpg|thumb|left|David Hume]]\n'''It seems obvious, but a necessary condition for causality is temporal order''' (Neumann 2014, pp.74-78). Temporal causal chains can be defined as relations where an effect has a cause. An event A may directly follow an action B. In other words, A is caused by B. Quite often, we think that we see such causal relations rooted in a temporal chain. The complex debate on [https://www.cdc.gov/vaccinesafety/concerns/autism.html vaccinations and autism] can be seen as such an example. The opponents of vaccinations think that the autism was caused by vaccination, while medical doctors argue that the onset of autism merely happened at the same time as the vaccinations are being made as part of the necessary protection of our society. The temporal relation is in this case seen as a mere coincidence. Many such temporal relations are hence assumed, but our mind often deceives us, as we want to get fooled. We want order in the chaos, and for many people causality is bliss. History often tries to find causalities, yet as it was once claimed, history is written by the victorious, meaning it is subjective. Having a model that explains your reality is what many search for today and having some sort of a temporal causal chain seems to be one of the cravings of many human minds. Scientific experiments were invented to test such causalities, and human society evolved. Today it seems next to impossible to not know about gravity -in a sense we all know, yes- but the first physical experiments helped us prove the point. Hence did the scientific experiment help us to explore temporal causality, and [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3835968/ medical trails] can be seen as one of the highest propelled continuation of these.\n\nWe may however build on Hume, who wrote in his treatise of human nature the three criteria mentioned above. Paraphrasing his words, causality is contiguous in space and time, the cause is prior to the effect, and there is a constant union between cause and effect. It is worthwhile to consider the other criteria he mentioned.\n \n1)   Hume claims that the same cause always produces the same effect. In statistics, this pinpoints at the criterion of reproducibility, which is one of the backbones of the scientific experiment. This may be seen difficult in times of single case studies, but all the while highlights that the value of such studies is clearly relevant, but limited according to this criterion.\n\n2)   In addition, if several objects create the same effect, then there must be a uniting criterion among them causing the effect. A good example for this is weights on a balance. Several weights can be added up to have the same -counterbalancing- effect. One would need to pile a high number of feathers to get the same effect as, let\u2019s say, 50 kg of weight (and it would be regrettable for the birds). Again, this is highly relevant for modern science, as looking for united criteria among is a key goal in the amounts of data we often analyse these days.\n\n3)   If two objects have a different effect, there must be a reason that explains the difference. This third assumption of Hume is a direct consequence from 2). What unites factors, can be important, but equally important can be, what differentiates factors. This is often the reason why we think there is a causal link between two things, when there clearly is not. Imagine some evil person gives you decaf coffee, while you are used to caffeine. The effect might be severe, and it is clear that the caffeine in the coffee that wakes you up differentiates this coffee from the decaffeinated one.  \n \nTo summarize, causality is a mess in our brains. We are wrong about causality more often than we think, our brain is hardwired to find connections and often gets fooled into assuming causality. Equally, we often want to neglect causality, when it is clearly a fact. We are thus wrong quite often. In case of doubt, stick with [https://statisticsbyjim.com/basics/causation/ Hume and the criteria] mentioned above. Relying on these when analysing correlations demand practice. The following examples may help you to this end:\n\n====Correlation is not Causality====\n[[File:Spurious correlations .png|center]]\n\nIf you'd like to see more [https://www.youtube.com/watch?v=GtV-VYdNt_g spurious correlations], click [http://www.tylervigen.com/spurious-correlations here].\n\nA non-causal link is the information that in regions with more stork nests you have more babies. If you think that storks bring babies, this might seem logical. However, it is clearly untrue, storks do not bring the babies, and regions with more storks do not have more babies. \n\nWhile a '''correlation''' tests a mere relation between two variables, a '''regression''' implies a certain theory on why two variables are related. In times of big data there is a growing amount of correlations that are hard to explain. From a theory-driven perspective, this can even be a good thing. After all, many scientific inventions were nothing short of being accidents. Likewise, through machine learning and other approaches, many patterns are found these days where we have next to no explanation to begin with. Hence, new theories even evolve out of the crunching of a lot of data, where prediction is often more important than explanation. (For more on regressions, please refer to the entry on [[Regression Analysis]].)\n\nModern medicine is at the forefront of this trend, as medicine had a long tradition to have the cure in the focus of their efforts, instead of being able to explain all patterns. Modern genetics correlate specific gene sequences with potential diseases, which can be helpful, but also creates a burden or even harm. It is therefore very important to consider theory when thinking of causal relationships. \n\nMuch mischief has been done in the name of causality, while on the other hand the modern age of data made us free to be bound down by theory if we have a lot of data on our hands. [https://www.thoughtco.com/correlation-and-causation-in-statistics-3126340 Knowing the difference] between the two endpoints is important, and the current scientific debate will have to move out of the muddy waters between inductive and deductive reasoning.\n\nIn this context, it should be briefly noted that a [https://www.wired.com/story/were-all-p-hacking-now/ discussion] about [https://www.youtube.com/watch?v=Gx0fAjNHb1M p-hacking] or data dredging has evolved. The terms describe the phenomenon when scientists try to artificially improve their results or make them seem to be significant. However p-hacking becomes more widespread, there is [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000/ evidence] (Head et al., 2015), that it - luckily - does not distort scientific work too deeply.\n\nTo give you an idea, how hard it can be to prove a causal relationship, we recommend this [https://www.youtube.com/watch?v=Bg9eEcs9cVY video] about the link of smoking and cancer.\n\n====Prediction====\n\nStatistics can be able to derive prediction based on available data. This means in general that based on the principle of extrapolation, there is enough statistical power to predict based on the available data beyond the range of the data. In other words, we have enough confidence in our initial data analysis so that we can try to predict what might happen under other circumstances, most prominently in the future, or in other places. Interpolation allows us to predict within the range of our data, spanning over gaps in the data. Hence while interpolation allows us to predict within our data, extrapolation allows prediction beyond our dataset.\n\nCare is necessary when considering interpolation or extrapolation, as validity decreases when we dwell beyond the data we have. It takes experience to know when prediction is possible, and when it is dangerous.\n\n1. We need to consider if our theoretical assumptions can be reasonable expanded beyond the data of our sample.\n\n2. Our statistical model may be less applicable outside or our data range.\n\n3. Mitigating or interacting factors may become more relevant in the space outside of our sample range.\n\n[[File:Bildschirmfoto 2020-05-10 um 09.34.37.png|thumb|The atmospheric CO2 at the Mauna Loa observatory is a good example for extrapolation because we predict beyond the range of the data.]]\nWe always need to consider these potential flaws or sources of error that may overall reduce the validity of our model when we use it for interpolation or extrapolation. As soon as we gather outside of the data space we sampled, we take the risk to produce invalid predictions. The [http://resolver.ebscohost.com/openurl?sid=google&auinit=RJ&aulast=Fogelin&atitle=Hume+and+the+missing+shade+of+blue&title=Philosophy+and+phenomenological+research&volume=45&issue=2&date=1984&spage=263&site=ftf-live \"missing shade of blue\"] problem from Hume exemplifies the ambiguities that can be associated with interpolation already, and extrapolation would be seen as worse by many, as we go beyond our data sample space.\n\nA common example of extrapolation would be a mechanistic model of climate change, where based on the trend in CO2 rates on Mauna Loa over the last decades we predict future trends. An prominent example of interpolation is the [https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/joc.5086?casa_token=tAsGRmH224cAAAAA%3AvU7NWkQFoEKrMqGFTni0vjxCFiweY0LOvD5fIXpucbo31opPewAvSgQanuOfLGW4axDlWQt5aVAERE4 Worldclim] [https://worldclim.org/data/index.html dataset], which generates a global climate dataset based on advanced interpolation. Based on ten thousands of climate stations and millions of records this dataset provides knowledge about the average temperature and precipitation of the whole terrestrial globe. The data has been used in thousands of scientific publications and is a good example how open source data substantially enabled a new scientific arena, namely Macroecology.\n\n==Significance in regressions==\n\n====P-values vs. sample size====\n[https://sustainabilitymethods.org/index.php/Data_distribution#A_matter_of_probability Probability] is one of the most important concepts in modern statistics. The question whether a relation between two variables is purely by chance, or following a pattern with a certain probability is the basis of all probability statistics (surprise!). In the case of linear relation, another quantification is of central relevance, namely the question how much variance is explained by the model. Between these two numbers - the amount of variance explained by a linear model, and the fact that two variables are not randomly related - are related at least to some amount. If a model is highly significant, it typically shows a high [https://www.youtube.com/watch?v=IMjrEeeDB-Y R<sup>2</sup> value]. If a model is marginally significant, then the R<sup>2</sup> value is typically low. This relation is however also influenced by the sample size. Linear models and the related p-value describing the model are highly sensitive to sample size. You need at least a handful of points to get a significant relation, while the [https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit R<sup>2</sup> value] in this same small sample sized model may be already high. Hence the relation between sample size, R<sup>2</sup> and p-value is central to understand how meaningful models are.\n\n==== Residuals ====\n[[File:Residuals.png|thumb|left|This plot displays the residuals which means the distance of the individual data points from the regression line.]]\nAll models are wrong, some models are useful. \n\nThis famous sentence from Box cannot be highlighted enough when thinking about models. In the context of residuals it comes in handy once more. [https://www.thoughtco.com/what-are-residuals-3126253 Residuals] are the sum (of squares) of the deviance the individual data points have from a perfectly fitted model. This is relevant for two things. First, the model assumes a perfect model. Second, reality bags to differ. Allow me to explain. \n\nReality is complex as people like to highlight today, but in a statistical sense that is often true. Most real datasets do not show a perfect model fit, but instead may individual data points deviate from the perfect fit the model assumes. What is now critical, is to assume that the error reflected by the residuals is normally distributed. Why, you ask? Because if it is not normally distributed, but instead shows a flawed pattern, then you missed an important variable that influences the distribution of the residuals. The distance from the points from the line should be normally distributed, which can be checked through a histogram.\n[[File:HistogramResiduals.png|thumb|right|The histograms shows you that the residuals of the model cars are normally distributed.]] What does this mean in terms of hands on inspection of the results? Just as in correlations, a scatterplot is a good first step to check out the [https://www.youtube.com/watch?v=-qlb_nZvN_U residuals]. If residuals are normally distributed, then you may realise that all errors are equally distributed across the relation. In reality, this is often not the case. Instead we often know less about one part of the data, and more about another part of the data. In addition to this we do often have a stronger relation across parts of the dataset, and a weaker understanding across other parts of the dataset. These differences are important, as they hint at underlying influencing variables or factors that we did not understand yet. Becoming versatile in reading the residuals of a linear model becomes a key skill here, as it allows you to rack biases and flaws in your dataset and analysis. This is probably the most advanced skill when it comes to reading a correlation plot.\n<syntaxhighlight lang=\"R\" line>\n\n#let's do a model for the cars data set\ncars\ncars<-cars\n#define two variables, columnes\ncar_speed<-c(cars$speed)\ncar_dist<-c(cars$dist)\n\n#decide on dependent and independent variable\n#dependent variable = car distance\n#independent variable = car speed\nmodel_cars<-lm(car_dist~car_speed, data = cars)\n\n#inspect residuals of the model\nplot(model_cars$residuals)\n\n#check if the model residuals follow a normal distribution\nhist(model_cars$residuals)\n\n</syntaxhighlight>\n[[File:Skewed residuals.png|thumb|left|These plots show you two examples of different distributions of residuals.]]\n\nA second case of a [https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/ skewed distribution] is when the residuals are showing any sort of clustering. Residuals should be distributed like stars in the sky. If they are not, then your error is not normally distributed, which basically indicates that you are missing some important information. \n\nThe third and last problem you can bump into with your residuals are gaps. Quite often you have sections in your data about which you know nothing about, as you have no data for this section. An example would be if you have a lot of height measurement of people between 140-180 cm in height, and one person that is 195 cm tall. About the section between 180-195cm we know nothing, there is a gap.\n \nSuch flaws within residuals are called errors. These errors are the reason why all models are wrong. If a model would be perfect, then there would be probably a mistake, at least when considering models in statistics. Learning to read statistical models and their respective residuals is the daily bread and butter of statistical modelling. The ability to see flaws in residuals as well as the ability to just see a correlation plot in consider the strength of the relation, if any exists, is an important skill in data mining. Learning about models means to also learn about the flaws of models.\n\n==Is the world linear?==\n[[File:IslandTheory.png|thumb|right|This graph shows you the linear relationship between island size and number of species. As written in MacArthur's and Wilson's book \"The Theory in Island Biogeography\" the larger islands are the more species they host.]]\n'''The short answer: Mostly yes, otherwise not.'''\n\nThe long answer: The world is more linear than you think it is.\n\nI believe that many people are puzzled by the complexity of the world, while indeed many phenomena as well as the associated underlying laws are rather simple. There are many phenomena that are [https://www.youtube.com/watch?v=wigmon6jlQE linear], and this is worth noticing. People today often think that the world is non-linear. Regime shifts, disasters and crisis are thought to be prominent examples. I would argue, that even these shifts follow linear patterns, though on a smaller temporal scale. Take the last financial crisis. A lot was building up towards this crisis, yet the collapse that happened in one day followed a linear pattern, if only a strongly exponential one. Many supposably non-linear shifts are indeed linear, they just happen very quickly.\n[[File:IslandTheoryBook.jpg|thumb|left|The book The Theory of Island Biogeography by MacArthur and Wilson tells you a lot about the relationship between island size and number of species.]]\nLinear patterns describe something like a natural law. Within a certain reasonable data section, you can often observe that one phenomena increases if another phenomena increases, and this relation follows a linear pattern. If you make the oven more hot, your veggies in there will be done quicker. However, this linearity only works within a certain reasonable section of the data. It would be possible to put the oven on 100 \u00b0C and the veggies would cook much slower than at 150 \u00b0C. However, this does not mean that a very long time at -30 \u00b0C in the freezer would boil your veggies in the long run as well.\n[[File:Linearity Ebola.png|thumb|right|This graph points out until which point the outbreak of the Ebola virus followed a linear pattern and at which point it increased exponentially.]]\n[[File:WaterPhaseHeatDiagram.png|thumb|right|On this picture you can see an example for non-linearity. The graph shows you the phase changes of water with increasing temperature.]]\nAnother prominent example is the [https://en.wikipedia.org/wiki/The_Theory_of_Island_Biogeography Island Theory] from MacArthur and Wilson. They counted species on islands, and found out that, the larger islands are, the more species they host. While this relation has a lot of complex underpinnings, it is -on a logarithmic scale- totally linear. Larger islands contain more species, with a clearly mathematical beauty. \n\nPhenomena that you can count are often linear on a log scale. Among coffee drinkers most coffee drinkers drink 1-2 cups per day, but few drink 10, which is probably good. Counting cup consumption per day in coffee drinker follows a log-linear distribution. The same hold for infections within an outbreak of a contagious disease. The Western African Ebola crisis was an incredibly complex and tragic event, but when we investigated the early increase in cases, the distribution is exponentially increasing -in this case-, and this increase follows an almost cruel linear pattern. \n\nNevertheless, there are some truly non-linear shifts, such as frost. Water is at -1 \u00b0C fundamentally different than at +1\u00b0C. This is a clear change governed by the laws of physics, yet such non-linear shifts are rare in my experience. If they occur, they are highly experimentally reproducible, and can be explained by some fundamental underlying law. Therefore, I would refrain from describing the complexity and wickedness of the modern world and its patterns and processes as non-linear. We are just not bright enough to understand the linearity in the data yet. I would advise you to seek linearity, if only logarithmic or exponential one.\n\n\n==References==\n\nNeuman, William Lawrence. ''Social Research Methods: Qualitative and Quantitative Approaches.'' 7. ed., Pearson new internat. ed. Pearson custom library. Harlow: Pearson, 2014.\n\nHead, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). ''The extent and consequences of p-hacking in science.'' PLoS Biology, 13 (3), e1002106. https://doi.org/10.1371/journal.pbio.1002106\n\n\n==Further Reading==\n\nPearl, J., & Mackenzie, D. (2018). The book of why: The new science of cause and effect (First edition). Basic Books.\n\n\n==External Links==\n\n====Videos====\n\n[https://www.youtube.com/watch?v=XlFywEtLZ9w Falsificationism]: A complete but understandable explanation\n\n[https://www.youtube.com/watch?v=Gx0fAjNHb1M What is P-Hacking?]: From A to izzard\n\n[https://www.youtube.com/watch?v=GtV-VYdNt_g Correlation doesn't equal Causation]: An informative video\n\n[https://www.youtube.com/watch?v=Bg9eEcs9cVY Proving Causality]: The case of smoking and cancer\n\n[https://www.youtube.com/watch?v=IMjrEeeDB-Y R square]: What does it tell us?\n\n[https://www.youtube.com/watch?v=-qlb_nZvN_U Residuals]: A detailed explanation\n\n[https://www.youtube.com/watch?v=wigmon6jlQE Linearity]: A few words on what it is\n\n====Articles====\n[https://sciencebasedmedicine.org/evidence-in-medicine-correlation-and-causation/ Evidence in medicine]: Some reflections on correlation and causation\n\n[https://www.ncbi.nlm.nih.gov/pubmed/23775705 Zink & the Common Cold]: A helpful article\n\n[https://www.cdc.gov/vaccinesafety/concerns/autism.html Vaccination & Autism]: an enlightening article\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3835968/ Medical trails]: A brief insight\n\n[https://justinhohn.typepad.com/blog/2013/01/milton-friedmans-thermostat-analogy.html Limits of statistics]: A nice antology\n\n[https://statisticsbyjim.com/basics/causation/ Hill's Criteria of Causation]: A detailed article\n\n[http://www.tylervigen.com/spurious-correlations Spurious correlations]: If you can't get enough\n\n[https://www.statpac.com/statistics-calculator/correlation-regression.htm Regression analysis]: An introduction\n\n[https://www.thoughtco.com/correlation-and-causation-in-statistics-3126340 Correlation & Causation]: Some helpful advice\n\n[https://www.wired.com/story/were-all-p-hacking-now/ What is this p-hacking discussion anyway?]: Genealogy of the term\n\n[http://resolver.ebscohost.com/openurl?sid=google&auinit=RJ&aulast=Fogelin&atitle=Hume+and+the+missing+shade+of+blue&title=Philosophy+and+phenomenological+research&volume=45&issue=2&date=1984&spage=263&site=ftf-live The Missing Shade of Blue]: Some interesting thoughts by Hume (VPN needed)\n\n[https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/joc.5086?casa_token=tAsGRmH224cAAAAA%3AvU7NWkQFoEKrMqGFTni0vjxCFiweY0LOvD5fIXpucbo31opPewAvSgQanuOfLGW4axDlWQt5aVAERE4 WorldClim 2]: An interesting paper on interpolation\n\n[https://worldclim.org/data/index.html WorldClim dataset]: Global climate and weather data\n\n[https://www.thoughtco.com/what-are-residuals-3126253 Residuals]: A detailed explanation\n\n[https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/ Checking residuals]: Some recommendations\n\n[https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit R square]: A very detailed article\n\n[https://en.wikipedia.org/wiki/The_Theory_of_Island_Biogeography Island Theory]: An example for linearity\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "2h7m24b62snb9hk1pr0xa76lx4zljm2"
                }
            },
            {
                "title": "Causality and correlation",
                "ns": "0",
                "id": "431",
                "revision": {
                    "id": "6722",
                    "parentid": "6374",
                    "timestamp": "2022-06-15T20:38:41Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "24675",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Causality and correlation (German)]]<br/>\n\n'''Note:''' This entry focuses on the connection between correlations and causality. More details on Correlations as a scientific method can be found in this entry: [[Correlations]]. More details on Causality in cience can be found in the entry on [[Causality]]. More on Regressions can be found here: [[Regression Analysis]].\n\n== Correlative relations ==\n'''Correlations can tell us whether two variables are related.''' A correlation does not tell us however what this correlation means. This is important to note, as there are many correlations being calculated, but it is up to us to interpret these relations. \n\nIt is potentially within our normative capacity to derive [[Experiments and Hypothesis Testing|hypotheses]], but it can also be powerful to not derive hypotheses and have a purely [[:Category:Inductive|inductive]] approach to a correlation. We live in a world of big data, and increasingly so. There is a wealth of information out there, and it is literally growing by the minute. While the [[History of Methods|Enlightenment and then the Modernity]] built a world of science based on the power of hypothesis, this science was also limited. We know today that the powerful step of building a hypothesis offers only a part of the picture, and powerful inventions and progress came from induction. Much progress in science was based on [https://en.wikipedia.org/wiki/Inductive_reasoning#History inductive approaches]. Consider [https://explorable.com/history-of-antibiotics antibiotics], whose discovery was a mere accident. \n\nWith the predictive power of correlations and the rise of of [[Machine Learning|machine learning]] and the associated, much more complex approaches, a new world dawned upon us. Today, predictions are based on simple correlations and the recognition of [[Glossary|power]] patterns in the wealth of data we face. Although much of the actual mathematics are much more complicated, [https://www.repricerexpress.com/amazons-algorithm-a9/ suggestions of prominent online shops] on what you may want to buy next are - in principle - sophisticated elaborations of correlations. We do not understand why certain people that buy one thing buy also another thing, but promiting this relation increases sales. Of course the world is more complicated, and once more these models cannot luckily explain everything. As long as my [https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe music service suggests me to listen to] Oasis - the worst insult to me - we are safe from the total prediction of the machines. \nStill, with predictive power once more comes great responsibility, and we shall see how correlations and their predictive power will allow us to derive more theories based on our inductive perspective on data. Much is to be learned through the digitalisation of data, but it is still up to us to interpret correlations. This will be very hard to teach to a machine, hence it is our responsibility to interpret data analysis through reasoning. \n\n=== The Rise of Correlations ===\nPropelled through the general development of science during the Enlightenment, numbers started piling up. The increasing technological possibilities to measure more and more information are slow to store this information, and people started wondering whether these numbers could lead to something. The increasing numbers had diverse sources, some were from science, such as [https://en.wikipedia.org/wiki/Least_squares#The_method Astronomy] or other branches of [https://en.wikipedia.org/wiki/Regression_toward_the_mean#History natural sciences]. Other prominent sources of numbers were from engineering, and even other from economics, such as [https://en.wikipedia.org/wiki/Bookkeeping#History double bookkeeping]. It was thanks to the tandem efforts of [https://www.britannica.com/biography/Adrien-Marie-Legendre Adrien-Marie Legendre] and [https://www.britannica.com/biography/Carl-Friedrich-Gauss Carl Friedrich Gauss] that mathematics offered the first approach to relate one line of data with another - the methods of least squares.\n\n'''How is one continuous variable related to another?''' Pandora's box was opened, and questions started to emerge. [https://en.wikipedia.org/wiki/Econometrics Economists] were the first who utilised [[Regression Analysis|regression analysis]] at a larger scale, relating all sorts of economical and social indicators with each other, building an ever more complex controlling, management and maybe even understanding of statistical relations. A regression implies a causal link between two continuous variables, which makes it different from a correlation, where two variables are related, but not necessarily causally linked. (For more on regressions, please refer to the entry on [[Regression Analysis]]. The [https://www.investopedia.com/terms/g/gdp.asp Gross domestic product] (GDP) became for quite some time kind of the [https://www.nature.com/news/development-time-to-leave-gdp-behind-1.14499 favorite] toy for many economists, and growth became a core goal of many analyses to inform policy. What people basically did is ask themselves how one variable is related to another variable. \n\n'''If nutrition of people increases, do they live longer?'''\n[[File:Bildschirmfoto 2019-10-18 um 10.38.48.png|thumb|600px|center|There is a '''positive correlation between nutrition and the life expectancy''' of people worldwide. Source: [https://www.gapminder.org/tools/ gapminder.org]]]\n<br/>\n'''Does a high life expectancy relate to more agricultural land area within a country?'''\n[[File:Bildschirmfoto 2019-10-18 um 10.51.34.png|thumb|600px|center|There is '''no correlation between life expectancy and the national amount of  of agricultural land.''' Source: [https://www.gapminder.org/tools/ gapminder.org]]]\n<br/>\n'''Is a higher income related to more CO2 emissions at a country scale?''' \n[[File:Bildschirmfoto 2019-10-18 um 10.30.35.png|thumb|600px|center|There is a '''positive correlation between national wealth and CO2 emissions.''' Source: [https://www.gapminder.org/tools/ gapminder.org]]]\n\n=== Core elements of correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br/>\n\n'''1) Is the relationship between two variables positive or negative?''' If one variable increases, and the other one increases, too, we have  a positive (\"+\") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\u00a0[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534609/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (\"-\"): for example, the relationship between 'pizza eaten' and 'pizza left' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship. \n\n'''2) Is the correlation coefficient small or large?''' It can range from -1 to +1, and is an important measure when we evaluate the strength of a statistical relationship. Data points may scatter widely in a [[Correlation_Plots#Scatter_Plot|scatter plot,]] or there may be a rather linear relationship - and everything in between. An example for a perfect positive correlation (with a correlation coefficient ''r'' of +1) is the relationship between temperature in [[To_Rule_And_To_Measure#Celsius_vs_Fahrenheit_vs_Kelvin|Celsius and Fahrenheit]]. This should not be surprising, since Fahrenheit is defined as 32 + 1.8\u00b0 C. Therefore, their relationship is perfectly linear, which results in such a strong correlation coefficient. We can thus say that 100% of the variation in temperature in Fahrenheit is explained by the temperature in Celsius.\n\nOn the other hand, you might encounter data of two variables that is scattered all the way in a scatter plot and you cannot find a significant relationship. The correlation coefficient ''r'' might be around 0.1, or 0.2. Here, you can assume that there is no strong relationship between these two variables, and that one variable does not explain the other one. \n\nThe stronger the correlation coefficient of a relation is, the more may these relations matter, some may argue. If the points are distributed like stars in the sky, then the relationship is probably not significant and interesting. Of course this is not entirely generalisable, but it is definitely true that a neutral relation only tells you, that the relation does not matter. At the same time, even weaker relations may give important initial insights into the data, and if two variables show any kind of relation, it is good to know the strength. By practising to quickly grasp the strength of a correlation, you become really fast in understanding relationships in data. Having this kind of skill is essential for anyone interested in approximating facts through quantitative data.  \n\n'''3) What does the relationship between two variables explain?''' \nThis is already an advanced skill, and is rather related to regression analysis. So if you have looked at the strength of a correlation, and its direction, you are good to go generally. But sometimes, these measures change in different parts of the data. \n\nTo illustrate this, let us have a look at the example of the percentage of people working in\u00a0[https://ourworldindata.org/employment-in-agriculture?source=post_page--------------------------- Agriculture]\u00a0within individual countries. Across the world, people at a low income (<5000 Dollar/year) have a high variability in terms of agricultural employment:  half of the population of the Chad work in agriculture, while in Zimbabwe it is only 10\u00a0%. However, at an income above 15000 Dollar/year, there is hardly any variance in the percentage of people that work in agriculture: it is always very low. If you plotted this, you would see that the data points are rather broadly spread in the lower x-values (with x as the income), but are more linearly spread in the higher income areas (= x values). This has reasons, and there are probably one or several variables that explain this variability. Maybe there are other factors that have a stronger influence on the percentage of farmers in lower income groups than for higher incomes, where the income is a good predictor. \n\nA correlation analysis helps us identify such variances in the data relationship, and we should look at correlation coefficients and the direction of the relationship for different parts of the data. We often have a stronger relation across parts of the dataset, and a weaker relation across other parts of the dataset. These differences are important, as they hint at underlying influencing variables or factors that we did not understand yet.\n\n[[File:Correlation coefficient examples.png|600px|thumb|center|'''Examples for the correlation coefficient.''' Source: Wikipedia, Kiatdd, CC BY-SA 3.0]]\n<br>\n\n== Causality ==\nWhere to start, how to end? \n\n'''Causality is one of the most misused and misunderstood concepts in statistics.''' All the while, it is at the heart of the fact that all statistics are normative. While many things can be causally linked, many are not. The problem is that we dearly want certain things to be causally linked, while we want other things not to be causally linked. This confusion has many roots, and spans across such untameable arenas such as faith, psychology, culture, social constructs and so on. Causality can be everything that is good about statistics, and it can be equally everything that is wrong about statistics. To put it in other words: it can be everything that is great about us humans, but it can be equally the root cause of everything that is wrong with us.\n\nWhat is attractive about causality? People search for explanations, and this constant search is probably one of the building blocks of our civilisation. Humans look for reasons to explain phenomena and patterns, often with the goal of prediction. If I understood a causal relation, I may be able to know more about the future, cashing in on being either prepared for this future, or at least being able to react properly. \n\nThe problem with causality is that different branches of science as well as different streams of philosophy have different explanations of causality, and there exists an exciting diversity of theories about causality. Let us approach the topic systematically.\n\n==== The high and the low road of causality ====\nLet's take the first extreme case: the theory that storks bring the babies. Obviously this is not true. Creating a causal link between these two is obviously a mistake. Now lets take the other extreme case, you fall down a flight of stairs, and in the fall break your leg. There is obviously some form of causal link between these two actions, that is falling down the stairs caused you to break your leg. However, this already demands a certain level of abstraction, including the acceptance that it was you who did fall down the stairs, you twisting your leg or hitting a stair with enough force, you not being too weak to withstand the impact etc. There is, hence, a very detailed chain of events happening between you starting to lose your balance, and you breaking your leg. Our mind simplifies this into \u201cbecause I fell down the stairs, I broke my leg\u201d. Obviously, we do not blame the person who built the stairs, and we do not blame our parents for bringing us into this world, where we then broke our leg. These things are not causally linked. \n\nBut, imagine now that the construction worker did not construct the stairs the proper way, and that one stair is slightly higher than the other stairs. We now claim that it is the fault of the construction worker. However, how much higher does this one stair need to be so that we blame not ourselves, but the construction worker? You get the point.\n\n'''Causality is a construction that is happening in our mind.''' We create an abstract view of the world, and in this abstract view we come up with a version of reality that is simplified enough to explain, for instance, future events, but it is not too simple, since this would not allow us to explain anything specific or any smaller groups of events. \n\n[[File:860-header-explainer-correlationchart.jpg|500px|thumb|left|'''Correlations can be deceitful'''. Source: [http://www.tylervigen.com/spurious-correlations Spurious Correlations]]]\n\nCausality is hence an abstraction that follows [[Why_statistics_matters#Occam.27s_razor|''Occam's Razor'']]. And since we all have our own version of Occam's Razor in our head, we often disagree when is comes to causality. this is merely related to the fact that everything dissolves under analysis. If we analyse any link between to events then the causal link can also be dissolved, or can become irrelevant. Ultimately, causality is a choice. \n\nTake the example of [https://sciencebasedmedicine.org/evidence-in-medicine-correlation-and-causation/ medical studies] where most studies build on a correlative design, testing how, for instance, taking Ibuprofen may help against the common cold. If I have a headache, and I take Ibuprofen, in most cases it may help me. But do I understand how it helps me? I may understand some parts of it, but I do not really understand it on a cellular level. There is again a certain level of abstraction.\n\nWhat is now most relevant for causality is the mere fact that one thing can be explained by another thing. We do not need to understand all the nitty-gritty details of everything, and indeed this would be ultimately be very hard on us. Instead, we need to understand whether taking one thing away prevents the other thing from happening. If I did not walk down the stairs, I would have not broken my leg. \n\nEver since Aristotle and his question \u201dWhat is its nature?\u201d, we are nagged by the nitty-grittiness of true causality or deep causality. I propose that there is a '''high road of causality''', and a '''low road of causality'''. The high road allows us to explain everything on how two things or phenomena are linked. While I reject this high road, many schools of thought consider it to be very relevant. I for myself prefer the low road of causality: May one thing or phenomena be causally linked to another thing or phenomena; if I take one away, will the other not happen? This automatically means that I have to make compromises of how much I understand about the world. \n\nit is clear we do not need to understand everything. Our ancestors did not truly understand why walking out in the dark without a torch and a weapon - or better even in a larger group - might result in death in an area with many predators. Just knowing that staying in at night would keep them safe was good enough for them. It is also good enough for me.\n\n==== Simple and complex Causality ====\n'''Let us try to understand causal relations step by step.''' To this end, we may briefly differentiate two types of causal relations.\n \nStatistical correlations imply causality if one variable (A) is actively driven by a variable (B). If B is taken away or changed, A changes as well or becomes non-existent. This relation is among the most abundantly known relation in statistics, but it has certain problems. First and foremost, two variables may be causally linked but the relation may be weak. [https://www.ncbi.nlm.nih.gov/pubmed/23775705 Zinc] may certainly help against the common cold, but it is not guaranteed that Zinc will cure us. It is a weak causal correlation. \n\nSecond, a causal relation of variable A and B may interact with a variable C, and further variables D, E, F etc. In this case, many people speak of complex relations. Complex relations can be causal, but they are still complex, and this [[Agency, Complexity and Emergence|complexity may confuse people]]. Lastly, statistical relations may be inflicted by biases, sample size restrictions, and many other challenges statistics face. These challenges are known, increasingly investigated, but often not solvable.\n\n==== Structure the chaos: Normativity and plausibility ====\n[[File:Black swan.jpg|thumb|right|This is probably one of the most famous examples for a universal theory, that can be disproven by one contradictory case. Are all swans white? It may seem trivial, but the black swan is representative for [https://www.youtube.com/watch?v=XlFywEtLZ9w Karl Popper's Falsificationism], an important principle of scientific work.]]\n\nBuilding on the thought that our mind wanders to find causal relations, and then having the scientific experiment as a powerful tool, scientists started deriving and revising theories that were based on the experimental setups. Sometimes it was the other way around, as many theories were only later proven by observation or scientific experiments. Having causality explained by scientific theories created a combination that led to physical laws, societal paradigms and psychological models, among many other things. \n\nPlausibility started its reign as a key criterion of modern science. Plausibility basically means that relations can only be causal if the relations are not only probable but also [https://justinhohn.typepad.com/blog/2013/01/milton-friedmans-thermostat-analogy.html reasonable]. Statistics takes care of the probability. But it is the human mind that derives reason out of data, making causality a deeply normative act. Counterfactual theories may later disprove our causality, which is why it was raised that we cannot know any truth, but we can approximate it. Our assumptions may still be falsified later.\n \nSo far, so good. It is worth noting that Aristoteles had some interesting metaphysical approaches to causality, as do Buddhists and Hindus. We will [[Big problems for later|ignore these here]] for the sake of simplicity.\n\n==== Hume's criteria for Causality ====\n[[File:David Hume.jpg|thumb|300px|left|'''David Hume''']]\n'''It seems obvious, but a necessary condition for causality is temporal order''' (Neumann 2014, pp.74-78). Temporal causal chains can be defined as relations where an effect has a cause. An event A may directly follow an action B. In other words, A is caused by B. Quite often, we think that we see such causal relations rooted in a temporal chain. The complex debate on [https://www.cdc.gov/vaccinesafety/concerns/autism.html vaccinations and autism] can be seen as such an example. The opponents of vaccinations think that the autism was caused by vaccination, while medical doctors argue that the onset of autism merely happened at the same time as the vaccinations are being made as part of the necessary protection of our society. The temporal relation is in this case seen as a mere coincidence. Many such temporal relations are hence assumed, but our mind often deceives us, as we want to get fooled. We want order in the chaos, and for many people causality is bliss. History often tries to find causalities, yet as it was once claimed, history is written by the victorious, meaning it is subjective. Having a model that explains your reality is what many search for today and having some sort of a temporal causal chain seems to be one of the cravings of many human minds. Scientific experiments were invented to test such causalities, and human society evolved. Today it seems next to impossible to not know about gravity - in a sense we all know, yes - but the first physical experiments helped us prove the point. Hence did the scientific experiment help us to explore temporal causality, and [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3835968/ medical trails] can be seen as one of the highest propelled continuation of these.\n\nWe may however build on Hume, who wrote in his treatise of human nature the three criteria mentioned below. Paraphrasing his words, causality is contiguous in space and time, the cause is prior to the effect, and there is a constant union between cause and effect. It is worthwhile to consider the other criteria he mentioned.\n \n1) Hume claims that the same cause always produces the same effect. In statistics, this pinpoints at the criterion of reproducibility, which is one of the backbones of the scientific experiment. This may be seen difficult in times of single case studies, but all the while highlights that the value of such studies is clearly relevant, but limited according to this criterion.\n\n2) In addition, if several objects create the same effect, then there must be a uniting criterion among them causing the effect. A good example for this is weights on a balance. Several weights can be added up to have the same -counterbalancing- effect. One would need to pile a high number of feathers to get the same effect as, let\u2019s say, 50 kg of weight (and it would be regrettable for the birds). Again, this is highly relevant for modern science, as looking for united criteria among is a key goal in the amounts of data we often analyse these days.\n\n3) If two objects have a different effect, there must be a reason that explains the difference. This third assumption of Hume is a direct consequence from the second one. What unites factors can be important, but equally important can be what differentiates factors. This is often the reason why we think there is a causal link between two things, when there clearly is not. Imagine some evil person gives you decaf coffee, while you are used to caffeine. The effect might be severe, and it is clear that the caffeine in the coffee that wakes you up differentiates this coffee from the decaffeinated one.  \n \nTo summarize, causality is a mess in our brains. '''We are wrong about causality more often than we think, our brain is hardwired to find connections and often gets fooled into assuming causality.''' Equally, we often want to neglect causality, when it is clearly a fact. We are thus wrong quite often. In case of doubt, stick with [https://statisticsbyjim.com/basics/causation/ Hume] and the criteria mentioned above. Relying on these when analysing correlations, ultimately, demands practice.\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "q4gtr5zi41pz98yyaimfyqbc7tg452k"
                }
            },
            {
                "title": "Causality and correlation (German)",
                "ns": "0",
                "id": "498",
                "revision": {
                    "id": "6783",
                    "parentid": "6782",
                    "timestamp": "2022-10-05T10:24:27Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "27852",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Causality and correlation]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.<br/>\n\n== Korrelative Zusammenh\u00e4nge ==\n'''Korrelationen k\u00f6nnen uns sagen, ob zwei Variablen miteinander in Beziehung stehen.''' Eine Korrelation sagt uns jedoch nicht, was sie bedeutet. Dies ist wichtig zu beachten, da viele Korrelationen berechnet werden, aber es liegt an uns, diese Beziehungen zu interpretieren. \n\nEs liegt potenziell in unserer normativen Kapazit\u00e4t, [[Experiments and Hypothesis Testing|Hypothesen]] abzuleiten, aber es kann auch wirkungsvoll sein, keine Hypothesen abzuleiten und einen rein [[:Category:Inductive|induktiven]] Ansatz f\u00fcr eine Korrelation zu haben. Wir leben in einer Welt der gro\u00dfen Datenmengen, und das in zunehmendem Ma\u00dfe. Es gibt eine F\u00fclle von Informationen, und sie w\u00e4chst buchst\u00e4blich von Minute zu Minute. W\u00e4hrend die [[History of Methods|Aufkl\u00e4rung und dann die Moderne]] eine Welt der Wissenschaft auf der Grundlage der Macht der Hypothese aufbauten, war diese Wissenschaft auch begrenzt. Wir wissen heute, dass der m\u00e4chtige Schritt, eine Hypothese aufzustellen, nur einen Teil des Bildes bietet, und m\u00e4chtige Erfindungen und Fortschritte kamen durch Induktion. Viele Fortschritte in der Wissenschaft beruhten auf [https://en.wikipedia.org/wiki/Inductive_reasoning#History induktiven Ans\u00e4tze]. Man denke nur an [https://explorable.com/history-of-antibiotics Antibiotika], deren Entdeckung ein reiner Zufall war. \n\nMit der Vorhersagekraft von Korrelationen und dem Aufstieg des [[Machine Learning|maschinellen Lernens]] und den damit verbundenen, viel komplexeren Ans\u00e4tzen brach eine neue Welt an. Heute basieren Vorhersagen auf einfachen Korrelationen und der Erkennung von Leistungsmustern in der F\u00fclle der Daten, mit denen wir konfrontiert sind. Obwohl ein Gro\u00dfteil der eigentlichen Mathematik sehr viel komplizierter ist, sind [https://www.repricerexpress.com/amazons-algorithm-a9/ Vorschl\u00e4ge prominenter Online-Shops] zu dem, was Sie vielleicht als n\u00e4chstes kaufen m\u00f6chten - im Prinzip - ausgefeilte Ausarbeitungen von Korrelationen. Wir verstehen nicht, warum bestimmte Leute, die eine Sache kaufen, auch eine andere Sache kaufen, aber das Aufzeigen dieser Beziehung erh\u00f6ht den Umsatz. Nat\u00fcrlich ist die Welt komplizierter, und wieder einmal k\u00f6nnen diese Modelle gl\u00fccklicherweise nicht alles erkl\u00e4ren. Ich wei\u00df selbst, dass ich vor der totalen Vorhersage der Maschinen sicher bin, solange mein [https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe Musikdienst mir vorschl\u00e4gt,] Oasis - die schlimmste Beleidigung f\u00fcr mich - zu h\u00f6ren. \nDennoch, mit der Vorhersagekraft kommt einmal mehr eine gro\u00dfe Verantwortung, und wir werden sehen, wie Korrelationen und ihre Vorhersagekraft es uns erm\u00f6glichen werden, mehr Theorien auf der Grundlage unserer induktiven Perspektive auf Daten abzuleiten. Durch die Digitalisierung der Daten ist viel zu lernen, aber es liegt immer noch an uns, die Korrelationen zu interpretieren. Es wird sehr schwer sein, dies einer Maschine beizubringen, daher liegt es in unserer Verantwortung, die Datenanalyse durch Schlussfolgerungen zu interpretieren. \n\n==== Der Aufstieg der Korrelationen ====\nAngetrieben durch die allgemeine Entwicklung der Wissenschaft w\u00e4hrend der Aufkl\u00e4rung begannen sich die Zahlen anzuh\u00e4ufen. Die zunehmenden technologischen M\u00f6glichkeiten, immer mehr Informationen zu messen, speichern diese Informationen nur langsam, und die Menschen begannen sich zu fragen, ob diese Zahlen zu etwas f\u00fchren k\u00f6nnten. Die wachsenden Zahlen hatten unterschiedliche Quellen, einige stammten aus der Wissenschaft, wie der [https://en.wikipedia.org/wiki/Least_squares#The_method Astronomie] oder anderen Zweigen der [https://en.wikipedia.org/wiki/Regression_toward_the_mean#History Naturwissenschaften]. Andere prominente Zahlenquellen stammten aus dem Ingenieur*innenwesen, und sogar andere aus der Wirtschaft, zum Beispiel durch die [https://en.wikipedia.org/wiki/Bookkeeping#History doppelte Buchhaltung]. Dank der gemeinsamen Anstrengungen von [https://www.britannica.com/biography/Adrien-Marie-Legendre Adrien-Marie Legendre] und [https://www.britannica.com/biography/Carl-Friedrich-Gauss Carl Friedrich Gau\u00df] bot die Mathematik den ersten Ansatz, eine Datenzeile mit einer anderen in Beziehung zu setzen - die Methode der kleinsten Quadrate.\n\n'''Wie h\u00e4ngt eine kontinuierliche Variable mit einer anderen zusammen?''' Die B\u00fcchse der Pandora wurde ge\u00f6ffnet, und es tauchten Fragen auf. [https://en.wikipedia.org/wiki/Econometrics \u00d6konom*innen] waren die ersten, die die [[Regression Analysis|Regressionsanalyse]] in gr\u00f6\u00dferem Ma\u00dfstab einsetzten, indem sie alle m\u00f6glichen wirtschaftlichen und sozialen Indikatoren miteinander in Beziehung setzten und so ein immer komplexeres Controlling, Management und vielleicht sogar Verst\u00e4ndnis f\u00fcr statistische Zusammenh\u00e4nge aufbauten. Das [https://www.investopedia.com/terms/g/gdp.asp Bruttoinlandsprodukt] (BIP) wurde f\u00fcr einige Zeit zu einer Art [https://www.nature.com/news/development-time-to-leave-gdp-behind-1.14499 Lieblings-]Spielzeug f\u00fcr viele \u00d6konom*innen, und Wachstum wurde zu einem Kernziel vieler Analysen, um die Politik zu informieren. Was die Leute im Grunde taten, war die Frage, wie eine Variable mit einer anderen Variable zusammenh\u00e4ngt. \n\n'''Erh\u00f6ht eine gute Ern\u00e4hrung die Lebenserwartung?'''\n[[File:Bildschirmfoto 2019-10-18 um 10.38.48.png|thumb|400px|center|Es gibt eine '''positive Korrelation zwischen Ern\u00e4hrung und Lebenserwartung''' weltweit. Quelle: [https://www.gapminder.org/tools/ gapminder.org]]]\n<br/>\n'''H\u00e4ngt eine h\u00f6here Lebenserwartung mit der landwirtschaftlichen Fl\u00e4che eines Landes zusammen?'''\n[[File:Bildschirmfoto 2019-10-18 um 10.51.34.png|thumb|400px|center|Es gibt '''keinen Zusammenhang zwischen Lebenserwartung und der nationalen landwirtschaftlichen Fl\u00e4che''' Quelle: [https://www.gapminder.org/tools/ gapminder.org]]]\n<br/>\n'''Korreliert ein h\u00f6heres Einkommen mit h\u00f6heren CO2-Emissionen?''' \n[[File:Bildschirmfoto 2019-10-18 um 10.30.35.png|thumb|400px|center|Es gibt eine '''positive Korrelation zwischen Wohlstand und CO2-Emissionen. Quelle: [https://www.gapminder.org/tools/ gapminder.org]]]\n\n====  Wichtige Elemente von Korrelationen ====\nEs gibt einige zentrale Fragen im Zusammenhang mit der Anwendung und dem Lesen von Korrelationen. Diese k\u00f6nnen immer dann von Interesse sein, wenn man den Korrelationskoeffizienten zur Hand hat - zum Beispiel in einer Statistiksoftware - oder wenn man ein Korrelationsdiagramm sieht.\n\n'''1) Ist die Beziehung zwischen zwei Variablen positiv oder negativ?''' Wenn eine Variable zunimmt und die andere ebenfalls zunimmt, haben wir eine positive (\"+\") Korrelation. Dies trifft auch zu, wenn beide Variablen abnehmen. Zum Beispiel f\u00fchrt eine gr\u00f6\u00dfere K\u00f6rpergr\u00f6\u00dfe zu einem signifikanten Anstieg des [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534609/ K\u00f6rpergewichts]. Wenn hingegen eine Variable zunimmt und die andere abnimmt, ist die Korrelation negativ (\"-\"): Zum Beispiel ist die Beziehung zwischen \"Pizza gegessen\" und \"Pizza \u00fcbrig\" negativ. Je mehr Pizzast\u00fccke gegessen werden, desto weniger St\u00fccke sind noch da. Diese Richtung der Beziehung sagt viel dar\u00fcber aus, wie zwei Variablen logisch miteinander verbunden sein k\u00f6nnten. Der normative Wert einer positiven oder negativen Beziehung hat in der Regel starke Implikationen, insbesondere wenn beide Richtungen theoretisch m\u00f6glich sind. Deshalb ist es wichtig, die Richtung einer korrelativen Beziehung interpretieren zu k\u00f6nnen. \n\n'''2) Ist der Korrelationskoeffizient klein oder gro\u00df?''' Der Korrelationskoeffizient kann zwischen -1 und +1 liegen und ist ein wichtiges Ma\u00df f\u00fcr die Bewertung der St\u00e4rke einer statistischen Beziehung. Datenpunkte k\u00f6nnen in einem [[Correlation_Plots#Scatter_Plot|scatter plot]] stark streuen oder es kann eine eher lineare Beziehung bestehen - und alles dazwischen. Ein Beispiel f\u00fcr eine perfekte positive Korrelation (mit einem Korrelationskoeffizienten ''r'' von +1) ist die Beziehung zwischen der Temperatur in [[To_Rule_And_To_Measure#Celsius_vs_Fahrenheit_vs_Kelvin|Celsius und Fahrenheit]]. Dies sollte nicht \u00fcberraschen, da Fahrenheit als 32 + 1,8\u00b0 C definiert ist. Daher ist ihre Beziehung vollkommen linear, was zu einem so starken Korrelationskoeffizienten f\u00fchrt. Wir k\u00f6nnen also sagen, dass 100 % der Temperaturschwankungen in Fahrenheit durch die Temperatur in Celsius erkl\u00e4rt werden.\n\nAndererseits kann es vorkommen, dass die Daten zweier Variablen in einem Streudiagramm weit verstreut sind und kein signifikanter Zusammenhang festgestellt werden kann. Der Korrelationskoeffizient ''r'' liegt dann vielleicht bei 0,1 oder 0,2. In diesem Fall kann man davon ausgehen, dass keine starke Beziehung zwischen den beiden Variablen besteht und dass die eine Variable die andere nicht erkl\u00e4rt. \n\nJe h\u00f6her der Korrelationskoeffizient einer Beziehung ist, desto mehr Bedeutung haben diese Beziehungen, so k\u00f6nnte man argumentieren. Wenn die Punkte wie Sterne am Himmel verteilt sind, dann ist die Beziehung wahrscheinlich nicht signifikant und interessant. Nat\u00fcrlich ist dies nicht ganz verallgemeinerbar, aber es stimmt auf jeden Fall, dass eine neutrale Beziehung nur aussagt, dass die Beziehung keine Bedeutung hat. Gleichzeitig k\u00f6nnen auch schw\u00e4chere Beziehungen wichtige erste Einblicke in die Daten geben, und wenn zwei Variablen irgendeine Art von Beziehung aufweisen, ist es gut, die St\u00e4rke zu kennen. Wenn man \u00fcbt, die St\u00e4rke einer Korrelation schnell zu erfassen, wird man sehr schnell in der Lage sein, Beziehungen in Daten zu verstehen. Diese F\u00e4higkeit ist f\u00fcr jede Person wichtig, die sich f\u00fcr die Ann\u00e4herung von Fakten durch quantitative Daten interessiert.  \n\n'''3) Was erkl\u00e4rt die Beziehung zwischen zwei Variablen?''' \nDies ist bereits eine fortgeschrittene F\u00e4higkeit, die eher mit der Regressionsanalyse zusammenh\u00e4ngt. Wenn also die St\u00e4rke einer Korrelation und ihre Richtung untersucht wurde, ist man im Allgemeinen gut ger\u00fcstet. Aber manchmal \u00e4ndern sich diese Ma\u00dfe in verschiedenen Teilen der Daten. \n\nUm dies zu veranschaulichen, betrachten wir das Beispiel des prozentualen Anteils der in der [https://ourworldindata.org/employment-in-agriculture?source=post_page--------------------------- Landwirtschaft] arbeitenden Menschen in den einzelnen L\u00e4ndern. Weltweit gibt es bei Menschen mit niedrigem Einkommen (<5000 Dollar/Jahr) eine gro\u00dfe Variabilit\u00e4t in Bezug auf die Besch\u00e4ftigung in der Landwirtschaft: die H\u00e4lfte der Bev\u00f6lkerung des Tschad arbeitet in der Landwirtschaft, w\u00e4hrend es in Simbabwe nur 10 % sind. Bei einem Einkommen von mehr als 15.000 Dollar/Jahr gibt es jedoch kaum Schwankungen beim Prozentsatz der Menschen, die in der Landwirtschaft arbeiten: er ist immer sehr niedrig. W\u00fcrde man dies grafisch darstellen, so w\u00fcrde man sehen, dass die Datenpunkte in den unteren x-Werten (mit x als Einkommen) eher breit gestreut sind, in den h\u00f6heren Einkommensbereichen (= x-Werte) jedoch linearer verteilt sind. Dies hat Gr\u00fcnde, und es gibt wahrscheinlich eine oder mehrere Variablen, die diese Variabilit\u00e4t erkl\u00e4ren. Vielleicht gibt es andere Faktoren, die einen st\u00e4rkeren Einfluss auf den Anteil der Landwirte in den unteren Einkommensgruppen haben als in den h\u00f6heren Einkommensgruppen, wo das Einkommen ein guter Pr\u00e4diktor ist. \n\nEine Korrelationsanalyse hilft uns, solche Schwankungen in der Datenbeziehung zu erkennen, und wir sollten die Korrelationskoeffizienten und die Richtung der Beziehung f\u00fcr verschiedene Teile der Daten betrachten. Oft gibt es eine st\u00e4rkere Beziehung in Teilen des Datensatzes und eine schw\u00e4chere Beziehung in anderen Teilen des Datensatzes. Diese Unterschiede sind wichtig, da sie auf zugrunde liegende Einflussvariablen oder Faktoren hindeuten, die wir noch nicht verstanden haben.\n\n[[File:Correlation coefficient examples.png|600px|thumb|center|'''Beispiele f\u00fcr unterschiedliche Korrelationskoeffizienten.''' Source: Wikipedia, Kiatdd, CC BY-SA 3.0]]\n<br>\n\n\n== Kausalit\u00e4t ==\nWie soll man hier Anfang und Ende finden?\n\n'''Kausalit\u00e4t ist eines der am meisten missbrauchten und missverstandenen Konzepte in der Statistik.''' Dabei ist es der Kern der Tatsache, dass alle Statistiken normativen Charakter haben. W\u00e4hrend viele Dinge kausal miteinander verbunden sein k\u00f6nnen, sind es viele nicht. Das Problem besteht darin, dass wir uns w\u00fcnschen, dass bestimmte Dinge kausal miteinander verbunden sind, w\u00e4hrend wir wollen, dass andere Dinge nicht kausal miteinander verbunden sind. Diese Verwirrung hat viele Wurzeln und erstreckt sich \u00fcber so unb\u00e4ndige Bereiche wie Glauben, Psychologie, Kultur, soziale Konstrukte und so weiter. Kausalit\u00e4t kann alles sein, was an der Statistik gut ist, und sie kann ebenso alles sein, was an der Statistik falsch ist. Mit anderen Worten: Sie kann alles sein, was an uns Menschen gut ist, aber sie kann auch die Ursache f\u00fcr alles sein, was bei uns falsch ist.\n\nWas ist attraktiv an der Kausalit\u00e4t? Menschen suchen nach Erkl\u00e4rungen, und diese st\u00e4ndige Suche ist wahrscheinlich einer der Bausteine unserer Zivilisation. Menschen suchen nach Gr\u00fcnden, um Ph\u00e4nomene und Muster zu erkl\u00e4ren, oft mit dem Ziel der Vorhersage. Wenn ich einen Kausalzusammenhang verstanden habe, kann ich vielleicht mehr \u00fcber die Zukunft wissen und davon profitieren, entweder auf diese Zukunft vorbereitet zu sein oder zumindest richtig reagieren zu k\u00f6nnen. \n\nDas Problem mit der Kausalit\u00e4t besteht darin, dass sowohl verschiedene Wissenschaftszweige als auch verschiedene philosophische Str\u00f6mungen unterschiedliche Erkl\u00e4rungen f\u00fcr Kausalit\u00e4t haben, und es gibt eine aufregende Vielfalt von Theorien \u00fcber Kausalit\u00e4t. Lassen Sie uns das Thema systematisch angehen.\n\n==== Die ''high road'' und die ''low road of causality'' ====\nNehmen wir den ersten Extremfall: die Theorie, dass St\u00f6rche die Babys bringen. Offensichtlich ist dies nicht wahr. Einen kausalen Zusammenhang zwischen diesen beiden herzustellen, ist offensichtlich ein Fehler. Nehmen wir nun den anderen Extremfall: Sie fallen eine Treppe hinunter und brechen sich beim Sturz das Bein. Es besteht offensichtlich eine Form des Kausalzusammenhangs zwischen diesen beiden Handlungen, n\u00e4mlich dass Sie sich beim Sturz die Treppe das Bein gebrochen haben. Dies erfordert jedoch bereits ein gewisses Ma\u00df an Abstraktion, einschlie\u00dflich der Annahme, dass Sie es waren, der die Treppe hinuntergefallen ist, dass Sie Ihr Bein verdreht oder mit gen\u00fcgend Kraft auf eine Treppe geschlagen haben, dass Sie nicht zu schwach waren, um dem Aufprall standzuhalten usw. Es gibt also eine sehr detaillierte Kette von Ereignissen, die sich abspielt, wenn Sie anfangen, das Gleichgewicht zu verlieren, und sich das Bein brechen. Unser Verstand vereinfacht dies in \"weil ich die Treppe hinuntergefallen bin, habe ich mir das Bein gebrochen\". Nat\u00fcrlich geben wir weder der Person, die die Treppe gebaut hat, noch unseren Eltern die Schuld daf\u00fcr, dass sie uns in diese Welt gebracht haben, wo wir uns dann das Bein gebrochen haben. Diese Dinge stehen nicht in kausalem Zusammenhang. \n\nAber stellen Sie sich nun vor, dass der Bauarbeiter die Treppe nicht richtig konstruiert hat und dass eine Treppenstufe etwas h\u00f6her ist als die andere. Wir behaupten nun, dass es die Schuld des Bauarbeitenden ist. Wie viel h\u00f6her muss diese eine Treppenstufe jedoch sein, damit wir nicht uns selbst die Schuld geben, sondern dem Bauarbeitenden? Sie haben den Punkt verstanden.\n\n'''Kausalit\u00e4t ist eine Konstruktion, die sich in unserem Kopf abspielt.''' Wir schaffen ein abstraktes Weltbild, und in diesem abstrakten Weltbild entwerfen wir eine Version der Wirklichkeit, die vereinfacht genug ist, um beispielsweise zuk\u00fcnftige Ereignisse zu erkl\u00e4ren, aber es ist nicht zu einfach, da wir damit nichts Bestimmtes oder kleinere Gruppen von Ereignissen erkl\u00e4ren k\u00f6nnten. \n\n[[File:860-header-explainer-correlationchart.jpg|500px|thumb|left|'''Korrelationen k\u00f6nnen t\u00e4uschen.''' Quelle: [http://www.tylervigen.com/spurious-correlations Spurious Correlations]]]\n\nKausalit\u00e4t ist also eine Abstraktion, die auf [[Why_statistics_matters#Occam.27s_razor|''Occam's Razor'']] folgt, schlage ich vor. Und da wir alle unsere eigene Version von Ockhams Rasiermesser im Kopf haben, sind wir oft anderer Meinung, wenn es um Kausalit\u00e4t geht. Ich glaube, das h\u00e4ngt nur damit zusammen, dass sich bei der Analyse alles aufl\u00f6st. Wenn wir irgendeinen Zusammenhang zwischen Ereignissen analysieren, dann kann sich auch der kausale Zusammenhang aufl\u00f6sen oder irrelevant werden. Letztendlich ist die Kausalit\u00e4t eine Wahl. \n\nNehmen wir das Beispiel von [https://sciencebasedmedicine.org/evidence-in-medicine-correlation-and-causation/ medizinischen Studien], wo die meisten Studien auf einem korrelativen Design aufbauen und testen, wie zum Beispiel die Einnahme von Ibuprofen gegen Erk\u00e4ltung helfen kann. Wenn ich Kopfschmerzen habe und Ibuprofen einnehme, kann es mir in den meisten F\u00e4llen helfen. Aber verstehe ich, wie es mir hilft? Ich verstehe vielleicht einige Teile davon, aber auf zellul\u00e4rer Ebene verstehe ich es nicht wirklich. Es gibt wieder eine gewisse Abstraktionsebene.\n\nWas jetzt f\u00fcr die Kausalit\u00e4t am relevantesten ist, ist die blo\u00dfe Tatsache, dass eine Sache durch eine andere Sache erkl\u00e4rt werden kann. Wir m\u00fcssen nicht alles in allen Einzelheiten verstehen, und ich habe oben bereits gesagt, dass dies letztlich sehr schwer f\u00fcr uns w\u00e4re. Stattdessen m\u00fcssen wir verstehen, ob das Wegnehmen der einen Sache verhindert, dass die andere Sache geschieht. Wenn ich nicht die Treppe hinuntergegangen w\u00e4re, h\u00e4tte ich mir nicht das Bein gebrochen. \n\nSeit Aristoteles und seiner Frage \"Was ist seine Natur?\" nervt uns die Knappheit der wahren Kausalit\u00e4t oder tiefen Kausalit\u00e4t. Ich schlage vor, dass es eine '''high road of causality''' und eine '''low road of causality''' gibt. Die ''high road'' erm\u00f6glicht es uns, alles dar\u00fcber zu erkl\u00e4ren, wie zwei Dinge oder Ph\u00e4nomene miteinander verbunden sind. Obwohl ich dies ablehne, halten ihn viele Denkschulen f\u00fcr sehr relevant. Ich f\u00fcr meinen Teil bevorzuge die ''low road'': Ist eine Sache oder ein Ph\u00e4nomen kausal mit einer anderen Sache oder einem anderen Ph\u00e4nomen verbunden; wenn ich das eine wegnehme, wird das andere nicht geschehen. Das bedeutet automatisch, dass ich Kompromisse eingehen muss, wie viel ich von der Welt verstehe. \n\nIch schlage vor, dass wir nicht alles verstehen m\u00fcssen. Unsere Vorfahren haben nicht wirklich verstanden, warum es in einem Gebiet mit vielen Raubtieren zum Tod f\u00fchren kann, wenn man im Dunkeln ohne Fackel und Waffe - oder besser noch in einer gr\u00f6\u00dferen Gruppe - hinausgeht. Es gen\u00fcgte ihnen, zu wissen, dass sie in der Nacht in Sicherheit sein w\u00fcrden. Das gen\u00fcgt mir auch.\n\n==== Einfache und komplexe Kausalit\u00e4t ====\nVersuchen wir, die kausalen Zusammenh\u00e4nge Schritt f\u00fcr Schritt zu verstehen. Zu diesem Zweck k\u00f6nnen wir kurz zwei Arten von kausalen Beziehungen unterscheiden.\n \nStatistische Korrelationen implizieren Kausalit\u00e4t, wenn eine Variable (A) aktiv durch eine Variable (B) getrieben wird. Wenn B weggenommen oder ver\u00e4ndert wird, ver\u00e4ndert sich auch A oder wird nicht mehr vorhanden. Diese Beziehung geh\u00f6rt zu den am h\u00e4ufigsten bekannten Beziehungen in der Statistik, aber sie hat gewisse Probleme. In erster Linie k\u00f6nnen zwei Variablen kausal miteinander verbunden sein, aber die Beziehung kann schwach sein. [https://www.ncbi.nlm.nih.gov/pubmed/23775705 Zink] kann sicherlich gegen die Erk\u00e4ltung helfen, aber es ist nicht garantiert, dass Zink uns heilen wird. Es ist eine schwache kausale Korrelation. \n\nZweitens kann eine kausale Beziehung der Variablen A und B mit einer Variablen C und weiteren Variablen D, E, F usw. interagieren. In diesem Fall sprechen viele Menschen von komplexen Beziehungen. Komplexe Beziehungen k\u00f6nnen kausal sein, aber sie sind dennoch komplex, und [[Agency, Complexity and Emergence (German)|diese Komplexit\u00e4t kann Menschen verwirren]]. Schlie\u00dflich k\u00f6nnen statistische Beziehungen durch Bias, Beschr\u00e4nkungen der Stichprobengr\u00f6\u00dfe und viele andere Herausforderungen, mit denen die Statistik konfrontiert ist, beeinflusst werden. Diese Herausforderungen sind bekannt, zunehmend untersucht, aber oft nicht l\u00f6sbar.\n\n==== Struktur ins Chaos bringen: Normativit\u00e4t und Plausibilit\u00e4t ====\n[[File:Black swan.jpg|thumb|right|Schw\u00e4ne sind vermutlich das bekannte Beispiel f\u00fcr eine Universaltheorie, die durch ein Gegenbeispiel widerlegt werden kann. Sind alle Schw\u00e4ne wei\u00df? Es mag trivial erscheinen, aber der schwarze Schwan repr\u00e4sentiert [https://www.youtube.com/watch?v=XlFywEtLZ9w Karl Popper's Falsifizierung|, ein wichtiges Prinzip wissenschaftlicher Arbeit.]]\nAusgehend von dem Gedanken, dass unser Verstand umherwandert, um kausale Zusammenh\u00e4nge zu finden, und dann mit [[Experiments and hypothesis|Experimenten und Hypothesen]] als m\u00e4chtige Werkzeuge, begannen Wissenschaftler*innen damit, Theorien abzuleiten und zu revidieren, die auf Versuchsanordnungen basierten. Manchmal war es auch umgekehrt, da viele Theorien erst sp\u00e4ter durch Beobachtung oder wissenschaftliche Experimente bewiesen wurden. Nachdem die Kausalit\u00e4t durch wissenschaftliche Theorien erkl\u00e4rt worden war, entstand eine Kombination, die unter vielen anderen Dingen zu physikalischen Gesetzen, gesellschaftlichen Paradigmen und psychologischen Modellen f\u00fchrte. \n\nDie Plausibilit\u00e4t begann ihre Herrschaft als ein Schl\u00fcsselkriterium der modernen Wissenschaft. Plausibilit\u00e4t bedeutet grunds\u00e4tzlich, dass Beziehungen nur dann kausal sein k\u00f6nnen, wenn die Beziehungen nicht nur wahrscheinlich, sondern auch [https://justinhohn.typepad.com/blog/2013/01/milton-friedmans-thermostat-analogy.html vern\u00fcnftig] sind. Um die Wahrscheinlichkeit k\u00fcmmert sich die Statistik. Aber es ist der menschliche Verstand, der die Vernunft aus den Daten ableitet, was die Kausalit\u00e4t zu einem zutiefst normativen Akt macht. Kontrafaktische Theorien k\u00f6nnen sp\u00e4ter unsere Kausalit\u00e4t widerlegen, weshalb wir keine absolute Wahrheit kennen k\u00f6nnen, aber wir k\u00f6nnen uns ihr ann\u00e4hern. Unsere Annahmen k\u00f6nnen auch sp\u00e4ter noch falsifiziert werden.\n \nSo weit, so gut. Es ist erw\u00e4hnenswert, dass Aristoteles einige interessante metaphysische Ans\u00e4tze zur Kausalit\u00e4t hatte, ebenso wie Buddhist*innen und Hindus. Wir werden [[Big problems for later| diese hier der Einfachheit halber ignorieren]].\n\n==== Hume's Kriterien f\u00fcr Kausalit\u00e4t ====\n[[File:David Hume.jpg|thumb|300px|left|'''David Hume''']]\nEs scheint offensichtlich, aber eine notwendige Bedingung f\u00fcr Kausalit\u00e4t ist die zeitliche Ordnung (Neumann 2014, S.74-78). Zeitliche Kausalketten k\u00f6nnen als Beziehungen definiert werden, in denen eine Wirkung eine Ursache hat. Ein Ereignis A kann direkt auf eine Handlung B folgen. Mit anderen Worten, A wird durch B verursacht. Wir denken oft, dass wir solche Kausalbeziehungen in einer zeitlichen Kette verwurzelt sehen. Die komplexe Debatte \u00fcber [https://www.cdc.gov/vaccinesafety/concerns/autism.html Impfungen und Autismus] kann als ein solches Beispiel angesehen werden. Impfgegner*innen sind der Meinung, dass Autismus durch Impfungen verursacht wurde, w\u00e4hrend Mediziner*innen argumentieren, dass der Ausbruch von Autismus lediglich zur gleichen Zeit stattfand, wie die Impfungen als Teil des notwendigen Schutzes unserer Gesellschaft vorgenommen werden. Der zeitliche Zusammenhang wird in diesem Fall als reiner Zufall angesehen. Viele solcher zeitlichen Beziehungen werden daher angenommen, aber unser Verstand t\u00e4uscht uns oft, da wir uns t\u00e4uschen lassen wollen. Wir wollen Ordnung im Chaos, und f\u00fcr viele Menschen ist Kausalit\u00e4t Gl\u00fcckseligkeit. Die Geschichte versucht oft, Kausalit\u00e4ten zu finden, doch wie einst behauptet wurde, wird die Geschichte von den Siegreichen geschrieben, d.h. sie ist subjektiv. Ein Modell zu haben, das die eigene Realit\u00e4t erkl\u00e4rt - danach suchen viele heute, und eine Art zeitliche Kausalkette zu haben, scheint eine der Sehns\u00fcchte vieler Menschen zu sein. Wissenschaftliche Experimente wurden erfunden, um solche Kausalit\u00e4ten zu testen, und die menschliche Gesellschaft entwickelte sich. Heute scheint es nahezu unm\u00f6glich zu sein, nichts \u00fcber die Schwerkraft zu wissen - in gewissem Sinne wissen wir das alle, ja - aber die ersten physikalischen Experimente haben uns geholfen, dies zu beweisen. So half uns das wissenschaftliche Experiment, die zeitliche Kausalit\u00e4t zu erforschen, und [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3835968/ medizinische Studien] k\u00f6nnen als eine der am st\u00e4rksten vorangetriebenen Fortsetzungen davon angesehen werden.\n\nWir k\u00f6nnen jedoch auf Hume aufbauen, der in seiner Abhandlung \u00fcber die menschliche Natur die drei unten erw\u00e4hnten Kriterien benannte. Um seine Worte zu paraphrasieren: Die Kausalit\u00e4t ist in Raum und Zeit zusammenh\u00e4ngend, die Ursache liegt vor der Wirkung, und es besteht eine st\u00e4ndige Verbindung zwischen Ursache und Wirkung. Es lohnt sich, auch die anderen von ihm erw\u00e4hnten Kriterien zu betrachten.\n \n1. Hume behauptet, dass die gleiche Ursache immer die gleiche Wirkung hervorruft. In der Statistik weist dies auf das Kriterium der Reproduzierbarkeit hin, das eines der R\u00fcckgrate des wissenschaftlichen Experiments ist. Dies mag in Zeiten von Einzelfallstudien als schwierig angesehen werden, unterstreicht aber gleichzeitig, dass der Wert solcher Studien eindeutig relevant, aber nach diesem Kriterium begrenzt ist.\n\n2. Wenn mehrere Objekte denselben Effekt hervorrufen, muss es dar\u00fcber hinaus ein verbindendes Kriterium unter ihnen geben, das den Effekt verursacht. Ein gutes Beispiel daf\u00fcr sind Gewichte auf einer Waage. Mehrere Gewichte k\u00f6nnen addiert werden, um die gleiche - gegenl\u00e4ufige - Wirkung zu erzielen. Man m\u00fcsste eine hohe Anzahl von Federn stapeln, um die gleiche Wirkung zu erzielen wie, sagen wir, 50 kg Gewicht (und es w\u00e4re bedauerlich f\u00fcr die V\u00f6gel). Auch dies ist f\u00fcr die moderne Wissenschaft von gro\u00dfer Bedeutung, da die Suche nach einheitlichen Kriterien bei den Datenmengen, die wir heutzutage oft analysieren, ein Schl\u00fcsselziel ist.\n\n3. Wenn zwei Objekte eine unterschiedliche Wirkung haben, muss es einen Grund geben, der den Unterschied erkl\u00e4rt. Diese dritte Annahme von Hume ist eine direkte Konsequenz aus der zweiten. Was Faktoren vereint, kann wichtig sein, aber ebenso wichtig kann sein, was Faktoren unterscheidet. Dies ist oft der Grund, warum wir glauben, dass es einen kausalen Zusammenhang zwischen zwei Dingen gibt, obwohl es eindeutig keinen gibt. Stellen Sie sich vor, eine b\u00f6se Person gibt Ihnen koffeinfreien Kaffee, w\u00e4hrend Sie an Koffein gew\u00f6hnt sind. Die Wirkung k\u00f6nnte schwerwiegend sein, und es ist klar, dass das Koffein in dem Kaffee, der Sie aufweckt, diesen Kaffee von dem entkoffeinierten Kaffee unterscheidet.  \n \nZusammenfassend l\u00e4sst sich sagen, dass die Kausalit\u00e4t ein Chaos in unseren Gehirnen ist. '''Wir t\u00e4uschen uns \u00f6fter \u00fcber die Kausalit\u00e4t, als wir denken, unser Gehirn ist fest verdrahtet, Verbindungen zu finden, und l\u00e4sst sich oft zur Annahme einer Kausalit\u00e4t verleiten.'''. Gleicherma\u00dfen wollen wir oft die Kausalit\u00e4t vernachl\u00e4ssigen, wenn sie eindeutig eine Tatsache ist. Wir irren uns also recht oft. Bleiben Sie im Zweifelsfall bei [https://statisticsbyjim.com/basics/causation/ Hume] und den oben genannten Kriterien. Sich bei der Analyse von Korrelationen auf diese zu verlassen, erfordert am Ende des Tages einfach \u00dcbung.\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "hfouckz50svrlgbi5s6rwzgzcixyob4"
                }
            },
            {
                "title": "Check In",
                "ns": "0",
                "id": "374",
                "revision": {
                    "id": "3247",
                    "parentid": "2337",
                    "timestamp": "2020-11-04T10:18:15Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4059",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n[[File:Chairs.jpg|thumb]]\n== What, Why & When ==\nA check-in at the beginning of each meeting, workshop or seminar can help teams and groups to get a better understanding of each other's energy level, personal environment or the status of work. You can basically use it with every group of people, even your clique or your family. The check-out helps to summarize decisions, distributed tasks and supports evaluating the atmosphere in the group. Both the check-in and check-out can become a habit for each meeting.\n\n== Goal(s) ==\n* create understanding of each other's personal situation\n* enhance empathy for the team members\n* get updated on each other's work and tasks\n* prevent misunderstandings\n\n== Description ==\nIn a check-in, different questions can be answered which have to be defined by the moderator of the meeting. Usually, at the beginning of the meeting, as soon as everybody has arrived and settled, the group sits in a circle. Then, the moderator asks each person to share how they are feeling, if something bothers them and invites them to answer a question such as \"The most inspiring thing I learned this week was...\". You should set a rough time limit to each answer, e.g. three minutes, to keep the check-in short. If somebody is rambling too long, as a moderator, you should remind them of the time limit.\n\nFor the check-out, you can also modify the question according to the situation. Sometimes, it's useful to ask what everybody has learned or what their take-home message is from the meeting. You can also ask whether there are any unanswered questions and how the energy level is. Further, you can ask for a quick feedback on the meeting or the session.\n\nBoth the check-in and check-out should not last longer than 20-30 minutes. If your group is too big, you can split it up in smaller groups.\n\n== Rules ==\n* no commenting on each other's statements \u2192 comparable to the flashlight method\n* active [[Listening|listening]]\n* you can go clockwise and if somebody does not want to say something, it's also fine\n* check-ins and check-outs are a routine, not a one time thing - try to establish it as the normal process of starting and finishing a meeting\n* this time belongs to personal insights and is not meant to complete tasks or make arrangements\n\n== Potential Pitfalls ==\nIt may happen that - especially in meetings in which the members do not know each other well - the members might not want to share their very personal stories or want to be honest about their current feelings. It can help that you as a moderator start the round and share a private insight if you like. Additionally, you should mention that the members should only share what they feel comfortable with.\n\nTeams that work together for a longer time, already know each other and had developed problems and arguments before, might raise them in a check-in. Hence, as a moderator of established teams, always plan some back-up time to discuss challenges or arguments in the team.\n\n== Links & Further Reading ==\n* [https://www.tlnt.com/for-more-productive-meetings-do-a-mindset-check-in-first/ For More Productive Meetings, Do a Mindset Check-In First]\n* [https://checkin.daresay.io/ Generator for Check-In questions]\n* [https://academy.nobl.io/why-a-quick-check-in-makes-meetings-more-effective/ Why a Quick Check-In Makes Meetings More Effective]\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Alexa B\u00f6ckel."
                    },
                    "sha1": "acllb96cyzyrc24cpr5qko0mc4cb4eo"
                }
            },
            {
                "title": "Chord Diagram",
                "ns": "0",
                "id": "682",
                "revision": {
                    "id": "4820",
                    "parentid": "4819",
                    "timestamp": "2021-03-30T08:59:46Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7258",
                        "#text": "'''Note:''' This entry revolves specifically around Chord Diagrams. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA chord diagram represents flows or connections between several entities (called nodes). Each entity is represented by a fragment on the outer part of the circular layout. Then, arcs are drawn between each entities. The size of the arc is proportional to the importance of the flow. \n\n=Why use chord diagram?=\nChord diagram is a good way to represent the migration flows. It works well if your data are directed and weighted like for migration flows between country.\nChord diagrams can answer questions about your data, such as the following:\n* What is the volume of flow between categories?\n* Are there anomalies, differences, or similarities in the volume of flow?\n\n=Basic usage of making Chord diagram=\nIn '''R''', the '''circlize''' package is the best option to build it. The '''chordDiagram()''' function of the circlize package makes it a breeze to build chord diagrams.\n\nFirst let\u2019s generate a random matrix\n[[File:Default_Chord_Diagram.png|450px|thumb|right|Fig.1]]\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"circlize\")\nlibrary(circlize)\n\n#Fig.1\nset.seed(999)\nmat = matrix(sample(18, 18), 3, 6) \nrownames(mat) = paste0(\"S\", 1:3)\ncolnames(mat) = paste0(\"E\", 1:6)\nmat\nchordDiagram(mat)\n## Output:\n##\n##    E1 E2 E3 E4 E5 E6\n## S1  4 14 13 17  5  2\n## S2  7  1  6  8 12 15\n## S3  9 10  3 16 11 18\n\n</syntaxhighlight>\n\nThe default Chord Diagram consists of a track of labels, a track of grids (or you can call it lines, rectangles) with axes, and links. Sectors which correspond to rows in the matrix locate at the bottom half of the circle.In following code, S1, S2 and S3 should better be put together since they belong to a same group, which is same for E* sectors. Of course, you can give a order which mix S* and E*\n\n<syntaxhighlight lang=\"R\" line>\npar(mfrow = c(1, 2))\nchordDiagram(mat, order = c(\"S2\", \"S1\", \"S3\", \"E4\", \"E1\", \"E5\", \"E2\", \"E6\", \"E3\"))\ncircos.clear()\n#Fig.2\nchordDiagram(mat, order = c(\"S2\", \"S1\", \"E4\", \"E1\", \"S3\", \"E5\", \"E2\", \"E6\", \"E3\"))\n\n\n</syntaxhighlight>\n[[File:Chord_Diagram2.png|500px|thumb|center|Fig.2]]\n\n<syntaxhighlight lang=\"R\" line>\n## NOTE: circos.clear() is used to reset the circular layout parameters\ncircos.clear()\n</syntaxhighlight>\n\nUnder default settings, the grid colors which represent sectors are randomly generated, and the link colors are same as grid colors which correspond to rows (or the first column if the input is an adjacency list) but with 50% transparency.\n\n=Adjust by circos.par()=\nSince Chord Diagram is implemented by basic circlize functions, like normal circular plot, the layout can be customized by '''circos.par()'''. The gaps between sectors can be set by '''circos.par(gap.after = ...)'''. It is useful when you want to distinguish sectors between rows and columns. Please note since you change default graphical settings, you need to use '''circos.clear()''' in the end of the plot to reset it.\n[[File:Chord_Diagram3.png|200px|thumb|left|Fig.3]]\n[[File:Chord_Diagram4.png|200px|thumb|right|Fig.4]]\n[[File:Chord_Diagram5.png|200px|thumb|right|Fig.5]]\n<syntaxhighlight lang=\"R\" line>\n\n#Fig.3\n\ncircos.par(gap.after = c(rep(5, nrow(mat)-1), 15, rep(5, ncol(mat)-1), 15))\n\nchordDiagram(mat)\n\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"R\" line>\n\ncircos.clear()\n\n# here is an example for how to add the gap between arcs\ncircos.par(gap.after = c(\"S1\" = 5, \"S2\" = 5, \"S3\" = 15, \"E1\" = 5, \"E2\" = 5,\n                         \"E3\" = 5, \"E4\" = 5, \"E5\" = 5, \"E6\" = 15))\n\n\n#Fig.4\nchordDiagram(mat) \ncircos.clear()\n\n\n#Fig.5\nchordDiagram(mat, big.gap = 30) \n</syntaxhighlight>\n\n= Set grid colors =\n[[File:Chord_Diagram6.png|200px|thumb|left|Fig.6]]\nGrids have different colors to represent different sectors. Generally, sectors are divided into two groups. One contains sectors defined in rows of the matrix , and the other contains sectors defined in columns of the matrix. Thus, links connect objects in the two groups. By default, link colors are same as the color for the corresponding sectors in the first group.Changing colors of grids may change the colors of links as well. Colors for grids can be set through grid.col argument. Values of grid.col better be a named vector of which names correspond to sector names. If it is has no name index, the order of grid.col is assumed to have the same order as sectors.\n\n<syntaxhighlight lang=\"R\" line>\n#Fig. 6\ngrid.col = c(S1 = \"red\", S2 = \"green\", S3 = \"blue\",\n             E1 = \"grey\", E2 = \"grey\", E3 = \"grey\", \n             E4 = \"grey\", E5 = \"grey\", E6 = \"grey\")\nchordDiagram(mat, grid.col = grid.col)\n</syntaxhighlight>\n\n[[File:Chord_Diagram7.png|200px|thumb|left|Fig.7]]\n[[File:Chord_Diagram8.png|200px|thumb|right|Fig.8]]\nIf you want colors to be the same as the sectors from the matrix columns or the second column in the data frame, simply transpose the matrix.\n<syntaxhighlight lang=\"R\" line>\n#Fig. 7\nchordDiagram(t(mat), grid.col = grid.col)\n</syntaxhighlight>\n\nIf you want to add radiant colors instead of transparent then you can use the '''transparency''' argument in the '''chordDiagram()''' function which value can be 1 or by default the value is set to be 0.5.\n<syntaxhighlight lang=\"R\" line>\n#Fig. 8\nchordDiagram(mat, grid.col = grid.col, transparency = 0)\n</syntaxhighlight>\n\n[[File:Chord_Diagram9.png|200px|thumb|right|Fig.9]]\n=Link border=\n'''link.lwd''', '''link.lty''' and '''link.border''' control the line width, the line style and the color of the link border. All these three parameters can be set either a single scalar or a matrix if the input is an adjacency matrix. If it is set as a single scalar, it means all links share the same style.\n<syntaxhighlight lang=\"R\" line>\n#Fig. 9\nchordDiagram(mat, grid.col = grid.col, link.lwd = 2, link.lty = 2, link.border = \"red\")\n</syntaxhighlight>\n\n=Highlight links=\n[[File:Chord_Diagram10.png|200px|thumb|left|Fig.10]]\nSometimes we want to highlight some links to emphasize the importance of such relations. There are two ways to highlight links, one is to set different transparency to different links and the other is to only draw links that needs to be highlighted. We can highlight links which come from a same sector by assigning colors with different transparency by '''row.col''' argument.\n<syntaxhighlight lang=\"R\" line>\n#Fig. 10\nchordDiagram(mat, grid.col = grid.col, row.col = c(\"#FF000080\", \"#00FF0010\", \"#0000FF10\"))\n</syntaxhighlight>\n\n[[File:Chord_Diagram11.png|200px|thumb|left|Fig.11]]\nWe can also highlight links with values larger than a cut-off. There are at least three ways to do it. First, construct a color matrix and set links with small values to full transparency. Since link colors can be specified as a matrix, we can set the transparency of those links to a high value or even set to full transparency. In following example, links with value less than 12 are set to #00000000.\n<syntaxhighlight lang=\"R\" line>\n#Fig. 11\ncol_mat = rand_color(length(mat), transparency = 0.5)\ncol_mat[mat < 12] = \"#00000000\"\nchordDiagram(mat, grid.col = grid.col, col = col_mat) \n</syntaxhighlight>\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "5d3qnrsz3l6t0a7px7lfwmz0mkv98bb"
                }
            },
            {
                "title": "Citations",
                "ns": "0",
                "id": "319",
                "revision": {
                    "id": "3248",
                    "parentid": "2317",
                    "timestamp": "2020-11-04T10:18:45Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4995",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n<big>'''Why'''</big>: Using correct citation is crucial for academic writing and the basis of all scholarly work. Therefore it is most important that you understand and apply the following rules. Whenever you research a topic, you will read other people\u2019s books and articles and use the information found there for your own interpretation of the subject. Therefore you need to make sure to respect other\u2019s intellectual property and do not violate '''copyright''' laws. Moreover, your statements become more credible if you can show that someone else has had similar results.\n\n<big>'''When'''</big>: Citations and references need to be used in all your academic works, whether these are essays, term papers, presentations, or your final thesis. Whenever you quote or paraphrase from another person\u2019s text, speech or medium, you want to hint at a certain publication or passage, or when you have taken information from another text you must credit your source.\n\n== Goal(s) ==\n* Make the sources we used for our own work visible and trackable, and our statements verifiable.\n* Give credit to the original author or creator of a text.\n\n== Getting Started ==\nOne of the difficulties of correct citation is that there are innumerable ways of doing it. Nearly every academic discipline and country has their own preference. The most important aspect here is that you '''stay consistent''' and do not mix several methods within one text (you can always ask your lecturers for their preferred style).<br>\n\nThere are two prominent ways of referencing you can use within your text:\n====In-Text-Citation====\nThis first method has gained more popularity in the last years, especially in international contexts, but also in Germany. By using this method the reader can keep his attention on the main text without having to skip to the bottom of the page for references. Here, a short reference in brackets is placed at the end of the respective sentence or paragraph, usually in front of any punctuation marks.\n*(Surname Year, Page) \u2192 (Copeland 1997, p. 132)\n*(Surname Title, Page) \u2192 (Copeland Money, p. 132)\n\n====Footnotes==== \nUsing footnotes for any references is especially common in the traditional humanities. They are numbered and placed at the bottom of the page which is useful if you want to keep your text neat and easy to read. Creating a footnote is quite easy, most writing programmes do it for you.\n*<sup>1</sup>Copeland 1997, p. 132.\n*<sup>2</sup>Miller 2003, p. 45. \n\u2192 Make sure to end every footnote with a full stop!\n\n===Bibliography===\nAt the end of your paper you need to compile all of your cited sources in a bibliography. This list should be ordered alphabetically using the authors\u2019 surnames. Here you give the full bibliographic information of your source containing at least author, title, year and place of publication.\nIn the bibliography we differentiate between monographies, journal articles, articles within anthologies, websites etc. Each of these categories has to be referenced differently. You can find detailed instructions in the sources listed below, but here are some examples:<br>\n\n'''MLA Style'''<br>\nEssay in a collection:\n*Last Name, First Name. \"Title.\" Title of collection, editor(s), publisher, year, page(s))<br>\n*Copeland, Edward. \u201cMoney.\u201d The Cambridge Companion to Jane Austen, edited by Copeland and Juliet McMaster, Cambridge UP, 1997, pp. 131-48.\n'''APA Style'''<br>\nMonography:\n*Last Name, Initials. (year). Title: Subtitle. Publisher.\n*Sapolsky, R. M. (2017). Behave: The biology of humans at our best and worst. Penguin Books.\n'''Chicago Style'''<br>\nWebsite:\n*Author Last Name, First Name. \u201cPage Title.\u201d Website Name. Month Day, Year. URL.\n*Scribbr. \u201cChicago Style Citation.\u201d Accessed June 16, 2020. https://www.scribbr.com/category/chicago-style/.\n\n'''Tip:''' Software like [https://www.citavi.com/de Citavi] can help you manage your references and create a bibliography for you.\n\n== Links & Further reading ==\n* citavi https://www.citavi.com/de\n* MLA Style (Modern Language Association) - mostly for humanities https://www.mla.org/\n* APA Style (American Psychological Association) - preferred for scientific papers https://apastyle.apa.org/\n* Chicago Style - for social sciences https://www.chicagomanualofstyle.org/home.html\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Katharina Kirn."
                    },
                    "sha1": "rji5xju2jowrhyxsq0cy3dzpu869r62"
                }
            },
            {
                "title": "Citizen Science",
                "ns": "0",
                "id": "243",
                "revision": {
                    "id": "6730",
                    "parentid": "6266",
                    "timestamp": "2022-07-15T08:07:10Z",
                    "contributor": {
                        "username": "Annrau",
                        "id": "128"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14963",
                        "#text": "[[File:ConceptCitizenScience.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization for Citizen Science]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>\n__NOTOC__\n<br/>\n'''In short:''' Citizen Science is the conduct of research by members of the general public, usually in collaboration with, or under the direction of, professional researchers.\n\n== Background ==\n[[File:ResultVisualisationCitizenScience.png|500px|thumb|right|Relative abundance of eastern phoebe (''Sayornis phoebe'') in the US between 2004 and 2007, based on data gathered by observations of amateur birders. Source: Bonney et al. 2009 p.982]]\n\nThe [[Glossary|concept]] Citizen Science publicly gained prominence by two different approaches. The most prominent approach in environmental science to this day goes back to the '''US ornithologist Rick Bonney''' from Cornell University who, starting in the late 1990s, framed Citizen Science as a form of public [[Glossary|participation]] in scientific activity, especially in the form of data gathering done by amateurs (1, 2). This understanding is rooted in the 1960s when Cornell researchers already included volunteers in ornithological research (1, 5). Even before, one could argue that the difference between scientists and amateurs was blurry, as naturalists - for instance in the Victorian age - were often not trained scientists yet a vital source of data (8). \n\nA second famous approach is based on the work of '''UK sociologist Alan Irwin''' who, in 1995 (see Key Publications), framed Citizen Science as \"developing concepts of scientific citizenship which foregrounds the necessity of opening up science and science policy processes to the public\" (Riesch & Potter 2014, p.1), focusing on the role of citizens as stakeholders in informed [[Glossary|decision-making]] processes. \n\n[[File:Citizen Science.png|400px|thumb|right|'''SCOPUS hits per year for Citizen Science until 2019.''' Search term: 'Citizen Science' in Title, Abstract, Keywords. Source: own.]]\n\nPartly due to these different backgrounds of the concept, \u201c[t]he meaning of \"citizen science\u201d is in fact not very clear, particularly when formulated on a science policy level, where it is often defined too broadly without making the distinctions that scientists work with\u201d (Kullenberg & Kasperowski 2016, p.2). Terms such as 'participatory science', 'civic science' or 'community-based monitoring' may also be understood as a form of Citizen Science (1). \n\nToday, Citizen Science is frequently used in biology, conservation and ecology (topics include e.g. birds, insects or water quality), followed by geographic information research and epidemiology (1, 2, 6, 8). It has also found other fields, including Astronomy, Agriculture, Arts and Education (see for example [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6410884/ this agricultural study] and [https://www.aavso.org/variable-stars-main this participatory approach] in Astronomy.) Its presence in scientific publications has increased significantly since the early 2000s, which may be attributed to an increased usage of digital platforms for citizen engagement (1).\n\n== What the method does ==\n\n''Annotation:'' While the following explanation of Citizen Science focuses on Rick Bonney's approach of amateurs gathering data, the differences between the two predominant understandings will be addressed below.\n\nIn the sense of Bonney, Citizen Science describes '''a form of data gathering that is done by amateur enthusiasts''' in the field, in cities or gardens. Observations are noted in data forms based on instructions in pre-determined protocols stating the how, when and where of data gathering (4). The data are transmitted online to the researchers and can often be accessed in form of visualisations (e.g. maps) by the contributors afterwards. The intended outcome of a Citizen Science project can range from increasing general data availability and scientific outreach to solving specific local problems or promoting public interest and knowledge on a certain topic. (7)\n\nCitizen Science's unique characteristic is that it enables researchers to sample data across different scales - from regional up to global spatial scales - as well as repeated measurements spanning several years. This allows a scaling of data based on contribution of many participants, allowing a broader and more holistic perspective that would be not possible solemnly on the workload of the researchers.  Bird-watching may take place in gardens of a smaller area for one season while the method can likewise integrate geographic information upscaled to a global scale over several years. Often, the collected data is numeric, e.g. number of birds counted, and at which date. This allows to accumulate larger observations about bird migration.\n\n==== Differences between Irwin and Bonney ====\n\n* In his 1995 book, Alan Irwin states that \"Citizen Science tries to find a way through the usual monolithic representations of \u2018science\u2019 and the \u2018public\" (Irwin 1995, p.ix). To him, \"such matters as policy are not merely 'applications' of academic analysis nor are they simply 'implementations' - instead the whole question of how interventions are made is a major issue in itself.\" (Irwin 1995, p.xi). His conceptualization addresses how public needs and knowledge can be implemented into scientific processes in order to open up the barriers between the two sub-systems. To him, ''Citizen Science'' means \"a form of science developed and enacted by citizens themselves\" (Irwin 1995, p.xi).\n* Rick Bonney, on the other hand, describes ''Citizen Science'' as a method that \"enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time\" (Bonney et al. 2009, p.977). This understanding has a stronger methodological perspective on the approach as opposed to Irwin whose definition also refers to science policy.\n* Overall, ''Citizen Science'' may be seen both as a form of data gathering conducted by amateurs but also concerns the analysis of data and, even more so, its interpretation and application in direct contact with public actors. Wiggins & Crowston (2011) summarize this diversity within the concept by defining ''Citizen Science'' as \"a form of research collaboration involving members of the public in scientific research projects to address real-world problems.\" (Wiggins & Crowston 2011, p.1). This combines the diverging perspectives and allows for a broader understanding of the concept's essentials. In a way, the [[Glossary|emergence]] of [[Glossary|transdisciplinary]] research may be seen as a concession to Irwin: while ''Citizen Science'' is today predominantly applied in Bonney's way, Irwin's idea of involving public actors into scientific processes is highly relevant - simply in a different methodological area.\n\n== Strengths & Challenges ==\n\n* Citizen Science allows for access to data that would otherwise be inaccessible (covering long periods of time and wide spatial arrays) (6, 8).\n* There is an educational quality to Citizen Science: participants not only learn about the respective subjects but also about scientific work (see Normativity).\n* The research questions addressed through Citizen Science (in form of amateur data gathering) must take into consideration that most contributors will not be able to assess complex data. Instead, simple counting or observations can be expected (4). \"Projects demanding high skill levels from participants can be successfully developed, but they require significant participant training and support materials such as training videos\" (Bonney et al. 2009, p.979).\n* In the case of bird watching (but also similar data types), errors may result from participants confusing bird species. This must be addressed through additional information material for the amateur scientists. (4)\n* Citizen science always demands some sort of supra-infrastructure or project, since the data cannot be analysed by the participating citizens.\n\n\n== Normativity\u00a0 ==\n\n==== Complexity of the method and the analysed data ====\n* The research questions addressed through Citizen Science (in form of amateur data gathering) must take into consideration that most contributors will not be able to assess complex data. Instead, simple counting or observations can be expected. \"Projects demanding high skill levels from participants can be successfully developed, but they require significant participant training and support materials such as training videos\" (Bonney et al. 2009, p.979).\n\n==== Everything normative related to this method ====\n* A strong [[Bias and Critical Thinking|bias]] may be imposed on the data due to divergent understandings of the data gathering procedure in the respective amateur individuals, diminishing the objectivity, reliability & validity of the process. This (alleged) lack of quality in the data gathered by amateurs has led to dispute over the validity of the method for scientific investigation and publication, which is to date an ongoing debate (6). A sample error can be minimized, for instance through additional information material, training and understandable, precise protocols for data collection (4, 6, 8). Moreover, to assess the diverse kinds of impacts that citizen science can generate, a large set of different methods has developed over the last years. Therefore, Wehn et al. (2021) present a set of guiding principles for a consolidated approach on impact assessments of citizen science (9).\n* While researchers gain data through Citizen Science, the involved citizens get involved in scientific processes, strengthening their scientific literacy and learning about the subjects their are participating in (4, 7). This can also lead to positive social impacts, enabling communities to address local issues in a scientific manner and providing [[Agency, Complexity and Emergence|agency]] or even empowerment\u201a to the respective actors (6).\n* Citizen Science is a method of transdisciplinary research since it includes public actors into research processes. This element induces some normative notions that are addressed in the wiki entry on transdisciplinarity.\n\n\n== Outlook ==\n\n==== Open questions for the method ====\n* The diversity of terms and lack of precise definition should be overcome in the future to improve the application of Citizen Science (6, 7).\n* The same is stated with regards to the redundancy in Citizen Science projects. Instead of designing new projects from the ground, functioning projects should be applied to new areas of interest (6).\n\n==== Possible future developments - thoughts about the future of the method ====\n* \"Citizen Science (...) has over the past decade become a very influential and widely discussed concept with many scientists and commentators seeing it as the future of genuine interactive and inclusive science engagement (...)\" (Riesch & Potter 2014, p.107). As new areas of academia develop participatory research designs, the potential of these approaches are becoming more and more visible (8). Therefore, after years of development and despite a lack of definite conceptualization, Citizen Science should be taken seriously and supported from more areas within academia (8).\n* The spreading of Citizen Science approaches to local communities may support the incorporation of traditional knowledge into science policy and improve the science-society relationship in the future (6).\n* The growing global availability of internet access improves amateurs' access to Citizen Science projects and increases the potential availability of data for researchers (7).\n\n\n== Key Publications ==\n\n==== Theoretical ====\nIrwin, A. 1995. ''Citizen Science: A Study of People, Expertise and Sustainable Development.'' London: Routledge.\n* Debates the role of the public in science and vice versa, focusing on matters of risk and sustainable development.\nKullenberg, C. Kasperowski, D. 2016. ''What is Citizen Science? - A Scientometric Meta-Analysis.'' PLoS One 11(1)\n* Examines the different conceptualizations and uses of Citizen Science in close to 2000 articles.\nBonney, R. et al. 2009. ''Citizen Science: A Developing Tool for Expanding Science knowledge and Scientific Literacy.'' BioScience 59(11).\n* Describes an explanatory model for developing a citizen science project.\n\n==== Empirical ====\nEvans, C., E. Abrams, R. Reitsma, K. Roux, L. Salmonsen, and P. P. Marra. 2005. ''The neighborhood nestwatch program: participant outcomes of a citizen-science ecological research project.'' Conservation Biology 19:589\u2013594\n* Focuses on the scientific literacy outcomes of the citizens involved in a Citizen Science project.\n\n== References ==\n\n(1) Kullenberg, C. Kasperowski, D. 2016. ''What is Citizen Science? - A Scientometric Meta-Analysis.'' PLoS One 11(1)\n\n(2) Riesch, H. Potter, C. 2014. ''Citizen science as seen by scientists: Methodological, epistemological and ethical dimensions.'' Public Understanding of Science 23 (1) : 107- 120\n\n(3) Irwin, A. 1995. ''Citizen Science: A Study of People, Expertise and Sustainable Development.'' London: Routledge.\n\n(4) Bonney, R. et al. 2009. ''Citizen Science: A Developing Tool for Expanding Science knowledge and Scientific Literacy.'' BioScience 59(11). 977-984.\n\n(5) Cohn JP. 2008. ''Citizen science: Can volunteers do real research?'' BioScience 58 (3): 192\u2013197.\n\n(6) Bonney, R. et al. 2014. ''Next Steps for Citizen Science.'' SCIENCE 343, 1436-1437.\n\n(7) Wiggins, A. Crowston, K. 2011. ''From Conservation to Crowdsourcing: A Typology of Citizen Science.'' 2011 44th Hawaii International Conference on System Sciences. \n\n(8) Dickinson, J.L. Zuckerberg, B. Bonter, D.N. 2010. ''Citizen Science as an Ecological Research Tool: Challenges and Benefits.'' Annu. Rev. Ecol. Evol. Syst. 41. 149-172.\n\n(9) Wehn, U. et al. (2021). Impact assessment of citizen science: state of the art and guiding principles for a consolidated approach. Sustainability Science, 16(5), 1683-1699.\n\n== Further Information ==\n* For a list of interesting '''Citizen Science-'''Projects from around the world please read the [https://en.wikipedia.org/wiki/Citizen_science#History '''Wikipedia''' entry] on the method.\n\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "8j8l8om2gaz9cwonclnmy7uzf0kosmm"
                }
            },
            {
                "title": "Clustering Methods",
                "ns": "0",
                "id": "142",
                "revision": {
                    "id": "5674",
                    "parentid": "4331",
                    "timestamp": "2021-06-07T11:31:08Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Background */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15269",
                        "#text": "[[File:ConceptClusteringMethods.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.\n\n== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Clustering until 2019.''' Search terms: 'Clustering', 'Cluster Analysis' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s.\n\n== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as ''similarity measure'' or ''distance measure'', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named \"unsupervised learning\", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term \"clustering\" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.\n\n== Data Simulation ==\nThis article deals with simulated data. This section contains the function used to simulate the data. For the purpose of this article, the data has three clusters. You need to load the function on your R environment in order to simulate the data and perform clustering.\n\n<syntaxhighlight lang=\"R\" line>\ncreate_cluster_data <- function(n=150, sd=1.5, k=3, random_state=5){\n    # currently, the function only produces 2-d data\n    \n    # n = no. of observation\n    # sd = within-cluster sd\n    # k = number of clusters\n    # random_state = seed\n    \n    set.seed(random_state)\n    dims = 2 # dimensions\n    xs = matrix(rnorm(n*dims, 10, sd=sd), n, dims)\n    clusters = sample(1:k, n, replace=TRUE)\n    centroids = matrix(rnorm(k*dims, mean=1, sd=10), k, dims)\n    clustered_x = cbind(xs + 0.5*centroids[clusters], clusters)\n    \n    plot(clustered_x, col=clustered_x[,3], pch=19)\n    \n    df = as.data.frame(x=clustered_x)\n    colnames(df) <- c(\"x1\", \"x2\", \"cluster\")\n    return(df)\n}\n</syntaxhighlight>\n\n=== k-Means Clustering ===\n\nThe k-means clustering method assigns '''n''' examples to one of '''k''' clusters, where '''n''' is the sample size and  '''k''', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:\n\n# The number of cluster '''k''' is chosen by the data analyst\n# The algorithm randomly picks '''k''' centroids and assigns each point to the closest centroid to get '''k''' initial clusters\n# The algorithm recalculates the centroid by taking average of all points in each cluster and updates the centroids and re-assigns the points to the closest centroid.\n# The algorithm repeats Step 3 until all points stop changing clusters.\n\nTo get an intuitive sense of how this k-means clustering algorithm works, visit: [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Visualizing K-Means Clustering]\n\n==== Implementing k-Means Clustering in R ====\n\nTo implement k-Means Clustering, the data table needs to only contain numeric data type. With a data frame or matrix with numeric value where the rows signify individual data example and the columns signify the ''features'' (number of features = the number of dimensions of the data set), k-Means clustering can be performed with the code below:\n\n<syntaxhighlight lang=\"R\" line>\n# Generate data and perform the clustering\ndf <- create_cluster_data(150, 1.25, 3)\ndata_cluster = kmeans(df, centers=3) # perform the clustering\n\n# plot for sd = 1.25\nplot(df$x1, df$x2, \n     pch=df$cluster, \n     col=data_cluster$cluster, cex=1.3, \n     xlab=\"x1\", ylab=\"x2\", \n     main=\"k-Means Clustering Example with Clearly Clustered Data\", \n     sub=\"(In-cluster variance when generating data = 1.25)\")\npoints(data_cluster$centers[1,1], data_cluster$centers[1,2], \n       pch=15, cex=2, col=\"black\")\npoints(data_cluster$centers[2,1], data_cluster$centers[2,2], \n       pch=15, cex=2, col=\"red\")\npoints(data_cluster$centers[3,1], data_cluster$centers[3,2], \n       pch=15, cex=2, col=\"green\")\nlegend(\"topleft\", legend=c(\"True Cluster 1\", \"True Cluster 2\", \"True Cluster 3\", \"Cluster Center\"), \n       col=c(\"black\",\"black\",\"black\", \"black\"), pch = c(1, 2, 3, 15), cex=0.8)\ntext(-3.15, 10, \n     \"Colors signify the clusters identified\\nby k-means clustering algorithm\", \n     adj = c(0,0), cex=0.8)\n</syntaxhighlight>\n\nHere is the result of the preceding code:\n\n[[File:Result of K-Means Clustering.png| |Result of K-Means Clustering]]\n\nWe can see that k-Means performs quite good at separating the data into three clusters. However, if we increase the variance in the dataset, k-Means does not perform as well. (See image below)\n\n[[File:Result of K-Means Clustering (High Variance).png| |Result of K-Means Clustering (High Variance)]]\n\n==== Strengths and Weaknesses of k-Means Clustering ====\n\n'''Strengths'''\n* It is intuitive to understand.\n* It is relatively easy to implement.\n* It adapts to new data points and guarantees convergence.\n\n'''Weaknesses'''\n* The number of clusters ''k'' has to be chosen manually.\n* The vanilla k-Means algorithm cannot accommodate cases with a high number of clusters (high ''k'').\n* The k-Means clustering algorithm clusters ''all'' the data points. As a result, the centroids are affected by outliers.\n* As the dimension of the dataset increases, the performance of the algorithm starts to deteriorate.\n\n=== Hierarchical Clustering ===\nAs the name suggests, hierarchical clustering is a clustering method that builds a ''hierarchy'' of clusters. \n\nUnlike k-means clustering algorithms - as discussed above -, this clustering method does not require us to specify the number of clusters beforehand. As a result, this method is sometimes used to identify the number of clusters that a dataset has before applying other clustering algorithms that require us to specify the number of clusters at the beginning.\n\nThere are two strategies when performing hierarchical clustering:\n\n==== Hierarchical Agglomerative Clustering ====\n\nThis is a \"bottom-up\" approach of building hierarchy which starts by treating each data point as a single cluster, and successively merging pairs of clusters, or agglomerating, until all clusters have been merged into a single cluster that contains all data points. Each observation belongs to its own cluster, then pairs of clusters are merged as one moves up the hierarchy.\n\n==== Implementation of Agglomerative Clustering ====\nThe first example of agglomerative clustering is performed using the hclust function built in to R.\n\n<syntaxhighlight lang=\"R\" line>\nlibrary(dendextend)\n\n# generate data\ndf <- create_cluster_data(50, 1, 3, random_state=7)\n# create the distance matrix\ndist_df = dist(df[, 2], method=\"euclidean\") \n# perform agglomerative hierarchical clustering\nhc_df = hclust(dist_df, method=\"complete\")\n# create a simple plot of the dendrogram\nplot(hc_df)\n\n# Plot a more sophisticated version of the dendrogram \ndend_df <- as.dendrogram(hc_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\nlabels_colors(dend_df) <-\n    c(\"black\", \"darkgreen\", \"red\")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\nlabels(dend_df) <- paste(as.character(\n    paste0(\"cluster_\",df[,3],\" \"))[order.dendrogram(dend_df)],\n    \"(\",labels(dend_df),\")\", \n    sep = \"\")\ndend_df <- hang.dendrogram(dend_df, hang_height=0.2)\ndend_df <- set(dend_df, \"labels_cex\", 0.8)\n\nplot(dend_df, \n     main = \"Clustered Data Set\n     (based on Agglomerative Clustering using hclust() function)\", \n     horiz =  FALSE,  nodePar = list(cex = 0.5), cex=1)\n\nlegend(\"topleft\", \n       legend = c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\"), \n       fill = c(\"darkgreen\", \"black\", \"red\"))\n</syntaxhighlight>\n\nHere is the plot of the dendrogram from the code above:\n[[File:Hierarchical Clustering Algorithm Example (hclust).png| |An example for Hierarchical Clustering Algorithm created using hclust() function in R.]]\n\nThe following example relies on the agnes function from cluster package in R in order to preform agglomerative hierarchical clustering.\n\n<syntaxhighlight lang=\"R\" line>\ndf <- create_cluster_data(50, 1, 3, random_state=7)\ndend_df <- agnes(df, metric=\"euclidean\")\n\n# Plot the dendrogram\ndend_df <- as.dendrogram(dend_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\nlabels_colors(dend_df) <-\n    c(\"black\", \"darkgreen\", \"red\")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\nlabels(dend_df) <- paste(as.character(\n    paste0(\"cluster_\",df[,3],\" \"))[order.dendrogram(dend_df)],\n    \"(\",labels(dend_df),\")\", \n    sep = \"\")\ndend_df <- hang.dendrogram(dend_df,hang_height=0.2)\ndend_df <- set(dend_df, \"labels_cex\", 0.8)\n\nplot(dend_df, \n     main = \"Clustered Data Set\n     (based on Agglomerative Clustering using agnes() function)\", \n     horiz =  FALSE,  nodePar = list(cex = 0.5), cex=1)\nlegend(\"topleft\", \n       legend = c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\"), \n       fill = c(\"darkgreen\", \"black\", \"red\"))\n</syntaxhighlight>\nFollowing is the dendrogram created using the preceding code:\n[[File:Hierarchical Clustering Algorithm Example (agnes).png| |An example for Hierarchical Clustering Algorithm created using agnes() function in R.]]\n\n==== Hierarchical Divisive Clustering ====\nThis is a \"top-down\" approach of building hierarchy in which all data points start out as belonging to a single cluster. Then, the data points are split, or divided, into separate clusters recursively until each of them falls into its own separate individual clusters.\n\n* All observations start as one cluster, and splits are performed recursively as one moves down the hierarchy.\n\n==== Implementation of Divisive Clustering ====\n\n<syntaxhighlight lang=\"R\" line>\n\n# Generate a dataset\ndf <- create_cluster_data(50, 1, 3, random_state=7)\n\n# Perform divisive clustering\ndend_df <- diana(df, metric=\"euclidean\")\n\n# Plot the dendrogram\ndend_df <- as.dendrogram(dend_df)\ndend_df <- rotate(dend_df, 1:50)\ndend_df <- color_branches(dend_df, k=3)\n\nlabels_colors(dend_df) <-\n    c(\"black\", \"darkgreen\", \"red\")[sort_levels_values(\n        as.numeric(df[,3])[order.dendrogram(dend_df)])]\n\nlabels(dend_df) <- paste(as.character(\n    paste0(\"cluster_\",df[,3],\" \"))[order.dendrogram(dend_df)],\n    \"(\",labels(dend_df),\")\", \n    sep = \"\")\n\ndend_df <- hang.dendrogram(dend_df, hang_height=1) # 0.1\ndend_df <- set(dend_df, \"labels_cex\", 0.8)\n\nplot(dend_df, \n     main = \"Clustered Data Set\n     (based on Divisive Clustering using diana() function)\", \n     horiz =  FALSE,  nodePar = list(cex = 0.5), cex=1)\nlegend(\"topleft\", \n       legend = c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\"), \n       fill = c(\"darkgreen\", \"black\", \"red\"))\n</syntaxhighlight>\n\nFollowing is the result of the code above:\n[[File:Hierarchical Clustering Algorithm Example (diana).png|center|An example for Hierarchical Clustering Algorithm created using diana() function from cluster function in R.]]\n\n==== Strengths and Weaknesses of Hierarchical Clustering ====\n'''Strengths'''\n* The method does not require users to pre-specify the number of clusters manually.\n* This method does not account for missing values.\n\n'''Weaknesses'''\n* Even though the number of clusters does not have to be pre-specified, users still need to specify the ''distance metric'' and the ''linkage criteria''.\n* This method becomes very slow when the size of the data set is large.\n\n== See Also ==\nAlthough this article only focused on two clustering algorithms. There are many other clustering methods that employ different strategies. Readers are suggested to investigate expectation maximization algorithm, and  density based clustering algorithms, and latent class analysis specifically.\n\n== Key Publications ==\n\n'''k-Means Clustering'''\n* MacQueen, James. \"Some methods for classification and analysis of multivariate observations.\" Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. No. 14. 1967.\n\n'''Hierarchical Clustering'''\n* Johnson, S.C. Hierarchical clustering schemes. Psychometrika 32, 241\u2013254 (1967). [https://doi.org/10.1007/BF02289588](https://doi.org/10.1007/BF02289588)\n* Murtagh, F., & Legendre, P. (2014). Ward\u2019s hierarchical agglomerative clustering method: which algorithms implement Ward\u2019s criterion?. Journal of classification, 31(3), 274-295.\n\n'''Other Clustering Methods'''\n* A. K. Jain, M. N. Murty, and P. J. Flynn. 1999. Data clustering: a review. ACM Comput. Surv. 31, 3 (September 1999), 264\u2013323. DOI:[https://doi.org/10.1145/331499.331504](https://doi.org/10.1145/331499.331504)\n* Lanza, S. T., & Rhoades, B. L. (2013). Latent class analysis: an alternative perspective on subgroup analysis in prevention and treatment. Prevention science : the official journal of the Society for Prevention Research, 14(2), 157\u2013168. [https://doi.org/10.1007/s11121-011-0201-1](https://doi.org/10.1007/s11121-011-0201-1)\n* Erich Schubert, J\u00f6rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. 2017. DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN. ACM Trans. Database Syst. 42, 3, Article 19 (July 2017), 21 pages. DOI:[https://doi.org/10.1145/3068335](https://doi.org/10.1145/3068335)\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Prabesh Dhakal."
                    },
                    "sha1": "s6mmf90kktgdnkuk0bxyor4a9h5dqma"
                }
            },
            {
                "title": "Code of Conduct",
                "ns": "0",
                "id": "298",
                "revision": {
                    "id": "6728",
                    "parentid": "5911",
                    "timestamp": "2022-06-19T13:10:12Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8162",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\n\n'''What''': The Code of Conduct is a written statement used in different circumstances to formalize the working procedures and expectations of individuals in a specific team. \"A code of conduct is a set of [[Glossary|rules]] outlining the norms, rules, and responsibilities or proper practices of an individual party or an organisation (Wikipedia).\"\n\n'''Why''':  It is oftentimes used in companies to set out certain rules and the respective responsibilities of their employers and employees and can be used to hold both [[Glossary|accountable]] for their actions. It is usually laid out in a written format. \n\n'''When''': Ideally a Code of Conduct is created before the intentional formation of a specific team. This helps to explicitly clarify the expectations, roles and responsibilities of each individual. As many organizations change over time this Code of Conduct should be iterated frequently to prove if it still fits the purpose of the team. \nLong term working environments benefit the most from a Code of Conduct (e.g. PhD-Supervision, Work in an association, Employee contracts, long term student assistants, long term group work) \n\n\n'''Caution'''\nAs the creation and maintenance of a formalized code of conduct takes a lot of time and effort, it is questionable if it is needed for every form of group endeavour (e.g. group essays, non-regularly meetings). However, it is still valuable to think in terms of code of conducts as every individual should be held accountable for their actions.\n\n== Goals ==\n\nA Code of Conduct provides clear Statements on how individuals want to work together in a specific organization. The Goal of the Code of Conduct can be summarized under the following three aspects: '''(1) Unity (2) Standards (3) Direction'''\n\n'''1. Unity:''' Certain ethical rules should be abided by everyone (e.g. letting everyone be heard, respecting each others arguments, being accountable)\n\n'''2. Standards:''' It should be clarified what the ethical standards and procedures within an organization should be in order to provide every individual with explicit procedures on how to behave.\n\n'''3. Direction:''' Should provide each individual about the purposes and goals of the organization to provide guidance on each respective role within that collective endeavor.\n\n== Getting started ==\n\nThe creation of a Code of Conduct should incorporate two aspects. (1) The topical points (2) procedural points\n\n'''1. Topical points to include in your Code of Conduct'''\n\n* ethical principles - includes workplace behaviour and respect for all people\n* values - includes an honest, unbiased and unprejudiced work environment\n* [[Glossary|accountability]] - includes taking responsibility for your own actions, ensuring appropriate use of information, exercising diligence and duty of care obligations and avoiding conflicts of interest\n* standard of conduct - includes complying with the job description, commitment to the organisation and proper computer, internet and email usage\n* standard of practice - includes current policies and procedures and business operational manual\n* disciplinary actions - includes complaints handling and specific penalties for any violation of the code.\n\n'''2. procedural points for creating and iterating your Code of Conduct'''\n\nMake sure everyone within the organization is able to express their opinion on roles, responsibilities and expectations. These questions could help for starting an open discussion about organizational values and how to formalize them: \n\n* What does ethics mean to you?\n* How effectively does the business put its values into practice?\n* Can we improve our ethical performance?\n* What do you think of the draft ethical guidelines?\n* Would this code of conduct help you make decisions?\n* How could it be more helpful?\n* Is there anything else we should include?\n\n(Based on Queensland Government Article, see link below)\n\n== Typical starting points ==\nAll groups are sooner or later riddled by by challenges when collaborating as a group. At Leuphana University, group works are from the very first semester part of the DNA of our teaching, since group work is indeed a realistic setting for later work reality. There are clearly positive and negative aspects one wants to cover when working on out a code of conduct. Let us start with all factors that make working in a group great, as collected from our students:\n\nDiverse ideas, different backgrounds and experience, reliability and splitting of the workload, giving each other critical feedback and emotional support are mentioned when people perceive group work to be positive. However, all these experience can be flipped into loosing time to gain coherence, endless feedback loops, lack of trust, unreliable work ethics, loss of energy due to repetitions, and the whole array of negative emotions we humans are capable of. This includes people's egos dominating the group, unbalanced speaking time, and lack of inclusion. Power does indeed quite often play a major role in group works, and this is also difficult to get rid of. The best way to start getting a great group together is obviously to start with yourself. How much do you listen to other? Do you give others appropriate space? How are your moderating skills? Do you make a conscious effort of integration? Can you uplift the spirits in the groups. Do you provide structure and are reliable? Are there specific behaviours of others that trigger you, and can you get rid of this counter-behaviour? Do you make an active effort to learn from the other? This list can go on for a long time, and is one of the best arguments why a code of conduct only works if you are committed to it. You cannot expect others to honour something that you ignore. We all make mistakes, yet keeping a research diary and learning how to serve a group best is a lifelong goal. A code of conduct is like a list of virtue driven guidelines that you may built in your life in order to facilitate yourself within groups. In academia many people are impatient, a trait probably all of us shared at times. Yet trying to push other people around will not help in the long run. Perfecting patience is hence a key skill. If you add reliability and awareness of your own insecurities -which are often the root cause of your triggers- then you are already halfway there. Almost the whole rest concerning group work is about trust, and this can only be built or earned, but not planned. \n\n== Links & Further reading ==\n\n'''Youtube Videos:'''\n\n[https://www.youtube.com/watch?v=C4si6n36n30 Quick youtube explanation of a code of conduct]\n\n[https://www.youtube.com/watch?v=HQcHR8-6IEA Use of code of conducts]\n\n'''Blogs:'''\n\n[https://www.business.qld.gov.au/running-business/employing/taking-on-staff/staff-code-conduct/writing Important aspects of a code of conduct]\n\n[https://projectinclude.org/writing_cocs Guide to writing a code of conduct]\n\n[https://blogs.cfainstitute.org/investor/2012/02/20/codes-of-ethics-if-you-adopt-one-will-they-behave/ Pro's and Con's of a code of conduct]\n\n'''Peer Reviewed Articles:'''\n\n[https://link.springer.com/content/pdf/10.1023/A:1006423220775.pdf Rezaee et al. (2001): Ethical Behavior in Higher Educational Institutions: The Role of the code of conduct]\n\n[https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8608.00100 Doig & Wilson (2002): The Effectiveness Of Codes Of Conduct]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Julius Rathgens (+ Henrik von Wehrden about staring points)."
                    },
                    "sha1": "hulig1en5zssizpp0ekcno9bkhsayiw"
                }
            },
            {
                "title": "Cohort Study",
                "ns": "0",
                "id": "262",
                "revision": {
                    "id": "4207",
                    "parentid": "3509",
                    "timestamp": "2021-02-15T10:00:29Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8078",
                        "#text": "[[File:ConceptCohortStudy.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/>\n'''In short:''' A Cohort Study is a form of quantitative data analysis where two or more groups are tracked over time from a specific exposure to an outcome.\n\n== Background ==\n[[File:Cohort Study SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Cohort Study until 2019.''' Search terms: 'Cohort Study', 'Cohort Analysis' in Title, Abstract, Keywords. Source: own.]]\nThe term 'cohort' historically refers to a 300-600 man unit in the Roman army (3). It found its way into scientific research thanks to Wade Hampton Frost, a medical doctor from the US, who invented the Cohort study method after his pioneering work in the field of epidemiology in the early 20th Century. In a study that was published after his death in 1938, he focused on the mortality rates of tuberculosis patients in Massachusetts and - presumably for the first time - assessed not only differences between age groups in any given year, but also in the development of each cohort over time.\n\nThe method and the term 'cohort' were subsequently adapted by the social sciences, mostly focusing on the role of \"generations\" in historical developments as well as in demography (4). Social sciences and epidemiology remain the most important applications fields of Cohort Studies to this day (2). However, cohorts are no more only defined based on their birth year, but determined based on any shared characteristic of interest (e.g. the experience of a specific event) within a defined time interval (4). It is worthwhile noting that the original term 'cohort analysis' today relates to a big data approach used for business analytics. The scientific method used in statistics and epidemiology today is referred to as 'cohort study' (2). More appropriate synonyms of the cohort study include incidence, longitudinal, forward-looking, follow-up, concurrent or prospective studies (3).\n\n\n== What the method does ==\n\n*\"A cohort study tracks two or more groups forward from exposure to outcome. In its simplest form, a cohort study compares the experience of a group exposed to some factor with another group not exposed to the factor. If the former group has a higher or lower frequency of an outcome than the unexposed, then an association between exposure and outcome is evident.\" (3, p.341). The exposure must be clearly defined, with the option of defining several degrees of exposure (e.g. light and heavy smokers compared to non-smokers) (3). The cohorts may be tracked over years, but also over decades, depending on the research question.\n* '''A Cohort Study is a form of quantitative data analysis.''' There are two main types: ''prospective'' (following cohorts into the future with original data gathering) and ''retrospective'' (following cohorts up to the present based on existent data). A third type, ''ambidirectional'', involves both data from the past and data gathered from the present on which is useful for studies that focus on both short-term and long-term outcomes of certain events or exposures (3).\n\n[[File:ResultVisualisationCohortStudy.png|300px|thumb|left|A table from Frost's original study. Source: Comstock 2001, p.9)]]\n\nThis table from Frost's original paper (see Key Publications) illustrates the mortality of tuberculosis patients (a both infectious and chronic disease), with the year of death in the columns and the age-group of the patients in the lines. '''When analyzed per year, this table suggests the highest mortality among infants, followed by a peak in the 20-29 year group and another increase for the elderly. However, the death rates for one cohort - seen diagonally over time - only indicates a high mortality for infants and 20-29 year olds, while the mortality rate decreases after that.''' By analyzing the temporal development of the cohort, Frost was able to reject the previous assumption that especially elderly people are at risk. Instead, he concluded that the high death rates in this age group are residuals from even higher death rates of this cohort in earlier years (1).\n\n\n== Strengths & Challenges ==\n\n* Cohort studies are \"(...) the best way to identify incidence and natural history of a disease, and can be used to examine multiple outcomes after a single exposure\" (3, p.341). In addition, they are useful when studying rare exposures that happen in limited numbers of environments (e.g. a factory). They can reduce the risk of survivor [[Bias and Critical Thinking|bias]] when studying diseases that are rapidly fatal, and they allow for the calculation of incidence rates, relative risks, and confidence intervals (3, p.342).\n* However, Cohort Studies are less useful for diseases and outcomes that are very rare or take a long time to develop (3). Also, loss to follow-up can be an issue with prospective Cohort Studies, especially for longitudinal studies that span over years or decades. In addition, the exposure status of the subjects may change over time (3).\n\n\n== Normativity ==\n\n* Most often, it is difficult to find control groups that share the exact same characteristics as the analyzed group(s) except for the exposure. Attempts to compensate for this can lead to biases in the analysis (3).\n* Since several outcomes may be attributed to the original exposure, researchers might focus on those outcomes that are significant or supportive of a certain assumption, instead of transparently communicating all outcomes (3).\n* The increasing wealth of data that become available in recent years through the internet an other sources led to many opportunities, however, now some studies are indicated to be cohort studies, when i fact they only use available data, but do not design a methodological sampling.\n\n\n== Key Publications ==\n\n* Frost, W.H. ''The age selection of mortality from tuberculosis in successive decades''. Am J Hyg 1939; 30: 91-6. (Reprinted in Am J Epidemio11995; 141: 4-9).\nThe original paper from Frost, published posthumously, that popularized the method.\n\n* Power, C. Elliott, J. 2006. ''Cohort profile: 1958 British birth cohort (National Child Development Study).'' International Journal of Epidemiology 35. p.34-41.\nA summary of the National Child Development Study, a British cohort study covering 17000 children born in a single week in 1958, that pursued these children's lives for several decades and investigated several health and social issues based on these datasets.\n\n\n== References ==\n\n(1) Comstock, G.W. 2001. ''Cohort analysis: W.H. Frost's contributions to the epidemiology of tuberculosis and chronic disease.'' Sozial- und Pr\u00e4ventivmedizin SPM 46. 7-12. Available at http://www.epi.msu.edu/janthony/requests/articles/Comstock_Cohort%20analysis%20Frost.pdf.\n\n(2) wikipedia. ''Cohort study.'' Available at https://en.wikipedia.org/wiki/Cohort_study.\n\n(3) Grimes, D.A. Schulz, K.F. 2002. ''Cohort studies: marching towards outcomes.'' Lancet 359. p.341-345.\n\n(4) Ryder, N.B. 1965. ''The Cohort As A Concept In The Study of Social Change.'' In: Mason, W.M. Fienberg, S. (eds.) 1985. ''Cohort Analysis in Social Research.'' Springer.\n\n\n== Further Information ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Catgory:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "lza6rwtdnv1hxhbtd3409d3krtpm95j"
                }
            },
            {
                "title": "Concept Maps",
                "ns": "0",
                "id": "839",
                "revision": {
                    "id": "6042",
                    "parentid": "6041",
                    "timestamp": "2021-07-15T11:18:44Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6271",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConcept Maps are a '''form of visually organizing information''' for groups and individuals. Conceptual terms or short statements are presented in boxes or bubbles and interlinked with other terms through commented arrows, resulting in a hierarchical network-like structure that provides an overview on a topic.\n\n== Goals ==\n* Organize '''conceptual knowledge''' in a visual form to identify learning processes and knowledge gaps.\n* Create new knowledge, ideas and solutions by '''arranging information in a structured manner''' around a focus question, topic, or problem.\n\n== Getting started ==\n[[File:Concept Maps - Example 1.png|500px|thumb|right|'''An example for a Concept Map on the knowledge structure required for understanding why we have seasons.''' . Source: [http://cmap.ihmc.us/docs/theory-of-concept-maps.php Novak & Ca\u00f1as 2008, Cmap]]]\nConcept Maps result from the work of '''Joseph Novak and colleagues at Cornell University in the 1970s''' in the field of education, and have since been applied in this area and beyond. The original underlying idea was to analyze how children learn, assuming that they do so by assimilating new concepts and positioning these in a cognitive conceptual framework. The idea of Concept Maps emerged from the demand for a form of assessing these conceptual understandings.\n\nImportantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of [[Group Concept Mapping]]. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a [[Mindmap]] in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.\n\nConcept Maps '''help identify a current state of knowledge''' and show gaps within this knowledge, e.g. a lack of understanding on which elements are of importance to a question or topic, and how concepts are interrelated. Identifying these gaps helps fill knowledge gaps. Therefore, they can be a helpful tool for students or anyone learning a new topic to monitor one's own progress and understanding of the topic, and how this changes as learning units continue.\n\nFurther, Concept Maps can be '''a way of approaching a specific question''' or problem and support a systematic solution-development process. The visual representation of all relevant elements nconcerning a specific topic can thus help create new knowledge. There are even more imaginable purposes of the approach, including management and planning, creative idea-generation and more.\n\n[[File:Concept Map Example 2.png|500px|thumb|center|'''This example from Novak (2016, p.178) shows how a student's conceptual understanding of a topic develops over time.''' It illustrates how Concept Maps can help identify knowledge gaps or flaws (which are existent still in the map below), how much (and what kind of) learning process was made over time, and what to focus on in future learning. Source: Novak 2016, p.178.]]\n\n\n== Step by step ==\n[[File:Concept Map - Step by Step 1.png|400px|thumb|right|'''A list of concepts (left) and a \"string map\" that focuses on cross-links between concepts, as a groundwork for the concept map.''' Source: [http://cmap.ihmc.us/docs/theory-of-concept-maps.php CMap]]]\n\n# When learning to work with Concept Maps, Novak & Canas (2008) recommend to '''start with a domain of knowledge that one is familiar with'''. Here, the learner should focus on a specific focus question and/or text item, activity, or problem to structure the presented elements and hierarchies around, and to contextualize the map by. As the authors highlight, often, \"learners tend to deviate from the focus question and build a concept map that may be related to the domain, but which does not answer the question. It is often stated that the first step to learning about something is to ask the right questions\".\n# After defining the domain of knowledge and focus question or problem, 15-25 '''key concepts should be identified and ranked''' according to their specificity. In a concept map, more general, broad concepts are on the top of the map, while more specific concepts are found below, resulting in a hierarchy from top to bottom. So the concepts should be ordered accordingly in a list first.\n# Then, a preliminary concept map is created, either digitally (with the 'official' IHMC CmapTools software, see below) or by using Post-Its and a whiteboard. Concepts can and should be moved around iteratively. Propositions  - connections between concepts with text - are added to the map to highlight and explain relationships between concepts. When a first good draft exists, one identifies cross-links, which are relationships between separate areas of the map, to further highlight conceptual linkages. Lastly, '''constant revision is applied''' and concepts can be re-positioned to further improve the map.\n\n\n== Links & Further reading ==\n* This entry was created based on the extensive entry by Novak & Canas (2008), available [http://cmap.ihmc.us/docs/theory-of-concept-maps.php here]. The authors are responsible for the introduction of Concept Maps in the 1970s and the development of the CmapTools.\n\n* Novak, J.D. 2016. The origins of the concept mapping tool and the continuing evolution of the tool. Information Visualisation 5. 175-184.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "ojnzpsn2p6w4dm0ipj5k646r6efi06b"
                }
            },
            {
                "title": "Conceptual Figures",
                "ns": "0",
                "id": "777",
                "revision": {
                    "id": "5962",
                    "parentid": "5450",
                    "timestamp": "2021-06-30T20:45:52Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8918",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nConceptual figures - or \"made-up figures\", as we like to call them - are [[Glossary|visualisations]] that do not represent data itself, but help illustrate a theoretical idea or a methodological design. They are often relevant for scientific publications, and can help improve presentations and pitches by making an idea or approach more understandable for the audience.\n\n== Goals ==\n* Create better figures to explain concepts or methodological designs\n* Expand on your text through visualisations\n\n== Getting started ==\n'''Here are some initial recommendable ideas on how to make this type of figure:'''\n\n1) '''Follow Ockham's razor''': just like for research designs - and much of life - Ockham's razor can guide your approach to developing a conceptual figure. It should include as much information as necessary, but as little as possible. Useless information that the figure does not benefit from should be avoided. Also, the information provided in the figure should be complementary to the text, not (entirely) identical, which would render the figure itself redundant. Good figures help us better understand the text that we read already, but do not take up space without providing anything new.\n\n2) '''Make your figures intuitive to understand.''' A figure with a lot of complex information usually overwhelms the reader. You want your figure to stimulate further engagement with what it shows. A good figure is graspable at first sight, at least in terms of the core elements. This also means that there should usually be no more than 3 - 5 five core elements within the figure, although there can be accompanying smaller elements around those. It should be clear right away what is shown, and how parts of the figure relate to each other. Then, the information that lies within the figure should be easily accessible, even if it takes some time to engage with everything that is shown. Be aware that people (at least in Western culture) usually scan figures from the top-left to the bottom right - build your figures accordingly.\n\n3) '''Avoid empty spaces or disbalances'''. A lot of blank space, or an unbalanced distribution of visual elements just feels off.\n\n4) '''Use clear fonts and a balanced color palette'''. Fonts are certainly a matter of taste, but some fonts are just more adequate for scientific publications or presentations than others. The text should be readable at first glance (see 2), so choose a font that is easy to decipher and ideally fits to the rest of the document. We recommend sans serif fonts like Ariel or Courier, but you should find your own preference or get inspired by other publications. Colors, then, should first make a figure more visually appealing. This can be done by using colors that fit together, for example by relying on a Wes-Anderson-Color-Palette, which is obviously the best set of colors. However, the use of certain signal colors can help guide the audience's attention to specific details. Colors can also hold information. For example, a color gradient can represent a decrease or increase of some value, or different categories of things. Also, colors can be used recurringly, for example when a certain color represents a specific actor, element, topic or place in one figure, and the same color represents the same element in another figure, which supports continuity in your design and guides the reader through the document.\n\n5) '''Create a narrative for the figure, and the figure as a narrative'''. A good figure can stand on its own, and represent something. It is alright if the full picture of the figure is only understandable with the help of the original text it expands upon. However, if you can show the figure to someone and (s)he'll understand what it is about, you are on the right track. For this, it is important to include all information necessary to understand what is shown (see 1). More importantly, a figure can give the reader information on how things relate to each other - focusing on temporal order, most commonly through the use of arrows or a timeline, or causal relations, often also done with arrows, although many more approaches are conceivable. What happens when, and what is a solution to, or consequence of, what? Also, proportions of information can contain information: if an element is smaller than others, it is maybe not as important.\n\n6) '''Learn from other figures, and develop your own style'''. It is okay to steal from others. There are lots of great conceptual figures in science, and studying them with a focus on what makes them great - or not great - can help find ideas to improve your own design. Of course, you shouldn't simply adapt other people's figures, but take parts or approaches that might fit your own needs. \n\n7) '''Some last tips:'''\n* Decide on one software solution to implement your figure in, and learn what it can do.\n* Acknowledge the expectations of your audience, or deliberately decide to break with their expectations when appropriate.\n* Always keep in mind how your figure relates to the character and content of the document it is placed in.\n* Follow your intuition: we all have our own ideas about what looks good, and we do our best work when we are convinced of what we are doing ourselves.\n* Last but not least, however, get feedback from peers when in doubt. They will be able to tell you if everything is understandable, and overcome the tunnel vision that you might develop when you are deep in a topic.\n<br/>\n[[File:Conceptual Figures.png|800px|frameless|center|'''A conceptual figure on conceptual figures.''' Source: own.]]\n<br/>\nEventually, however, our main recommendation remains to '''keep it simple'''. Don't overburden the reader with information, but focus on what is really important. For more helpful tips on visualisations, please refer to the entry on Statistical Figures and Poster Design.\n\n=== Examples ===\n\n* '''Doughnut Economics'''\n[[File:Conceptual Figures - Doughnut Economics.png|400px|thumb|left|'''The Doughnut Economics concept.''' Source: [https://doughnuteconomics.org/about-doughnut-economics Doughnut Economics Lab]]]\n\nThe famous Doughnut Economics figure is a good example for a great conceptual figure. \n* It is understandable at first sight, with the doughnut itself limiting itself to three elements ''(ecological ceiling, social foundation, the safe and just space for humanity''). This is what our eyes fall onto at first glance. \n* The doughnut shape, due to the underlying planetary boundary concept, is akin to the shape of the globe, which is recognizable, and the arrows tell the main narrative of the figure. \n* There is a lot of information here, but it is clearly divided into categories (social necessities, environmental boundaries) and rather easy to read, despite the cyclical arrangement. \n* The colors support the narrative: red is a problem, dark green is okay, light green is ideal. \n* The doughnut works as a figure on its own, despite it being part of a larger book that explains it in more detail (Doughnut Economics by Kate Raworth). It is a narrative by itself, and helps communicate an idea.\n\n<br>\n<br>\n\n* '''The Sustainable Development Goals'''\n[[File:E SDG Poster 2019 without UN emblem WEB.png|600px|thumb|right|'''The SDGs.''' Source: [https://www.un.org/sustainabledevelopment/news/communications-material/ United Nations]]]\nThe Sustainable Development Goals are another good example of how a visualisation can hold a lot of information, but still be understandable and - what is very important here - recognizable.\n* Each piece of text is amended through a fitting picture, and has a number, which provides structure at first sight. Also, the 18 square panels provide order, which is necessary considering the busy individual icons.\n* The SDGs have a distinct coloring palette, which is used again in the shortened circular version (bottom right corner). Wherever you go, the SDGs will be recognizable. Interestingly, they still work when used individually, which speaks for the strong narrative of the arrangement.\n* The font is easily readable and limited to the title of each goal, although there is (a lot) more information beneath.\n\n<br>\n<br>\n\n''More Examples will be added''\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Normativity of Methods]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Matteo Ramin and Christopher Franz."
                    },
                    "sha1": "idrsleo7gov5v5fi07hluut5vjqkmwc"
                }
            },
            {
                "title": "Conditions and Branching in Python",
                "ns": "0",
                "id": "1028",
                "revision": {
                    "id": "7255",
                    "parentid": "7014",
                    "timestamp": "2023-06-30T05:21:20Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to conditions and branching (if-statements) in Python.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4971",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n===Conditions===\nConditions are comparison operations that will produce the answer Yes/ No or Boolean (True/ False) in the end. \n\nThere are many conditions that you can use to compare 2 values: \n\n* equal: ==\n* not equal: !=\n* greater than: >\n* less than: <\n* greater than or equal: >=\n* less than or equal: <=\n\nTo try your first conditions, we can start easy:\n<syntaxhighlight lang=\"Python\" line>\na = 1\n\n#The result is False because 1 is not equal to 2. \na == 2\n\n#If you try again, the result will be True. \na == 1\n\na > 0 # Result: True\n\na < 0 #Result: False\n</syntaxhighlight>\n\nWe can also use it to check strings and other types of data:\n\n<syntaxhighlight lang=\"Python\" line>\n\u201cKora\u201d != \u201cAang\u201d\n</syntaxhighlight> \n\n===Branching===\n\nIn programming, branching is more known as if-statement. Why? Because the logic is similar to a tree. You check if something is True or not. \n\nFor example, if Aang can learn how to bend all four elements, he does not have to learn anymore, otherwise, he has to learn them to save the world. \n\n==If statement==\n<syntaxhighlight lang=\"Python\" line>\nage = 4\n\n#expression that can be true or false\nif age >= 4:\n\n\t\t#within an indent, we have the expression that is run if the condition is true\n\t\tprint(\"you started to speak\" )\n\n#The statements after the if statement will run regardless if the condition is true or false\nprint(\"print this anyway\")\n</syntaxhighlight>\n\n\nThe \":\" in \"if age >=4:\" is necessary to let Python know that another line of command telling python what to do if the condition is fulfilled will come. In a normal sentence, this would be a comma: \"If you are already four or older''',''' then you have already started to speak.\" The \":\" in the if-statement demands an indentation (as you can see the \"print(\"you started to speak\")\" by one tab. This means that this print command is nested in the if-statement. You can therefore imagine the indentation to create a logical dependency between commands. The commands that are on the same level do not logically depend on each other. The commands with an indentation are logically dependent on the commands with one indentation less. For our case, this means: \"You started to speak\" will '''only''' be printed if the age is 4 or older, whereas \"print this anyway\" will be printed independently from the age. You can try it out by changing the age:\n\n<syntaxhighlight lang=\"Python\" line>\nage = 3\n\n#expression that can be true or false\nif age >= 4:\n\n\t\t#within an indent, we have the expression that is run if the condition is true\n\t\tprint(\"you started to speak\" )\n\n#The statements after the if statement will run regardless if the condition is true or false\nprint(\"print this anyway\")\n</syntaxhighlight>\n\nYou can see here that only \"print this anyway\" has been printed in this case.\n\n==Else Statement==\n\nAfter \u201cif\u201d statement, we have \u201celse\u201d statement. Else statement will be triggered if the \u201cif\u201d statement does not get activated.\n<syntaxhighlight lang=\"Python\" line>\nage = 4\n#age = 6\nif age > 5 :\n\tprint(\u201dolder than 5 y.o\u201d)\n\nelse: \n\tprint(\u201dyounger than 5 y.o\u201d)\n</syntaxhighlight>\n\nYou can try changing the age between 4 and 6 to see the difference reaction that the code shows with \u201cif\u201d and \u201celse\u201d statement. \n\n==Elif Statement==\n\nBetween \u201cif\u201d and \u201celse\u201d statement, we have \u201celif\u201d (else if). The logic follows: \n\n* if A:\n* elif B1:\n* elif B2:\n* else C:\n\nMeaning that if A is wrong, the code will check the condition of \u201celif\u201d. If it is still False, then go one row further.\n\n<syntaxhighlight lang=\"Python\" line>\nname = \u201ctoph\u201d\n\nif name == \u201ctoph\u201d:\n\n\tprint(\u201dearth bending\u201d)\n\nelif name == \u201ckatara\u201d:\n\n\tprint(\u201dwater bending\u201d)\n\nelif name == \u201caang\u201d:\n\n\tprint (\u201dair bending\u201d)\n\nelse \n\n\tprint(\u201dfire bending\u201d)\n</syntaxhighlight>\nThe code above will check one by one whether the name is \u201ctoph\u201d, \u201ckatara\u201d, \u201caang\u201d, or others to check what bending skills the person has. \n\n==Logical Operators==\nSometimes you want to combine the conditions together to have more complex conditions. For example: only males that are older than 18 years old must go into the military. In Python, there are 3 logical operators that can join two conditions or more: \n\n* and\n* or\n* not\n\nExample:\n<syntaxhighlight lang=\"Python\" line>\nmilitary={\"gender\":(\"male\", \"female\", \"male\"), \"age\":[17,18,20]}\ngender=military[\"gender\"]\n    age=military[\"age\"]\nif gender == \"male\" and age >=18:\n    print(\"military\")\nelse:\n    print(\"no military\")\n</syntaxhighlight>\n\n==Quiz==\n1. Write an if statement to check if \u201cSaka\u201d have any bending skills. \n\n2. Write a logic that checks 2 variables: color and fruit. \ncolors can be : red, green, yellow\nfruits: apple, banana, pinaple\nAnd print : \u201cyellow banana\u201d if it the color is yellow, and the fruit is banana.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "675b2ene9bjpi97y43hunbek2d6avvq"
                }
            },
            {
                "title": "Content Analysis",
                "ns": "0",
                "id": "278",
                "revision": {
                    "id": "6418",
                    "parentid": "6414",
                    "timestamp": "2021-10-18T10:57:23Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15734",
                        "#text": "[[File:ConceptContentAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Content_Analysis]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/><br/>\n''' In short:''' Content Analysis relies on the summarizing of data (most commonly text) into content categories based on pre-determined rules and the analysis of the \"coded\" data.\n\n== Background ==\n[[File:Content Analysis SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Content Analysis until 2019.''' Search term: 'content analysis' in Title, Abstract, Keywords. Source: own.]]\nEarly approaches to text analysis can be found in Hermeneutics, but quantitative Content Analysis itself emerged during the 1930s based on the sociological work of Lazarsfeld and Lasswell in the USA (4). The method prospered in the study of mass communications in the 1950s (2). [[History of Methods|Starting in the 1960s]], the method was subsequently adapted to the specific needs of diverse disciplines, with the Qualitative Content Analysis emerging from a critique on the lack of communicative contextualization in the earlier purely quantitative approaches (4). Today, the method is applied in a wide range of social sciences and humanities, including communicative science, anthropology, political sciences, psychology, education, and literature, among others (1, 2).\n\n\n== What the method does ==\nContent Analysis is a \"(...) systematic, replicable technique for compressing many words of text into fewer content categories based on explicit rules of coding\" (Stemler 2000, p.1). However, the method entails more than mere word-counting. Instead, Content Analysis relies on the interpretation of the [[Glossary|data]] on behalf of the researcher. The mostly qualitative data material is assessed by creating a category system relevant to the material, and attributing parts of the content to individual categories (Schreier 2014). Not only the content of a source is evaluated, but also formal aspects as well as contextual psychological, institutional, and cultural elements of the communication process (1, 4). \"[Content Analysis] seeks to analyze data within a specific context in view of the meanings someone - a group or a culture - attributes to them.\" (Krippendorff 1989, p.403). Because of this, Content Analysis is a potent method to identify trends and patterns in (text) sources, to determine authorship, or to monitor (public) opinions on a specific topic (3). \n\nApart from text, a diverse set of data can be analyzed using Content Analysis. \"Anything that occurs in sufficient numbers and has reasonably stable meanings for a specific group of people may be subjected to content analysis.\" (Krippendorff 1989, p.404). The data must convey a message to the receiver and be durable (2, 3). Often, Content Analysis focuses on data that are difficult or impossible to interpret with other methods (3). The data may exist 'naturally' and be publicly available, for example verbal discourse, written documents, or visual representations from mass media (newspapers, books, films, comics etc.); or be rather unavailable to the public, such as personal letters or witness accounts. The data may also be generated for the research purpose (e.g. interview transcripts) (1, 2, 4).  \n\nWhile there is a wide range of qualitative Content Analysis approaches, this entry will focus on joint characteristics of these. For more information on the different variations, please refer to Schreier (2014) in the Key Publications.\n\n==== General Methodological Process ====\n* Any Content Analysis starts with the ''Design Phase'' in which the research questions are defined, the potential sources are gathered, and the analytical constructs are established that connect the prospective data to the general target of the analysis. These constructs can be based on existing theories or practices, the experience and knowledge of experts, or previous research (2).\n* Next, the ''Unitizing'' is done, i.e. the definition of analysis units. It may be distinguished between sampling units (= sources of data, e.g. newspaper articles, interviews) and analysis units (= units of data that are coded and analyzed, e.g. single words or broader messages), with different approaches to identifying both (see Krippendorff 2004).\n* Also, the ''sampling'' method is determined and the sample is drawn.\n* Then, the ''Coding Scheme'' - or 'Codebook' - is developed and the data are coded. 'Coding' generally describes the transfer of the available data into more abstract groups. A code is a label that represents a group of words that share a similar meaning (3). Codes may also be grouped into categories if they thematically belong together. There are two typical approaches to developing the coding scheme: a rather theory-driven and a rather data-driven approach. In a theory-driven approach, codes are developed based on theoretical constructs, research questions and elements such as an interview guide, which leads to a rather deductive coding procedure. In a data-driven approach, the coding system is developed as the researcher openly scans subsamples of the available material and develops codes based on these initial insights, which is a rather inductive process. Often, both approaches are combined, with an initial set of codes being derived from theory, and then iteratively adapted through a first analysis of the available material.\n* With the coding scheme at hand, the researcher reads (repeatedly) through the data material and assigns each analysis unit to one of the codes. The ''coding process'' may be conducted by humans, or - if sufficiently explicit coding instructions are possible - by a computer. In order to provide reliability, the codes should be intersubjective, i.e. every researcher should be able to code similarly, which is why the codes must be exhaustive in terms of the overarching construct, mutually exclusive, clearly defined, have unambiguous examples as well as exclusion criteria.\n* Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural context of the message, or on the message's effect. The learnings may be validated in the face of other information sources, which is often difficult when Content Analysis is used in cases where there is no other possibility to access this knowledge. Finally, new hypotheses may also be formulated (all from 1, 2).\n\n[[File:Example Content Analysis (1).jpg|500px|thumb|right|'''An exemplary interview transcript from a person that was admitted to an emergency center.''' Source: Erlingsson & Brysiewicz 2017.]]\n\n[[File:Example Content Analysis (2).png|500px|thumb|right|'''The coding results for statements from the interview transcript above, grouped into one of several themes.''' Source: Erlingsson & Brysiewicz 2017.]]\n\nQualitative Content Analysis is a rather inductive process. The process is guided by the research questions - hypotheses may be tested, but this is not the main goal. As with much of qualitative research, this method attempts to understand the (subjective) meaning behind the analyzed data and to draw conclusions from there. Its purpose is to get hold of the 'bigger picture', including the communicative context surrounding the genesis of the data. The qualitative Content Analysis is a very iterative process. The coding scheme is often not determined prior to the coding process. Instead, its development is guided by the research questions and done in close contact to the data, e.g. by reading through all data first and identifying relevant themes. The codes are then re-defined iteratively as the researcher applies them to the data. As an example, the comparative analysis of two texts can be mentioned, where codes are developed based on the first text, re-iterated by reading through the second text, then jointly applied to the first text again. Also, new research questions may be added during the analysis if the first insights into the data raise them. This approach is closely related to the methodological concept of [[Grounded Theory]] approaches.\n\nWith regards to the coding process, the analysis units are assigned to the codes but not statistically analysed. There is an exemption to this - the evaluative content analysis, where statements are evaluated on a nominal or ordinal scale, which makes simple quantitative (statistical) analyses possible. However, for most forms of qualitative Content Analysis, the assigned items per code are interpreted qualitatively and conclusions are inferred from these interpretations. Examples and quotes may be used to illustrate the findings. Overall, in this approach, a deep understanding of the data and their meaning is relevant, as is a proper description of specific cases, and possibly a triangulation with other data sources on the same subject. \n\n\n== Strenghts & Challenges ==\n* Content Analysis counteracts biases because it \"(...) assures not only that all units of analysis receive equal treatment, whether they are entered at the beginning or at the end of an analysis but also that the process is objective in that it does not matter who performs the analysis or where and when.\" (see Normativity) (Krippendorff 1989, p.404). In this case, 'objective' refers to 'intersubjective'. Yet, biases cannot be entirely prevented, e.g. in the sampling process, the development of the coding scheme, or the interpretation and evaluation of the coded data.\n* Content Analysis allows for researchers to apply their own social-scientific constructs \"by which texts may become meaningful in ways that a culture may not be aware of.\" (Krippendorff 1989, p.404)\n* However, especially in case of qualitative analysis of smaller data sample sizes, the theory and hypotheses derived from data cannot be generalized beyond this data (1). Triangulation, i.e. the comparison of the findings to other knowledge on the same topic, may provide more validity to the conclusions (see Normativity).\n\n\n== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.\n* Validity of the inferred results - i.e. the fitting of the results to the intended kind of knowledge - may be reached a) through the researchers' knowledge of contextual factors of the data and b) through validation with other sources, e.g. by the means of triangulation. However, the latter can be difficult to do due to the uniqueness of the data (1-3). Any content analysis is a reduction of the data, which should be acknowledged critically, and which is why the coding scheme should be precise and include the aforementioned explanation, examples and exclusion criteria.\n\n==== Connectedness ====\n* Since verbal discourse is an important data source for Content Analysis, the latter is often used as an analysis method of transcripts of standardized, [[Open Interview|open]], or [[Semi-structured Interview|semi-structured interviews]].\n* Content Analysis is one form of textual analysis. The latter also includes other approaches such as discourse analysis, rhetorical analysis, or [[Ethnography|ethnographic]] analysis (2). However, Content Analysis differs from these methods in terms of methodological elements and the kinds of questions it addresses (2).\n\n\n== Outlook ==\n* The usage of automated coding with the use of computers may be seen as one important future direction of the method (1, 5). To date, the human interpretation of ambiguous language imposes a high validity of the results which cannot (yet) be provided by a computer. Alas, the development of an appropriate algorithm and text recognition software pose a challenge. The meaning of words changes in different contexts and several expressions may mean the same. Especially in terms of qualitative analyses, this currently makes human coders indispensable. Yet, the emergence of big data, Artificial Intelligence and [[Machine Learning]] might make it possible in the foreseeable future to use automated coding more regularly and with a high validity.\n* Another relevant direction is the shift from text to visual and audio data as a primary form of data (5). With ever-increasing amounts of pictures, video and audio files on the internet, future studies may refer to these kinds of sources more often.\n\n== Key publications ==\nKuckartz, U. (2016). ''Qualitative Inhaltsanalyse: Methoden, Praxis, Computerunterst\u00fctzung (3., \u00fcberarbeitete Auflage). Grundlagentexte Methoden.'' Weinheim, Basel: Beltz Juventa.\n* A very exhaustive (German language) work in which the author explains different types of qualitative content analysis by also giving tangible examples.\n\nKrippendorff, K. 2004. ''Content Analysis - An Introduction to Its Methodology. Second Edition''. SAGE Publications.\n* A much-quoted, extensive description of the method's history, conceptual foundations, uses and application. \n\nBerelson, B. 1952. ''Content analysis in communication research.'' Glencoe, Ill.: Free Press.\n* An early review of concurrent forms of (quantitative) content analysis.\n\nSchreier, M. 2014. ''Varianten qualitativer Inhaltsanalyse: Ein Wegweiser im Dickicht der Begrifflichkeiten.'' Forum Qualitative Sozialforschung 15(1). Artikel 18.\n* A (German language) differentiation between the variations of the qualitative content analysis.\n\nErlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n* A very helpful guide to content analysis, using the examples shown above.\n\n\n== References ==\n(1) Krippendorff, K. 1989. ''Content Analysis.'' In: Barnouw et al. (Eds.). ''International encyclopedia of communication.'' Vol. 1. 403-407. New York, NY: Oxford University Press.\n\n(2) White, M.D. Marsh, E.E. 2006. ''Content Analysis: A Flexible Methodology.'' Library Trends 55(1). 22-45. \n\n(3) Stemler, S. 2000. ''An overview of content analysis.'' Practical Assessment, Research, and Evaluation 7. Article 17.\n\n(4) Mayring, P. 2000. ''Qualitative Content Analysis''. Forum Qualitative Social Research 1(2). Article 20.\n\n(5) Stemler, S. 2015. ''Content Analysis. Emerging Trends in the Social and Behavioral Sciences: An Interdisciplinary, Searchable, and Linkable Resource.'' 1-14.\n\n(6) Erlingsson, C. Brysiewicz, P. 2017. '' A hands-on guide to doing content analysis.'' African Journal of Emergency Medicine 7(3). 93-99.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "iyiephl940jm757bepcn2qii9kjgjzk"
                }
            },
            {
                "title": "Coping with psychological problems",
                "ns": "0",
                "id": "1018",
                "revision": {
                    "id": "6968",
                    "parentid": "6967",
                    "timestamp": "2023-03-06T18:03:38Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "comment": "/* Getting professional support */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19085",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nThe contents of this entry are meant to provide you with support in psychologically challenging times. We'll give you a structured way of getting help as well as provide tips on what you can do for yourself.\n\n== Goals ==\nHelp you to be well.\n<br><br>\n'''Disclaimer'''\nThis article is based solely on our own experience and research. When using the first person singular below, it is the authors own experience, whereas \u2018we\u2019 means that the Wiki team sprinkled in their experiences. Thus, this page can and should not be a substitute for professional support. Nonetheless, we hope to provide you with some valuable resources that can provide some help in challenging times. It is also mainly written for students in L\u00fcneburg, and therefore has local context. We hope it might still be helpful to others.\n\nIf you have urgent and acute psychological problems, please contact one of the professional points of contact below!\n\n'''Institutsambulanz der Psychiatrischen Klinik L\u00fcneburg (PKL)'''<br>\nAm Wieneb\u00fcttler Weg 121339 L\u00fcneburg<br>\nTel. (04131) 601 16 00 oder<br>\nTel. (04131) 601 16 22 (Aufnahme)\n\n'''Psychiatrischer Krisendienst am Wochenende'''<br>\nTel. (04131) 602 60\n\n'''Sozialpsychiatrischer Dienst L\u00fcneburg'''<br>\nAm Graalwall 4\n21339 L\u00fcneburg\nTel. (04131) 26 14 97\n\n== Introduction ==\n\nSometimes, changes beyond our own control can impact our mental well-being to the extent that we are unable to cope with it just by ourselves. Many of us have never been taught how to cope with mental hardship, and societal norms of individualism and toughness can make it hard to acknowledge that at some point, we can simply not carry on with our lives using our usual coping mechanisms. \n\nThe pandemic amplified these issues manyfold, and we know how many students are currently struggling with their mental health, especially when university, jobs and life are still setting high demands on our ability to \"function\". We know this is for many parts a societal problem. Nevertheless, if you feel depressed, anxious or overwhelmed at any one moment, you have little options but to start with yourself. This is why we want to provide this article: as a means to help you get through difficult times.\n\nWe offer two resources here.\n\nThe first is a guide on how to get professional help. From our experience, this can be one of the hardest things to do when you're already down, and the existing health care systems don't make it easier to actually find support. Here, we tried to collect resources and mould them into a step-by-step instruction to help you find professional help as straightforwardly as possible.\n\nThe second is a set of practices and perspectives we found helpful to cope with mental issues ourselves, which you can start implementing right now. This is important for two reasons. For one, they can help you get a little better. Additionally, they may help you to find the strength which you need to get professional help.\n\nThis is a personal topic, and it might feel a bit awkward to use university resources for this. We can't really circumvent that, but still hope that this might provide you with some kind of help. We wish you all the best!\n\n== Getting professional support ==\n\nGetting professional help is by no means easy, but it is very possible. Below, we tried to draw an overview on how this can work. The truth is that the medical system is messy and reality might be a little different. Still, sticking to some form of structured process should help you in getting professional support. Here's an overview:\n\n<br>[[File:Finding a therapist.png|600px|center|finding_a_therapist_overview]]<br>\n\nThere\u2019s three things you can do besides going for a full therapy which may already help: \n\nA) Finding a frame for yourself to better understand that having a mental illness is not your responsibility or fault. \n\nB) Getting psychological counselling from the Studierendenwerk (if you\u2019re a student in L\u00fcneburg)\n\nC) Talking to your doctor. He or she will be able to talk you through the process as well, and give you a recipee which can help you to find a therapist faster.\n\nThese three things don\u2019t have any prerequisites and you can do them at any time. They will however not directly result in you getting therapy, which is what the numbered blocks (1-4) below are for. Hereafter, we describe each individual step from A to C and 1 to 4.\n\n=== I) Immediate Steps ===\n\n==== A) Find a frame for yourself ====\n\nThe first thing besides all the technical and organisational stuff is finding a way to think about this that doesn't make you feel worse about your situation than it needs to. For me personally, it was really helpful to understand that **psychological problems are an illness that we bear no more personal responsibility for than for a cold - possibly a lot less**. Before looking for a therapist, I started reading and watching some content about the topic which helped me to somewhat normalise the situation. A conversation I found really empowering was between Thorsten Str\u00e4ter and Kurt Kr\u00f6mer here: [https://www.youtube.com/watch?v=ZxL1lJnHJNQ Link to Youtube]\n\nHowever, it\u2019s possibly sensible to not sink yourself into too much content about mental illness - some of it might also drag you down. \n\n==== B) Psychological Counselling for students ====\n\nIf you're a ''student'' in L\u00fcneburg, you have the right to get counselling from the psychological counsel of the Studierendenwerk. They offer open walk-ins which you can attend rather spontaneously when calling upfront, and up to 8 sessions of individual counselling. Note, however, that they cannot provide a diagnosis or therapy and are therefore no full substitute for a therapist. Still, they can confidentially listen to what you have to say and are struggling with, and can support you in making decisions on the next steps.\n\nUnfortunately, the demand is usually very high, so it is not necessarily faster to get an appointment here than with a professional therapist. Nonetheless, we think it is worth a try, also because at least from our experience, the threshold to just \"talk to someone\" is a lot lower than consulting an actual therapist. Below, you will find the webpage and contact data.\n\n* [https://stw-on.de/l%C3%BCneburg/beratung/pbs Psychologische Beratungsstelle] <br>\n* Go to open hours (currently Tuesday, 12-14, appointment via mail ([http://mailto:pbs.lg@stw-on.de pbs.lg@swt-on.de]) or phone ([tel://(04131)%20789%2063%20-%2025 (04131) 789 63 - 25])<br>\n* There is also an option to do group workshops: [https://stw-on.de/l%C3%BCneburg/beratung/pbs/workshops-gruppen Link]\n\n==== C) Talk to your doctor ====\n\nYour doctor (\u201dHausarzt\u201d) is typically mentioned as one of the first points of contact in usual therapy guidance. If you have one and sufficiently trust him or her, you can absolutely make an appointment and talk things through with them. They will also be able to hand out a recipee which can speed up the process of finding a therapist, and may forward you to other specialists e.g. for medication. \n\nHowever, it is not necessary to immediately see a \u201cnormal\u201d doctor. At some point you will have to meet one since they are required to ensure that your mental illness is not caused by any somatic problems (such as a lack of vitamins or neurological problems). Still, this can also happen when you have already found a therapist, but before you start the actual therapy. \n\n=== II) Finding a Therapist ===\n\nBasically, finding a therapist is like finding any other specialist medical practitioner: you look them up, you call them, you make an appointment. The only problem is that the supply of therapy slots is very thin, and there\u2019s no good system in place that makes it easy to find open slots. So you kind of have to just call a lot of therapists and find out whether or not they have a slot available. Below, we try our best to give you useful tips for the process.\n\nBefore you start, you might want to think about the kind of therapy you want. There are many different sub-disciplines, cognitive behavioural and analytical therapy being the predominant ones. It might make sense to [look up the differences](https://hellobetter.de/blog/therapieformen/) and decide for yourself which sounds the most promising to you. However, it is commonly said that the relation between therapist and patient is a much stronger success factor than the specific type of therapy you do. So probably, you might just want to take whichever therapist has a slot open and you kind of click with. \n\n==== 1) Look up nearby therapists ====\n\nThe first step is to look up therapists nearby. There\u2019s basically four sources you can use to find them which we listed below. Unfortunately, there\u2019s no unified register that contains all therapists and is fully up-to-date, so you might have to draw on all these sources one by one. We tried to order them descending by exhaustiveness, but this is rather subjective. If you feel overwhelmed, it\u2019s probably enough to stick to the first and second options.\n\n# '''The public [https://www.eterminservice.de/terminservice eTerminService]'''. It is probably the most exhaustive search engine with only a few problems here and there. If you have a recipee from a doctor or therapist that includes a code, this is where you can enter it. If you have been told by anyone to call 116 117 to get a slot: don\u2019t. From our experience, all they do is look at the same website, but you have to wait an extra 30-45 minutes in the phone queue for them to do it for you. \n# '''[https://psych-info.de/ psych-info.de]''' is a register for therapists. It might not be available in your region, and it is not working super-smooth. We did however have the experience that some therapists listed here are not listed in the eTerminservice above. \n# '''[https://www.jameda.de/ Jameda]''' also offers a search engine for therapists, but this should probably only be a last resort. The ratings can be rather misleading.  \n# '''Use a search engine''' of your liking, be it Google or Ecosia or DuckDuckGo or anything. \n\nAdditionally, you have the opportunity to contact institutes where you may get slots with therapists in training. This need not be a downside, because they may have more time and they might also be closer to the current state of research. We compiled this non-exhaustive list in the Hamburg/L\u00fcneburg area for you:\n\n* [https://zap-nord.de/hamburg.html ZAP Nord (TP)]<br>\n* [https://www.mova-institut.de/ MoVA Institut]<br>\n* [https://www.ivah.de/ IVAH]<br>\n* [https://www.gzstpauli.de/dgvt---ausbildungszentrum.html DGVT]<br>\n* [https://www.psy.uni-hamburg.de/arbeitsbereiche/klinische-psychologie-und-psychotherapie/psychotherapeutische-hochschulambulanz.html Uni HH]\n\n==== 2) Call therapists ====\n\nThis part is, unfortunately, just a big chunk of work. Because there\u2019s no central service for appointments, you simply have to call or mail all potential therapists. This is especially annoying because usually therapists have differing times when they are available, and many either have no slots or a year-long waiting list. Unfortunately, you can\u2019t really see that on any webpage, so you plainly have to call. There are however some things you can do that may make it easier:\n\n* '''Ask friends and family.''' You don\u2019t need to call all therapists on your own to find out whether or not they have open slots.\n* '''Talk to your medical insurance''' Some insurances have a service that calls therapists for you and notifies you if they found one. It\u2019s also important to keep them in the loop because at some point, if you can document that you were unable to find a therapist for some time, your public insurance will also have to pay for a private therapist. The amount of time differs between insurances.\n* '''Use a system to keep track of all the therapists''' Below, we give you some templates which you may use as a basis. Speaking from experience, it is a good idea to have some central place where you collect information like consultation hour times, whether you already tried calling, and so on.\n\nAs a sidenote, and this is just our experience: the waiting list times can differ significantly from what the therapist tells you at first. They might be longer, but they might also be significantly shorter. So don\u2019t be discouraged, and let yourself be registered on the list.\n\n==== 3) Have preliminary sessions ====\n\nYou have the right to five preliminary sessions with each therapist. This is on the one hand useful to get a first diagnosis, and on the other to see if you can confide in the person you\u2019re talking to - recall that one of the most important factors for successful therapy is the relationship between therapist and client.\n\nMany therapists will offer you preliminary sessions, but don\u2019t have a full therapy slot available. Therefore, don\u2019t be tempted to do too many preliminary sessions with different therapists and ask beforehand if there\u2019s a realistic chance to get a full treatment within a reasonable timeframe. \n\n==== 4) Get a slot ====\n\nIdeally, all of the above leads you to finding a therapist that can actually offer you a treatment within a few weeks. You may end up on waiting lists, but if you\u2019ve called enough therapists, you should be able to move your insurance to pay for a private therapist if the wait time is too long. \n\nFrom here on, your therapist will discuss everything relevant with you.\n\n=== III) Managing the process ===\n\nAs alluded to above, there\u2019s a lot of information to handle at the same time, and it can get overwhelming very quickly. Therefore, we\u2019d advise to somehow keep track of your long list of therapists, who you\u2019ve called when and whether or not they have slots available. To support you in this, we provide both a Notion and an Excel template which you may use to manage the process. Feel free to use them either directly or as inspiration for your own system - digital or analog. \n\n'''Important:''' We cannot guarantee for the information in the tables to be up-to-date. The list is from 2021, and information such as consultation hours or telephone numbers may have changed. \n\n==== Notion page ====\n\nYou may use this Notion page as a basis: https://www.notion.so/25a5980e10e3402da19b3d254db6381d. If you have your own Notion account already, you should be able to duplicate the page into your own workspace by pressing the \u201cDuplicate\u201d button to the top right. \n\nIf you do not yet know what Notion is, feel free to glance over our article here: [[Notion]]. We found the tool to be particularly useful because of the rich functionality, but doing the same via a normal table will probably also be fine. \n\n==== Spreadsheet ====\n\nIf you don\u2019t want to use Notion, you may also use this spreadsheet as a basis: https://docs.google.com/spreadsheets/d/1CiBIILxfJK4H3_c22Ty-mJ7gESbSDDEi093-bTLZr94/edit?usp=sharing\n\n=== IV) What you can do on your own ===\n\nAs we stated above, there are a few things that can help you to lift yourself out of the worst phases. Here\u2019s what we found helpful. \n\n=== Practices ===\n\n'''Meditation''' can help you get out of negative thought spirals and liften your mood. If you haven\u2019t tried meditation yet, it might be easiest to download an app or listen to guided meditations on YouTube. Apps we tried out and found helpful are [https://www.headspace.com/ Headspace] and [https://www.tenpercent.com/ 10% Happier]. You can try them for free!\n\nAn '''appreciation journal''' might also be helpful in lightening your mood. It\u2019s rather trivial: just take a few minutes everyday to write down what you\u2019re thankful for. This might feel awkward, or stupid, and you might find it hard to find things you\u2019re sincerely thankful for. But with time, you might be able to direct your focus to the things that are actually good, even if they are comparably small.\n\n'''Journalling''' might also generally be helpful to get thoughts out of your head and onto a piece of (digital) paper. Just sit down every evening for 10 minutes or so and write down whatever is going through your head. If you find it hard to start, just write down what happened on that day. \n\n'''Confide in people you trust.''' It might be hard at first, but our experience is that not only does it help to explicate your thoughts and feelings, but also to understand that you are not alone in how you feel.\n\n===  Lifestyle choices ===\n\nA friend once called these lifestyle choices the \u201cKleines 1x1 der Psychohygiene\u201d (Psychological Hygiene 101):\n\n# '''Sleep regularly and long enough.''' For most people, this means at least 7 and no more than 9 hours of sleep every day, at the same time. From personal experience I would say this makes all the difference in the world. \n# '''Eat & drink regularly.''' Try to eat good, healthy food, and drink at least 2 litres of water per day. \n# '''Do sports.''' Seriously, whatever it is. Walking for 30 mins, jogging, climbing, doing Yoga, going to the Gym, doing jumping jacks. Whatever it is, it\u2019s almost certainly helpful.  \n# '''Regularity.''' This is a big one. Structures help a lot. If you have to study or work, try to do it at a place outside of your home, every day at the same time. The place does not really matter as long as it is not your own room, be it the university or a caf\u00e9 or a friends place. \n\nThese might seem trivial, and they certainly do not solve any underlying causes of whatever problem you might have. They do however help to get out of the hole and into a state where you can actually do something about your situation, and that is incredibly important.\n\n=== The student perspective on lifestyle choices ===\n\nAs a student, this can be especially frustrating. People might be partying and you don\u2019t want to leave at 11 to go to bed. After all, this is supposed to be the time of your life. Everyone else might seem like they just live into their day and do whatever feels right at the moment, and having a structured everyday life might prevent you from doing the same. What might help you is to think about this as a transitional phase: you\u2019re not going to do this forever. It\u2019s a phase you need in order to get back up on your feet and enjoy life again. It might suck, but it won\u2019t suck forever. \n\nWe wish you all the best.\n\n=== Links & Further reading ===\n\n* '''Das Kind in dir muss Heimat finden''' von Stefanie Stahl: https://www.stefaniestahl.de/buecher_daskind_page1/ (Recommendation by the team)\n\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "ackkow96qnszaoiriscq2b6k4w4ceu6"
                }
            },
            {
                "title": "Correlation Plots",
                "ns": "0",
                "id": "720",
                "revision": {
                    "id": "7122",
                    "parentid": "6581",
                    "timestamp": "2023-04-25T21:00:41Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14177",
                        "#text": "'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>\n== What are correlation plots? ==\nIf you want to know more about the relationship of two or more variables, correlation plots are the right tool from your toolbox. And always have in mind, '''correlations can tell you whether two variables are related, but cannot tell you anything about the causality between the variables!'''\n\nWith a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram\n\n\n'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.\n\n== Scatter Plot ==\n=== Definition ===\nScatter plots are easy to build and the right way to go, if you have two numeric variables. They show every observation as a dot in the graph and the further the dots scatter, the less they explain. The position of the dot on the x- and y-axis represent the values of the two numeric variables.\n[[File:MilesHorsePower2.png|350px|thumb|right|Fig.1]]\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\ndata(\"mtcars\")\n#Plotting the scatter plot\nplot(x = mtcars$mpg,\n     y = mtcars$hp,\n     main = \"Correlation between Miles per Gallon and Horsepower\",\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     pch = 16,\n     col = \"red\",\n     las = 1,\n     xlim = c(min(mtcars$mpg), max(mtcars$mpg)),\n     ylim = c(min(mtcars$hp), max(mtcars$hp)))\n</syntaxhighlight>\n\nIn this scatter plot you can easily recognize a strong negative relationship between the variables \u201cmpg\u201d and \u201chp\u201d from the \u201cmtcars\u201d dataset. The Pearson's correlation coefficient is -0.7761684.\n\n<syntaxhighlight lang=\"R\" line>\n#Calculating the coefficient\ncor(mtcars$hp,mtcars$mpg)\n\n## Output: [1] -0.7761684\n</syntaxhighlight>\n\nTo create such a scatter plot, you need the <syntaxhighlight lang=\"R\" inline>plot()</syntaxhighlight> function and define several graphical parameter arguments. In this example, the following parameters were defined:\n\n* '''x:''' variable, that will be displayed on the x-axis.\n* '''y:''' variable, that will be displayed on the y-axis.\n* '''xlab:''' title for the x-axis.\n* '''ylab:''' title for the y-axis.\n* '''pch:''' shape and size of the plotted observations, in this case, filled circles. [http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r Here] you can find an overview of the different possibilities.\n* '''col:''' plotting color. You can either write the name of the color or use the [https://www.r-graph-gallery.com/41-value-of-the-col-function.html color number].\n* '''las:''' style of axis labels. By default it is always parallel to the axis. 1 is always horizontal, 2 is always perpendicular and 3 is always vertical to the axis.\n* '''xlim:''' set the limit of the x-axis.\n* '''ylim:''' set the limit of the y-axis.\n* '''abline:''' this function creates a regression line for the two variables.\n\n== Scatter Plot Matrix ==\n=== Definition ===\nThe normal scatter plot is only useful if you want to know the relationship between two variables, but often you are interested in more than two variables. A convenient way to visualize multiple variables in a scatter plot matrix is offered by the PerformanceAnalytics package. To access the scatter plot matrix from this package, you have to install the package and import the library. After doing that, you can start to select the variables which will be displayed in the plot.\n\n=== R Code ===\n[[File:Scatterplotmatrix.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.2\nlibrary(PerformanceAnalytics)\n\n# Now calling the chart.Correlation() function and defining a few parameters.\ndata <- mtcars[, c(1,3,4,6,7)]\nchart.Correlation(data, histogram = TRUE)\n</syntaxhighlight>\n\n\nThe scatter plot matrix from this package is already very nice by default. It splits the plot into an upper, lower and diagonal part. The upper part consists of the correlation coefficients for the different variables. The red stars show you the results of the implemented correlation test. There is a range from zero to three stars and the higher the number of stars, the higher is the significance of the results for the test. In the diagonal part of the plot are histograms for every variable and show you the distribution of the variable. The bivariate scatter plots can be found on the lower part of the plot and contain a fitted line by default.\n\n\n== Line chart ==\n=== Definition ===\nA line chart can help show how quantitative values for different categories have changed over time. They are typically structured around a temporal x-axis with equal intervals from the earliest to latest point in time. Quantitative values are plotted using joined-up lines that effectively connect consecutive points positioned along a y-axis. The resulting slopes formed between the two ends of each line provide an indication of the local trends between points in time. As this sequence is extended to plot all values across the time frame it forms an overall line representative of the quantitative change over time story for a single categorical value.  \n\nMultiple categories can be displayed in the same view, each represented by a unique line. Sometimes a point (circle/dot) is also used to substantiate the visibility of individual values. The lines used in a line chart will generally be straight. However, sometimes curved line interpolation may be used as a method of estimating values between known data points. This approach can be useful to help emphasise a general trend. While this might slightly compromise the visual accuracy of discrete values if you already have approximations, this will have less impact.\n\n=== R Code ===\nWe will first plot a basic line chart based on a built-in dataset called <syntaxhighlight lang=\"R\" inline>EuStockMarkets</syntaxhighlight>. The data set contains data on the closing stock prices of different European stock indices over the years 1991 to 1998.\n\nTo make things easier, we will first transform the built-in dataset into a data frame object. Then, we will use that data frame to create the plot.\n\nThe table that contains information about the different market indices looks like this:\n\n{| class=\"wikitable\"\n|-\n! DAX !! SMI !! CAC !! FTSE\n|-\n| 1628.75|| 1678.1 || 1772.8 || 2443.6\n|-\n| 1613.63|| 1688.5 || 1750.5 || 2460.2\n|-\n| 1606.51|| 1678.6 || 1718.0 || 2448.2\n|-\n| ... || ... || ... || ...\n|}\n[[File:Simple line chart.png|350px|thumb|right|Fig.3]]\nHere, the data for all the columns are numeric.\n\nThe following line chart shows how the <syntaxhighlight lang=\"R\" inline>DAX</syntaxhighlight> index from the table from previous section.\n\n<syntaxhighlight lang=\"R\" line>\n# Fig.3\n#read the data as a data frame\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX,  # simply select a stock index\n     type='l')       # choose 'l' for line chart\n</syntaxhighlight>\n\n[[File:Line chart.png|350px|thumb|right|Fig.4]]\nAs you can see, the plot is very simple. We can enhance the way this plot looks by making a few tweaks, making it more informative and aesthetically pleasing.\n\n<syntaxhighlight lang=\"R\">\n# Fig.4\n# get the data\neu_stocks <- as.data.frame(EuStockMarkets)\n\n# Plot a basic line chart\nplot(eu_stocks$DAX, # select the data\n     type='l',      # choose 'l' for line chart\n     col='blue',    # choose the color of the line\n     lwd = 2,       # choose the line width \n     main = 'Line Chart of DAX Index (1991-1998)',         # title of the plot\n     xlab = 'Time (1991 to 1998)', ylab = 'Prices in EUR') # x- and y-axis labels\n</syntaxhighlight>\n\nYou can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot</syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n</syntaxhighlight>\n\nClear and meaningful coding and plots are important. In order to achieve this, we have to change the names of the variables from the \u201cmtcars\u201d dataset into something meaningful. One way to do this, is to change the names of the columns and rows of the correlation variable.\n<syntaxhighlight lang=\"R\" line>\ncorrelations <- cor(mtcars)[1:11, 1:11]\ncolnames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\nrownames(correlations) <- c(\"Miles per Gallon\", \"Cylinders\", \n                            \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\n</syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=\"R\" line>\n# Fig.5\ncorrplot(correlations,\n         method = \"circle\",\n         type = \"upper\",\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         tl.cex = 0.6)\n</syntaxhighlight>\n\nThe parameters are different from the previous scatter plots. Obviously, here you need the corrplot() function and define your parameters, regarding to your preferred taste, in this function. Some of the parameters will be explained briefly.\n\n* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (\u201ccircle\u201d, \u201csquare\u201d, \u201cellipse\u201d, \u201cnumber\u201d, \u201cshade\u201d, \u201ccolor\u201d, \u201cpie\u201d), \u201ccircle\u201d is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* '''type''': how the correlation matrix will be displayed. It can either be \u201cupper\u201d, \u201clower\u201d or \u201cfull\u201d. Full is called by default.\n* '''order''': order method for the correlation coefficients. The \u201chclust\u201d method orders them in hierarchical order, but it also possible to order them alphabetical (\u201calphabetical\u201d) or with a [[Principal_Component_Analysis|principal component analysis]] (\u201cPCA\u201d).\n* '''tl.col''': color of the labels.\n* '''tl.srt:''' rotation of the labels in degrees.\n* '''tl.cex:''' size of the labels.\n\n== Visualisation with ggplot ==\n=== Overview ===\n=== R code ===\nCOMING SOON\n\nAs you can see, there are many different ways to visualize correlations between variables. The right correlation plot depends on your data and on the number of variables you want to analyze. But never forget, correlation plots show you only the relationship between the variables and nothing about the causality.\n\n\n== References ==\n* https://sustainabilitymethods.org/index.php/Causality_and_correlation\n* https://en.wikipedia.org/wiki/Correlation_and_dependence\n* https://codingwithmax.com/correlation-vs-causation-examples/\n* https://towardsdatascience.com/what-it-takes-to-be-correlated-ce41ad0d8d7f\n* http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient\n\nA nice example that shows how easy it is to create a spurious correlation:\n\n* https://rstudio-pubs-static.s3.amazonaws.com/4192_1180a799cd6c4d2ba6e4ed2702860efb.html\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden and ?."
                    },
                    "sha1": "lj4ksome631y9bqsgh1v4r2xp6ch0yn"
                }
            },
            {
                "title": "Correlations",
                "ns": "0",
                "id": "13",
                "revision": {
                    "id": "6371",
                    "parentid": "6213",
                    "timestamp": "2021-09-14T15:39:16Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19258",
                        "#text": "[[File:ConceptCorrelation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Correlations]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n\n<br/><br/>\n'''In short:''' Correlation analysis examines the statistical relationship between two continuous variables. For R examples on Correlations, please refer to [[Correlation Plots]].\n\n== Background ==\n[[File:Correlation.png|400px|thumb|right|'''SCOPUS hits per year for Correlations until 2020.''' Search terms: 'Correlation' in Title, Abstract, Keywords. Source: own.]]\n\nKarl Pearson is considered to be the founding father of mathematical statistics; hence it is no surprise that one of the central methods in statistics - to test the relationship between two continuous variables - was invented by him at the brink of the 20th century (see Karl Pearson's \"Notes on regression and inheritance in the case of two parents\" from 1895). His contribution was based on work from Francis Galton and Auguste Bravais. With more data becoming available and the need for an \u201cexact science\u201d as part of the industrialization and the rise of modern science, the Pearson correlation paved the road to modern statistics at the beginning of the 20th century. While other approaches such as the t-test or the Analysis of Variance ([[ANOVA]]) by Pearson's arch-enemy Fisher demanded an experimental approach, the correlation simply required data with a continuous measurement level. Hence it appealed to the demand for an analysis that could be conducted based solely on measurements done in engineering, or on counting as in economics, without being preoccupied too deeply with the reasoning on why variables correlated. '''Pearson recognized the predictive power of his discovery, and the correlation analysis became one of the most abundantly used statistical approaches in diverse disciplines such as economics, ecology, psychology and social sciences.''' Later came the \u200bregression analysis, which implies a causal link between two continuous variables. This makes it different from a correlation, where two variables are related, but not necessarily causally linked. This article focuses on correlation analysis and only touches upon regressions. For more, please refer to the entry on [[Regression Analysis]].)\n\n\n== What the method does ==\nCorrelation analysis examines the relationship between two [[Data formats|continuous variables]], and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called ''correlation coefficient'' indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. \n\nCorrelations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including [[Survey]]s and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date.\n\n'''There are different forms of correlation analysis.''' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https://365datascience.com/students-t-distribution/ Student's t-distribution]. Alternative correlation measures like [https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ Kendall's tau and Spearman's rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data.\n\n==== Calculating Pearson's correlation coefficient r ====\nThe formula to calculate [https://www.youtube.com/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br/>\n\n'''1) Is the relationship between two variables positive or negative?''' If one variable increases, and the other one increases, too, we have  a positive (\"+\") correlation. This is also true if both variables decrease. For instance, being taller leads to a significant increase in\u00a0[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534609/ body weight]. On the other hand, if one variable increases, and the other decreases, the correlation is negative (\"-\"): for example, the relationship between 'pizza eaten' and 'pizza left' is negative. The more pizza slices are eaten, the fewer slices are still there. This direction of the relationship tells you a lot about how two variables might be logically connected. The normative value of a positive or negative relation typically has strong implications, especially if both directions are theoretically possible. Therefore it is vital to be able to interpret the direction of a correlative relationship. \n\n'''2) Is the correlation coefficient small or large?''' It can range from -1 to +1, and is an important measure when we evaluate the strength of a statistical relationship. Data points may scatter widely in a [[Correlation_Plots#Scatter_Plot|scatter plot,]] or there may be a rather linear relationship - and everything in between. An example for a perfect positive correlation (with a correlation coefficient ''r'' of +1) is the relationship between temperature in [[To_Rule_And_To_Measure#Celsius_vs_Fahrenheit_vs_Kelvin|Celsius and Fahrenheit]]. This should not be surprising, since Fahrenheit is defined as 32 + 1.8\u00b0 C. Therefore, their relationship is perfectly linear, which results in such a strong correlation coefficient. We can thus say that 100% of the variation in temperature in Fahrenheit is explained by the temperature in Celsius.\n\nOn the other hand, you might encounter data of two variables that is scattered all the way in a scatter plot and you cannot find a significant relationship. The correlation coefficient ''r'' might be around 0.1, or 0.2. Here, you can assume that there is no strong relationship between these two variables, and that one variable does not explain the other one. \n\nThe stronger the correlation coefficient of a relation is, the more may these relations matter, some may argue. If the points are distributed like stars in the sky, then the relationship is probably not significant and interesting. Of course this is not entirely generalisable, but it is definitely true that a neutral relation only tells you, that the relation does not matter. At the same time, even weaker relations may give important initial insights into the data, and if two variables show any kind of relation, it is good to know the strength. By practising to quickly grasp the strength of a correlation, you become really fast in understanding relationships in data. Having this kind of skill is essential for anyone interested in approximating facts through quantitative data.  \n\n'''3) What does the relationship between two variables explain?''' \nThis is already an advanced skill, and is rather related to regression analysis. So if you have looked at the strength of a correlation, and its direction, you are good to go generally. But sometimes, these measures change in different parts of the data. \n\nTo illustrate this, let us have a look at the example of the percentage of people working in\u00a0[https://ourworldindata.org/employment-in-agriculture?source=post_page--------------------------- Agriculture]\u00a0within individual countries. Across the world, people at a low income (<5000 Dollar/year) have a high variability in terms of agricultural employment:  half of the population of the Chad work in agriculture, while in Zimbabwe it is only 10\u00a0%. However, at an income above 15000 Dollar/year, there is hardly any variance in the percentage of people that work in agriculture: it is always very low. If you plotted this, you would see that the data points are rather broadly spread in the lower x-values (with x as the income), but are more linearly spread in the higher income areas (= x values). This has reasons, and there are probably one or several variables that explain this variability. Maybe there are other factors that have a stronger influence on the percentage of farmers in lower income groups than for higher incomes, where the income is a good predictor. \n\nA correlation analysis helps us identify such variances in the data relationship, and we should look at correlation coefficients and the direction of the relationship for different parts of the data. We often have a stronger relation across parts of the dataset, and a weaker relation across other parts of the dataset. These differences are important, as they hint at underlying influencing variables or factors that we did not understand yet.\n\n[[File:Bildschirmfoto 2019-10-18 um 10.38.48.png|400px|thumb|center|'''This graph shows the positive correlation between global nutrition and the life expectancy.''' Source: Gapminder]]\n\n[[File:Bildschirmfoto 2019-10-18 um 10.51.34.png|400px|thumb|center|'''By comparison, life expectancy and agricultural land have no correlation - which obviously makes sense.''' Source: Gapminder]]\n \n[[File:Bildschirmfoto 2019-10-18 um 10.30.35.png|400px|thumb|center|'''Income does have an impact on how much CO2 a country emits.''' Source: Gapminder]]\n\n\n=== A quick introduction to regression lines ===\nAs you can see, '''correlation analysis is first and foremost a matter of identifying ''if'' and ''how'' two variables are related.''' We do not necessarily assume that we can predict the value of one variable based on the value of the other variable - we only see how they are related. People often show a correlation in a scatter plot - the x-axis is one variable, the y-axis the other one. You can see this in the example below. Then, they put a line on the data. This line - the \"regression line\" - represents the correlation coefficient. It is the best approximation for all data points. This means that this line has the minimum distance to all data points. If all data points are exactly on the line, we have a correlation of +1 or -1 (depending on the direction of the line). However, the further the data points are from the line, the closer the correlation coefficient is to 0, and the less meaningful the correlation is.\n\n[[File:Correlation coefficient examples.png|600px|thumb|center|'''Examples for the correlation coefficient.''' Source: Wikipedia, Kiatdd, CC BY-SA 3.0]]\n<br>\n'''It is however important to know two things:'''<br>\n1) Do not confuse the slope of this line (the 'regression coefficient') - i.e. the number of y-values that the regression line steps per x-value - with the correlation coefficient. They are not the same, and this often leads to confusion. The regression coefficient of the line can easily be 5 or 10, but the correlation coefficient will always be between -1 and +1.\n\n2) Regressions only really make sense if there is some kind of causal explanation for the relationship. We can create a regression line for all correlations of all pairs of two variables, but we might end up suggesting a causal relationship when there really is none. As an example, have a look at the correlation below. There is no regression line here, but the visualisation implies that there is some connection, right? However, it does not really make sense that the divorce rate in Maine and the margarine consumption are related, even though their correlation coefficient is obviously quite high! So you should always question correlations, and ask yourself which kinds of variables are tested for their relationship, and if you can derive meaningful results from doing so.\n\n[[File:860-header-explainer-correlationchart.jpg|500px|thumb|center|'''Correlations can be deceitful'''. Source: [http://www.tylervigen.com/spurious-correlations Spurious Correlations]]]\n<br>\n\n== Strengths & Challenges ==\n* Correlation analysis can be a powerful tool both for inductive reasoning, without a theoretical foundation; or deductive reasoning, which is based on theory. This makes it versatile and has enabled new discoveries as well as the support of existing theories.\n* The versatility of the method expands over all spatial and temporal scales, and basically any discipline that uses continuous data. This makes it clear why correlation analysis has become such a powerhorse for many researchers over time, and is so prevalent also in public debates.\n* Correlations are rather easy to apply, and most software allows to derive simple scatterplots that can then be analyzed using correlations. However, you need some minimal knowledge about data distribution, since for instance the Pearson correlation is based on data that is normally distributed.\n\n\n== Normativity ==\n* With the power of correlations comes a great responsibility for the researcher. It can be tempting to infer causality and a logical relatoinship between two variables purely from the results of correlations. Economics and other fields have a long history of causal interpretation based on observed associations from the results of correlation analyses. However, researchers should always question whether there is a plausible connection between two variables, even if - or especially when - the correlation seems so clear. A regression analysis, that allows for the prediction of data beyond what can be observed, should especially only be done if there is a logical underlying connection. Keep in mind that regression = correlation + causality. For more thoughts on the connection between correlations and causality, have a look at this entry: [[Causality and correlation]].\n* Another normative problem of correlations is rooted in so called statistical fishing. With more and more data becoming available, there is an increasing chance that certain correlations are just significant by chance, for which there is a corrective procedure available called [https://www.youtube.com/watch?v=HLzS5wPqWR0 Bonferroni correction]. However, this is seldom applied. Today, p-value-driven statistics are increasingly seen critical, and the resulting correlations should be seen as no more than an initial form of a mostly inductive analysis. With some practice, p-value-driven statistics can be a robust tool to compare statistical relations in continuous data, but more complex methods may be useful to better understand the relationships in the data.\n* There is an endless debate about what constitutes a meaninful, strong correlation. Yet, this depends widely on the context and the field of research - for some disciplines, topics, or research questions, a correlation of +0.4 may be meaningful, while it is mostly irrelevant in others. It is a matter of experience and contextualisation how much meaning we infer on correlation coefficients. Furthermore, finding no correlation between two variables is an important statistical result, too.\n\n\n== Outlook ==\nCorrelations are among the foundational pillars of frequentist statistics. Nonetheless, with science engaging in more complex designs and analysis, correlations will increasingly become less important. As a robust working horse for initial analysis, however, they will remain a good starting point for many datasets. Time will tell whether other approaches - such as [[Bayesian Inference|Bayesian statistics]] and [[Machine Learning|machine learning]] - will ultimately become more abundant. Correlations may benefit from a clear comparison to results based on Bayesian statistics. Until then, we should all be aware of the possibilities and limits of correlations, and what they can - and cannot - tell us about data and its underlying relationships.\n\n\n== Key Publications ==\nHazewinkel, Michiel, ed. (2001). ''Correlation (in statistics)''. Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers.\n\n\ufeffBabbie, Earl. 2016. ''The Practice of Social Research.'' 14th ed. Boston: Cengage Learning.\n\nNeuman, W. Lawrence. 2014. ''Social Research Methods: Qualitative and Quantitative Approaches.'' 7th ed. Pearson.\n\n\n\n== Further Information ==\n* If you want to practise recognizing whether a correlation is weak or strong I recommend spending some time on this website. There you can guess the correlation coefficients based on graphs: http://guessthecorrelation.com/\n\n* [https://online.stat.psu.edu/stat501/lesson/1/1.6 The correlation coefficient]: A very detailed and vivid article\n\n* [https://online.stat.psu.edu/stat501/lesson/1/1.7 The relationship of temperature in Celsius and Fahrenheit]: Several examples of interpreting the correlation coefficient\n\n* [https://www.mathbootcamps.com/reading-scatterplots/ How to read scatter plots]\n\n* [https://ourworldindata.org/employment-in-agriculture?source=post_page--------------------------- Employment in Agriculture]: A detailed database\n\n* [https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ Kendall's Tau & Spearman's Rank]: Two examples for other forms of correlation\n\n* [https://study.com/academy/lesson/scatter-plot-and-correlation-definition-example-analysis.html Strength of Correlation Plots]: Some examples\n\n* [https://explorable.com/history-of-antibiotics History of antibiotics]: An example for findings when using the inductive approach\n\n* [https://www.youtube.com/watch?v=372iaWfH-Dg Pearson's correlation coefficient]: Many examples\n\n* [https://www.youtube.com/watch?v=WpZi02ulCvQ Pearson correlation]: A quick explanation\n\n* [https://www.youtube.com/watch?v=2B_UW-RweSE Pearson's r Correlation]: An example calculation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz."
                    },
                    "sha1": "fr387xvdce3365uqhzgu75pgc7v6nig"
                }
            },
            {
                "title": "Counting Birds",
                "ns": "0",
                "id": "1069",
                "revision": {
                    "id": "7276",
                    "parentid": "7156",
                    "timestamp": "2023-07-31T11:27:38Z",
                    "contributor": {
                        "username": "Annrau",
                        "id": "128"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19004",
                        "#text": "[[File:ConceptBIRDS.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[COUNTINGBIRDS]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n\n\n'''In short:''' Territory mapping, point counts and line transects are key methods for counting birds. They are suitable for many different groups and species of birds. Additional methods can be found in Sutherland (1996).\n\n'''Table 1. The use of the methods territory mapping, point counts and line transects for different groups of birds (modified after Sutherland 1996).''' These methods were selected because they can be applied to most bird species.\n\n'''*'''=method usually applicable , '''+'''=method often applicable.  \n\n{| class=\"wikitable\"\n|-\n| || Territory Mapping|| Line transects|| Point counts\n|-\n| Waterbirds|| + || + || + \n|-\n| Seabirds|| || * || + \n|-\n| Wading birds|| + || + ||  \n|-\n| Raptors||  +|| + ||  \n|-\n| Gamebirds|| + || + ||  \n|-\n| Near passerines|| + || + || + \n|-\n| Passerines|| * || * || + \n|}\n\n\n== Territory Mapping ==\n=== What the method does ===\n\n'''Creating a map of all sampling points for each occasion''' \n\nTerritory mapping is a method used to study populations of territorial breeding species such as some ducks, gamebirds, raptors and most passerines. These species are territorial during the breeding season.  Males sing in their territory and defend the borders of their territory by disputes. Therefore, the breeding territory is used as a census unit. Territory mapping determines the number of territories of each species in a given area. Multiple species can be mapped during one visit (1). \nIn the first step, the land cover of the study plot is mapped at a scale of commonly 1:2500. Then, obvious features of the plot, such as houses, isolated trees, hedges, ponds etc. are marked on the map. In closed habitats such as temperate woodlands plot sizes of 15-20 ha are often sufficient. In a tropical forest, even half of this area could already be suitable. However, in more open habitats, an area of 60-80 ha is needed (1). \n\nThe length of the plot borders should be kept as short as possible by not selecting a long, thin area. This is because many bird territories would be overlapping if the plot boundary was long. To determine if a territory belongs to the plot or not might be difficult in some cases. Therefore, round or square plots should be preferred. Species-rich features such as hedges should also not be located at the plot boundary (1).\nIn temperate woodlands, 10 sampling visits per breeding season are recommended. In temperate open habitats, 5 visits may already be sufficient. To cover both early and late breeding species, the visits should be equally distributed throughout the whole breeding season. For songbirds, the census should usually be started after the first hour of dawn and continue no longer than midday. The first hour after dawn may commonly be omitted because most birds are then singing, which might be confusing in areas with high bird densities. After midday, most birds stop singing which usually makes this time frame unsuitable as well. Each visit to a plot can be completed during a morning (1).\n\nIn practical terms, in the beginning of the breeding season, several plot maps are produced, one for each visit. The map is attached to a clipboard that can be covered with a large polyethylene bag in case of rain. By walking at a slow pace, the plot is covered with several routes in parallel with a 50 m distance in between the routes. Each bird that was heard or observed is marked at the location with a species code. Comments are made for evidence of nesting such as nests or birds carrying nesting material (1).\n\nIf only a certain group of birds, such as raptors, is censused, the same area can be covered much faster by ignoring all other species and mapping at a bigger scale (e.g., 1:10.000 (2)).\n\n'''Analysis of maps'''\n\nAfter the end of the breeding season, the results for each species in each single-visit map are transferred to a species map covering all visits. This is done by marking each observation for visit 1 with an A on the species map, for visit 2 with a B, and so on. After the map is finished for each species, circles are drawn around clusters of observations. Each cluster should contain at least two registrations for 5-7 visits and at least three for 8-10 visits. Each nest is automatically considered to be a cluster. There are different methods to deal with edge territories (1). We suggest considering a cluster to be within the plot if more than 50% of its registrations lie within the plot.\n\n===Strengths & challenges===\nTerritory mapping is a very work-intensive method, that requires good skills in bird identification by the observer. It cannot be applied to species that are colonial or live in loose groups. However, due to the longer time spent in the field compared to line transects and point counts, there is a lower impact of environmental variation such as the weather or the time of visit (1).\n\n== Point Counts ==\n=== What the method does ===\nPoint counts are suitable for assessing the relative abundance of vocal or at least highly visible bird species such as passerines. They can be applied in a wide variety of habitats and at any time of the year, not restricted to the breeding season. Fixed counting stations are placed across the study area, either in a grid or in a random manner. From these locations, birds are observed for a fixed time period, usually between 3 and 10 minutes. Often, five minutes are adequate (3).\n\nThe distance between the counting stations should at least be 200 m to prevent counting the same individual twice. If the distance between the points is too large, it takes too much time to travel from one counting station to the next. Per study plot, at least 20 counting stations should be installed. To keep the minimum distance of 200 m in between the points, the plot should not be too small (1). \n\nAfter arriving at the counting station, wait a few minutes for the birds to resume their normal behavior. Then, during the observation time, all birds seen or heard are counted. As most of the birds will be counted in the first few minutes, spending more than 10 minutes is usually inefficient and may lead to increased double counting. Only in areas with very rich bird fauna or where species are especially hard to detect, spending more than 10 minutes might be necessary. \n\nTo avoid double counting, the approximate location of every bird can be noted on a page of a notebook (1). The page can, for example, be divided into four quarters and birds recorded in these quarters (i.e. front-left, front-right, behind-left and behind-right).\n\nIf only abundances are required, birds may be counted irrespective of the distance to the observer. Such approach is not suited, though, if comparisons are to be made between species and/or different habitats. To determine bird densities, a \u201cnear belt\u201d and a \u201cfar belt\u201d are established. The near belt is usually 25 m (in closed habitats such as forests) or 50 m (in open habitats) around the counting station. Beyond this distance is the \u201cfar belt\u201d. For each individual, the observer notes if it was first seen in the near belt or in the far belt (1). Formulae for easy calculation of bird density based on this assessment can be found in (4) and (5). \n\nAt every point, at least two observations should be conducted, one in the beginning and one in the end of the period that should be covered, e.g. the breeding season. For each plot, the time of the day should be noted as this influences the activity and detectability of birds (1). \n\n=== Strengths & challenges ===\nPoint counts are suitable for quickly collecting large amounts of data, even outside of the breeding season. This method can be applied for all birds that can easily be detected by song. The observer, however, needs to have a high level of experience in determining different bird species by song. Compared to territory mapping, counting stations can be distributed relatively easily in a random pattern (1). \nSince point counts often rely on the birds singing (or sometimes detecting them by eye), this method is unsuitable for less detectable species. The approach may also not be suited for open habitats where birds are likely to flee from the observer (1). \n\n== Line transects ==\n=== What the method does ===\nLine transects are a method for counting birds of extensive open habitats. It is suitable for shrub-steppe, moorland, offshore seabirds and waterbirds. The observer moves freely through the land, sea or air. Observers move along a route and note down all birds they see on either side. In the first step, a route is determined that should be followed. This route should be positioned relatively randomly. It should not follow a path, hedge, stream, road or similar features as the results obtained here may strongly differ from the surrounding area. Moreover, counting seabirds from fishing trawlers should be prevented as they may attract birds (1). \n\nA transect route does not have to be straight, but can also be rectangular allowing the observer to end at the starting point. The route should be planned so that following it during subsequent visits is as easy as possible. Therefore, circular routes are not recommended (1).\n\nIt is recommended to split the total length of the transect into smaller distances. These can directly continue into each other or they can be separated. If several transects should be undertaken, they should be at least 150 m apart in closed habitats and at least 250 m apart in open habitats to prevent the observer from counting the same individual twice. Additionally, the observers need to decide how often they want to visit the transects. It makes sense to complete the same transect several times as species detectability varies seasonally (1). \n\nSimilar to point counts, to determine bird density, a \u201cnear belt\u201d and a \u201cfar belt\u201d may need to be established for line transects. Commonly, the near belt is 25 m to either side of the transect line. Beyond this distance is the \u201cfar belt\u201d. For each individual, the observer notes if it was first seen in the near belt or in the far belt. Formulae for easy calculation of bird density based on this assessment can be found in (4) and (5). \n\nWalking speed should be around 1 km (forests) to 2 km (open habitats) per hour. In a notebook with the schematic representation of the transect including the near belt and the far belt, the location where the bird was first seen is recorded. Birds landing or singing overhead are recorded in the central belt while birds flying over are recorded in the far belt (1). \n\n=== Strengths & challenges ===\nLine transects are especially useful for large areas of homogenous habitat and areas where bird populations occur at low density. It is possible to calculate estimates of density. The costs for this method depend on the habitat. Terrestrial habitats can be censused relatively cheaply whereas sea habitats are more expensive as a ship is needed (1).\n\n== Normativity ==\nTerritory mapping has a slight bias towards under-estimating the number of breeding pairs as paired males of some species sing less than unpaired males (6). The assumption of this method, that birds live in pairs in non-overlapping territories, is false for some species such as polygynous species and polyterritorial species. As there is a large proportion of subjectivity involved even when a standard protocol is used, inter-observer variation can be an issue (1).\n\nThe main bias for point counts is that the length of time spent in the field is rather short. Therefore, the results can be strongly influenced by weather conditions. For this reason, strong winds, rain or cold weather are unsuitable for point counts (1). Moreover, the presence of the observer might repel or attract birds that are close to the observer which can seriously impact calculations of bird density, as the area sampled by a point count increases geometrically with the distance from the observer (7).\n\nBird densities that are calculated based on line transects can be biased by several factors. For instance, birds can easily be missed by walking too fast or counted twice by walking too slowly. Furthermore, errors in the estimate of distances have a strong impact on the calculation of bird densities. Also, it might happen that one bird is not detected because of being alarmed by other birds. This highlights that this method relies on assumptions that are, in practice, often not met (1).\n\n== Outlook ==\nThe development of bird census data sets is increasingly impacted by the growing movement of citizen science. Especially during the Covid-19 pandemic where many other leisure activities were not available due to restrictions, many people turned to outdoor activities such as bird watching. \n\nThis trend in citizen participation is supported by technical development making bird identification easier through the usage of identification apps. Smartphone apps such as \u201ceBirds\u201d help to identify species and also upload photos and the location where the individual was observed. This leads to a growing global online-community of bird watchers who create new data points in observation databases that scientists and environmentalists can make use of for conservation research or planning. For instance, in Canada, the number of people submitting photos to eBirds increased by 30% between 2019 and 2020 (8).\n\nBecause many bird watchers are novices, bird identification is not always carried out correctly. This is a general problem of citizen science. In the case of eBirds, bird experts volunteer as reviewers who check photos for correct determination, especially in the case of rare species that are uncommon in a certain area. This increases the quality of the data (8). However, a study conducted in the UK has shown that citizen science data may be reliable only for widespread and common species, even with many data points and a good coverage of the area (9). \n\nScientists hope that [[Citizen_Science|citizen science]] data may fill data gaps, for instance in the tropics. However, comparisons to data from Bird Life International have shown that the bird abundances for rare species in the tropics are strongly over-estimated. A likely reason is that bird observers are very determined to find these rare species and thereby overlook or under-record more common species and focus on rare ones (10). \nNevertheless, as technology progresses, identification apps are becoming better and easier to use. Moreover, scientists are working on new approaches to include such data into their research in meaningful ways. This trend of increasing citizen participation is thus likely to continue.\n\nTelemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is \u201cthe system of determining information about an animal through the use of radio signals from or to a device carried by the animal\u201d (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===\n\nFuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\nSutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\nRobertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work. British Trust for Ornithology, Tring.\n\n=== Empirical ===\n\nGibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tama\u00f1o de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n== References ==\n(1) Sutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. p. 227-259. Cambridge University Press.\n\n(2) Robertson, J. G. M., & Skoglund, T. (1985). A method for mapping birds of conservation interest over large areas. Bird census and atlas work.\n \n(3) Fuller, R. J., & Langslow, D. R. (1984). Estimating numbers of birds by point counts: how long should counts last?. Bird study, 31(3), 195-202.\n\n(4) Bibby, C. J., Burgess, N. D., Hillis, D. M., Hill, D. A., & Mustoe, S. (1992).\u00a0Bird census techniques. Elsevier.\n\n(5) Buckland, S. T., Anderson, D. R., Burnham, K. P., & Laake, J. L. (1993). Distance sampling: estimating abundance of biological populations. Chapman & Hall, London.\n\n(6) Gibbs, J. P., & Wenny, D. G. (1993). Song Output as a Population Estimator: Effect of Male Pairing Status (El Canto Utilizado para Estimar el Tama\u00f1o de Poblaciones: El Efecto de Machos Apareados y No-apareados). Journal of Field Ornithology, 316-322.\n\n(7) Verner, J. (1985). Assessment of counting techniques. Current Ornithology: Volume 2, 247-302.\n\n(8) CBC (2021). How birding\u2019s pandemic popularity is expanding data collection for science.  https://www.cbc.ca/news/science/science-birding-pandemic-data-wildlife-1.6113333 (accessed on 06.03.2023)\n\n(9) Boersch-Supan, P. H., Trask, A. E., & Baillie, S. R. (2019). Robustness of simple avian population trend models for semi-structured citizen science data is species-dependent. Biological Conservation, 240, 108286.\n\n(10) Science Daily (2020). Community science birding data does not yet capture global bird trends. https://www.sciencedaily.com/releases/2020/07/200707084012.html (accessed on 06.03.2023)\n\n(11) Gutema, T. M. (2015). Wildlife radio telemetry: use, effect and ethical consideration with emphasis on birds and mammals. Int J Sci Basic Appl Res, 24(2), 306-313.\n\n(12) Barron, D. G., Brawn, J. D., & Weatherhead, P. J. (2010). Meta\u2010analysis of transmitter effects on avian behaviour and ecology. Methods in Ecology and Evolution, 1(2), 180-187.\n\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau.\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]"
                    },
                    "sha1": "2qnuxeuvn16x295jl2qm0vuxm308k8j"
                }
            },
            {
                "title": "Courses",
                "ns": "0",
                "id": "312",
                "revision": {
                    "id": "5742",
                    "parentid": "5741",
                    "timestamp": "2021-06-09T07:51:25Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "1098",
                        "#text": "'''The Courses section will be dedicated to entry points into diverse topics of science.'''<br>\nMore specifically, we will curate online courses, consisting of selected Wiki entries, videos, and exercises. These will help you gain an overview on the basics of specific thematical fields, understand how topics relate to each other, and receive further material to engage with. Each course may take between a day and several weeks, depending on your intended focus.\n\n\nCurrently, there is a selection of Wiki entries for two Leuphana university courses:\n\n'''[[Different paths to knowledge]]'''<br/>\nThis course introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for current and future academic inquiry.\n\n\n'''[[Methods of Environmental Sciences]]'''<br/>\nThis course covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective."
                    },
                    "sha1": "e0s21qremt1kxrbkazpye15tf11frju"
                }
            },
            {
                "title": "Cronbach's Alpha",
                "ns": "0",
                "id": "607",
                "revision": {
                    "id": "5311",
                    "parentid": "4145",
                    "timestamp": "2021-05-18T19:42:01Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What is Cronbach's Alpha? */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7358",
                        "#text": "'''In short:''' Cronbach\u2019s alpha is a measure used to estimate the reliability, or internal consistency, of a composite score or test items. It is calculated by correlating the score for each scale item with the total score for each observation (usually individual test takers), and then comparing that to the variance for all individual item scores.\n\n== What is Cronbach's Alpha? ==\n\nIn the context of test-reliability Cronbach's Alpha is one way of measuring the extent to which a given measurement is a consistent measure of a [[Glossary|concept]]. The concept of Internal consistency can be connected to the interrelatedness of a set of items. It describes the extent to which all items in a test measure the same concept; it is connected to the inter-relatedness of the items within the test.\n\nImagine an individual takes a Happiness Survey. Your calculated happiness score would be highly reliable (consistent) if your test produces a similar result when the same individual re-takes the survey, under the same conditions. However, the measure would not be reliable at all if an individual at the same level of real happiness takes the Survey twice back-to-back and receives one high and one low happiness score. Cronbach's Alpha is sed under the assumption that you have multiple items measuring the same underlying construct. In this context you might have five questions asking different things, but when combined, could be said to measure overall happiness.\n\n== How to calculate Cronbach's Alpha? ==\nCronbach's Alpha is calculated by correlating the score for each scale item with the total score for each observation (usually individual test takers), and then comparing that to the variance for all individual item scores:\n\n[[File:formulas.png|550px|frameless|center]]\n\n=== Example: World Happiness Report ===\nData available at [https://drive.google.com/file/d/1DLZ_gVFhsT0dBROLE79h2Y5_6NXRF2Br/view Google Drive]. \nFor the sake of clarity, only 4 items are selected in this example to measure the construct \"Happiness\" (Happiness.Score), among others.\n\n<syntaxhighlight lang=\"R\" line>\n# use libraries tidyverse and psych\nlibrary(tidyverse)\nlibrary(psych)\n\n# load data and select four exemplary columns\nWorld_Happiness_Report %>% \n  select(economy = Economy..GDP.per.Capita., Family, health = Health..Life.Expectancy., Freedom) -> World_Happiness_Report_4col\n\n# calculate Cronbach's Alpha\n# The argument 'check.keys' checkes if all items have the same polarity, if necessary wrongly polarized items are reversed by R\nalpha(World_Happiness_Report_4col, check.keys=TRUE)\n\n#Output:\n# Reliability analysis   \n# Call: alpha(x = World_Happiness_Report_4col, check.keys = TRUE)\n# \n#   raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n#       0.81      0.83    0.83      0.55 4.8 0.017 0.78 0.23     0.52\n# \n#  lower alpha upper     95% confidence boundaries\n#   0.78 0.81 0.85 \n\n#  Reliability if an item is dropped:\n#         raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\n# economy      0.71      0.72    0.65      0.46 2.6    0.034 0.018  0.42\n# Family       0.73      0.77    0.77      0.52 3.3    0.024 0.078  0.37\n# health       0.70      0.75    0.70      0.49 2.9    0.029 0.029  0.42\n# Freedom      0.85      0.88    0.86      0.71 7.5    0.016 0.014  0.69\n# \n#  Item statistics \n#           n raw.r std.r r.cor r.drop mean   sd\n# economy 155  0.94  0.89  0.90   0.82 0.98 0.42\n# Family  155  0.85  0.84  0.76   0.71 1.19 0.29\n# health  155  0.88  0.86  0.85   0.80 0.55 0.24\n# Freedom 155  0.55  0.66  0.45   0.42 0.41 0.15\n\n</syntaxhighlight>\n'''Output:'''\n\nThe first column contains the value for Cronbach's Alpha for the total scale (in this case 0.81)\nimage.png\n\nThe first column (raw_alpha) lists the scale's internal consistency if it does not contain the specific item.\nThe smaller this value is compared to the internal consistency for the total scale, the better the contribution of this item to the reliability.\nIn this example the total alpha could be increased to 0.85 by dropping out the item 'Freedom'\nimage.png\n\nN explains the number of valid values for each item. The columns mean (average) and sd (standard deviation) can be used as an indicator for the difficulty of the items.\nThe higher the mean of an item, the easier it is to agree with this item. Items with very high or very low mean values should not be included in the scale because they are too easy or too difficult, respectively.\nimage.png\n\n== How to interpret Cronbach's Alpha? ==\n* The resulting alpha coefficient ranges from 0 to 1\n* ''\u03b1 = 0'' if all the items share no covariance and are not correlated which means that all the scale items are entirely independent from one another\n* ''\u03b1 will approach 1'' if the items have high covariances and are highly correlated. The higher the coefficient the more the items probably measure the same underlying concept\n* ''Negative values for \u03b1'' indicate problems within your data e.g. you may have forgotten to reverse score some items.\n\nWhen interpreting Cronbach's Alpha understandings for what makes a 'good' coefficient may differ according to your application field and depend on one's theoretical knowledge.\nMost often methodologists recommend a minimum coefficient between ''0.65'' and ''0.8''. Values above 0.8 are considered as best in many cases where values less than 0.5 are unacceptable.\n\n== Be aware of: ==\n* Too high \u03b1 coefficient as a possible sign for redundancy\nWhen interpreting a scale\u2019s \u03b1 coefficient, one should think about the alpha being a function of covariances among items and the number of items in the analysis. Therefore a high coefficient is not solely a markt of a reliable set of items as alpha can simply be increased by increasing the number of items in the analysis. Therefore a too high \u03b1 coefficient (i.e. > 0.95) can be a sign of redundancy in the scale items.\n\n* Cronbach's Alpha is also affected by the length of the test\nA larger number of items can result in a larger \u03b1, smaller number of items in a smaller \u03b1.\nIf alpha is too high, your analysis may include items asking the same things whereas a low value for alpha may mean that there aren\u2019t enough questions on the test.\n\n* For Accessing how 'good' a scale is at measuring a concept you'll need more than a simple test of reliability\nHighly reliable measurements are defined as containing zero or very little random measurement error resulting in inconsistent results. However, having a scale with a high \u03b1 coefficient does not tell about the scale's validity, e.g. if the items mirror the underlying concept in an appropriate way. Regarding examplary the construct validity one could adress it by examining whether or not there exist empirical relationships between the measure of the underlying concept of interest and other concepts to which it should be theoretically related.\n\n== Further useful links: ==\n* [https://www.rdocumentation.org/packages/psych/versions/2.0.9/topics/Alpha alpha-function in R documentation]\n* [https://link.springer.com/article/10.1007/s11165-016-9602-2 The Use of Cronbach\u2019s Alpha when Developingand Reporting Research Instruments in Science Education]\n* [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2792363/ On the Use, the Misuse, an the very Limited Usefulness of Cronbach's Alpha]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "tb72o1lz0uk7vbgy2z4uqn9ikrg9fk9"
                }
            },
            {
                "title": "Data Inspection in Python",
                "ns": "0",
                "id": "1025",
                "revision": {
                    "id": "7283",
                    "parentid": "7261",
                    "timestamp": "2023-08-29T09:55:28Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This articles provides a thorough introduction to data inspection which is needed to know how to approach data cleaning.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "10980",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n== Pretext ==\nData inspection is an important and necessary step in data science because it allows data scientists to understand the characteristics of their data and identify potential problems or issues with it. By carefully inspecting the data, data scientists can ensure that the data is accurate, complete, and most importantly relevant to the problem they are trying to solve.\n\n===Where to find Data?===\n\nThere are many platforms where you can find data. These are five common and reliable ones:\n\n* https://www.kaggle.com/datasets\n* https://datasetsearch.research.google.com/\n* https://data.gov/\n* https://datahub.io/collections\n* https://archive.ics.uci.edu/ml/datasets.php\n\n==Let's Start the Data Inspection==\nFor our data inspection, we will be using the following packages:\n1. '''Pandas''', which is a standard data analysis package, and\n2. '''Matplotlib''', which can help us create beautiful visualizations of our data.\nWe can import the packages as follows:\n\n<syntaxhighlight lang=\"Python\" line>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# shows more columns\npd.set_option('display.max_columns', 500)\n</syntaxhighlight>\n\n===Load the data===\nTo load the data, we need to use the appropriate command depending on the type of data file we have. For example, if we have a CSV file, we can load it using the read_csv method from Pandas, like this:\n\n<syntaxhighlight lang=\"Python\" line>\n# data source: https://www.kaggle.com/datasets/ssarkar445/crime-data-los-angeles?resource=download\n# downloaded 25.11.2022\n# load data\ndata = pd.read_csv(\"Los_Angeles_Crime.csv\")\n</syntaxhighlight>\n\nIf we have an Excel file, we can load it using the read_excel method, like this:\n\n<syntaxhighlight lang=\"Python\" line>\ndata = pd.read_excel(\"data_name.xlsx\") #excel file\n</syntaxhighlight>\n\nFor more information about how to load different types of data files using Pandas, you can check out the pandas documentation\n(https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) (https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n(https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)).\n\n===Size of Data===\nWe check the size of the data by using the shape method, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nnum_rows, num_cols = data.shape\nprint('The data has {} rows and {} columns'.format(num_rows, num_cols))\n</syntaxhighlight>\nThe data has 407199 rows and 28 columns. \n\nThe first number is the number of rows (data entries). The second number is the number of columns (attributes).\nKnowing the number of rows and columns in the data can be useful for a few reasons. First, it can help you make sure that the data has been loaded correctly\nand that you have the expected number of rows and columns. This can be especially important when working with large datasets, as it can be easy to\naccidentally omit or include extra rows or columns.\n\n===Basic info about data===\nTo get basic information about the data, we can use the info method from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\nprint(data.info())\n</syntaxhighlight>\n\nThis will print out a range of information about the data, including the number of entries and columns, the data types of each column, and the number of non-\nnull entries. This information is useful for understanding the general structure of the data and for identifying potential issues or inconsistencies that need to be addressed. \nThe following information is then shown:\n* '''RangeIndex:''' number of data entries\n* '''Data columns:''' number of columns\n* '''Table with'''\n* #: ''Column Index''\n* ''Column: Column Name''\n* ''Non-Null Count: number of non-null entries''\n* ''Dtype: datatype (int64 and float64 are numerical, object is a string)''\n\nKnowing the data types of each column is important because different types of data require different types of analysis and modeling. For example, numeric data (e.g. numbers) can be used in mathematical calculations, while categorical data (e.g. words or labels) can be used in classification models. The following table shows all important data types.\n\n{| class=\"wikitable\"\n|-\n! Pandas Data Type !! Python Data Type !! Purpose\n|-\n| object || string || text/characters\n|-\n| int64 || int || integer\n|-\n| float64 || float || Decimal numbers\n|-\n| bool || bool || True or False\n|-\n| category || not available || Categorical data\n|-\n| datetime64 || not avaiable || Date and time\n|}\n[https://medium.com/nerd-for-tech/learning-python-for-data-science-data-inspection-and-data-types-5b5e80954a6b Table Source]\nThe data types can also be specifically called upon using the dtypes method:\n<syntaxhighlight lang=\"Python\" line>\n#types of data\nprint(data.dtypes)\n</syntaxhighlight>\n\n===First look at data===\nTo get a first look at the data, we can use the head and tail methods from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\n# shows first and last five rows\nprint(data.head())\nprint(data.tail())\n</syntaxhighlight>\n\nThe head method will print the first few rows of the data, while the tail method will print the last few rows. This is a common way to quickly get an idea of the data and make sure that it has been loaded correctly. When working with large datasets, it is often not practical to print the entire dataset, so printing the first and last few rows can give you a general sense of the data without having to view all of it.\nAlso, looking at the actual data helps us find out what type of data we are dealing with.\n'''Tip:''' Check out the documentation of the data source. In this case in kaggle about the \"Crime Data Los Angeles\", one can find some explanations about the columns.\n\n'''Statistical info about data:'''\nTo get a descriptive statistical overview of the data, we can use the described method from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\nprint(data.describe())\n</syntaxhighlight>\nThis will calculate basic statistics for numeric columns, such as the mean, standard deviation, minimum and maximum values, and other summary statistics. This can be a useful way to get a general idea of the data and identify potential issues or trends.\nThe output of the described method will be a table with the following attributes:\n* count: the number of non-null entries\n* mean: the mean value\n* std: the standard deviation\n* min: the minimum value\n* 25%, 505, 75%: the lower, median, and upper quartiles\n* max: the maximum value\n\n'''Check for missing values'''\nTo check if the data has any missing values, we can use the isnull and any methods from Pandas, like this:\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().any())\n</syntaxhighlight>\n\nThis will return a table with a True value for each column that has missing values, and a False value for each column that does not have missing values.\nChecking for missing values is important because most modeling techniques cannot handle missing data. If your data contains missing values, you will need to\neither impute the missing values (i.e. replace them with estimated values) or remove the rows with missing values before fitting a model.\nIf there are missing values in the data, you can use the sum method to check the number of missing values per column, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().sum())\n</syntaxhighlight>\n\nAlternatively, you can use the shape attribute to calculate the percentage of missing values per column, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.isnull().sum()/data.shape[0]*100)\n</syntaxhighlight>\n\nThis can help you understand the extent of the missing values in your data and decide how to handle them.\n\n===Check for duplicate entries===\nTo check if there are duplicate entries in the data, we can use the duplicated method from Pandas, like this:\n\n<syntaxhighlight lang=\"Python\" line>\nprint(data.duplicated())\n</syntaxhighlight>\n\nThis will return a True value for each row that is a duplicate of another row, and a False value for each unique row.\nIf there are any duplicate entries in the data, we can remove them using the drop_duplicates method, like this:\n\n<syntaxhighlight lang=\"Python\" line>\ndata = data.drop_duplicates()\n</syntaxhighlight>\n\nThis will return a new dataframe that contains only unique rows, with the duplicate rows removed. This can be useful for ensuring that the data is clean and ready for analysis.\n\n'''Short introduction to data visualization'''\nData visualization can be a powerful tool for inspecting data and identifying patterns, trends, and anomalies. It allows you to quickly and easily explore the data, and get insights that might not be immediately obvious when looking at the raw data.\nFirst, we start by getting all columns with numerical data by using the select_dtypes method and filtering for in64 and float64:\n\n<syntaxhighlight lang=\"Python\" line>\n# get all numerical data columns\nnumeric_columns = data.select_dtypes(include=['int64', 'float64'])\nprint(numeric_columns.shape)\n# Print the numerical columns\nprint(numeric_columns)\n\n===Boxplot===\nThey are particularly useful for understanding the range and interquartile range of the data, as well as identifying outliers and comparing data between groups.\nIn the following, we want to plot multiple boxplots using the subplots method and the boxplot method:\n\n<syntaxhighlight lang=\"Python\" line>\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a boxplot for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i // 3, i % 3]\nax.boxplot(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n[[File:Boxplot inspection.PNG|centre right]]\n\n===Histogram===\nHistograms are a type of graphical representation that shows the distribution of a dataset. They are particularly useful for understanding the shape of a distribution and identifying patterns, trends, and anomalies in the data.\nIn the following, we want to plot multiple histograms using the subplots method and the hist method:\n\n<syntaxhighlight lang=\"Python\" line>\n12/23/22, 4:25 PM Data_Inspection_ASDA_Wiki - Jupyter Notebook\nlocalhost:8888/notebooks/1000/Data_Inspection_ASDA_Wiki.ipynb 9/9\nIn [15]:\nAt this point, we have most of the information needed to start the Data Cleaning process.\n# Create a figure with a grid of subplots\nfig, axs = plt.subplots(5, 3, figsize=(15, 15))\n# Iterate over the columns and create a histogram for each one\nfor i, col in enumerate(numeric_columns.columns):\nax = axs[i // 3, i % 3]\nax.hist(numeric_columns[col])\nax.set_title(col)\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Hist inspection.PNG|centre right]]\n\nAt this point, we have most of the information needed to start the data-cleaning process.\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is Sian-Tang Teng. Edited by Milan Maushart"
                    },
                    "sha1": "34n35rfc98862bhgfigbts1rehu4fyq"
                }
            },
            {
                "title": "Data Versioning with Python",
                "ns": "0",
                "id": "1063",
                "revision": {
                    "id": "7257",
                    "parentid": "7244",
                    "timestamp": "2023-06-30T05:27:24Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to data versioning using Data Version Control (DVC) and DAGsHub",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4196",
                        "#text": "==Motivation==\nAs data projects evolve more data is collected, annotated and modified, and models are built, optimized, and re-built on new datasets. If we manage several versions of a code with different modifications and complementary explanatory comments, we call that versioning. It is especially important to guarantee the reproducibility of an experiment.\n\nMost data scientists work with GitHub for versioning code, but it is not ideal to version datasets, models, and metrics as Github has a strict file limit & it cannot handle large files & directories. It can also get very tricky for comparing different versions of data. \n\nDVC (Data Version Control) is built to make different data projects shareable and reproducible. It is an ideal tool for versioning binary data and is especially designed to handle large files, data sets, machine learning models, and metrics. However, we cannot view the DVC tracked files on GitHub.\n\nDAGsHub platform enables data scientists and machine learning engineers to version their data, models, experiments, and code.\n\n==Entering the Era of Machine Learning Operations & Machine Learning Lifecycle Processes==\n\nMachine Learning Operations (MLOps) is a term used to describe the set of processes and tools that help manage the end-to-end lifecycle of machine learning models. This includes everything from data preparation, feature engineering, model training, model deployment as well as model monitoring. \n\n[[File:mlops.png|center|700px]]\n                                   Sketch of an ML Project Lifecycle. Figure [https://towardsdatascience.com/a-gentle-introduction-to-mlops-7d64a3e890ff?gi=77cd0d749952 source]\n\n==Introduction to Data Version Control==\n\n''DVC is to Machine Learning Engineers what Git is to Software Engineers''\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart\n\n[https://dvc.org DVC] is the main tool for MLOps and enables data versioning through codification and captures the versions of the data and models in Git commits. It also provides a mechanism to switch between different data contents.\n\nImportance of Data Versioning,\n*   Ensure better training data\n*   Track data schema\n*   Continuous model training\n\n==Installing DVC==\nFollow [[https://dvc.org/doc/install|this]] guide to install DVC on alternative machines.\n<syntaxhighlight lang=\"Python\" line>\npip install dvc\n</syntaxhighlight>\n\n==Adding Datasets into Git Repo==\n<syntaxhighlight lang=\"Python\" line>\n dvc add path/to/dataset\n</syntaxhighlight>\n\n'''NOTE:''' Regardless of the size of the dataset, the data will be added to the repository.\n\n \nBasic Uses of DVC:\n*   Similar to code versioning, track and save data and machine learning models\n*   Easily switch between versions of data and ML models\n*   Understand how datasets and ML artifacts were built initially\n[[File:dvc.png|center|700px]]\n                                   DVC matches the right versions of data, code, and models, [https://dvc.org source]\n\n==Introduction to DAGsHub==\n\n''DAGsHub is to Machine Learning Engineers what GitHub is to Software Engineers''\n\n[https://dagshub.com/ DAGsHub] is a web platform that leverages popular open-source tools to version datasets models, track experiments, label data, and visualize results. It is a free-to-use web platform similar to GitHub for an open-source data science project. DAGsHub supports inbuilt tools like Git for source code tracking, DVC for data version tracking, and MLflow for experiment tracking, which allows us to connect everything in one place with zero configuration.\n\n[[File:dagshub_platform.jpg|center|700px]]\n                                   DAGsHub all-in-one Platform, [https://miro.medium.com/max/1392/1*d8A13xRhLkUtOLIXTrQF-A.webp source]\n\nWe can further integrate tools like MLflow, DVC, Jenkins, etc. into the platform to perform experiments and gather insights on processes. \n\nFollow [https://dagshub.com/docs/experiment_tutorial/ this] tutorial to start with the basic project on DAGsHub using DVC\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "nqqiyrmjc3got2czl3ywavpa94zh0e9"
                }
            },
            {
                "title": "Data distribution",
                "ns": "0",
                "id": "3",
                "revision": {
                    "id": "5764",
                    "parentid": "5459",
                    "timestamp": "2021-06-13T13:44:49Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Data distribution */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23526",
                        "#text": "== Data distribution ==\n[https://www.youtube.com/watch?v=bPFNxD3Yg6U Data distribution] is the most basic and also a fundamental step of analysis for any given data set. On the other hand, data distribution encompasses the most complex concepts in statistics, thereby including also a diversity of concepts that translates further into many different steps of analysis. Consequently, without [https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/ understanding the basics of data distribution], it is next to impossible to understand any statistics down the road. Data distribution can be seen as the [https://www.statisticshowto.datasciencecentral.com/probability-distribution/ fundamentals], and we shall often return to these when building statistics further.\n\n===The normal distribution===\n[[File:Bell curve deviation.jpg|thumb|500px|left|'''This is an ideal bell curve with the typical deviation in per cent.''' The \u03c3 sign (sigma) stands for standard deviation: within the range of -1 to +1 \u03c3 you have about 68,2% of your [[Glossary|data]]. Within -2 to +2 \u03c3 you have 95,4% of the data and so on.]] \nHow wonderful, it is truly a miracle how almost everything that can be measured seems to be following the normal distribution. Overall, the normal distribution is not only the most abundantly occurring, but also the [https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf earliest distribution] that was known. It follows the premise that most data in any given dataset has its majority around a mean value, and only small amounts of the data are found at the extremes. \n\n'''Most phenomena we can observe follow a normal distribution.''' The fact that many do not want this to be true is I think associated to the fact that it makes us assume that the world is not complex, which is counterintuitive to many. While I believe that the world can be complex, there are many natural laws that explain many phenomena we investigate. The Gaussian [https://www.youtube.com/watch?v=mtbJbDwqWLE normal distribution] is such an example. [https://studiousguy.com/real-life-examples-normal-distribution/ Most things] that can be measured in any sense (length, weight etc.) are normally distributed, meaning that if you measure many different items of the same thing, the data follows a normal distribution. \n\nThe easiest example is [https://statisticsbyjim.com/basics/normal-distribution/ tallness of people]. While there is a gender difference in terms of height, all people that would identify as e.g. females have a certain height. Most have a different height from each other, yet there are almost always many of a mean height, and few very small and few very tall females within a given population. There are of course exceptions, for instance due to selection biases. The members of a professional basketball team would for instance follow a selection [[Bias in statistics|bias]], as these would need to be ideally tall. Within the normal population, people\u2019s height follow the normal distribution. The same holds true for weight, and many other things that can be measured. \n<br/>\n[[File:Gauss Normal Distribution.png|thumb|400px|center|'''Discovered by Gauss, it is only consecutive that you can find the normal distribution even at a 10DM bill.''']]\n\n\n==== Sample size matters ====\n[[File:NormalDistributionSampleSize.png|thumb|500px|right|'''Sample size matters.''' As these five plots show, bigger samples will more likely show a normal distribution.]]\n\nMost things in their natural state follow a normal distribution. If somebody tells you that something is not normally \ndistributed, this person is either very clever or not very clever. A [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/ small sample] can hamper you from finding a normal distribution. '''If you weigh five people you will hardly find a normal distribution, as the sample is obviously too small.''' While it may seem like a magic trick, it is actually true that many phenomena that can be measured will follow the normal distribution, at least when your sample is large enough. Consequently, much of the probabilistic statistics is built on the normal distribution.\n\n\n==== Why some distributions are skewed ====\n[[File:SkewedDistribution.png|thumb|500px|right|'''Data can be skewed.''' These graphs show you how distributions can differ according to mode, median and mean of the displayed data.]]\n\nThe most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.\n\n'''Distributions can have different [https://www.youtube.com/watch?v=XSSRrVMOqlQ skews].''' There is the symmetrical skew which is basically a normal distributions or bell curve that you can see on the picture. But normal distributions can also be skewed to the left or to the right depending on how mode, median and mean differ. For the symmetrical normal distribution they are of course all the same but for the right skewed distribution (mode < median < mean) it's different.\n\n\n==== Detecting the normal distribution ====\n[[File:Car Accidents Barplot 2.jpg|thumb|400px|left|'''This is a time series visualized through barplots.''']]\n[[File:Car Accidents Histogram 2.jpg|thumb|400px|left|'''This is the same data as a histogram.''']]\n[[File:Car Accidents Boxplot 2.jpg|thumb|400px|left|'''And this the data as a boxplot.''' You can see that the data is normally distributed because the whiskers and the quarters have nearly the same length.]]\n'''But when is data normally distributed?''' And how can you recognize it when you have a [[Barplots, Histograms and Boxplots|boxplot]] in front of you? Or a histogram? The best way to learn it, is to look at it. Always remember the ideal picture of the bell curve (you can see it above), especially if you look at histograms. If the histogram of your data show a long tail to either side, or has multiple peaks, your data is not normally distributed. The same is the case if your boxplot's whiskers are largely uneven.\n\nYou can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n'''If you would like to know how one can create the diagrams which you see here, this is the R code:'''\n\n<syntaxhighlight lang=\"R\" line>\n\n# If you want some general information about the \"Seatbelt\" dataset, at which we will have look, you can use the ?-function.\n# As \"Seatbelts\" is a dataset in R, you can receive a lot of information here. You can see all datasets available in R by typing data().\n\n?Seatbelts\n     \n# to have a look a the dataset \"Seatbelts\" you can use several commands\n  \n## str() to know what data type \"Seatbelts\" is (e.g. a Time-Series, a matrix, a dataframe...)\nstr(Seatbelts)\n        \n## use show() or just type the name of the dataset (\"Seatbelts\") to see the table and all data it's containing\nshow(Seatbelts)\n# or\nSeatbelts\n      \n## summary() to have the most crucial information for each variable: minimum/maximum value, median, mean...\nsummary(Seatbelts)\n\n     \n# As you saw when you used the str() function, \"Seatbelts\" is a Time-Series, which makes it hard to work with it. We should change it into a dataframe (as.data.frame()). We will also name the new dataframe \"seat\", which is more handy to work with.\n  \nseat<-as.data.frame(Seatbelts)\n     \n# To choose a single variable of the dataset, we use the '$' operator. If we want a barplot with all front drivers,\n# who were killed or seriously injured:\n     \nbarplot(seat$front)\n     \n# For a histogram:\n     \nhist(seat$front)\n  \n## To change the resolution of the histogram, you can use the \"breaks\"-argument of the hist-command, which states\n## in how many increments the plot should be divided\n     \nhist(seat$front, breaks = 30)\nhist(seat$front, breaks = 100)\n\n# For a boxplot:\n     \nboxplot(seat$front)\n\n</syntaxhighlight>\n\n==== The QQ-Plot ====\n[[File:Data caterpillar.png|thumb|right|1. Growth of caterpillars in relation to tannin content in food]]\nThe command <syntaxhighlight land = \"R\" inline>qqplot</syntaxhighlight> will return a Quantile-Quantile plot. This plot allows for a visual inspection on how your model residuals behave in relation to a normal distribution. On the y-axis there are the standardised residuals and on the x-axis the theoretical quantiles. The simple answer is, if your data points are on this line you are fine, you have normal errors, and you can stop reading here. If you want to know more about the theory behind this please continue. \nResiduals is the difference of your response variable and the fitted values. \n<br>\n<br>\n'''Residuals = response variable - fitted values'''\n<br>\n<br>\nFor a regression analysis this would be the difference of your data points to the regression line. \nThe standardised residuals depend on the model function you are applying.\n\nIn the following example, the standardised residuals are the residuals divided by the standard deviation. Let's take the caterpillar data set as an example. On the right you can see the table with the data: growth of caterpillars in relation to tannin content of their diet. Below, we will discuss some correlation plots between these two factors.\n\n[[File:Plot caterpillar.png|thumb|left|2. Plotting the data in an x-y plot already gives you an idea that growth probably depends on the tannin content.]]\n[[File:Qqplot2.png|thumb|right|4. The qqplot for this model looks good. Here the points are mostly on the line with point 4 and point 7 being slightly above and below the line. Still you would consider the residuals in this case to behave normally.]]\n[[File:Plot regression.png|thumb|center|3. Plotted regression line of the regression model \n<syntaxhighlight land = \"R\" inline>lm(growth~tannin)</syntaxhighlight> for testing the relation between two factors]]\n\n[[File:Qqplot notnomral.jpg|thumb|left|5. A gamma distribution, where the variances increases with the square of the mean.]]\n[[File:Qqplot negbinom.jpg|thumb|center|6. A negative binomial distribution that is clearly not following a normal distribution. In other words here the points are not on the line, the visual inspection of this qqplot concludes that your residuals are not normally distributed.]]\n\n===Non-normal distributions===\n'''Sometimes the world is [https://www.statisticshowto.com/probability-and-statistics/non-normal-distributions/ not normally distributed].''' At a closer examination, this makes perfect sense under the specific circumstances. It is therefore necessary to understand which [https://www.isixsigma.com/tools-templates/normality/dealing-non-normal-data-strategies-and-tools/ reasons] exists why data is not normally distributed. \n\n==== The Poisson distribution ====\n[[File:Bildschirmfoto 2020-04-08 um 12.05.28.png|thumb|500px|'''This picture shows you several possible poisson distributions.''' They differ according to the lambda, the rate parameter.]]\n\n[https://www.youtube.com/watch?v=BbLfV0wOeyc Things that can be counted] are often [https://www.britannica.com/topic/Poisson-distribution not normally distributed], but are instead skewed to the right. While this may seem curious, it actually makes a lot of sense. Take an example that coffee-drinkers may like. '''How many people do you think drink one or two cups of coffee per day? Quite many, I guess.''' How many drink 3-4 cups? Fewer people, I would say. Now how many drink 10 cups? Only a few, I hope. A similar and maybe more healthy example could be found in sports activities. How many people make 30 minute of sport per day? Quite many, maybe. But how many make 5 hours? Only some very few. In phenomenon that can be counted, such as sports activities in minutes per day, most people will tend to a lower amount of minutes, and few to a high amount of minutes. \n\nNow here comes the funny surprise. Transform the data following a [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 Poisson distribution], and it will typically follow the normal distribution if you use the decadic logarithm (log). Hence skewed data can be often transformed to match the normal distribution. While many people refrain from this, it actually may make sense in such examples as [https://sustainabilitymethods.org/index.php/Is_the_world_linear%3F island biogeography]. Discovered by MacArtur & Wilson, it is a prominent example of how the log of the numbers of species and the log of island size are closely related. While this is one of the fundamental basic of ecology, a statistician would have preferred the use of the Poisson distribution.\n\n===== Example for a log transformation of a Poisson distribution =====\n[[File:Poisson Education small.png|thumb|400px|left]]\n[[File:Poisson Education log small.png|thumb|400px|left]]\nOne example for skewed data can be found in the R data set \u201cswiss\u201d, it contains data about socio-economic indicators of about 50 provinces in Switzerland in 1888. The variable we would like to look at is \u201cEducation\u201d, which shows how many men in the army (in %) have an education level beyond primary school. \nAs you can see when you look at the first diagram, in 30 provinces only 10 percent of the people received education beyond the primary school.\n\nTo obtain a normal distribution (which is useful for many statistical tests), we can use the natural logarithm.\n\nIf you would like to know, how to conduct an analysis like on the left-hand side, we uploaded the code right below:\n\n<syntaxhighlight lang=\"R\" line>\n\n# we will work with the swiss() dataset.\n# to obtain a histogram of the variable Education, you type\n\nhist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n</syntaxhighlight>\n\n====The Pareto distribution====\n[[File:Bildschirmfoto 2020-04-08 um 12.28.46.png|thumb|300px|'''The Pareto distribution can also be apllied when we are looking at how wealth is spread across the world.''']]\n\n'''Did you know that most people wear 20 % of their clothes 80 % of their time?''' This observation can be described by the [https://www.youtube.com/watch?v=EAynHZE-lK4 Pareto distribution]. For many phenomena that describe proportion within a given population, you often find that few make a lot, and many make few things. Unfortunately this is often the case for workloads, and we shall hope to change this. For such proportions the [https://www.statisticshowto.com/pareto-distribution/ Pareto distribution] is quite relevant. Consequently, it is rooted in [https://www.pragcap.com/the-pareto-principle-and-wealth-inequality/ income statistics]. Many people have a small to average income, and few people have a large income. This makes this distribution so important for economics, and also for sustainability science.\n\n\n=== Visualizing data: Boxplots ===\nA nice way to visualize a data set is to draw a [[Barplots,_Histograms_and_Boxplots#Boxplots|boxplot]]. You get a rough overview how the data is distributed and moreover you can say at a glance if it\u2019s normally distributed. The same is true for [[Barplots,_Histograms_and_Boxplots#Histograms|histograms]], but we will focus on the boxplot for now. For more information on both these forms of data visualisation, please refer to the entry on [[Barplots, Histograms and Boxplots]].\n\n\n'''What are the components of a boxplot and what do they represent?'''\n[[File:Boxplot.png|frameless|500px|right]]\nThe '''median''' marks the exact middle of your data, which is something different than the mean. If you imagine a series of random numbers, e.g. 3, 5, 7, 12, 26, 34, 40, the median would be 12.\nBut what if your data series comprises an even number of numbers, like 1, 6, 19, 25, 26, 55? You take the mean of the numbers in the middle, which is 22 and hence 22 is your median.\n\nThe box of the boxplot is divided in the '''lower''' and the '''upper quartile'''. In each quarter there are, obviously, a quarter of the data points. To define them, you split the data set in two halves (outgoing from the median) and calculate again the median of each half. In a random series of numbers (6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95) your median is 43, your lower quartile is 14 and your upper quartile 87.\n\nThe space between the lower quartile line and the upper quartile line (the box) is called the interquartile range ('''IQR'''), which is important to define the length of the '''whiskers'''. The data points which are not in the range of the whiskers are called '''outliers''', which could e.g. be a hint that they are due to measuring errors. To define the end of the upper whisker, you take the value of the upper quartile and add the product of 1,5 * IQR.\n\n[[File:Boxplot Boxplot Text 2.jpg|thumb|400px|right|'''The boxplot for the series of data:''' 6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95]]\n\n\n'''Sticking to our previous example:'''\nThe IQR is the range between the lower (14) and the upper quartile (87), therefore 73.\nMultiply 73 by 1,5 and add it to the value of the upper quartile: 87 + 109,5 = 196,5\n\nFor the lower whisker, the procedure is nearly the same. Again, you use the product of 1,5*IQR, but this time you subtract this value from the lower quartile:\nHere is your lower whisker: 14 \u2013 109,5 = -95,5\n\nAnd as there are no values outside of the range of our whiskers, we have no outliers. Furthermore, the whiskers to not extend to their extremes, which we calculated above, but instead mark the most extreme data points.\n\n<syntaxhighlight lang=\"R\" line>\n\n#boxplot for our random series of numbers 6, 7, 14, 15, 21, 43, 76, 81, 87, 89, 95\n\nboxplot.example<-c(6,7,14,15,21,43,76,81,87,89,95)\nsummary(boxplot.example)\n\n# minimum = 6\n# maximum = 95\n# mean = 48.55\n# median = 43\n# 1Q = 14.5\n# 3Q = 84\n# don't worry about the difference between our calculated quartile-values above and the values that were calculated by R. R works just a little more precisely here, but the approach we introduced above is a good approximation.\n\n# with this information we can calculate the interquartile range\nIQR(boxplot.example)\n# IQR = 69.5\n\n#lastly we can visualize our boxplot using this comment\nboxplot(boxplot.example)\n\n</syntaxhighlight>\n\n\nIf you want to learn more about Boxplots, check out the entry on [[Histograms and Boxplots]]. Histograms are also very useful when attempting to detect the type of distribution in your data.\n\n'''For more on data visualisation, check out the [[Introduction to statistical figures]].'''\n\n=== More forms of data distribution ===\nOf course, there are more types of data distribution. We found this great overview by [http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf Aswath Damodaran], which helps you investigate the type of distribution in your data. [[File:Different distributions.png|frameless|1000px|center| '''A guide to detecting the right distribution.''' Source: [http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf Aswath Damodaran]]]\n\n==External links==\n====Videos====\n[https://www.youtube.com/watch?v=bPFNxD3Yg6U Data Distribution]: A crash course\n\n[https://www.youtube.com/watch?v=mtbJbDwqWLE The normal distribution]: An explanation\n\n[https://www.youtube.com/watch?v=XSSRrVMOqlQ Skewness]: A quick explanation\n\n[https://www.youtube.com/watch?v=BbLfV0wOeyc The Poisson distribution]: A mathematical explanation\n\n[https://www.youtube.com/watch?v=EAynHZE-lK4 The Pareto Distribution]: Some real life examples\n\n[https://www.youtube.com/watch?v=b2C9I8HuCe4 The Boxplot]: A quick example\n\n[https://www.youtube.com/watch?v=uzkc-qNVoOk Probability]: An Introduction\n\n[https://www.youtube.com/watch?v=9TDjifpGj-k Bayes theorem]: A detailed explanation\n\n\n====Articles====\n[https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/ Probability Distributions]: 6 common distributions you should know\n\n[https://www.statisticshowto.datasciencecentral.com/probability-distribution/ Distributions]: A list of Statistical Distributions\n\n[https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf Normal Distribution]: The History\n\n[https://statisticsbyjim.com/basics/normal-distribution/ The Normal Distribution]: Detailed Explanation\n\n[https://studiousguy.com/real-life-examples-normal-distribution/ The Normal Distributions]: Real Life Examples\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/ The Normal Distribution]: A word on sample size\n\n[https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf The weight of nations]: How body weight is distributed across the world\n\n[https://www.statisticshowto.com/probability-and-statistics/non-normal-distributions/ Non normal distributions]: A list\n\n[https://www.isixsigma.com/tools-templates/normality/dealing-non-normal-data-strategies-and-tools/ Reasons for non normal distributions]: An explanation\n\n[http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf Different distributions]: An overview by Aswath Damodaran, S.61\n\n[https://www.britannica.com/topic/Poisson-distribution The Poisson Distribution]: The history\n\n[https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 The Poisson Process]: A very detailed explanation with real life examples\n\n[https://www.statisticshowto.com/pareto-distribution/ The Pareto Distribution]: An explanation\n\n[https://www.pragcap.com/the-pareto-principle-and-wealth-inequality/ The pareto principle and wealth inequality]: An example from the US\n\n[https://www.britannica.com/science/probability/Risks-expectations-and-fair-contracts History of Probability]: An Overview\n\n[https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist vs. Bayesian Approaches in Statistics]: A comparison\n\n[https://365datascience.com/bayesian-vs-frequentist-approach/ Bayesian Statistics]: An example from the wizarding world\n\n[https://www.stat.colostate.edu/~vollmer/stat307pdfs/LN4_2017.pdf Probability and the Normal Distribution]: A detailed presentation\n\n[http://www.oecd.org/statistics/compare-your-income.htm Compare your income]: A tool by the OECD\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "bzoc55kp6h7hval4pruah8qrlcym71r"
                }
            },
            {
                "title": "Data formats",
                "ns": "0",
                "id": "4",
                "revision": {
                    "id": "6542",
                    "parentid": "6541",
                    "timestamp": "2022-03-14T14:00:46Z",
                    "contributor": {
                        "username": "Heike",
                        "id": "39"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17721",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Data formats (German)]].\n\n'''In short''': This entry introduces different data formats.\n\n==Data formats in statistics==\nThe format of your data influences everything else you do further down the road. To paraphrase a proverb, data is in a format, and the format is the data. Therefore, it is essential to know which [https://www.youtube.com/watch?v=hZxnzfnt5v8 different data formats] exist, and how these may be beneficial, and where you may encounter pitfalls. For more information on different means of measurement, please refer to the [[To Rule And To Measure|'To Rule And To Measure' entry.]]\n\nThe most important difference is between quantitative data and qualitative data. Quantitative data can consist of integers and discrete data, while qualitative data can be factorial -meaning in truely different categories- nominal or [[Data_formats#Ordinal_data|ordinal]], with the latter two providing a link to quantitative data. However, within different areas of science, the nomenclature for data formats widely differs, and to be honest, it is a mess. Here, we try to be consistent, yet please be aware that these names are not consistent across science.\n\n\n====Examples of different data formats====\n[[File:DataFormatsDiet.png|thumb|right|Tracking your diet is just one of many examples how you can approach different data formats.]]\nImagine you want to track your diet. Many people do this today, there are diet books and advice everywhere, much information has become available. Now you want to start and become more familiar with what you eat. How would you start? Counting calories? Differentiating between carbs, fat and greens? Maybe you just count every time you ate a Pizza? Or ice cream? Or too much? There are many ways to measure your diet. And these measurement can be in different data formats.\n\nMost data formats can be transformed into other data formats, which is often confusing for many people. For instance nominal data can be counted repeatedly, you may for instance count the quite diverse and different cups of coffee you drink every day, such as a Flat-white, an American, and two Espressi. The numbers of cups would then add up to cups of coffee, which would represent discrete data. A different example would be temperature, which could be represented as continuous data in degrees Celsius (never mind the Fahrenheit). While this can be represented in numbers, it could also be represented in frosty temperature or temperature above 0\u00b0C.\n\n=== Quantitative data ===\n'''Quantitative data (numeric) is data that is expressed in numbers which can be used in a numerical sense,''' i.e. the numbers can be used to do calculations. There are three types of quantitative data: Continuous, discrete and interval data.\n\n====Continuous data====\n'''Continuous data is numerical data that cannot be counted because it exists on a finite or infinite number line.''' We are all familiar with continuous numbers. Much of our society is ruled by these numbers, and thus much of data analysed in statistics is represented by continuous numbers. Since much of modern measurement is automated within a given predefined system, we often do not have to worry too much how data looks like. Take for instance weight or size. Within [https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Central Europe], this is clearly measured in Grams or Kilograms, and in Centimeters or Meters, respectively. However, if you move to the US, it becomes a whole different story, because of the [[To Rule And To Measure|metric system, or the lack thereof.]] Suddenly you are some feet tall, and you may weigh some \"stones\". Many [https://www.factmonster.com/math-science/weights-measures/the-international-system-metric diverse measurement systems] exist, and one has to be aware of how these were measured. Hence these systems are constructs, and these constructs build on continuous numbers. Continuous numbers are widely used to express data, but we have to be aware that this then still represents normative information.\n\nContinuous data has a true zero. A true zero is defined as a total absence of something that can be represented in numbers. Although a weight of 0 kg or a length of 0 m is abstract, the values represent the absence of weight and length, respectively. \n\n'''Examples of continuous data:'''<br/>\n- the number Pi: 3,14159265359...\n<br>\n- typical weight of a naked mole-rat: 30 grams\n<br>\n- the height of the Empire State Building: 443,2m\n<br>\n\n==== Discrete data ====\n'''Discrete data is numeric data that can be counted because it only exists as natural numbers''' (1, 2, 3, 4...). Examples of this are students in a lecture, where the use a fraction numbers is not helpful. Of course, you can think of an halved apple, but usually, if we count apples or birds or students, we consider them as complete units and stick to natural numbers. Discrete data is often also referred to as 'abundance' or 'counting' data, and within the R language it is called \"integer\".\n\nDiscrete data also has a true zero. Take again the number of students in a statistics lecture. Although the lecture is good, for example because it includes songs of Sesame Street, there might be no students in the lecture. 0 students in a lecture \u2013 there you got your true zero.\n\n==== Interval data ====\n'''Interval data consists of measured or counted values, but it does not have a true zero.''' Also, the difference between two data points is equal no matter where on the scale you look. The best example is temperature if measured in \u00b0C. The difference between 30\u00b0C and 40\u00b0C is equal to the difference between 100\u00b0C and 110\u00b0C. However, there is no true zero to the Celsius scale: 0\u00b0C does not mean that there is no temperature. Rather, 0\u00b0C represents a specific value on the temperature scale. Therefore, you can subtract and add up temperature data, but you cannot meaningfully multiply or divide with it. In addition, the lack of a real zero means that 40\u00b0C is not twice as much energy as 20\u00b0C, although the value is twice as high.\n\n\n=== Qualitative data ===\n'''Qualitative (categorical) data in a statistical sense is data that can be stored in labeled categories which are independent from each other.''' Such categories are typically constructed, and thus contain information that is deeply normative or designed. An example would be hair color, which can be in human perceptions of colours, yet is often also described with different names when it comes to professional hair products. Within statistics, categories are often designed so that within a scientific experiment, categories are constructed in a sense that allows for a meaningful testing of the hypothesis, and meaningful is then in the eye of the beholder. Different levels of fertiliser would be such an example, and the categories would often be built around previous knowledge or pre-tests. Categories are thus of particular importance when it comes to the reduction of the complexity of the world, as it would not be possible to test all sorts of different levels of fertiliser in an experiment. Instead, you might go with \"little\", \"moderate\", \"much\" and \"very much\" fertilizer. Nevertheless, this demands a clear recognition that and how categories are constructed, and deeply normative.\n\nThere are two types of qualitative data: ordinal data and nominal data - and then there is binary data, which is basically also nominal.\n\n====Ordinal data====\n[[File: Ordinal Data.jpg|thumb|right|'''School grades are an example of ordinal data.''']] \n'''Ordinal data is categorical data that can be ranked, but not calculated with, even if it is represented in numbers'''. Remember your school grades? A \"1\" is the best grade in the German grading system, but is it twice as good than a \"2\"? Hardly. Such grades are [http://intellspot.com/nominal-vs-ordinal-data/ ordinal numbers]. These are a system of numbers that are ranked in some sense, but the numbers per se do not necessarily reflect a numeric system. In other words, they are highly normative and contested. A \"2\" might be a good grade for some, and a disaster for others. Ordinal formats are often clearly defined scales that allow people to grade, evaluate or rank certain information. One of the most prominent examples is the [[Likert Scale|Likert scale]] that is often used in Psychology. In this case, the scaling is often not reflected in numbers at all, but in levels such as \"Strongly Agree\" or \"Rather Disagree\". Such constructed scales may make a true statistician very unhappy, since these scales are hard to analyse, yet there is hardly any alternative since it also does not make any sense to ask: \"How happy are you on a scale from 1 to 100?\". Therefore, ordinal scales are often relevant in order to create a scaling system that allows for wide comparability or even becomes a norm, such as school grades. My advise would be to [https://sciencing.com/advantages-disadvantages-using-ordinal-measurement-12043783.html use ordinal scales] when this is common practise in this branch of science. Read other studies in the field, and then decide. These are highly constructed scales, hence there needs to be clear reasoning on why you want to use them.\n\n====Nominal data====\n[[File: Gummy Bears.jpg|thumb|right|'''Gummy bears are a nice example for data formats.''' You can classify them by color, which would be nominal data. But if you weigh them, you get continuous data again.]]\n'''Whenever you have categorical data that cannot be ranked, it is called [https://formpl.us/blog/nominal-data nominal data]'''. An example would be different ethnicities, countries of birth, or different types of gender. This already highlights that we are here confronted by often completely different worldviews, thus nominal data represents a stark case of a normative view of the world. Gender is a prominent example, since some people still define gender by a biological stereotype (Female/Male) and thus binary (see below), which according to my worldview is clearly wrong, and I see Gender as nominal with more than two categories. Nominal data formats hence demand an even clearer reflection than ordinal data, where at least you may say that a certain school grade is higher than another one. This is not the case for nominal data. Therefore, one has to be extra careful about the implications, that a specific constructed scale may imply.\n\n====Binary data====\n[[File: Mainzelm\u00e4nnchen-Ampel.png|200px|thumb|right|'''An example of binary data''']]\n'''Binary data is the most reduced data format, which basically consists of two levels: 1 and 0.''' It is, strictly speaking, nominal data, but nominal data that only exists in two versions which can be translated into 1 and 0: On / Off, Yes / No. In [https://www.youtube.com/watch?v=ewokFOSxabs computer science] binary data is used directly as simple 0 and 1, but the great breakthrough of that dataset was early on in the insurance business as well as in medicine, where 'dead' or 'alive' are often the most fundamental questions. Binary information is clearly simplistic, but quite often this matches with a certain view of reality. Take the example of being able to play an instrument. If somebody asks you whether you can play the piano, you will probably say ''yes'' or ''no''. You may most likely not qualify your answer by saying \"I play better than a monkey, but worse than Horowitz\". Some modest folks may say \"I can play a bit\", or \"I am not very good\", or \"I used to be better\", but very often people answer ''yes'' or ''no''. Hence binary data allows for a simple view of reality, and this may often match with the world how we perceive it. But be aware: Other people may have a less simple view.\n\n\n== Choosing the right data format ==\nYou may wonder now how to choose the right data format for your data gathering. The answer to that is quite simple. '''Any data format should be as simple as possible, and as complex as necessary.''' Follow Occam's razor, and you will be fine. Of course this sounds appealing, but how to know what is too simple, and what is too complex? Here, I suggest you build on the available literature. Read other publications that examined a certain phenomenon before, these papers may guide you in choosing the right scale.\n\nThis table gives you some more information on different data formats - maybe it can help you design your study?\n[[File:Data Formats Table small 7.jpg|thumb|1000px|center|'''Different data formats and their characteristics.'''. Source: own]]\n\n\n== Which simple test works for which data format? ==\nThe following table which we compiled shows which statistical tests are useful depending on the data you have. To learn more about these tests, please refer to the entries on [[Simple Statistical Tests]], [[Regression Analysis]], [[Correlations]] and [[ANOVA]]. Note: for combinations that lead to different methods (e.g. ordinal x continuous), please refer to all mentioned approaches.<br/>\n[[File:Table Simple Tests.png|600px|frameless|center|'''Which simple tests do you use for which kinds of data formats?''' Source: own.]]\n\n\n== A word on indices ==\nIn economics and finance, an index is a statistical measure of change in a representative group of individual data points. A good example of the application of an index that most people know is the [https://www.investopedia.com/terms/g/gdp.asp GDP], the gross domestic product of a country. Although it has largely been criticised for being too generalised and not offering enough nuance to understand the complexity of the single country, many social, economical and other indicators are correlated with the GDP. \n[[File:Bildschirmfoto 2020-04-11 um 11.24.41.png|thumb| Indices appear also during our every day life like a picture of the latest developments at the stock market.]]\nIn ecology, a prominent example for an index is the so-called [https://www.youtube.com/watch?v=ghhZClDRK_g Shannon Wiener index], which represents abundance corrected diversity measures. A prominent example from economy again is the [https://www.youtube.com/watch?v=_PXFVNWINQc Dow Jones index] while the [http://hdr.undp.org/en/content/human-development-index-hdi human development index] tries to integrate information about life expectancy education and income in order to get a general understanding about several components that characterise countries. The [https://www.investopedia.com/terms/g/gini-index.asp GINI coefficient] tries to measure inequality, as surely daring endeavour, but nevertheless quite important. In psychology the [https://www.youtube.com/watch?v=7p2a9B35Xn0 intelligence quotient], which is of course heavily criticised, is a known example of reducing many complex tests into one overall number. In the face and quotients are hence constructs that are often based on many variables and try to reduce the complexity of these diverse indicators into one set of numbers.\n\n== Further Information ==\n====Videos====\n[https://www.youtube.com/watch?v=7p2a9B35Xn0 Intelligence Quotient]: Answering the question if the IQ really measures how smart you are\n\n[https://www.youtube.com/watch?v=hZxnzfnt5v8 Different data formats]: An overview\n\n[https://www.youtube.com/watch?v=ewokFOSxabs Binary data]: How our computer works\n\n[https://www.youtube.com/watch?v=ghhZClDRK_g The Shannon Wiener index]: An example from ecology\n\n[https://www.youtube.com/watch?v=_PXFVNWINQc The Dow Jones Index]: An example from economy\n\n[https://www.youtube.com/watch?v=7p2a9B35Xn0 The Intelligence Quotient]: A critical reflection\n\n[https://www.youtube.com/watch?v=h8EYEJ32oQ8&list=PLU5aQXLWR3_yYS0ZYRA-5g5YSSYLNZ6Mc Descriptive Statistics]: A whole video series about descriptive statistics from the Khan academy\n\n[https://www.youtube.com/watch?v=MRqtXL2WX2M Standard Deviation]: A brief explanation\n\n[https://www.youtube.com/watch?v=mk8tOD0t8M0 Mode, Median, Mean, Range & Standard Deviation]: A good summary\n\n[https://www.youtube.com/watch?v=bAU1MLRwh7c Back-of-envelope office space conundrum]: A real life example\n\n====Articles====\n[https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Measurement]: Reflecting upon different measurement systems across the globe\n\n[https://www.iqmindware.com/wiki/what-does-my-iq-score-mean IQ IQ]: An explanation\n\n[http://intellspot.com/nominal-vs-ordinal-data/ Nominal vs. ordinal data]: A comparison\n\n[https://www.simplypsychology.org/likert-scale.html Likert scale]: The most popular rating scale\n\n[https://sciencing.com/advantages-disadvantages-using-ordinal-measurement-12043783.html Ordinal data]: Limitations\n\n[https://formpl.us/blog/nominal-data Nominal data]: An explanation\n\n[https://en.wikipedia.org/wiki/Binary_data Binary data]: An explanation\n\n[https://www.investopedia.com/terms/g/gdp.asp GDP]: A detailed article\n\n[http://hdr.undp.org/en/content/human-development-index-hdi The Human Development Index]: An alternative to the GDP\n\n[https://www.investopedia.com/terms/g/gini-index.asp The GINI index]: A measure of inequality\n\n[https://www.investopedia.com/terms/d/descriptive_statistics.asp Descriptive Statistics]: An introduction\n\n[http://intellspot.com/descriptive-statistics-examples/ Descriptive Statistics]: A detailed summary\n\n[https://www.investopedia.com/terms/b/back-of-the-envelope-calculation.asp Back of the Envelope Calculation]: An explanation\n\n[https://www.stlouisfed.org/on-the-economy/2020/march/back-envelope-estimates-next-quarters-unemployment-rate Estimates of Next Quarter\u2019s Unemployment Rate]: An Example For Back of the Envelope Statistics\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "68z204lp9l1cklgs66jvkf4fl667vvu"
                }
            },
            {
                "title": "Data formats (German)",
                "ns": "0",
                "id": "573",
                "revision": {
                    "id": "6525",
                    "parentid": "6524",
                    "timestamp": "2022-02-14T22:10:50Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Ordinale Daten */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19137",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here:  [[Data formats]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag stellt unterschiedliche Datenformate vor. \n\n== Datenformate in der Statistik ==\nDas Format Ihrer Daten beeinflusst alles Andere, was Sie im weiteren Verlauf Ihrer Arbeit tun. Um ein Sprichwort zu paraphrasieren: Die Daten haben ein Format, und das Format sind die Daten. Deshalb ist es wichtig zu wissen, welche [https://www.youtube.com/watch?v=hZxnzfnt5v8 verschiedenen Datenformate] es gibt, und wie diese von Vorteil sein k\u00f6nnen, und wo Sie auf Fallstricke sto\u00dfen k\u00f6nnen. Weitere Informationen zu verschiedenen Messmethoden finden Sie im Eintrag [[To Rule And To Measure (German)]]. Der wichtigste Unterschied besteht zwischen quantitativen Daten und qualitativen Daten. Quantitative Daten k\u00f6nnen aus ganzen Zahlen und diskreten Daten bestehen, w\u00e4hrend qualitative Daten faktoriell - d. h. in wirklich verschiedenen Kategorien -, nominal oder ordinal sein k\u00f6nnen, wobei die beiden letzteren eine Verbindung zu quantitativen Daten herstellen. Die Nomenklatur f\u00fcr Datenformate ist jedoch in den verschiedenen Wissenschaftsbereichen sehr unterschiedlich, und um ehrlich zu sein, ein einziges Durcheinander. Wir versuchen hier, konsistent zu sein, aber seien Sie sich bitte bewusst, dass diese Bezeichnungen in der Wissenschaft nicht einheitlich sind. \n\n\n====Ein Beispiel f\u00fcr verschiedene Datenformate====\n[[File:DataFormatsDiet.png|thumb|right|Ein Ern\u00e4hrungs-Tagebuch ist nur eines von vielen Beispielen, wie Sie verschiedene Datenformate angehen k\u00f6nnen]]\nStellen Sie sich vor, Sie wollen Ihre Ern\u00e4hrung tracken. Viele Menschen tun dies heute, es gibt \u00fcberall Di\u00e4tb\u00fccher und -ratgeber, viele Informationen sind verf\u00fcgbar geworden. Nun m\u00f6chten Sie damit beginnen und sich mit dem, was Sie essen, besser vertraut machen. Wie w\u00fcrden Sie anfangen? Mit dem Z\u00e4hlen von Kalorien? Unterscheiden Sie zwischen Kohlenhydraten, Fett und Gr\u00fcnzeug? Vielleicht z\u00e4hlen Sie einfach jedes Mal, wenn Sie eine Pizza gegessen haben? Oder Eiscreme? Oder \"zu viel\"? Es gibt viele M\u00f6glichkeiten, Ihre Ern\u00e4hrung zu messen. Und diese Messungen k\u00f6nnen in verschiedenen Datenformaten erfolgen.\n\nDie meisten Datenformate k\u00f6nnen in andere Datenformate umgewandelt werden, was f\u00fcr viele Menschen oft verwirrend ist. Zum Beispiel k\u00f6nnen nominale Daten wiederholt gez\u00e4hlt werden, z. B. die Tassen Kaffee, die Sie jeden Tag trinken. Die Anzahl der Tassen w\u00fcrde sich dann aufaddieren, was diskrete Daten darstellen w\u00fcrde. Ein anderes Beispiel w\u00e4re die Temperatur, die als kontinuierliche Daten in Grad Celsius (besser nicht in Fahrenheit) dargestellt werden k\u00f6nnte. Diese kann zwar in Zahlen dargestellt werden, aber auch als Frosttemperatur oder Temperatur \u00fcber 0 \u00b0C.\n\n\n=== Quantitative Daten ===\n'''Quantitative (numerische) Daten sind Daten, die in Zahlen ausgedr\u00fcckt werden, mit denen sich rechnen l\u00e4sst.''' Es gibt drei Arten numerischer Daten: Kontinuierliche und diskrete Daten, sowie Intervalldaten.\n\n====Kontinuierliche Daten====\n'''Kontinuierliche Daten sind numerische Daten die nicht gez\u00e4hlt werden k\u00f6nnen, weil sie auf einer endlichen oder unendlichen Skala existieren'''. Wir alle sind mit kontinuierlichen Zahlen vertraut. Ein Gro\u00dfteil unserer Gesellschaft wird von diesen Zahlen beherrscht, und daher wird ein Gro\u00dfteil der in der Statistik analysierten Daten durch kontinuierliche Zahlen dargestellt. Da ein Gro\u00dfteil der modernen Messungen innerhalb eines vorgegebenen Systems automatisiert ist, m\u00fcssen wir uns oft nicht allzu viele Gedanken dar\u00fcber machen, wie die Daten aussehen. Nehmen Sie zum Beispiel Gewicht oder Gr\u00f6\u00dfe. Innerhalb von [https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Mitteleuropa] wird dies eindeutig in Gramm oder Kilogramm bzw. in Zentimetern oder Metern gemessen. Wenn Sie jedoch in die USA ziehen, wird es eine ganz andere Geschichte, wegen des [[To Rule And To Measure|metrischen Systems, oder eher dessen mangelnder Nutzung]]. Pl\u00f6tzlich sind Sie einige Fu\u00df gro\u00df und wiegen vielleicht einige \"stones\". Es gibt viele [https://www.factmonster.com/math-science/weights-measures/the-international-system-metric verschiedene Messsysteme], und man muss sich bewusst sein, wie diese genutzt werden. Diese Systeme sind also Konstrukte, und diese Konstrukte bauen auf kontinuierlichen Zahlen auf. Dies zeigt, dass kontinuierliche Zahlen weit verbreitet sind, um Daten auszudr\u00fccken, aber wir m\u00fcssen uns bewusst sein, dass es sich dabei immer noch um normative Informationen handelt. \n\nKontinuierliche Daten haben eine echte Null. Eine echte Null ist definiert als v\u00f6llige Abwesenheit von etwas, das in Zahlen dargestellt werden kann. Obwohl ein Gewicht von 0 kg oder eine L\u00e4nge von 0 m abstrakt sind, stellen die Werte die Abwesenheit von Gewicht bzw. L\u00e4nge dar. \n\n'''Beispiele f\u00fcr kontinuierliche Daten:'''<br/>\n- die Zahl Pi: 3,14159265359...\n<br>\n- das typische Gewicht einer Nacktmullratte: 30 Gramm\n<br>\n- die H\u00f6he des Empire State Buildings: 443,2m\n<br>\n- die Schmelztemperatur von dunkler Schokolade: 45-50\u00b0C\n\n==== Diskrete Daten ====\n'''Diskrete Daten sind numerische Daten die gez\u00e4hlt werden k\u00f6nnen, da sie nur als nat\u00fcrliche Zahlen (1, 2, 3, 4...) vorliegen'''. Beispiele hierf\u00fcr sind Sch\u00fcler*innen in einer Klasse, oder das eigene Alter. Hier ergibt es keinen Sinn, mit kontinuierlichen Daten zu arbeiten. Nat\u00fcrlich kann man auch an einen halbierten Apfel denken, aber wenn wir \u00c4pfel, V\u00f6gel oder Studierende z\u00e4hlen, betrachten wir sie normalerweise als vollst\u00e4ndige Einheiten und halten uns an nat\u00fcrliche Zahlen. Diskrete Daten werden oft auch als \"H\u00e4ufigkeits-\" oder \"Z\u00e4hldaten\" bezeichnet, und in der Sprache R werden sie als \"integer\" (Ganzzahlen) bezeichnet.\n\nDiskrete Daten haben auch eine echte Null. Nehmen wir noch einmal die Anzahl der Studierenden in einer Statistikvorlesung. Auch wenn die Vorlesung gut ist, zum Beispiel weil sie Lieder aus der Sesamstra\u00dfe enth\u00e4lt, kann es sein, dass keine Studierenden in der Vorlesung sind. 0 Studierende in einer Vorlesung - da haben Sie Ihre echte Null.\n\n====Intervalldaten====\n'''Intervalldaten bestehen aus gemessenen oder gez\u00e4hlten Werten, allerdings gibt es keine echte Null.''' Au\u00dferdem ist der Unterschied zwischen zwei Messwerten auf der Skala immer gleich gro\u00df, egal, wo man schaut. Das beste Beispiel ist Temperatur, wenn man sie in \u00b0C misst. Der Unterschied zwischen 30\u00b0C und 40\u00b0C ist genauso gro\u00df wie der Unterschied zwischen 100\u00b0C und 110\u00b0C. Allerdings gibt es auf der Celsius-Skala keine echte Null: 0\u00b0C bedeutet nicht, dass es keine Temperatur g\u00e4be. Stattdessen stellt 0\u00b0C einfach einen bestimmten Wert auf der Temperaturskala dar. Daher kann man Temperaturen zwar addieren und subtrahieren, aber nicht sinnvoll multiplizieren oder dividieren. Au\u00dferdem f\u00fchrt dieser Mangel einer echten Null dazu, dass 40\u00b0C nicht doppelt soviel Energie wie 20\u00b0C bedeutet, auch wenn die Zahl doppelt so gro\u00df ist.\n\n\n=== Qualitative Daten ===\n'''Qualitative (kategorische) Daten sind qualitative Daten, die in benannten Kategorien gesammelt werden k\u00f6nnen, die voneinander unabh\u00e4ngig sind.''' Solche Kategorien sind typischerweise konstruiert und enthalten daher Informationen, die zutiefst normativ oder konstruiert sind. Ein Beispiel w\u00e4re die Haarfarbe, die in der menschlichen Wahrnehmung von Farben stattfinden kann, aber auch bei professionellen Haarprodukten oft mit unterschiedlichen Namen beschrieben wird. Innerhalb der Statistik werden Kategorien oft so gebildet, dass innerhalb eines wissenschaftlichen Experiments die Kategorien in einem Sinne konstruiert werden, der eine sinnvolle Pr\u00fcfung der Hypothese erm\u00f6glicht, und sinnvoll liegt dann im Auge des Betrachters. Unterschiedliche D\u00fcngemittelmengen w\u00e4ren ein solches Beispiel, und die Kategorien werden oft auf Basis von Vorwissen oder Vortests gebildet. Kategorien sind also von besonderer Bedeutung, wenn es um die Reduktion der Komplexit\u00e4t der Welt geht, da es nicht m\u00f6glich w\u00e4re, alle m\u00f6glichen unterschiedlichen D\u00fcngemittelmengen in einem Experiment zu testen. Stattdessen entscheidet man sich z.B. f\u00fcr \"wenig\", \"moderat\", \"viel\" und \"sehr viel\" D\u00fcnger. Dennoch muss man sich dar\u00fcber im Klaren sein, dass - und wie - Kategorien konstruiert und damit zutiefst normativ sind.\n\nEs gibt zwei Arten kategorischer Daten: ordinale und nominale Daten - und dann gibt es bin\u00e4re Daten, die eigentlich nichts anderes sind als nominale Daten.\n\n==== Ordinale Daten ====\n[[File:Likert scale.jpg|thumb|right|Die Likert-Skala]]\n[[File:Ordinal Data.jpg|thumb|right|Auch wenn man sich \u00fcber die Objektivit\u00e4t und den Sinn von Noten streiten kann, ist es ein anschauliches Beispiel f\u00fcr ordinale Daten.]] \n'''Ordinale Daten sind kategorische Daten, die in eine Reihenfolge gebracht werden k\u00f6nnen, mit denen sich aber nicht rechnen l\u00e4sst, selbst wenn sie als Zahlen ausgedr\u00fcckt werden.''' Erinnern Sie sich an Ihre Schulnoten? Eine \"1\" ist die beste Note im deutschen Notensystem, aber ist sie doppelt so gut wie eine \"2\"? Wohl kaum. Solche Noten sind [http://intellspot.com/nominal-vs-ordinal-data/ ordinale Zahlen]. Es handelt sich dabei um ein System von Zahlen, die in gewisser Weise geordnet sind, aber die Zahlen an sich spiegeln nicht unbedingt ein numerisches System wider. Mit anderen Worten: Sie sind h\u00f6chst normativ und umstritten. Eine \"2\" mag f\u00fcr die einen eine gute Note sein, f\u00fcr die anderen eine Katastrophe. Ordinale Formate sind oft klar definierte Skalen, die es Menschen erm\u00f6glichen, bestimmte Informationen zu benoten, zu bewerten oder in eine Rangfolge zu bringen. Eines der bekanntesten Beispiele ist die [[Likert Scale|Likert-Skala]], die h\u00e4ufig in der Psychologie verwendet wird. In diesem Fall wird die Skalierung oft gar nicht in Zahlen wiedergegeben, sondern in Stufen wie \"stimme voll zu\" oder \"stimme eher nicht zu\". Solche konstruierten Skalen k\u00f6nnen echten Statistiker*innen sehr ungl\u00fccklich machen, da die Ergebnisse schwer zu analysieren sind, aber es gibt kaum eine Alternative, da es auch keinen Sinn macht, zu fragen: \"Wie gl\u00fccklich sind Sie auf einer Skala von 1 bis 100?\" Daher sind Ordinalskalen oft relevant, um ein Skalensystem zu schaffen, das eine breite Vergleichbarkeit erm\u00f6glicht oder sogar zur Norm wird, wie z.B. Schulnoten. Mein Rat w\u00e4re, [https://sciencing.com/advantages-disadvantages-using-ordinal-measurement-12043783.html Ordinalskalen zu verwenden], wenn dies in diesem Wissenschaftszweig \u00fcblich ist. Lesen Sie andere Studien auf dem Gebiet, und entscheiden Sie dann. Es handelt sich um hochgradig konstruierte Skalen, daher muss es eine klare Begr\u00fcndung geben, warum Sie sie verwenden wollen.\n\n====Nominale Daten====\n[[File: Gummy Bears.jpg|thumb|right|Gummib\u00e4rchen sind ein nettes Beispiel, da man sie nach ihrer Farbe klassifizieren kann, was nominale Daten w\u00e4ren. Aber wenn man sie wiegt, erh\u00e4lt man wieder kontinuierliche Daten.]]\n'''Wann immer Sie kategorische Daten haben, die nicht in eine Rangfolge gebracht werden k\u00f6nnen, nennt man sie [https://formpl.us/blog/nominal-data nominale Daten]'''. Ein Beispiel w\u00e4ren verschiedene Ethnien, Geburtsl\u00e4nder, oder verschiedene Arten von Geschlechtern. Dies verdeutlicht bereits, dass wir es hier mit oft v\u00f6llig unterschiedlichen Weltanschauungen zu tun haben, sodass nominale Daten einen krassen Fall einer normativen Sicht auf die Welt darstellen. Das Geschlecht ist ein prominentes Beispiel, da manche Menschen das Geschlecht immer noch \u00fcber ein biologisches Stereotyp (weiblich/m\u00e4nnlich) und damit bin\u00e4r (siehe unten) definieren, was nach meinem Weltbild eindeutig falsch ist, weshalb ich Geschlecht nominal mit mehr als zwei Kategorien definieren w\u00fcrde. Nominale Datenformate verlangen daher eine noch deutlichere Reflexion als ordinale Daten, bei denen man zumindest sagen kann, dass eine bestimmte Schulnote h\u00f6her ist als eine andere. Das ist bei nominalen Daten nicht der Fall. Deshalb muss man besonders vorsichtig sein mit den Implikationen, die eine bestimmte konstruierte Skala implizieren kann.\n\n====Bin\u00e4re Daten====\n[[File: Mainzelm\u00e4nnchen-Ampel.png|200px|thumb|right|Ein weiterer Fall von Bin\u00e4rdaten]]\n'''Bin\u00e4re Daten sind das am meisten reduzierte Datenformat, das grunds\u00e4tzlich aus zwei Ebenen besteht: 1 und 0'''. Streng genommen sind bin\u00e4re Daten nominale Daten, aber eben nominale Daten, die nur in zwei Varianten vorliegen, die sich in 1 und 0 \u00fcbersetzen lassen: An / Aus, Ja / Nein. In der [https://www.youtube.com/watch?v=ewokFOSxabs Informatik] werden bin\u00e4re Daten direkt als einfache 0 und 1 genutzt, aber der gro\u00dfe Durchbruch dieses Datensatzes kam schon fr\u00fch in der Versicherungsbranche sowie in der Medizin, wo \"tot\" oder \"lebendig\" oft die grundlegendsten Fragen sind. Bin\u00e4re Informationen sind eindeutig vereinfachend, was aber oft mit einer bestimmten Sicht der Realit\u00e4t \u00fcbereinstimmt. Nehmen Sie das Beispiel, ein Instrument spielen zu k\u00f6nnen. Wenn Sie jemand fragt, ob Sie Klavier spielen k\u00f6nnen, werden Sie wahrscheinlich ''ja'' oder ''nein'' sagen. Sie werden Ihre Antwort h\u00f6chstwahrscheinlich nicht qualifizieren, indem Sie sagen \"Ich spiele besser als ein Affe, aber schlechter als Horowitz\". Einige bescheidene Leute sagen vielleicht \"Ich kann ein bisschen spielen\", oder \"Ich bin nicht sehr gut\", oder \"Ich war mal besser\", aber sehr oft antworten Menschen mit ''ja'' oder ''nein''. Bin\u00e4re Daten erlauben also eine einfache Sicht auf die Realit\u00e4t, und diese mag oft mit der Welt \u00fcbereinstimmen, wie wir sie wahrnehmen. Aber seien Sie sich bewusst: Andere Menschen haben vielleicht eine weniger einfache Sichtweise.\n\n\n==Die Auswahl des richtigen Datenformats==\nSie fragen sich jetzt vielleicht, wie Sie das richtige Datenformat ausw\u00e4hlen. Die Antwort darauf ist ganz einfach. Jedes Datenformat sollte so einfach wie m\u00f6glich und so komplex wie n\u00f6tig sein. Folgen Sie Occams Rasiermesser, und Sie werden gut zurechtkommen. Das klingt nat\u00fcrlich verlockend, aber woher wei\u00df man, was zu einfach und was zu komplex ist? Hier schlage ich vor, dass Sie sich auf die vorhandene Literatur st\u00fctzen. Lesen Sie andere Ver\u00f6ffentlichungen, die ein bestimmtes Ph\u00e4nomen bereits untersucht haben, diese Ver\u00f6ffentlichungen k\u00f6nnen Ihnen bei der Wahl der richtigen Skala helfen.\n\nDie folgende Tabelle gibt Ihnen noch mehr Informationen \u00fcber verschiedene Fatenformate - vielleicht hilft Sie Ihnen, Ihre Studie zu konzipieren?\n\n[[File:Data Formats Table small 7.jpg|center|Datenformate und Eigenschaften]]\n\n\n==Welcher einfache Test eignet sich f\u00fcr welches Datenformat?==\nDie folgende Tabelle, die wir zusammengestellt haben, zeigt, welche statistischen Tests in Abh\u00e4ngigkeit von Ihren Daten sinnvoll sind. Um mehr \u00fcber diese Tests zu erfahren, lesen Sie bitte die Eintr\u00e4ge \u00fcber [[Simple Statistical Tests|einfache statistische Tests]], [[Regression Analysis|Regressionsanalyse]], [[Correlations|Korrelationen]] und [[ANOVA]]. Hinweis: F\u00fcr Kombinationen, die zu unterschiedlichen Methoden f\u00fchren (z. B. ordinal x kontinuierlich), verweisen wir auf alle genannten Ans\u00e4tze.\n[[File:Table Simple Tests.png|600px|frameless|center|'''Welcher einfache Test eignet sich f\u00fcr welches Datenformat?''' Quelle: eigene Darstellung.]]\n\n\n==Ein Wort \u00fcber Indizes==\nIn der Wirtschafts- und Finanzwelt ist ein Index ein statistisches Ma\u00df f\u00fcr die Ver\u00e4nderung einer repr\u00e4sentativen Gruppe von einzelnen Datenpunkten. Ein gutes Beispiel f\u00fcr die Anwendung eines Indexes, den die meisten Menschen kennen, ist das [https://www.investopedia.com/terms/g/gdp.asp BIP], das Bruttoinlandsprodukt eines Landes. Obwohl es weitgehend kritisiert wurde, weil es zu allgemein gehalten ist und nicht gen\u00fcgend Nuancen bietet, um die Komplexit\u00e4t des einzelnen Landes zu verstehen, sind viele soziale, wirtschaftliche und andere Indikatoren mit dem BIP korreliert.\n[[File:Bildschirmfoto 2020-04-11 um 11.24.41.png|thumb| Indizes erscheinen auch in unserem Alltag, wie ein Bild der neuesten Entwicklungen an der B\u00f6rse.]]\nIn der \u00d6kologie ist ein bekanntes Beispiel f\u00fcr einen Index der so genannte [https://www.youtube.com/watch?v=ghhZClDRK_g Shannon Wiener index], der Diversit\u00e4tsma\u00dfe darstellt, welche hinsichtlich der H\u00e4ufigkeit bestimmter Spezies angepasst sind. Ein prominentes Beispiel aus der Wirtschaft ist wiederum der [https://www.youtube.com/watch?v=_PXFVNWINQc Dow Jones index], w\u00e4hrend der [http://hdr.undp.org/en/content/human-development-index-hdi Index der menschlichen Entwicklung] versucht, Informationen \u00fcber Lebenserwartung, Bildung und Einkommen zu integrieren, um ein allgemeines Verst\u00e4ndnis f\u00fcr verschiedene Komponenten zu erhalten, die L\u00e4nder charakterisieren. Der [https://www.investopedia.com/terms/g/gini-index.asp GINI-Koeffizient] versucht, die Ungleichheit zu messen, was sicherlich ein gewagtes Unterfangen ist, aber dennoch sehr wichtig. In der Psychologie ist der [https://www.youtube.com/watch?v=7p2a9B35Xn0 Intelligenzquotient], der nat\u00fcrlich stark kritisiert wird, ein bekanntes Beispiel f\u00fcr die Reduzierung vieler komplexer Tests auf eine Gesamtzahl. Indizes und Quotienten sind also Konstrukte, die oft auf vielen Variablen beruhen und versuchen, die Komplexit\u00e4t dieser vielf\u00e4ltigen Indikatoren auf eine einzige Zahl zu reduzieren. \n\n\n==Weitere Informationen==\n[https://www.youtube.com/watch?v=7p2a9B35Xn0 Intelligence Quotient]: Answering the question if the IQ really measures how smart you are\n\n[https://www.youtube.com/watch?v=hZxnzfnt5v8 Different data formats]: An overview\n\n[https://www.youtube.com/watch?v=ewokFOSxabs Binary data]: How our computer works\n\n[https://www.youtube.com/watch?v=7p2a9B35Xn0 The Intelligence Quotient]: A critical reflection\n\n[https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Measurement]: Reflecting upon different measurement systems across the globe\n\n[https://www.iqmindware.com/wiki/what-does-my-iq-score-mean IQ]: An explanation\n\n[http://intellspot.com/nominal-vs-ordinal-data/ Nominal vs. ordinal data]: A comparison\n\n[https://www.simplypsychology.org/likert-scale.html Likert scale]: The most popular rating scale\n\n[https://sciencing.com/advantages-disadvantages-using-ordinal-measurement-12043783.html Ordinal data]: Limitations\n\n[https://formpl.us/blog/nominal-data Nominal data]: An explanation\n\n[https://en.wikipedia.org/wiki/Binary_data Binary data]: An explanation\n\n[https://www.investopedia.com/terms/g/gdp.asp GDP]: A detailed article\n\n[https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Measurement]: Reflecting upon different measurement systems across the globe\n\n[http://hdr.undp.org/en/content/human-development-index-hdi The Human Development Index]: An alternative to the GDP\n\n[https://www.investopedia.com/terms/g/gini-index.asp The GINI index]: A measure of inequality\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "dxiyepf4xwhnpinborhu80to7auzymq"
                }
            },
            {
                "title": "Deconstruction",
                "ns": "0",
                "id": "826",
                "revision": {
                    "id": "5872",
                    "parentid": "5871",
                    "timestamp": "2021-06-22T06:53:37Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* References */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7867",
                        "#text": "'''In short:''' Deconstruction is a practice of reading texts that revolves around the (in)stability of linguistic signs and their meaning.\n\n== What is Deconstruction? ==\nStructuralism, which emerged at the beginning of the 20th century, reached its peak in France in the 1960s. Structuralism, grounded in the theory of signs by the Swiss linguist Ferdinand de Saussure (3), is particularly concerned with the assumption that there are underlying structures to everything that produce objective, definitive meaning and provide the framework within which actions can be performed. Poststructuralism, which began to emerge in the mid-1960s, called into question the existence of such structures. It was now necessary to break up the previously assumed stable structures and let them collapse (3). '''An important approach to this was deconstruction, which goes back to the French philosopher Jacques Derrida (1930-2004).''' He first spoke of deconstruction in his book \"Of grammatology\", published in 1967.\n\n[[File:Deconstruction Image.jpg|400px|thumb|right|'''Linguistic sign \u201etree\u201c according to Saussure\u2019s sign system.''' Source: own.]]\n\nWith his approach of deconstruction, Derrida criticized structuralism and illustrated this with the example of Saussure's sign system (7). '''Fundamental to Saussure's sign system is the assumption that a linguistic sign always consists of a signified  (e.g., an object), and a signifier (e.g., a word for this object).''' The figure on the right exemplifies the linguistic sign \"tree\" with its two components - the content level (signified) and the expression level (signifier). According to Saussure, a signifier is assigned - arbitrarily but nevertheless unambiguously - to a signified, and the meaning of the signified results from the demarcation of the signifier from other signifiers - often as binary opposites (3, 6). The idea is that the tree  - the signified - can be unambiguously explained by the word \u201etree\u201c \u2013 the signifier - because the word \u201etree\u201c can clearly be differentiated from other words. \n\nDerrida saw a problem in this unambiguous assignment and the resulting binary opposition structure. '''For Derrida, signifier and signified do not stand in an unambiguous relationship'''; rather, their positions in the system can change. That is, a signified can become a signifier and vice versa. For the example of \"tree\" this can be explained as follows: If the sign \"tree\" is unknown in its meaning, one can look up the signifier  T R E E and receive a (simplified) meaning, i.e. the signified, \"plant\". At the same time, \"plant\" in this case is also another linguistic sign. If one now also does not know the meaning of the sign \"plant\", one again looks up the signifier P L A N T. This way, \"plant\", which was originally the signified for the sign \u201etree\u201c becomes the signifier of another sign with ist own signified. Consequently, the previously assumed/existing structure of the sign is not rigid and unambiguous, but unstable and movable (3).\n\nIn addition, '''Derrida assumed that an unambiguous meaning could never be achieved''', since every sign with its signifier always refers to other signs that either precede or follow it, and accordingly deconstructs and shifts its own meaning (7). He therefore introduced the neologism \u201adiff\u00e9rance\u2018. Diff\u00e9rance describes the never-ending shift in meaning that results from the instability and ambiguity of concepts. It highlights that a term can have several meanings or its meaning can be shifted, depending on the context in which it exists and is read (5).\n\nIn this sense, a sign is to be understood as a construct and deconstruction offers a possibility to deal with it (5). The main point is to emphasize the non-naturalness, the non-originality of a term and to show the external influences that have shaped it in the course of time (5). That is, terms can never depict absolutely unambiguously and universally what they try to depict, since they usually once held a completely different - or at least slightly shifted - meaning. Thus, they unconsciously always say more or something else than they want to say. '''With deconstruction, this historicity - Derrida spoke of \u201atrace\u2018 - of concepts is to be uncovered and made visible.''' It shall be shown that there is neither a first, original nor a last, final meaning (5).\n\nEspecially the rigid, binary opposition structures of structuralism (e.g. language/writing, man/woman, reason/madness, heterosexuality/homosexuality) are supposed to be broken up with the help of deconstruction. Derrida perceived this in its strict hierarchy of values as not neutral and consequently not logical (3). Due to the hierarchical structure, one part would always be perceived as original/\"normal\"/superior, while the other part would be seen as derivative and thus less valuable. However, Derrida's goal in deconstruction was not to completely abandon the concepts or to reverse the hierarchy of values, because in his opinion the concepts depend on each other and constitute each other in the first place. Instead, deconstruction should shift the entire system, which with its rigid structures enables the formation of such hierarchies, by making visible the instability and ambiguity of the individual concepts (2).\n\nDeconstruction, however, was conceived by Derrida less as \"a method or some tool that you apply to something from the outside\" (Derrida & Caputo 2020, p.9). While scientific methods are seen as \"settled and stable\" (Feustel 2015, p.15), deconstruction instead emphasizes the instability not only of the respective object, but also of one's own approach and goal (5). '''Accordingly, deconstruction is to be seen more as an art of reading, of shifting meaning, which occurs from within but must be made visible''' (4). \nDerrida's deconstruction influenced, especially in the USA, the so-called Yale School, whose representatives included Paul de Man, Harold Bloom, Geoffrey Hartman and Hillis Miller. They conceived of deconstruction as a poststructuralist reading for the analysis and critique of texts, and developed it further in their own work (1).\n\nDeconstruction is also drawn upon in gender and queer studies. Deconstruction is particularly prominently linked to gender theory in the work of the US philo-sopher Judith Butler. Butler questions and deconstructs the sex-gender distinction by hypothesizing that not only social sex (gender) but also biological sex (sex) is only constituted by the - partly socially enforced - constant repetition of certain practices and thus cannot be assumed as natural (2). Other representatives of deconstructive feminist theory include H\u00e9l\u00e8ne Cixous, Luce Irigaray, and Gayatri Chakravorty Spivak.\n\n\n== References ==\n(1) Babka, A. Posselt, G. 2003. ''Dekonstruktion.'' In: ''produktive differenzen.forum f\u00fcr differenz- und genderforschung.'' Available at https://differenzen.univie.ac.at/glossar.php?sp=3.<br>\n(2) Babka, A. Posselt, G. 2016. ''Gender und Dekonstruktion. Begriffe und kommentierte Grundlagentexte der Gender- und Queer-Theorie.'' Wien, Stuttgart: Facultas.<br>\n(3) Dahlerup, P. 1998. ''Dekonstruktion. Die Literaturtheorie der 1990er.'' Berlin: De Gruyter.<br>\n(4) Derrida, J. Caputo, J. D. 2020. ''Deconstruction in a Nutshell.'' New York: Fordham University Press.<br>\n(5) Feustel, R. 2015. ''Die Kunst des Verschiebens. Dekonstruktion f\u00fcr Einsteiger.'' Paderborn: Wilhelm Fink.<br>\n(6) Posselt, G. 2003. ''Struktur.'' In: ''produktive differenzen.forum f\u00fcr differenz- und genderforschung.'' Available at https://differenzen.univie.ac.at/glossar.php?sp=5.<br>\n(7) Zima, P. V. 2016. ''Die Dekonstruktion. Einf\u00fchrung und Kritik.'' 2nd Edition. T\u00fcbingen: A. Francke Verlag.\n----\n[[Category:Normativity of Methods]]\n[[Category:Qualitative]]\n\nThe [[Table of Contributors|author]] of this entry is Greta Bosse."
                    },
                    "sha1": "bmhaki2ttl08kyyfk7ft3f256h513th"
                }
            },
            {
                "title": "Delphi",
                "ns": "0",
                "id": "221",
                "revision": {
                    "id": "6230",
                    "parentid": "6227",
                    "timestamp": "2021-08-12T14:28:29Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18308",
                        "#text": "[[File:ConceptDelphi.png|450px|left|frameless|[[Sustainability Methods:About|Method categorization for Delphi]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/>\n<br/>\n'''In short:''' The Delphi Method is an interactive form of data gathering in which expert opinions are summarized and consensus is facilitated.\n\n== Background ==\nThe Delphi method originates from work at RAND Corporation, a US think-tank that advises the US military, in the late 1940s and 1950s (2, 3, 5). RAND developed \"Project Delphi\" as a mean of obtaining \"(...) the most reliable consensus of opinion of a group of experts.\" (Dalkey & Helmer 1963, p.1). At the time, the alternative - extensive gathering and analysis of quantitative data as a basis for forecasting and deliberating on future issues - was not technologically feasible (4, 5). Instead, experts were invited and asked for their opinions - and Delphi was born (see (1)).\n\n[[File:Delphi Method SCOPUS.png|400px|thumb|right|'''SCOPUS hits for the Delphi method until 2019.''' Search terms: 'Delphi Panel', 'Delphi Method', 'Delphi Methodology', 'Delphi Study', 'Delphi Survey' in Title, Abstract, Keywords. Source: own.]]\n\nIn 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).\n\n\n== What the method does ==\nThe Delphi method is \"(...) a systematic and interactive research technique for obtaining the judgment of a panel of independent experts on a specific topics\" (Hallowell & Gambatese 2010, p.99). It is used \"(...) to obtain, exchange, and develop informed opinion on a particular topic\" and shall provide \"(...) a constructive forum in which consensus may occur\" (Rayens & Hahn 2000, p.309). Put simply, experts on a topic are gathered and asked in a systematic process what they think about the future, until consensus is found. \n\n==== The Delphi procedure ====\n[[File:ResultVisualisationDelphi.png|400px|thumb|right|Questionnaire results for the original RAND study, asking for an estimate of bomb requirements. The estimated numbers per participant converge over the course of the Delphi procedure. Source: Dalkey & Helmer 1963, p.15]]\n\n'''A Delphi process typically undergoes four phases''' (see (4), (6)):\n\n1. A group of experts / stakeholders on a specific issue is identified and invited as participants for the Delphi. These participants represent different backgrounds: academics, government and non-government officials as wel, as practitioners. They should have a diverse set of perspectives and profound knowledge on the discussed issues. They may be grouped based on their organizations, skills, disciplines or qualifications (3). Their number typically ranges from 10 up to 30, depending on the complexity of the issue (2, 3, 5, 6). \n\nThe researchers then develop a questionnaire. It is informed by previous research as well as input from external experts (not the participants) who are asked to contribute knowledge and potential questions on the pertinent issue (2, 5). The amount of [[Glossary|consultation]] depends on the expertise of the researchers on the respective issue (2).\n\n2. The questionnaire is used to ask for the participants' opinions and positions related to the underlying issue. The questions often take a 'ranking-type' (3): they ask about the likelihood of potential future situations, the desirability of certain goals, the importance of specific issues and the feasibility of potential policy options. Participants may be asked to rank the answer options, e.g. from least to most desirable, least to most feasible etc. (2). Participants may also be asked yes/no questions, or to provide an estimate as a number. They can provide further information on their answers in written form. (8)\n\nThe questioning is most commonly conducted in form of a questionnaire but has more recently also been realized as individual, group, phone or digital interview sessions (2, 5). Digital questioning allows for real-time assessments of the answers and thus a quicker process. However, a step-by-step procedure provides more time for the researchers to analyze the responses (4).\n\n3. After the first round, the participants' answers are analyzed both in terms of tendency and variability. The questionnaire is adapted to the new insights: questions that already indicated consensus on a specific aspect of the issue are abandoned while disagreements are further included. 'Consensus' may be defined based on a certain percentage of participants agreeing to one option, the median of the responses or a degree of standard deviation, among other definitions (2, 5, 6, 7). New questions may be added to the questionnaire and existing questions may be rephrased based on the first set of answers (4).\n\nNext, the experts are again asked for their opinions on the newly adapted set of questions. This time, the summarized but - this is important - anonymous group results from the first round are communicated to them. This feedback is crucial in the Delphi method. It incentivizes the participants to revise their previous responses based on their new knowledge on the group's positions and thus facilitates consensus. The participants may also provide reasons for their positions (5, 6). Again, the results are analyzed. The process continues in several rounds (typically 2-5) until a satisfactory degree of consensus among all participants is reached (2-6).\n\n4. Finally, the results of the process are summarized and evaluated for all participants (4).\n\n\n== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).\n* Delphi can be \"(...) a highly motivating experience for participants\" (Rayens & Hahn 2000, p.309) due to the feedback on the group's opinions that is provided in subsequent questioning stages.\n* The Delphi method with its feedback characteristic has advantages over direct confrontation of the experts, which \"(...) all too often induces the hasty formulation of preconceived notions, an inclination to close one's mind to novel ideas, a tendency to defend a stand once taken, or, alternatively and sometimes alternately, a predisposition to be swayed by persuasively stated opinions of others.\" (Okoli & Pawlowski 2004, p.2, after Dalkey & Helmer)\n* Additionally, Delphi provides several advantages over traditional surveys:\n** Studies have shown that averages of group responses are superior to averages of individual responses. (3)\n** Non-response and drop-out of participants is low in Delphi processes. (3)\n** The availability of the experts involved allows for the researchers to (a) get their precognitions on the issue verified by the participating experts and to (b) gain further qualitative data after the Delphi process. (3)\n\nHowever, several potential pitfalls and challenges may arise during the Delphi process:\n* Delphi should not be used as a surrogate for every other type of communication - it is not feasible for every issue (4, 5, 6).\n* A specific Delphi format that was useful in one study must not work as well in another context. Instead, the process must be adapted to the research design and underlying problem (4).\n* The proper selection of participating experts constitutes a major challenge for Delphi processes (3, 4, 5, 6). In addition, the researchers should be aware that any expert is likely to forecast based on their specific sub-system perspective and might neglect other factors (4).\n* The monitor (= researcher) must not impose their own preconceptions upon the respondents when developing the questionnaire but be open for contributions from the participants. The questions should be concise and understandable and should not incentivise the participant to \"get the job over with\" (Linstone & Turoff 1975, p.568; 5).\n* Diverse forms of [[Glossary|bias]] might occur on the part of the participants that need to be anticipated by the researcher. These include discount of the future, over-optimism / over-pessimism, misinterpretations with regard to the complexity and uncertainty involved in forecasting the future as well as other forms of bias that may be imposed through the feedback process (4, 6).\n* The responses must be adequately summarized, analyzed and presented to the participants (see the variety of measures for 'consensus' in What the method does)). \"Agreement about a recommendation, future event, or potential decision does not disclose whether the individuals agreeing did so for the same underlying reasons. Failure to pursue these reasons can lead to dangerously false results.\" (Linstone & Turoff 1975, p.568).\n* Disagreements between participants should be explored instead of being ignored so that the final consensus is not artificial (4).\n* The participants should be recompensated for their demanding task (4)\n\n\n== Normativity ==\n==== Connectedness / nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).\n\n==== Everything normative related to this method ====\n* The Delphi method is highly normative because it revolves around the subjective opinions of stakeholders.\n* The selection of the participating experts is a normative endeavour and must be done carefully so as to ensure a variety of perspectives.\n* Delphi is an instrument of [[Transdisciplinarity|transdisciplinary]] research that may be used both to find potential policy options as well as to further academic proceedings. Normativity is deeply rooted in this connection between academia and the 'real-world'.\n\n\n== Outlook ==\n==== Open questions ====\n* The diverse fields in which the Delphi method was applied has diversified and thus potentially confounded its methodological homogeneity, raising the need for a more comparable application and reporting of the method (6, 7)\n\n\n== An exemplary study ==\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 title.png|600px|frameless|center|The title of the exemplary study for Delphi method. Source: kauko & Palmroos 2014]]\nIn their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. '''The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets.''' Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks' foreign net assets etc.) with simple point estimates. These variables were chosen to \"fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible.\" (p.316). The participants were provided with information on the past developments of each of these variables.\n\nThe researchers decided to go with three rounds until consensus should be reached. For the first round, questionnaires were distributed by mail, and the participants had one week to answer them. The second and third round were held on the same day after this one-week period. '''The responses from the respective previous round were re-distributed to the participants''' (each individual answer including additional comments, as well as the group average and the median for each variable). The participants were asked to consider this information, and answer all 15 questions - i.e., forecast all 15 variables - again. \n\nAfter the third round, the participants were additionally asked to fill out survey questions on a 1-5 [[Likert Scale]] about how reliable their considered their own forecasts, and how much attention they had paid to the others' forecasts and comments when these were re-distributed, both regarding each variable individually. This was done to better understand each individual's thought process.\n<br>\n[[File:Delphi - Exemplary study Kauko & Palmroos 2014 results.png|800px|thumb|center|'''The results for the Delphi process.''' It shows that the mean estimates of the group became better over time, and were most often quite close to the actual realisation. Source: Kauko & Palmroos 2014, p.326.]]\n<br>\nThe forecasting results from the Delphi process could be verified or falsified with the real developments over the next months and years, so that the researchers were able to check whether the responses actually got better during the Delphi process. '''They found that the individual responses did indeed converge over the Delphi process, and that the \"Delphi group improved between rounds 1 and 3 in 13 of the questions.\"''' (p.320). They also found that \"[d]isagreeing with the rest of the group increased the probability of adopting a new opinion, which was usually an improvement\" (p.322) and that the Delphi process \"clearly outperformed simple trend extrapolations based on the assumption that the growth rates observed in the past will continue in the future\", which they had calculated prior to the Delphi (p.324). Based on the post-Delphi survey answers, and the results for the 15 variables, the researchers further inferred that \"paying attention to each others' answers made the forecasts more accurate\" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.\n\n\n== Key Publications ==\n* Linstone, H. Turoff, M. 1975. ''The Delphi Method: Techniques and Applications''. Addison-Wesley, Boston.\nAn extensive description of the characteristics, history, pitfalls and philosophy behind the Delphi method.\n* Dalkey, N. Helmer, O. 1963. An experimental application of the Delphi method to the use of experts. Management Science 9(3). 458-467.\nThe original document illustrating the first usage of the ''Delphi'' method at RAND.\n* Gordon, T.J. Helmer, O. 1964. Report on a long-range forecasting study. RAND document P-2982.\nThe report that popularized ''Delphi'' outside of the military defense field.\n\n\n== References ==\n* (1) Dalkey, N. Helmer, O. 1963. ''An experimental application of the Delphi method to the use of experts.'' Management Science 9(3). 458-467.\n* (2) Rayens, M.K. Hahn, E.J. 2000. ''Building Consensus Using the Policy Delphi Method''. Policy, Politics, & Nursing Practice 1(4). 308-315.\n* (3) Okoli, C. Pawlowski, S.D. 2004. ''The Delphi Method as a Research Tool: An Example, Design Considerations and Applications.'' Information & Management 42(1). 15-29.\n* (4) Linstone, H. Turoff, M. 1975. ''The Delphi Method: Techniques and Applications''. Addison-Wesley, Boston.\n* (5) Gordon, T.J. 2009. ''The Delphi Method.'' Futures Research Methodology V 3.0.\n* (6) Hallowell, M.R. Gambatese, J.A. 2010. ''Qualitative Research: Application of the Delphi method to CEM Research''. Journal of Construction Engineering and Management 136(1). 99-107.\n* (7) Boulkedid et al. 2011. ''Using and Reporting the Delphi Method for Selecting Healthcare Quality Indicators: A Systematic Review.'' PLoS ONE 6(6). 1-9.\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "d6fmxc3q01ddcmx505z7pmsmbogd8ni"
                }
            },
            {
                "title": "Dendrochronology",
                "ns": "0",
                "id": "1042",
                "revision": {
                    "id": "7118",
                    "parentid": "7117",
                    "timestamp": "2023-04-18T16:04:24Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17181",
                        "#text": "[[File:ConceptDENDROCHRONOLOGY.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[DENDROCHRONOLOGY]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n\n\n'''In short:''' Dendrochronology is concerned with the analysis of tree rings, whose chronological sequence can be used to date wood samples and infer information on environmental conditions, stressors and their effects on the plant species.\n\n==Visualisation of a typical result==\n[[File:ResultVisualisationDENDROCHRONOLOGY.png|450px|thumb|left|Figure 1: modern dendrochronology lab with software for measuring and cross-dating rings and a scanner (1)]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==Background==\n[[File:Figure3 CrosssectionDENDROCHRONOLOGY.png|thumb|right|Figure 3: cross section of tree with pith, cambium, xylem, phloem and bark (4).]]\n\n[[File:Figure4 CrosssectionDENDROCHRONOLOGY.png|thumb|right|Figure 4: Cross section of basswood stained and 40x magnified (Tilia). (a) One year old stem. (b) Two years old stem. (c) Three years old stem. Numbers in figures b and c indicate the growth rings (based on (5), (6), (7)).]]\nVarious scientists had been aware of tree rings and their connection to climate for several centuries already, however the term \u2018dendrochronology\u2019 was coined by A. E. Douglass from the University of Arizona in 1929. He used the method of tree-ring dating to determine the age of ancient indigenous buildings, which helped the Navajo peoples\u2019 compensatory claims to succeed (2).\nTree ring analysis or dendrochronology is applied in many disciplines like climatology, archaeology, biology, hydrology and forestry. It can be applied to any plant species that has a woody growth type (including shrubs) and any woody parts of the plants (including branches, twigs and roots). Seasonal growth, the number of growth rings and the ring widths, is determined by the interaction of genetic and environmental factors. For example, a poplar forms wider rings than a bilberry growing under the same climatic conditions (3). Furthermore, in order to analyse seasonal growth rings only one environmental factor must dominate in limiting the growth. This limiting factor can be precipitation in arid, semi-arid or tropical parts of the world or temperature like in the temperate regions (e. g. dry and wet season in tropical forests, winter and summer in temperate forests). \n\nThe structure we see as rings is a sequence of earlywood and latewood in the secondary xylem tissue of a stem. Secondary growth reflects an increase in thickness, or lateral additions of new tissue, and essentially this secondary xylem is an important resource known as wood. The xylem is the water and nutrient conducting tissue in vascular plants. Cells for the secondary xylem are generated by the cambium layer. The cambium is a zone of undifferentiated cells that produce cells of the xylem to the centre of the stem and cells of phloem to the outer part of the stem. The phloem is the plant tissue for transport of sugars (assimilates). Xylem and phloem together form the vascular system of plants throughout the stems, branches and roots. For every vegetation period, governed by one limiting environmental factor, the cambium generates xylem to the inside of the stem and phloem to the outside. The cambium will generate more secondary xylem than phloem and old outer phloem tissue will be crushed and eventually become bark. This is why woody species accumulate more and more secondary xylem each year and not secondary phloem. The oldest rings are close to the centre of the tree stem, the youngest near the cambium and bark (Fig. 3).\n\nGrowth rings can be recognized because in the beginning of the vegetation period earlywood is being produced with larger, thin-walled cells, whereas at the end of the vegetation period latewood with smaller, thick-walled cells is produced that appears darker. The abrupt change between latewood and earlywood cells is the annual growth boundary that can be usually also recognized without magnification (Fig. 4). \n\n\n==What the method does==\n[[File:Figure5 CrossdatingDENDROCHRONOLOGY.png|300px|thumb|right|Figure 5: Cuts of two specimen that have been correctly cross-dated (2)]]\nDendrochronologists are interested in the chronological sequence of growth rings. In order to establish a dendrochronology of a certain area, scientists compare tree rings of individuals of one species. For this purpose, this species must only add one ring per growing season, the growth-limiting climatic factor must vary in intensity from year to year, and should be uniformly effective over a large geographic area (i. e. the relative annual difference in intensity is similar throughout the macroclimatic region). The variation in the growth-limiting factor must be reflected in the width of the tree rings. If these conditions are fulfilled scientists can recognize specific sequences that are of use for cross-dating or matching ring patterns between specimens (see (2) for an introduction and Fig. 5). Which species makes most sense to study depends on many factors, like the research objective, conditions in the study area, abundance and properties of the species.\n\n\n'''Sampling tree rings'''\n[[File:Figure6 IncrementBorerDENDROCHRONOLOGY.png|thumb|right|Figure 6: Increment borer (8)]]\n\nDendrochronology is a method of data gathering and analysis. The findings can then be interpreted, for instance in regards to tree wellbeing or climate. To collect the data, core sampling of living trees is performed with an increment borer (Fig 6). The increment borer consists of a handle, a borer and an extraction spoon. This tool removes a sample from the stem leaving a hole of about 5 mm wide half the stem diameter deep. This hole quickly fills with sap; however, it is still an invasive method that could allow parasites or fungi to enter the bore hole. Therefore, the tool should be cleaned after each usage and sampling should be done with care and only when it is necessary.\n\nSelection of sampling sites and individual trees depends on the research question. Usually, researchers are interested in investigating variability within years. To this end, trees that do not have underground water access are sampled. Trees growing under these conditions will have so called sensitive ring sequences useful for dating, in contrast to trees with underground water access, that will have so called complacent ring sequences with equally narrow or wide rings. Core samples are therefore preferably taken from trees growing on the top of the slope. The increment borer is placed at a 90\u00b0 angle on the stem facing the side slope. This means neither facing uphill nor downhill to avoid areas where the ring patterns are likely to be distorted\n\n\n'''Sampling'''\n\nIt is important to take field notes on the immediate environment of the sampled tree, since this information on growth-affecting parameters will be necessary for interpretation later (e. g. Table 1). When possible, samples are taken well below the first branch facing the side of the slope. After choosing a suitable area on the stem, put beeswax on the borer tip and place the tip of the increment borer at a 90\u00b0 angle on the stem. Turn the handle clockwise to drill into the tree. Aim the borer at the pith. After the borer has reached the desired depth (middle of the stem), insert the extraction spoon into the borer from the handle end. When the extraction spoon is fully inserted between the wood core and the metal sides of the borer, give the borer a full turn counter clockwise. Remove the extraction spoon with the sample from the borer. Place the sample in the envelope or mount (most recent rings orientated to the right side of the mount). Remove the borer from the tree by turning it counter clockwise. Clean the inside with WD 40.\n\nTable 1: Example of site data sheet (DBH = diameter breast height at 130 cm) \n{| class=\"wikitable\"\n{| class=\"wikitable\"\n{| class=\"wikitable\"\n|-\n| Date|| Observer|| Sample ID|| Location|| Inclination\n|-\n| Vegetation|| Species sampled|| DBH (in cm)|| Distance to surrounding trees\n|| Canopy cover\n|-\n| soil type|| Remarks (disturbance, pests, distance to open water, distance to path\u2026)\n|}\n\n\n'''Preparation'''\n[[File:Figure7 MicrotomeDENDROCHRONOLOGY.png|170x270 px|thumb|right|Figure 7: microtome (9)]]\n\nThe core sample will shrink and bend while drying. Therefore, place the core sample in a slotted mount and fixate with pushpins for air drying. After drying for about one week, cut off the core top with a core microtome to create a smooth straight surface (Fig. 7).\n\n\n'''Analysis'''\n\nThe visual inspection is done on a measuring table with a resolution of 0.01 mm and a microscope. Values are then digitalised, plotted and analysed with computer software (e. g. IML Software T-Tools Pro, Instrumenta Mechanik Labor GmbH). The number of rings and any distinct patterns (frequency of narrow or wide rings) are often what to look out for in this step. \n\n\n==Strengths and Challenges==\nAn advantage of this method is, that it can be applied to all woody (parts of) plants. One can sample the living organisms by taking only a small-diameter core sample. After dendrochronologically analysing a representative amount of the population, one can statistically calculate an estimate, which allows the researcher to assume the age of other branches/roots/etc. from that population based on their diameter without sampling them. The method can also be applied to tree stumps of logged trees or wood used as building material and charcoal samples to date an archaeological site.\n\nPicking a representative sample of plants to account for the natural variability as well as taking the necessary number of samples can be difficult, for example in steep terrain. The high pressure from competition in dense plant coverage and also injuries and diseases can affect a plants growth and obscure the influence of the climatic factor. Decayed wood is not useful for dendrochronological analysis. One should also consider the hardness of the wood of the species of interest. Bore holes are more difficult to drill into certain species, like Hornbeam and Robinia.\n\nDistinguishing the growth rings is another challenge of varying difficulty depending on the species. Coniferous trees tend to have very clear growth rings. With oak and beech, the rings usually also are fairly clear, whereas the growth rings of hornbeam are way less easy to see. \n\nFurthermore, abnormalities can occur in the ring patterns, which one has to be aware of when cross dating several samples. The ring of a year with extremely little growth could be locally absent at the sampled point of the stem. It is not missing on the whole stem, widths of rings vary lightly across the stem of a tree. Similarly, double or false rings can form in parts of the stem, meaning that there are two dark bands from one year. Causes for this are not well understood yet. Taking two samples of the same tree decreases the chance that any such irregularity is missed in analysis.\n\n==Normativity==\nAlthough the bore hole is relatively small in diameter, this method still is invasive and pathogens could enter the plant system through the wound caused by the borer, potentially leading to disease or even death of the plant due to the research. As a guiding principle for ecological sampling, researchers should cause as little disturbance as possible. One should therefore consider alternatives and choose the least invasive method applicable and accessible.\n\nUncertainty results from the subjective experience and view of the researcher. The quality and clarity of the core samples depends on the experience of the researcher with taking and preparing samples. At the analysis step, different people might count the growth rings differently. Especially if multiple researchers are working on the same project, it thus is necessary to agree on and document how is counted, in order to ensure comparability and reproducibility.\n\nOne practical application of dendrochronology was the Navajo Land Claim Project. For 18 years from 1951 onwards, the University of Arizona\u2019s Laboratory of Tree-Ring Research and the Navajo Tribe worked together, closely linking science and society. They took thousands of samples from living trees and old Navajo hogans in and around the Navajo Reservation. Cross-dating the samples provided evidence, that the Navajo peoples had formerly occupied areas outside their reservation, which they had been evicted from (10). Therefore, the United States Land Claims Commission acknowledged the tribe to be eligible for compensation (2).\n\n==Outlook==\nTree ring analysis is also applied to determine the health status of individuals or forests and effects of climate change (dendroclimatology). Based on the growth of these long-lived species one can interfere with past environmental conditions and current stressors. Due to climate change, years with droughts increase but on the other hand the length of growing seasons also increases. How this affects different species and forest composition can be investigated through tree ring analysis. Thus, dendrochronology can help foresters in making their woods future-proof and sustainable. It could also play a role in the selection and management of city trees. On a local or regional scale, insights gained from tree ring analysis might inform climate adaptation measures which involve plants.\n\nNew, less invasive tools are being developed for collecting data on growth rings of living trees. In forestry, even thinner borers are in use already, reducing the possible negative impacts on plant health.\n\n==Key publications==\nH\u00e4rdtle, W., Niemeyer, T., Assmann, T., Aulinger, A., Fichtner, A., Lang, A., ... & von Oheimb, G. (2013). Climatic responses of tree-ring width and \u03b413C signatures of sessile oak (Quercus petraea Liebl.) on soils with contrasting water supply.\u00a0Plant ecology,\u00a0214 (9), 1147-1156.\n\nHeklau, H., & von Wehrden, H. (2011). Wood anatomy reflects the distribution of Krascheninnikovia ceratoides (Chenopodiaceae).\u00a0Flora-Morphology, Distribution, Functional Ecology of Plants,\u00a0206 (4), 300-309.\n\nMausolf, K., H\u00e4rdtle, W., Jansen, K., Delory, B. M., Hertel, D., Leuschner, C., ... & Fichtner, A. (2018). Legacy effects of land-use modulate tree growth responses to climate extremes.\u00a0Oecologia,\u00a0187 (3), 825-837.\n\nSchweingruber, F. H. (2001).\u00a0Dendro\u00f6kologische Holzanatomie. Eidgen\u00f6ssische Forschungsanstalt WSL, Haupt, Bern, Stuttgart, Wien.\n\nFurthermore, see references.\n\n==References== \n(1) Foster, K. (2013a). Dendrochronology Lab at ICRAF Nairobi. Online at: https://www.flickr.com/photos/icraf/9245105189/in/photostream/ (08.02.2023).\n\n(2) Stokes, M. A. & Smiley, T.L. (1996).\u00a0An introduction to tree-ring dating. University of Arizona Press.\n\n(3) Schweingruber, F. H. (1988).\u00a0Tree rings: basics and applications of dendrochronology. Kluwer, Dordrecht, Boston, Lancaster, Tokyo.\n\n(4) USDA (1968). Growth rings. Cross section of tree. USDA Forest Service, Pacific Northwest Region, State and Private Forestry, Forest Health Protection. Portland Station Collection, La Grande, Oregon. Online at: https://www.flickr.com/photos/151887236@N05/27086026918 (08.02.2023).\n\n(5) Fayette, A., Reynolds, M. S. (2014a). Woody Dicot Stem: One Year Tilia. Berkshire Community College Bioscience Image Library. Online at: https://www.flickr.com/photos/146824358@N03/34315403424 (08.02.2023).\n\n(6) Fayette, A., Reynolds, M. S. (2014b). Woody Dicot Stem: Three Year Old Tilia. Berkshire Community College Bioscience Image Library. Online at: https://www.flickr.com/photos/146824358@N03/34714986052 (08.02.2023).\n\n(7) Fayette, A., Reynolds, M. S. (2014c). Woody Dicot Stem: Two Annual Rings in Tilia. Berkshire Community College Bioscience Image Library. Online at: https://www.flickr.com/photos/146824358@N03/35031940521 (08.02.2023).\n\n(8) Beentree (2006). Pressler drill work on Populus alba. Online at: https://commons.wikimedia.org/wiki/File:Pressler_drill_5_beentree.jpg (08.02.2023).\n\n(9) Foster, K. (2013b). Dendrochronology Lab at ICRAF Nairobi. Online at: https://www.flickr.com/photos/icraf/9247888378/in/photostream/ (08.02.2023).\n\n(10) Stokes, M.A., Smiley, T.L. (1963). Tree-ring dates from the Navajo Land Claim I. The northern sector. Tree-Ring Bulletin, 25(3-4), 8-18.\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|authors]] of this entry are Heike Zimmermann and Fabienne Friedrichs."
                    },
                    "sha1": "71dkwf2ygfa96dnzmg7tmympnqootff"
                }
            },
            {
                "title": "Descriptive statistics",
                "ns": "0",
                "id": "119",
                "revision": {
                    "id": "6009",
                    "parentid": "5767",
                    "timestamp": "2021-07-08T13:24:46Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "minor": null,
                    "comment": "/* Mode */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3382",
                        "#text": "__NOTOC__\n[[File:Bildschirmfoto 2020-03-28 um 15.39.37.png|200px|right|frameless|]]\n\n'''Descriptive stats are what most people think stats are all about.''' Many people believe that the simple observation of ''more'' or ''less'', or the mere calculation of an average value, is what statistics are all about. Of course, this is not the case - statistics is more than descriptive statistics, or whimsical [[Introduction to statistical figures|bar plots or even pie charts]]. Still, knowing the basics is important, and most of you probably already calculated things like mean and median in school. So let us have another look to refresh your memory.\n\n== Basics of descriptive statistics ==\n[[File:Bildschirmfoto 2020-03-28 um 15.48.41.png|200px|thumb|right|This graphic visualizes what mean, mode and median explain regarding a dataset.]]\n\n====Mean====\nThe [https://www.youtube.com/watch?v=mk8tOD0t8M0 mean] is the average of numbers you can simply calculate by adding up all the numbers and then divide them by how many numbers there are in total.\n\n====Median====\nThe median is the middle number in a sorted set of numbers. It can be substantially different from the mean value, for instance when you have large gaps or cover wide ranges within your [[Glossary|data]]. Therefore, it is more robust against outliers.\n\n====Mode====\nThe mode is the value that appears most often. It can be helpful in large datasets or when you have a lot of repetitions within the dataset.\n\n====Range====\nThe range is simply the difference between the lowest and the highest value and consequently it can also be calculated like this.\n\n[[File:Bildschirmfoto 2020-03-28 um 15.51.31.png|thumb|right|This graph shows how the standard deviation is spread from the mean.]]\n\n====Standard deviation====\nThe standard deviation is calculated as the square root of variance by determining the variation between each data point relative to the mean. It is a measure of how spread out your numbers are. If the data points are further from the mean, there is a higher deviation within the data set. The higher the standard deviation, the more spread out the data.\n\n== R examples ==\nNow, let us have a look at how to calculate these values in R.\n<syntaxhighlight lang=\"R\" line>\n\n#descriptive statistics using the Swiss dataset\nswiss\nswiss_data<-swiss\n\n#we are choosing the column fertility for this example\n#let's begin with calculating the mean\nmean(swiss_data$Fertility)\n\n#median\nmedian(swiss_data$Fertility)\n\n#range\nrange(swiss_data$Fertility)\n\n#standard deviation\nsd(swiss_data$Fertility)\n\n#summary - includes minimum, maximum, mean, median, 1st & 3rd Quartile\nsummary(swiss_data$Fertility)\n\n</syntaxhighlight>\n\n\n==External Links==\n====Videos====\n* [https://www.youtube.com/watch?v=h8EYEJ32oQ8&list=PLU5aQXLWR3_yYS0ZYRA-5g5YSSYLNZ6Mc Descriptive Statistics]: A whole video series about descriptive statistics from the Khan academy\n* [https://www.youtube.com/watch?v=MRqtXL2WX2M Standard Deviation]: A brief explanation\n* [https://www.youtube.com/watch?v=mk8tOD0t8M0 Mode, Median, Mean, Range & Standard Deviation]: A good summary\n\n====Articles====\n* [https://www.investopedia.com/terms/d/descriptive_statistics.asp Descriptive Statistics]: An introduction\n* [http://intellspot.com/descriptive-statistics-examples/ Descriptive Statistics]: A detailed summary\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "1r8b6lg3nf38ok14bndzp0dfkkpaa9g"
                }
            },
            {
                "title": "Design Criteria of Methods",
                "ns": "0",
                "id": "450",
                "revision": {
                    "id": "6719",
                    "parentid": "6405",
                    "timestamp": "2022-06-15T20:29:35Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19672",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Design Criteria of Methods (German)]]\n\n'''In short:''' This entry provides a critical overview and understanding of the methodological canon of science. It presents the underlying conceptualisation of scientific methods that guides the categorisation of [[Methods|the presented methods]] on this Wiki.\n\n\n== Why change the perspective on methods? ==\nA strong reasoning why we need a different way to think about scientific methods is actually rooted in the history of science, and how the way we think about methods in the future should be different from these past developments. [https://de.wikipedia.org/wiki/Thomas_S._Kuhn Kuhn] talks about revolution and [[Glossary|paradigm]] shifts in scientific thinking, and many would firmly argue that such a paradigm shift is immanent right now. Actually, it is already happening. \n\nThe entry on the [https://plato.stanford.edu/entries/epistemology/ ''epistemology''] of scientific methods outlines in detail how we came to the current situation, and explains why our thinking about scientific methods - and with it our thinking about science in general - will shift due to and compared to the past. For now, let us focus on the present, and even more importantly, the future. For the epistemology please refer you to the [[History of Methods|respective article]], since much is rooted in the history of science, and humankind as such. \n\n[[File:globalwealthreport_statista.jpeg|thumb|400px|right|'''Global Wealth Distribution'''. Source: [https://www.statista.com/chart/11857/the-global-pyramid-of-wealth/ Statista]]] \n'''Today, we face unprecedented challenges in terms of magnitude and complexity.''' While some problems are still easy to solve, others are not. Due to the complex phenomena of globalisation we now face challenges that are truly spanning across the globe, and may as well affect the whole globe. Globalisation once started with the promise of a creation of a united and connected humankind. This is of course a very naive worldview and, as we see today, has failed in many regards. Instead, we see a rise in global inequality in terms of many indicators, among them [https://www.gapminder.org/tools/#$chart-type=bubbles global wealth distribution]. Climate change, biodiversity loss and ocean acidification showcase that the challenges are severe, and may as well lead to an end of the world as we know it, if you are a pessimist. If your are not a pessimist, then stewing in a grim scenario of our future demise will not help us to prevent this from happening. Instead, [https://www.pnas.org/content/108/49/19449 sustainability science] and many other branches of science opted for a solution orientated agenda, which proclaims that instead of merely describing problems, and despite the strong urge to complain about it, we need to work together and find solutions for these wicked problems. Over the last few decades, science inched its way slowly out of its ''ivory towers'', and the necessity of science and society [[Transdisciplinarity| working together]] emerged. While this will often bring us closer to potential solutions that ideally serve more and more people, science is still stuck in a mode that is often arrogant, resource driven and anachronistic. While this is surely a harsh word, some people call this \"normal science\". However, the current challenges demand a new mode of science; one that is radical yet rooted in long traditions, collaborative yet able to focus deeply on specific parts of knowledge, solution orientated yet not naive, and peaceful instead of confrontational and resource-focussed. \n\nScience has long realised that due to globalisation, shared and joined perspectives between disciplines and the benefits of modern communication, we - scientists from all disciplinary and cultural backgrounds - often have a shared interest in specific topics. For every topic, there are different scientific disciplines that focus on it, albeit from different perspectives. Take climate change, where many disciplines offer their five cent to the debate. '''Will any of the singular scientific disciplines solve the problems alone? Most likely not.''' Werewolves may be killed by silver bullets, yet for most problems we currently face, there is no silver bullet. We will have to think of more complex solutions, often working in unisono across different disciplines, and together with society. \n\nHowever, most disciplines have established specific traditions when it comes to the methods that are being used. While these traditions guarantee experience, they may not allow us to solve the problem we currently face. Some methods are exclusive for certain disciplines, yet most methods are used by several disciplines. Sometimes these methods co-evolved, but more often than not the methods departed from a joined starting point and then become more specific in the respective context of the particular branch of science. For instance, [[Open Interview|interviews]] are widely used in many areas of science, yet the implementation is often very diverse, and specific branches of sciences typically favour specific types of interviews. This gets us stuck if we want to share insights, work together and combine knowledge gathered in different disciplines. Undisputedly, modern science increasingly builds on a canon of diverse approaches, and there is a clear necessity to unpack the \u2018silos\u2019 that methods are currently packed in. While nothing can be said against deep methodological expertise, some disciplines have the tendency to rely on their methods, and declare their approaches as the most valid ones. However, we all look at the Universe from different perspectives, through different lenses, and create different models. '''The choice of the scientific method strongly determines the outcome of our research.''' If someone insist on a specific method, then this person will receive the specific knowledge that this method is able to unlock. However, other parts of knowledge may be impossible to be unlocked through this method, and there is not a single method that can unlock all knowledge. \n\n'''We need to choose and apply methods depending on the type of knowledge we aim to create, regardless of the disciplinary background or tradition.''' We should aim to become more and more experienced and empowered to use the method that is most ideal for each research purpose and not rely solely on what our discipline has always been doing. In order to achieve this, design criteria of methods can help to create a conceptualization of the nature of methods. In other words: what are the underlying principles that guide the available scientific methods? First, we need to start with the most fundamental question:\n\n\n== What are scientific methods? ==\nGenerally speaking, ''scientific methods create knowledge''. This knowledge creation process follows certain principles and has a certain rigour. Knowledge that is created through scientific methods should be ideally [https://plato.stanford.edu/entries/scientific-reproducibility/ ''reproducible''], which means that someone else under the given circumstances would come up with the same insights when using the same respective methods. This is insofar important, as other people would maybe create different data under a similar setting, but all the data should answer research questions or hypotheses in the same way. However, there are some methods that may create different knowledge patterns, which is why documentation is pivotal in the application of scientific methods. Some forms of knowledge, such as the perception of individuals, cannot be reproduced, because these are singular perspectives. '''Knowledge created through scientific methods hence either follows a systematic application of methods, or a systematic documentation of the application of methods'''. Reproducible approaches create the same knowledge, and other approaches should be equally well documented to safeguard that for all steps taken it can be understood what was being done precisely. \n\nAnother possibility to define methods concerns the different stages of research in which they are applied. '''Methods are about gathering data, analysing data, and interpreting data.''' Not all methods do all of these three steps, in fact most methods are even exclusive to one or two of these steps. For instance, one may analyse [[Semi-structured Interview|structured interviews]] - which are one way to gather data - with statistical tests, which are a form of analysis. The interpretation of the results is then built around the design of the interviews, and there are norms and much experience concerning the interpretation of statistical tests. Hence, gathering data, analysing it, and then interpreting the results are part of a process that we call ''design criteria of methods''. Established methods often follow certain more or less established norms, and the norms can be broken down into the main design criteria of methods. Let us have a look at these. \n\n==== Quantitative vs Qualitative ====\n'''One of the strongest discourses regarding the classification of methods revolves around the question whether a method is quantitative or qualitative'''. [[:Category:Quantitative|''Quantitative'']] methods focus on the measurement, counting and constructed generalisation, linking the statistical or mathematical analysis of data, as well as the interpretation of data that consists of numbers to extract [[Glossary|patterns]] or support theories. Simply spoken, quantitative methods are about numbers. [[:Category:Qualitative|''Qualitative'']] methods, on the other hand, focus on the human dimensions of the observable or conceptual reality, often linking observational data or interpretation of existing data directly to [[Glossary|theory]] or concepts, allowing for deep contextual understanding. Both quantitative and qualitative methods are ''[[Normativity of Methods|normative]]''. These two generally different lines of thinking are increasingly linked in recent decades, yet the majority of research - and more importantly, disciplines - are dominated by either qualitative or quantitative methods. While this is perfectly fine per se, there is a deep ideological trench between these two approaches, and much judgement is passed on which approach is more valid. This can be misleading if not wrong, and propose instead to choose the appropriate approach depending on the intended knowledge. However, there is one caveat: much of the scientific canon of the last decades was dominated by research building on quantitative approaches. However, new exciting methods emerge especially in qualitative research. Since novel solutions are necessary for the problems we currently face, it seems necessary that the amount as well as the proportion of qualitative research increases in the future. \n\n==== Deductive vs Inductive ====\n[[File:deductiveinductive_danielmiessler.png|thumb|left|'''Deductive vs. Inductive Reasoning'''. Source: [https://danielmiessler.com/images/Screen-Shot-2016-07-15-at-9.57.07-AM.png Daniel Miessler]]] \n'''Methods can further be distinguished into whether they are inductive or deductive'''. [[:Category:Deductive|''Deductive'']] reasoning builds on statements or theories that are confirmed by observation or can be confirmed by logic. This is rooted deeply in the tradition of [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon] and has become an important baseline of especially the natural sciences and research utilising quantitative methods. While Bacon can be clearly seen as being more leaning towards the inductive, he paved the road towards a clearer distinction between the inductive and the deductive. If there is such a thing as the scientific method at all, we owe much of the systematic behind it to Bacon. [[:Category:Inductive|''Inductive'']] methods draw conclusions based on data or observations. Many argue that these inferences are only approximations of a probable truth. Deriving the knowledge from data and not from [[Glossary|theory]] in inductive research can be seen as a counterpoint to hypothesis-driven deductive reasoning, although this is not always the case. While research even today is mostly stated to be clearly either inductive or deductive, this is often a hoax. You have many folks using large datasets and stating clear hypotheses. If these folks were honest, it would become clear that they analysed the data until they had these hypotheses, yet they publish the results with hypotheses they pretend they had from the very beginning. This is a clear indicator of scientific tradition getting back at us which, up until today, is often built on a culture that demands hypothesis building. Then again, other research is equally dogmatic in demanding an inductive approach, widely rejecting any deduction. Here we will not discuss [[Big problems for later|the merits of inductive reasoning]] here, but will only highlight that its openness and urge to avoid being deterministic provides an important and relevant counterpoint to deductive research. Just as in the realms of quantitative and qualitative methods, there is a deep trench between inductive and deductive research. One possibility to overcome this in an honest and valuable approach would be abductive reasoning, which consists of iterations between data and theory. Modern research is actually framed and conducted in such a way with a constant exchange between data and theory. Considering the complexity of modern research, this also makes sense: on the one hand, not all research can be broken down into small \"hypothesisable\" units, and on the other hand, so much knowledge already exists, which is why much research builds on previous theories and knowledge. Still, today, and for years to come, we will have to acknowledge that much of modern research to date will fall into the traditionally established inductive or deductive category. It is therefore not surprising that [[Different_paths_to_knowledge#Critical_Theory_.26_Bias|critical theory]] and other approaches cast doubt over these rigid criteria, and the knowledge they produce. Nevertheless, they represent an important categorisation for the current state of the art of many areas of science, and therefore deserve critical attention, least because many methods imply either one or the other approach. \n\n==== Spatial and Temporal Scales ====\n'''Different scientific methods are focussing on or even restricted to certain spatial [[Glossary|scales]], yet others may span across scales'''. Some methods are operationalised on a global scale, while others focus on the individual. For instance, a statistical correlation analysis can be applied on data about individuals gathered in a survey, but also be applied to global economic data. A ''global'' spatial scale of sampling is defined by data that covers the whole globe, or a non-deliberately chosen part of the globe. Such global analyses are of increasing importance, as they allow us to have a truly united understanding of societal and natural phenomena that concern the global scale. An ''individual'' scale of sampling is a scale that focuses on individual objects, which includes people and other living entities. The individual scale can give us deep insight into living beings, with a pronounced focus on research on humans. Since our concepts of humans are diverse, there are also many diverse approaches found here, and many branches of science focus on the individual. This scale certainly gained importance over the last decades, and we are only starting to understand the diversity of knowledge that an individual scale has to offer. \n\nIn between, there is a huge void that can have different names in different domains of science - \u2018landscapes\u2019, \u2018systems\u2019, \u2018institutions\u2019, \u2018catchments\u2019, and others. Different branches of science focus on different mid-scales, yet this is often considered to be one of the most abundantly investigated scales, since it can generate a great diversity of data. We will call it the \u2018system scale\u2019. A ''system'' scale of sampling is defined by any scale that contains several individual objects that interact or are embedded in a wider matrix surrounding. Regarding our understanding of systems, this is a relevant methodological scale, because it can generate a great diversity of empirical data, and many diverse methods are applied at this scale. In addition, it is also quite a relevant scale in terms of normativity because it can generate knowledge about change in systems which can be operationalised with the goal to foster or hinder such change. Hence, this system scale can be quite relevant in terms of knowledge that focuses on policy. Changing the globe takes certainly longer, and understanding change in people has many deeper theoretical foundations and frameworks. \n\n'''The last criterion to conceptualise methods concerns time.''' The vast majority of empirical studies look at one slice of time (the ''present''), and this makes the studies that try to understand the past or make the bold attempt to understand more about the future all the more precious. Analysis of the ''past'' can be greatly diverse, from quantitative longitudinal data analysis to deep [[Hermeneutics|hermeneutical]] text analysis and interpretation of writers long gone. There are many windows into the past, and we only start to unravel many of the most fascinating perspectives, as much of the treasures available are only now investigated by science. Still, there is a long tradition in history studies, cultural studies, and many more branches of sciences that prove how much we can learn from the past. With the utilisation of predictive models, the development of scenarios about the ''future'' and many other available approaches, science also attempts to generate more knowledge about what might become. The very concept of the future is a human privilege that science increasingly focuses on because of the many challenges we face. New approaches emerged over the last decades, and our knowledge about the future certainly grows. To date, we have to acknowledge that these studies are still all too rare, and much of our empirical knowledge builds on snapshots of a certain point in time. If you want to learn more about the design criterion of time, please refer to the entry on [[Time]].\n\n'''Based on these considerations, one needs to remember the following criteria when you approach a concrete scientific method:''' \n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scale: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scale: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\n\nTry to understand how methods can be operationalised in view of these criteria. Some methods only tick one option per criterion, and others may span across many different combinations of criteria. Out of the [[Methods|great diversity of methods]], this categorization gives you a first positioning of each specific method. There are literally hundreds of methods out there. Instead of understanding all of them, try to understand the criteria that unite and differentiate them. Only through an understanding of these methodological design criteria may you be able to choose the method that creates the knowledge that may be needed. \n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "2xxctktrd3ryudaxonpxbhlab0p6krn"
                }
            },
            {
                "title": "Design Criteria of Methods (German)",
                "ns": "0",
                "id": "493",
                "revision": {
                    "id": "6785",
                    "parentid": "6407",
                    "timestamp": "2022-10-10T08:31:02Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "22040",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Design Criteria of Methods]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag bietet einen kritischen \u00dcberblick und ein Verst\u00e4ndnis des methodischen Kanons der Wissenschaft. Er stellt  die zugrundeliegende Konzeptualisierung der wissenschaftlichen Methoden vor, die der Kategorisierung der [[Methods|auf diesem Wiki vorgestellten Methoden]] zugrundeliegt.\n\n== Wozu der Perspektivwechsel? ==\nDie beste Argumentation, die mir einf\u00e4llt, warum wir eine andere Art und Weise brauchen, \u00fcber wissenschaftliche Methoden nachzudenken, ist eigentlich in der Wissenschaftsgeschichte verwurzelt, und wie die Art und Weise, wie wir \u00fcber Methoden in der Zukunft denken, sich von diesen vergangenen Entwicklungen unterscheiden sollte. [https://de.wikipedia.org/wiki/Thomas_S._Kuhn Kuhn] spricht von Revolution und Paradigmenwechsel im wissenschaftlichen Denken, und ich glaube fest daran, dass ein solcher Paradigmenwechsel gerade jetzt immanent ist. Tats\u00e4chlich findet er bereits statt. \n\n[[File:globalwealthreport_statista.jpeg|thumb|400px|right|'''Die Globale Verteilung von Geld'''. Quelle: [https://www.statista.com/chart/11857/the-global-pyramid-of-wealth/ Statista]]] \n'''Heute stehen wir vor beispiellosen Herausforderungen in Bezug auf deren Umfang und Komplexit\u00e4t.''' W\u00e4hrend einige Probleme noch leicht zu l\u00f6sen sind, sind es andere nicht. Aufgrund der komplexen Ph\u00e4nomene der Globalisierung stehen wir heute vor Herausforderungen, die sich wirklich \u00fcber den ganzen Globus erstrecken und ebenso gut den ganzen Globus betreffen k\u00f6nnen. Die Globalisierung begann einst mit dem Versprechen, eine geeinte und vernetzte Menschheit zu schaffen. Dies ist nat\u00fcrlich eine sehr naive Weltsicht und ist, wie wir heute sehen, in vielerlei Hinsicht gescheitert. Stattdessen sehen wir eine Zunahme der globalen Ungleichheit in Bezug auf viele Indikatoren, darunter [https://www.gapminder.org/tools/#$chart-type=bubbles globale Wohlstandsverteilung]. Der Klimawandel, der Verlust der biologischen Vielfalt und die Versauerung der Ozeane zeigen, dass die Herausforderungen gravierend sind und ebenso gut zu einem Ende der Welt, wie wir sie kennen, f\u00fchren k\u00f6nnen, wenn man Pessimist*in ist. Ich bin kein Pessimist, denn das Schmoren in einem d\u00fcsteren Szenario unseres zuk\u00fcnftigen Untergangs wird uns nicht helfen, dies zu verhindern. Stattdessen haben sich [https://www.pnas.org/content/108/49/19449 Nachhaltigkeitswissenschaft] und viele andere Wissenschaftszweige f\u00fcr eine l\u00f6sungsorientierte Agenda entschieden, die verk\u00fcndet, dass wir, anstatt Probleme nur zu beschreiben, und trotz des starken Drangs, uns dar\u00fcber zu beschweren, zusammenarbeiten und L\u00f6sungen f\u00fcr diese b\u00f6sen Probleme finden m\u00fcssen. In den letzten Jahrzehnten hat sich die Wissenschaft langsam aus ihren \"Elfenbeint\u00fcrmen\" herausgearbeitet, und die Notwendigkeit der [[Transdisciplinarity|Zusammenarbeit von Wissenschaft und Gesellschaft]] wurde deutlich. Obwohl uns dies oft n\u00e4her an m\u00f6gliche L\u00f6sungen bringt, die im Idealfall mehr und mehr Menschen dienen, steckt die Wissenschaft immer noch in einem Modus fest, der oft arrogant, ressourcenorientiert und anachronistisch ist. Obwohl dies sicherlich harte Worte sind, nennen einige Leute dies \"normale Wissenschaft\". Die aktuellen Herausforderungen erfordern jedoch einen neuen Wissenschaftsmodus; einen radikalen, aber in langen Traditionen verwurzelten, kooperativen, der aber dennoch in der Lage ist, sich tief auf bestimmte Teile des Wissens zu konzentrieren, l\u00f6sungsorientiert, aber nicht naiv, und friedlich, anstatt konfrontativ und ressourcenorientiert zu sein. \n\nDie Wissenschaft hat l\u00e4ngst erkannt, dass wir - Wissenschaftler*innen aus allen Disziplinen und Kulturen - aufgrund der Globalisierung, der gemeinsamen und gemeinsamen Perspektiven der Disziplinen und der Vorteile der modernen Kommunikation oft ein gemeinsames Interesse an bestimmten Themen haben. F\u00fcr jedes Thema gibt es verschiedene wissenschaftliche Disziplinen, die sich damit befassen, wenn auch aus unterschiedlichen Perspektiven. Nehmen Sie den Klimawandel, wo viele Disziplinen ihren f\u00fcnf Cent f\u00fcr die Debatte anbieten. '''Wird eine der einzelnen wissenschaftlichen Disziplinen die Probleme allein l\u00f6sen? H\u00f6chstwahrscheinlich nicht.'''' Werw\u00f6lfe k\u00f6nnen durch Silberkugeln get\u00f6tet werden, doch f\u00fcr die meisten Probleme, mit denen wir derzeit konfrontiert sind, gibt es keine Silberkugel. Wir werden uns komplexere L\u00f6sungen ausdenken m\u00fcssen, die oft in unisono \u00fcber verschiedene Disziplinen hinweg und gemeinsam mit der Gesellschaft funktionieren. \n\nDie meisten Disziplinen haben jedoch spezifische Traditionen etabliert, wenn es um die Methoden geht, die angewendet werden. Diese Traditionen garantieren zwar Erfahrung, aber sie erlauben es uns vielleicht nicht, das Problem zu l\u00f6sen, mit dem wir derzeit konfrontiert sind. Einige Methoden sind f\u00fcr bestimmte Disziplinen exklusiv, doch die meisten Methoden werden von mehreren Disziplinen angewendet. Manchmal haben sich diese Methoden gemeinsam entwickelt, aber in den meisten F\u00e4llen haben sie sich von einem gemeinsamen Ausgangspunkt entfernt und werden dann im jeweiligen Kontext des jeweiligen Wissenschaftszweigs spezifischer. Beispielsweise sind [[Open Interview|Interviews]] in vielen Bereichen der Wissenschaft weit verbreitet, doch die Umsetzung ist oft sehr unterschiedlich, und bestimmte Wissenschaftszweige bevorzugen in der Regel bestimmte Arten von Interviews. Das bringt uns nicht weiter, wenn wir Einsichten austauschen, zusammenarbeiten und das in verschiedenen Disziplinen gesammelte Wissen kombinieren wollen. Unbestritten ist, dass die moderne Wissenschaft zunehmend auf einem Kanon unterschiedlicher Ans\u00e4tze aufbaut, und es besteht eine klare Notwendigkeit, die \"Silos\", in die die Methoden derzeit gepackt sind, auszupacken. Zwar kann nichts gegen tiefes methodologisches Fachwissen gesagt werden, aber einige Disziplinen neigen dazu, sich auf ihre Methoden zu verlassen und ihre Ans\u00e4tze als die g\u00fcltigsten zu deklarieren. Wir alle betrachten das Universum jedoch aus unterschiedlichen Perspektiven, durch unterschiedliche Linsen und erstellen unterschiedliche Modelle. '''Die Wahl der wissenschaftlichen Methode bestimmt stark das Ergebnis unserer Forschung.''' Wenn ich auf einer bestimmten Methode bestehe, erhalte ich das spezifische Wissen, das diese Methode freizusetzen vermag. Es kann jedoch sein, dass es unm\u00f6glich ist, andere Teile des Wissens durch diese Methode freizuschalten, und es gibt nicht eine einzige Methode, die das gesamte Wissen freischalten kann. \n\n'''Ich bin \u00fcberzeugt, dass wir Methoden je nach der Art des Wissens, das wir schaffen wollen, ausw\u00e4hlen und anwenden m\u00fcssen, unabh\u00e4ngig vom disziplin\u00e4ren Hintergrund oder der Tradition.''' Wir sollten danach streben, immer erfahrener und bef\u00e4higt zu werden, die f\u00fcr jeden Forschungszweck am besten geeignete Methode anzuwenden und uns nicht allein auf das zu verlassen, was unsere Disziplin schon immer getan hat. Um dies zu erreichen, schlage ich Designkriterien von Methoden vor - eine Konzeptualisierung der Natur von Methoden. Mit anderen Worten: Welches sind die zugrunde liegenden Prinzipien, die die verf\u00fcgbaren wissenschaftlichen Methoden leiten? Zun\u00e4chst m\u00fcssen wir mit der grundlegendsten Frage beginnen: \n\n\n== Was sind wissenschaftliche Methoden? ==\nAllgemein gesprochen: ''wissenschaftliche Methoden schaffen Wissen''. Dieser Prozess der Wissenssch\u00f6pfung folgt bestimmten Prinzipien und hat eine gewisse Sorgfalt. Wissen, das durch wissenschaftliche Methoden geschaffen wird, sollte idealerweise [https://plato.stanford.edu/entries/scientific-reproducibility/ ''reproduzierbar''] sein, was bedeutet, dass jemand anderes unter den gegebenen Umst\u00e4nden bei Anwendung der jeweils gleichen Methoden zu den gleichen Erkenntnissen kommen w\u00fcrde. Dies ist insofern wichtig, als andere Personen unter \u00e4hnlichen Bedingungen vielleicht andere Daten erstellen w\u00fcrden, aber alle Daten sollten Forschungsfragen oder Hypothesen auf die gleiche Weise beantworten. Es gibt jedoch einige Methoden, die unterschiedliche Wissensmuster erzeugen k\u00f6nnen, weshalb die Dokumentation bei der Anwendung wissenschaftlicher Methoden von zentraler Bedeutung ist. Einige Wissensformen, wie z.B. die Wahrnehmung von Individuen, k\u00f6nnen nicht reproduziert werden, da es sich dabei um singul\u00e4re Perspektiven handelt. '''Wissen, das durch wissenschaftliche Methoden geschaffen wird, folgt daher entweder einer systematischen Anwendung von Methoden oder einer systematischen Dokumentation der Anwendung von Methoden'''. Reproduzierbare Ans\u00e4tze schaffen dasselbe Wissen, und andere Ans\u00e4tze sollten ebenso gut dokumentiert werden, um sicherzustellen, dass bei allen Schritten, die unternommen werden, nachvollzogen werden kann, was genau getan wurde. \n\nEine weitere M\u00f6glichkeit, Methoden zu definieren, betrifft die verschiedenen Stadien der Forschung, in denen sie angewandt werden. '''Bei Methoden geht es darum, Daten zu sammeln, Daten zu analysieren und Daten zu interpretieren.''' Nicht alle Methoden erf\u00fcllen alle diese drei Schritte, die meisten Methoden sind sogar ausschlie\u00dflich auf einen oder zwei dieser Schritte beschr\u00e4nkt. Zum Beispiel kann man [[Semi-structured Interview|strukturierte Interviews]]] - die eine M\u00f6glichkeit sind, Daten zu sammeln - mit statistischen Tests analysieren, die eine Form der Analyse darstellen. Die Interpretation der Ergebnisse st\u00fctzt sich dann auf das Design der Interviews, und es gibt Normen und viel Erfahrung mit der Interpretation statistischer Tests. Daher sind das Sammeln von Daten, ihre Analyse und die anschlie\u00dfende Interpretation der Ergebnisse Teil eines Prozesses, den wir \"Designkriterien f\u00fcr Methoden\" nennen. Etablierte Methoden folgen oft bestimmten mehr oder weniger etablierten Normen, und die Normen lassen sich in die wichtigsten Designkriterien von Methoden unterteilen. Werfen wir einen Blick auf diese. \n\n==== Quantitativ oder Qualitativ ====\n'''Einer der st\u00e4rksten Diskurse \u00fcber die Klassifizierung von Methoden dreht sich um die Frage, ob eine Methode quantitativ oder qualitativ ist'''. [[:Category:Quantitative||''Quantitative'']] Methoden konzentrieren sich auf die Messung, Z\u00e4hlung und konstruierte Verallgemeinerung, wobei die statistische oder mathematische Analyse von Daten sowie die Interpretation von Daten, die aus Zahlen bestehen, miteinander verkn\u00fcpft werden, um Muster zu extrahieren oder Theorien zu untermauern. Einfach ausgedr\u00fcckt geht es bei quantitativen Methoden um Zahlen. [[:Category:Qualitative||''Qualitative'']] Methoden hingegen konzentrieren sich auf die menschlichen Dimensionen der beobachtbaren oder konzeptuellen Realit\u00e4t, wobei oft Beobachtungsdaten oder die Interpretation vorhandener Daten direkt mit der Theorie oder den Konzepten verkn\u00fcpft werden, was ein tiefes kontextuelles Verst\u00e4ndnis erm\u00f6glicht. Sowohl quantitative als auch qualitative Methoden sind ''[[Normativity of Methods|normativ]]''. Diese beiden im Allgemeinen unterschiedlichen Denkrichtungen werden in den letzten Jahrzehnten zunehmend miteinander verbunden, doch die Mehrheit der Forschung - und noch wichtiger, der Disziplinen - wird entweder von qualitativen oder quantitativen Methoden dominiert. Obwohl dies an sich vollkommen in Ordnung ist, gibt es einen tiefen ideologischen Graben zwischen diesen beiden Ans\u00e4tzen, und es wird viel dar\u00fcber geurteilt, welcher Ansatz g\u00fcltiger ist. Ich halte dies f\u00fcr irref\u00fchrend, wenn nicht gar falsch, und schlage stattdessen vor, je nach den angestrebten Erkenntnissen den geeigneten Ansatz zu w\u00e4hlen. Es gibt jedoch einen Vorbehalt: Ein Gro\u00dfteil des wissenschaftlichen Kanons der letzten Jahrzehnte wurde von der Forschung dominiert, die auf quantitativen Ans\u00e4tzen aufbaute. Vor allem in der qualitativen Forschung entstehen jedoch neue spannende Methoden. Da f\u00fcr die Probleme, mit denen wir derzeit konfrontiert sind, neuartige L\u00f6sungen erforderlich sind, erscheint es notwendig, dass sowohl der Umfang als auch der Anteil der qualitativen Forschung in Zukunft zunimmt. \n\n==== Deduktiv oder Induktiv ====\n[[File:deductiveinductive_danielmiessler.png|thumb|left|'''Deductive vs. Inductive Logik'''. Quelle: [https://danielmiessler.com/images/Screen-Shot-2016-07-15-at-9.57.07-AM.png Daniel Miessler]]] \n'''Methoden k\u00f6nnen weiter danach unterschieden werden, ob sie induktiv oder deduktiv sind'''. [[:Category:Deductive|''Deduktive'']] Argumentation baut auf Aussagen oder Theorien auf, die durch Beobachtung best\u00e4tigt werden oder durch Logik best\u00e4tigt werden k\u00f6nnen. Dies ist tief in der Tradition von [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon] verwurzelt und ist insbesondere in den Naturwissenschaften und in der Forschung, die sich quantitativer Methoden bedient, zu einer wichtigen Grundlinie geworden. [[:Category:Inductive|''Induktive'']] Methoden ziehen Schlussfolgerungen auf der Grundlage von Daten oder Beobachtungen. Viele argumentieren, dass diese Schlussfolgerungen nur Ann\u00e4herungen an eine wahrscheinliche Wahrheit sind. Die Ableitung des Wissens aus Daten und nicht aus der Theorie in der induktiven Forschung kann als Kontrapunkt zur hypothesengeleiteten deduktiven Argumentation gesehen werden, obwohl dies nicht immer der Fall ist. Zwar wird auch heute noch meist behauptet, Forschung sei eindeutig entweder induktiv oder deduktiv, doch handelt es sich dabei oft um einen Schwindel. Es gibt viele Leute, die gro\u00dfe Datens\u00e4tze verwenden und klare Hypothesen aufstellen. Ich denke, wenn diese Leute ehrlich w\u00e4ren, w\u00fcrde klar werden, dass sie die Daten analysiert haben, bis sie diese Hypothesen hatten, und dennoch ver\u00f6ffentlichen sie die Ergebnisse mit Hypothesen, die sie vorgeben, von Anfang an gehabt zu  haben. Das ist ein klarer Hinweis darauf, dass die wissenschaftliche Tradition sich an uns r\u00e4cht, die bis heute oft auf einer Kultur aufbaut, die eine Hypothesenbildung verlangt. Andererseits fordern andere Disziplinen ebenso dogmatisch einen induktiven Ansatz und lehnen jede Deduktion weitgehend ab. Ich werde hier nicht auf [[Big problems for later] die Vorz\u00fcge der induktiven Argumentation]] eingehen, sondern nur hervorheben, dass ihre Offenheit und ihr Drang, nicht deterministisch zu sein, einen wichtigen und relevanten Kontrapunkt zur deduktiven Forschung darstellen. Genau wie im Bereich der quantitativen und qualitativen Methoden gibt es einen tiefen Graben zwischen induktiver und deduktiver Forschung. Eine M\u00f6glichkeit, diesen Graben in einem ehrlichen und wertvollen Ansatz zu \u00fcberwinden, w\u00e4re das abduktive Denken, das aus Iterationen zwischen Daten und Theorie besteht. Ich glaube, dass ein gro\u00dfer Teil der modernen Forschung tats\u00e4chlich so gestaltet ist und so durchgef\u00fchrt wird, dass ein st\u00e4ndiger Austausch zwischen Daten und Theorie stattfindet. Angesichts der Komplexit\u00e4t der modernen Forschung macht dies auch Sinn: Einerseits kann nicht jede Forschung in kleine \"hypothesenf\u00e4hige\" Einheiten zerlegt werden, und andererseits ist bereits so viel Wissen vorhanden, weshalb viel Forschung auf fr\u00fcheren Theorien und Kenntnissen aufbaut. Dennoch m\u00fcssen wir heute und auf Jahre hinaus anerkennen, dass ein Gro\u00dfteil der bisherigen modernen Forschung in die traditionell etablierte induktive oder deduktive Kategorie fallen wird. Es ist daher nicht \u00fcberraschend, dass [[Different_paths_to_knowledge#Critical_Theory_.26_Bias|kritische Theorie]] und andere Ans\u00e4tze diese starren Kriterien und das Wissen, das sie hervorbringen, in Zweifel ziehen. Nichtsdestotrotz stellen sie eine wichtige Kategorisierung f\u00fcr den gegenw\u00e4rtigen Stand der Technik in vielen Bereichen der Wissenschaft dar und verdienen daher kritische Aufmerksamkeit, zumindest weil viele Methoden entweder den einen oder den anderen Ansatz implizieren. \n\n==== R\u00e4umliche und zeitliche Skalen ====\n'''Verschiedene wissenschaftliche Methoden konzentrieren sich auf bestimmte r\u00e4umliche Skalen oder sind sogar auf diese beschr\u00e4nkt, andere wiederum k\u00f6nnen sich \u00fcber mehrere Skalen erstrecken'''. Einige Methoden werden auf globaler Ebene operationalisiert, w\u00e4hrend andere sich auf das Individuum konzentrieren. Beispielsweise kann eine statistische Korrelationsanalyse auf Daten \u00fcber Einzelpersonen angewandt werden, die in einer Umfrage erhoben wurden, aber auch auf globale Wirtschaftsdaten angewandt werden. Eine ''globale'' r\u00e4umliche Skala der Stichprobenziehung wird durch Daten definiert, die den gesamten Globus oder einen nicht absichtlich ausgew\u00e4hlten Teil des Globus abdecken. Solche globalen Analysen sind von zunehmender Bedeutung, da sie es uns erm\u00f6glichen, ein wirklich einheitliches Verst\u00e4ndnis der gesellschaftlichen und nat\u00fcrlichen Ph\u00e4nomene zu erhalten, die den globalen Ma\u00dfstab betreffen. Eine \"individuelle\" Stichprobenskala ist eine Skala, die sich auf einzelne Objekte konzentriert, zu denen Menschen und andere Lebewesen geh\u00f6ren. Die individuelle Skala kann uns einen tiefen Einblick in Lebewesen geben, mit einem ausgepr\u00e4gten Schwerpunkt auf der Forschung am Menschen. Da unsere Vorstellungen vom Menschen vielf\u00e4ltig sind, finden sich hier auch viele verschiedene Ans\u00e4tze, und viele Zweige der Wissenschaft konzentrieren sich auf das Individuum. Diese Skala hat in den letzten Jahrzehnten sicherlich an Bedeutung gewonnen, und wir fangen erst an, die Vielfalt des Wissens zu verstehen, die eine individuelle Skala zu bieten hat. \n\nDazwischen gibt es eine riesige L\u00fccke, die in verschiedenen Bereichen der Wissenschaft unterschiedliche Namen haben kann - \"Landschaften\", \"Systeme\", \"Institutionen\", \"Einzugsgebiete\" und andere. Verschiedene Wissenschaftszweige konzentrieren sich auf unterschiedliche mittlere Skalen, doch wird dies oft als eine der am h\u00e4ufigsten untersuchten Skalen angesehen, da sie eine gro\u00dfe Vielfalt an Daten erzeugen kann. Ich nenne sie die \"System-Skala\". Eine \"System\"-Skala der Stichprobenziehung ist definiert durch jede Skala, die mehrere einzelne Objekte enth\u00e4lt, die miteinander interagieren oder in eine breitere Matrixumgebung eingebettet sind. Im Hinblick auf unser Systemverst\u00e4ndnis ist dies eine relevante methodische Skala, da sie eine gro\u00dfe Vielfalt empirischer Daten erzeugen kann, und auf dieser Skala werden viele verschiedene Methoden angewandt. Dar\u00fcber hinaus ist sie auch im Hinblick auf die Normativit\u00e4t eine recht relevante Skala, weil sie Wissen \u00fcber den Wandel von Systemen erzeugen kann, das mit dem Ziel operationalisiert werden kann, diesen Wandel zu f\u00f6rdern oder zu behindern. Daher kann diese Systemskala in Bezug auf Wissen, das sich auf die Politik konzentriert, recht relevant sein. Es dauert sicherlich l\u00e4nger, den Globus zu ver\u00e4ndern, und das Verst\u00e4ndnis des Wandels bei den Menschen hat viele tiefere theoretische Grundlagen und Rahmenbedingungen. \n\n'''Das letzte Kriterium, um Methoden zu konzeptualisieren, ist die Zeit.''' Die \u00fcberwiegende Mehrheit der empirischen Studien befasst sich mit einem Zeitabschnitt (der ''Gegenwart''), und dies macht die Studien, die versuchen, die Vergangenheit zu verstehen oder den k\u00fchnen Versuch unternehmen, mehr \u00fcber die Zukunft zu erfahren, umso wertvoller. Die Analyse der ''Vergangenheit'' kann sehr vielf\u00e4ltig sein, von der quantitativen L\u00e4ngsschnittdatenanalyse bis zur tiefen [[Hermeneutics|hermeneutischen]] Textanalyse und Interpretation l\u00e4ngst vergangener Autor*innen. Es gibt viele Fenster in die Vergangenheit, und wir fangen erst an, viele der faszinierendsten Perspektiven zu entwirren, da viele der verf\u00fcgbaren Sch\u00e4tze erst jetzt von der Wissenschaft erforscht werden. Dennoch gibt es eine lange Tradition in den Geschichts- und Kulturwissenschaften und vielen anderen Wissenschaftszweigen, die beweisen, wie viel wir aus der Vergangenheit lernen k\u00f6nnen. Mit dem Einsatz von Vorhersagemodellen, der Entwicklung von Szenarien \u00fcber die ''Zukunft'' und vielen anderen verf\u00fcgbaren Ans\u00e4tzen versucht die Wissenschaft auch, mehr Wissen \u00fcber das, was werden k\u00f6nnte, zu generieren. Das Konzept der Zukunft selbst ist ein menschliches Privileg, auf das sich die Wissenschaft aufgrund der vielen Herausforderungen, vor denen wir stehen, zunehmend konzentriert. In den letzten Jahrzehnten sind neue Ans\u00e4tze entstanden, und unser Wissen \u00fcber die Zukunft w\u00e4chst mit Sicherheit. Bis heute m\u00fcssen wir feststellen, dass diese Studien noch viel zu selten sind, und ein gro\u00dfer Teil unseres empirischen Wissens baut auf Momentaufnahmen eines bestimmten Zeitpunkts auf. \n\n'''Basierend auf diesen \u00dcberlegungen schlage ich vor, dass Sie die folgenden Designkriterien im Hinterkopf behalten, wenn Sie sich einer spezifischen wissenschaftlichen Methode ann\u00e4hern:''' \n* [[:Category:Quantitative|Quantitativ]] - [[:Category:Qualitative|Qualitativ]]\n* [[:Category:Inductive|Induktiv]] - [[:Category:Deductive|Deduktiv]]\n* Spatial scale: [[:Category:Individual|Individuum]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scale: [[:Category:Past|Vergangenheit]] - [[:Category:Present|Gegenwart]] - [[:Category:Future|Zukunft]]\n\nVersuchen Sie zu verstehen, wie Methoden im Hinblick auf diese Kriterien operationalisiert werden k\u00f6nnen. Einige Methoden kreuzen nur eine Option pro Kriterium an, und andere k\u00f6nnen sich \u00fcber viele verschiedene Kombinationen von Kriterien erstrecken. Aus der [[Methods|gro\u00dfen Vielfalt der Methoden]] gibt Ihnen diese Kategorisierung eine erste Positionierung jeder spezifischen Methode. Es gibt buchst\u00e4blich Hunderte von Methoden. Anstatt sie alle zu verstehen, versuchen Sie, die Kriterien zu verstehen, die sie vereinen und differenzieren. Nur durch das Verst\u00e4ndnis dieser methodologischen Gestaltungskriterien k\u00f6nnen Sie die Methode ausw\u00e4hlen, die das m\u00f6glicherweise ben\u00f6tigte Wissen schafft. \n----\n[[Category: Normativity of Methods]]"
                    },
                    "sha1": "hdndv5zn05qehyvt8whjiws6oqwowyk"
                }
            },
            {
                "title": "Design Criteria of Methods in Sustainability Science",
                "ns": "0",
                "id": "451",
                "revision": {
                    "id": "6356",
                    "parentid": "5856",
                    "timestamp": "2021-09-10T11:58:53Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11777",
                        "#text": "'''Note:''' This entry focuses especially on Methods of Sustainability Science. For a more general conceptual view on Methods, please refer to the entry on the [[Design Criteria of Methods]].\n\n'''In short:''' In this entry, you will find out more about how we can distinguish and design our methodological approaches in Sustainability Science.\n\n== Design Criteria in Sustainability Science - ''Why?'' ==\nThe [[Design Criteria of Methods|design criteria of methods]] that I propose for all methods - quantitative vs. qualitative, inductive vs. deductive, spatial and temporal scales - are like the usual suspects of scientific methods. Within normal science, these design criteria are what most scientists may agree upon to be central for the current debate and development about methods. Consequently, '''it is important to know these ''normal science'' design criteria also when engaging with sustainability science.''' However, some arenas in science depart from the current paradigm of science - ''sensu strictu'' [https://plato.stanford.edu/entries/thomas-kuhn/#DeveScie Kuhn] - and this means also that they depart potentially from the design criteria of the normal sciences. \n\nThe knowledge science currently produces is not enough to solve the problems we currently face. In order to arrive at new solutions, we need to re-consider, adapt and innovative the current raster of methods in science. Many methods that are long established are valuable, and all scientific methods have their [[History of Methods|history and origin]], and thus are important. Nevertheless, it becomes more and more clear that in order to arrive at solutions for the problems we currently face, we need to consider if we need methodological innovation. In the past,  methodological innovation worked in one of these ways:\n# new methods were invented,\n# new combinations of methods were attempted,\n# and methods were re-designed to be applied in a novel context where they had never been used before. \nWhile all this is fascinating in itself, here I present a different approach towards an amendment of the methodological canon, for one simple reason: Developing the methodological canon of science takes experience in the application of scientific methods, and ideally also contextual experience in diverse methodological approaches. While I am personally deeply critical about scientific disciplines, this is a good argument that in order to become versatile in scientific methods, you may need to channel your development towards being versatile in scientific methods based on textbook knowledge. Normal sciences have textbooks to teach their methodological canon, and while I think we need to be critical of these methodological approaches, they can have a lot of value. If you ask why, the I would say simply, because you cannot re-apply methods in a different context or attempt recombinations of methods if you are not experienced in the application of methods. To this end, it is important to highlight that these methods only look at parts of reality, which is the main reason for being critical. [[Bias and Critical Thinking|Bias]] and the fact that accompanying [[Levels of Theory|paradigms and theoretical viewpoints]] of specific methods restrict the validity of specific methods makes me even more critical. Nevertheless, there is no alternative to building experience than through applying methods within active research. You need to get your hands dirty to get experience. Here, I propose that we start with the knowledge we want to produce, and the goal we aim at to produce this knowledge. If we want to empower stakeholders, we need to be aware which methods out of the existing canon might help us, and how we may need to combine these methods in order to produce the knowledge we aim at. Therefore, we present three types of design criteria that serve as a basis for reflection of what knowledge we want to produce.\n\n== Key Competencies in Sustainability ==\nIn 2011, Wiek et al. analyzed the prevalent literature and presented five Key Competencies that students of Sustainability Science should strive for. This is an excellent scheme to be reflexive about the [[Glossary|competencies]] you want to gain, and to get a better understanding on which competencies can be aimed at through specific methods. These competencies are as follows:\n* ''Systems Thinking'': Systems Thinking competency is the ability to analyze and understand [[Glossary|complex systems]] including the dynamics in the interrelation of their parts. Systems thinking integrates different domains (society, environment, economy, etc.) as well as different scales (from local to global). \n\n* ''Anticipatory'': Anticipatory competency describes the ability to develop realistic scenarios of future trajectories within complex systems, including positive (e.g. a carbon-neutral city) and negative (e.g. flooding stemming from climate change) developments. This may include rigorous concepts as well as convincing narratives and visions.\n\n* ''Normative'': Normative competency refers to the ability to evaluate, discuss and apply (sustainability) values. It is based on the acquisition of normative knowledge such as concepts of justice or equality.\n\n* ''Strategic'': In simple terms, this competence is about being able to \"get things done\". Strategic competency is the capability to develop and implement comprehensive strategies (i.e. interventions, projects, measures) that lead to sustainable future states across different societal domains (social, economic, ecologic, ...) and scales (local to global). It requires an intimate understanding of strategic concepts such as path dependencies, barriers and alliances as well as knowledge about viability, feasibility, effectiveness and efficiency of systemic interventions as well as potential of unintended consequences.\n\n* ''Interpersonal'': Interpersonal competence is the ability to motivate, enable, and facilitate collaborative and participatory sustainability research and problem solving. This capacity includes advanced skills in communicating, deliberating and negotiating, collaborating, [[Glossary|leadership]], pluralistic and trans-cultural thinking and empathy.\n\n[[File:SustainabilityCompetencies.png|750px|thumb|center|'''Key Competencies for Sustainability.''' Source: Wiek et al. 2011, p.206]]\nThe criteria from Wiek et al are outstanding in the capacity to serve as boundary object, since I do not see these categories as mutually exclusive, but instead strongly interwoven with each other. I can whole-heartedly recommend to return to these criteria then and again, and to reflect on yourself through the lense of these criteria.\n\n== Knowledge for action-oriented sustainability science ==\nAt the heart of Sustainability Science are, among other elements, the premise of intentionally developing practical and context-sensitive solutions to existent problems, as well as the implementation of cooperative research modes to do so jointly with societal actors, supporting social learning and capacity building in society. To this end, Caniglia et al. (2020) suggest three types of knowledge that should be developed and incorporated by Sustainability Science:\n[[File:Types of knowledge for action-oriented sustainability science.png|900px|thumb|center|'''Types of knowledge for action-oriented sustainability science.''' Source: Caniglia et al. 2020, p.4]]\n\nThis showcases that this knowledge - and more importantly - the perspective from a philosophy-of-science viewpoint is only starting to emerge, and much more work will be needed until our methodological canon and the knowledge we want to produce enable us to solve the problems we are facing, but also to create these solution in ways that are closer to a mode how we want to create these solutions. We may well be able to solve certain things, and to produce knowledge that can be seen as solutions. I would however argue, that it also matters how we create these solutions and how we create knowledge. Only if people are empowered and society and science work seamlessly together - with ethical restrictions and guidelines in place, of course - will we not only produce the knowledge needed, but we also produce it in a way how we should as scientists. '''Science is often disconnected and even arrogant, and building an educational system that is reflexive and interconnected will be maybe the largest challenge we face.''' This is why we give you these criteria here, because I think that we need to consider what further design criteria can be in order to enhance and diversify our conceptual thinking about the scientific methodological canon. Plurality on scientific methods will necessarily mean to evolve, and in this age of interconnectedness, our journey is only beginning.\n\n== Interaction with stakeholders == \nScientific methods can engage with non-scientific actors on diverse levels, depending on the extent of their involvement in the process of scientific inquiry. Interaction with stakeholder may be especially relevant in [[Transdisciplinarity|transdisciplinary]] research. \n<br/>'''Here, we refer to four levels of interaction:'''\n* ''Information'': Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation.\n* ''Consultation'': A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process.\n* ''Collaboration'': Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue.\n* ''Empowerment'': The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.\n\nYou can find more on these four categories in Brandt et al 2013, where a general introduction to the research landscape of transdisciplinary research in sustainability science is given. More will follow later on such approaches, and so much more still has to follow in science overall, since the declared distinction of science being in an ivory tower is only slowly crumbling. We need to question this [[Levels of Theory|paradigm]], and [[Questioning the status quo in methods|be critical of the status quo of normal science]]. More knowledge is needed, and especially, different knowledge.\n\n== References ==\n* Wiek et al. 2011. ''Key competencies in sustainability: a reference framework for academic program development''. Sustainability Science 6. 203-218.\n* Caniglia, G., Luederitz, C., von Wirth, T., Fazey, I., Mart\u00edn-L\u00f3pez, B., Hondrila, K., K\u00f6nig, A., von Wehrden, H., Sch\u00e4pke, N.A., Laubichler, M.D. and Lang, D.J., 2020. ''A pluralistic and integrated approach to action-oriented knowledge for sustainability.'' Nature Sustainability, pp.1-8.\n* Brandt, P., Ernst, A., Gralla, F., Luederitz, C., Lang, D.J., Newig, J., Reinert, F., Abson, D.J. and Von Wehrden, H., 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological economics, 92, pp.1-15.\n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz."
                    },
                    "sha1": "khtluxqdeusf9lcoachof85qvy7chrd"
                }
            },
            {
                "title": "Design Thinking",
                "ns": "0",
                "id": "311",
                "revision": {
                    "id": "5921",
                    "parentid": "5848",
                    "timestamp": "2021-06-30T08:28:21Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Rules */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8490",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n'''In short:''' Design Thinking is a process for creative problem solving and [[Glossary|innovation]] which focuses on understanding the target user and which differs from other creative problem-solving processes by developing and testing prototypes of potential solutions.\n<br>\n== What, Why & When ==\n* Design Thinking is a creative process in product or service design with a user-centered perspective. It helps empathise with others, understand their perspectives and develop solutions to their problems. This way, it prevents you from creating a solution when there is no problem.\n* Disclaimer: Design Thinking does not necessarily help you to compromise and balance different interests on the same resource e.g. clean air.\n* It is an iterative process of [[Glossary|brainstorming]], [[Persona Building|persona building]], prototyping and testing. A Design Thinking team should not be larger than five individuals, but several teams can work on the same problem at once. The length of the process is expandable (up to a week), but may also be squeezed into a 3-hour workshop <br>\n\n\n== Getting started ==\n==== Material ====\n[[File:Workshop Material.jpg|thumb|Workshop Material]]\nFirst, you have to decide under which conditions you want to host a Design Thinking session. What is the '''topic or the problem''' you want to work on? Which '''participants, perspectives and skills''' do you need to go through the process? It is recommendable to not work in larger groups than five, if you are more participants, you can split up in smaller teams and find out with what kind of solutions you come up with. Define a workshop facilitator who guides the process, and create an agenda. Find a room and organize the '''material and resources''' that you need (paper, pen, post-its, whiteboard etc.)<br> \nSecond, you need to communicate what you expect from the participants: Time, energy, attention. Try to make use that every participant is actually committed as one team member less is easier to work with than somebody dropping in and out.  \n\n==== The Process ====\n[[File:Design thinking.png|thumb|600px|Design Thinking Process]]\n=== Empathise ===\nIn the first phase of the process, you try to change your perspective from your own to the user that you want to focus on. Define your target/ user group and research about their background. You can make use of [[Persona Building|persona building]] to visualize your target group. A look into demographics can be of help to understand the specific situation of your target group. Maybe you'll also want to have a look at the [https://www.sinus-institut.de/sinus-milieus SINUS group] approach. Do all you can to understand the life that your target group is living. Your goal is to be able to think and feel like the person or group of persons you want to empathize with.\n\n=== Define ===\nIn the second step, you want to define your problem from the perspective of the user. What are the needs and wishes? What are their daily problems? Where do they need help?\n\n=== Ideate ===\nThis is the wild phase of Design Thinking. You throw all ideas you have into the discussion, for which you can use different methods such as [[Glossary|brainstorming]], brainwriting, a written discussion, who-writes-the-most-ideas-on-post-its and more. The goal is to broaden your view - no idea is too big or too far out of the box. Everything is allowed at this point, no matter how unrealistic it seems. Don't restrict yourself, this comes later. Try to challenge general [[Glossary|assumptions]] - this leads to the best, unforeseen solutions.\n\n=== Prototype ===\nNow you are allowed to criticise. The goal of this phase is to identify one solution out of the many that seems to be most promising, and design a prototype for it. Decide on the solution as a group, and weigh all your options. Why do you think this option seems to be the best? What makes it stand out? Why and how does it best address your target groups needs? The prototype can take any form or shape, it can be a drawing, a mock-up of an application, a [[Glossary|framework]], a staged situation - anything that helps you visualize and communicate your solution quickly to anyone. It should be understandable and not be too detailled yet.\n\n=== Test ===\nIn this final phase, you try to get feedback from the user group. The feedback should come from 'real' users - so you can call your colleagues, friends, ask people on the street or even reach out to people who might actually be your target group. You need to collect reactions on your prototype. After that, depending on the feedback, you iterate and get back to another earlier step in the process. If your users mirror you that your prototype is far from what is needed, you should start at understanding your user group again. If only little tweaks are missing, you can work on your prototype some more and test it again more quickly. As soon as their are only minor critical comments from your target group, you are ready to realize the solution.<br>\n\n==== Rules ====\n* '''[[Yes, and]]''': The rule of \"yes-and\" means, that you should try to build up on each others ideas. Even though you have the impression that the suggestion by your team member is not suitable, add something that makes it fitting<br>\n* '''Quantity first, quality second - first storm, then criticise''': Not everything that is brought up in the process will be the next big thing. Try to not judge ideas and collect everything at first. You can throw out unnecessary stuff later.<br>\n* '''Keep the target in mind''': As it happens fast that you get carried away by the brainstorm-thunder, try to get back to the problem description multiple times and check whether your solution fits to the problem and target group. This helps you to stay on track and adjust if necessary. <br>\n* '''Work as one group''': You might have the impression that your idea is better than the other one. Try to focus on one idea with the whole group and do not start to do your own thing. Use the attention of the whole group to progress. <br>\n* '''Be visual''': Depending on your preference, you can draw an idea, make a mindmap, write everything on post-its or use a written discussion - the most important aspect is:  write it down. Therefore, you will have a written documentation later and can get back to ideas that were brought up hours ago.<br>\n\n==== An example for sustainability science ====\nImagine you want to find solutions for different stakeholders on '''land use in the city of L\u00fcneburg'''. \n# At first, you would need to define the different needs of the stakeholder and cluster them into need groups, e.g. all stakeholders that are interested in climate change mitigation aspects. \n# Then, you would assign one team per need group and let them dive into the process. The process steps are still the same for all of the groups.<br>\n# After having finished the process, you can compare the different solutions. One group might have come up with a participation format of how to include a group that is not yet involved in a decision process. Another group might have developed a ranking scheme for decision criteria and so on. Now, you can see whether there are similarities between the different solutions and how they can be combined, transferred to another need group.<br>\n\n\n== Links & Further reading ==\n* https://hpi.de/en/school-of-design-thinking/design-thinking/background/design-thinking-principles.html <br>\n* Tips for organizing design thinking sessions: https://newkind.com/12-design-thinking-rules-from-david-burney/ <br>\n* Conference Paper on Design Thinking Methods: https://link.springer.com/chapter/10.1007/978-3-319-20886-2_2\n* Youtube Video about Design Thinking principles (very entrepreneurial): https://www.youtube.com/watch?v=gHGN6hs2gZY\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Alexa B\u00f6ckel."
                    },
                    "sha1": "q0vybmi9btgiv9i8fsufnw902v5v3ts"
                }
            },
            {
                "title": "Designing studies",
                "ns": "0",
                "id": "12",
                "revision": {
                    "id": "6377",
                    "parentid": "5005",
                    "timestamp": "2021-09-20T07:27:53Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Randomization */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4318",
                        "#text": "This entry revolves around key things to know when designing a scientific study.\n\nWORK IN PROGRESS\n\n=== Before designing the experiment ===\nPlease read the entry on [[Experiments and Hypothesis Building]] first.\n<br>\nThe principle states that one should not make more [[Glossary|assumptions]] than the minimum needed.\n\n[[File:SampleSizeStudyDesign.png|thumb|right|Here you can see the minimum needed sample sizes (in brackets) for several statistical measures such as mean or range. The other figure points at the minimum amount of data points needed to perform the test.]]\n\n<strong>Scientific results are:</strong>\n# Objective\n# Reproducible\n# Transferable\n\n=== Sample size ===\n[[Why statistics matters#Occam's razor|Occam's razor]] the hell out of it!\n\n==== Population ====\n\n=== P-value ===\nIn statistical hypothesis testing, the [https://www.youtube.com/watch?v=-MKT3yLDkqk p-value] or probability value is the probability of obtaining the test results as extreme as the actual results, assuming that the null hypothesis is correct. In essence, the p-value is the percentage probability the observed results occurred by chance.\n\nProbability is one of the most important concepts in modern statistics. The question whether a relation between two variables is purely by chance, or following a pattern with a certain probability is the basis of all probability statistics (surprise!). In the case of linear relation, another quantification is of central relevance, namely the question how much variance is explained by the model. Between these two numbers -the amount of variance explained by a linear model, and the fact that two variables are not randomly related- are related at least to some amount. \n\nIf a model is highly significant, it typically shows a high r<sup>2</sup> value. If a model is marginally significant, then the r<sup>2</sup> value is typically also low. This relation is however also influenced by the sample size. Linear models and the related p-value describing the model are highly sensitive to sample size. You need at least a handful of points to get a significant relation, while the r<sup>2</sup> value in this same small sample sized model may be already high. Hence the relation between sample size, r<sup>2</sup> and p-value is central to understand how meaningful models are. \n\n[[File:Lady Tasting Tea.jpg|thumb|Lady tasting tea]]\n\nIn Fisher's example of a [https://www.youtube.com/watch?v=lgs7d5saFFc lady tasting tea], a lady claimed to be able to tell whether tea or milk was added first to a cup. Fisher gave her 8 cups, 4 of each variety, in random order. One could then ask what the probability was for her getting the specific number of cups she identified correct, but just by chance. Using the combination formula, where n (total of cups) = 8 and k (cups chosen) = 4, there are 8!/4!(8-4)! = 70 possible combinations. In this scenario, the lady would have a 1.42% chance of correctly guessing the contents of 4 out of 8 cups. If the lady is able to consistently identify the contents of the cups, one could say her results are statistically significant.\n\n=== Block effects ===\nIn statistical design theory, blocking is the practice of separating experimental units into similar, separate groups (i.e. \"blocks\"). A blocking group allows for greater accuracy in the results achieved by removing previously unaccounted for variables. A prime example is blocking an experiment based on male or female sex.\n\n[[File:Block Experiments.jpg|thumb|Designing an experiment using block effects.]]\n\nWithin each block, multiple treatments can be administered to each experimental unit. A minimum of two treatments are necessary, one of which is the control where, in most cased, \"nothing\" or a placebo is the treatment.\n\nReplicates of the treatments are then made to ensure results are statistically significant and not due to random chance.\n\n=== Randomization ===\nYou can find this paragraph in the section about [[Field_experiments#Randomisation|Field Experiments]].\n\n===External links===\n====Articles====\n\n====Videos====\n[https://www.youtube.com/watch?v=-MKT3yLDkqk P-Value]: What it is and what it tells us\n\n[https://www.youtube.com/watch?v=lgs7d5saFFc Lady Tasting Tea]: The story\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "3b338mdo2lbtot09bowjubrsar4kv6i"
                }
            },
            {
                "title": "Different paths to knowledge",
                "ns": "0",
                "id": "396",
                "revision": {
                    "id": "5953",
                    "parentid": "5942",
                    "timestamp": "2021-06-30T19:49:14Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Mixed Methods */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14857",
                        "#text": "The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods. \n\n'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research. \n\n* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.\n\n'''Inductive vs. deductive'''\nSome branches of science try to verify or falsify hypotheses, while other branches of science are open towards the knowledge being created primarily from the data. Hence the difference between a method that derives [[Glossary|theory]] from data, or one that tests a theory with data, is often exclusive to specific branches of science. To this end, out of the larger availability of data and the already existing knowledge we built on so far, there is a third way called abductive reasoning. This approach links the strengths of both [[Glossary|induction]] and [[Glossary|deduction]] and is certainly much closer to the way how much of modern research is actually conducted. \n\n'''Scales'''\nCertain scientific methods can transcend spatial and temporal scales, while others are rather exclusive to a specific partial or temporal scale. While again this does not make one method better than another, it is certainly relevant since certain disciplines almost focus exclusively on specific parts of scales. For instance, psychology or population ecology are mostly preoccupied with the individual, while macro-economics widely work on a global scale. Regarding time there is an ever increasing wealth of past information, and a growing interest in knowledge about the future. This presents a shift from a time when most research focused on the presence. \n\n* [[Design Criteria of Methods]]\n* [[Design Criteria of Methods (German)]]\n\n=== Critical Theory & Bias ===\n'''Critical theory'''\nThe rise of empiricism and many other developments of society created critical theory, which questioned the scientific [[Glossary|paradigm]], the governance of systems as well as democracy, and ultimately the norms and truths not only of society, but more importantly of science. This paved the road to a new form of scientific practice, which can be deeply normative, and ultimately even transformative.  \n\n'''The pragmatism of [[Glossary|bias]]'''\nCritical theory raised the alarm to question empirical inquiry, leading to an emerging recognition of bias across many different branches of science. With a bias being, broadly speaking, a tendency for or against a specific construct (cultural group, social group etc.), various different forms of bias may flaw our recognition, analysis or interpretation, and many forms of bias are often deeply contextual, highlighting the presence or dominance of constructed groups or knowledge. \n\n'''Limitations in science'''\nRooted in critical theory, and with a clear recognition of bias, science(s) need to transform into a reflexive, inclusive and solution-oriented domain that creates knowledge jointly with and in service of society. The current scientific paradigms are hence strongly questioned, reflecting the need for new societal paradigms. \n\n* [[Bias and Critical Thinking]]\n* [[Bias and Critical Thinking (German)]]\n\n=== Experiment & Hypothesis ===\n'''The scientific method?'''\nThe testing of a [[Glossary|hypothesis]] was a breakthrough in scientific thinking. Another breakthrough came with Francis Bacon who proclaimed the importance of observation to derive conclusions. This paved the road for repeated observation under conditions of manipulation, which in consequence led to carefully planned systematic methodological designs. \n\n'''Forming hypotheses'''\nOften based on previous knowledge or distinct higher laws or assumptions, the formulation of hypotheses became an important step towards systematic inquiry and carefully designed experiments that still constitute the baseline of modern medicine, psychology, ecology and many other fields. Understanding the formulation of hypotheses and how they can be falsified or confirmed is central for large parts of science. Hypotheses can be tested, are ideally parsimonious - thus build an existing knowledge - and the results should be reproducible and transferable. \n\n'''Limitations of hypothesis'''\nThese criteria of hypotheses showcase that despite allowing for a systematic and - some would say - \u2018causal\u2019 form of knowledge, hypotheses are rigid at best, and offer a rather static worldview at their worst. Theories explored in hypothesis testing should be able to match the structures of experiments. Therefore, the underlying data is constructed, which limits the possibilities of this knowledge production approach.\n\n* [[Experiments and Hypothesis Testing]]\n* [[Experiments and Hypothesis Testing (German)]]\n\n=== Causality & Correlation ===\n'''Defining causality'''\nBuilding on the criteria from David Hume, we define causality through temporal links (\"if this - then this\"), as well as through similarities and dissimilarities. If A and B cause C, then there must be some characteristic that makes A and B similar, and this similarity causes C. If A causes C, but B does not cause C, then there must be a dissimilarity between A and B. Causal links can be clearly defined, and it is our responsibility as scientists to build on this understanding, and understand its limitations. \n\n'''Understanding correlations''' Correlations statistically test the relation between two continuous variables. A relation that - following probability - is not a coincidence but from a statistical standpoint meaningful, can be called a significant correlation. \n\n'''The difference between causality and correlation'''\nWith increasing statistical analysis being conducted, we sometimes may find significant correlations that are non-causal. Disentangling causal form correlative relations is deeply normative and needs to acknowledge that we often build science based on deeply constructed ontologies. \n\n* [[Causality and correlation]]\n* [[Causality and correlation (German)]]\n\n=== Scientific methods and societal paradigms ===\n\n'''How scientific methods drive societal paradigms'''\nScientific methods partly dealt with the necessary approaches to enable a flourishing yet often abusive trade, work out solutions for mechanisation, and develop modern agriculture based on systematic inquiry. Modern medicine - or better medical research - is one example of a framing of knowledge production on and around scientifc method.\n\n'''How societal paradigms drive scientific methods'''\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often [[Glossary|funding]] science as a means to an end. Consequently did science often act morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational change. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n'''The grand abduction of science and society'''\nSince the antique, science and society have been in a continuous spiralling movement around each other. While scientific methods are shaped by their time, the times are also shaped by scientific methodology. What is however necessary is the close and mutual interaction between empirical inquiry and the question how we ought to act based on our growing knowledge.\n\n* [[Scientific methods and societal paradigms]]\n* [[Scientific methods and societal paradigms (German)]]\n\n=== Emergence of agency ===\n'''[[Glossary|Agency]], complexity and emergence'''\nSystem thinking gained prominence during the last century, allowing to take interactions and interdependencies into account when investigating a system. To this end, a \u2018system\u2019 stands for something that is in the focus of the investigation, such as a catchment area, a business enterprise, a social institution, or a population of wasps. Such systems show signs of [[Glossary|complexity]], which means that the interactions of smaller entities in the system may be unpredictable or chaotic, which makes systems more than the sum of their parts. Under this definition, solving problems within a system is often the greatest challenge: while the [[Glossary|system dynamics]] may be understandable, the solutions for an emerging problem are not instantly available. Consequently, solutions for complex problems emerge, and this demands a new line of thinking about systems.\n\n* [[Agency, Complexity and Emergence]]\n* [[Agency, Complexity and Emergence (German)]]\n\n=== Data and methods ===\n'''A new age of data'''\nThe increasing availability of data offers many new possibilities, and the emergence of new ways of getting data through the internet poses not only opportunities, but also - among others - ethical challenges. All the while, ever more diversity of data becomes available to people, leading to an even larger wealth of qualitative data. Again, this poses many ethical questions, and imposes an all new agenda onto many methodological approaches, including data security, picture rights, normative interpretations and even culture wars. \n\n'''The limitations of technology'''\nWhile technology has surely changed the way we live our lives, it has certainly also changed the way we do science. We should however recognise that technology cannot produce knowledge, and therefore can only be a means to an end, but not an end in itself.\n\n'''A way forward for methods'''\nMethods need to acknowledge the increasing diversity but also the new challenges that emerge from the exponential growth of science. Interactions between different disciplines are strongly increasing, and the core goal for science will be how to facilitate this through suitable [[Glossary|communication]] and additional resources. \n\n* [[To Rule And To Measure]]\n* [[To Rule And To Measure (German)]]\n* [[Data formats]]\n* [[Data formats (German)]]\n\n=== Mixed Methods ===\n'''Why combine methods?'''\nThe new challenges we face demand the production of new knowledge to approximate solutions. Scientific collaboration is therefore necessary in order to open up domains that were previously sealed and oftentimes arrogant.\n\n'''How to combine methods'''\nMethods are often characterized by a specific language, which is why a lot of time needs to be invested into understanding each other. In addition, experts in one method are often deeply invested in their specific focus, which is why interdisciplinary collaboration is mainly built on [[Glossary|trust]]. \n\n'''How not to combine methods'''\nMethods are not like cooking recipes that anyone with the right recipe can unlock. Instead, methods evolve, just as the harmonisation of methods evolves. A mixed methods approach is thus not like a mere recipe, but should be approached as something new, and only time will tell how we may become more experienced and systematic in the combination of different methods.\n\n* [[Mixed Methods]]\n* [[Mixed Methods (German)]]\n\n=== Methods of transdisciplinary research ===\n'''Shifting paradigms in science'''\nFor a long time, scientific disciplines existed alongside each other and separated from the rest of society, using the world as an object of inquiry and gathering data all the while. However, the state of our world does not allow for such separation anymore. We need a new collaboration within science and between science and society in order to find solutions to urgent challenges.\n\n'''A new contract between science and society'''\nTransdisciplinary research is built on the premise of scientific and non-scientific actors from diverse backgrounds jointly framing, understanding and solving societally relevant problems. [[Glossary|Transdisciplinarity]] is reflexive, participatory and holistic and able to develop mutually beneficial approaches to complex and normative issues.\n\n'''Learning from and with each other'''\nDespite the rather recent emergence of transdisciplinary research, a lot has been developed in terms of appropriate research modes, methods and surroundings. Examples of this are [[Visioning & Backcasting|Visioning]], Scenario Planning and Living Labs. More is yet to come, but the foundations have been set.\n\n* [[Transdisciplinarity]]\n* [[Transdisciplinarity (German)]]\n* [[Visioning & Backcasting]]\n* [[Scenario Planning]]\n* [[Living Labs & Real World Laboratories]]\n----\n[[Category: Courses]]"
                    },
                    "sha1": "pmyln6kwpevykux8kpcdgtlo6by50r1"
                }
            },
            {
                "title": "Digital Energizers",
                "ns": "0",
                "id": "459",
                "revision": {
                    "id": "3251",
                    "parentid": "2974",
                    "timestamp": "2020-11-04T10:19:42Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2386",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]\n|}'''\n\n== What, Why & When? ==\nEver been in a 3 hour video call and had the burning desire to coffee-break every 30 minutes? Despair no longer! Below is our hand-picked (and partly self-invented) list of digital energizers that may spice up your online meetings and free you from your caffeine addiction. \n\n== Goals ==\n* Keep energy high in digital meetings.\n* Increase fun online.\n\n== Getting Started ==\nHere's the list. You obviously don't need to do all of them in one meeting, but may pick according to your own judgement.\n\n==== \ud83e\udd38\u200d\u2640\ufe0f Jumping Jacks ====\nHave everyone do 5 Jumping Jacks\n\n==== \ud83d\udd22 Do a digital headcount ==== \nDo a headcount (German: \"Durchz\u00e4hlen\") where the same number isn't allowed to be mentioned twice. If the same number is said more than once, start at 1 again.\n\n==== \ud83c\udf99 Sing together (mic's muted) ==== \nPlay a karaoke song (you can just use a \"lyrics\" video on youtube [https://www.youtube.com/watch?v=o6piTG5EdhQ such as this one]). Everyone mute's their mic and sings along. \n\n==== \ud83e\uddfe Telling a story together ==== \nEveryone says a few words or sentences and hands off to another person which has to continue the story.\n\n==== \ud83e\udd20 Model your favorite emoticons ==== \nTry to form your favorite emoji with your face, hands and body. Others can guess which it is.\n\n==== \ud83c\udfb0 Play a mini-game together ==== \nThere are some games which you can play online together easily, such as [https://play.typeracer.com/ Typeracer] \ud83d\ude95. Maybe you'll find others!\n\n==== \ud83e\udd57 Elevator-Pitch your last meal ==== \nHave everyone pitch their last meal so as to convince everyone why that was the best meal ever.\n\n== Links & Further Reading ==\n\n__NOTOC__\n[[Category:Skills_and_Tools]]\n\n[[Category:Collaborative Tools]]\n\n[[Category:Team Size 2-10|Team Size 2-10]]\n\n[[Category:Team Size 11-30]]\n\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "tgutkbd96bk2em4067fgcpsutm8ja34"
                }
            },
            {
                "title": "Digital Workshop Facilitation",
                "ns": "0",
                "id": "472",
                "revision": {
                    "id": "5061",
                    "parentid": "3487",
                    "timestamp": "2021-04-09T15:12:20Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Have clear communication and moderation rules */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7916",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThis article will provide you with ideas helpful to preparing and carrying out digital workshops or online sessions in general. It is especially meant to prevent \"online fatigue\" and share best practices that make online communication less error-prone and exhausting.\n\n== Goals ==\nProvide an overview of methods, tools and approaches useful to organizing online workshops with multiple participants.\n\n== The Tips ==\n[[File:Communication2.jpg|thumb|Photo by Volodymyr Hryshchenko on Unsplash]]\n=== Communication \ud83d\udcac ===\n==== Get the basics right ====\nJust as in the analogue world, you should stick to the basics that make meetings effective and organized. We think it is even more important in the digital realm:\n\n# Have a clear agenda and goals for the meeting\n# Time yourself. You can use tools to support that, e.g. [https://pomodoro-tracker.com/ Pomodoro Tracker]\n# Clarify roles (Who is moderating? Who is writing protocol?)\n\n==== Have clear [[Glossary|communication]] and moderation rules ====\nAs there are so many channels online, you will need to establish some ground rules regarding what to communicate where and when. You can think about these prompts:\n\n* How are questions being asked? In the chat? Verbally? Raising a virtual hand first?\n* What's going to be the chat's function?\n* How is agreement being signalled (e.g. via chat, a separate tool, non-verbal-communication-features)?\n* How can participants signal that they need a break?\n* Who is writing the protocol?\n* For longer workshops, you can start by collaboratively writing a Code of Conduct to which everyone agrees in an Etherpad.\n\n==== Create a feeling of personality and proximity ====\nWe only ever see our heads and hear our voices, and it is hard to chitchat online - it can be difficult to get to know each other. Here's some suggestions to alleviate the issue:\n\n\ud83d\udcf9 Turn on cameras! If webcams are missing, suggest uploading a profile picture.\n\n\ud83d\udd4a Have everyone prepare a \"tweet\" (140 / 280 characters) about themselves which can be read out during an initial introduction round.\n\n\ud83c\udf4e Let people introduce themselves with one or multiple questions such as \n* ''\"If you were a fruit/vegetable/animal/..., which would you be and why?\"'' or \n* ''\"What's the first thing you're going to do when the CoViD-19 lockdown is officially over?\"''\n\n\ud83d\udc49 Do a [[Check In|Check-In]] / Check-Out\n\n==== Ensure participation possibilities ====\nWorking with digital tools can be great, but it can also be exclusive if people lack the experience or technical requirements to participate. If you are using tools, ensure that everyone has access and knows how to use it.\n\n==== Get Live-Feedback ====\nAs a moderator, it can be hard to estimate how your group is feeling, especially when you're not in the same room. Try to keep in mind asking for your groups energy level. \n\nHere's a few suggestiongs on how you can do that\n\n* Have people raise their hands on the left side of the screen, indicating how high their energy level currenty is\n* Do a poll on it (see section \"Polls & Quizzes\" below)\n* Ask people to give a thumbs-up or -down\n\n<br/>\n<br/>\n=== Interactivity & Variety \u2728 ===\n''There are several opportunities to create interactivity online. Here are our suggestions:''\n==== Use Digital Energizers ====\n[[File:Interaction.jpg|thumb|Photo by Brooke Cagle on Unsplash]]\nEnergizers are very common in the analogue world, and they are sometimes hard to reproduce in the digital. Here are our suggestions on what you can do: [[Digital Energizers]]\n==== Facilitate interaction between participants ====\nSpeaking in nerd terms, online meetings are often \"one-to-many\" communication, where one person is broadcasting to everyone else. Encourage people to talk to one another! You can use break-outs (see section below) or have participants pick other participants to talk next.\n==== Incorporate polls & quizzes ====\nPolls are a great way to get feedback from your audience. There are several ways and different tools to do this. We can recommend e.g. [https://www.mentimeter.com/ Mentimeter].\n\n==== Use Break-Out-Sessions ====\nBreak-Out-Sessions are a great way to dissolve your group into smaller discussion groups, work out a specific question or problem and then come together to discuss in the larger group. Unfortunately, this is, as far as we know, restricted to the software [https://zoom.us/ zoom] at the moment.\n\nHere's some things we find important for using them:\n\n\ud83d\udce3 Communicate clearly a) how much time is to be assigned for the break-out-sessions and b) what the result of the break-out should be and how it should be presented.\n\n\u23f2 Give participants time to organize themselves. They might see each other for the first time.\n\n\ud83d\ude08 Suggest roles for the sessions and encourage participants to distribute those among themselves. (''Yes, that's meant to be a [https://en.wikipedia.org/wiki/Devil%27s_advocate Devil's Advocate]'')\n\n==== Use different types of media ====\nListening to one single person for an elongated timespan can become strenuous no matter his or her rhetorical sophistication. We suggest you use different types of media to spark interest and keep thoughts flowing, including but not limited to videos or music.\n\n==== Use real-time collaboration tools ====\nWith the absence of physical white-/brownboards and paper, real-time collaboration can be tricky. There are however a few tools that can get the job done. Here are some of our suggestions in ascending sophistication:\n\n# The chat of your video conferencing tool is a good place to start.\n# Collaborative writing tools such as '''Etherpad''' or '''Google Docs'''\n# '''Simple online whiteboards'''. Some video conferencing tools have them integrated (e.g. zoom), but there are also stand-alone alternatives with a low entry threshhold such as [https://awwapp.com/ AWW App]\n# '''Full-blown real-time collaboration tools'''. They usually need some getting used to and have a learning curve, but the potential is equally bigger. We tried [https://miro.com/ Miro] and approve (they also offer free education accounts [https://miro.com/education/ here]).\n\n<br/>\n<br/>\n=== Retaining Results \ud83e\udde0 ===\nRetaining results is one of the most important yet overlooked parts of workshops that can get lost in the heat of the moment. Here's some suggestions on how to not go down that road:\n[[File:Memory.jpg|thumb|Photo by Fredy Jacob on Unsplash]]\n\n* Make sure you '''have a slot in the agenda''' for reflection and wrap-up\n* Have a common file share or workspace to which all participants have access\n* Do the equivalent of a photo protocoll in a PowerPoint (or something similar)\n* Leave some room for open questions. Remember that it is important to give this at least a few minutes of time as questions have to start building up. You can play music in the background if you can't stand the silence.\n* Do a poll at the end: What did people learn? What did you get out of the event?\n* Don't forget to brush up the results and provide them to your participants!\n\n== Links & Further Reading == \n[http://cdn2.hubspot.net/hubfs/3071166/The%20Definitive%20Guide%20To%20Facilitating%20Remote%20Workshops%20(V1.1).pdf The Definitive Guide to Facilitating Remote Workshops]\n\n[[Category:Skills_and_Tools]]\n\n[[Category:Collaborative Tools]]\n\n[[Category:Personal Skills]]\n\n[[Category:Team Size 2-10|Team Size 2-10]]\n\n[[Category:Team Size 11-30]]\n\n[[Category:Team Size 30+]]"
                    },
                    "sha1": "d70uicjmu2saz3ttnfde467vvmgkds4"
                }
            },
            {
                "title": "Disney Method",
                "ns": "0",
                "id": "314",
                "revision": {
                    "id": "5048",
                    "parentid": "3252",
                    "timestamp": "2021-04-09T14:34:40Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4448",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Disney Method is a fairly simple (group) [[Glossary|brainstorming]] technique''' that revolves around the application of different perspectives to any given topic. One person or a group comes up with ideas, then envisions their implementation and finally reflects upon their feasibility in a circular process. The Disney Method may be used to come up with ideas for projects or products, to solve problems and conflicts, to develop strategies and to make decisions. The method was invented by Walt Disney who thought of a movie not only as a director, but also as an audience member and a producer to come up with the best possible result.\n\n== Goals ==\n* Productive brainstorming\n* Understanding for other perspectives\n* Strong team spirit \n\n== Getting started ==\nThe Disney method process is circular. A group of people (ideally five or six) is split into three different roles: the Dreamers, the Realists, the Critics. \n\n[[File:Disney Method.png|450px|thumb|right|The Disney Method process]]\n\nThe '''Dreamers'''...\n* try to come up with new ideas\n* are creative and imaginative and do not set themselves any limits\n* everything is possible!\n* Guiding questions: ''Which ideas come to mind? What would be an ideal solution to the problem?''\n\nThe '''Realists'''...\n* think about what needs to be done to implement the ideas\n* are practical and realistic\n* Guiding Questions: ''How does the idea feel? How could it be implemented? Who should do it and at what cost?''\n\nThe '''Critics'''...\n* look at the idea objectively and try to identify crucial mistakes\n* are critical, but constructive - they do not want to destroy the ideas, but improve them constructively.\n* Guiding Questions: ''What was neglected by the Dreamers and Realists? What can be improved, what will not work? Which risks exist?''\n\nEach role receives a specific area within a room, or even dedicated rooms or locations, that may also be decorated according to the respective role. '''The process starts with the Dreamers, who then pass on their ideas to the Realists, who pass their thoughts on to the Critics.''' Each phase should be approx. 20 minutes long, and each phase is equivalently important. \nAfter one cycle, the Critics pass back the feedback to the ideas to the Dreamers, who continue thinking about new solutions based on the feedback they got. Every participant should switch the role throughout the process (with short breaks to 'neutralize' their minds) in order to understand the other roles' perspectives. The process goes on for as long as it takes, until the Dreamers are happy about the ideas, the Realists are confident about their feasibility and the Critics do not have any more remarks.\n\nA fourth role (the neutral moderator) may be added if the process demands it. He/she is then responsible for starting and ending the process and moderating the discussions. The method may also be applied by an individual who goes through the process individually.\n\n\n== Links & Further reading ==\n''Sources:''\n* Tools Hero - [https://www.toolshero.com/creativity/walt-disney-method Walt Disney Method]\n* Arbeit Digital - [https://arbeitdigital.de/wirtschaftslexikon/kreativitaetstechniken/walt-disney-methode/ Walt-Disney-Methode]\n* Karrierebibel - [https://karrierebibel.de/disney-methode/ Disney Methode: Denkblockaden \u00fcberwinden]\n* Impulse - [https://www.impulse.de/management/selbstmanagement-erfolg/walt-disney-methode/3831387.html Walt Disney Methode]\n\n[https://www.youtube.com/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "6qhbnybkiwhk8kbty1hycjaoik0krx8"
                }
            },
            {
                "title": "Dummy variables",
                "ns": "0",
                "id": "984",
                "revision": {
                    "id": "6776",
                    "timestamp": "2022-09-19T13:39:53Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "comment": "Created page with \"'''In short:''' A dummy variable is a variable that is created in regression analysis to represent a given qualitative variable through a quantitative one, which takes one of...\"",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16313",
                        "#text": "'''In short:''' A dummy variable is a variable that is created in regression analysis to represent a given qualitative variable through a quantitative one, which takes one of two values: zero or one. It is needed to make algorithm calculations possible, since they always rely on numeric values. The entry further provides code for both, R and Python programming languages.\n\nThis entry focuses on the dummy variables and the way they can be encoded with the help of R and Python programming languages to be further used for regression analysis. For more information on regression, please refer to the entry on [[Regression Analysis]]. To see more examples of dummy variables applied, please refer to the [[Price Determinants of Airbnb Accommodations]] and [[Likert Scale]] entries.\n\n__TOC__\n\n== Dummy variables ==\n\nA '''dummy variable''' is a variable that is created in regression analysis to represent a given qualitative variable through a quantitative one, which takes one of two values: zero or one.\n\nTypically we use quantitative variables for [[Regression_Analysis|linear regression model]] equations. These can be a specific size of an object, age of an individual, population size, etc. But sometimes the predictor variables can be qualitative. Those are variables that do not have a numeric value associated with them, e.g. gender, country of origin, marital status. Since machine learning algorithms including regression rely on numeric values, these qualitative values have to be converted. \n\nSome qualitative variables have a natural idea of order and can be easily converted to a numeric value accordingly, e.g. number of the month instead of its name, such as 3 instead of \u201cMarch\u201d. But it is not possible for nominal variables. However, this can be a common trap when preparing one\u2019s dataset for the regression analysis. For the linear model converting ordinal values to numeric ones would not make much sense, as the model assumes the change between each value to be constant. What is usually done instead is a series of binary variables to capture the different levels of the qualitative variable.\n\nIf the qualitative variable, which is also known as a factor, has only two levels, then integrating it into a regression model is very simple: we need to use dummy variables. For example, our variable can describe if a given individual smokes or does not:\n\n[[File:One_and_two.jpg|700px|frameless|center]]\n\nIf a qualitative variable has more than two levels, a single dummy variable cannot represent all possible values. In this case we create additional dummy variables. The general rule that applies here, is the following: ''If you have k unique terms, you use k - 1 dummy variables to represent.''\n\nLet\u2019s consider a variable representing one\u2019s marital status. Possible values are \u201csingle\u201d, \u201cdivorced\u201d or \u201cmarried\u201d. In this case we create two dummy variables.\n\n[[File:Three_four_five.jpg|700px|frameless|center]]\n\nIf, for example, our model helps us predict an individual\u2019s average insurance rate, then ''\u03b2<sub>0</sub>'' can be interpreted as the average insurance rate for a single person, ''\u03b2<sub>1</sub>'' can be interpreted as the difference in the average insurance rate between a single person and a married one, and ''\u03b2<sub>2</sub>'' can be interpreted as the difference in the average insurance rate between a single person and a divorced one. As mentioned before, there will always be one fewer dummy variable than the number of levels. Here, the level with no dummy variable is \u201csingle\u201d, also known as the baseline.\n\n== How to encode dummy variables in R/Python ==\n\nConverting a single column of values into multiple columns of binary values, or dummy variables, is also known as '''\u201cone-hot-encoding\u201d'''. Not all machines know how to convert qualitative variables into dummy variables automatically, so it is important to know different methods how to do it yourself. We will look at different ways to code it with the help of both, R and Python programming languages. The dataset \"ClusteringHSS.csv\" used in the following examples can be downloaded from [https://www.kaggle.com/datasets/harrimansaragih/clustering-data-id-gender-income-spending?resource=download Kaggle].\n\n=== R-Script ===\n\n<syntaxhighlight lang=\"R\" line>\nlibrary(readr) # for the dataset importing\n\ndata <- read_csv(\"YOUR_PATHNAME\")\n\nhead(data)\n#Output:\n# A tibble: 6 x 5\n#     ID Gender_Code Region Income Spending\n#  <dbl> <chr>       <chr>   <dbl>    <dbl>\n#1     1 Female      Rural      20       15\n#2     2 Male        Rural       5       12\n#3     3 Female      Urban      28       18\n#4     4 Male        Urban      40       10\n#5     5 Male        Urban      42        9\n#6     6 Male        Rural      13       14\n\nsummary(data) # Two variables are categorical, gender and region\n              # We see that our dataframe has some NAs in the Income and Spending columns\n#Output:\n#       ID       Gender_Code           Region              Income         Spending    \n# Min.   :   1   Length:1113        Length:1113        Min.   : 5.00   Min.   : 5.00  \n# 1st Qu.: 279   Class :character   Class :character   1st Qu.:14.00   1st Qu.: 7.00  \n# Median : 557   Mode  :character   Mode  :character   Median :25.00   Median :10.00  \n# Mean   : 557                                         Mean   :26.02   Mean   :11.28  \n# 3rd Qu.: 835                                         3rd Qu.:37.00   3rd Qu.:15.00  \n# Max.   :1113                                         Max.   :50.00   Max.   :20.00  \n#                                                      NA's   :6       NA's   :5  \n\n\ndata <- na.omit(data) # Apply na.omit function to delete the NAs\nsummary(data)         # Printing updated data. No NAs\n\n#Let's look at the possible values within each variable\nunique(data$Gender_Code) \n#Output: [1] \"Female\" \"Male\" -> binary\nunique(data$Region)    \n#Output: [1] \"Rural\" \"Urban\" -> binary\n\n#OPTION 1\n#create dummy variables manually. k = k-1\nGender_Code_Male <- ifelse(data$Gender_Code == 'Male', 1, 0) #if male, gender equals 1, if female, gender equals 0.\nRegion_Urban <- ifelse(data$Region == 'Urban', 1, 0) #if urban, region equals 1, if rural, region equals 0.\n\n#create data frame to use for regression\ndata <- data.frame(ID = data$ID, \n                       Gender_Code_Male = Gender_Code_Male, \n                       Region_Urban = Region_Urban, \n                       Income = data$Income, \n                       Spending = data$Spending)\n#view data frame\ndata\n\n#Output:\n#     ID Gender_Code_Male Region_Urban Income Spending\n#1     1                0            0     20       15\n#2     2                1            0      5       12\n#3     3                0            1     28       18\n#4     4                1            1     40       10\n#5     5                1            1     42        9\n#(...)\n\n\n#OPTION 2\n#Using the fastDummies Package\n# Install and import fastDummies:\ninstall.packages('fastDummies')\nlibrary('fastDummies')\n\ndata <- read_csv(\"YOUR_PATHNAME\") #prepare the data frame again if needed\ndata <- na.omit(data) \n\n# Make dummy variables of two columns and remove the previous columns with categorical values:\ndata <- dummy_cols(data, select_columns = c('Gender_Code', 'Region'), remove_selected_columns = TRUE)\n\n#view data frame\ndata\n#Output:\n# A tibble: 1,090 x 7\n#      ID Income Spending Gender_Code_Female Gender_Code_Male Region_Rural Region_Urban\n#   <dbl>  <dbl>    <dbl>              <int>            <int>        <int>        <int>\n# 1     1     20       15                  1                0            1            0\n# 2     2      5       12                  0                1            1            0\n# 3     3     28       18                  1                0            0            1\n# 4     4     40       10                  0                1            0            1\n# 5     5     42        9                  0                1            0            1\n\n\n# CREATING A LINEAR MODEL\n# In our multiple regression linear model we will try to predict the income based on \n# other variables given in our data set. \n# In the formula we drop the Gender_Code_Female and Region_Rural to avoid singularity error, \n# as they have an exact linear relationship with their counterparts.\nmodel <- lm(Income ~ Spending + Gender_Code_Male + Region_Urban, \n            data = data)\n\nsummary(model)\n\n#Call:\n#lm(formula = Income ~ Spending + Gender_Code_Male + Region_Urban, \n#    data = data)\n#\n#Residuals:\n#     Min       1Q   Median       3Q      Max \n#-13.1885  -6.1105  -0.1535   5.8825  12.9356 \n#\n#Coefficients:\n#                 Estimate Std. Error t value Pr(>|t|)    \n#(Intercept)      14.06584    0.63227  22.246   <2e-16 ***\n#Spending          0.02634    0.04577   0.575   0.5651    \n#Gender_Code_Male  0.83435    0.42116   1.981   0.0478 *  \n#Region_Urban     22.78781    0.42049  54.193   <2e-16 ***\n#---\n#Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n#\n#Residual standard error: 6.927 on 1086 degrees of freedom\n#Multiple R-squared:  0.7319,\tAdjusted R-squared:  0.7312 \n#F-statistic: 988.5 on 3 and 1086 DF,  p-value: < 2.2e-16\n</syntaxhighlight>\n\nThe fitted regression line turns out to be:\n\n''Income = 14.06584 + 0.02634*(Spending) + 0.83435*(Gender_Code_Male) + 22.78781*(Region_Urban)''\n\nWe can use this equation to find the estimated income for an individual based on their monthly spendings, gender and region. For example, an individual who is a female living in the rural area and spending 5 mln per month is estimated to have an income of 14.19754 mln per month:\n\n''Income = 14.06584 + 0.02634*5 + 0.83435*0 + 22.78781*0 = 14.19754''\n\n=== Python Script ===\n\n<syntaxhighlight lang=\"Python\" line>\n\nimport pandas as pd # for data manipulation\nimport statsmodels.api as sm # for statistical computations and models\n\ndf = pd.read_csv(\"YOUR_PATHNAME\")\n\n#Let's look at our data.\n#We have five variables and 1112 entries in total\ndf\n\n#        ID Gender_Code Region  Income  Spending\n#0        1      Female  Rural    20.0      15.0\n#1        2        Male  Rural     5.0      12.0\n#2        3      Female  Urban    28.0      18.0\n#3        4        Male  Urban    40.0      10.0\n#4        5        Male  Urban    42.0       9.0\n#    ...         ...    ...     ...       ...\n#1108  1109      Female  Urban    33.0      16.0\n#1109  1110        Male  Urban    48.0       7.0\n#1110  1111        Male  Urban    31.0      16.0\n#1111  1112        Male  Urban    50.0      14.0\n#1112  1113        Male  Urban    26.0      11.0\n#[1113 rows x 5 columns]\n\n\ndf.info()           # Two variables are categorical, Gender_Code and Region\n#<class 'pandas.core.frame.DataFrame'>\n#RangeIndex: 1113 entries, 0 to 1112\n#Data columns (total 5 columns):\n# #   Column       Non-Null Count  Dtype  \n#---  ------       --------------  -----  \n# 0   ID           1113 non-null   int64  \n# 1   Gender_Code  1107 non-null   object \n# 2   Region       1107 non-null   object \n# 3   Income       1107 non-null   float64\n# 4   Spending     1108 non-null   float64\n#dtypes: float64(2), int64(1), object(2)\n\ndf.isnull().sum()   # We see that our dataframe has some NAs in every variable except for ID\n#ID             0\n#Gender_Code    6\n#Region         6\n#Income         6\n#Spending       5\n#dtype: int64\n\ndf = df.dropna() # Apply na.omit function to delete the\ndf.info()        # No NAs\n\npd.unique(df.Gender_Code)\n# Output: array(['Female', 'Male'], dtype=object) -> The variable is binary\n\npd.unique(df.Region)\n# Output: array(['Rural', 'Urban'], dtype=object) -> The variable is binary\n\n\n#OPTION 1\n# using .map to create dummy variables\n# dataframe['category_name'] = df.Category.map({'unique_term':0, 'unique_term2':1})\ndf['Gender_Code_Male'] = df.Gender_Code.map({'Female':0, 'Male':1})\ndf['Region_Urban'] = df.Region.map({'Rural':0, 'Urban':1})\n\n#drop the categorical columns that are no longer useful\ndf.drop(['Gender_Code', 'Region'], axis=1, inplace=True)\n#view data frame\ndf.head()\n#   ID  Income  Spending  Gender_Code_Male  Region_Urban\n#0   1    20.0      15.0                 0             0\n#1   2     5.0      12.0                 1             0\n#2   3    28.0      18.0                 0             1\n#3   4    40.0      10.0                 1             1\n#4   5    42.0       9.0                 1             1\n\n#OPTION 2\n#Using the pandas.get_dummies function.\n# Create dummy variables for multiple categories\n# drop_first=True handles the k - 1 rule\ndf = pd.get_dummies(df, columns=['Gender_Code', 'Region'], drop_first=True)\n# this drops original Gender_Code and Region columns\n# and creates dummy variables\n\n#view data frame\ndf.head()\n#   ID  Income  Spending  Gender_Code_Male  Region_Urban\n#0   1    20.0      15.0                 0             0\n#1   2     5.0      12.0                 1             0\n#2   3    28.0      18.0                 0             1\n#3   4    40.0      10.0                 1             1\n#4   5    42.0       9.0                 1             1\n\n\n#CREATING A LINEAR MODEL\n# with statsmodels\n# Setting the values for independent (X) variables (what we use for prediction)\n# and dependent (Y) variable (what we want to predict).\n\nx = df[['Spending', 'Gender_Code_Male', 'Region_Urban']]\ny = df['Income']\n\nx = sm.add_constant(x)  # adding a constant, or the intercept\n\nmodel = sm.OLS(y, x).fit()\npredictions = model.predict(x)\n\nprint_model = model.summary()\nprint(print_model)\n\n#                            OLS Regression Results                            \n#==============================================================================\n#Dep. Variable:                 Income   R-squared:                       0.732\n#Model:                            OLS   Adj. R-squared:                  0.731\n#Method:                 Least Squares   F-statistic:                     988.5\n#Date:                Mon, 19 Sep 2022   Prob (F-statistic):          7.50e-310\n#Time:                        15:24:54   Log-Likelihood:                -3654.3\n#No. Observations:                1090   AIC:                             7317.\n#Df Residuals:                    1086   BIC:                             7337.\n#Df Model:                           3                                         \n#Covariance Type:            nonrobust                                         \n#====================================================================================\n#                       coef    std err          t      P>|t|      [0.025      0.975]\n#------------------------------------------------------------------------------------\n#const               14.0658      0.632     22.246      0.000      12.825      15.306\n#Spending             0.0263      0.046      0.575      0.565      -0.063       0.116\n#Gender_Code_Male     0.8343      0.421      1.981      0.048       0.008       1.661\n#Region_Urban        22.7878      0.420     54.193      0.000      21.963      23.613\n#==============================================================================\n#Omnibus:                      535.793   Durbin-Watson:                   1.977\n#Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.649\n#Skew:                           0.042   Prob(JB):                     1.12e-13\n#Kurtosis:                       1.857   Cond. No.                         39.2\n#==============================================================================\n#Notes:\n#[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</syntaxhighlight>\n\nThe fitted regression line turns out to be:\n\n''Income = 14.0658 + 0.0263*(Spending) + 0.8343*(Gender_Code_Male) + 22.7878*(Region_Urban)\n''\n\nWe can use this equation to find the estimated income for an individual based on their monthly spendings, gender and region. For example, an individual who is a female living in the rural area and spending 5 mln per month is estimated to have an income of 14.1973 mln per month:\n\n''Income = 14.0658 + 0.0263*5 + 0.8343*0 + 22.7878*0 = 14.1973''\n\n== Useful sources ==\n\n* [https://www.coursera.org/lecture/machine-learning-algorithms-r-business-analytics/dummy-variables-lJgVJ Coursera course on dummy variables]\n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n[[Category:Quantitative]]\n\nThe [[Table_of_Contributors| author]] of this entry is Olga Kuznetsova."
                    },
                    "sha1": "ejopjakzhmbtxmycoweains4xtshquy"
                }
            },
            {
                "title": "Elevator Pitch",
                "ns": "0",
                "id": "302",
                "revision": {
                    "id": "3253",
                    "parentid": "2960",
                    "timestamp": "2020-11-04T10:20:28Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4002",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nThe Elevator Pitch is a precise and short (30-120 second) presentation format. Its name originates from the attempt of salesmen to enthuse potential customers and investors during the duration of an elevator ride - which seldomly took longer than 60 seconds. However, it can be used to 'sell' basically anything - an idea, a product, a company, or one's self. This does not have to happen in a seller-customer-relationship, but can also be of help when you want to make an impression on job interviewers or new acquaintances. It also applies to research, when an idea has to be 'sold' to potential supporters or to colleagues.\n\n== Goals ==\n* Make someone exicted about what you have to offer and make them remember you.\n* Learn to describe your idea or person concisely - find out what is most important about it.\n\n== Getting started ==\nAn Elevator Pitch is not done spontaneously. Instead, it should be well prepared and rehearsed so that it can be acted out whenever necessary.\nThe structure of the Elevator Pitch may follow the ''AIDA'' scheme: \n# ''Attention:'' raise attention, for example with an innovative introduction, a question, a controversial statement or a joke.\n# ''Interest'': raise Interest by highlighting your professional experiences, the product's qualities or the idea's USP. What about your offer is of interest to the listener?\n# ''Desire'': Make the listener want what you have to offer. Illustrate the benefits of them hiring you or supporting your idea.\n# ''Action'': A good Elevator Pitch should yield results, so you have to build up to these. Sketch the next steps, include a Call-To-Action and make sure that there is a follow-up conversation to the pitch.\n\n(A very true-to-life) Example:\n  \"Energy, huh? Hi, my name is John, I just got my Master's degree in Sustainability Science. I've learned about and done research on energy issues for a couple of years now. It is a topic that really fascinates me, and I now intend to work on the science-policy interface to help decision-makers shape the future of this sector. So if you'd like investigate some ideas, I'd be happy to help!\"\n\n'''Some general tips on how to pitch include:'''\n* Have a first sentence that immediately raises attention and present it with self-confidence.\n* Prioritize what you say - focus on what is important.\n* Relate to the interests of who you want to persuade.\n* Be authentic and enthusiastic.\n* Keep it simple and understandable.\n* Allow for the other to react to what you said.\n* Rehearse the pitch so that you're prepared to present it whenever necessary.\n\n== Links & Further reading ==\n''Sources:''\n* Karrierebibel - [https://karrierebibel.de/elevator-pitch/ Elevator Pitch. 10 Tipps, wie Sie in 60 Sekunden begeistern]\n* F\u00fcr Gr\u00fcnder - [https://www.fuer-gruender.de/kapital/eigenkapital/elevator-pitch/richtig-vorbereiten/ Tipps f\u00fcr Ihren Elevator Pitch]\n* IONOS Startup Guide - [https://www.ionos.de/startupguide/produktivitaet/elevator-pitch/ Elevator-Pitch: \u00dcberzeugen in wenigen Minuten.]\n* thebalancecareers - [https://www.thebalancecareers.com/elevator-speech-examples-and-writing-tips-2061976 How to Create an Elevator Pitch With Examples]\n\nFor a short example on what to do - and what NOT to do - have a look at this [https://www.youtube.com/watch?v=uyxfERV5ttY YouTube-Video].\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "oyj9h9kw0pti08xusom20p63qr2iuj6"
                }
            },
            {
                "title": "Empathetic Listening",
                "ns": "0",
                "id": "716",
                "revision": {
                    "id": "7021",
                    "parentid": "5782",
                    "timestamp": "2023-04-08T14:50:01Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5776",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n__NOTOC__\n\n== What, Why & When ==\n(Active) Empathetic listening is a skill that helps to understand a [[Glossary|dialogue]] partner on a deeper level. It strengthens the bond between conversation partners, soothes the work atmosphere during stressful phases, prevents the reproduction of injustice, and enriches creative thinking. This ability can become especially useful during [[Open Interview|qualitative interviews]], customer/client/patient conversations, and conversations between co-workers, e.g. [[Transdisciplinarity|(trans)disciplinary]] researchers, or co-workers in any other field of work. It is best performed without time pressure.\n\n== Goals ==\n(Active) Empathetic listening exceeds mere punctual information gathering, and even effective listening which enables to actively read a more complete image of what is being said. Through empathetic listening, emotional information - which includes experiences being told or preferences being shared - is not only listened to but empathetically engaged with. It involves a set of other skills that must and will be acquired throughout its training.\n\n== Getting started ==\nIt all starts with the setting. It is best to create a quiet and comfortable atmosphere, although this may no longer be neccessary with rising experience with the skill. A comfortable atmosphere enhances the concentration on what is being said. Also, visual distractions, fidgeting or eating, and drinking should be held to a minimum. A setting free of distraction also includes taking enough time for the conversation, so that nothing must be rushed through and consequently enough patience to not only sit out the meeting but actively participate as the listener.\n\nHowever, your positioning towards the speaker is fundamental. This includes for one your actual pose and your body language, which should be open, genuine, and neither too neutral nor too emotionally charged with your own emotions. This also includes your actual positioning as a person. First, take yourself back. It is very important to be selfless and genuinely interested in the other. This creates the key element of what makes somebody ''listen to'' - and not only ''hear'' - others. To enhance your interest and show it to your dialogue partner, try to think of open-ended questions. You can also paraphrase the speaker\u2019s perspective to remember and internalize better what has been said. \n\nThis way, it becomes easier to actively imagine the other one\u2019s experiences, ideas, and emotions. Since emotionally charged topics might come up, being emotionally available and tolerant is another key [[Glossary|competence]]. This must exclude interruption of the speaker and may exclude criticism if the speaker does not precisely ask for it. Also, filling the silence with your own words instead of giving time for the speaker to think thoroughly and continue the talk is not desirable. At the end of the conversation, you may want to ask for a follow-up conversation, especially if time unexpectedly runs short or the topic was complex or emotionally challenging. Remember the well-being of the speaker is of the utmost importance. If no follow-up conversation is wished for, do not pressure the speaker for one.  \n\n'''Here is a brief list of the presented competencies you need to practice empathetic listening:'''\n# Create/chose a quiet, comfortable, and non-distracting environment to be \u201cin\u201d the moment.\n# Take time and be patient.\n# Use open and friendly body language.\n# Take yourself back.\n# Be selfless.\n# Be reserved about criticism.\n# Do not interrupt or fill up the silence with your thoughts, except for signs of understanding.\n# Do not only show interested, but actually be interested. Remember you can learn from anybody and anything. Ask open-ended questions.\n# Be empathetic!\n# Imagine and remember what is being told.\n# Paraphrase what has been said to remember better.\n# Be emotionally invested and open-minded.\n# Ask for a follow-up conversation.\n\nAs you can see, the practice of empathetic listening does not only depend on these competencies but they are also trained throughout the act of empathetic listening.\n\n== Links and further reading ==\n* Lucette B. Comer and Tanya Drollinger (1999): ''Active Empathetic Listening and Selling Success: A Conceptual Framework.'' Journal of Personal Selling & Sales Management, pp. 15-29.\n* Clowse MEB (2020). ''Learning to listen: how empathetic engagement with patients can help overcome reproductive injustice.'' Lupus Science & Medicine; 7:e000455. doi: 10.1136/lupus-2020-000455\n* ''Empathic Listening: Definition, Examples and Tips.'' (2021, February 8). Indeed Career Guide. https://www.indeed.com/career-advice/career-development/empathic-listening\n* Mind Tools Content Team. (2018, July 19). ''Empathic Listening: Going Beyond Active Listening.'' Mind Tools. https://www.mindtools.com/CommSkll/EmpathicListening.htm\n* Saleem, A. (2019, June 18). ''Empathic Listening Skill -10 Best Exercises To Acquire Empathic Listening.'' The Life Virtue. https://thelifevirtue.com/empathic-listening-skill/\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table of Contributors|author]] of this entry is Mona H\u00fcbner."
                    },
                    "sha1": "r7cbihzhfavkn4kyq8h4s9mhr4awgo5"
                }
            },
            {
                "title": "Ethics and Statistics",
                "ns": "0",
                "id": "703",
                "revision": {
                    "id": "5867",
                    "parentid": "5862",
                    "timestamp": "2021-06-21T09:42:25Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Looking at statistics from philosophy of science */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "35372",
                        "#text": "[[File:Bildschirmfoto 2020-06-22 um 08.25.14.png|400px|thumb|right|'''Derek parfit is one of the most famous philosophers of the 20th and early 21st century.''' His thesis on normative ethics can be seen as his most important work.]]\n\nFollowing Sidgwicks \"Methods of ethics\", ethics can be defined as ''the world how it ought to be''. Derek Parfit argued that if philosophy be a mountain, Western philosophy climbs it from three sides: \n* The first side is Utilitarianism, which is widely preoccupied with the question how we can evaluate the outcome of an action. The most ethical choice would be the action that creates the most good for the largest amount of people.\n* The second side is reason, which can be understood as the human capability to reflect what one is ought to do. Kant said much to this end, and Parfit associated it to the individual, or better, the reasonable individual.\n* The third side of the mountain is the social contract, which states that a range of moral obligations are agreed upon in societies, a thought that was strongly developed by Locke. Parfit associated this even wider, referring to the whole society in his triple theory. \n\n\n== Do ethics matter for statistics? ==\nPersonally, I think ethics matters deeply for statistics. Let me try to convince you. \nLooking at the epistemology of statistics, we learned that much of modern civilisation is built on statistical approaches, such as the design and analysis of [[Experiments]] or the [[Correlations|correlation]] of two continuous variables. Statistics propelled much of the exponential growth in our [[Scientific methods and societal paradigms|society]], and Statistics is responsible for many of the problems we currently face through our unsustainable behaviour. After all, Statistics was willing to utilize means and accomplish goals that led us more into the direction of a more unsustainable path. Many would argue now that if statistics were a weapon, itself would not kill. Instead, it would be the human hand that uses it. This is insofar true as statistics would not exist without us, just as weapons were forged by us. However, I would argue that statistics are still deeply [[Normativity of Methods|normative]], as they are associated with our culture, society, social strata, economies and so much more. This is why we should embrace a critical perspective on statistics. Much in our society depends on statistics, and many decisions are taken because of statistics. As we have learned, some of these statistics might be problematic or even wrong, and consequently, this can render the decisions wrong as well. More strangely, our statistics can be correct, but as we have learned, statistics can even then contribute to our downfall, for instance when they contribute to a process that leads to unsustainable production processes. '''We may calculate something correctly, but the result can be morally wrong.''' Ideally, our statistics would always be correct, and the moral implications that follow out of our actions that are informed by statistics are also right.\n \n[[File:Bildschirmfoto 2020-06-22 um 08.35.42.png|600px|left|thumb|'''GDP is a good example of a construct used in statistics which influences our daily life and especially politics to a huge extent.''']]\n'''However, statistics is more often than not seen as something that is not normative, and some people consider statistics to create objective knowledge.''' This is rooted in the deep traditions and norms of the disciplines where statistics are an established methodological approach, and in the history and theory of science that governs our research. Many scientists are regrettably still [[Bias and Critical Thinking|positivists]], and often consider the knowledge they create to be objective. More often than not, this is not a conscious choice, but the combination of unreflected teachers in some education system in general.  Today, obvious moral dilemmas and ambiguities are generally part of complex ethical pre-checks in many study designs, for which medicine provides the gold standard. Here, preventing blunders was established early on, and is now part of the canon of many disciplines, with medicine leading the way. Such problems often deal with questions of sample size, randomisation and the question when a successful treatment should be given to all participants. These are indirect reflections on validity and plausibility within the study design, acknowledging that failures or flaws in these elements may lead to biased or even plain wrong results of the study. \n\nWhat is however widely lacking within the broader debates in science is how the utilisation of statistics can have wider normative consequences, both during the application of statistics, but also due to the consequences that arise out of results that were propelled by statistics. In her book \"Making comparisons count\", Ruth Chang explores one example of such relations, but the gap between ethics and statistics is so large, we might define it altogether as widely undiscovered country. More will need to be done to overcome the gap between ethics and statistics, and a connection is hampered by the fact that both fields are widely unclear in terms of the overall accepted norms. While many statistical textbooks exist, these are more often than not disciplinary, and consolidating the field of ethics this diversity is certainly no small endeavour. Creating connections between statistics and ethics is a challenge,  because there is a long history of flaws in this connection that triggered blurred, if not wrong decisions. We will therefore look at specific examples from both directions, starting with a view on ethics from the perspective of statistics.\n\n== Looking at ethics from statistics ==\n[[File:Food-supply-kcal.png|600px|right|thumb|[https://slides.ourworldindata.org/hunger-and-food-provision/#/kcalcapitaday-by-world-regions-mg-png '''Our World in Data] provides plenty information''', for example on how the worldwide food provision changed over the last decades.]]\n\nWhen looking at the relation between ethics and statistics from the perspective of statistics, there are several items which can help us understand their interactions. First and foremost, the concept of validity can be considered to be highly normative. Two extreme lines of thinking come into mind: One technical, orientating on mathematical formulas; and the other line of thinking, being informed actually by the content. Both are normative, but of course there is a large difference between a model being correct in terms of a statistical validation, and a model approximating a reality that is valid. Empirical research makes compromises by looking at pieces of the puzzle of reality. Following [[Bias and Critical Thinking|critical realism]], we may be able to unlock the strata of everything we can observe (the \u2018empirical\u2019, as compared to the \u2018real\u2019 and the \u2018actual\u2019), but ultimately this will always be a subjective perspective. Hence empirical research will always have to compromise as we choose versions of reality in our pursuit for knowledge. \n\nIf there is such a thing as general truth, you may find it in philosophy and probably thus also in ethics. Interestingly, philosophy originally served to generate criteria for validity in empirical research. '''However, knowledge in philosophy itself will never be empirical, at least not if it equally fulfils the criterion of being generally applicable.''' Through empirical research, we can observe and understand and thus generate knowledge, while philosophy is about the higher norms, principles and mechanism that transcend the empirical. Statistics is able to generate such empirical knowledge, and philosophy originally served to generate criteria for validity of this empirical process. Philosophy and empirical branches of science were vividly interacting in the [[History of Methods|Antique]], they started to separate with the Enlightenment. The three concepts that (Western?) philosophy is preoccupied with - utilitarianism, reason and social contract - dissolved into scientific disciplines as economics, psychology, social science or political science. These disciplines subsequently tried to bridge the void between philosophy and empirical research, yet often had to settle for compromises when integrating the empirical versions of reality with higher theories from philosophy. An example would be the Friedman doctrine in economics, which was originally postulated with good intentions, but its empirical translation into policies wreaked havoc for whole nations. \n\nStatistics did however indirectly contribute to altering our moral values. Take utilitarianism. It depends deeply on our moral values how we calculate utility, and hence how we manage our systems through such calculations. Statistics in itself could be argued to be non-normative if it dealt with totally arbitrary data that has no connection to any reality. However, as soon as statistics deals with data of any version of reality, we make Statistics normative by looking at these versions of reality. '''It is our subjective views that make statistics no longer objective.''' One might argue that physics would be an exception, and it probably is. It is more than clear however that social sciences, economics, ecology and psychology offer subjective epistemological knowledge, at least from a viewpoint of critical realism. The numbers are clear, yet our view of them is not. Consider for instance that on the one hand, world hunger decreased over the last decades and more people get education or medical help than ever before. At the same time, other measures propose that inequality increases, for instance income inequality. While ethics can compare two statistical values in terms of their meaning for people, statistics can generate the baseline for this. Statistics provides the methodological design, mathematical calculations and experience in interpretation that we need to make ethical decisions. This brings us to plausibility.\n\n'''Statistics builds on plausibility: a good model should be both probable and reasonable.''' While statistics focuses predominantly on probability, this is already a normative concept, as the significance level \u03b1 is arbitrary (although the common level of .05 is more often than not relatively robust). However, p-values are partly prone to statistical fishing, which is why probability in statistics is widely established but still often prone to flaws and errors. For more on this, have a look at the entry on [[Bias in statistics]].\n\nStatistical calculations can also differ in terms of the model choice, which is deeply normative and a case of disciplinary schools of thinking. Many models build on the precondition of a [[Data distribution|normal distribution]], which is however not always the case. This illustrates that assumed preconditions are often violated, and hence that statistical rigor can be subjective or normative. Different people have different experiences, and some parts of science are more rigorous than others. Non-normal distribution may matter deeply, for instance when it is about income inequalities and associated policies, and statistics shall not evade the responsibility that is implied in such an analysis.\n\nAnother example is the question of [[Non-equilibrium dynamics|non-linearity]]. Preferring non-linear relationships to linear relationships is something that has become more important especially recently due to the wish to have a higher predictive power, to understand non-linear shifts, or for other reasons. Bending models - mechanically speaking - into a situation that allows to increase the model fit (that is, how much the model can predict) comes with the price that we sacrifice any possibility of [[Causality|causal]] understanding, since the zig-zag relations of non-linear models are often harder to match with our theories. While of course there are some examples of rapid shifts in systems, this has next to nothing to do with [[Glossary|non-linearity]] that is assumed in many modern statistical analysis schemes and the predictive algorithms whose data power over us governs wide parts of our societies. '''Many disciplines gravitated towards non-linear statistics in the last decades, sacrificing explainable relationships for an ever increasing model fit.''' Hence the clear demarcation between [[:Category:Deductive|deductive]] and [[:Category:Inductive|inductive]] research became blurry, adding an epistemological/ontological burden on research. In many papers, the question of plausibility and validity has been deeply affected by this issue, and I would argue that this casts a shadow on the reputation of science. Philosophy alone cannot solve this problem, since it looks at the general side of reality. Only by teaming up with empirical research, we may arrive at measures of validity that are not on a high horse, but also can be connected to the real world. To this end, I would currently trust critical realism the most, yet time will tell where theory of science will lead us in the future.\n\n[[File:Bildschirmfoto 2020-06-22 um 08.46.28.png|400px|left|thumb|'''The [[Likert Scale]] is one example of a construct coming from the discipline of psychology.''' Today it is very often used also in surveys from other disciplines as you for sure also already came in touch with it.]]\n\nAnother obvious example regarding the normativity of statistical analysis are constructs. [[To Rule And To Measure|Constructs]] are deeply normative, and often associated to existing norms within a discipline, and may even transcend whole world views. The Likert Scale in psychology is such an example. The obvious benefit of that scale is that it may unlock perceptions form a really diverse array of people, which is a positive aspect of such a scaling. Hence it became part of the established body of knowledge, and much experience is available on the benefits and drawbacks of this scaling, yet it is a normative choice whether we use it or not. '''Often, scales are even part of the tacit signature of cultural and social norms, and these indirectly influence the science that is conducted in this setting.''' An example would be the continuous Arabian numbering that dominates much of Western thinking, and that many of us grow into these days. Philosophy and especially cultural studies have engaged in such questions increasingly in the last decades, often focusing on isolated constructs or underlying fundamental aspects such as racism, colonialism and privilege. It will be challenging to link these important and timely concepts directly to the flaws of empirical research. These days, luckily, I recognize a greater awareness among empirical researchers to avoid errors implemented through such constructs. Much needs to be done, but I conceive reason for hope as well. Personally, I believe that constructs will always be associated to personal identity and thus pose a problem for a general acceptance. Time will tell if and how this gap can be bridged.\n\n== Looking at statistics from philosophy of science ==\n'''Science often claims to create objective knowledge.''' Famously, Karl Popper and others stated that this claim is false - or should at least be questioned. Still, it is not only perceived by many people that science is de facto objective, but more importantly, many scientists often claim that the knowledge they produce is objective. Large parts of academia like to spread an air of superiority and arrogance, which is one of many explanations for the science-society-gap. When scientists should talk about potential facts, they often just talk about \u2018facts\u2019. When they could highlight our inability as scientists to grasp the true nature of reality, they instead present their version of reality to be the best or maybe even the only version of reality. Of course, this is not only their fault, since society increasingly demands easy explanations. A philosopher would be probably deeply disturbed if they understood how statistics are applied today, but even more disturbed if this philosopher saw how statistics are being interpreted and marketed. Precision and clarity in communicating results, as well as information on bias and caveats, are missing more often that we should hope for. \n\n[[File:Bildschirmfoto 2020-06-23 um 16.42.31.png|400px|right|thumb|'''The maxim ''Publish or Perish'' is dominating most of the scientific world today.''' Especially universities and scientific journals play an important role in this.]]\n\nThere are many facets that could be highlighted under such a provocative heading. Since Larry Laudan, it has become clear that the assumption that developments of science which are initiated by scientists are reasonable, is a myth at best. Take the example of one dominating [[Glossary|paradigm]] in science right now: Publish or perish. This paradigm highlights the current culture dominating most parts of science. If you do not produce a large amount of peer-reviewed publications, your career will not continue. This created quite a demand on statistics as well, and the urge to arrive at significant results that probably created frequent violations of Occam's razor (that is, things should be as complex as necessary, but as simple as possible). The reproducibility crisis in psychology is one example of these developments, yet all other disciplines building on statistics struggle to this end, too. Another problem is the fact that with this [[Questioning the status quo in methods|ever-increasing demand for \"scientific innovation\"]], models evolve, and it is hard to catch up. '''Thus, instead of having robust and parsimonious models, there are more and more unsuitable and overcomplicated models.''' The level of statistics at an average certainly increased. There are counter-examples where this is not necessarily the case, and rightly so. In medicine, for instance, the canon of statistics is fairly established and solidified for large parts of the research landscape. Thus, while many innovative publications are exploring new ways of statistics, and a highly innovative to this end, there is always a well-defined set of statistical methods that research can default on. Within many other branches of science, however, there is a certain increase in errors or at least slippery slopes. Statistics are part of the publish or perish, and with this pressure still rising, unreasonable actions in the application of statistics may still increase. Many other examples exist to this end, but this should highlight that statistics is not only still developing further, but statistics should also keep evolving, otherwise we may not be able the diversity of epistemological knowledge it can offer.\n\n== The imperfection of measurements ==\nAs a next step, let us look at the three pillars of Western ethics and how these are intertwined with statistics. We all discuss how we act reasonable, how we can maximize our happiness and how we live together as people. Hence reason, utilitarianism and the social contract are all connected to statistics in one way or the other. While this complicated relation may in the future be explored in more than a short wiki entry, lets focus on some examples here.\n\nOne could say that utilitarianism created the single largest job boost for the profession of statisticians. From predicting economic development, to calculating engineering problems, to finding the right lever to tilt elections, statistics are dominating in almost all aspects of modern society. '''Calculating the maximisation of utility is one of the large drivers of change in our globalised economy.''' But it does not stop there. Followers on social media and reply time to messages are two of many factors how to measure success in life these day, often draining their direct human interactions in the process, and leading to distraction or even torment. Philosophy is deeply engaged in discussing these conditions and dynamics, yet statistics needs to embed these topics, which are strongly related to ethics, more into their curriculum. If we become mercenaries for people with questionable goals, then we follow a long string of people that maximised utility for better or worse. History teaches us of the horrors that were conducted in the process of utility maximisation, and we need to end this string of willing aids to illegitimate goals. Instead, statistics should not only be about numbers, but also about the fact that these numbers have meaning. Numbers are not abstract representations of the world, but can have different meanings for different people. Hence numbers can add information that is missing, and can serve as a starting point for an often necessary deeper reflection.\n\n\n== Of inabilities to compromise ==\nAnother problem that emerges out of statistics as seen from a perspective of ethics is the inability to accept different versions of reality. Statistics often arrives at a specific construction of reality, which is the confirmed or rejected. After all, most applications and interpretations in statistics are still from a standpoint of positivism. At best, such a version of reality becomes then more and more refined, until it becomes ultimately the accepted reality. Today, more people would agree that versions of reality are mere snapshots and are probable to change in the future. Linking our empirical snapshots or reality with ethical concepts I possible but challenging. Hedonism would be one example where statistics can serve as a blunt and unreflected tool, with the hedonist being the center of the universe. However, personal identity is increasingly questioned, and may play a different role in the future than it does today. We have to acknowledge that statistics is about epistemological knowledge, while ethics can be about ontological truths. Statistics may thus build a refined version of reality, while ethics can claim and alter between different realities that allow for the reflection of higher concepts or principles that transcend subjective realities and are thus non-empirical in a sense that these principles may be able to integrate all empirical realities. While this is one of the most daring endeavors in philosophy that emerged over the last decades, it builds on the premise that ethics can start lines of thinking outside of empirical realities. This freedom in ethics i.e., through [[Thought Experiments]], is not possible in statistics. '''In Statistics, the empirical nature requires a high workload and time effort, which creates a penalty that ultimately makes statistics less able to compromise or integrate different versions of ethics.''' Future considerations need to clearly indicate what the limitations of statistics are, and how this problem of linking to other lines of thinking i.e. ethicscan be solved, even if such other worldviews violate statistical results or assumptions that characterize empirical realities. In other words, you can consider your subjective perspective as your epistemological reality, and I would argue that experience in statistics can enable you to develop a potentially better version of your epistemological reality. This may even change your ontological reality, as it can interact with your ontological principles, i.e. your moral views and ethical standpoints.\n\n== Combining ethics and statistics ==\n'''A famous thought experiment from Derek Parfit is about 50 people dying of thirst in the desert.''' Imagine you could give them one glass of water, leaving a few drops for each person. This would not prevent them from dying, but it would still make a difference. You tried to help, and some would even argue, that if these people die, they still die with the knowledge that somebody tried to help them. Hence your actions may matter even if they are ultimately futile, yet classical ethics do not manage to account for such imperceptible consequences of our actions. Equally would a utilitarian statistician tell you that calculating the number of drops for each person is not only hard to measure, but would probably anyway evaporate before the people could sip it. \n \n'''Another example is the tragic question of triage.''' Based on previous data and experience, it is sometimes necessary to prioritize patients with the highest chance of survival, for instance in the aftermath of a catastrophe. This has severe ethical ramifications and is known to put a severe emotional burden on medical personnel. While this follows a utilitarian approach, there are wider consequences that fall within the realms of medical ethics. \n\n'''A third example is the repugnant conclusion.''' Are many quite happy people comparable to few very happy people? This question is widely considered to be a paradox, which is also known as the mere addition problem. We cannot \u2018objectively\u2019 measure happiness, and much less calculate it. What this teaches us is that it is difficult for us to judge whether a life is happy and worth living, even if we personally do not consider it to be this way. People can have fulfilled lives without us understanding this. Here, statistics fail once more, partly because of our normative perspective to interpret the different scenarios. \n\n'''Another example, kindly borrowed from Daniel Dennet, are nuclear disasters.''' In Chernobyl many workers had to work under conditions that were creating a large harm to them, and a great emotional and medical burden. To this date, assumptions about the death toll vary widely. However, without their efforts it would be unclear what would have happened otherwise, as the molten nuclear lava would probably have reached the drainage water, leading to an explosion that might have rendered many parts of Eastern Europe uninhabitable. All these dynamics were hard to anticipate, and even more difficult to balance against the efforts of the workers. Basically, this disaster posed a wicked problem. One must state that these limitations of calculations and the evaluations of outcomes were already stated in Utilitarianism in Moore's Principia Ethics, but this has been widely overseen by many, who still guide their actions through imperfect predictions.\n\n\n== The Trolley problem ==\nOne last example of the relation between ethics and statistics is the problem of inaction. What if you could save 5 people, but you would know that somebody else then dies through the action that saves the other five. Many people prefer not to act. To them, an inaction is ethically more ok than the actual act, which would make them responsible for the death of one person, even if they saved five times as many people. This is also related to knowledge and certainty. Much knowledge exists that should lead people to act, but they prefer to not act at all. Obviously, this is a great challenge, and while psychology investigates this knowledge-action gap, I propose that it will still be here to stay for an unfortunately long time. If people were able to emotionally connect to information derived from numbers, just as the example above, and would be able to act reasonable, much harm could be avoided or at least minimised. This consequentialism is widely missing to date, albeit much of our constructed systems are based on quite similar rules. '''For instance, many people today do in fact understand that their actions have a negative impact on the climate, but nevertheless continue with these actions.''' Hence it is not only an inaction between knowledge and action, but also a gap between constructed systems and individuals. There is much to explore here, and as Martha Nussbaum rightly concluded in the Monarchy of Fear, even under dire political circumstances, \"hope really is both a choice and a practical habit.\" Hence it is not only utilitarianism that links statistics to ethics, but these questions also links to the social contract, and how we act reasonably or unreasonably. Many experimental modifications of the Trolley experiment were investigated by Psychology, and these led to many interpretations on why people act unreasonably. One main result is that humans may tend to minimize the harm they do to others, even if this may create more harm to people on an indirect level. Pushing a lever is a different thing from pushing a person.\n\n== The way forward ==\n[https://www.feynmanlectures.caltech.edu/ The Feynman lectures] provide the most famous textbook volumes on physics. Richard Feynman and his coauthors compiled these books in the 1960, yet to this day, many consider these volumes to be the most concise and understandable introduction to physics. Granted, Feynman's work is brilliant. '''At the same time, however, this fact also means that in the 1960s the majority of the knowledge necessary for an introduction in physics was already available.''' While we may consider that much happened ever since, students can still use these textbooks today. Something similar can be claimed for basic statistics, although it should be noted that physics is a scientific discipline, while statistics is a scientific method. Some disciplines use different methods, and many disciplines use statistics as a method. However, the final word on statistics has not been said, as the differences between [[A matter of probability|probability]] statistics and [[Bayesian Inference|Bayesian]] statistics are not yet deeply explored. Especially Bayesian statistics may provide a vital link to experiments, yet this has been hardly explored to date. Critical spirits may say that the teaching of statistics is often the most normal part of any sort of normal science study program, and this reputation led to statistics more often than not being characterized by a lack of excitement for students. We should not only know and teach the basics, but also engage in the discussion what these all may mean.\n \n'''Philosophy can be thought together well with statistics.''' Unfortunately until today they are not very much associated with each other.\nSomething surprisingly similar can be said about philosophy. While much of the basic principles are known, these are hardly connected. Philosophy hence works ever deeper on specifics, but most of its contributors move away from a unified line of thinking. This is what makes Derek Parfits work stand out, since he tried to connect the different dots, and it will be the work of the next decades to come to build on his work, and improve it if necessary.\n\nWhile philosophy and statistics both face a struggle to align different lines of thinking, it is even more concerning how little these two are aligned with each other. Statistics make use of philosophy at some rare occasions, for instance when it comes to the ethical dilemmas of negatively affecting people that are part of a study. While these links are vital for the development of specific aspects of statistics, the link between moral philosophy and statistics has hardly been explored so far. In order to enable statistics to contribute to the question how we ought to act, a systematic interaction is needed. I propose that exploring possible links is a first step, and we start to investigate how such connections work. The next step would be the proposal of a systematic conceptualisation of these different connections. This conceptualisation would then need to be explored and amended, and this will be very hard work, since both statistics and moral philosophy are scattered due to a lack of a unified theory, and hardly anyone is versatile in both fields.. This makes a systematic exploration of such a unifying link even more difficult to explore is the question who would actually do that. Within most parts of the current educational system, students learn either empirical research and statistics, or the deep conceptual understanding of philosophy. Only when we enable more researcher to approach from both sides - empirical and conceptual - will we become increasingly able to bridge the gap between these two worlds.\n\n\n== One last thought: Statistics as a transformational experience towards ethics ==\nMost links between statistics and ethics are tacit, sparse and more often than not singular. Overall, the link is difficult to establish because much of ethics are about concepts and thus holistic ways of thinking, while statistics engages in the empirical, which is after all subjective. I would however argue that it is still possible to link these two, and to illustrate this I use the example of Occam\u2019s razor. Remember: \u201cEverything needs to be as simple as possible, and as complex as necessary\u201d. I would propose now that Occam\u2019s razor is not really a principle - like justice or altruism - but a heuristic. Heuristics can help us guide our actions, or better, the way we act. '''To this end, we must consider that Occam\u2019s razor is different to different people, since 'simple' or 'complex' may mean different things to them.''' Take an advanced statistical dataset. To me, this is probably a quite straightforward endeavour to analysis this, while for some of you, an analysis and hence the dataset and your view of it are rather complex. Another important point is that different branches of science have different approaches to defining [[Agency, Complexity and Emergence|complexity]], posing another reason why this is a subjective or normative perspective on the simple and the complex. The process of balancing simplicity and complexity is however a process that would argue to be objective, in a sense that we all struggle to balance the simple and the complex. This process of reflection can be a quite catalytic experience, and this learning - I would argue - unites us all. Hence Ocam\u2019s razor is partly about epistemological knowledge, and partly about ontological truth. I believe this can make the case that Occam\u2019s razor provides a link between statistics and ethics, since within ethics we can discuss how we negotiate, mitigate, and integrate in a reflexive way the balance between the simple and the complex.\n\n\n== Further Information ==\n====Articles====\n[https://earlymoderntexts.com/assets/pdfs/sidgwick1874.pdf The Methods of Ethics]: The whole book by the famous Henry Sidgwick\n\n[https://www.britannica.com/biography/Derek-Parfit Derek Parfit]: A short biography & a summary of his most important thoughts\n\n[https://slides.ourworldindata.org/hunger-and-food-provision/#/title-slideHunger and Food Provision]: some data\n\n[https://explorable.com/data-dredgingData dredging]: An introduction into statistical fishing\n\n[https://online.stat.psu.edu/statprogram/ethics Ethics & Statistics]: A guideline\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5090700/ The Science-Society Gap]: An interesting paper on the main issues between science and society\n\n[https://www.investopedia.com/terms/u/utilitarianism.asp#utilitarianisms-relevance-in-a-political-economy Utilitarianism & Economics]: A few words from the economic point of view\n\n====Videos====\n[https://www.youtube.com/watch?v=-a739VjqdSI Utilitarianism]: An introduction with many examples\n\n[https://www.youtube.com/watch?v=8Y5cftds7-8 The Nature of Truth]: some thoughts from philosophy\n\n[https://www.youtube.com/watch?v=9L7r3Uc4fGU Model Fit]: An explanation using many graphs and example calculations\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "hlzcss9ri6zqclaljafmsijvfzhv9ol"
                }
            },
            {
                "title": "Ethnography",
                "ns": "0",
                "id": "355",
                "revision": {
                    "id": "5936",
                    "parentid": "5673",
                    "timestamp": "2021-06-30T17:56:11Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Ethnography as a research approach */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18923",
                        "#text": "[[File:ConceptEthnography.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Ethnography]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br>\n\n'''Annotation''': Ethnography is both a process and an outcome - the final written product - of qualitative research (3). This article will focus on the process, giving an overview of different kinds of ethnographic research and its correlating methods.<br>\n\n'''In short:''' Ethnography encompasses diverse methodological approaches to gathering field data on social structures and phenomena.\n\n== Background ==\n[[File:Ethnography.png|400px|thumb|right|'''SCOPUS hits per year for Ethnography until 2020.''' Search terms: 'Ethnography', 'Ethnographic' in Title, Abstract, Keywords. Source: own.]]\n'''Ethnography can be regarded one of the most important qualitative research methods''' that looks back on a long tradition but also many transitions. The foundations of modern Ethnography reach back about a hundred years. Until 1900, ethnographic information mostly originated from the collection of anthropological artifacts and descriptions of indigenous communities that were collected and reported by amateurs, e.g. missionaries or travellers, and subsequently evaluated by 'armchair' anthropologists. By the beginning of the 20th Century, then, anthropologists began to go into the field and get in contact with people themselves instead of relying on second-hand information (1).\n\nAn influential figure for the subsequent development of Ethnography was '''Bronislaw Malinowski''', a Polish anthropologist who is considered to be the founder of fieldwork and participant observation methods relevant to Ethnography to this day (1, 8). He invested himself in 'classical' ethnographic work, spending months with a Melanesian community and gathering insights that he published in his 1922 work \"Argonauts of the Western Pacific\". He systematically recorded and later taught his approach to fieldwork, which heavily furthered the methodological foundations of anthropology (1).\n\nThe methodological approach to Ethnography was further influenced by the early 20th century work of the '''Chicago School of sociology''', which is also responsible for major developments of interview methodology (see Open Interviews and Semi-structured Interviews). Sociologists in Chicago attempted to study individuals within the city by observing and interviewing them in their everyday lives, and furthered the methodological groundwork for the field this way (1). \n\nOverall, Ethnography is historically and practically most closely related to the discipline of Anthropology and constitutes a defining method of this discipline (8). Still, the theoretical reflections and methodological approaches also apply to research endeavours in other Social Sciences. Today, '''Ethnographic research no longer focuses on investigating 'exotic' communities''', but deals with a diverse range of topics, including media studies, health care, work, education, communication, gender, relations to nature, and others (7, 9).\n\n== What the method does ==\n==== Ethnography as a research approach ====\n'''Ethnography is not strictly a method, but rather a \"culture-studying culture\"''' (Spradley (2), p.9). It is a scientific approach to how research should be conducted that includes a set of methods, but also a set of theoretical considerations on how to apply these methods (1). Ethnography attempts to understand the social world and actions of human beings in a specific cultural and societal surrounding of interest to the researcher (8). The researcher intends to systematically describe this [[Glossary|culture]] and understand another way of life from the 'native point of view' (Spradley (2), p.3). \"Rather than studying people, ethnography means learning from people.\" (Spradley, p.3). According to Malinowski, three aspects are of interest to the researcher: what people say they do (customs, traditions, institutions, structures); what they actually do; and typical ways of thinking and feeling associated with these elements (1). The latter may be expressed directly by the studied individuals, but may also be ''[[Glossary|tacit knowledge]]'', i.e. knowledge that is inherent to the culture but taken for granted and communicated only indirectly through word and action (Spradley (2), p.5). Ethnographic research attempts to infer this knowledge by listening carefully, observing and studying the culture in detail (2, see below).\n\nThe term ''culture'' \"(...) refers to the acquired knowledge that people use to interpret experience and generate social behavior.\" (Spradley (2), p.5). In this regard, not only 'exotic' enclosed societies such as the indigenous community studied by Malinowski are of interest in ethnographic research, but also small-scale cultures such as a classroom, a family or a restaurant (3).\n\n[[File:EthnographyScales.png|500px|thumb|right|'''Variations in research scope in Ethnography.''' Source: Spradley (3), p.30]]\n\n==== Observing & Interviewing ====\nMethodologically, Ethnography is special due to its focus on field work, i.e. gathering data within the context that is to be studied. It is important here that ethnographers immerse themselves in the daily praxis of the context they are studying to gain a 'native' perspective (1). Ethnographers immerse themselves in the social situations they study, and attempt to openly engage with the activities of the daily lives of the individuals of interest while asking them questions, observing, listening to and interacting with (8). '''Field work can therefore include a wide array of activities''', such as\"(...) asking questions, eating strange foods, learning a new language, watching ceremonies, taking field notes, washing clothes, writing letters home, tracing out genealogies, observing play, interviewing informants, and hundreds of other things.\" (Spradley, p.3). The researcher acknowledges the complexity of the social world and attends the studied situation for a substantial period of time to build trust and get acquainted with the individuals involved (1, 3). (Learning and) speaking the native language is an important part of the participation process (8). '''Participant observation is the primary method''' throughout this process. The researcher systematically observes situations according to his/her (current) research questions and takes mental and written notes (10). A diary may be kept to reflect upon the research experiences. Since it is not always clear from the beginning which information may be of interest, the researcher needs to find a balance between noting everything worthwhile and still finding sufficient time to actually conduct the research (1).\n\n[[File:EthnographyResultVisualisation.png.png|600px|thumb|right|'''Exemplary field notes.''' Source: [https://medium.com/media-ethnography/field-notes-and-participant-observation-in-ethnographic-studies-a-skill-summary-bb74e3881258 MEDIUM]]]\n\n'''Observations are often supplemented by qualitative ethnographic interviews''' to gain a deeper understanding into previously observed situations. These are a form of [[Open Interview|open interview]] that focus on how the interviewees classify and describe their experiences and positions concerning their social context. Interviews may take place in-between observations, or in dedicated, set-up interview situations, and also with groups of interviewees (1). The Interviewees may be asked about broad or specific situations. Elements that may be learned about are: people involved, places used, individual acts, groups of acts that combine into activities or routines, events, objects, goals, time and feelings (4). The ethnographic interview differs from standard open interviews in that it tries not to impose any pre-conceived notions and structures on how the interviewee might view, define or classify these elements according to his/her worldview. Instead, the questions are formulated so that the interview is almost entirely guided by the interviewee's responses (1, 5). This way, the researcher may be able to extract insight into \"(...) contextual understandings, shared assumptions and common knowledge upon which a respondent's answers are based (...). Ethnographic questions are used to elicit the perceptions and knowledge that guide behavior, while discouraging individuals from translating this information into a form corresponding to the researcher's revealed understanding and language.\" (Johnston et al. 1995, p.57f). In such an interview, the power relation between researcher and interviewee is shifted, because the researcher does not have much that he/she wants to learn about, but the interviewee has all the information to offer that is of interest to the researcher. Therefore, a trustful relationship between the researcher and the interviewee is of special importance (4, see Normativity).\n\n==== Inductive research ====\n[[File:EthnographyCircularProcess.png|600px|thumb|left|'''The Ethnographic Research Cycle.''' Source: Spradley (3), p.29]]\n\n'''Ethnography is thus a very open and inductive process''', with the researcher acting like an explorer who does not rely on strictly pre-defined questions leading his/her research, but rather goes into the field openly and develops new questions as the first results emerge from the data collected after some time (1, 3, 8). In this reflexive practice, the research design continuously evolves during the study. The research is done in a circular process, which sets ethnographic research apart from classical theory-led, linear social science approaches (3, see Figure below). The scope of the research is decreased with every circulation: In terms of the research questions, the researcher first asks rather general ''descriptive'' questions about the situation at hand. The data is analyzed and based on the results, the focus is narrowed down: next, ''structural'' questions are asked, before ''contrast'' questions are used in the next step to further reduce the scope of the research design. The same applies to the data collection: Initially, the observations are rather descriptive, but become more and more focused and selective as the ethnographic research continues (3)\n\nThe data gathered in Ethnography may be quantitative (e.g. statistical summaries of specific actions), but are primarily qualitative, e.g. photographies, audio files, maps, descriptions of phenomena, structures and ideas, or even objects (1, 9). Overall, therefore, ethnographic methodology may be defined as qualitative and inductive, focusing on the present individual while allowing for inferences on the past and the whole societal system that is observed.\n\n== Strengths & Challenges ==\nThe focus of Ethnography on gathering data in the 'natural' context is crucial for the quality of the results. Being in the very situation and watching what people do and learning about their thoughts on the situation as it happens allows for more insightful conclusions. The alternative - having individuals report in a dedicated, external setting, before or after the situation happening - might be biased since people do not always do what they say they do (1). For further challenges, see Normativity.\n\n== Normativity ==\n* Early ethnographic work focused on the understanding and exploration of 'exotic' communities, 'hidden' somewhere in a different part of the world, as part of colonial interventions. Today, Ethnography has shifted, and any cultural or societal setting may be analyzed using ethnographic methods. Even seemingly mundane situations in cultural contexts more familiar to the researcher may reveal 'strange' and 'exotic' elements when analyzed thoroughly (1, 8). This realisation emphasizes that 'reality' is not the same to all people. While this idea of ''naive realism'' is a tempting assumption, it should be set aside for ethnographic research which attempts to learn about what different elements of life - words, but also concepts - mean to people in different social and cultural settings (2). As Spradley puts it: \"Ethnography starts with a conscious attitude of almost complete ignorance.\" (Spradley, p.4) This new perspective on life is generally interesting as it scrutinizes what is normal and what is not.\n* Ethnography can be seen as a powerful tool to inform people about other people's lifeworlds, connect societies and broaden perspectives (see (2)). It may therefore be helpful for sustainable development which relies on the acceptance and incorporation of diverse perspectives and (sometimes conflicting) demands.\n* O'Reilly (1) discusses implications of ethical field work. For ethical reasons, the researcher should not disguise his/her presence but be open about his/her role and research intent. Consent should be given by all individuals studied and disclosure on the subsequent usage of the data as well as confidentiality should be provided. At the same time, being too open and transparent might complicate the immersion of the researcher in the studied situation, thus negatively affect the data gathered, or even make it impossible due to the iteractive nature of the research process. The researcher should attempt to balance openness so that no harm is done, but not constantly remind everyone of his/her role as a researcher in order to ensure useful research results. For further elaborations on ethical considerations, refer to O'Reilly (1).\n\n'''Quality criteria'''\n* Malinowski emphasized that the observations done in the field should not be conducted randomly, but systematically. They should not only focus on the extraordinary elements of each situation, but rather provide a comprehensive collection of all individual elements. This also involves the detailed, written description of the context and setting as well as the methods of the observation (1).\n* Time is a crucial factor for observation since it takes some time for the researcher, being an outsider to the analyzed context, to get acquainted with the situation and gain a feeling of the people's perspective (8). This also reduces the risk of the people behaving differently than they usually would due to the researcher's presence, since they get used to his/her presence after some time. Additionally, spending a sufficient amount of time with the situation of interest allows for the researcher to change the directions of the research and narrow down the research focus after first conclusions emerge (see What the method does) (1).\n* Participation is an important element of ethnographic fieldwork. Instead of relying only on external observations, the researcher should join the observed people and get in contact with the respective situations to get a better feel for an insider's perspectives. However, this participation might influence the 'objectivity' of the observation (1).\n\n== Outlook ==\nThe book \"Digital Environments' (6) reflects upon the future development of digital anthropology in the face of the increasing role of 'digital environments' as a sphere for social interaction, which raises new methodological challenges for researchers investigating communities in this field.\n\nBrewer (2001) claims that \"(...) [g]lobalization poses a threat to Ethnography\", stating that \"[l]ocal 'fields' as sites for interesting and innovative social action and particularistic social meanings, which ethnography once explored, get subsumed under the homogenization that occurs with globalization. Globalization creates a cultural glob in which there is no space for difference, and thus for ethnography's stress on bounded fields as sites for localized social meanings.\" However, he sees a future for Ethnography in this regard: \"]E]thnography's role under globalization is to: chart the experience of people in a local setting to demonstrate how global\nprocesses are mediated by local factors; address the persistence of tradition; describe how traditional identities interface with globally structured ones.\" The role of Ethnography under globalizational processes may change, but the method must not become obsolent.\n\n== Key Publications == \nMalinowski, B. 1922. Argonauts of the Western Pacific. An Account of Native Enterprise and Adventure in the Archipelagoes of Melanesian New Guinea. Routledge London, New York. Available at http://www.bohol.ph/books/Argonauts/Argonauts.html (last accessed on 15.07.2020)\n* The original work of Malinowski.\n\nO'Reilly, K. 2005. Ethnographic Methods. Routledge Oxon.\n* An extensive description of how Ethnography is applied.\n\nBrewer, John D. 2001. Ethnography. Understanding Social Research. Open University Press. \n* A compact overview on Ethnography.\n\n== References ==\n(1) O'Reilly, K. 2005. ''Ethnographic Methods.'' Routledge Oxon.\n\n(2) Spradley, J.P. 2016. ''The Ethnographic Interview.'' Waveland Press.\n\n(3) Spradley, J.P. 2016. ''Participant Observation.'' Waveland Press.\n\n(4) Westby, C. Burda, A. Mehta, Z. 2003. ''Asking the Right Questions in the Right Ways. Strategies for Ethnographic Interviewing.'' The ASHA Leader 8(8). 4-17.\n\n(5) Johnston, R.J. Weaver, T.F. Smith, L.A. Swallow, S.K. 1995. ''Contingent Valuation Focus Groups: Insights From Ethnographic Interview Techniques.'' Agricultural and Resource Economics Review 24. 56-69.\n\n(6) Fr\u00f6mming, U.U. K\u00f6hn, S. Fox, S. Terry, M. (eds). 2017. ''Digital Environments. Ethnographic Perspectives Across Global Online and Offline Spaces.'' transcript Verlag, Bielefeld.\n\n(7) Brewer, J.D. 2003. ''The future of ethnography.'' Qualitative Social Work 1. 245-249.\n\n(8) Mader, E. et al. ''Einf\u00fchrung und Pr\u00e4podeutikum Kultur- und Sozialanthropologie.'' Available at [https://www.univie.ac.at/sowi-online/esowi/cp/einfpropaedksa/einfpropaedksa-1.html](https://www.univie.ac.at/sowi-online/esowi/cp/einfpropaedksa/einfpropaedksa-1.html) (last accessed on 21.07.2020)\n\n(9) Atkinson, P. Delamont, S. Coffey, A. 2007. ''Handbook of Ethnography.'' London et al.: Sage.\n\n(10) Creswell, J. 2013. ''Qualitative Inquiry and Research Design.'' London et al.: Sage.\n\n(11) Brewer, J.D. 2001. ''Ethnography. Understanding Social Research.'' Open University Press. \n\n== Further Information ==\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "9q6gxkzw4nz8lun5spmqcspyzju7933"
                }
            },
            {
                "title": "Exceptions in Python",
                "ns": "0",
                "id": "1031",
                "revision": {
                    "id": "7256",
                    "parentid": "7242",
                    "timestamp": "2023-06-30T05:24:15Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to the different kinds of exceptions and how to handle them",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3123",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Exception- good to know==\n\nAn Exception is an error when you execute your code. When a code tried to do something and fail, without proper \u201cexception handling\u201d it will stop and won\u2019t continue its next line. \n\n===Example===\nRun the code below to see different types of exception\n<syntaxhighlight lang=\"Python\" line>\n1/0\n</syntaxhighlight>\nYou will get \"ZeroDivisionError\" when you try to divide a number by 0.\n\n<syntaxhighlight lang=\"Python\" line>\ny = x + 1\n# or\ny = new_function()\n</syntaxhighlight>\nYou will get \"NameError\" if you try to use a variable that has not been defined before.\n\n<syntaxhighlight lang=\"Python\" line>\na = [1,2,3,4,5]\na[10]\n</syntaxhighlight>\nYou will get \"IndexError\" when you try to call an array with an index larger than the array itself.\n\nThere are many more exceptions, which you can find by searching for \u201cpython exception documentation\u201d on the internet. \n\n==Exception Handling==\nAs discussed before, exceptions can be handled better. By using \u201ctry - except\u201d block, we can manually avert the error. \nA basic Try-Except Example will look like this:\n\n<syntaxhighlight lang=\"Python\" line>\n# other code running perfectly fine\n\ntry:\n\t# code that can go wrong\nexcept: \n\t# code to execute when \"try\" block is triggered\n\n# code will still continue when there is an exception\n</syntaxhighlight>\n\nLet's look at Try-Exception in action\n<syntaxhighlight lang=\"Python\" line>\na = 1\ntry: \n\tb = int(input(\"a number to divide a\")) # insert here for example 5 or 0\n\ta = a / b\n\tprint(\"Success, a = \", a)\n\nexcept: \n\tprint(\"There was an error\")\n</syntaxhighlight>\n\nTry using 0 and other input as input to see if you get a success or not. \n\nThere is a way to differentiate reactions based on the exception type. For example, if you want to print different error codes for each of the exceptions. Well, you can!\n\n<syntaxhighlight lang=\"Python\" line>\na = 1\nz=(4,2,9,6)\n#y=1\ntry: \n\tb = int(input(0))\n\ta = (a / b)*y-z[2]\n\tprint(a)\nexcept ZeroDivisionError:\n    a= (a / 1)*1-z[2]\n    print(\"Zero Division Error, check your code for division by 0, 0 replaced by one, y replaced by 1. a= \", a)\nexcept NameError:\n    a= (a / b)*1-z[2]\n    print(\"Name Error, check the variable / function name of your code, y replaced by one, a= \", a)\nexcept IndexError:\n    a= (a / b)*1-1\n    print(\"Index Error, check the array again, z[7] replaced by 1, a= \", a)\n</syntaxhighlight>\nRun the code so that no zero division error occurs. What do you see? What do you see if you take b=0? You can also play a little with the different errors that occur: What do you see if you take z[9], define y, and b=2?\n\n==Quiz==\n# Open the search engine of your choice and look for other exceptions. Pick one and try to add it to the last code in this tutorial\n# Modify the code so that it runs until the end (and add descriptions of exceptions where needed):\n<syntaxhighlight lang=\"Python\" line>\nz=[0,4,6,3,34439,404]\nx= 5/z[0]\na= (x-z[8])**2\n</syntaxhighlight>\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "n9y2w6fy9l4bmco4eem2upkdua5y6pc"
                }
            },
            {
                "title": "Experiments",
                "ns": "0",
                "id": "121",
                "revision": {
                    "id": "5893",
                    "parentid": "5789",
                    "timestamp": "2021-06-27T15:03:07Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* How do I compare more than two groups ? */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26779",
                        "#text": "'''Note:''' This entry revolves mostly around laboratory experiments. For more details on experiments, please refer to the entries on [[Experiments and Hypothesis Testing]], [[Case studies and Natural experiments]] as well as [[Field experiments]].\n\n==History of laboratory experiments==\n[[File:Experiment.jpg|thumb|right|Experiments are not limited to chemistry alone]]\nExperiments describe the systematic and reproducible design to test specific hypothesis.\n\nStarting with [https://sustainabilitymethods.org/index.php/Why_statistics_matters#The_scientific_method Francis Bacon] there was the theoretical foundation to shift previously widely un-systematic experiments into a more structured form. With the rise of disciplines in the [https://sustainabilitymethods.org/index.php/Why_statistics_matters#A_very_short_history_of_statistics enlightenment] experiments thrived, also thanks to an increasing amount of resources available in Europe due to the Victorian age and other effects of colonialism. Deviating from more observational studies in physics, [https://en.wikipedia.org/wiki/History_of_experiments#Galileo_Galilei astronomy], biology and other fields, experiments opened the door to the wide testing of hypothesis. All the while Mill and others build on Bacon to derive the necessary basic debates about so called facts, building the theoretical basis to evaluate the merit of experiments. Hence these systematic experimental approaches aided many fields such as botany, chemistry, zoology, physics and [http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf much more], but what was even more important, these fields created a body of knowledge that kickstarted many fields of research, and even solidified others. The value of systematic experiments, and consequently systematic knowledge created a direct link to practical application of that knowledge. [https://www.youtube.com/watch?v=UdQreBq6MOY The scientific method] -called with the ignorant recognition of no other methods beside systematic experimental hypothesis testing as well as standardisation in engineering- hence became the motor of both the late enlightenment as well as the industrialisation, proving a crucial link between basically enlightenment and modernity.\n[[File:Bike repair 1.jpg|thumb|left|Taking a shot at something or just trying out is not equal to a systematic scientific experiment.]]\nDue to the demand of systematic knowledge some disciplines ripened, meaning that own departments were established, including the necessary laboratory spaces to conduct [https://www.tutor2u.net/psychology/reference/laboratory-experiments experiments.] The main focus to this end was to conduct experiments that were as reproducible as possible, meaning ideally with a 100 % confidence. Laboratory conditions thus aimed at creating constant conditions and manipulating ideally only one or few parameters, which were then manipulated and therefore tested systematically. Necessary repetitions were conducted as well, but of less importance at that point. Much of the early experiments were hence experiments that were rather simple but produced knowledge that was more generalisable. There was also a general tendency of experiments either working or not, which is up until today a source of great confusion, as an [https://www.psychologydiscussion.net/learning/learning-theory/thorndikes-trial-and-error-theory-learning-psychology/13469 trial and error] approach -despite being a valid approach- is often confused with a general mode of \u201cexperimentation\u201d. In this sense, many people consider preparing a bike without any knowledge about bikes whatsoever as a mode of \u201cexperimentation\u201d. We therefore highlight that experiments are systematic. The next big step was the provision of certainty and ways to calculate [https://sustainabilitymethods.org/index.php/Hypothesis_building#Uncertainty uncertainty], which came with the rise of probability statistics.\n\nFirst in astronomy, but then also in agriculture and other fields the notion became apparent that our reproducible settings may sometimes be hard to achieve. Error of measurements in astronomy was a prevalent problem of optics and other apparatus in the 18th and 19th century, and Fisher equally recognised the mess -or variance- that nature forces onto a systematic experimenter. The laboratory experiment was hence an important step towards a systematic investigation of specific hypothesis, underpinned by newly established statistical approaches.\n\n==Key concepts of laboratory experiments \u2013 sampling data in experimental designs==\nStatistics enabled '''replication''' as a central principle that was first implemented into laboratory experiments. [https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/doe/supporting-topics/basics/replicates-and-repeats-in-designed-experiments/ Replicates] are basically the repetition of the same experiment in order to derive whether an effect is constant or has a specific variance. This variance is an essential feature of many natural phenomena, such as plant growth, but also caused by systematic errors such as measurement uncertainty. Hence the validity and reliability of an experiment could be better tamed. \n\nWithin laboratory experiment, '''control''' of certain variables is essential, as this is the precondition to statistically test the few variables that are in the focus of the investigation. [https://www.youtube.com/watch?v=VhZyXmgIFAo Control of variables] means to this end, that such controlled variables are being held constant, thus the variables that are being tested are leading to the variance in the analysis. Consequently, such experiments are also known [https://www.khanacademy.org/math/ap-statistics/gathering-data-ap/statistics-experiments/v/causality-from-study controlled experiments]. \n\nBy increasing the '''[https://sciencing.com/meaning-sample-size-5988804.html sample size]''', it is possible to test the hypothesis according to a certain probability, and to generate a measure of reliability. The larger the sample is, the higher is the statistical power to be regarded. Within controlled experiments the so-called '''[http://www.stat.yale.edu/Courses/1997-98/101/expdes.htm treatments]''' are typically groups, where continuous gradients are converted into factors. An example would be the amount of fertilizer, which can be constructed into \u201clow\u201d, \u201cmiddle\u201d and \u201dhigh\u201d amount of fertilizer. This allows a systematic testing based on a smaller number of replicates. The number of treatments or '''factor levels''' defines the '''[https://www.youtube.com/watch?v=Cm0vFoGVMB8 degrees of freedom]''' of an experiment. The more levels are tested, the higher does the number of samples need to be, which can be calculated based on the experimental design. Therefore, scientists design their experiments very clearly before conducting the study, and within many scientific fields are such experimental designs even submitted to a precheck and registration to highlight transparency and minimize potential flaws or manipulations. \n[[File:Fertilization-campaign-overview-ultrawide.jpg|thumb|right|What increased the yield, the fertilizer or the watering levels? Or both?]]\nSuch experimental designs can even become more complicated when '''[https://statisticsbyjim.com/regression/interaction-effects/ interaction effects]''' are considered. In such experiments, two different factors are manipulated and the interactions between the different levels are investigated. A standard example would be quantification of plant growth of a specific plant species under different watering levels and amounts of fertilizer. Taken together, it is vital for researchers conducting experiments to be versatile in the diverse dimensions of the design of experiments. Sample size, replicates, factor levels, degrees of freedom and statistical power are all to be considered when conducting an experiment. Becoming versatile in designing such studies takes practice.\n\n==How do I compare more than two groups ?==\n[[File:Sunset-field-of-grain-5980.jpg|thumb|right|How to increase the yield systematically?]]\nPeople knew about the weather, soils, fertilizer and many other things, and this is how they could maximize their agricultural yield. Or did they? People had local experience but general [[Glossary|patterns]] of what contributes to a high yield of a certain crop were comparably anecdotal. Before Fisher arrived in agriculture, statistics was less applied. There were applied approaches, which is why the [[Simple_Statistical_Tests#One_sample_t-test|t-test]] is called student test. However, most statisticians back then did statistics, and the agricultural folks did agriculture.\n\n[https://www.britannica.com/biography/Ronald-Aylmer-Fisher Fisher] put these two things together, and they became one. His core question was how to increase yield. For this, there was quite an opportunity at that time. Industrialisation contributed to an exponential growth of many, many things (among them artificial fertilizer) which enabled people to grow more crops and increase yield. Fisher was the one who made the question of how to increase yield systematically. '''By developing the Analysis of Variance ([[ANOVA]]), he enabled comparison of more than two groups in terms of a continuous phenomenon.''' He did this in a way that you could, for example, compare which fertilizer would produce the highest yield. With Fisher's method, one could create an experimental design, fertilize some plants a little bit, some plants more, and others not at all. This enabled research to compare what is called different treatments, which are the different levels of fertilizer; in this case: none, a little, and more fertilizer. This became a massive breakthrough in [https://www.youtube.com/watch?v=9JKY74fPNVM agriculture], and Fisher's basic textbook became one of the most revolutionary methodological textbooks of all time, changing agriculture and enabling exponential growth of the human population.\n[[File:Yield.jpg|thumb|400px|left|Does the soil influence the yield? And how do I find out if there is any difference between clay, loam and sand? Maybe try an ANOVA...]]\nOther disciplines had a similar problem when it came to comparing groups, most prominently [https://qsutra.com/anova-in-pharmaceutical-and-healthcare/ medicine] and psychology. Both disciplines readily accepted the ANOVA as their main method in early systematic experiments, since comparing different treatments is quite relevant when it comes to their experimental setups. Antibiotics were one of the largest breakthroughs in modern medicine, but how should one know which antibiotics work best to cure a specific disease? The ANOVA enables a clinical trial, and with it a systematic investigation into which medicine works bests against which disease. This was quite important, since an explanation on how this actually works was often missing up until today. Much of medical research does not offer a causal understanding, but due to the statistical data, at least patterns can be found that enable knowledge. This knowledge is, however, mostly not causal, which is important to remember. The ANOVA was an astounding breakthrough, since it enabled pattern recognition without demanding a causal explanation. \n\n====Analysis of Variance====\n'''The [https://www.investopedia.com/terms/a/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment. \n\nThe general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.\n\n====Preconditions====\nRegarding the preconditions of the [https://www.youtube.com/watch?v=oOuu8IBd-yo ANOVA], it is important to realize that the data should ideally be '''normally distributed''' on all levels, which however is often violated due to small sample sizes. Since a non-normal distribution may influence the outcome of the test, boxplots are a helpful visual aid, as these allow for a simple detection tool of non-normal distribution levels. \n\nEqually should ideally the variance be comparable across all levels, which is called '''[https://blog.minitab.com/blog/statistics-and-quality-data-analysis/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia homoscedastic]'''. What is also important is the criteria of '''independence''', meaning that samples of factor levels should not influence each other. For this reason are for instance in ecological experiments plants typically planted in individual pots. In addition does the classical ANOVA assume a '''balanced design''', which means that all factor levels have an equal sample size. If some factor levels have less samples than others, this might pose interactions in terms of normals distribution and variance, but there is another effect at play. Larger sample sizes on one factor level may create a disbalance, where factor levels with larger samples pose a larger influence on the overall model result. \n\n====One way and two way ANOVA====\nSingle factor analysis that are also called '[https://www.youtube.com/watch?v=nvAMVY2cmok one-way ANOVAs]' investigate one factor variable, and all other variables are kept constant. Depending on the number of factor levels these demand a so called [https://en.wikipedia.org/wiki/Latin_square randomisation], which is necessary to compensate for instance for microclimatic differences under lab conditions.\n\nDesigns with multiple factors or '[https://www.thoughtco.com/analysis-of-variance-anova-3026693 two way ANOVAs]' test for two or more factors, which then demands to test for interactions as well. This increases the necessary sample size on a multiplicatory scale, and the degrees of freedoms may dramatically increase depending on the number of factors levels and their interactions. An example of such an interaction effect might be an experiment where the effects of different watering levels and different amounts of fertiliser on plant growth are measured. While both increased water levels and higher amounts of fertiliser right increase plant growths slightly, the increase of of both factors jointly might lead to a dramatic increase of plant growth.\n\n====Interpretation of ANOVA====\n[[File:Bildschirmfoto 2020-05-15 um 14.13.34.png|thumb|These boxplots are from the R dataset ToothGrowth. The boxplots which you can see here differ significantly.]]\nBoxplots provide a first visual clue to whether certain factor levels might be significantly different within an ANOVA analysis. If one box within a boxplot is higher or lower than the median of another factor level, then this is a good rule of thumb whether there is a significant difference. When making such a graphically informed assumption, we have to be however really careful if the data is normally distributed, as skewed distributions might tinker with this rule of thumb. The overarching guideline for the ANOVA are thus the p-values, which give significance regarding the difference between the different factor levels. \n\nIt can however also be relevant to compare the difference between specific groups, which is made by a '''[https://www.statisticshowto.com/post-hoc/ posthoc test]'''. A prominent example is the [https://sciencing.com/what-is-the-tukey-hsd-test-12751748.html Tukey Test], where two factor levels are compared, and this is done iteratively for all factor level combinations. Since this poses a problem of multiple testing, there is a demand for a [https://www.statisticshowto.com/post-hoc/ Bonferonni correction] to adjust the p-value. Mechanically speaking, this is comparable to conducting several t-tests between two factor level combinations, and adjusting the p-values to consider the effects of multiple testing.\n\n====Challenges of ANOVA experiments====\nThe ANOVA builds on a constructed world, where factor levels are like all variables constructs, which might be prone to errors or misconceptions. We should therefore realize that a non-significant result might also be related to the factor level construction. Yet a potential flaw can also range beyond implausible results, since ANOVAs do not necessarily create valid knowledge. If the underlying theory is imperfect, then we might confirm a hypothesis that is overall wrong. Hence the strong benefit of the ANOVA - the systematic testing of hypothesis - may equally be also its weakest point, as science develops, and previous hypothesis might have been imperfect if not wrong. \n\nFurthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the deductive world become intertwined, and this poses a risk for the validity of scientific results.\n\nFor more on the Analysis of Variance, please refer to the [[ANOVA]] entry.\n\n==Examples==\n[[File:Guinea pig computer.jpg|thumb|right|The tooth growth of guinea pigs is a good R data set to illustrate how the ANOVA works]]\n===Toothgrowth of guinea pigs===\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the ToothGrowth data set is about: ?ToothGrowth\n\n#The code is partly from boxplot help (?boxplot). If you like to know the meaning of the code below, you can look it up there\ndata(ToothGrowth)\n\n# to create a boxplot \nboxplot(len ~ dose, data = ToothGrowth,\n                           boxwex = 0.25, \n                           at = 1:3 - 0.2,\n                           subset = supp == \"VC\", col = \"yellow\",\n                           main = \"Guinea Pigs\u2019 Tooth Growth\",\n                           xlab = \"Vitamin C dose mg\",\n                           ylab = \"tooth length\",\n                           xlim = c(0.5, 3.5), ylim = c(0, 35), yaxs = \"i\")\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n                           boxwex = 0.25,\n                           at = 1:3 + 0.2,\n                           subset = supp == \"OJ\", col = \"orange\")\nlegend(2, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c (\"yellow\", \"orange\"))\n\n#to apply an ANOVA\nmodel1<-aov(len ~ dose*supp, data = ToothGrowth)\nsummary(model1) \n#Interaction is significant\n</syntaxhighlight>\n\n====Insect sprays====\n[[File:A man using RIPA insecticide to kill bedbugs Wellcome L0032188.jpg|thumb|right|To find out, which insectide works effictively, you can approach an ANOVA]]\n<syntaxhighlight lang=\"R\" line>\n#To find out, what the InsectSprays data set is about: ?InsectSprays\ndata(InsectSprays)\nattach(InsectSprays)\ntapply(count, spray, length)\nboxplot(count~spray) \n# can you guess which sprays are effective by looking at the boxplot?\n# to find out which sprays differ significantly without applying many t-tests, you can use a postdoc test\nmodel2<-aov(count~spray)\nTukeyHSD(model2)\n# compare the results to the boxplot if you like\n</syntaxhighlight>\n\n==Balanced vs. unbalanced designs==\nThere is such a thing as a perfect statistical design, and then there is reality.\n\nStatistician often think in so called balanced designs, which indicate that the samples across several levels were sampled with the same intensity. Take three soil types, which were sampled for their agricultural yield in a ''mono crop''. Ideally, all soil types should be investigated with the same amount of samples. If we would have three soil types -clay, loam, and sand- we should not sample sand 100 times, and clay only 10 times. If we did, our knowledge about sandy soils would be much higher compared to clay soil. This does not only represent a problem when it comes to the general knowledge, but also creates statistical problems. \n\nFirst of all, the sandy soil would be represented much more in an analysis that does not or cannot take such an unbalanced sampling into account. \n\nSecond, many analysis have assumptions about a certain statistical distribution, most notably the normal distribution, and a smaller sample may not show a normal distribution, which in turn may create a [[Bias in statistics|bias]] within the analysis. In order to keep this type of bias at least constant across all levels, we either need a balanced design, or use an analysis that compensates for such unbalanced designs. This analysis was realised with so called Type III ANOVA, which can take different sampling intensities into account. Type III ANOVA corrects for the error that is potentially inferred due to differing sample density, that means number of samples per level.\n\n'''Why is this relevant, you ask?''' \n\nBecause the world is messy. Plants die. So do animals. Even people die. It is sad. For a statistician particularly because it makes one miss out on samples, and dis-balances your whole design. And while this may not seem like a bad problem for laypeople, for statistician it is a real mess. Therefore the ways to deal with unbalanced designs were such a breakthrough, because they finally allowed the so neatly thinking statisticians to not only deal with the nitty-gritty mess of unbalanced designs, but with real world data. \n\nWhile experimental designs can be generated to be balanced, the real world data is almost never balanced. Hence, the ANOVA dealing with unbalanced data was one substantial step towards analysing already existing data, which is extraordinary, since the ANOVA was originally designed as the sharpest tool of the quantitative branch of science. Suddenly we were enabled to analyse real world data, often rather large, and often without a clear predefined hypothesis. \n\nI (Henrik) propose that this was a substantial contributon away from a clear distinction between inductive or deductive research. People started analysing data, but following the tradition of their discipline still had to come up with hypotheses, even if they only saw patterns after the analysis. While this paved the road to machine learning, scientific theory still has to recover from it, I say. Rule of thumb: Always remember that you have hypotheses before a study, everything else is inductive, which is also ok.\n\n==External Links==\n\n====Articles====\n\n[https://sustainabilitymethods.org/index.php/Why_statistics_matters#A_very_short_history_of_statistics The Enlightenment]: Also some kind of repetition\n\n[https://en.wikipedia.org/wiki/History_of_experiments#Galileo_Galilei History of experiments in astronomy]: A short but informative text\n\n[http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf History of the Clinical Laboratory]: A brief article\n\n[https://www.tutor2u.net/psychology/reference/laboratory-experiments Laboratory Experiments]: Some strengths and weaknesses\n\n[https://www.psychologydiscussion.net/learning/learning-theory/thorndikes-trial-and-error-theory-learning-psychology/13469 Trial and Error Approach]: A very detailed explanation\n\n[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/doe/supporting-topics/basics/replicates-and-repeats-in-designed-experiments/ Replicates]: A detailed explanation\n\n[https://sciencing.com/meaning-sample-size-5988804.html Sample size]: Why it matters!\n\n[http://www.stat.yale.edu/Courses/1997-98/101/expdes.htm Treatments in Experiments]: Some definitions for terms in experimental design\n\n[https://statisticsbyjim.com/regression/interaction-effects/ Interaction effects]: An article with many examples\n\n[https://www.britannica.com/biography/Ronald-Aylmer-Fisher Ronald Fisher]: A short biography\n\n[https://qsutra.com/anova-in-pharmaceutical-and-healthcare/ ANOVA in pharmaceutical and healthcare]: Examples from real life\n\n[https://www.investopedia.com/terms/a/anova.asp ANOVA]: An introduction\n\n[https://www.thoughtco.com/analysis-of-variance-anova-3026693 One-Way ANOVA vs. Two-Way ANOVA]: Some different types\n\n[https://blog.minitab.com/blog/statistics-and-quality-data-analysis/dont-be-a-victim-of-statistical-hippopotomonstrosesquipedaliophobia Homoscedasticity]: A quick explanation\n\n[https://www.statisticshowto.com/post-hoc/ Posthoc Tests]: A list of different types\n\n[https://sciencing.com/what-is-the-tukey-hsd-test-12751748.html The Tukey Test]: A short article about this posthoc test\n\n====Videos====\n\n[https://www.youtube.com/watch?v=UdQreBq6MOY The Scientific Method]: An insight into Bacons, Galileos and Descartes thoughts\n\n[https://www.youtube.com/watch?v=VhZyXmgIFAo Controlled Experiments]: A short example from biology\n\n[https://www.khanacademy.org/math/ap-statistics/gathering-data-ap/statistics-experiments/v/causality-from-study What is good experiment?]: A very short and simple video\n\n[https://www.youtube.com/watch?v=Cm0vFoGVMB8 Degrees of Freedom]: A detailed explanation\n\n[https://www.youtube.com/watch?v=9JKY74fPNVM Ronald Fisher]: A short introduction\n\n[https://www.youtube.com/watch?v=oOuu8IBd-yo ANOVA]: A detailed explanation\n\n[https://www.youtube.com/watch?v=nvAMVY2cmok One-Way ANOVA vs. Two-Way ANOVA]: A short comparison\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "pjh1mwfvi9d1put6dlkxusnhg9q814q"
                }
            },
            {
                "title": "Experiments and Hypothesis Testing",
                "ns": "0",
                "id": "539",
                "revision": {
                    "id": "6721",
                    "parentid": "6358",
                    "timestamp": "2022-06-15T20:35:20Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "36194",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Experiments and Hypothesis Testing (German)]]<br/>\n\n'''Note:''' This entry is a brief introduction to experiments. For more details, please refer to the entries on [[Experiments]], [[Case studies and Natural experiments]] as well as [[Field experiments]].\n\n'''In short:''' This entry introduces you to the concept of hypotheses and how to test them systematically.\n\n[[File:Concept visualisation - Experiments.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Experiments]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n== Hypothesis building ==\n[[File:James Lind, A Treatise on the Scurvy, 1757 Wellcome M0013130.jpg|thumb|James Lind's \"A Treatise on Scurvy\", in which he systematically assessed what's most useful healing scurvy.]]\n\n'''Life is full of hypotheses.''' We are all puzzled by an [https://www.youtube.com/watch?v=HZ9xZHWY0mw endless flow of questions]. We constantly test our surrounding for potential ''what ifs'', cluttering our mind, yet more often is our cluttered head unable to produce any generalizable knowledge. Still, our brain helps us filter our environment, also by constantly testing questions - or hypotheses - to enable us to better cope with the challenges we face. It is not the really the big questions that keep us pre-occupied every day, but these small questions keep us going.\n \nDid you ever pick the wrong food in a restaurant? Then your brain writes a little note to itself, and next time you pick something else. This enables us to find our niche in life. However, at some point, [https://www.sciencedirect.com/science/article/pii/S0092867408009537 people started observing patterns in their environment] and communicated these to other people. Collaboration and communication were our means to become communities and much later, societies. This dates back to before the dawn of humans and can be observed for other animals as well. Within our development, it became the backbone of our modern civilization \u2013 for better or worse - once we tested our questions systematically. \nJames Lind, for example, conducted one of these [https://journals.sagepub.com/doi/pdf/10.1177/014107680309601201 first systematic approaches] at sea and managed to find a cure for the disease scurvy, which caused many problems to sailors in the 18th century.\n \n==== Hypothesis Testing ====\n[[File:Bildschirmfoto 2020-01-17 um 11.18.08.png|thumb|left|Here you can see one example for both the H0 and the H1.]]\n\nSpecific questions rooted in an initial idea, a theory, were thus tested repeatedly. Or perhaps it's better to say these hypotheses were tested with replicates and the influences disturbing our observation were kept constant. This is called <strong><i>testing of hypothesis</i></strong>, the repeated systematic investigation of a preconceived theory, where during the testing ideally everything is kept constant except for what is being investigated. While some call this the \"[https://www.youtube.com/watch?v=ptADSmJCVwQ scientific method],\" Many researchers are not particularly fond of this label, as it implies that it is THE [https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology scientific method], which is certainly not true. Still, the systematic [https://www.youtube.com/watch?v=ZzeXCKd5a18 testing of the hypothesis] and the knowledge process that resulted from it was certainly a scientific revolution (for more information, [[History_of_Methods|click here]]). \n\nWhat is important regarding this knowledge is the notion that a confirmed hypothesis indicates that something is true, but other previously unknown factors may falsify it later. This is why hypothesis testing allows us to approximate knowledge, many would argue, with a certain confidence, but [https://www.brgdomath.com/philosophie/erkenntnistheorie-tk10/falsifikation-popper/ we will never truly know something]. While this seems a bit confusing, a prominent example may help illustrate this. Before discovering Australia, all swans known in Europe were white. With the discovery of Australia, and to the surprise of Natural History researchers at the time, black swans were discovered. Hence before that, everybody would have hypothesized: [https://www.youtube.com/watch?v=wf-sGqBsWv4 all swans are white]. Then Australia was discovered, and the world changed, or at least, the knowledge of Western cultures about swans. \n\n'''At this point it is important to know that there are two types of hypotheses.''' Let us imagine that you assume, there are not only white swans in the world, but also black swans.\nThe null hypothesis (H0) states that this is not the case - everything is as we assumed before you came around the corner with your crazy claim. Therfore, the null hypothesis would be: all swans are white. The alternative hypothesis (H1) now challenges this null hypothesis: there are also black swans. This is what you claimed in the first place. We do not assume however right away that whatever you came up with is true, so we make it the alternative hypothesis. The idea is now to determine whether there is enough evidence to support the alternative hypothesis. To do so, we test the likelihood of the H0 being true in order to decide whether to accept the H0, or if we reject it and rather accept the H1. Both go together - when we accept one of both hypotheses, we simultaneously reject the other hypothesis, and vice-versa. Caution: Hypotheses can only be rejected, they cannot be verified based on data. When it is said that a hypothesis is 'accepted' or 'verified' or 'proven', this only means that the alternative cannot be supported, and we therefore stick to the more probable hypothesis.\n\n====Errors in Hypothesis Testing====\n[[File:Bildschirmfoto 2020-01-17 um 10.36.55.png|thumb|right|This table shows you the correct assignment of the two different types of error regarding the pregancy example.]]\nIn hypothesis testing, there are [https://www.youtube.com/watch?v=a_l991xUAOU two different errors] that can occur. The false positive (\"Error Type I\") and false negative (\"Error Type II\").\nIf a doctor diagnoses someone to be pregnant and the person is not, this is a typical false positive error. The doctor has claimed that the person is pregnant, which is not the case. The other way around, if the doctor diagnoses someone not to be pregnant, but in fact, that person is, this is a typical false negative error. Each and everyone may judge by themselves which kind of error is more hazardous. Nevertheless, both errors are to be avoided. During the Corona crisis, this became an important topic, as many of the tests were often wrong. If someone with Corona showed symptoms but tested negative, it was a false negative. This depends seemingly on the time of testing and the respective tests. Antibody tests are only relevant after someone had Corona, but sometimes even these may not find any antibodies. During an active case only a PCR can confirm an infection, but taking a sample is tricky, hence again there were some false negatives. False positives are however very rare, as the error rate of the test to this end is very low. Still, sometimes samples got messed up. Rarely, but it happened, creating a big stir in the media.\n\n====Knowledge may change over time====\nIn modern science great care is taken to construct hypotheses that are systematically sound, meaning that black swans are not to be expected, but sometimes they cannot be prevented. An example is whether a medication is suitable for pregnant women. For ethical reasons, this is hardly ever tested, which had severely negative impacts in the past. Hence, knowledge of how a specific medication affects pregnant women is often based on experience, but not systematically investigated. Taken together, hypotheses become a staple of modern science, although they represent only one concept within the wide canon of scientific methods. The dependency on [[:Category:Deductive|deductive]] approaches has in the past also posed problems, and may indirectly lead to building a wall between knowledge created by [[Design Criteria of Methods|inductive vs. deductive]] approaches. \n\n==== An example: Finding a vaccine for Covid-19 ====\nMany consider a vaccine to be the safest end of the pandemic. Only time will tell if this is actually true, yet research points in a clear direction. We thus have the hypothesis, but we did not test the hypothesis yet, as it was not possible. More importantly, almost all vaccines currently run through clinical trials that test so many more hypotheses. Is the vaccine safe for people, are there no side effect, allergic reaction or other severe medical problems? Does the vaccine create an immune response? And does it affect different age groups or other groups differently? In order to safely test these strategies, clinical trials emerged over the last decades as the gold standard in modern day experiments. Many may criticise modern science, and question much of what science has done to contribute not only to positive development in our society. Nevertheless, many of us owe our lives to such clinical trails - the safe and sound testing of scientific hypotheses, under very reproducible conditions. Much of modern medicine, psychology, product testing and also agriculture owe their impact onto our societies to the experiment.\n\n==== Key messages ====\n\u2022  Hypothesis testing is the repeated systematic investigation of a preconceived idea through observation\n\n\u2022  There is a differentiation between the null hypothesis (H0), which we try to reject, and alternative hypothesis (H1)\n\n\u2022  There are two different errors that can occur, false positive (\"Error Type I\") and false negative (\"Error Type II\")\n\n\n== Validity ==\n[[File:Bildschirmfoto 2020-04-24 um 12.42.47.png|thumb|This picture shows the famous dartboard example which explains the relationship of validity and reliability. The bulls eye represents the accuracy of our measures (validity), whereas the positions of the darts represent the precision of our hits (reliability).]]\n\nValidity is derived from the Latin word \"validus\" which translates as \"strong\". It is thus a [https://explorable.com/statistical-validity central concept within hypothesis testing], as it qualifies to which extent a hypothesis is true. Its meaning should not be confused with reliability which indicates whether certain results are consistent, or in other words reproducible. Validity instead indicates the extent to which a hypothesis can be confirmed. As a consequence, it is central for the deductive approach in science; in other words: it should not be misused beyond that line of thinking. Validity is therefore strongly associated with reasoning, which is why it indicates the overall strength of the outcome of a statistical result. \n\nWe should however be aware that validity encompasses the [https://www.youtube.com/watch?v=F6LGa8jsdjo whole process of the application of statistics]. '''Validity spans from the postulation of the original hypothesis over the methodological design, the choice of analysis and finally the interpretation.''' Validity is hence not an ''all or nothing''. Instead we need to learn the diverse approaches and flavours of validity, and only experience across diverse datasets and situations will allow us to judge whether a result is valid or not. This is one of the situations where statistics and philosophy are maybe the closest. In philosophy it is very hard to define reason, and much of Western philosophy has been indeed devoted to reason. While it is much easier to define unreasonable acts (and rationality is altogether a different issue), reason in itself is more complex, many would argue. Consequently, validity of the question whether the confirmation of a hypothesis is reasonable, is much harder to answer. Take your time to learn about validity, and take even more time to gain experience on wether a result is valid.\n\n\n==Reliability==\n[https://www.scribbr.com/methodology/reliability-vs-validity/ Reliability] is one of the key concepts of statistics. It is defined as the consistency of a specific measure. '''A reliable measure is hence producing comparable results under the same conditions.''' Reliability gives you an idea whether your analysis is accurate under the given conditions and also reproducible. We already learned that most, if not all, measures in statistics are constructed. Reliability is central since we can check whether our constructed measures bring us on the right track. In other words, reliability allows us to understand if our constructed measures are working.\n\nThe question of [https://explorable.com/statistical-reliability reproducibility] which is associated to reliability is a [https://www.youtube.com/watch?v=m0W8nnupcUk central aspect of modern science]. Reliability is hence the concept that contains at least several components. \n\n1. The extent and complexity of the measure that we include in our analysis. While it cannot be generally assumed that [[To Rule And To Measure|extremely constructed measures such as GDP or IQ]] are less reliable than less constructed ones, we should still consider Occam's razor when thinking about the reliability of measures, i.e. keep your constructs short and simple. \n\n2. Relevant for reliability is the question of our [[sampling|sample]]. If external conditions of the sample change then, due to new information, reliability might decrease. An easy example would again be swans. If we state that there are just white swans, this hypothesis is reliable as long as there are just white swans sampled. But when the external conditions change - for example when we discover black swans in Australia - the reliability of our first hypothesis decreases.\n\n3. Furthermore, sampling can also be affected. If sampling is done by several observers then a reduction in reliability may be explained by the differences of the respective observers. \n\n4. We have to recognise that different methodological approaches - or more specifically in our case, statistical tests - may consist of different measures of reliability. Hence the statistical model of our analysis might directly translate into different reliabilities. An easy example to explain this a bit better is the one sample test with a perfect coin. This test should work but it does not work anymore when the coin is biased. Then we have to consider a new method/model to explain this (namely the \"two sample test\"). The important thing to remember is: The choice of your method/model has an impact on your test results. \n\n5. The last aspect of reliability is related to our theoretical foundation itself. If our underlying theory is revised or changed, then this might tinker with the overall reliability of our results. This is important to highlight, since we might have an overall better theoretical foundation within a given statistical analysis, however the statistical measure of reliability might be lower. This is a source of continuous confusion among apprentices in statistics. \n\nThe question how reliable statistical analysis or better the result of a scientific study is, is in my experience the source of endless debates. There is no [https://www.statisticssolutions.com/directory-of-statistical-analyses-reliability-analysis/ measure of reliability] that is accepted by everybody. Instead, statistics are still evolving, and measures of reliability change. While this seems confusing at first, we should differentiate between results that are extremely reliable and results that are lower in terms of reliability. [https://opinionator.blogs.nytimes.com/2012/05/17/how-reliable-are-the-social-sciences/ Results from hard science] such as physics often show a high reliability when it comes to statistical testing, while soft science such as psychology and economics can be found at the lower end of reliability. There is no normative judgement employed here, since modern medicine often produces results that are reliable, but only just so. Consider research in cancer. Most current breakthroughs change the lives of a few percentages of patients and there is hardly a breakthrough that can affect a larger proportion of patients. We thus have to accept reliability in statistics as a moving target, and it depends very much on the context whether a result is having a high or low reliability.\n\n\n== Uncertainty ==\n[[File:Bildschirmfoto 2020-04-24 um 15.03.40.png|thumb|Uncertainty means that even the most careful and rigorous scientific investigation could not yield an exact measurement.]]\n\nIn ancient Greece there were basically two types of knowledge. The first was the metaphysical which was quite diverse and some may say vague. The second was the physical or better mathematical which was quite precise and likewise diverse. With the rise of irrational numbers in medieval times, however, and the combination of the hypothesis testing thoughts in the enlightenment, uncertainty was born. By having larger samples i.e. through financial accounting and by making repeated measures in astronomy, biology and other sciences, a certain lack of precision became apparent in the empirical sciences. More importantly, the recognition of fluctuation, modifications, and other reasons for error became apparent. Soft sciences were on the rise, and this demanded a clearer recognition of errors within scientific analysis and reasoning.  \n\nWhile validity encompasses the everything from theory building to a final confirmation, uncertainty is only preoccupied with the methodological dimension of hypothesis testing. Observations can be flawed, measurements can be wrong, an analysis can be biased and mis-selected, but at least this is it what uncertainty can tell us. [https://www.visionlearning.com/en/library/Process-of-Science/49/Uncertainty-Error-and-Confidence/157 '''Uncertainty] is hence a word for all the errors that can be done within a methodological application.''' Better yet, uncertainty can thus give us the difference between a perfect model and our empirical results. Uncertainty is important to recognise how good our hypothesis or model can be confirmed by empirical data, and all the measurements errors and missed selections in analysis associated to it. \n\nWhat makes uncertainty so practical is the fact that many statistical analysis have a measure to quantify uncertainty, and due to repeated sampling we can thus approximate the so-called \"error\" of our statistical test. The [https://www.youtube.com/watch?v=tFWsuO9f74o confidence interval] is a classic example of such a quantification of an error of a statistical result. Since it is closely associated with probability, the confidence interval can tell us the chance with which our results are reproducible. While this sounds like a small detail, it is of huge importance in modern statistics. Uncertainty allowed us to quantify whether a specific analysis can be reliably repeated and thus the result can have relevance under comparable conditions. \n\nConsider the example of medical treatment that has a high uncertainty identified with it. Would you undergo the procedure? No one in their right mind would trust treatment that has a high uncertainty. Hence uncertainty became the baseline of scientific outcome in quantitative science. It should be noted that this also opened the door to misuse and corruption. Many scientists measure their sceintific merit by their model with the lowest uncertainty, and quite some misuse has been done through the boundless search of tackling uncertainty. Yet we need to be aware that wrong understandings of statistics and severe cases of scientific misconduct are an exception and not the norm. If scientific results are published, we can question them, but we cannot question their sound and safe measures that have been established over the decades. Uncertainty can still be a guide helping us to identify the likelihood of our results matching a specific reality.\n\n\n== The history of laboratory experiments ==\n[[File:Experiment.jpg|thumb|right|Experiments are not limited to chemistry alone]]\nExperiments describe the systematic and reproducible design to test specific hypothesis.\n\nStarting with [https://sustainabilitymethods.org/index.php/Why_statistics_matters#The_scientific_method Francis Bacon] there was the theoretical foundation to shift previously widely un-systematic experiments into a more structured form. With the rise of disciplines in the [https://sustainabilitymethods.org/index.php/Why_statistics_matters#A_very_short_history_of_statistics enlightenment] experiments thrived, also thanks to an increasing amount of resources available in Europe due to the Victorian age and other effects of colonialism. Deviating from more observational studies in physics, [https://en.wikipedia.org/wiki/History_of_experiments#Galileo_Galilei astronomy], biology and other fields, experiments opened the door to the wide testing of hypothesis. All the while Mill and others build on Bacon to derive the necessary basic debates about so called facts, building the theoretical basis to evaluate the merit of experiments. Hence these systematic experimental approaches aided many fields such as botany, chemistry, zoology, physics and [http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf much more], but what was even more important, these fields created a body of knowledge that kickstarted many fields of research, and even solidified others. The value of systematic experiments, and consequently systematic knowledge created a direct link to practical application of that knowledge. [https://www.youtube.com/watch?v=UdQreBq6MOY The scientific method] -called with the ignorant recognition of no other methods beside systematic experimental hypothesis testing as well as standardisation in engineering- hence became the motor of both the late enlightenment as well as the industrialisation, proving a crucial link between basically enlightenment and modernity.\n\n[[File:Bike repair 1.jpg|thumb|left|Taking a shot at something or just trying out is not equal to a systematic scientific experiment.]]\n\nDue to the demand of systematic knowledge some disciplines ripened, meaning that own departments were established, including the necessary laboratory spaces to conduct [https://www.tutor2u.net/psychology/reference/laboratory-experiments experiments.] The main focus to this end was to conduct experiments that were as reproducible as possible, meaning ideally with a 100 % confidence. Laboratory conditions thus aimed at creating constant conditions and manipulating ideally only one or few parameters, which were then manipulated and therefore tested systematically. Necessary repetitions were conducted as well, but of less importance at that point. Much of the early experiments were hence experiments that were rather simple but produced knowledge that was more generalisable. There was also a general tendency of experiments either working or not, which is up until today a source of great confusion, as an [https://www.psychologydiscussion.net/learning/learning-theory/thorndikes-trial-and-error-theory-learning-psychology/13469 trial and error] approach -despite being a valid approach- is often confused with a general mode of \u201cexperimentation\u201d. In this sense, many people consider preparing a bike without any knowledge about bikes whatsoever as a mode of \u201cexperimentation\u201d. We therefore highlight that experiments are systematic. The next big step was the provision of certainty and ways to calculate [https://sustainabilitymethods.org/index.php/Hypothesis_building#Uncertainty uncertainty], which came with the rise of probability statistics.\n\n'''First in astronomy, but then also in agriculture and other fields the notion became apparent that our reproducible settings may sometimes be hard to achieve.''' Error of measurements in astronomy was a prevalent problem of optics and other apparatus in the 18th and 19th century, and Fisher equally recognised the mess -or variance- that nature forces onto a systematic experimenter. The laboratory experiment was hence an important step towards a systematic investigation of specific hypothesis, underpinned by newly established statistical approaches.\n\nFor more on the laboratory experiment, please refer to the entry on [[Experiments]].\n\n== The history of the field experiment ==\n[[File:Bildschirmfoto 2020-05-21 um 15.46.03.png|thumb|Field experiments were - as you might guess - first conducted in agriculture.]]\nWith a rise in knowledge, it became apparent that the controlled setting of a laboratory was not enough. First in astronomy, but then also in agriculture and other fields the notion became apparent that our [https://sustainabilitymethods.org/index.php/Experiments#History_of_laboratory_experiments reproducible settings may sometimes be hard to achieve]. Observations can be unreliable, and error of measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter. The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct [https://explorable.com/field-experiments experiments under field conditions]. Making experiments in the laboratory reached its outer borders, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory, and it was questioned whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a [https://www.tutor2u.net/psychology/reference/field-experimentsme dramatic effect on their design, conduct and outcome]. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. [https://en.wikipedia.org/wiki/Field_experiment#Examples Establishing the field experiment] became thus a step in the scientific development, but also in the industrial development. Science contributed directly to the efficiency of production, for better or worse.\n\nFor more details on field experiments, please refer to the entry on [[Field experiments]].\n\n\n== Enter the Natural experiments ==\n[[File:Easter Island.jpg|thumb|right|Although this seems to be a little contradictory here, the impossibility of replication is a problem in the case of the Easter Island.]]\n\nOut of a diverse rooting in discussions about complexity, [https://learningforsustainability.net/systems-thinking/ system thinking] and the need to understand specific contexts more deeply, the classic experimental setting did at some point become more and more challenged. What emerged out of the development of [https://sustainabilitymethods.org/index.php/Interactions#The_field_experiment field experiments] was an almost exact opposite trend considering the reduction of complexity. What do we learn from singular cases? How do we deal with cases that are of pronounced importance, yet cannot be replicated? And what can be inferred from the design of such case studies? A famous example from ethnographic studies is the [http://www.eisp.org/818/ Easter Island]. Why did the people there channel much of their resources into building gigantic statues, thereby bringing their society to the brink of collapse? While this is a surely intriguing question, there are no replicates of the Easter Islands. This is at a first glance a very specific and singular problem, yet it is often considered to be an important example on how unsustainable behaviour led to a collapse of a while civilisation. Such settings are referred to as [https://www.britannica.com/science/natural-experiment Natural Experiments]. From a certain perspective, our whole planet is a Natural Experiment, and it is also from a statistical perspective a problem that we do not have any replicates, besides other ramifications and unclarity that derives such single case studies, which are however often increasingly relevant on a smaller scale as well. With a rise in qualitative methods both in diversity and abundance, and an urge for understanding even complex systems and cases, there is clearly a demand for the integration of knowledge from Natural Experiments. '''From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant, plausible and valid.''' To this end, the concept of the niche in order to illustrate and conceptualise how single cases can still contribute to the production and canon of knowledge.\n\nFor example the [https://academic.oup.com/rcfs/article/4/2/155/1555737#113865691 financial crisis from 2007], where many patterns where comparable to previous crisis, but other factors were different. Hence this crisis is comparable to many previous factors and patterns regarding some layers of information, but also novel and not transferable regarding other dynamics. We did however understand based on the single case of this financial crisis that certain constructs in our financial systems are corrupt if not broken. The contribution to develop the financial world further is hence undeniable, even so far that many people agree that the changes that were being made are certainly not enough. \n\nAnother prominent example of a single case or phenomena is the Covid pandemic that emerges further. While much was learned from previous pandemics, this pandemic is different, evolves different, and creates different ramifications. The impact of our societies and the opportunity to learn from this pandemic is however undeniable. While classical experiments evolve knowledge like pawns in a chess game, moving forward step by step, a crisis such as the Covid pandemic is more like the horse in a chess game, jumping over larger gaps, being less predictable, and certainly harder to master. The evolution of knowledge in an interconnected world often demands a rather singular approach as a starting point. This is especially important in normative sciences, where for instance in conservation biology many researchers approach solutions through singular case studies. Hence the solution orientated agenda of sustainability science emerged to take this into account, and further.\n\n[[File:Lueneburg 2030.jpg|thumb|left|A wonderful example for a bunch of real world experiments is the project L\u00fcneburg 2030+. This map provides an overview of the different experiments.]]\n\nTo this end, [https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 real world experiments] are the latest development in the diversification of the arena of experiments. These types of experiments are currently widely explored in the literature, yet there is no coherent understanding of what real-world experiments are to date in the available literature, yet approaches are emerging. These experiments can however be seen as a continuation of the trend of natural experiments, where a solution orientated agenda tries to generate one or several interventions, the effects of which are tested often within singular cases, but the evaluation criteria are clear before the study was conducted. Most studies to date have defined this with vigour; nevertheless, the development of real-world experiments is only starting to emerge.\n\nFor more details on natural experiments, please refer to the entry on [[Case studies and Natural experiments]].\n\n\n== Experiments up until today ==\n[[File:Experiment Scopus.png|400px|thumb|right|'''SCOPUS hits per year for Experiment until 2020.''' Search terms: 'Experiment' in Title, Abstract, Keywords. Source: own.]]\nDiverse methodological approaches are thus integrated under the umbrella of the term 'experiment'. While simple manipulations such as medical procedures were already known as experiments during the enlightenment, the term 'experiment' gained in importance during the 20th century. Botanical experiments had been conducted long before, but it was the agricultural sciences that evolved the necessary methodological designs together with the suitable [[Bachelor Statistics Lecture|statistical analyses]], creating a statistical revolution that created ripples in numerous scientific fields. The Analysis of Variance ([[ANOVA]]) become the most important statistical approach to this end, allowing for the systematic design of experimental settings, both in the laboratory and in agricultural fields. \n\nWhile psychology, medicine, agricultural science, biology and later ecology thus thrived in their application of experimental designs and studies, there was also an increasing recognition of information that was creating [[Bias and Critical Thinking|biases]] or otherwise falsely skewed the results. '''The ANOVA hence became amended by additional modifications, ultimately leading to more advanced statistics that were able to focus on diverse statistical effects''', and reduce the influence of skews rooted, for instance, in sampling bias, statistical bias or other flaws. Hence, [[Mixed-Effect Models]] became an advanced next step in the history of statistical models, leading to more complex statistical designs and experiments, taking more and more information into account. In addition, meta-analytical approaches led to the combination and summarising of several case studies into a systematic overview. This was the dawn of a more integrational understanding of different studies that were combined into a [[Meta-Analysis]], taking different contexts of the numerous studies into account as well. In addition, research also focused more and more onto a deeper understanding of individual case studies, with a stronger emphasis on the specific context of the respective case. Such singular cases have been of value in medical research for decades now, where new challenges or solutions are frequently published despite the obvious lack of a wider contribution. Such medical case studies report novel findings, emerging problems or other so far unknown case dynamics, and often serve as a starting point for further research. Out of such diverse origins such as [[System Thinking & Causal Loop Diagrams|system thinking]], Urban Research, [[Ethnography]] and other fields in research, [[Living Labs & Real World Laboratories|real world experiments]] emerged, which take place in everyday social or cultural settings. The rigid designs of laboratory or field experiments is traded off for a deeper understanding of the specific context and case. While real world experiments emerged some decades ago already, they are only starting to gain wider recognition. All the while, the reproducibility crisis challenges the classical laboratory and field experiments, as a wider recognition that many results - for instance from psychological studies - cannot be reproduced. All this indicates that while much of our scientific knowledge is derived from experiments, much remains to be known, also about the conduct of experiments themselves.\n\n----\n[[Category: Normativity_of_Methods]]\n[[Category: Methods]]\n[[Category: Statistics]]\n[[Category: Qualitative]]\n[[Category: Deductive]]\n[[Category: Individual]]\n[[Category: System]]\n[[Category: Global]]\n[[Category: Present]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "qax2ahyn8z5f07gd6jun6de09gs2hks"
                }
            },
            {
                "title": "Experiments and Hypothesis Testing (German)",
                "ns": "0",
                "id": "540",
                "revision": {
                    "id": "6784",
                    "parentid": "6359",
                    "timestamp": "2022-10-05T10:47:12Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "40562",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Experiments and Hypothesis Testing]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag stellt das Konzept von Hypothesen vor und zeigt, wie man sie systematisch testen kann.\n\n[[File:Concept visualisation - Experiments.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Experiments]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n== Hypothesenentwicklung ==\n[[File:James Lind, A Treatise on the Scurvy, 1757 Wellcome M0013130.jpg|thumb|James Linds \"A Treatise on Scurvy\", in dem er systematisch ergr\u00fcndet, was das beste Mittel gegen Skorbut ist.]]\n\n'''Das Leben ist voller Hypothesen.'' Ich denke, wenn ich zuerst einen Kaffee trinken w\u00fcrde, w\u00e4re es viel einfacher, diesen Eintrag zu schreiben. Aber ich habe hier keinen Kaffee. Daher werde ich nie wissen, ob ein Kaffee diesen Eintrag besser oder zumindest einfacher zu schreiben gemacht h\u00e4tte.\n\nDieser kleine, nicht allzu konstruierte Fall veranschaulicht, wie wir alle durch einen [https://www.youtube.com/watch?v=HZ9xZHWY0mw endlosen Fluss von Fragen] verwirrt werden. Wir testen unsere Umgebung st\u00e4ndig auf m\u00f6gliche \"Was-w\u00e4re-wenn\"-Fragen, die unseren Verstand \u00fcberfordern, doch noch h\u00e4ufiger ist unser \u00fcberladener Kopf nicht in der Lage, verallgemeinerbares Wissen zu produzieren. Dennoch hilft uns unser Gehirn dabei, unsere Umgebung zu filtern, auch indem es st\u00e4ndig Fragen - oder Hypothesen - pr\u00fcft, damit wir die Herausforderungen, denen wir uns stellen, besser bew\u00e4ltigen k\u00f6nnen. Es sind nicht die wirklich gro\u00dfen Fragen, die uns jeden Tag besch\u00e4ftigen, aber diese kleinen Fragen halten uns in Bewegung.\n \nHaben Sie schon einmal in einem Restaurant das falsche Essen gew\u00e4hlt? Dann schreibt Ihr Gehirn eine kleine Notiz an sich selbst, und beim n\u00e4chsten Mal w\u00e4hlen Sie etwas anderes aus. So k\u00f6nnen wir unsere Nische im Leben finden. Irgendwann jedoch [https://www.sciencedirect.com/science/article/pii/S0092867408009537 begannen die Menschen, Muster in ihrer Umgebung zu beobachten] und diese anderen Menschen mitzuteilen. Zusammenarbeit und Kommunikation waren unsere Mittel, um zu Gemeinschaften und viel sp\u00e4ter zu Gesellschaften zu werden. Das geht auf die Zeit vor den Anf\u00e4ngen des Menschen zur\u00fcck und kann auch bei anderen Tieren beobachtet werden. Im Rahmen unserer Entwicklung wurde es zum R\u00fcckgrat unserer modernen Zivilisation - im Guten wie im Schlechten - als wir begannen, unsere Fragen systematisch zu testen. James Lind zum Beispiel f\u00fchrte einen dieser [https://journals.sagepub.com/doi/pdf/10.1177/014107680309601201 ersten systematische Ans\u00e4tze] auf See durch und schaffte es, eine Heilung f\u00fcr die Krankheit Skorbut zu finden, die den Seeleuten im 18. viele Probleme bereitet hatte.\n \n==== Das Testen einer Hypothese ====\n[[File:Bildschirmfoto 2020-01-17 um 11.18.08.png|thumb|left|Nullhypothese (H0) und Gegenhypothese (H1).]]\n\nKonkrete Fragen, die in einer ersten Idee, einer Theorie, wurzeln, wurden so wiederholt getestet. Oder vielleicht sollte man besser sagen, dass diese Hypothesen mit Wiederholungen getestet wurden und die unsere Beobachtung st\u00f6renden Einfl\u00fcsse konstant gehalten wurden. Dies nennt man das Testen einer Hypothese: die wiederholte systematische Untersuchung einer vorgefassten Theorie, bei der w\u00e4hrend der Pr\u00fcfung idealerweise alles konstant gehalten wird, au\u00dfer dem, was untersucht wird. W\u00e4hrend einige dies als die \"[https://www.youtube.com/watch?v=ptADSmJCVwQ wissenschaftliche Methode]\" bezeichnen, gef\u00e4llt mir diese Bezeichnung nicht besonders, da sie impliziert, dass es sich um DIE [https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology wissenschaftliche Methode] handelt, was sicherlich nicht stimmt. Dennoch war die systematische [https://www.youtube.com/watch?v=ZzeXCKd5a18 Pr\u00fcfung der Hypothese] und der daraus resultierende Erkenntnisprozess zweifellos eine wissenschaftliche Revolution (weitere Informationen finden Sie unter [[History of Methods|hier klicken]]). \n\nWichtig in Bezug auf dieses Wissen ist die Vorstellung, dass eine best\u00e4tigte Hypothese darauf hinweist, dass etwas wahr ist, dass aber andere, bisher unbekannte Faktoren diese Hypothese sp\u00e4ter verf\u00e4lschen k\u00f6nnen. Aus diesem Grund erm\u00f6glicht uns die Pr\u00fcfung von Hypothesen eine Ann\u00e4herung an das Wissen, wie viele argumentieren w\u00fcrden, mit einer gewissen Sicherheit, aber [https://www.brgdomath.com/philosophie/erkenntnistheorie-tk10/falsifikation-popper/ wir werden nie wirklich etwas wissen]. Das scheint zwar etwas verwirrend, aber ein prominentes Beispiel kann dies vielleicht veranschaulichen. Bevor Australien entdeckt wurde, waren alle in Europa bekannten Schw\u00e4ne wei\u00df. Mit der Entdeckung Australiens, und zur \u00dcberraschung der damaligen Naturhistoriker*innen, wurden schwarze Schw\u00e4ne entdeckt. Vor dieser Entdeckung h\u00e4tte also jeder eine Hypothese aufgestellt: [https://www.youtube.com/watch?v=wf-sGqBsWv4 alle Schw\u00e4ne sind weiss]. Dann wurde Australien entdeckt, und die Welt ver\u00e4nderte sich oder zumindest das Wissen der westlichen Kulturen \u00fcber Schw\u00e4ne. \n\n'''An diesem Punkt ist es wichtig zu wissen, dass es zwei Arten von Hypothesen gibt.''' Stellen wir uns vor, Sie n\u00e4hmen an, es g\u00e4be nicht nur wei\u00dfe Schw\u00e4ne auf der Welt, sondern auch schwarze Schw\u00e4ne. Die Nullhypothese (H0) besagt, dass das nicht der Fall ist - alles bleibt so, wie wir es annahmen, bevor Sie mit Ihrer merkw\u00fcrdigen Idee um die Ecke gekommen sind. Die Nullhypothese w\u00e4re also: alle Schw\u00e4ne sind wei\u00df. Die Alternativhypothese (H1) fordert diese Nullhypothese nun heraus: es gibt auch schwarze Schw\u00e4ne. Das haben Sie ja behauptet. Wir gehen aber nicht sofort davon aus dass das, was Sie behaupten, auch stimmt, also machen wir es zur Alternativhypothese. Jetzt wollen wir herausfinden, ob es eine ausreichende Beweislast zugunsten der Alternativhypothese gibt. Hierf\u00fcr testen wir die Wahrscheinlichkeit, dass die H0 wahr ist, um zu entscheiden, ob wir H0 akzeptieren oder sie verwerfen und uns stattdessen f\u00fcr die H1 entscheiden. Diese Entscheidung geht Hand in Hand: wenn wir eine der beiden Hypothesen akzeptieren, weisen wir gleichzeitig die andere zur\u00fcck. Achtung: Hypothesen kann man nur zur\u00fcckweisen, nicht aber basierend auf Daten best\u00e4tigen. Wenn es hei\u00dft, dass eine Hypothese 'akzeptiert' oder 'verifiziert' oder 'bewiesen' wird, hei\u00dft das nur, dass die Alternative nicht unterst\u00fctzt werden kann, und dass wir deshalb bei der wahrscheinlicheren Hypothese bleiben.\n\n==== Fehler beim Testen einer Hypothese ====\n[[File:Bildschirmfoto 2020-01-17 um 10.36.55.png|thumb|right|Verschiedene Arten von Fehlern bei Schwangerschaftstests.]]\nBeim Testen von Hypothesen k\u00f6nnen [https://www.youtube.com/watch?v=a_l991xUAOU zwei verschiedene Fehler] auftreten. Der falsch positive (\"Fehlertyp I\") und der falsch negative (\"Fehlertyp II\"). Wenn ein*e \u00c4rzt*in bei einer Person eine Schwangerschaft diagnostiziert und diese Person nicht schwanger ist, handelt es sich um einen typischen falsch-positiven Fehler. Der*die \u00c4rzt*in hat behauptet, dass die Person schwanger ist, was nicht der Fall ist. Umgekehrt ist es ein typischer falsch-negativer Fehler, wenn der*die \u00c4rzt*in bei einer Person diagnostiziert, dass sie nicht schwanger ist, es aber tats\u00e4chlich ist - ein typischer falsch-negativer Fehler. Jede*r kann f\u00fcr sich selbst beurteilen, welche Art von Fehler gef\u00e4hrlicher ist. Dennoch sind beide Fehler zu vermeiden. W\u00e4hrend der Corona-Krise wurde dies zu einem wichtigen Thema, da viele der Tests oft falsch waren. Wenn jemand Corona Symptome zeigte, aber negativ getestet wurde, war es ein falsches Negativ. Dies h\u00e4ngt anscheinend vom Zeitpunkt der Untersuchung und den jeweiligen Tests ab. Antik\u00f6rpertests sind nur relevant, nachdem jemand Corona hatte, aber manchmal finden auch diese keine Antik\u00f6rper. Bei einem aktiven Fall kann nur eine PCR eine Infektion best\u00e4tigen, aber die Entnahme einer Probe ist trickreich, daher gab es auch hier einige falsch-negative Ergebnisse. Falsch-positive Ergebnisse sind jedoch sehr selten, da die Fehlerquote des Tests zu diesem Zweck sehr gering ist. Dennoch kam es manchmal vor, dass Proben verwechselt wurden. Selten, aber es kam vor, und sorgte f\u00fcr gro\u00dfes Aufsehen in den Medien.\n\n==== Wissen ver\u00e4ndert sich mit der Zeit ====\nIn der modernen Wissenschaft wird gro\u00dfe Sorgfalt darauf verwendet, Hypothesen zu konstruieren, die systematisch fundiert sind, was bedeutet, dass schwarze Schw\u00e4ne nicht zu erwarten sind, aber manchmal nicht verhindert werden k\u00f6nnen. Ein Beispiel ist die Frage, ob ein Medikament f\u00fcr Schwangere geeignet ist. Aus ethischen Gr\u00fcnden wird dies kaum getestet, was in der Vergangenheit schwerwiegende negative Auswirkungen hatte. Das Wissen dar\u00fcber, wie sich ein bestimmtes Medikament auf Schwangere auswirkt, beruht daher oft auf Erfahrungen, wird aber nicht systematisch untersucht. Zusammengenommen werden Hypothesen zu einem Grundnahrungsmittel der modernen Wissenschaft, obwohl sie nur ein Konzept innerhalb des breiten Kanons wissenschaftlicher Methoden darstellen. Die Abh\u00e4ngigkeit von [[:Category:Deductive|deduktiven]] Ans\u00e4tzen hat in der Vergangenheit ebenfalls Probleme aufgeworfen und kann indirekt dazu f\u00fchren, dass eine Mauer zwischen dem durch [[Design Criteria of methods|induktive vs. deduktive]] Ans\u00e4tze geschaffenen Wissen errichtet wird. \n\n==== Key messages ====\n* Das Testen einer Hypothese ist die wiederholte systematische Untersuchung einer vorgefassten Idee durch Beobachtung.\n\n* Es gibt eine Unterscheidung zwischen der Nullhypothese (H0), die wir zu verwerfen versuchen, und der Alternativhypothese (H1).\n\n* Es gibt zwei verschiedene Fehler, die auftreten k\u00f6nnen: falsch positiv (\"Fehlertyp I\") und falsch negativ (\"Fehlertyp II\").\n\n==== Ein Gegenmittel f\u00fcr COVID-19 ====\nViele betrachten einen Impfstoff als das sicherste Ende der Pandemie. Ich w\u00fcrde dem zwar zustimmen, aber das wird sich erst mit der Zeit herausstellen. Wir haben also die Hypothese, aber wir haben die Hypothese noch nicht getestet, da es nicht m\u00f6glich war. Noch wichtiger ist, dass fast alle Impfstoffe derzeit klinische Studien durchlaufen, die so viele weitere Hypothesen testen. Ist der Impfstoff f\u00fcr die Menschen sicher, gibt es keine Nebenwirkungen, allergische Reaktionen oder andere schweren medizinischen Probleme? Erzeugt der Impfstoff eine Immunantwort? Und wirkt er auf verschiedene Altersgruppen oder andere Gruppen unterschiedlich? Um diese Strategien sicher zu testen, haben sich in den letzten Jahrzehnten klinische Studien als Goldstandard in modernen Experimenten herauskristallisiert. Viele m\u00f6gen die moderne Wissenschaft kritisieren und vieles von dem in Frage stellen, was die Wissenschaft getan hat, um nicht nur zu einer positiven Entwicklung in unserer Gesellschaft beizutragen. Dennoch verdanken viele von uns ihr Leben solchen klinischen Versuchen - der sicheren und soliden Pr\u00fcfung wissenschaftlicher Hypothesen unter sehr reproduzierbaren Bedingungen. Ein gro\u00dfer Teil der modernen Medizin, der Psychologie, der Produkttests und auch der Landwirtschaft verdankt ihre Auswirkungen auf unsere Gesellschaften dem Experiment.\n\n== Validit\u00e4t ==\n[[File:Bildschirmfoto 2020-04-24 um 12.42.47.png|thumb|Das bekannte Darts-Beispiel, das die Beziehung zwischen Validit\u00e4t und Realiabilit\u00e4t aufzeigt. Das Bull-Eye stellt die Genauigkeit unserer Messung dar (Validit\u00e4t) w\u00e4hrend die Position der Pfeile die Genauigkeit unserer Treffer darstellt (Reliabilit\u00e4t).]]\n\nValidit\u00e4t leitet sich vom lateinischen Wort \"validus\" ab, das mit \"stark\" \u00fcbersetzt wird. Es handelt sich somit um ein [https://explorable.com/statistical-validity zentrales Konzept innerhalb der Hypothesenpr\u00fcfung], da es qualifiziert, inwieweit eine Hypothese wahr ist. Seine Bedeutung sollte nicht mit der Reliabilit\u00e4t verwechselt werden, die angibt, ob bestimmte Ergebnisse konsistent oder, mit anderen Worten, reproduzierbar sind. Die Validit\u00e4t gibt vielmehr an, inwieweit eine Hypothese best\u00e4tigt werden kann. Folglich ist sie zentral f\u00fcr den deduktiven Ansatz in der Wissenschaft; mit anderen Worten: sie sollte nicht \u00fcber diese Denkweise hinaus missbraucht werden. Die Validit\u00e4t ist daher stark mit der Argumentation verbunden, weshalb sie die Gesamtst\u00e4rke des Ergebnisses eines statistischen Resultats angibt. \n\nWir sollten uns jedoch bewusst sein, dass die Validit\u00e4t den [https://www.youtube.com/watch?v=F6LGa8jsdjo gesamten Prozess der Anwendung von Statistik] umfasst. '''Validit\u00e4t erstreckt sich von der Postulierung der urspr\u00fcnglichen Hypothese \u00fcber das methodische Design, die Wahl der Analyse und schlie\u00dflich die Interpretation.''' Die Validit\u00e4t ist also kein \"alles oder nichts\". Stattdessen m\u00fcssen wir die verschiedenen Ans\u00e4tze und Geschmacksrichtungen der Validit\u00e4t kennen lernen, und nur die Erfahrung mit verschiedenen Datens\u00e4tzen und Situationen wird es uns erm\u00f6glichen zu beurteilen, ob ein Ergebnis g\u00fcltig ist oder nicht. Dies ist eine der Situationen, in denen sich Statistik und Philosophie vielleicht am n\u00e4chsten kommen. In der Philosophie ist es sehr schwer, Vernunft zu definieren, und ein Gro\u00dfteil der westlichen Philosophie hat sich in der Tat der Vernunft verschrieben. W\u00e4hrend es viel einfacher ist, unvern\u00fcnftige Handlungen zu definieren (und Rationalit\u00e4t ist ein ganz anderes Thema), ist die Vernunft an sich, glaube ich, komplexer. Folglich ist die G\u00fcltigkeit der Frage, ob die Best\u00e4tigung einer Hypothese vern\u00fcnftig ist, viel schwieriger zu beantworten. Nehmen Sie sich Zeit, um etwas \u00fcber die G\u00fcltigkeit zu lernen, und nehmen Sie sich noch mehr Zeit, um Erfahrungen dar\u00fcber zu sammeln, ob ein Ergebnis g\u00fcltig ist.\n\n== Reliabilit\u00e4t ==\n[https://www.scribbr.com/methodology/reliability-vs-validity/ Reliabilit\u00e4t] ist eines der Schl\u00fcsselkonzepte der Statistik. Sie ist definiert als die Konsistenz eines bestimmten Ma\u00dfes. '''Eine zuverl\u00e4ssige Messung liefert also vergleichbare Ergebnisse unter den gleichen Bedingungen.''' Die Reliabilit\u00e4t gibt Ihnen eine Vorstellung davon, ob Ihre Analyse unter den gegebenen Bedingungen genau und auch reproduzierbar ist. Wir haben bereits gelernt, dass die meisten, wenn nicht sogar alle Ma\u00dfe in der Statistik konstruiert sind. Reliabilit\u00e4t ist von zentraler Bedeutung, da wir \u00fcberpr\u00fcfen k\u00f6nnen, ob unsere konstruierten Ma\u00dfe uns auf den richtigen Weg bringen. Mit anderen Worten, die Reliabilit\u00e4t erm\u00f6glicht es uns zu verstehen, ob unsere konstruierten Ma\u00dfe funktionieren.\n\nDie Frage der [https://explorable.com/statistical-reliability Reproduzierbarkeit], die mit der Reliabilit\u00e4t verbunden ist, ist ein [https://www.youtube.com/watch?v=m0W8nnupcUk zentraler Aspekt der modernen Wissenschaft]. Reliabilit\u00e4t ist daher das Konzept, das mindestens mehrere Komponenten enth\u00e4lt. \n\n1. Das Ausma\u00df und die Komplexit\u00e4t des Ma\u00dfes, das wir in unsere Analyse einbeziehen. Zwar kann im Allgemeinen nicht davon ausgegangen werden, dass [[To Rule And To Measure (German)|extrem konstruierte Ma\u00dfe wie das BIP oder der IQ]] weniger zuverl\u00e4ssig sind als weniger konstruierte, aber wir sollten dennoch Occams Rasiermesser ber\u00fccksichtigen, wenn wir \u00fcber die Zuverl\u00e4ssigkeit von Ma\u00dfen nachdenken, d.h. Ihre Konstrukte kurz und einfach halten. \n\n2. Relevant f\u00fcr die Reliabilit\u00e4t ist die Frage nach unserer Stichprobe. Wenn sich die \u00e4u\u00dferen Bedingungen der Stichprobe \u00e4ndern, kann die Reliabilit\u00e4t aufgrund neuer Informationen abnehmen. Ein einfaches Beispiel w\u00e4ren wieder Schw\u00e4ne. Wenn wir feststellen, dass es nur wei\u00dfe Schw\u00e4ne gibt, ist diese Hypothese zuverl\u00e4ssig, solange es nur wei\u00dfe Schw\u00e4ne in der Stichprobe gibt. Wenn sich jedoch die \u00e4u\u00dferen Bedingungen \u00e4ndern - zum Beispiel wenn wir schwarze Schw\u00e4ne in Australien entdecken - nimmt die Reliabilit\u00e4t unserer ersten Hypothese ab.\n\n3. Dar\u00fcber hinaus kann auch die Probenahme beeintr\u00e4chtigt werden. Wenn die Beprobung von mehreren Beobachtern durchgef\u00fchrt wird, kann eine Verringerung der Reliabilit\u00e4t durch die Unterschiede der jeweiligen Beobachtenden erkl\u00e4rt werden. \n\n4. Wir m\u00fcssen erkennen, dass unterschiedliche methodische Ans\u00e4tze - oder in unserem Fall statistische Tests - aus unterschiedlichen Reliabilit\u00e4tsma\u00dfen bestehen k\u00f6nnen. Daher k\u00f6nnte sich das statistische Modell unserer Analyse direkt in unterschiedlichen Reliabilit\u00e4ten niederschlagen. Ein einfaches Beispiel, um dies etwas besser zu erkl\u00e4ren, ist der Ein-Stichproben-Test mit einer perfekten M\u00fcnze. Dieser Test sollte funktionieren, aber er funktioniert nicht mehr, wenn die M\u00fcnze verzerrt ist. Dann m\u00fcssen wir eine neue Methode/Modell in Betracht ziehen, um dies zu erkl\u00e4ren (n\u00e4mlich den \"Zwei-Stichproben-Test\"). Wichtig ist, sich daran zu erinnern: Die Wahl Ihrer Methode/Modell hat einen Einfluss auf Ihre Testergebnisse. \n\n5. Der letzte Aspekt der Reliabilit\u00e4t h\u00e4ngt mit unserer theoretischen Grundlage selbst zusammen. Wenn unsere zugrundeliegende Theorie revidiert oder ge\u00e4ndert wird, kann dies die Zuverl\u00e4ssigkeit unserer Ergebnisse insgesamt beeintr\u00e4chtigen. Es ist wichtig, dies hervorzuheben, da wir innerhalb einer gegebenen statistischen Analyse eine insgesamt bessere theoretische Grundlage haben k\u00f6nnten, das statistische Ma\u00df der Reliabilit\u00e4t jedoch niedriger sein k\u00f6nnte. Dies ist eine Quelle st\u00e4ndiger Verwirrung unter Statistik-Lernenden.\n\nDie Frage, wie zuverl\u00e4ssig eine statistische Analyse oder besser das Ergebnis einer wissenschaftlichen Studie ist, ist meiner Erfahrung nach die Quelle endloser Debatten. Es gibt kein [https://www.statisticssolutions.com/directory-of-statistical-analyses-reliability-analysis/ Ma\u00df der Reliabilit\u00e4t], das von allen akzeptiert wird. Stattdessen entwickelt sich die Statistik immer noch weiter, und die Ma\u00dfe der Zuverl\u00e4ssigkeit \u00e4ndern sich. Auch wenn dies auf den ersten Blick verwirrend erscheint, sollten wir zwischen Ergebnissen, die extrem reliabel sind, und Ergebnissen, die in Bezug auf die Reliabilit\u00e4t geringer sind, unterscheiden. [https://opinionator.blogs.nytimes.com/2012/05/17/how-reliable-are-the-social-sciences/ Ergebnisse aus der harten Wissenschaft] wie Physik zeigen oft eine hohe Reliabilit\u00e4t, wenn es um statistische Tests geht, w\u00e4hrend weiche Wissenschaften wie Psychologie und Wirtschaft am unteren Ende der Reliabilit\u00e4t zu finden sind. Das ist keine normative Beurteilung, da die moderne Medizin oft Ergebnisse liefert, die zuverl\u00e4ssig sind, aber nur gerade so. Denken Sie an die Krebsforschung. Die meisten aktuellen Durchbr\u00fcche ver\u00e4ndern das Leben einiger weniger Prozent der Patienten, und es gibt kaum einen Durchbruch, der einen gr\u00f6\u00dferen Teil der Patient*innen betreffen kann. Wir m\u00fcssen daher die Zuverl\u00e4ssigkeit in der Statistik als ein bewegliches Ziel akzeptieren, und es h\u00e4ngt sehr stark vom Kontext ab, ob ein Ergebnis eine hohe oder niedrige Zuverl\u00e4ssigkeit hat.\n\n== Unsicherheit ==\n[[File:Bildschirmfoto 2020-04-24 um 15.03.40.png|thumb|Unsicherheit bedeutet, dass selbst die vorsichtigste und sorgf\u00e4ltigste wissenschaftliche Untersuchung keine exakten Ergebnisse hervorbringen konnte.]]\n\nIm antiken Griechenland gab es im Wesentlichen zwei Arten von Wissen. Die erste war das metaphysische, das sehr vielf\u00e4ltig war und von manchen vielleicht als vage bezeichnet wird. Die zweite war das physikalische oder besser mathematische Wissen, das recht pr\u00e4zise und ebenfalls vielf\u00e4ltig war. Mit dem Aufkommen irrationaler Zahlen im Mittelalter und der Kombination mit dem Testen von Hypothesen in der Aufkl\u00e4rung wurde jedoch die Unsicherheit geboren. Durch gr\u00f6\u00dfere Stichproben, d.h. durch die Finanzbuchhaltung und durch wiederholte Messungen in der Astronomie, Biologie und anderen Wissenschaften, wurde ein gewisser Mangel an Pr\u00e4zision in den empirischen Wissenschaften deutlich. Wichtiger noch, Fluktuation, Modifikationen und andere Fehlerursachen wurden offensichtlich. Die weichen Wissenschaften waren auf dem Vormarsch, was eine deutlichere Erkennung von Fehlern in der wissenschaftlichen Analyse und Argumentation erforderte.  \n\nW\u00e4hrend die Validit\u00e4t alles von der Theoriebildung bis zur endg\u00fcltigen Best\u00e4tigung umfasst, besch\u00e4ftigt sich die Unsicherheit nur mit der methodologischen Dimension des Hypothesentests. Beobachtungen k\u00f6nnen fehlerhaft sein, Messungen k\u00f6nnen falsch sein, eine Analyse kann verzerrt und falsch ausgew\u00e4hlt sein, aber das ist es zumindest, was uns die Unsicherheit sagen kann. [https://www.visionlearning.com/en/library/Process-of-Science/49/Uncertainty-Error-and-Confidence/157 '''Unsicherheit] ist daher ein Wort f\u00fcr alle Fehler, die innerhalb einer methodologischen Anwendung gemacht werden k\u00f6nnen.''' Besser noch, Unsicherheit kann uns somit den Unterschied zwischen einem perfekten Modell und unseren empirischen Ergebnissen aufzeigen. Die Unsicherheit ist wichtig, um zu erkennen, wie gut unsere Hypothese oder unser Modell durch empirische Daten best\u00e4tigt werden kann, und alle damit verbundenen Messfehler und Fehlauswahlen in der Analyse. \n\nWas die Unsicherheit so praktisch macht, ist die Tatsache, dass viele statistische Analysen \u00fcber ein Ma\u00df zur Quantifizierung der Unsicherheit verf\u00fcgen, und durch wiederholte Stichproben k\u00f6nnen wir so den so genannten \"Fehler\" unseres statistischen Tests ann\u00e4hern. Das [https://www.youtube.com/watch?v=tFWsuO9f74o Konfidenzintervall] ist ein klassisches Beispiel f\u00fcr eine solche Quantifizierung eines Fehlers eines statistischen Ergebnisses. Da es eng mit der Wahrscheinlichkeit verbunden ist, kann uns das Konfidenzintervall die Wahrscheinlichkeit sagen, mit der unsere Ergebnisse reproduzierbar sind. Dies klingt zwar wie ein kleines Detail, ist aber in der modernen Statistik von enormer Bedeutung. Die Unsicherheit erlaubt es uns, zu quantifizieren, ob eine bestimmte Analyse zuverl\u00e4ssig wiederholt werden kann und das Ergebnis unter vergleichbaren Bedingungen relevant sein kann. \n\nNehmen wir das Beispiel einer medizinischen Behandlung, bei der eine hohe Unsicherheit identifiziert wird. W\u00fcrden Sie sich dem Verfahren unterziehen? Niemand, der bei klarem Verstand ist, w\u00fcrde einer Behandlung vertrauen, die mit einer hohen Unsicherheit behaftet ist. Daher wurde die Unsicherheit zur Grundlinie des wissenschaftlichen Ergebnisses in der quantitativen Wissenschaft. Es ist zu beachten, dass dies auch Missbrauch und Korruption T\u00fcr und Tor \u00f6ffnete. Viele Wissenschaftler*innen messen ihre wissenschaftlichen Verdienste an ihrem Modell mit der geringsten Unsicherheit, und so mancher Missbrauch wurde durch die grenzenlose Suche nach der Bew\u00e4ltigung der Unsicherheit betrieben. Wir m\u00fcssen uns jedoch bewusst sein, dass falsch verstandene Statistiken und schwere F\u00e4lle von wissenschaftlichem Fehlverhalten eine Ausnahme und nicht die Norm sind. Wenn wissenschaftliche Ergebnisse ver\u00f6ffentlicht werden, k\u00f6nnen wir sie in Frage stellen, aber wir k\u00f6nnen nicht ihre soliden und sicheren Ma\u00dfnahmen in Frage stellen, die sich \u00fcber Jahrzehnte hinweg etabliert haben. Unsicherheit kann immer noch ein Leitfaden sein, der uns hilft, die Wahrscheinlichkeit zu bestimmen, mit der unsere Ergebnisse einer bestimmten Realit\u00e4t entsprechen.\n\n== Die Geschichte des Laborexperiments ==\n[[File:Experiment.jpg|thumb|right|Experimente finden nicht nur in der Chemie statt.]]\n\nExperimente sind systematische und reproduzierbare Designs zur Pr\u00fcfung spezifischer Hypothesen.\n\nBeginnend mit [https://sustainabilitymethods.org/index.php/Why_statistics_matters#The_scientific_method Francis Bacon] entstand eine theoretische Grundlage, um zuvor weitestgehend unsystematische Experimente in eine strukturiertere Form zu \u00fcberf\u00fchren. Mit dem Aufkommen der Disziplinen in der [https://sustainabilitymethods.org/index.php/Why_statistics_matters#A_very_short_history_of_statistics Aufkl\u00e4rung] gediehen die Experimente, auch dank der zunehmenden Ressourcen, die in Europa aufgrund des viktorianischen Zeitalters und anderer Auswirkungen des Kolonialismus zur Verf\u00fcgung standen. Abweichend von eher beobachtenden Studien in der Physik, [https://en.wikipedia.org/wiki/History_of_experiments#Galileo_Galilei Astronomie], Biologie und anderen Bereichen, \u00f6ffneten die Experimente die T\u00fcr zu einer breiten Pr\u00fcfung von Hypothesen. W\u00e4hrenddessen bauten Mill und andere auf Bacon auf, um die notwendigen Grundsatzdebatten \u00fcber so genannte 'Fakten' abzuleiten und die theoretische Grundlage f\u00fcr die Bewertung des Mehrwerts von Experimenten zu schaffen. Daher unterst\u00fctzten diese systematischen experimentellen Ans\u00e4tze viele Gebiete wie Botanik, Chemie, Zoologie, Physik und [http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf viel mehr], aber was noch wichtiger war, diese Gebiete schufen einen Wissensfundus, der viele Forschungsbereiche in Schwung brachte und andere sogar festigte. Der Wert systematischer Experimente und folglich des systematischen Wissens schuf eine direkte Verbindung zur praktischen Anwendung dieses Wissens. [https://www.youtube.com/watch?v=UdQreBq6MOY 'Die wissenschaftliche Methode'] - so bezeichnet ohne die Anerkennung anderer Methoden neben der systematischen experimentellen Hypothesenpr\u00fcfung sowie der Standardisierung im Ingenieurwesen - wurde somit zum Motor sowohl der sp\u00e4ten Aufkl\u00e4rung als auch der Industrialisierung und erwies sich als entscheidende Verbindung zwischen grundlegender Aufkl\u00e4rung und Moderne.\n\n[[File:Bike repair 1.jpg|thumb|left|Etwas ins Gr\u00fcne hinein auszuprobieren ist nicht mit einem systematischen wissenschaftlichen Experiment gleichzusetzen.]]\n\nAufgrund des Bedarfs an systematischem Wissen reiften einige Disziplinen heran, sodass eigene Abteilungen gegr\u00fcndet wurden, einschlie\u00dflich der notwendigen Laborr\u00e4ume zur Durchf\u00fchrung von [https://www.tutor2u.net/psychology/reference/laboratory-experiments Experimenten]. Das Hauptaugenmerk lag dabei auf der Durchf\u00fchrung m\u00f6glichst reproduzierbarer Experimente, d.h. im Idealfall mit 100%iger Sicherheit. Die Laborbedingungen zielten also darauf ab, konstante Bedingungen zu schaffen und idealerweise nur einen oder wenige Parameter zu manipulieren, die damit systematisch getestet werden konnten. Notwendige Wiederholungen wurden ebenfalls durchgef\u00fchrt, was aber zu diesem Zeitpunkt von geringerer Bedeutung war. Viele der fr\u00fchen Experimente waren daher eher einfache Experimente, die aber zu verallgemeinerbaren Erkenntnissen f\u00fchrten. Es gab auch eine allgemeine Tendenz, dass Experimente entweder funktionierten oder nicht, was bis heute eine Quelle gro\u00dfer Verwirrung ist, da ein [https://www.psychologydiscussion.net/learning/learning-theory/thorndikes-trial-and-error-theory-learning-psychology/13469 'trial and error'-] Ansatz - obwohl er ein g\u00fcltiger Ansatz ist - oft mit einem allgemeinen Modus des \"Experimentierens\" verwechselt wird. In diesem Sinne betrachten viele Menschen die Vorbereitung eines Fahrrads ohne jegliche Kenntnisse \u00fcber Fahrr\u00e4der als einen Modus des \"Experimentierens\". Wir betonen daher, dass Experimente systematisch sind. Der n\u00e4chste gro\u00dfe Schritt war die Bereitstellung von Gewissheit und M\u00f6glichkeiten zur Berechnung von [https://sustainabilitymethods.org/index.php/Hypothesis_building#Uncertainty Unsicherheit], die mit dem Aufkommen der Wahrscheinlichkeitsstatistik einherging.\n\n'''Zuerst in der Astronomie, dann aber auch in der Landwirtschaft und in anderen Bereichen wurde deutlich, dass reproduzierbare Bedingungen manchmal schwer zu erreichen sein k\u00f6nnen.''' Messfehler in der Astronomie waren ein weit verbreitetes Problem der Optik und anderer Apparate im 18. und 19. Jahrhundert, und Fisher erkannte ebenso die Unordnung - oder Varianz -, die die Natur einem systematischen Experimentator aufzwingt. Das Laborexperiment war daher ein wichtiger Schritt zu einer systematischen Untersuchung bestimmter Hypothesen, die durch neu etablierte statistische Ans\u00e4tze untermauert wurde.\n\n== Die Geschichte des Feldexperiments ==\n[[File:Bildschirmfoto 2020-05-21 um 15.46.03.png|thumb|Feldexperimente wurden - wie man sich vielleicht denken kann - zuerst in der Landwirtschaft durchgef\u00fchrt.]]\n\nMit zunehmendem Wissensstand zeigte sich, dass die kontrollierte Einrichtung eines Labors nicht ausreichte. Zuerst in der Astronomie, dann aber auch in der Landwirtschaft und in anderen Bereichen wurde die Vorstellung deutlich, dass [https://sustainabilitymethods.org/index.php/Experiments#History_of_laboratory_experiments reproduzierbare Einstellungen manchmal schwer zu erreichen sein k\u00f6nnen]. Die Nachfrage nach mehr Nahrung aufgrund des Bev\u00f6lkerungswachstums und die Verf\u00fcgbarkeit potenter Saatgutsorten und D\u00fcngemittel - beides dank wissenschaftlicher Experimente m\u00f6glich - warfen die Frage auf, wie man [https://explorable.com/field-experiments Experimente unter Feldbedingungen] durchf\u00fchren sollte. Die Durchf\u00fchrung von Experimenten im Labor stie\u00df an ihre \u00e4u\u00dferen Grenzen, da Pflanzenwachstumsexperimente in den kleinen, engen R\u00e4umen eines Labors schwer durchf\u00fchrbar waren, und es wurde in Frage gestellt, ob die Ergebnisse tats\u00e4chlich in der realen Welt anwendbar waren. Daher verlagerten sich die Experimente buchst\u00e4blich ins Feld, mit [https://www.tutor2u.net/psychology/reference/field-experimentsme dramatischen Auswirkungen auf ihre Gestaltung, Durchf\u00fchrung und Ergebnisse]. W\u00e4hrend die Laborbedingungen darauf abzielten, die Varianz zu minimieren - im Idealfall wurden die Experimente mit hoher Zuverl\u00e4ssigkeit durchgef\u00fchrt -, wurde bei den neuen Feldexperimenten die Stichprobengr\u00f6\u00dfe erh\u00f6ht, um die Variabilit\u00e4t - oder Unordnung - von Faktoren zu z\u00e4hmen, die nicht kontrolliert werden konnten, wie z.B. subtile Ver\u00e4nderungen des Bodens oder des Mikroklimas. [https://en.wikipedia.org/wiki/Field_experiment#Examples Die Etablierung des Feldexperiments] wurde so zu einem Schritt in der wissenschaftlichen Entwicklung, aber auch in der industriellen Entwicklung. Die Wissenschaft trug direkt zur Effizienz der Produktion bei, im Guten wie im Schlechten.\n\n\n== Vorhang auf f\u00fcr das nat\u00fcrliche Experiment ==\n[[File:Easter Island.jpg|thumb|right|Auch wenn dies etwas widerspr\u00fcchlich wirkt - die Unm\u00f6glichkeit, ein Experiment zu wiederholen, ist ein Problem im Falle der Osterinseln.]]\n\nAuf Basis vielf\u00e4ltiger Diskussionen \u00fcber Komplexit\u00e4t, [https://learningforsustainability.net/systems-thinking/ Systemdenken] und der Notwendigkeit, bestimmte Zusammenh\u00e4nge tiefer zu verstehen, wurde die klassische Versuchsanordnung irgendwann mehr und mehr in Frage gestellt. Was aus der Entwicklung von [https://sustainabilitymethods.org/index.php/Interactions#The_field_experiment Feldexperimenten] hervorging, war angesichts der Komplexit\u00e4tsreduktion ein fast genau entgegengesetzter Trend. Was lernen wir aus singul\u00e4ren F\u00e4llen? Wie gehen wir mit F\u00e4llen um, die signifikante Ergebnisse zeigen, aber nicht repliziert werden k\u00f6nnen? Und was l\u00e4sst sich aus dem Design solcher Fallstudien ableiten? Ein ber\u00fchmtes Beispiel aus ethnographischen Studien ist die [http://www.eisp.org/818/ Osterinsel]. Warum haben die Menschen dort einen Gro\u00dfteil ihrer Ressourcen in den Bau gigantischer Statuen gesteckt und damit ihre Gesellschaft an den Rand des Zusammenbruchs gebracht? Obwohl dies eine sicherlich faszinierende Frage ist, gibt es keine Nachbildungen der Osterinseln. Dies ist auf den ersten Blick ein sehr spezifisches und einzigartiges Problem, doch wird es oft als ein wichtiges Beispiel daf\u00fcr angesehen, wie unnachhaltiges Verhalten zu einem Zusammenbruch der damaligen Zivilisation f\u00fchrte. Solche Bedingungen werden als [https://www.britannica.com/science/natural-experiment 'nat\u00fcrliche Experimente'] bezeichnet.\n\nAus einer bestimmten Perspektive ist unser ganzer Planet ein nat\u00fcrliches Experiment, und es ist auch aus statistischer Sicht ein Problem, f\u00fcr das wir keine Wiederholungen haben, abgesehen von anderen Verzweigungen und Unklarheiten, die sich aus solchen Einzelfallstudien ergeben, die jedoch oft auch in kleinerem Ma\u00dfstab zunehmend relevant sind. Mit der Zunahme qualitativer Methoden in Vielfalt und F\u00fclle und dem Drang, auch komplexe Systeme und F\u00e4lle zu verstehen, besteht eindeutig ein Bedarf an der Integration von Wissen aus nat\u00fcrlichen Experimenten. '''Aus statistischer Sicht sind solche F\u00e4lle schwierig und herausfordernd, da sie nicht reproduzierbar sind, aber das Wissen kann dennoch relevant, plausibel und g\u00fcltig sein.''' Zu diesem Zweck proklamiere ich das Konzept der Nische, um zu veranschaulichen und zu konzeptualisieren, wie einzelne F\u00e4lle immer noch zur Produktion und zum Kanon des Wissens beitragen k\u00f6nnen.\n\nZum Beispiel die [https://academic.oup.com/rcfs/article/4/2/155/1555737#113865691 Finanzkrise von 2007], bei der viele Muster mit fr\u00fcheren Krisen vergleichbar waren, aber andere Faktoren anders waren. Daher ist diese Krise in Bezug auf einige Informationsebenen mit vielen fr\u00fcheren Faktoren und Mustern vergleichbar, aber auch in Bezug auf andere Dynamiken neu und nicht \u00fcbertragbar. Wir haben jedoch aufgrund des Einzelfalls dieser Finanzkrise verstanden, dass bestimmte Konstrukte in unseren Finanzsystemen korrupt sind, wenn nicht gar inh\u00e4rent fehlerhaft. Der Beitrag zur Weiterentwicklung der Finanzwelt ist daher unbestreitbar, auch soweit, dass viele Menschen darin \u00fcbereinstimmen, dass die vorgenommenen Ver\u00e4nderungen sicherlich nicht ausreichen. \n\nEin weiteres prominentes Beispiel f\u00fcr einen oder mehrere Einzelf\u00e4lle oder Ph\u00e4nomene ist die Covid-19-Pandemie, die sich weiter ausbreitet, w\u00e4hrend ich diese Zeilen schreibe. W\u00e4hrend aus fr\u00fcheren Pandemien viel gelernt wurde, ist diese Pandemie anders, entwickelt sich anders und hat andere Auswirkungen. Die Auswirkungen unserer Gesellschaften und die M\u00f6glichkeit, aus dieser Pandemie zu lernen, sind jedoch unbestreitbar. W\u00e4hrend klassische Experimente Wissen wie Bauern in einem Schachspiel entwickeln und Schritt f\u00fcr Schritt voranschreiten, gleicht eine Krise wie die Covid-Pandemie eher dem Pferd in einem Schachspiel, springt \u00fcber gr\u00f6\u00dfere L\u00fccken, ist weniger vorhersehbar und sicherlich schwieriger zu bew\u00e4ltigen. Die Evolution des Wissens in einer vernetzten Welt erfordert oft einen eher singul\u00e4ren Ansatz als Ausgangspunkt. Dies ist besonders wichtig in den normativen Wissenschaften, wo zum Beispiel in der Naturschutzbiologie viele Forscher L\u00f6sungen durch singul\u00e4re Fallstudien angehen. Aus diesem Grund entstand die l\u00f6sungsorientierte Agenda der Nachhaltigkeitswissenschaften, die diesem Umstand und dar\u00fcber hinaus Rechnung tr\u00e4gt.\n\n[[File:Lueneburg 2030.jpg|thumb|left|L\u00fcneburg 2030+ ist ein wunderbares Beispiel f\u00fcr eine Reihe von Realwelt-Eperimenten. Diese Karte bietet einen \u00dcberblick \u00fcber die verschiedenen Experimente.]]\n\nIn dieser Hinsicht sind [https://journals.sagepub.com/doi/pdf/10.1177/0963662505050791 Realwelt-Experimente] die neueste Entwicklung in der Diversifizierung des Experimentierfeldes. Diese Art von Experimenten wird derzeit in der Literatur umfassend untersucht, doch ich erkenne in der verf\u00fcgbaren Literatur bisher kein einheitliches Verst\u00e4ndnis dessen, was Experimente aus der realen Welt sind. Diese Experimente k\u00f6nnen jedoch als eine Fortsetzung des Trends der nat\u00fcrlichen Experimente angesehen werden, bei denen eine l\u00f6sungsorientierte Agenda versucht, eine oder mehrere Interventionen zu generieren, deren Wirkungen oft in Einzelf\u00e4llen getestet werden, wobei die Bewertungskriterien jedoch bereits vor der Durchf\u00fchrung der Studie klar sind. Die meisten Studien haben dies bisher mit Nachdruck definiert; dennoch zeichnet sich die Entwicklung von Experimenten aus der realen Welt erst langsam ab.\n\n\n== Experimente bis heute ==\nUnter dem Oberbegriff 'Experiment' werden somit unterschiedliche methodische Ans\u00e4tze zusammengefasst. W\u00e4hrend einfache Manipulationen wie medizinische Verfahren bereits in der Aufkl\u00e4rung als Experiment bekannt waren, gewann der Begriff 'Experiment' im Laufe des 20. Jahrhunderts an Bedeutung. Botanische Experimente wurden schon lange vorher durchgef\u00fchrt, aber es waren die Agrarwissenschaften, die die notwendigen methodischen Entw\u00fcrfe zusammen mit den geeigneten [[Bachelor Statistics Lecture|statistischen Analysen]] entwickelten und damit eine statistische Revolution ausl\u00f6sten, die in zahlreichen Wissenschaftsbereichen Wellen schlug. Die Varianzanalyse ([[ANOVA]]) wurde zum wichtigsten statistischen Ansatz zu diesem Zweck und erm\u00f6glichte die systematische Gestaltung von Versuchsanordnungen, sowohl im Labor als auch im landwirtschaftlichen Bereich. \n\nW\u00e4hrend Psychologie, Medizin, Agrarwissenschaft, Biologie und sp\u00e4ter auch die \u00d6kologie auf diese Weise in der Anwendung von Versuchspl\u00e4nen und Studien gediehen, gab es auch eine zunehmende Anerkennung von Informationen, die [[Bias and Critical Thinking|Bias]] erzeugten oder die Ergebnisse anderweitig verzerrten. '''Die ANOVA wurde daher durch zus\u00e4tzliche Modifikationen erg\u00e4nzt, was schlie\u00dflich zu fortgeschritteneren Statistiken f\u00fchrte, die in der Lage waren, sich auf verschiedene statistische Effekte zu konzentrieren'', und den Einfluss von Bias zu reduzieren, z.B. in Stichproben, statistischem Bias oder anderen Fehlern. Somit wurden [[Mixed-Effect Models]] zu einem fortgeschrittenen n\u00e4chsten Schritt in der Geschichte der statistischen Modelle, der zu komplexeren statistischen Designs und Experimenten f\u00fchrte, bei denen immer mehr Informationen ber\u00fccksichtigt wurden. Dar\u00fcber hinaus f\u00fchrten meta-analytische Ans\u00e4tze dazu, mehrere Fallstudien zu einem systematischen \u00dcberblick zusammenzufassen und zusammenzufassen. Dies war der Beginn eines integrativeren Verst\u00e4ndnisses verschiedener Studien, die zu einer [[Meta-Analysis|Meta-Analyse]] zusammengefasst wurden, wobei auch die unterschiedlichen Kontexte der zahlreichen Studien ber\u00fccksichtigt wurden. Dar\u00fcber hinaus konzentrierte sich die Forschung mehr und mehr auf ein tieferes Verst\u00e4ndnis einzelner Fallstudien, wobei der spezifische Kontext des jeweiligen Falles st\u00e4rker betont wurde. Solche Einzelfallstudien sind in der medizinischen Forschung seit Jahrzehnten von Wert, wo trotz des offensichtlichen Mangels an einem breiteren Beitrag h\u00e4ufig neue Herausforderungen oder L\u00f6sungen ver\u00f6ffentlicht werden. Solche medizinischen Fallstudien berichten \u00fcber neue Erkenntnisse, auftauchende Probleme oder andere bisher unbekannte Falldynamiken und dienen oft als Ausgangspunkt f\u00fcr weitere Forschung. Aus so unterschiedlichen Urspr\u00fcngen wie [[System Thinking & Causal Loop Diagrams|Systemdenken]], Stadtforschung, [[Ethnography|Ethnographie]] und anderen Forschungsfeldern entstanden [[Living Labs & Real World Laboratories|Realwelt-Experimente]], die im allt\u00e4glichen sozialen oder kulturellen Umfeld stattfinden. Die starren Entw\u00fcrfe von Labor- oder Feldexperimenten werden gegen ein tieferes Verst\u00e4ndnis des spezifischen Kontexts und Falls eingetauscht. W\u00e4hrend Experimente aus der realen Welt bereits vor einigen Jahrzehnten entstanden sind, beginnen sie erst jetzt, breitere Anerkennung zu finden. Gleichzeitig stellt die Reproduzierbarkeitskrise die klassischen Labor- und Feldexperimente in Frage, da man sich dar\u00fcber im Klaren ist, dass viele Ergebnisse - zum Beispiel aus psychologischen Studien - nicht reproduziert werden k\u00f6nnen. All dies deutet darauf hin, dass zwar ein gro\u00dfer Teil unseres wissenschaftlichen Wissens aus Experimenten stammt, dass aber auch \u00fcber die Durchf\u00fchrung der Experimente selbst noch viel zu lernen verbleibt.\n----\n[[Category: Normativity_of_Methods]]\n[[Category: Methods]]\n[[Category: Statistics]]\n[[Category: Qualitative]]\n[[Category: Deductive]]\n[[Category: Individual]]\n[[Category: System]]\n[[Category: Global]]\n[[Category: Present]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "ish96vhnrqzw12ankf2oa6dv3eovf5p"
                }
            },
            {
                "title": "Feynman Method",
                "ns": "0",
                "id": "810",
                "revision": {
                    "id": "5709",
                    "parentid": "5708",
                    "timestamp": "2021-06-08T09:10:29Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Links & Further Reading */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3742",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:\n\n# '''Select the topic you want to learn more about.''' This can be something you need to learn for an exam, or something you are just interested in knowing more about. Don't go to broad - focus on a specific topic. You will not be able to explain \"Economics\" or \"Physics\" in one go.\n# '''Find someone to talk to'''. Ideally, this person does not know anything about this topic. If you don't have someone to talk to, you can also just speak out loud to yourself, or write your presentation down. Start explaining the topic in simple terms.\n# '''Make notes.''' You will quickly realize yourself which parts of the topic you are not able to explain, and/or have not understood yourself. You might feel bad for a moment, but this step is important - it prevents you from pretending to yourself that you understood everything, when in fact you did not. Write down what you do not understand sufficiently! If you get feedback on which parts you did not properly explain, write this down, too. Lastly, write down where you used very technical, specific terms, even if your audience might have understood them. Someone else might not, and you should be able to do without them.\n# '''Have a look at your notes and try to find more information.''' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!\n\n== Links & Further Reading ==\n* [https://karrierebibel.de/feynman-methode/ Karrierebibel]\n* [https://blog.doist.com/feynman-technique/ ToDo-ist]\n* [https://www.goodwall.io/blog/feynman-technique/ Goodwall]\n* [https://www.youtube.com/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "2ea1bcf5crtltmcgghvelrylp6miecd"
                }
            },
            {
                "title": "Field experiments",
                "ns": "0",
                "id": "361",
                "revision": {
                    "id": "7284",
                    "parentid": "7185",
                    "timestamp": "2023-09-09T21:22:02Z",
                    "contributor": {
                        "username": "Stepkurniawan",
                        "id": "129"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "25245",
                        "#text": "'''Note:''' This entry revolves mostly around field experiments. For more details on experiments, please refer to the entries on [[Experiments and Hypothesis Testing]], [[Case studies and Natural experiments]] as well as [[Experiments]].\n\n==The field experiment==\n[[File:Bildschirmfoto 2020-05-21 um 15.46.03.png|thumb|Field experiments were as you may could guess first conducted in agriculture.]]\nWith a rise in knowledge, it became apparent that the controlled setting of a laboratory was not enough. First in astronomy, but then also in agriculture and other fields the notion became apparent that our [https://sustainabilitymethods.org/index.php/Experiments#History_of_laboratory_experiments reproducible settings may sometimes be hard to achieve]. Observations can be unreliable, and error of measurements in astronomy was a prevalent problem in the 18th and 19th century. Fisher equally recognised the mess - or variance - that nature forces onto a systematic experimenter. The demand for more food due to the rise in population, and the availability of potent seed varieties and fertiliser - both made possible thanks to scientific experimentation - raised the question how to conduct [https://explorable.com/field-experiments experiments under field conditions]. Making experiments in the laboratory reached its outer borders, as plant growth experiments were hard to conduct in the small confined spaces of a laboratory, and it was questioned whether the results were actually applicable in the real world. Hence experiments literally shifted into fields, with a [https://www.tutor2u.net/psychology/reference/field-experimentsme dramatic effect on their design, conduct and outcome]. While laboratory conditions aimed to minimise variance - ideally conducting experiments with a high confidence -, the new field experiments increased sample size to tame the variability - or messiness - of factors that could not be controlled, such as subtle changes in the soil or microclimate. [https://en.wikipedia.org/wiki/Field_experiment#Examples Establishing the field experiment] became thus a step in the scientific development, but also in the industrial development. Science contributed directly to the efficiency of production, for better or worse.\n\n==Key concepts in the application of field experiments==\n====The arbitrary p value====\n[[File:Hypothesis Testing \u00fcberarbeitet.png|thumb|You are probably already familiar with this type of overview but it is one of the most commen research designs.]]\nThe analyis of variance was actually inspired widely by field experiments, as Fisher was working with crop data which proved as a valuable playground to develop and test his statistical approaches. It is remarkable that the p-value was actually chosen by him and did not follow any wider thinking within a community. Probably not since Linne or Darwin did a whole century of researchers commit their merits to a principle derived by one person, as scientific success and whole careers are build on results measured by the p-value. It should however be noted that the work needed to generate enough replicates to measure probability with Fishers measure of 0.05 was actually severe within crop experiments when compared to lab settings. In a lab you can have a few hundred Petri dishes or plant pots and these can be checked by a lab technician. Replicates in an agricultural field are more space and labour intense, while of course lab space is more expensive per square meter. In order to design field experiments where enough replicates were made, in order to enable testing at an appropriate significance level, demanded nothing less than a revolution in the planning of studies. With all its imperfections and criticism, Fishers use of the p-value can probably be credited for increasing sample number in experiments towards a sensible and sufficient level. [https://conjointly.com/kb/experimental-design/ Study designs] became the new norm, and are an essential part of science ever since.\n\n====Randomisation====\n[[File:Bildschirmfoto 2020-05-21 um 15.54.10.png|thumb|Field experiments are often divided into quadrangles, and these then represent a certain treatment or treatment combination. In a randomized experiment, a study sample is then randomly divided into treatment and control groups.]]\nField experiments try to compensate for gradients that infer variance within a sample design by increasing the number of samples. However, in order to uncouple the effects of underlying variance, [https://isps.yale.edu/node/16697 randomisation] is central. Take the example of an agricultural field that has more sandy soils on one end, and more loamy soils on the other end. If now all samples of one treatment would be on the loamy end of the field, and all samples of another treatment would be at the sandy end of the field, then the treatment effect would contain an artefact of the soil gradient. Randomisation in fields is hence essential to maximize independence of the treatment effect from the underlying variance. A classical chessboard illustrates the design step of randomisation quite nicely, since field experiments are often divided into quadrangles, and these then represent a certain treatment or treatment combination. Especially in [https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design agricultural fields], such an approach was also following a certain pragmatism, since neighbouring quadrangles might have a tendency to influence each other, i.e. through root competition. While this error is often hard to take into account, it is quite relevant to notice that enough replicates might at least allow to tame this problem, and make the error introduced by such interactions less relevant overall. Just as we should apply randomisation when sampling for instance a representative sample from a population, the same approach is applied in the design of experiments.\n\n====Blocking====\n[[File:Block Experiments.jpg|thumb|Designing an experiment using block effects.]]\nIn order to tame the variance of the real world, [https://www.youtube.com/watch?v=10ikXret7Lk blocking] was a plausible approach. By basically introducing agricultural fields as [https://www.ndsu.edu/faculty/horsley/RCBD.pdf blocks], the variance from individual blocks can be tamed. This was one of the breakthroughs, as the question of what we want to know i.e. hypothesis testing, was statistically uncoupled from the question what we do not want to know, i.e. the variance inferred from individual blocks. Consequently, the samples and treatment combinations need to be randomised within the different blocks, or can be alternatively replicated within these blocks. This has become established as a standard approach in the designing of experiments, often for rather pragmatic reasons. For instance are [https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design/Common-Research-Designs-for-Farmers agricultural fields] often showing local characteristics in terms of soil and microclimate, and these should be tamed by the clear designation of blocks and enough blocks in total within the experiment. The last point is central when thinking in terms of variance, since it would naturally be very hard to think in terms of variance regarding e.g. only two blocks. A higher number of blocks allow to better tame the block effect. This underlines the effort that often needs to go into designing experiments, since a sufficient number of blocks would basically mean that the effort can be multiplied by the number of blocks that are part of the design. Ten blocks means ten times as much work, and maybe with the result that there is no variance among the blocks overall.\n\n====Nested designs====\n[[File:Bildschirmfoto 2020-05-21 um 17.06.05.png|thumb|A nested design is used for experiments in which there is an interest in a set of treatments and the experimental units are sub-sampled.]]\nWithin field experiments, one factor is often nested within another factor. The [https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ principle of nestedness] works generally like the principle of Russian dolls: Smaller ones are encapsulated within larger ones. For instance can a block be seen as the largest Russian doll, and the treatments are then nested in the block, meaning each treatment is encapsulated within each block. This allows for a testing where the variance of the block effect can be minimised, and the variance of the treatment levels can be statistically compared. Quite often the variance across different levels of nestedness is a relevant information in itself, meaning for instance how much variance is explained by a different factor. Especially spatially [https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf nested designs] can have such a hierarchical structure, such as neighbourhoods within cities, streets within neighbourhoods and houses in streets. The nested structure would in this case be Cities/neighbourhoods/streets/houses. Just as with blocks, a nested structure demands a clear designing of an experiment, and greatly increase the sample size. Hence such a design should be implemented after much reflection, based on experience, and ideally by [[Glossary|consultation]] with experts both in statistics as well as the given system.\n\n==Analysis==\n[[File:Farm-fields-crops-green.jpg|thumb|left|How to grow our plants best? If we simplify our model and eliminate nonsignificant treatments, we may find out.]]\nThe analysis of field experiments demands great care, since this is mostly a deductive approach where equal emphasis is put on what we understand, and what we do not understand. Alternatively, we could highlight that rejection of the hypothesis is the most vital step of any experiment. In statistical terms the question of explained vs. unexplained variance is essential. \nThe first step is however checking the p-value. Which treatments are significant, and which ones are not? When it comes to two-way [[ANOVA]]s, we may need to reduce the model to obtain a minimum adequate model. This basically equals a reduction of the full model into the most parsimonious version, following Occam's razor. While some researchers tend to report the full model, with all non-significant treatments and treatment combinations, I think this is wrong. If we reduce the model, the p-values change. This can make the difference between a treatment that is significant, and a non-significant model. Therefore, [https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/ model reduction] is being advised for. For the sake of simplicity, this can be done by first reducing the highest level interactions that are not significant. However, the single effects always need to be included, as the single effects are demanded if interactions of a treatment are part of the model, even if these single effects are not significant. An example of such a procedure would be the NPK dataset in R - it contains information about the effect of nitrogen, phosphate and potassium on the growth of peas. There, a full model can be constructed, and then the non-significant treatments and treatment interactions are subsequently removed to arrive at the minimum adequate model, which is the most parsimonious model. This illustrates that Occam's razor is not only a theoretical principle, but has direct application in statistics. \n\n<syntaxhighlight lang=\"R\" line>\n\n#the dataset npk contains information about the effect of nitrogen, phosphate and potassium on the growth of peas\n#let us do a model with it\ndata(npk)\nstr(npk)\nsummary(npk)\nhist(npk$yield)\n\npar(mfrow=c(2,2))\npar(mar=c(2,2,1,1))\nboxplot(npk$yield~npk$N)\nboxplot(npk$yield~npk$P)\nboxplot(npk$yield~npk$K)\nboxplot(npk$yield~npk$block)\ngraphics.off()\n\nmp<-boxplot(npk$yield~npk$N*npk$P*npk$K)#tricky labels\n#we can construct a full model\nmodel<-aov(yield~N*P*K,data=npk)\nsummary(model)\n\n#then the non-significant treatments and treatment interactions are subsequently removed \n#to arrive at the minimum adequate model\nmodel2<-aov(yield~N*P*K+Error(block),data=npk)\nsummary(model2)\n\nmodel3<-aov(yield~N+P+K+N:P+N:K+P:K+Error(block),data=npk)\nsummary(model3)\n\nmodel4<-aov(yield~N+P+K+N:P+N:K+Error(block),data=npk)\nsummary(model4)\n\nmodel5<-aov(yield~N+P+K+N:P+Error(block),data=npk)\nsummary(model5)\n\nmodel6<-aov(yield~N+P+K+Error(block),data=npk)\nsummary(model6)\n\nmodel7<-aov(yield~N+K+Error(block),data=npk)\nsummary(model7)\n\n</syntaxhighlight>\n\nOnce we have a minimum adequate model, we might want to check the explained variance as well as the unexplained variance. Within a block experiment we may want to check how much variance is explained on a block level. In a nested experiment, the explained variance among all levels should be preferably checked. \nIn a last step, it could be beneficial to check the residuals across all factor levels. While this is often hampered by a smaller sample size, it might be helpful to understand the behaviour of the model, especially when initial inspection in a boxplot showed flaws or skews in the distribution.\n\n==Fixed effects vs. Random effects==\n[[File:Smoking trooper.jpg|thumb|right|If smoking is a fixed or a random effect depends on the study design]]\nWithin [[ANOVA]] designs, the question whether a variable is a [https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To this end, it is helpful to consult existing studies to differentiate between [https://www.youtube.com/watch?v=Vb0GvznHf8U random and fixed factors]. Current medical trials may consider many variables, and have to take even more random factors into account. Testing the impact of random factors on the raw data is often a first step when looking at initial data, yet this does not help if it is a purely deductive design. In this case, simplified pre-tests are often a first step to make initial attempts to understand the system and also check whether variables - both fixed or random - are feasible and can be utilised in the respective design. Initial pre-tests at such smaller scales are a typical approach in medical research, yet other branches of research reject them as being too unsystematic. Fisher himself championed small sample designs, and we would encourage pre-tests in field experiments if at all possible. Later flaws and errors in the design can be prevented, although form a statistical standpoint the value of such pre-tests may be limited at best.\n\n==Unexplained variance==\n[[File:Karl Popper.jpg|thumb|left|Karl Popper emphasised that facts can only be approximated, i.e. are only considered to be true as long as they are not falsified.]]\n[[File:Ronald Fisher.jpg|thumb|right|Fisher's approach of considering the variance of the real world in statistics can be seen as perfectly complementary to Popper's epistemological reflections.]]\nAcknowledging unexplained variance was a breakthrough in modern science, as we should acknowledge that understanding a phenomena fairly well now is better than understanding something perfectly, but never. In a sense was the statistical developing of uncertainty reflected in philosophical theory, as [https://sustainabilitymethods.org/index.php/Hypothesis_building#Testing_of_hypothesis Karl Popper] highlighted the imperfection of experiments in testing or falsification of theories. Understanding the limitations of the scientific endeavour thus became an important baseline, and the scientific experiments tried partly to take this into account through recognising the limitations of the result. What is however often confused is whether the theory is basically imperfect - hence the results are invalid or implausible - or whether the experiment was conducted in an imperfect sense, making the results unreliable. The imperfection to understand the difference between flaws in theory and flaws in conducting the experiment is a continuous challenge of modern science. When looking at unexplained variance, we always have to consider that our knowledge can be limited through theory and empirical conduct, and these two flaws are not clearly separated. Consequently, unexplained variance remains a blank in our knowledge, and should always be highlighted as such. As much as it is important to acknowledge what we know, it is at least equally important to highlight what we do not know.\n\n==Interpretation of field experiments==\nInterpreting results from field experiments demands experience. First of all, we shall interpret the p-value, and check which treatments and interactions are significant. Here, many researchers argue that we should report the full model, yet I would disagree. P-values in ANOVA summaries differ between the so called full models -which include all predictors- and minimum adequate models -which thrive to be the most parsimonious models. Model reduction is essential, as the changing p-values may make a difference between models that are reporting true results, or flawed probabilities that vaporize once the non-significant terms are subsequently reduced. Therefore, one by one we need to minimize the model in its complexity, and reduce the model until it only contains significant interaction terms as well as the maybe even non-significant single terms, which we have to include if the interaction is significant. This will give us a clear idea which treatments have a significant effect on the dependent variable. \nSecond, when expecting model results we should interpret the sum of squares, thereby evaluating how much of the respective treatment is explain the effect of the dependent variable. While this is partly related to the p-value, it is also important to note how much variance is explained by potential block factors. In addition, it is also important to notice how much remains unexplained in total, as this residual variance indicates how much we do not understand using this experimental approach. This is extremely related to the specific context, and we need to be aware that knowledge of previous studies may aid us in understanding the value of our contribution. \nLastly, we need to take further flaws into our considerations when interpreting results from field experiments. Are there extreme outliers. How do the residuals look like? Is any treatment level showing signs of an uneven distribution or gaps? Do the results seem to be representative? We need to be very critical of our own results, and always consider that the results reflect only a part of reality.\n\n==Replication of experiments==\n[[File:Bildschirmfoto 2020-05-21 um 17.10.27.png|thumb|One famous example from the discipline of psychology is the Milgram shock experiment carried out by Stanley Milgram a professor from the Yale University in 1963.]]\nField experiments became a revolution for many scientific fields. The systematic testing of hypotheses allowed first for [https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 agriculture] and [https://revisesociology.com/2016/01/17/field-experiments-sociology/ other fields] of production to thrive, but then also did medicine, [https://www.simplypsychology.org/milgram.html psychology], ecology and even [https://www.nature.com/articles/s41599-019-0372-0 economics] use experimental approaches to test specific questions. This systematic generation of knowledge triggered a revolution in science, as knowledge became subsequently more specific and detailed. Take antibiotics, where a wide array of remedies was successively developed and tested. This triggered the cascading effects of antibiotic resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can biased towards a specific results. Third, statistics slowly eroded into a culture where more complex models and the rate of statistical fishing increased. Here, a preregistration of your design can help, which is often done now in psychology and medicine. Researchers submit their study design to an external platform before they conduct their study, thereby safeguarding from later manipulation. Much can be said to this end, and we are only starting to explore this possibility in other arenas. However, we need to be aware that also when we add complexity to our research designs, especially in field experiments the possibility of replication diminished, since we may not take factors into account that we are unaware of. In other words, we sacrifice robustness with our ever increasing wish for more complicated designs in statistics. Our ambition in modern research thus came with a price, and a clear documentation is one antidote how we might cure the flaws we introduced through  our ever more complicated experiments. Consider Occam\u2019s razor also when designing a study.\n\n==External Links==\n===Articles===\n\n[https://explorable.com/field-experiments Field Experiments]: A definition\n\n[https://www.tutor2u.net/psychology/reference/field-experiments Field Experiments]: Strengths & Weaknesses\n\n[https://en.wikipedia.org/wiki/Field_experiment#Examples Examples of Field Experiments]: A look into different disciplines\n\n[https://conjointly.com/kb/experimental-design/ Experimental Design]: Why it is important\n\n[https://isps.yale.edu/node/16697 Randomisation]: A detailed explanation\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design Experimental Design in Agricultural Experiments]: Some basics\n\n[https://www.ndsu.edu/faculty/horsley/RCBD.pdf Block Design]: An introduction with some example calculations\n\n[https://www.sare.org/Learning-Center/Bulletins/How-to-Conduct-Research-on-Your-Farm-or-Ranch/Text-Version/Basics-of-Experimental-Design/Common-Research-Designs-for-Farmers Block designs in Agricultural Experiments]:Common Research Designs for Farmers\n\n[https://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/ Difference between crossed & nested factors]: A short article\n\n[https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ANOVA-III.pdf Nested Designs]: A detailed presentation\n\n[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/ Model reduction]: A helpful article\n\n[https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https://www.simplypsychology.org/milgram.html Field Experiments in Psychology]: A famous example\n\n[https://www.nature.com/articles/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https://revisesociology.com/2016/01/17/field-experiments-sociology/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https://www.youtube.com/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction\n\n[https://www.youtube.com/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "pwtisuzwy5qsq2y181gvae1kgckfp9z"
                }
            },
            {
                "title": "Fishbowl Discussion",
                "ns": "0",
                "id": "350",
                "revision": {
                    "id": "3255",
                    "parentid": "2387",
                    "timestamp": "2020-11-04T10:21:17Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4329",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\n'''A Fishbowl is a medium-length dialogue format for large groups, lead by a moderator'''. Four to five people discuss a question or topic while being surrounded by a typically much bigger audience, which is enabled to participate in the discussion by taking one of the chairs in the discussion group. The Fish Bowl format can be used for teaching (e.g. group discussions in seminars), for workshops (e.g. in work environments, events, organisational activities), or for stage discussions (e.g. on conferences). Depending on the context, the lenght of a fishbowl discussion can range from 15-20 minute sessions to more than an hour, with 30-45 minutes being rather typical.\n\n== Goal ==\n* Engage the whole audience in a discussion in an active and structured manner, which makes the discussion more interesting, lively and and richer in perspectives.\n\n== Getting started ==\n[[File:Fishbowl visualisation.png|350px|thumb|right|The Fishbowl arrangement. Source: [https://en.wikipedia.org/wiki/Fishbowl_%28conversation%29#/media/File:Fishbowl_diagram_172.png Wikimedia]]]\n\n* In a typical Fishbowl, four to five chairs are arranged in a circle, with a larger number of chairs forming another circle around them. In a variation, only two chairs may be placed for a face-to-face conversation. Participants for the discussions, including one moderator, occupy the chairs, with the audience filling the surrounding chairs. This arrangement (see visualisation) gives the Fishbowl its name. The discussants should be rather heterogenous to allow for an interesting and potentially controversial discussion.\n\n* '''There are two different forms of the Fishbowl:'''\n** The open Fishbowl, where one chair is left empty and any audience member may, at any time of the discussion, enter the discussion circle, take the chair and thus force one of the discussants to leave his/her own chair and go back into the audience. Instead of leaving one chair unoccupied, the audience members may also tap one participant on the shoulder to indicate them to leave.\n** The closed fishbowl, where an audience member may only enter the stage when one of the discussants leaves after a pre-determined amount of time.\n\n* The audience member that takes the free chair immediately receives the right to speak. In general, only one person is allowed to speak at a time, with the moderator ensuring a respectful discussion. The amount of audience members that participate in the discussion depends on the size of the audience, the purpose of the discussion and the time budget. (commonly 30-45 minutes).\n* The discussion should be led by the moderator in a way that makes everybody feel like they can contribute. Questions should not be too specific or complex for the general audience to understand and the moderator (as well as the other discussants) should provide a feeling of welcoming all opinions and levels of expertise.\n* A challenge with the Fishbowl format is that shy people are often not joining the discussion as much as people who feel like they have a lot to say. In a classroom setting, this may be solved both by splitting the students into groups and thus limiting the size of the audience, as well as through time limits for each discussant so that all students have to participate at some point.\n\n== Links & Further reading ==\n''Sources''\n* [https://en.wikipedia.org/wiki/Fishbowl_(conversation) Wikipedia]\n- [https://www.ines-stade.de/was-ist-fishbowl/ INES STADE: Was ist Fishbowl?]\n- [http://nexttimeteaching.com/everything-wrong-fishbowl-discussions/ Next Time Teaching: Everything that could go wrong with your fishbowl discussions]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "o2rcd9a7xp9vd1z89ls4kels5g0cyoi"
                }
            },
            {
                "title": "Flashlight",
                "ns": "0",
                "id": "466",
                "revision": {
                    "id": "3254",
                    "parentid": "2984",
                    "timestamp": "2020-11-04T10:20:56Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3161",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> ''Is everyone on the same page?''\n<br> ''Are there important issues that have been neglected so far?''\n<br> ''Is there unspoken dissonance?''\n<br> ''Is there an elephant in the room?''\n<br> ''What are we actually talking about?''\n\n== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended. \n\n===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes.\n* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> ''What song would you choose to characterize the current state of discussion, and why?'' <br> ...\n\n== Links ==\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/blitzlicht/\n<br> https://www.bpb.de/lernen/formate/methoden/62269/methodenkoffer-detailansicht?mid=115\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar M\u00f6lleken."
                    },
                    "sha1": "so5b3fgwh9zr5x82vlodqsrg8aq3rne"
                }
            },
            {
                "title": "Flipped Classroom",
                "ns": "0",
                "id": "729",
                "revision": {
                    "id": "6777",
                    "parentid": "5034",
                    "timestamp": "2022-09-21T09:48:38Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* Advantages and Challenges of Flipped Classroom approaches */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5339",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\n'''A Flipped Classroom is a teaching concept that flips the classical structure of in-class vs out-of-class.''' Teaching material is provided for the students (e.g. digitally) to engage with at home, while the in-class sessions presuppose knowledge of the material and are then used for interactive discussions and exercises.\n\n== Goals ==\n* Change the structure of your class to support different learning types and schedules\n* Make classes more fun, engaging and effective\n\n== Getting started ==\n'''There is no unified conceptual approach to the *Flipped Classroom* as the concept is rather new.''' However, there are general elements that all approaches share.\n\nBased on these, the flipped classroom elements can be categorized two-fold: structurally and didactically. \n\n'''On a structural level, the flipped classroom can be split into in-class and out-of-class activities.''' The ''out-of-class'' activities provide the students with a range of learning material, such as pre-recorded lectures, online tests, reading material, homework exercises, additional videos, podcasts etc. The students receive this material, preferably digitally via a learning platform, and can generally engage with the material whenever they like. However, the instructor may expect the students to know the content of the material when the in-class session starts, and may check this knowledge at the beginning of the session. In presence (= ''in-class''), the students are not faced with more frontal learning, but instead, these sessions are structured around active learning exercises. These may include group discussions, group exercises, Q&A sessions or interactive tests (for example via Mentimeter), challenges, panel discussions etc. The students should not receive (much) new content, but rather consolidate their knowledge in these sessions and actively engage with the topics in their peer groups.\n<br/>\n[[File:Flipped Classroom.png|800px|thumb|center|'''A comparison between traditional and flipped classroom structures.''' Here, out-of-class content is limited to videos and assessments, but more formats are imaginable. Source: Velegol et al. 2015, p.3]]\n<br/>\n'''On a didactic level, the teacher (professor) assumes a different role in Flipped Classrooms compared to traditional lectures''': (s)he is no longer only an information provider, but rather becomes a learning guide to support active learning. To this end, the Flipped Classroom intends to put an emphasis on student-centered learning theories such as active learning, peer-assisted learning and collaborative learning (Ak\u00e7ay\u0131r & Ak\u00e7ay\u0131r, 2018, p. 335). This changes the atmosphere of the sessions, and allows for the students to engage with the content of the lecture from more diverse perspectives, which may support different learning types.\n\n== Advantages and Challenges of Flipped Classroom approaches ==\nAk\u00e7ay\u0131r & Ak\u00e7ay\u0131r (2018) provide a range of hypothesized advantages and disadvantages which they base on a systematic literature review. These include improved learning outcomes, higher student satisfaction, enjoyment and engagement, better student-student and student-instructor interaction and more efficient use of class time. On the other hand, challenges include higher workloads for both students and instructors, a subsequent lack of preparedness for the in-class sessions and technical issues. Since Flipped Classrooms are a rather young concept, there is not much evidence on its effectiveness. Still, the existing literature indicates positive outcomes compared to traditional lecturing styles. It may be noted that the benefits of the concept may vary depending on the topic of the class, as well as other factors such as class size. Just as any other learning format are flipped classroom settings context dependent, and need to be adapted to the respective settings.\n\n== Links & Further reading ==\n* Bishop, J. L., & Verleger, M. A. (Eds.) 2013. ''The flipped classroom: A survey of the research.'' Vol. 9.\n* Akcayir, G. Akcayir, M. 2018. ''The flipped classroom: A review of its advantages and challenges.'' Computers & Education 126. 334-345.\n* Findlay-Thompson; S. Mombourquette, P. 2014. ''Evaluation of a flipped classroom in an undergraduate business course.'' Business education & accreditation 6(1). 63-71.\n* Velegol, S.B. Zappe, S. E. Mahoney, E. 2015. ''The Evolution of a Flipped Classroom: Evidence-Based Recommendations.'' Advances in Engineering Education.\n* Tucker, B. 2012. ''The Flipped Classroom. Online instruction at home frees class time for learning.'' Education Next.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Matteo Ramin and Christopher Franz."
                    },
                    "sha1": "gijuer8ugrl6ts04fd25v1dwp0g9mno"
                }
            },
            {
                "title": "Focus Groups",
                "ns": "0",
                "id": "742",
                "revision": {
                    "id": "6312",
                    "parentid": "6220",
                    "timestamp": "2021-09-01T07:14:27Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "22007",
                        "#text": "[[File:ConceptFocusGroups.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Focus Groups]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Focus Groups are a specific type of group discussion with which qualitative data on the participants\u2019 viewpoints concerning a topic of focus is gathered by making use of the group interaction and dynamics. For more Interview forms, and more on Interview methodology in general, please refer to the [[Interviews||Interview overview page]].\n\n\n== Background ==\n[[File:Focus Group.png|400px|thumb|right|'''SCOPUS hits per year for Focus Group until 2020.''' Search terms: 'Focus Group' in Title, Abstract, Keywords. Source: own.]]\n'''The first use of Focus Groups can be dated back to market research the 1920s''' and later to the 1950s when scientists of the Columbia University studied the effect that the governments\u2019 war propaganda on the radio had on citizens (1, 7). In the latter, the scientists had initially started out with a quantified approach where the participants pressed red and green buttons depending on how they felt about the radio reports they heard, until one researcher of the team, Robert Merton, proposed to instead follow a qualitative technique that he had developed for this project. More precisely, the participants would be interviewed as a group to discover their \u201csubjective reactions\u201d (Bloor et al. 2001, p. 2) to the radio clips. Together with a team of scientists, Merton devised \u201ca fairly standardized set of procedures for these interviews\u201d (Bloor et al. 2001, p. 2) and published them together with Patricia Kendall in the American Journal of Sociology under the title \u201cThe focused interview\u201d in 1946 (1).\n\nWhile the Focus Group methodology as such did not find much further use in sociology at first, its \u201ccommercial potential\u201d for marketing and advertising continued to be apparent, so that Focus Groups started to be used increasingly in commercial market research from the 1960s onwards (1).\n\n'''Seeing the successful application of Focus Groups by private marketing companies, organizations of the public sector started to adopt the method''' to e.g. examine the impact of state-run campaigns. However, the use of Focus Groups differs between the public and the private sector. This divergence is mostly a consequence of costs being kept low in the private sector and relates to a less rigorous analysis of the data then in the public sector as well as academia (1).\n\nAlready by 2001, Focus Groups were considered a \u201cmainstream method\u201d (Bloor et al. 2001, p. 17) which enable access to norms and meanings of \u201cincreasingly privatised societies which are less open to observational methods\u201d (Bloor et al. 2001, p. 17). Even though Focus Groups can be used as a stand-alone method, they are usually applied \u201cto complement, prepare for, or extend\u201d other methods (Bloor et al. 2001, p. 18). Further, Focus Groups can be used as a means to \u201cdemocratize the research process by functioning as a forum for public participation\u201d (Bloor et al. 2001, p. 18).\n\n\n== What the method does ==\n'''Focus Groups are a specific type of group discussion held in a rather informal setting''' (3, 5). The ''focus'' in Focus Groups refers to a specific topic or a set of topics being discussed, or an activity being undertaken by the participants during the focus group session (3, 4)\n\nFocus Groups clearly differ from group interviews, as the moderator does not just aim to gather answers from the participants but to achieve group interaction (5, 7). It is not just of interest what participants say, but also how they say it, and how they react to what others say. The objective lies also in understanding \u201cthe meanings and norms which underlie those group answers\u201d (Bloor et al., p. 43) and to \u201cdraw out the cognitive structures which previously have been unarticulated\u201d (Kitzinger, p. 106).\n\n[[File:Focus Group example.png|500px|thumb|right|'''Exemplary Focusing Exercise in order to stimulate attention and discussion among the Focus Group participants on the topic of interest'''. Source: Bloor et al. 2001, p. 44]]\n\nThe types of data gathered during a Focus Group concern individual, group as well as group interaction data (5, 4). More precisely, the interest lays \u201cnot solely in what people thought but in ''how'' they thought and ''why'' they thought as they did\u201d (Kitzinger, p. 104).\n\nConcerning the composition of Focus Groups, one can choose to recruit either pre-existing groups \u201cwho already [know] each other through living, working or socialising together\u201c (Kitzinger, p. 105), or to organize people into newly formed groups \u201cthat the researcher constructs by selecting members either randomly or, much more commonly, via [other] [\u2026] sampling techniques\u201d (Onwuegbuzie, p. 4). The benefits of choosing pre-existing groups are that one might \u201ctap into fragments of interactions which approximated to 'naturally occurring' data\u201d (Kitzinger, p. 105) as people are used to converse with one another on a regular basis and that the participants can \u201crelate each other's comments to actual incidents in their shared daily lives\u201d (Kitzinger, p. 105).\n\n'''Further, one distinguishes between complementary and argumentative interaction''', which refers to the similarities and differences between the Focus Group participants. A group composed of people with similar experiences might make participants feel comfortable voicing and explaining their viewpoints and perceptions, which \u201cmay be particularly useful when one wishes to gain access to critical comments from groups\u201d (Kitzinger, p. 112). A diverse group composition on the other hand \u201censures that people are forced to explain the reasoning behind their thinking\u201d (Kitzinger, p. 113) to the other participants and allows the facilitator to study the differences of opinion as well as their roots together with the group (3). However, a group can never be fully homogeneous, and both the complementary as well as the argumentative interaction are important for the data collection process (3).\n\n'''The data gathered during Focus Groups \u2013 i.e. audiotapes, notes on non-verbal data, and items recalled \u2013 can be analyzed with a [[Content Analysis]]''' (4, 5). However, several aspects unique to Focus Group data should be additionally considered, e.g. regarding the relative contribution of single participants to the discussion, social factors affecting the degree of [[Glossary|participation]], the group and temporal context of statements and opinions as well as their potential alteration throughout the discussion as well as measures of dominance (e.g. the number of interventions and words spoken by different participants and their impact on the course of the discussion) (5).\n\n==== Organisation ====\nThe Focus Group size, i.e. the number of participants per Focus Group, should allow for enough information and viewpoints on the given topic while at the same time ensuring a comfortable atmosphere and enough opportunities for everyone to express themselves (4, 6). Around four to twelve participants can serve as a guide for the Focus Group size (4, 6). Over-recruiting the Focus Groups to factor in participants who do not show up is recommended (4). As opposed to the Focus Group size, the sample size is the number of Focus Groups conducted to answer the research question (2). The number of Focus Groups should ideally be based on the point of data or theoretical saturation (4, 2). However, financial and time constraints often become the cornerstone of decision-making in this regard (2).\n\nDuring recruitment, a short questionnaire may be handed out to the potential participants of the Focus Group. The filled-out questionnaires can help in composing the Focus Groups along criteria such as gender, age, and other factors important to the research (5).\n\nAs group interaction in Focus Groups is crucial, '''the role of the researcher is rather peripheral''' and consists of the moderation and [[Glossary|facilitation]] of the group discussion. '''What counts are the group dynamics and the interpersonal relationships between the participants''' (5). The facilitator should thus \u201cencourage people to engage with one another\u201d (Kitzinger, p. 106) and to animate the discussion (5). This is usually achieved by asking open-ended questions (5). Further, group exercises can be included into the Focus Group session. Besides encouraging interaction among the participants, those exercises are meant to draw the participants' attention away from the facilitator and towards the other participants and topic of interest, and to make them feel comfortable in the group. Further, such exercises can evoke physical reactions (such as twitching when a card is put into the \u2018wrong\u2019 category) which may hint towards approval or disagreement within the group and can then be explored further (3). The figure above shows an exemplary focusing exercise (Bloor et al. 2001, p. 44).\n\nIn addition to the moderator, an assistant moderator is suggested who is responsible for \u201crecording the session (i.e., whether by audio- or videotape), taking notes, creating an environment that is conducive for group discussion (e.g., dealing with latecomers, being sure everyone has a seat, arranging for refreshments), providing verification of data, and helping the researcher/moderator to analyze and/or interpret the focus group data\u201d (Onwuegbuzie, p. 4). The duration of Focus Groups commonly ranges between one to two hours (4). One and the same Focus Group can be invited a single or multiple times depending on the research objective (4).\n\n\n== Strengths & Challenges ==\n==== Strengths ====\n* Focus Groups allow to explore people\u2019s views in a social context: \u201cWe learn about the 'meaning' of AIDS, (or sex, or health or food or cigarettes) through talking with and observing other people, through conversations at home or at work; and we act (or fail to act) on that knowledge in a social context. When researchers want to explore people's understandings, or to influence them, it makes sense to employ methods which actively encourage the examination of these social processes in action.\u201d (Kitzinger, p. 117). As S\u00e4yn\u00e4joki et al. (2014, p.6625) highlight: \"Focus groups are particularly useful in studies where the researcher seeks to uncover attitudes, perceptions and beliefs.\"\n* Furthermore, the group atmosphere makes many participants feel more at ease and encourages them to share their thoughts and perceptions.\n* Finally, Focus Groups are a way to gather data from several people at a time and are thus considered to be cost-effective and timesaving (4).\n\n==== Challenges ====\n* A challenge one needs to be aware of when conducting and analyzing Focus Groups is the censorship of certain \u2013 e.g. minority or marginalized \u2013 viewpoints, which can arise from the group composition (3). As Parker and Tritter (2006, p. 31) note: \u201cAt the collective level, what often emerges from a focus group discussion is a number of positions or views that capture the majority of the participants\u2019 standpoints. Focus group discussions rarely generate consensus but they do tend to create a number of views which different proportions of the group support.\"\n* Further, considering the effect group dynamics have on the viewpoints expressed by the participants is important, as the same people might answer differently in an individual interview. Depending on the focus of the study, either a Focus Group, an individual interview or a combination of both might be appropriate (3).\n* Last but not least - and as with other qualitative research methods - Focus Groups do not necessarily aim for representativeness but for an in-depth understanding of the perspectives studied (3). However, \u201cas the number of focus groups in the overall sample increases and their composition broadens, there is an extent to which the representativeness of their findings might be viewed as more acceptable and valid.\u201d (Parker, pp. 30).\n\n\n== Outlook ==\nWith regards to future research, Bloor et al. (2001) see the necessity to employ more resources and effort regarding \u201cgroup composition, recruitment, planning, conduct, transcription and analysis\u201d (p. 89) for Focus Groups to fulfill the requirements on methods used in the field of academic social research (in comparison to pure market research for example).\n\nAdditionally, future publications of studies which applied Focus Groups should place more focus on the impact the recruitment and the sampling process had on the collection, analysis, and interpretation of the data. Thus, more information on how Focus Group participants were sampled or on which basis a certain sample size (i.e. number of groups) was chosen should be provided, and their implications on the interaction among the participants as well as the data analysis and interpretation should be discussed (5, 2).\n\nGiven the complexity of Focus Group data, also the development of further methodologies for data analysis should be continued (5, 4).\n\n\n== An exemplary study ==\n[[File:Focus Groups exemplary study S\u00e4yn\u00e4joki et al. 2014 - Title.png|600px|frameless|center|The title of the exemplary Focus Group study (S\u00e4yn\u00e4joki et al. 2014).]]\n'''S\u00e4yn\u00e4joki et al (2014) used Focus Groups to investigate how Finnish urban planners perceive the potential positive impact of their field on environmental sustainability.''' Focus Groups were chosen since \"the focus of the study was not solely on what people think but on how they articulate, rationalise, challenge each other's views\" (p.6625). The researchers invited 32 urban planning specialists to a workshop, including individuals \"from fourteen Finnish cities, the Finnish environment ministry, two architectural firms, four consulting companies, one of Finland\u2019s largest energy companies, a market leading construction company, the Green Building Council Finland and the Finnish Association of Building Owners and Construction Clients (RAKLI)\" (p.6626). \n\n'''The researchers asked this group three questions as a prompt to the subsequent focus group sessions:'''\n* \"Why is environmental sustainability assessed in urban planning?\"\n* \"How does environmental assessment steer decision-making in urban planning?\"\n* \"What is the role of urban planning in environmental sustainability?\"\n\n'''Then, the 32 participants were divided into three groups of 11, 11 and 10 participants, respectively, with one moderating researcher each.''' All moderators were provided with the same guideline, and their role was \"to enhance interaction and to ensure that all participants had an equal chance to contribute.\" However, \"[w]ithin these limits, much of the discussion was left to the participants in order to learn what they found interesting and important. Nevertheless, occasionally the moderators attempted to develop the discourses by encouraging the participants to explain their views, or even through discreet provocation. The group discussions, each approximately an hour in length, were audio-recorded and manually transcribed and also video-recorded. In parallel with the recordings, the moderators made notes concerning mainly the atmosphere, the interaction and the participants\u2019 reactions.\" (p.6627). Importantly, in addition to the three questions that were posed prior to the focus groups, two further questions guided the moderators' actions:\n* \"How is the power of urban planners to promote environmental sustainability limited?\"\n* \"How is urban density considered in terms of environmentally sustainable land use?\"\n\n'''The analysis of the data, i.e. the transcribed recordings and notes, was done thematically in accordance with Guest et al. (2012).''' This means that not every statement was assessed, but that the researchers focused on \"on identifying and describing themes\" in the data (p.6626). For this, all parts of the transcripts were marked with codes representing the five guiding questions. All data for each code was grouped, and each group of data was then categorized more specifically until \"conclusions could be drawn\" (p.6628). The researchers' notes as well as the recording were used to contextualize the data. \n\nFurther, all transcript data was separated into ''articulated'' data, which is found in participants' direct responses to the moderator's questions, and ''attributional'' data, which is derived from the questions that were brought into the conversations indirectly by the researchers. A last type of data according to Guest et al. (2012) would have been data that emerged from the group interactions, but none was identified in this study. The distinction between the data types was also done in the results.\n\nRegarding their findings, the researchers provide exemplary quotes and highlight which focus groups mentioned which thematic answer to a given question. They further present a range of thematically grouped answers to each question, both in articulated and attributional data.\n<br>\n[[File:Focus Groups exemplary study S\u00e4yn\u00e4joki et al. 2014.png|700px|thumb|center|'''Excerpts from the analysis of the Focus Group transcripts.''' Individual statements are highlighted to support the identified themes for each guiding question. Source: S\u00e4yn\u00e4joki et al. 2014, p.6629.]]\n<br>\n[[File:Focus Groups exemplary study S\u00e4yn\u00e4joki et al. 2014 - 2.png|700px|thumb|center|'''The results to guiding question 1: Why is environmental sustainability assessed in urban planning?''' Source: S\u00e4yn\u00e4joki et al. 2014, p.6630.]]\n<br>\n'''The researchers conclude''' that \"land use planners are not by themselves able to deploy the full potential power of urban planning to impact environmental sustainability\" (p.6640), and depict a range of suggestions to better support land use planners in doing so. Overall, the focus group approach allowed for the researchers not only to gather diverse perspectives on the topic of interest, but also to perceive the underlying motivations and frustrations of the stakeholders and contextualize their statements accordingly.\n\n\n== Key Publications ==\n* Bloor, M., Frankland, J., Thomas, M., & Robson, K (Eds.) 2001. ''Focus Groups in Social Research.'' SAGE Publications.\nThis book provides a thorough insight into the Focus Group methodology \u2013 ranging from an account of the method\u2019s development and current use in academia, guiding information on how to plan and conduct Focus Groups and on how to analyze the data as well as quality criteria for the effective use of Focus Groups.\n\n* Kitzinger, J. 1994. ''The methodology of Focus Groups: the importance of interaction between research participants''. Sociology of Health and Illness 16(1). 103\u2013121. https://doi.org/10.1111/1467-9566.ep11347023\nIn this paper, the process of preparing and conducting Focus Groups is outlined in much detail. The author provides many examples from her own research and explains the reasoning behind certain choices, e.g. regarding group composition, on the way. The importance of interaction in Focus Groups and its implications for data analysis are displayed extensively.\n\n* Onwuegbuzie, A. J., Dickinson, W. B., Leech, N. L., & Zoran, A. G. 2009. A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research. International Journal of Qualitative Methods 8(3). 1\u201321. https://doi.org/10.1177/160940690900800301\nThis article presents different techniques for analyzing Focus Group data and describes how the particularities of the different types of Focus Group data can be adequately considered in their analysis. Further, the authors provide a good overview of the past research on Focus Groups and give guidance in how to recruit participants and in what to consider when conducting Focus Groups from a practical perspective.\n\n\n== References ==\n(1) Bloor, M., Frankland, J., Thomas, M., & Robson, K (Eds.). 2001. ''Focus Groups in Social Research''. SAGE Publications.\n\n(2) Carlsen, B., & Glenton, C. 2011. ''What about N? A methodological study of sample-size reporting in focus group studies.'' BMC Medical Research Methodology 11. 26. https://doi.org/10.1186/1471-2288-11-26\n\n(3) Kitzinger, J. 1994. ''The methodology of Focus Groups: the importance of interaction between research participants.'' Sociology of Health and Illness 16(1). 103\u2013121. https://doi.org/10.1111/1467-9566.ep11347023\n\n(4) Onwuegbuzie, A. J., Dickinson, W. B., Leech, N. L., & Zoran, A. G. 2009. ''A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research.'' International Journal of Qualitative Methods 8(3). 1\u201321. https://doi.org/10.1177/160940690900800301\n\n(5) Parker, A., & Tritter, J. 2006. ''Focus group method and methodology: current practice and recent debate.'' International Journal of Research & Method in Education 29(1). 23\u201337. https://doi.org/10.1080/01406720500537304\n\n(6) Tang, K. C., & Davis, A. 1995. ''Critical factors in the determination of focus group size.'' Family Practice 12(4). 474\u2013475. https://doi.org/10.1093/fampra/12.4.474\n\n(7) S\u00e4yn\u00e4joki, E.-S. Heinonen, J. Junnila, S. 2014. ''The Power of Urban Planning on Environmental Sustainability: A Focus Group Study in Finland.'' Sustainability 6. 6622-6643.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Fine B\u00f6ttner."
                    },
                    "sha1": "rkliyhr8n3xfw7e6l16ggbqovww49t9"
                }
            },
            {
                "title": "Former Homepage",
                "ns": "0",
                "id": "117",
                "revision": {
                    "id": "470",
                    "parentid": "469",
                    "timestamp": "2020-03-07T13:34:45Z",
                    "contributor": {
                        "username": "Prabesh",
                        "id": "2"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3101",
                        "#text": "This page used to be the homepage of the wiki and has now been archived. \n\n== Welcome to Sustainability Methods! ==\n\n=== Day 1 - Intro ===\n# [[Why statistics matters|Do models and statistics matter?]] Why does it pay to be literate in statistics and R?\n# Getting concepts clear: Generalisation, Sample, and Bias\n#:- See also, [[Misunderstood concepts in statistics|misunderstood concepts]]\n# [[Why statistics matters#A very short history of statistics|History of statistics]]\n\n=== Day 2 - Data formats based on R ===\n# [[Data formats#Continuous data|Continuous vs. categorical, and subsets]]\n# [[Data distribution|Normal distribution]]\n# Poisson, binomial, Pareto\n\n=== Day 3 - Simple tests ===\n# [[Simple Tests|Parametric and non-parametric]]\n# [[Hypothesis building|Hypothesis testing]]\n# [[Designing studies#P-value|The power of probability]]\n\n=== Day 4 - Correlation and regression ===\n# [[Correlations and regressions|Correlations and regressions]]\n#:- See also, [[Misunderstood concepts in statistics#Correlation|misunderstood concepts]]\n# [[Causality|Causality]]\n# [[Is the world linear?|Is the world linear?]]\n# Transformation\n\n=== Day 5 - Correlation and regression ===\n# [[Designing studies#P-value vs. Designing studies#Sample|P_values_vs._sample_size]]\n# [[Residuals|Residuals]]\n# [[Reading correlation plots|Reading correlation plots]]\n\n=== Day 6 - Designing studies Pt. 1 ===\n# [[How do I compare more than two groups?|How do I compare more than two groups?]]\n# Designing experiments - degrees of freedom\n# One way and two way\n\n=== Day 7 - Designing studies Pt. 2 ===\n# [[Balanced vs. unbalanced designs|Balanced vs. unbalanced designs]] - Welcome to the Jungle\n# [[Designing studies#Block effects|Block effects]]\n# Interaction and reduction\n\n=== Day 8 - Types of experiments ===\n# Are all laboratory experiment really made in labs?\n# Are all field experiment really made in fields?\n# What are natural experiments?\n\n=== Day 9 - Statistics from the Faculty ===\n\n=== Day 10 - Statistics down the road ===\n# Multivariate Statistics\n# AIC\n\n=== Day 11 - The big recap ===\n# Distribution & simple test\n# Correlation and regression\n#:- See also, [[Misunderstood concepts in statistics|misunderstood concepts]]\n# [[Analysis of Variance]]\n\n=== Day 12 - Models ===\n# Are models wrong?\n# Are models causal?\n# Are models useful?\n\n=== Day 13 - Ethics and norms of statistics ===\n# What is informed consent?\n# How does a board of ethics work?\n# How long do you store data?\n\n\nView [https://sustainabilitymethods.org/index.php/Special:AllPages All Pages].\n\n== Admin Tools ==\n* [https://www.mediawiki.org/wiki/Special:MyLanguage/Manual:Configuration_settings Configuration settings list]\n* [https://www.mediawiki.org/wiki/Special:MyLanguage/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [https://www.mediawiki.org/wiki/Special:MyLanguage/Localisation#Translation_resources Localise MediaWiki for your language]\n* [https://www.mediawiki.org/wiki/Special:MyLanguage/Manual:Combating_spam Learn how to combat spam on your wiki]"
                    },
                    "sha1": "igq6tzoglzp0jopp3lrkluqpjkpa3o6"
                }
            },
            {
                "title": "Framing a research question",
                "ns": "0",
                "id": "985",
                "revision": {
                    "id": "6806",
                    "parentid": "6781",
                    "timestamp": "2022-10-28T13:39:59Z",
                    "contributor": {
                        "username": "LindavH",
                        "id": "46"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7044",
                        "#text": "'''In short:''' This entry provides an introduction on how to frame a research question.\n\nFraming a research question is exactly how the wording already prescribes: You surround just as a you frame a painting with some parts of wood that allow you to basically differentiate a picture from its surrounding. You ask a question, and this question is about research. These are the three parameters, frame, research, question. Now let us break these three components down point by point and let us see how we can frame our research question best.\n\n'''The framing'''<br>\nWe all stand on the shoulders of giants. Any given research question is based on previous research. No research is an island. Whatever we do as researchers, we have to start with what was done before. Reading is the most essential skill of any person new to research. Balance is key to this end. If you read everything there is on a specific topic, then you are in for a long ride. Otherwise, if you miss something important that has been published before, you basically reinvent the wheel, which is a waste of your times and the time of your readers. Hence, make sure that you get the main approaches that have been attempted before, and the main knowledge that has been gained. You will eventually have to make a cut at some point, otherwise you would basically read everything there is, since almost everything is connected. Hence try to focus on what is specific for your topic and for the area you focus on, thereby gaining some insights about the respective context. Context knowledge matters to this end, because only if you know the context of the specific topic, it allows you to aim with the right ratio of focus and distance. Why both? If you look too close, your knowledge is too singular, too specific, or just beyond the point, or any point at all. If it is too broad, it may be generic, trivial, and thus again beyond any point whatsoever once more. Ideally, you work in a specific system. This may be a group of people, and institution, or any other constructed entity. This allows you to add specificity to your research topic. You aim at creating knowledge about this system, people or entity. Hence your framing can be about a specific topic, an entity or institution, but also about a theory. What matters to this end is to have the right angle. Did you ever try to look at a painting from the side? Basically, this does not work, because the framing will block the view, and you do not see the picture. In framing your research, this is basically the same. You need to have the same angle, not from the side, but up front. Within research, we often look at a topic or problem through a certain theory. While testing a theory is restricted to deductive research, regarding a research question our framing and viewpoint is more open minded. It is thus not restricted to the yes/no categories of a hypothesis, but instead allows us to ask a broader question that allows us to create contextual and novel knowledge. \n\n'''The question'''<br>\nOur research question may start with a \"How\", thereby examining explanations of dynamics or patterns. \"Where\" questions will typically try to spatially locate phenomena, while \"when\" questions examine dimensions of temporality. \"Why\" questions try often to go deeper into reasonings and examine patterns that may in one extreme be pretty foundational, yet can in other extreme cases be borderline trivial. \"What\" question are one last example of research question that can either be rather procedural or again end up being vague. Some research question may avoid such question formats altogether, which is for instance true for much of the realms of descriptive research. Yet another example is critical research, which may not be occupied with questions at all, but instead offer a critical reflection or specific viewpoint.\nWhat should however be clear is that research questions can be at least partly answered, or we may conclude that based on the current design and data the research question cannot be answered. This is perfectly ok, and a part of scientific research. However, research is often biased towards positive or even potentially exciting research, while research rarely report that they could not find anything out at all, and the their initial research question remains unanswered. \n\n'''The research'''<br>\nTestability and rejection is actually a key criteria that should differentiate knowledge gained through science from mere opinions, because we can have all sorts of opinions, but scientific empirical knowledge can be tested and can also be falsified, refined or changed if it turns out to be wrong. Hence wrong facts can be debunked, while it is really hard to debunk a conspiracy theory that is based on mere opinions. In addition to the process of testing or examining within the framework of research it is possible and indeed perfectly normal that our knowledge evolves; this is actually what science is all about. Consequently, the question we try to examine should represent a scientific process that can be handled by us. We cannot come up with a research question that is perfect in terms of everything but the fact that it is impossible to conduct it. \nOur research question does not need to be  a breakthrough that changes the whole world, but more often than not it is a stepping stone in the bigger picture that is our united research working to unravel. Our framing is a piece of what we call reality, and building on a respective theory or theoretical foundation may help us to focus our research and create knowledge that is specific and not generic. Equally can a contextual focus or specification help within a research question to understand what we work about, or where our sample entity is located or rooted in. All this is part of our framing. Naturally, the framing also needs to be clear. If your wording or different parts of the frame are too many, then other researchers will not be able to follow. Hence we need to make sure to use fewer words instead of too many, and each word should be chosen carefully. \n\n'''Taken together''', we may indeed ask questions in our research questions, but these should be specifically frames, state the respective context, root deeply in previous research and knowledge, be neither generic nor hyper-specific, need to be feasible, and allow for a scientific conduct that can be documented, reproduced, or both. Specific theories allow for refined viewpoints, and empirical research may benefit from stating where it is conducted. The most difficult challenge in framing a research question is then to decide what to include and what to exclude. Researchers make choices, and need to focus. Creating a research question demands to keep the most important information, and omit all that is not central to the initial question. It takes practice to get good at framing research questions. Be patient, and keep going.\n\n----\n[[Category: Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "rbta1e87ywd31fst6ppjk9mxpb8zyh2"
                }
            },
            {
                "title": "Functions in Python",
                "ns": "0",
                "id": "1030",
                "revision": {
                    "id": "7241",
                    "parentid": "7016",
                    "timestamp": "2023-06-29T05:16:16Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to functions in Python",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5591",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n===Description===\nFunctions (or \u201cMethods\u201d in some other programming languages) are reusable blocks of code that do a specific thing and can be reused multiple times in a program. It is good for task breakdown and code readability. \n\nWe have already used several functions in the tutorials above, such as the \u201cprint()\u201d function. These functions are default functions readily available in Python. However, it might be handy sometimes to develop your own functions for specific occasions. First, we will look at pre-defined functions and then develop our own user-defined functions.\n===Pre-defined Functions===\nUnderstanding all the pre-defined functions in python is a journey of a lifetime. However, through our exercises, you will know and get used to the most common functions that exist.  \n\nWe can start with the 3 examples that most of you have seen before:\n\n<syntaxhighlight lang=\"Python\" line>\n# suppose we have a height of pupils in a class\nheights = [150, 166, 154, 145, 159]\n\n# print() is a predefined function\nprint(heights)\n\n# sum() is also a predefined function to add every element in a list\nsum(heights)\n\n# len() is a predefined function to show the length of a list\nlen(heights)\n</syntaxhighlight>\nThis was quite intuitive, right? Next, we will make our own functions, just like the pre-defined functions have been created by someone.\n===User Defined Functions===\n==String Example==\nAn example function looks as follows: \n<syntaxhighlight lang=\"Python\" line>\n# First function example: A function to greet people\ndef greet(name):\n    \"\"\"\n    this is a documentation: explaining the function\n\t\tinput: name (str)\n\t\toutput: says hello, and the person name\n    \"\"\"\n    print(\"Hi, how are you,\", name)\n    return\n</syntaxhighlight>\n\nAs you can see above:\n\n* The function begins with a special word: \"'''def'''\",  and then the function name. In this case, our function name is \"greet\"\n* After that, we need parentheses \"( )\". Inside parentheses is what we call argument (or input parameters). Arguments can be more than one. In the example above, we only have 1 argument, which is the \u201cname\u201d to store the name of the person we want to greet.\n* To finish the first line of code, a colon (:) is needed.\n* The body of a function is the \u201cinside\u201d of a function. Important is to '''indent''' (adding TAB before code) **the body.\nAdding documentation to your function will make it look professional and will help with the readability of your whole program.\n* Lastly, using the \u201creturn\u201d statement will end your function.\n\nWe can call the documentation of a function using:\n<syntaxhighlight lang=\"Python\" line>\nhelp(greet)\n</syntaxhighlight>\n\nAnd of course, we can use (or \"call\") the function\n<syntaxhighlight lang=\"Python\" line>\ngreet('Stephen')\n</syntaxhighlight>\n\nAnd as you can see above, the syntax is similar to $print('argument')$. That is because \u201cprint\u201d is also a function, however, it is by default defined by the system. \n\n==Math Example==\n<syntaxhighlight lang=\"Python\" line>\ndef multiplication(a, b):\n\t\"\"\"\n\tDefine a function to multiply 2 numbers\n\t\"\"\"\n\tc = a * b\n\treturn (c)\n\tprint(\"not printed\", c)\nresult = multiplication(2,3)\nprint(result)\n</syntaxhighlight>\n\nThe example above is really similar to our first example with greeting names. However, now you know that we can use functions for not only strings, but also numbers, tuples, arrays, and so on, depending on your function. \n\nA short quiz for you, can you multiply a string with a number?\n\n<syntaxhighlight lang=\"Python\" line>\nmultiplication(4, 'Arthuria')\n</syntaxhighlight>\n\n\n===Default Argument Values===\nIt is possible to set a default value for arguments in a function.\n<syntaxhighlight lang=\"Python\" line>\ndef is_mood_good(good_mood=True):\n\t\"\"\"\n\tTrue if good mood\n\tFalse if bad mood\n\t\"\"\"\n\tif(good_mood == True):\n\t\tprint(\"I feel good\")\n\telse:\n\t\tprint(\"I feel good... just kidding\")\n</syntaxhighlight>\n\nAnd you can try the default argument, and your own argument: \n\n<syntaxhighlight lang=\"Python\" line>\nis_mood_good()\nis_mood_good(True)\nis_mood_good(False)\n</syntaxhighlight>\n\n===Local versus Global Variables===\nWe can create a variable in a function. That variable can only \u201clive\u201d inside that function. On the opposite, if we create a variable outside a function, it can \u201clive\u201d both inside and outside the function. The advantage of local variables is that they cannot be altered by any other commands in your script and make it easier to oversee. Let's say you only need a part of a string for one certain calculation you are doing, but the whole string for other calculations. It would be best to then store the partial string as a local variable so it does not get confused later on.\n\nExample:\n\n<syntaxhighlight lang=\"Python\" line>\nglobal_var = \"global is here\"\n\ndef local_func():\n\t\"\"\"\n\texample of a local function\n\t\"\"\"\n\tlocal_var = \"local_var is here\"\n\tprint(local_var)\n\treturn\n\nprint(global_var)\nprint(local_var) # this will throw an error because it doesn't exist outside the local_func\n</syntaxhighlight>\n\n===Quiz===\n# Write a function to divide the first input by the second input\n# Complete the function below to return a product of the first and second input\n<syntaxhighlight lang=\"Python\" line>\ndef f(x1, x2):\n\t# TODO\n\treturn\n</syntaxhighlight>\n# Consider c = [1,2,3,4,5] and g is a function that computes its sum\n<syntaxhighlight lang=\"Python\" line>\ndef g(c):\n\t# TODO\n\treturn \n</syntaxhighlight>\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "c9nl4a0o4ew9jdxrm6cnoaejyil7pj1"
                }
            },
            {
                "title": "Gender First Aid Kit",
                "ns": "0",
                "id": "962",
                "revision": {
                    "id": "6773",
                    "parentid": "6712",
                    "timestamp": "2022-09-09T11:51:21Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "46033",
                        "#text": "==Introduction==\nThis Wiki article aims at giving a first and basic introduction to the topic of gender. Our intention is to offer some helpful tools and advice on how gender is important to consider in everyday life at university. We hope that students and teachers find helpful information and suggestions of how to act more aware regarding gender.\n\nAs this is meant to be a gender first aid kit we start this article by offering some practical advice on how to use gender neutral language and different pronouns as well as reflect on one\u2019s own talking behaviour. After that we will describe the history of the concept of gender and thereby highlighting to you the most important information on the topic from our point of view. In the end, we will point out the relation of institutions such as universities with gender and explain why it is one important factor to consider while studying or working in an institution.\n\n'''Disclaimer:''' The contents of this article require a very sensitive use of language and certain words. We as authors reflected upon that and want to share our thoughts with you before reading.\nDuring this article we will refer to and write about BIPoC (Black, Indigenous and People of Colour). We decided to rather use the term non-white people as this is more descriptive, and BIPoC is often used as empowering self-designation, which is why we as only white authors do not want to claim that. Furthermore, we are aware of our own privileged position regarding gender identity, social and academic background.\n\n==History of Gender==\nThis section will provide a brief historical outline of how the concept of '''sex''', and later '''gender''', came into being from the 18th century onward until today. This is by no means a complete historical account but rather tries to give an introduction to the topic. The further links we provide might be a good starting point to deepen your knowledge if interested.  We will mainly focus on the Enlightenment period in which natural sciences such as anthropology and biology emerged to explain the development of the sex binary and the differentiation of humans in races. After that, there will be a short summary of the four waves of feminism focusing on each\u2019s main claims and developments regarding sex and gender. Furthermore, this historical section refers to the German/European/Western history of sex and gender and cannot be generalized to other societies, cultures and regions of the world.\n\n[[File:Gender definition.png|thumb|right|'''Gender definition''' Arevalo 2020; Lieu et al. 2020; Mechlenborg, Gram-Hanssen 2020; Moyo, Dhliwayo 2019; Curth, Evans 2011]]\n\n'''Disclaimer''': We decided to use the historical terms and meanings for sex and gender as they were used at the given time. By that we do not wish to reproduce the biological differentiation into women and men as right gender understanding but to increase the comprehensibility for the reader and to display the understanding of sex and gender at the respected historical time.\nTrigger warning: Throughout this chapter, references to racism and colonialism are being made. There are never detailed descriptions of racist violence.\n\n===The Enlightenment===\nSociety in the 18th century was widely built upon a hierarchical system rooted in gender and race, which constructed social positions based on gender and racial differences. This separation of society into women and men and white and non-white people emerged during the Enlightenment which was an intellectual movement in the 18th century that strived for objective answers based on reason.\n\n'''All people are equal?'''\n\nThus, the previous focus of society on religion and culture was tried to be replaced by reason. Based on the idea that all people are equal by nature, social, civil and universal human rights slowly evolved. An early example of these developments were the demands of European women for more rights, thereby questioning long-established social arrangements with respect to women\u2019s rights and duties. This is now considered the first wave of feminism. The demands of women at the time raised the question of whether and how such changes in rights would be justified. In addition, it was unclear whether males of different skin colour had equal property rights and, in the course of this, how slavery could be continued, since the system was depending on injust labour force. A supposedly \u2018scientific\u2019 answer rooted in reason on how to justify the continuation of social inequality was needed (Eagle Russett 1989, Schiebinger 1993). \"They were looking to nature for solutions to questions about sexual and racial equality. [...] Scientists took up the task of uncovering differences imagined as natural to bodies and hence foundational to societies based on natural law. [They] did not draw their research priorities and conclusions from a quiet contemplation of nature, but from political currents of their times\" (Schiebinger 1993; 9,183). Under the pretext of reason and building on what was believed to be modern science, biological sexism and racism emerged, paving the way to biological determinism.\n\n===Biological determinism as social theory===\n[[File:Biological determinism.png|thumb|right]]\n\n====Biological determinism and sex====\nThe Western concept of two seyes, men and women, is based on the theory of biological determinism (Oyewumi 1997). It emerged in Europe during the 18th century, when natural sciences (above all medicine and anthropology) became more concerned with the human body, which was before rather the concern of philosophers, theologists and folklore thinkers.\n\n====Biological determinism and race====\n\n===Exclusive science===\nThe beard as a feeble indicator for superiority was already mentioned, yet over the next decades and centuries, further explanations to justify social inequalities were investigated. Criteria of inclusion and exclusion of certain people also applied to academic communities, creating a closed ingroup. Knowledge production thus consolidated itself as an exclusive white-male club, yet deprived itself of other perspectives, whose ideas and experiences were not considered valid. The white-male science of the time became restrictive in their theoretical and empirical focus. The division of people into the categories of male and female, white and non-white and the simultaneous establishment of a superiority of white males, argued through characteristics possessed only by them, led to the exclusion of non-white people  from science (Schiebinger 1993). \"European men, dominating academic science, increasingly tightened the reins on what was recognised as legitimate knowledge and who could produce that knowledge\" (Schiebinger 1993, 142). While this Zeitgeist in science and society is now rightfully considered wrong, it is also relevant to mention the uncountable inequalities and injustices that got propelled out of that time.\n\n===The gender binary as a colonial object===\n[[File:Heteronormativity.png|thumb|right]]\n\n====Two-spirit====\n[[File:Cultural appropriation.png|thumb|right|[https://www.britannica.com/story/what-is-cultural-appropriation Britannica]]]\n\n====Hijra====\n[[File:Othering.png|thumb|right|[https://diversity-arts-culture.berlin/woerterbuch/othering Diversity Arts Culture]]]\n\n====Yor\u00f9b\u00e1====\n\n===Colonisation and Christianisation===\nSince Colonisation was a multi-faceted process, main actors were not just statesmen who executed their orders from their European home countries. It was also missionaries who enforced a process of Christianisation in the colonies. Christian missionaries fulfilled the task of education on colonised land. The school often functioned as the church also was a place where the culture should be transformed to align with European values. Especially the family system was targeted with a focus on polygamy. \u201cFor the missionaries, having multiple wives was not only primitive but against God's law: polygamy was adultery, pure and simple\u201d (Oyewumi 1997: 137). So the Christian education focused on conveying Western gender roles that were in their eyes more \u2018civilised\u2019. Besides learning to read the Bible children were taught skills and behaviour that suited their respective gender roles and expectations.\n\nAt this point, it must also be pointed out that Christianity was and is not the only religion that transported and exported certain ideas of gender, gender roles and sexuality with its spread. For example during the expansion of Islam starting around 800 throughout the Middle East and North Africa, but also to southern Europe, India, sub-Saharan Africa, Central Asia and Southeast Asia patriarchal and heterosexual norms were spread originating from Islam and Middle-Eastern traditions (Stearns 2015).\n\n===Waves of feminism===\n\n[[File:First Wave.png|thumb|right|[https://www.womenshistory.org/exhibits/feminism-first-wave-0 Womens History], [https://www.pacificu.edu/magazine/four-waves-feminism Rampton, M.]]]\n====The whiteness of the first wave of feminism====\n\n\n====Intersexuality and the deconstruction of the binary?====\n[[File:Intersexuality.png|thumb|right]]\n\n====The personal is political and the second wave of feminism====\n[[File:Second wave.png|thumb|right|[https://www.britannica.com/topic/feminism/The-second-wave-of-feminism Britannica], [https://www.womenshistory.org/exhibits/feminism-second-wave Womens History]]]\n\n====Third wave of feminism and gender performativity====\n[[File:Third wave.png|thumb|right|[https://www.britannica.com/topic/feminism/The-third-wave-of-feminism Britannica]]]\n\n====Fourth Wave of feminism and the joint fight====\n\n===Take Home Message===\nIn the 18th century, the concept of sex was created by natural scientists who applied self-selected differentiating characteristics on the basis of biological determinsm. After centuries of reflection on biases, systematic preconceptions, and oppression, parts of the scientific community came to the conclusion that neither sex nor gender are objective truths. Today, both sex and gender are considered socially constructed. What we have tried to show with this historical outline is that even supposedly scientifically produced knowledge is not objectively true but is permeated by social and cultural norms of the time. Scientific knowledge was and is used to justify policies and practices but one has to keep in mind that people have and still suffer because of these. As scientists, we therefore have an obligation to be critical of ourselves and of how we navigate this.\n\n===Further links===\n\n'''Colonialism'''\n\n[https://www.youtube.com/watch?v=-wRbfOmHgts Hetero-patriarchy and Settler Colonialism]: Ried Gustavson takes a two-spirited look at the effects of hetero-patriarchy on settler colonialism of indigenous peoples in the form of a TedTalk.\n\n[https://www.youtube.com/watch?v=-YtllAe6cYg Pre-colonial attitudes to sex and gender fluidity]:  In this episode of On the Rag, host Leonie talks to Takat\u0101puhi activist Elizabeth Kerekere about pre-colonial attitudes to sex and gender fluidity. \n\n[https://www.youtube.com/watch?v=WI5M7U5PSxk Decolonising gender and sexuality in Wellington city]: This video explores the M\u0101oritanga and the nooks of the city of Wellington that allow them to thrive.\n\n'''Gender'''\n\n[https://www.youtube.com/watch?v=5e12ZojkYrU&list=RDLVCquRz_cceH8&index=6 The Origin of Gender]: This video tries to answer the question of  \"Why do we think there are only two genders when there are cultures that believe there are many more?\" And \"what\u2019s the difference between sex and gender?\".\n\n[https://www.youtube.com/watch?v=I6xuJu7gLe0&list=PLMqvIFT6YElZooc3ADQfsW8Bw4v1EssxU The gender-fluid history of the Philippines]: In much of the world, gender is viewed as binary: man or woman, each assigned characteristics and traits designated by biological sex. But that's not the case everywhere, says France Villarta. In a talk that's part cultural love letter, part history lesson, he details the legacy of gender fluidity and inclusivity in his native Philippines -- and emphasizes the universal beauty of all people, regardless of society's labels.\n\n[https://open.spotify.com/show/6or8b4avxUAA47yDKnq3kY?si=XQ_MOsgwRtu2K14PAtdt_Q&nd=1 Alle f\u00fcr Alle]: This German podcast by Dissens - Institut f\u00fcr Bildung und Forschung explores the current state of gender roles, masculinity, femininity, queerness and diversity from an anti-oppression, anti-hierarchies and profeminist point of view.\n\n[https://www.youtube.com/watch?v=c1LB8kDW67M Gender diversity & identity in Queertopia]: What does gender diversity and gender identity means nowadays? And what does gender diversity include?  What if you are born with a body that does not meet society\u2019s typical idea of \u2018man\u2019 or \u2018woman\u2019? Gender diversity has become more and more important and discussed. That\u00b4s why this documentary is investigating the world of gender diversity, identity and sexual diversity. \n\n[https://www.youtube.com/watch?v=CquRz_cceH8 Theories of Gender: Crash Course Sociology]: Why is gender even a thing? To answer that, this Crash Course video is going back to the three sociological paradigms and how each school of thought approaches gender theory. It looks at the structural functionalist view that gender is a way of organizing society into complementary roles, the symbolic interactionist take on how gender guides our daily life, and conflict theory\u2019s ideas about how gender distributes power within society.\n\n[https://lux.leuphana.de/vufind/Record/878325417 Queer : a graphic history]: In this graphic novel, basic terminology regarding the field of Gender and Queer Studies is explained. From identity politics and gender roles to privilege and exclusion, Queer explores how we came to view sex, gender and sexuality in the ways that we do; how these ideas get tangled up with our culture and our understanding of biology, psychology and sexology; and how these views have been disputed and challenged. Along the way we look at key landmarks which shift our perspective of what's 'normal' - Alfred Kinsey's view of sexuality as a spectrum, Judith Butler's view of gendered behaviour as a performance, the play Wicked, or moments in Casino Royale when we're invited to view James Bond with the kind of desiring gaze usually directed at female bodies in mainstream media. Presented in a brilliantly engaging and witty style, this is a unique portrait of the universe of queer thinking.\n\n[https://www.youtube.com/watch?v=koud7hgGyQ8 Social Constructs]: This video by Philosophy Tube neatly explains what lies behind the term \"social construct\" and why some people get so upset about it.\n\n[https://interventionen.dissens.de/materialien/glossar Glossary for gender and sexual diversity]: Do you find yourself in a position of not knowing what a term within the field of Gender Studies means? This glossary has an answer to (most) of them. \nNote: this website is unfortunately only in German.\n\n'''Feminism'''\n\n[https://engendered.us/episode-153-rafia-zakaria-on-her-book-against-white-feminism/ Against White Feminism]: On this episode of en(gender)ed, guest host Roman James interviews feminist lawyer, human rights activist, political philosopher, columnist and author, Rafia Zakaria about her newly released book \"Against White Feminism, Notes on Disruption\". Roman speaks with Rafia about the historic domination of the feminist lexicon by upper middle class white women, the de-centering of Black and Brown voices, and the role that patriarchy plays in perpetuating white supremacist, capitalist, imperialist feminism which often subjugates the very populations it asserts to be empowering.\n\n[https://open.spotify.com/episode/20mwLP5LIJj2zLsAxt93uC Postcolonial Feminism]: Podcast hosts Eva and Emma get into an introduction to Postcolonial Feminism. They discuss the origins of postcolonial theory, examine how Western feminists frame so-called \"third world\" women and highlight the ways in which feminist rhetoric gets weaponized to perpetuate colonialism. \n\n[https://plato.stanford.edu/entries/feminism-gender/ Feminist Perspectives on Sex and Gender]: This entry of Stanford Encyclopedia of Philosophy outlines and discusses distinctly feminist debates on sex and gender considering both historical and more contemporary positions regarding questions such as \"What does it mean for gender to be distinct from sex, if anything at all? How should we understand the claim that gender depends on social and/or cultural factors? What does it mean to be gendered woman, man, or genderqueer?\".\n\n[https://www.youtube.com/watch?v=OpE3aWyG-As Argentinien: Frauen gegen M\u00e4nner-Gewalt]:The ARTE-reporter accompanied the parents of M\u00f3nica Garnica to the trial of her husband. Two years ago he had set her on fire and thus murdered her. The public prosecutor M\u00f3nica Cu\u00f1arro fights for harsh sentences - and for better support for the victims: \"When women file charges, they live particularly dangerously. Then the violence increases - to prevent the woman from testifying in court.\" In Argentina, a new self-confidence can be felt, which women carry into all areas of life. However, the \"Ni una mentos\" movement is no longer an Argentinean phenomenon. Women in other countries, such as Peru, Mexico or Colombia, are also protesting under this motto.\nTrigger warning: sexualised violence, suicidal thoughts, death\nNote: This documentary is in German.\n\n[https://time.com/5189945/whats-the-difference-between-the-metoo-and-times-up-movements/ #MeToo and Time's Up]: #MeToo and Time's Up Founders Explain the Difference Between the 2 Movements \u2014 And How They're Alike.\n\n[https://www.boell.de/en/reproduktive-gerechtigkeit Reproductive justice]: What is meant by the term reproductive justice and how does it play a role within intersectionality?\n\n'''Intersex'''\n\n[https://www.youtube.com/watch?v=MB7nbvD8rQk What it means to be intersex]: Susannah Temko reveals the shame, prejudice and harm faced by the intersex community, as they're forced to conform to a binary understanding of sex that ultimately hinders their health and well-being. She calls on us all to discard outdated notions of biological sex and accept the complexity within humanity.\n\n[https://www.youtube.com/watch?v=SlT3u_CYTPk I\u2019m intersex and I wish doctors left my body alone]: When Mikayla Cahill was 23 she had her testes surgically removed. She is one of many intersex people who have been operated on so their bodies fit binary ideas of what male or female bodies \u201cshould\u201d be like. But Mikayla felt proud and healthier with her testes, and is now questioning why she couldn't live her life with them intact.  But more than anything, she wants to know why doctors told her to keep her intersex condition a secret, and why she was left feeling like she was the only person like her who existed.\n\n[https://www.youtube.com/watch?v=trPkQT3tMkQ Intersex people: Non-consensual surgeries must stop]: For decades, doctors have surgically chosen a gender for thousands of intersex people, without their consent. Serena Daniari met up with two people who don\u2019t fit the average definition of \u201cmale\u201d or \u201cfemale.\u201d\n\n\n'''Intersectionality'''\n\n[https://www.ted.com/talks/kimberle_crenshaw_the_urgency_of_intersectionality The urgency of intersectionality]: Now more than ever, it's important to look boldly at the reality of race and gender bias -- and understand how the two can combine to create even more harm. Kimberl\u00e9 Crenshaw uses the term \"intersectionality\" to describe this phenomenon; as she says, if you're standing in the path of multiple forms of exclusion, you're likely to get hit by both. In this moving talk, she calls on us to bear witness to this reality and speak up for victims of prejudice.\n\n[https://open.spotify.com/episode/3WoI7vvUx9nr83oSKY1XGd Warum Intersektionalit\u00e4t?]: We are supposed to think intersectionally, but what does that mean exactly and where does the term come from? Dissens podcast host Carla Kaspari talks about this with political scientist and author Emilia Zenzile Roig.\nNote: this podcast episode is in German\n\n\n==Gender Neutral Language==\n\n\n===What is gender neutral language?===\n''\u201cGender-neutral language is a generic term covering the use of '''non-sexist''' language, '''inclusive''' language or '''gender-fair''' language. The purpose of gender-neutral language is to avoid word choices which may be interpreted as biased, discriminatory or demeaning by implying that one sex or social gender is the norm.\u201c'' (European Parliament 2018)\n\n===How is it used in different languages?===\nThere are different strategies for using gender neutral language, depending on which language is spoken. In English, but also in some Scandinavian languages, many nouns are already '''gender neutral'''. The easiest strategy would be to reduce gender-specific terms as much as possible and thus to neutralise them. The European Parliament provides some examples of this: words that are not gender-specific and refer to people in general, with no reference to the person's gender (\u2018chairwoman\u2019 or \u201cchairman\u201d is replaced by \u2018Chair\u2019 or \u2018chairperson\u2019, \u2018policeman\u2019 or \u2018policewoman\u2019 by \u2018police officer\u2019, \u2018spokesman\u2019 or \u201cspokeswoman\u201d by \u2018spokesperson\u2019, \u2018stewardess\u2019 or \u201csteward\u201d by \u2018flight attendant\u2019, \u2018headmaster\u2019 or \u2018headmistress\u2019 by \u2018director\u2019 or \u2018principal\u2019, etc.)\u201d (European Parliament, 2018).\nIn contrast, in German, but also in other Romance languages, it is not quite so simple as every noun has a grammatical gender and therefore gender-specific article. This in itself is random and not necessarily problematic. However, regarding nouns referring to people or job descriptions, the '''generic masculine''' has established itself as the norm since money-earning jobs were historically performed by men and job descriptions stem from that time. This means that the masculine version of job or person related titles are used to describe people of all genders, for example Sch\u00fcler, B\u00fcrger and Maler. Many people do not question the generic masculine and accept it as 'normal' and 'correct'. However, it can be perceived as discriminatory by people who do not feel addressed and included by this generic. \nMoreover, psycho-lingual studies have shown that the way we phrase our sentences and what words we use shape our reality. For example, when using only the generic masculine form to describe occupational titles, people imagine a male worker. This effect is especially crucial for children and their own perception of job status and self-efficacy (Vervecken & Hannover 2015). Thus, language is a tool to reinforce or counteract sexist, binary, and discriminatory structures and hence crucial to use carefully and with intent.\n\nA variant of gender-neutral language or gender-sensitive language would be, on the one hand, to find a gender-neutral word for that noun, such as Studierende, Lehrpersonen, etc., or to gender it. There are now some well-known variants and various institutions already use them. For example, the generic masculine is no longer used, and there is no longer talk of Sch\u00fcler and Studenten, but of Sch\u00fcler*innen and Student*innen (other variants are, for example, with a \u2018:\u2019 or \u2018_\u2019 i.e., Sch\u00fcler:innen or Student_innen). The advantage '''\u201cdes Gendern\u201d''' and the use of gender-neutral terms over another strategy, namely the use of the feminine and masculine form such as Sch\u00fcler and Sch\u00fclerinnen or Studentinnen und Studenten, is that not only persons of the feminine and masculine gender are addressed, but also people that do not identify with one of the two binary genders.\n\n===Where to get some more help and advice?===\nThe Leuphana Equal Opportunities Office has summarised some important and helpful tips for gender-sensitive language in German and links to other interesting sites that can make it easier to use gender-neutral language:\n* [https://www.leuphana.de/fileadmin/user_upload/ZentraleEinrichtungen/frauenb/Gender_und_Diversity/sprache/PDF/Kurzinfo_Geschlechtergerechte_Sprache.pdf Information about gender-sensitive language]\n* [https://www.leuphana.de/fileadmin/user_upload/ZentraleEinrichtungen/frauenb/Gender_und_Diversity/sprache/PDF/Arbeitshilfe_GeschlechtergerechteSprache_final.pdf Working aid on gender-sensitive language]\n* [https://www.genderleicht.de/ genderleicht.de]: Website with lots of helpful advice and tools on how to speak and write as less discriminatory as possible (German)\n* [https://www.gender-glossar.de/glossar Glossary] with terms, persons, and organisations from Gender Studies and Diversity (German)\n* [https://www.youtube.com/watch?v=OCQ3KdCgifM Gendern - nervig oder notwendig?]: A video in which psychologeek argues for and against the use of gender-sensitive language (German)\n* [https://www.gendern.de/ Deutsches Lexikon] f\u00fcr genderneutrale Personenbezeichnungen\n\nMore examples and tips on how to easily implement gender neutral language in English can be found here:\n* United Nations [https://www.un.org/en/gender-inclusive-language/guidelines.shtml guidelines] for gender-inclusive language in English.\n\nThe following links are regarding gender-neutral language in Spanish, Arabic and French:\n* Gender-neutral language in Spanish: [https://nonbinary.wiki/wiki/Gender_neutral_language_in_Spanish Wiki entry] by the Non-Binary Wiki, [https://www.youtube.com/watch?v=R6kr3HZkzpE Video] about gender-neutral language in Spanish\n* [https://www.un.org/ar/gender-inclusive-language/guidelines.shtml Guidelines] United Nations in Arabic Article about gender-neutral language in Arabic/English\n* [https://authoring.prod.unwomen.org/sites/default/files/Headquarters/Attachments/Sections/Library/Gender-inclusive%20language/Guidelines-on-gender-inclusive-language-fr.pdf Guidelines] United Nations in French\n\nIf you have not found your first/native language or preferred language here and would also like to get some insights into gender-neutral language in that language, just write as an email and we will try to add it.\n\n==Pronouns==\n\n\n===What are pronouns? Why do they matter?===\nPronouns are a way to refer to a person in a conversation besides using their name. Therefore, they are often intertwined with '''gender identity'''. Using the right pronouns when talking about another person acknowledges and validates their identity and shows acceptance and respect for the other person.\n\nThere are different pronouns commonly used, for example she/her, he/him or they/them. However, pronouns are not limited to just these terms. People can use multiple sets of pronouns (e.g., she/they), which shall be used interchangeably when referring to this person. Additionally, '''neopronouns''' e.g. xe, thon, ens or dey (in German) have been on the rise especially for '''non-binary''' and '''genderqueer''' people. Learning how to use and conjugate them is a way of showing that you care. In the German language, there is no established gender neutral pronoun yet. Thus, using no pronouns and simply being addressed by one\u2019s name is common.\n\nIt is important to remember that '''gender expression''' is not the same as gender identity. The way a person presents (for example by wearing certain clothes or make-up) does not say anything about what pronouns they use and what their gender identity is, meaning you cannot visually tell what a person's gender is. Hence, a person can present in a traditionally feminine way and identify as non-binary and use they/them pronouns.\n\n[[File:Non-binary-genderqueer.png|thumb|right]]\n\n===How to ask for pronouns===\nWhen meeting new people, an easy way to let people know your pronouns is by introducing yourself with your name and your pronouns (e.g. Hello! My name is ... and I use .../... pronouns. Nice to meet you.). It may feel a little weird and unfamiliar at first, but the more you do it the more it will become normal and natural. Furthermore, it often leads to the other person introducing them with their pronouns as well. If not, you can kindly ask for it (\"What pronouns do you use?\").\nEspecially for '''cisgender''' people, introducing oneself with pronouns is a good sign of showing awareness and being an '''ally''' to '''transgender'''/genderqueer/non-binary people since it contributes to normalising this process and helps to reduce assuming pronouns based on appearance.\n\nIf you have already met a person but are unsure of their pronouns it is okay to ask for them again or at a later point. It might be convenient to do so in a more private setting and not in front of a group to take away pressure. This applies to the usage of pronouns you don't know as well. It is always better to ask for clarification respectfully than to assume and/or use wrong pronouns.\n\nSometimes, what pronouns or names people want to be addressed with may vary due to contexts and feelings of safety. If possible, ask people beforehand.\n\n===What (not) to do when messing up===\nIf you accidentally use the wrong pronoun for a person, it is important to '''correct yourself'''. This shows your respect for the other person and their identity. A correction and a short apology may look like this: \"\u201cHe likes \u2013 I\u2019m sorry, they like noodles more than rice.\" Making excuses however can be frustrating - therefore avoid doing so. We propose a similar approach when someone else corrects you: thank them for doing so and move on with your sentence using the correct pronoun - don\u2019t react offended if someone informs you that you have used the wrong pronouns.\n\nIf you notice another person using wrong pronouns, speaking up is a good way to show '''solidarity''' and awareness. However, it is important to keep in mind that this should be at the comfort of the affected person.\n\n'''Be trustworthy!'''\n\nIf you know a name or pronoun a person has previously used, keep it to yourself. Try to avoid situations, especially in public, where a person is pressured to reveal their '''\u201cdead name\u201d''' (=old name), email-addresses, letters, IDs ...\n\n===For teachers===\nYou as a teacher are in a position to set a positive example for your students. Sensitivizing them for discriminating language, behaviour, working environment etc. is important (as this entry hopefully has shown).\n\nHowever, there is more you can do to actively create a safer environment for all:\n\u2022Give students a chance to let you know if their name and pronouns differ from their legal documents used in the registration process e.g by having an introduction round in which everyone can write down their preferred name and pronouns on a nameplate\nHowever, respect if they remain silent!\n\n\u2022Include a chance to communicate other needs which are important for their participation, for example accessibility.\n(Both points can be mentioned in an email that goes out to all students before the first class)\n\n\u2022Go through attendance lists without \"Mr/Mrs. + first name\" but simply read down last names to avoid dead naming or assuming the wrong gender\n\n\u2022Address students in the seminar with their first name and not with Mr/Mrs + last name\n\n===Links & further reading===\n[https://www.youtube.com/watch?v=4NcMV5dsmgI All about pronouns]: Ash Hardell in this YouTube Video introduces you to pronouns as well as neo-pronouns.\n\nThey/them pronouns in popculture:\n* Cal in [https://www.netflix.com/watch/81191727?trackId=255824129&tctx=0%2C0%2CNAPA%40%40%7C_titles%2F1%2F%2Fsex%20education%2F0%2F0%2CNAPA%40%40%7C_titles%2F1%2F%2Fsex%20education%2F0%2F0%2Cunknown%2C Sex Education] (Netflix series) time stamp: 18:04-19:17\n* Adira in [https://www.youtube.com/watch?v=NNTGwWypUDs Star Trek]\n* [https://www.netflix.com/watch/80175355?trackId=255824129 One day at a time] (Netflix series) time stamp: 0:00-2:25, 5:36-6:40\n\n[https://pronouns.minus18.org.au/ Online game]:Here you can learn how to conjugate (neo)pronouns. \n\n[https://neopronounss.carrd.co/ Neopronoun guide]: A guide that introduces you to the different kinds of neo-pronouns and how to use them.\n\n[https://www.youtube.com/watch?v=Nn1TC7VEpf4&list=PLJic7bfGlo3p3MZQ28prCQxWSZQTytqF8&index=50 Pronouns]: Interview of people by the channel Cut and the meaning of pronouns to them.\n\n==Talking Behaviour==\nLanguage matters. It is very important within any form of institution and group setting to be aware of the respective ripples that language may create, for example reinforcing power imbalances regarding gender, race or class. Therefore, reflecting on your own usage of language, talking behaviour, and role within a group is important. There shortly will be a whole Wiki entry regarding this topic, however here is a summary worth looking at.\n\n===Reflexive questions for students===\n\u2022Am I contributing to the whole group in a balanced way?\n\n\u2022What is the experience of other people, and do I perceive and include their experiences?\n\n\u2022Do I presuppose experience or knowledge that not all people have?\n\n\u2022Is my language inclusive and respectful?\n\n\u2022Do I try to avoid stereotypes?\n\n\u2022Am I repeating arguments someone else has already said?\n\n\u2022How do I respond to criticism? Do I feel the direct urge to comment on it or can I just listen to it first?\n\n\u2022How do I voice criticism towards others?\n\n\u2022Which gestures do I use while talking? How might these affect others?\n\n\u2022How is my talking time in comparison to others?\n\n\u2022Is my experience representative for most/ all?\n\n===Reflexive questions for teachers===\n\u2022Are there certain structures in the seminar that contribute to the fact that certain groups of people predominantly speak up? How diversity-sensitive is the design? (e.g. one-sided methodology, no consideration of different learning types, gender-sensitive language, which texts by which authors are being read?)\n\n\u2022Is the setting of the group meeting inclusive, empowering, fosters diversity and builds on diverse learning strategies and needs?\n\n\u2022Are people interrupting others, or dominating the group? How am I responding to this?\n\n===General tools===\n\u2022(Quoted) list or round of speakers\n\n\u2022Code of conduct\n\n\u2022Reflexive rounds to share perceptions within the group\n\n\u2022Moderators\n\n\u2022Protocol of the discussion, including a sharing of this protocol among participants and the possibility for edits.\n\n\u2022Recording with an app, given that everybody agrees.\n\n\u2022Anonymous feedback tool\n\n==Toilets==\nMost toilets and changing rooms in public buildings, schools, restaurants and workplaces are divided into women and men only. Many inter*, trans*, non-binary and genderqueer people have discriminatory and violent experiences in such toilets and changing rooms because they visit a supposedly \"wrong\" toilet. For example, they report being misgendered, stared at or called names and asked intimate questions about their gender or body. Many therefore try to avoid going to the public toilet altogether by drinking as little as possible or only being in public spaces as often as absolutely necessary.\nAll-gender toilets and changing rooms pose a solution and '''safer space''' by providing access to all people, regardless of gender, with as little fear and discrimination as possible.\n\nEven though many universities in Germany for example started to build all gender toilets on their campuses, \u201cgendered\u201d bathrooms are still the norm. When entering those bathrooms, keep comments to yourself or if you witness a person calling out on another of whom they think doesn't belong there, inform them and be an ally.\n\nAs advice for teachers, if all gender toilets are not common in every building, point out where the next one is. Also, be aware that people who go to the toilet during the lecture or seminar have every right to do so. They can have any reason why seeking to go to the bathroom is valid, be it wanting to go when toilets are not too frequented or needing to due to menstruation. Please avoid asking them to go during the break instead.\n\n===Reflexive questions while visiting the next public bathroom===\n\u2022For whom are bathrooms a safe space, who is excluded?\n\n\u2022Whose needs are respected there?\n\n\u2022Which norms are reproduced by the signs/names of the bathrooms?\n\n\u2022Are there toilets for all genders, barrier-free ones, ones accessible for all with diaper changing tables?\n\n\u2022How much time does it take to reach them and are break times thus appropriate? (especially for teachers)\n\n\u2022Are there bins for sanitary products in all toilets and if not, which assumption about people who menstruate is thus reproduced?\n\n==Institutional commitment==\nWithin most constructed institutions, the legal status concerning gender is focusing on inequalities related to the differences between women and men. These societal constructed categories are for sure not the only categories that create inequalities and injustices (see '''Intersectionality'''), but most constructed institutions to this day unfortunately focus on these specific inequalities and injustices. There are other legal changes currently happening in some countries that take other viewpoints and a more diverse understanding into account, yet these changes are slow, often singular, and by a lack of implementation even a backlash. Since 1st January 2019, \u2018divers\u2019 has been a third positive gender entry in German civil status law alongside \u2018female\u2019 and \u2018male\u2019. In addition, it is also possible to have \u2018no gender\u2019 registered. However, this change of gender entry is currently only available to intersex persons. This also expands the understanding of gender that institutions base their gender equality work on. But there are also other developments happening creating a backlash in LGBTQ+ rights. A couple of weeks ago the state of Florida in the US passed a law titled \u2018Parental laws in Education\u2019. In the media it got popular with the name \u2018Don\u2019t say gay bill\u2019. The new law seeks to ban discussion of gender and sexuality issues from schools. A separate analysis by the CNN journalist Giselle Rhoden found that in the current legislative session around 150 anti-LGBT bills have been introduced at the state level. They range from restricting access to gender-affirming health care for minors to excluding transgender children from athletics.\n\nThe structures and mechanisms in constructed institutions -such as universities- are often slow to adapt to societal changes. And yet, universities as center for innovation, should lead the way also to this end. What is however often happening is that the legal situation became more and more complex over time, and the adaptability decreased. This should not be the case, and it is clear to recognize that we are part of a continuous change where we have already come a long way. However, even within diverse institutions the legal ramifications are lacking behind what we perceive to be the baseline of societal change. An example of this is the dgti supplementary ID. The dgti supplementary ID is a standardised identity document that documents all self-selected personal data and shows a current passport photo. This is helpful because for trans* people the official identity documents do not match their own gender identity before the official change of name and/or civil status. Similarly, this can also be the case with external appearance. This often leads to unpleasant, stressful and humiliating questions or even dangerous situations during a personal check. The dgti ID card is already known and accepted by some banks, universities, insurance companies and others but not everywhere since it is voluntary and not obligatory to accept it.\n\nIt is not ok that the people who live this change are reduced to their specific identity, while the institution does not recognise their identity as they perceive it. We are committed to try to change the institutions as they currently are and are equally committed to support each other in creating this change.\n\n===Where to turn for advice in our institution?===\n[https://www.leuphana.de/en/university/organisation/ombudsperson.html '''Ombudsperson.''']\n\nThe ombudsperson for students and lecturers is responsible for the internal communication with the University's management and administration. Furthermore, the Ombudsperson listens to the ideas, suggestions and problems expressed by the teaching staff and students, and acts as a mediator between the University's various stakeholder groups. The Ombudsperson offers the possibility of dealing with concerns anonymously, confidentially and in all neutrality, and can contribute to the resolution of personal conflicts on the basis of mediation.\n\n[https://www.leuphana.de/en/institutions/office-for-equal-opportunities.html '''Office for Equal Opportunities.''']\n\nThe Office for Equal Opportunities coordinates measures to promote equality and diversity at Leuphana University. It develops measures for the advancement of women as well as antidiscrimination strategies. The Office for Equal Opportunities is committed to the goal of creating a pleasant study, work, career and research environment for all members of Leuphana University and actively supports a diverse and family-friendly university culture.\n\n* [https://www.leuphana.de/en/institutions/office-for-equal-opportunities/portal-sexualized-discrimination.html Information Portal \u201cAgainst sexual harassment\u201d.]\nThe information portal \"Against Sexual Harassment\", which is managed by the Equal Opportunities Office, offers information material on prevention as well as an overview of contact and support options in cases of sexualised discrimination and violence at Leuphana and beyond.\n\n* Other Contact Points and Counselling Services are listed [https://www.leuphana.de/en/institutions/office-for-equal-opportunities/offers-and-information/contact-points-and-counselling.html here] by the Office of Equal Opportunities.\n\n[https://www.asta-lueneburg.de/ '''Student Representation (AStA).''']\n\n* [https://www.asta-lueneburg.de/mitmachen/archipel/ ARCHIPEL.]\nARCHIPEL stands for \"Autonomes Referat f\u00fcr Chronische Erkrankungen, Handicaps und Inklusion, Psychische Erkrankungen, Empowerment und Lernbeeintr\u00e4chtigungen\" (Autonomous Unit for Chronic Illnesses, Handicaps and Inclusion, Mental Illnesses, Empowerment and Learning Disabilities). The unit is committed to an inclusive everyday study life and the dismantling of structural barriers and offers a platform for exchange and diverse opportunities to participate. Contact at archipel@asta-lueneburg.de\n\n* [https://www.asta-lueneburg.de/mitmachen/antira/ AntiRa.]\nThe AStA's anti-racism department aims to address racist and discriminatory structures and work together with the student body to combat them. Contact at antira@asta-lueneburg.de\n\n* [https://www.asta-lueneburg.de/mitmachen/quarg/ QuARG.]\nQuARG stands for Queer, Awareness, Equal Rights and Gender Matters. The unit deals with everything that revolves around the topics of gender and sexuality. This includes homosexuality, bisexuality, heterosexuality and asexuality, as well as intersex/intersexuality and trans*identity, and feminism, sexism and queerness.Contact at quarg@astalueneburg.de\n\n* Student anti-discrimination officer.\nThe student anti-discrimination officer can be contacted in case of discrimination experience as a student at University. The person will try to offer help as best as possible with advice, assists in the communication with the university or just listens to experienced problems. Contact at Antidis@asta-lueneburg.de\n\n==References==\nButler, Judith (1990). Gender Trouble: Feminism and the Subversion of Identity. New York: Routledge.\n\nCrenshaw, Kimberl\u00e9 (1991). Mapping the Margins: Intersectionality, Identity Politics and Violence against Women of Colour.\n\nCombahee River Collective. (1977). The Combahee River Collective Statement. In K.-Y. Taylor (Publ.), How we get free. Black Feminism and the Combahee River Collective (pp. 15\u201327). Haymarket Books.\n\nCameron, Michelle (2005). Two-spirited Aboriginal People: Continuing Cultural Appropriation By on-Aboriginal Society. Canadian Women\u2019s Studies, 24(2), 123-127.\n\nEagle Russett, Cynthia (1989). Sexual Science. The Victorian Construction of Womanhood. Harvard University Press.\n\nEuropean Parliament (2018). Gender Neutral Language in the European Parliament.\n\nFausto-Sterling, Anne (2000). Sexing the Body. Gender Politics and the Construction of Sexuality.\n\nGillis, Stacy; Howie, Gillian & Munford, Rebecca (2004). Third Wave Feminism A Critical Exploration.\n\nGill-Peterson, Julian (2018). Histories of the Transgender Child. Minnesota University Press.\n\nHinchy, Jessica (2022). Obscenity, Moral Contagion and Masculinity: Hijras in Public Space in Colonial North India. Asian Studies Review, 38:2, 274-294, DOI: 10.1080/10357823.2014.901298\n\nHussain, Salman (2019). State, gender and the life of colonial laws: the hijras/khwajasaras\u2019 history of dispossession and their demand for dignity and izzat in Pakistan. Postcolonial Studies, 22:3, 325-344, DOI: 10.1080/13688790.2019.1630030\n\nNewman, Louise M. (1999). White Women\u2019s Rights. The Racial Origins of Feminism in the United States. Oxford University Press.\n\nOrr, Catherine M. (1997). Charting the Currents of the Third Wave.\n\nOyewumi, Oyeronke (1997). The invention of Women. Making an African Sense of Western Gender Discourses. University of Minnesota Press.\n\nSchiebinger, Londa (1993). Nature\u2019s Body. Gender in the Making of Modern Science. Rutgers University Press.\n\nSchuller, Kyla (2018). The biopolitics of feeling: race, sex, and science in the nineteenth century. Durham, NC: Duke University Press.\n\nStearns, Peter N. (2015): Gender in World History. Third Edition. Routledge Taylor & Francis Group.\n\nVervecken, Dries & Hannover, Bettina. (2015). Yes I Can!. Effects of Gender Fair Job Descriptions on Children\u2019s Perceptions of Job Status, Job Difficulty, and Vocational Self-Efficacy. Social Psychology.\n\n\n--------\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Collaborative Tools]]\nThe [[Table of Contributors|authors]] of this entry are Elisabeth Frank, Oskar Lemke, and Henrik von Wehrden."
                    },
                    "sha1": "bz2o0f0gy5a3ietq553c8xtemcocy5w"
                }
            },
            {
                "title": "Generalized Linear Models",
                "ns": "0",
                "id": "646",
                "revision": {
                    "id": "5778",
                    "parentid": "5480",
                    "timestamp": "2021-06-13T22:55:12Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "10883",
                        "#text": "[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.\n\n\n== Background ==\n[[File:GLM.png|400px|thumb|right|'''SCOPUS hits per year for Generalized Linear Models until 2020.''' Search terms: 'Generalized Linear Model' in Title, Abstract, Keywords. Source: own.]]\nBeing baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics.\n\n\n== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.\n\n== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling. \nIn addition to this, GLMs allow to investigate different and more precise questions compared to analyses built on the normal distribution. For instance, methodological designs that investigate the survival in the insurance business, or the diversity of plant or animal species, are often building on GLMs. In comparison, simple regression approaches are often not only flawed within regression schemes, but outright wrong. \n\nSince not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.\n\nAnother advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.\n\nRegarding count data, GLMs prove to be the better alternative to log-transformed regressions, not only in terms of robustness, but also regarding the statistical power of the analysis. Such models have become a staple in biodiversity research with the counting of species and the respective explanatory calculations. However, count models are not very abundantly used in econometrics. This is insofar surprising, as one would expect count models are most prevalent here, as much of economic dynamics is not only represented by count data, but much of economic data is actually count data, and follows a Poisson distribution. \n\n== Normativity ==\nTo date, there is a great diversity when it comes to the different ways how GLMs can be calculated, and more importantly, how their worth can be evaluated. In simple regression, the parameters that allow for an estimation of the quality of the model fit are rather clear. By comparison, GLMs depend on several parameters, not all of which are shared among the diverse statistical distributions that the calculations are built upon. More importantly, there is a great diversity between different disciplines regarding the norms of how these models are utilised. This makes comparisons between these models difficult, and often hampers a knowledge exchange between different knowledge domains. The diversity in calculations and evaluations is made worse by the associated diversity in terms and norms used in this context. GLMs are surely established within advanced statistics, yet more work will be necessary to approximate coherence until all disciplines are on the same page.\n\nIn addition, GLMs are often a part of very specific parts of science. Whether researchers implement GLMs or not is often depending on their education: it is not guaranteed that everybody is aware of their necessity and able to implement these advanced models. What makes this even more challenging is that within larger analyses, different parts of the dataset may be built upon different distributions, and it can be seen as inconvenient to report diverse GLMs that are based on different distributions, particularly because these are then utilising different evaluative criteria. The ideal world of a statistician may differ from the world of a researcher using these models, showcasing that GLMs cannot be taken for granted as of yet. Often, researchers still prioritise to follow disciplinary norms rather than go for comparability and coherence. Hence the main weakness of GLMs is a failure or flaw in the application of the model, which can be due to a lack of experience. This is especially concerning in GLMs, since such mistakes are more easily made than identified. \n\nSince analyses using GLMs are often part of a larger analysis scheme, experience is typically more important than, for instance, with simple regressions. Particularly, questions of model reduction showcase how the pure reasoning of the frequentists and their probability values clashes with more advanced approaches such as Akaike Information Criterion (AIC) that builds upon a penalisation of complexity within models. Even within the same branch of science, the evaluation of p-values vs other approaches may differ, leading to clashes and continuous debates, for instance within the peer-review process of different approaches. Again, it remains to be seen how this development may end, but everything below a sound and overarching coherence will be a long-term loss, leading maybe not to useless results, but to at least incomparable ones. In times of [[Meta-Analysis]], this is not a small problem.\n\n== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==\n\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "kpehoav7p7y99qkoc09yog7l7pqf4xt"
                }
            },
            {
                "title": "Geographical Information Systems",
                "ns": "0",
                "id": "508",
                "revision": {
                    "id": "6912",
                    "parentid": "6909",
                    "timestamp": "2023-02-25T14:27:11Z",
                    "contributor": {
                        "username": "Ben Richter",
                        "id": "160"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20488",
                        "#text": "[[File:ConceptGIS.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Geographical Information Systems]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n'''In short:''' Geographical information systems (GIS) subsume all approaches that serve as a data platform and analysis tools for spatial data.\n\n== Background ==\nWhile Geographical Information Systems are typically associated with digital systems, the systematic creation of knowledge through the analysis of spatial data can be dated back long before the invention of modern computers. The first spatial analysis was focussed on the spread of Cholera in Paris (1832, \"Rapport sur la marche et les effets du chol\u00e9ra dans Paris et le d\u00e9partement de la Seine. Ann\u00e9e 1832\") and London (1854). '''While the map of Paris showed Cholera cases on a scale of the districts, John Snow showed in London the cases of citizens that died of Cholera as dots'''. This allowed for a clear representation of the spread of the disease. Since cases were clustered around a well, John Snow could infer that the water sources is the main spread vector of the disease. This is insofar remarkable, as is clearly highlights how knowledge could be derived inductively without any understanding of the deeper mechanics of Cholera, or let alone even the knowledge of bacteria. \n\nAt this time, topographic maps were already perfected as part of the Nation States creating inventories and understandings of their territories, and also of their colonies and their wealth. Topographic maps were increasingly able to depict different layers of information, as maps were printed in different colours that all represented different kinds of information, such as vegetation, thematic information, water, and more. With the [[History_of_Methods#Internet_and_Computers_-_The_New_Science_Of_Interconnectedness|rise of the computer age]], this information was implemented into computers, and Canada created the first digital Geographical Information System in the 1960. This highlights how [[Glossary|innovation]] of Geographical Information Systems was strongly driven by application, since such systems are of direct benefit in planning and governance. The main driver of innovation in early GIS development was however [http://apogeospatial.com/the-enduring-legacy-of-howard-fisher/ Howard T. Fisher] from Harvard, who - with his team - developed important cornerstones that still serve as a basis for GIS, including different data formats and the general architecture of GIS systems. '''First commercial systems started to thrive in the 1970s and 1980s, yet only with the rise of personal computers and the Internet, Geographical Information Systems unleashed their full potential.'''\n\nWhile simple vector formats were already widely used in the 1990s and even before, the development of a faster bandwidth and computer power proved essential to foster the exchange and usage of satellite data. While much of this data (NOAA, LANDSAT) can be traced back to the 1970s, it was the computer age that made this data widely available to the common user. Thanks to the 'Freedom of Information Act', much of the US-based data was quickly becoming available to the whole world, for instance through the University of Maryland and the NASA. The development of GPS systems that originated in military systems allowed for a solution to spatial referencing and location across the whole globe. Today, commercial units from around the millennium are implemented in almost every smartphone in a much smaller version. This wide establishment allows for the location of not only our technical gadgets, but also goods and transportation machines. An initial NASA system showed a strong resemblance to Google Earth in the early 2000s, and with the commercial launch of Google Earth and Google Maps, the tech giant built a spatial database that is to date almost unrivalled. \n\n[[File:GIS Scopus.png|400px|thumb|right|'''SCOPUS hits per year for GIS until 2019.''' Search term: 'GIS' in Title, Abstract, Keywords. Source: own.]]\n\nWith the Smartphone age, this information became rapidly available to the wider public, and ever more complex growing databases and apps are today adding millions of spatial entries to the global databases. The early and often clumsy GIS software suits are more diversified and broadly available today, making GIS systems a common staple of many branches of science. Open street map data and QGIS prove that data and software solutions are possible beyond the commercial business sector. Geographical Information Systems, and with them geospatial data and complex analysis, have become one of the main branches of the Internet and indeed the Modern Age. Augmented Reality and self-driving cars showcase the potential of this technology for the future, but the potentially constant geotagging of all citizens that own a smartphone also highlights problems of data security and globalisation.\n\n\n'''What is GIS?'''\n\nWith the rise of computers the processing, usage and analysis of spatial data  has been playing a key role across all facets of our societies. Within science, diverse fields such as geography, spatial science, data science and many other fields have harnessed the use of information technology to access information sources and to integrate the processing, analysis and interpretation of spatial data. This development continuously unleashed new potential in how we observe, communicate, and evaluate the spatial patterns, processes and mechanisms  that utilize but equally drive the internet and computerization. Within geographical information systems, a generalisation of the world is saved within diverse data formats, ultimately deriving a coarse version that represents the respective spatial phenomena.\nGeographical information systems -or short GIS- are computer systems for managing, analysing, and displaying data related to positions and geographic information. GIS can show many kinds of data on one map. A GIS facilitates diverse ways to visualize, analyse, and understand spatial patterns and processes. Since much of the data we utilize can be spatially referenced by a coordinate system, GIS data can be facilitated to analyse such spatially referenced information. For example, if we want to know about the number of students participating in different lectures taking place at Leuphana, a simple table cross-referenced with the respective lecture halls might suffice. Yet in order to enable a spatial representation of the location the students within those lecture halls, a geographical visualization is required.\nIllustration of this example?\n\nThere are diverse definitions available for geographic information system, e.g. \n\n\u201cGIS is an integrated system of computer hardware, software, and trained personnel linking topographic, demographic, utility, facility, image and other resource data that is geographically referenced. -------NASA\n\n\u201cIn the strictest sense, a GIS is a computer system capable of assembling, storing, manipulating, and displaying geographically referenced information, i.e. data identified according to their locations. Practitioners also regard the total GIS as including operating personnel and the data that go into the system.\u201d -------USGS\n\n\u201cA geographic information system (GIS) is a computer-based tool for mapping and analysing things that exist and events that happen on earth. GIS technology integrates common database operations such as query and statistical analysis with the unique visualization and geographic analysis benefits offered by maps.\u201d -------ESRI\n\nExamples of Applications of GIS: \n\nGIS operate on various levels and typically serve multiple functions. The most basic level to operate GIS is cartography, which is commonly used to map making. GIS can also be used for spatial data analysis. \n\na.\tGIS Cartography\nMaps are an integral part of human development, and many cultures have utilized maps as crucial tools to navigate, explore, describe and explain. Early maps were essentially an attempt on story telling of distant voyages by cartographers. Cartographers performed the role of artists by creating images based on traveller knowledge, memories, and concerns. With the rise of mathematics, particularly trigonometrics, and the development of technology, maps evolved into digital forms that are now a part of our everyday life due to smartphones, which basically all integrate GIS for localisation and navigation.\n\nGeographic Information Systems (GIS), which offer a visual interpretation of data, are mostly focused on mapping. GIS and cartography are being used for an increasing diversity of applications, including integration of Global Positioning System (GPS), remote sensing, global data bases, internet use, web mapping services, and diverse software applications.\n\nb.\tSpatial Analysis\n\nWhile most GIS applications are related to map making, the term GIS is increasingly being associated with spatial analysis and planning as well.  Procedures such as the capturing, visualization, manipulation and analysis of geographic information are continuously evolving within GIS systems. Environmental management and spatial planning are two crucial fields of application for GIS along with agricultural value chain, public health and surveying (Masser and Wegener, 1996). GIS has become a standard tool within spatial planning. A broad variety of pre-programmed tools for spatial analysis are available in modern software solutions, such as ARC/GIS and QGIS. Finer scales are approached by architectural tools such as AutoCAD, and programming languages such as Python and R offer an array of tools for spatial analysis. These tools include network analysis, modelling of digital terrain, and spatial overlay. The ability to modify complex and intricate analysis of sequences using continuously evolving programming languages and data bases has become essential in all components of our daily lives. By linking spatial data to advanced analysis approaches the ability of GIS to allow for future predictions and adaptations has become a breakthrough in modern science. \n\nElements of GIS:\nGIS can process both geographic and spatial information at the same time. This requires a variety of functional elements and components. There are five major components including hardware (computer systems, laptops), software (ArcGIS, QGIS, ERDAS etc.), human resources (people with ample knowledge of handling and applying GIS), datasets and analytical methods. \n \n[[File:COmponents GIS.png|thumb|Figure 1: A working integrating GIS five components.]]\n\nGIS also includes five major functional or procedural elements for successful operation or application. Those are namely: data acquisition, pre-processing, data management and manipulation, analysis and results generation. \n\nData acquisition: It is the process of identifying and gathering new data by preparing maps, aerial photography and surveys. Following table shows few methods of data acquisition: \n\n[[File:Table 1.png|thumb|Table 1 showing the various data sources and their acquisition methods.]]\n\nPreprocessing: It entails changing the data in a variety of ways before entering it into the GIS. Data format conversion and methodically locating objects in the source data are two of the main responsibilities of preprocessing. Extracting data from maps, photos, and printed records (such demographic reports) and then entering it in a computer database are common methods for changing the format of the original data. This process is often time-consuming and expensive exercise. This is especially true when one estimates the costs of moving sizable data volumes from an automated GIS based on computerized databases to paper maps and transparent overlays.\n\nData Management and Manipulation: The development and accession of the database itself are controlled by data-management operations. Data entry, consistent update, deletion, and retrieval are offered by these functions. The users of contemporary database management systems are shielded from the specifics of data storage. Users typically are not aware of well completed data management processes. Security considerations relate to data management. There must be procedures in place to give various users varying levels of access to the system and its database. Data manipulation consists of working with the database contents to derive new information. \n\nAnalysis: Analysis is often the system user's focus of attention. Many people mistakenly think that this module alone makes up a geographic information system. The analytical operators that use the database contents to generate new information are in this part of the system. We need specific facilities to be able to transfer data and information between systems because one system cannot provide the full range of analytical capabilities a user might conceive. \n\nResults Generation: Final outputs from the GIS like statistical reports, maps, and graphics of various kinds are the parts of result generation.\n\n'''Normativity'''\n\n\u2022\tThis weakness of data security highlights that GIS information is not only presenting representations of the real world, but also represents generalisations that can be deeply normative. 'Gerrymandering' is an example where the spatial designation of US election districts is clearly altered by politicians to indirectly influence election results. Also, the visual representation of data in maps can be deeply normative, if not manipulative.\n\n\u2022\tHardly any standards of representation of spatial data exist to date. Instead, there are many diverse and more or less aesthetically appealing maps and GIS-derived representations, and the number is growing. This complicates comparability, and increases the risk of falling for manipulative maps.\n\n\u2022\tLastly, as any given empirical data source, a GIS system is only as good as the data that feeds it. Many wrong decisions and imperfection in planning as well as in spatially explicit science were made based on imperfect and limited GIS systems. Despite this known shortcoming, GIS-based analyses are often not amended by a limitation of the given analysis, but are instead simply accepted as is. More critical evaluations are needed to make GIS analysis not only clearly designated in terms of the content, but also the limitations. GIS systems would benefit from such a critical perspective.\n\n'''Outlook'''\n\nGIS-based analyses are one of the silent revolutions that came out of the new and interconnected systems that represent our postmodern reality. Spatial data will become even more important in the future, and this demands a pronounced recognition of the responsibility associated with this. There are ample examples how spatial data is used to maximise profit and to use users as a data source. While the technological potential of GIS system will surely increase in the future, the underlying ethical questions and standards will have to be matched to the wealth of data that will become available. It would have been hard to imagine for people twenty years ago how GIS systems are intertwined with our society as well as our realties today. In the next decades, we need to learn more about the possibilities and challenges that may yet arise out of Geographical Information Systems. \n\n'''An exemplary study'''\n \nIn their 2015 publication, Partelow et al. (see References) analyzed the extent of pollution exposure on global marine protected areas (MPAs). As a starting point, they characterized protected areas based on five different attributes that they considered critical in defining an MPA's biophysical signature, which were \n\u2022\tdistance from the shore,\n\u2022\tamount of biodiversity,\n\u2022\tbathymetry (= the ocean's depth relative to the sea level),\n\u2022\tmean surface temperature, and\n\u2022\tmean sea surface salinity.\n\nThey projected each attribute as one layer of raster data into the ArcMAP software, and global marine protected areas as another layer. They selected 2,111 areas that fit their research intent of focusing on conservation, and exported all data into R for statistical analyses. Each MPA was now attributed with data for each of the five categories, based on which a cluster analysis was conducted. This led to the designation of five kinds of groups. \n \n\n[[File:Picture3.png|thumb|'''This map shows the analyzed MPAs.''' Each color / shape represents one type of marine protected area (MPA) as a result of the cluster analysis based on biophysical characteristics. Source: Partelow et al. 2015, p.354]]\n\n\nThe five groups differ in terms of their biophysical characteristics, which is why the cluster analysis grouped them together. Group 1 is distinctive through lower mean sea surface temperatures and lower biodiversity levels. Group 2 represents MPAs with higher bathymetry and higher shore distances. Group 3 have mid-range biodiversity values and seasonal sea surface temperatures. Group 4 has very high biodiversity values and shallow waters. Group 5 has a significantly lower mean sea surface salinity. \nIn the next step, the researchers added data on pollution in two forms. Current pollution data was gathered as secondary data from another study, representing current impacts on the MPAs such as shipping traffic frequency, fishing rates, invasive species and others. This data was projected as another raster layer in ArcMAP and attributed to the MPAs in R. Also, future pollution data was added as a layer regarding the impacts of change for UV light on the ocean surface, changes in ocean acidification rates, and sea surface temperature changes. This data was based on changes in these variables until today and was considered representative of which pressure exists on the ecosystems. \n \n[[File:Picture1.png|thumb|'''This map shows the impacts of current pollution on the identified types of MPAs'''. Source: Partelow et al. 2015, p.354]]\n\n[[File:Picture2.png|thumb|'''This map shows the impacts of future pollution on the identified types of MPAs.''' Source: Partelow et al. 2015, p.355]]\n\nBased on their analysis of current and future pollution impacts on the MPAs, the authors concluded that: \n\n\u2022\ta majority of current MPAs is affected by pollution,\n\n\u2022\tcurrent pollution is strongest in the global north, while future pollution may impact tropical marine ecosystems more strongly,\n\n\u2022\tfuture pollution will on average be higher, and have stronger effects, than current pollution for all MPA groups, and\n\n\u2022\tan increase in rates of future pollution may be less dependent on the biophysical characteristics of the MPAs than current pollution.\n\nBased on their results, the authors propose diverse recommendations for conservation management. Overall, the study shows how spatial raster data can help assess the state of ecosystems and guide conservation management, both on a global and local scale. \n\n'''Key Publications'''\n\n\u2022\tSnow, John. 1855. On the mode of communication of cholera. John Churchill.\n\n'''References'''\n\n(1) Partelow, S. von wehrden, H. 2015. Pollution exposure on marine protected areas: A global assessment. Marine Pollution Bulletin 100(1). 352-358. \n\nFurther Information\n\n\u2022\t[http://apogeospatial.com/the-enduring-legacy-of-howard-fisher/ Some information] on the importance of Howard T. Fisher's work for GIS.\n\n\u2022\tA QGIS overview on [https://docs.qgis.org/2.8/en/docs/gentle_gis_introduction/vector_data.html vector data] and [https://docs.qgis.org/2.8/en/docs/gentle_gis_introduction/raster_data.html raster data].\n\n\u2022\tAnother overview of raster and vector data by [https://gisgeography.com/spatial-data-types-vector-raster/ gisgeography.com].\n\n\nThe authors of this article are [https://www.leuphana.de/institute/institut-fuer-oekologie/personen/henrik-von-wehrden.html Prof. Henrik von Wehrden] and [https://www.leuphana.de/en/institutes/institute-of-ecology/team/neha-chauhan.html Neha Chauhan]."
                    },
                    "sha1": "4rfeb69kpin2hfl3t1u8zx1u714jnmi"
                }
            },
            {
                "title": "Git and GitHub",
                "ns": "0",
                "id": "749",
                "revision": {
                    "id": "6578",
                    "parentid": "5288",
                    "timestamp": "2022-03-21T07:41:58Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8750",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n__TOC__\n\n== What, Why & When ==\n[[File:GitHubLogo.png|250px|thumb|right|Fig.1: GitHub Logo]]\n[[File:GitLogo.png|250px|thumb|right|Fig.2: git Logo]]\nHave you ever accidentally overwritten or deleted your teammates\u2019 code in a Google Drive? Ever wanted to go back to a previous version of your script because your bug fixing just created more problems? Ever struggled to combine different snippets of code that were collected through email or even WhatsApp hours before the deadline?\n\nFear no more, because this article will introduce you to GitHub and help manage the above issues with ease.\n\nGit is an open-source distributed version control (DVCS) and source code management (SCM) system that was initiated by Linus Torvalds \u2013 the same guy who played a major role in the creation of the Linux operation system. Git manages changes made to a code repository, allowing developers to go back to previous versions, duplicate the code to other environments and merge the work of different developers.\n\n== Goals ==\nGitHub is a web-based hosting service that gives a graphical user interface to the underlying functionality of git and extends it by many more features that '''facilitate collaboration between developers'''. These functionalities include user and access management, bug tracking, continuous integration and many more. \n\nAlmost every company that is involved with software development uses GitHub, making GitHub not only the largest host of source code in the world but also a basic skill for every developer, engineer or data scientist.\n\n== Getting started ==\nIn the following, the basic functionalities of git and GitHub will be described. Hopefully, this helps to streamline your efforts to deliver the best team reports every submitted in Software for Analyzing Data.\n\n===COMPONENTS OF GITHUB===\n\n====Repositories====\nRepositories (\u2018repos\u2019) are the central object of git that contains the actual code files of a project together with each file\u2019s revision history.  \n\n====Organizations, Teams & People====\nThe Users of GitHub are called people on the site. They can be members of a team, which then can be part of a larger organization. Teams can also be nested inside each other, depicting the organizational structure of your company. Your belonging to a team or organization is relevant for your access to their repositories.\n\n====Issues====\nIssues is the build in bug tracking function on GitHub. You can directly mark, discuss and assign bugs and other problems with the code on the platform.\n\n====Projects====\nProjects is the build in project management solution of GitHub. You can use Management frameworks like Kanban to assign tasks to team members and review their completion.\n\n===GITHUB FUNCTIONS & WORKFLOWS===\n\nSince working with GitHub usually is a team effort, everybody develops their own style or model to using it over time. Different teams will make different agreements on how to use the functionalities of git in order to best meet their needs. Some might use a shared repository model where you branch & merge your development efforts while others might prefer a Fork & Pull model.\n\nIn the following, the different functions of git will be described along a generic workflow that could serve as a starting point to develop a more refined process.\n[[File:GitHubworkflow.png|550px|thumb|center|Fig.3: A basic GitHub workflow]]\n\n====Branch and Clone====\nWhen you are working on a project and want to implement new ideas, a branch should be created. The branch will be a duplicate of the main project at the time of its inception and enable you to work on your ideas without affecting the main branch inside a safe \u201csandbox\u201d environment. Using separate branches enables a team to work simultaneously on the same project. You could then clone the branch to your local system to make changes to its files. \n\nAn alternative to branching could be forking the repository. A branch stays within the same repository and only saves the changed files that can later be merged into the main branch, whereas a fork is an independent hard copy of the entire repository that can be united by way of a pull request. It depends on your style and the organization\u2019s agreement, which model is applied. In this case, we are assuming shared repository model with branching as the main method.\n\n====Commit====\nAfter completing a small increment or milestone in your development, a commit saves this state as a revertible snapshot in the project\u2019s history. Every feature or change should be committed individually in order to be able to retrace the development process and revert if necessary. The changes made should be described in the commit message to make it easier to follow along.\n\n====Pull request====\nPull Requests are designed to start a discussion and encourage feedback of your code. A pull request lets you compare two branches of a repository and initiates a discussion about the changed features. If you were developing on a forked project, the pull request will notify the original maintainers of the project repository to review and consider your changes. \n\n====Discussion, issues and review====\nFollowing the Pull Request, other collaborators can discuss and review the changes with you or even add follow-up commits before you merge your changes into the main branch. \n\n====Deploy====\nTo test your changes in the production system, it is usually deployed to a test environment before merging into the main branch. This is not always the case and depends on the specific usage of your team.\n\n====Push and Merge====\nWhen everything is in order, you can publish your branch by pushing it to the server. Only then, the committed changes are made publicly available. The changes can then be merged into the main branch, bringing together your work with potential other adaptations made through different branches at the same time.\n[[File:refinedgithubflow.png|550px|thumb|center|Fig.4: A more refined GitHub workflow]]\n\n===INSTALLATION GUIDE===\n\n====Basic: GitHub desktop====\nAll the functions of git can be accessed through the graphical user interface of GitHub desktop without having to worry about the command line. Installing GitHub desktop is the recommended way to get started if you are new to GitHub. Just download and execute the installer from this link, it comes packaged with all necessary dependencies.\n\nhttps://desktop.github.com\n\n====Command Line====\nMore advanced, lightweight and fast to use can be the command line interface. Depending on your Operating System, it can be installed though homebrew or git-scm.\n\nhttps://github.com/git-guides/install-git\n\nOnce installed, the git functions can be accessed by typing commands in the command line:\n\n'''git clone'''\t- Duplicate a repository to your HD\n'''git add'''\t- Add changed and new files to your next commit\n'''git commit'''- Commit changes as a snapshot of the current state\n'''git status'''- View which files have been changed and staged\n'''git branch'''- Create a new branch of the repository\n'''git merge'''\t- Merge the active branch into main\n'''git pull'''\t- Pull changes from the server\n'''git push'''\t- Push committed changes to the server\n\n===IDE integrations===\n[[File:Gitintegration.png|300px|thumb|right|Fig.5: Git integration in JetBrains PyCharm]]\nMost popular Integrated Development Environments (IDEs such as JetBrains (IntelliJ, PyCharm, \u2026) VSCode, DBeaver, Eclipse, Slack, \u2026 ) offer a direct integration with GitHub.\n\nThis can be very convenient, since it allows to commit and push changes directly from the development interface without interrupting your workflow. At the same time, updates can be pulled just as quickly. In addition, it usually gives some visual cues about the status of your local branch compared to the remote repository which can be very helpful.\n\nIf available for your IDE, I would recommend installing the github extension for it.\n\n== Links & Further reading ==\n* https://guides.github.com\n* https://docs.github.com/en/\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Software]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Moritz Kath."
                    },
                    "sha1": "3t5ih4eaiakwudj8llweta86yr1ry50"
                }
            },
            {
                "title": "Giving Feedback",
                "ns": "0",
                "id": "297",
                "revision": {
                    "id": "5847",
                    "parentid": "5055",
                    "timestamp": "2021-06-21T08:43:27Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Goal */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2443",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== Why & When ==\nGiving feedback is a central element of any well-functioning relationship or institution; people face each other and complement their self-image through external evaluation.\n\n== Goal ==\nIf a person is unclear about the effects of his or her actions, he or she has no indication of what changes are necessary. \nThe motive for giving feedback should therefore always be to help the other person to make possible course corrections.\n\n== Getting started ==\nThe basis for good feedback should be a solution-oriented approach, it should be formulated in a positive and motivating way, showing new perspectives and possibilities. \nIn order to constructively point out a way for change, feedback should be situation-specific, timely and concrete, so that the person receiving feedback is able to react. Feedback should be marked as a personal perspective and, especially in the case of critical feedback, it should be appreciative and empathetic. In some situations, however, feedback can also be inappropriate, for example, if the person opposite is not in the emotional state to deal with it. The person giving feedback should therefore make sure in advance to what extent his or her feedback is welcome.\n\nTo sum it up, feedback should be:\n\n'''timely''': context should be clear\n\n'''positive and constructive''': are new perspectives presented?\n\n'''requested''': does the other person want to hear your opinion?\n\n\nPossible steps: \n\n1. I observe... your behaviour.\n\n2. I guess... how I interpret you.\n\n3. I feel... my feelings about what I'm experiencing.\n\n4. I want... my wishes and demands are...\n\n== Links & Further reading ==\n\nhttps://www.entrepreneur.com/article/219437\n\nhttps://www.youtube.com/watch?v=wtl5UrrgU8c\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Max Kretschmer."
                    },
                    "sha1": "qeqfv40012osujqnf8vtzalwgvz81el"
                }
            },
            {
                "title": "Glossary",
                "ns": "0",
                "id": "399",
                "revision": {
                    "id": "5168",
                    "parentid": "4718",
                    "timestamp": "2021-04-27T07:32:54Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23055",
                        "#text": "'''This Glossary lists terms and words that are relevant to this Wiki.''' \n* For many Glossary entries, you will find a list of Wiki articles that prominently contain this term. \n* To see all entries that contain the term, please type the term into the search bar on the top right.\n* The Glossary is work in progress and continuously amended.\n* A considerable part of these definitions is adapted from the [https://i2insights.org/ ''Integration and Implementation Insights'' Blog.] We are thankful for their contribution and highly recommend visiting the blog!\n\n{| class=\"wikitable sortable\" | + style=\"width: 100%; background-color: white; text-align: center\"\n|-\n! style=\"width: 15%\"|Term !! style=\"width: 80%\"|Explanation !! style=\"width: 5%\"| See\n|-\n| Accountability || Being responsible for one\u2019s actions, performance, behaviours, decisions and more, both on an individual and an institutional level, including the responsibility for negative outcomes and consequences. ||\n|-\n| Adaptation || Adaptation is both an adjustment to actual or expected change and the adjustments required to achieve change, and is most prominently used on climate change research, yet can be valuable way beyond that. The adjustments aim to moderate, mitigate or altogether avoid harm and to exploit beneficial opportunities and may require on-going flexibility where there is continuous change. ||\n|-\n| Advocacy || Activity by an individual or group that aims to influence decisions in a particular way. ||\n|-\n| Agency || The capacity of an individual to act intentionally with the assumption of a causal outcome based on this action. || [[Agency, Complexity and Emergence|1]]\n|-\n| Analogy || A cognitive process useful in problem solving. It involves reasoning by transferring information or meaning from a particular problem to another problem to develop solutions. There is also a more common use of the term \u2018analogy\u2019 which is a linguistic expression comparing things with similar features to help explain an idea. ||\n|- \n| Art || The expression of creativity in objects, environments and experiences which are beautiful or have emotional power, allowing our senses to be at their fullest. Includes painting, sculpture, architecture, music, theatre, film, dance, literature. ||\n|- \n| Assumptions || For individuals, assumptions are essentially mental models that consist of a prerequisite that is considered to be true or false without immediate evidence. For theories, methods and models, assumptions are often simplifications that are an important element that allow for their construction and that affect how useful they are. ||\n|-\n| Bias || The action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgement. || [[Bias and Critical Thinking|1]], [[Bias in statistics|2]]\n|-\n| Brainstorming || A divergent thinking technique to generate numerous and diverse ideas, including quirky ones, which is used when creative thinking is required, e.g. in problem solving. || [[Yes, and|1]], [[Disney Method|2]], [[Design Thinking|3]]\n|-\n| Change || Various aspects of altering reality as we perceive it, which may range from minor to transformational and which include, but do not necessarily lead to, improvement. Concrete considerations include modifying policy and/or practice in government, business or civil society, as well as planning for the future. ||\n|- \n| Change resistance || Opposing alterations or suggested alterations to the status quo. This can be by, for example, individuals, groups or organisations. Resistance to change also occurs in natural and social systems. ||\n|-\n| Collective intelligence || The shared wisdom and knowledge that emerges out of a group\u2019s collective efforts, that is more than an individual can produce, allowing for consensus decisions. Such knowledge is often more than the sum of the parts of knowledge of all individuals, thereby enabling novel solutions. ||\n|-\n| Colleges of peers || Groups of people with similar expertise in research who can effectively assess each other\u2019s research grant applications and publications. This is analogous to the way normal science operates. ||\n|-\n| Communication || Sharing information, by various means, especially to increase understanding between individuals or groups. ||\n|-\n| Competencies || Knowledge, skills, abilities and attributes required to understand, integrate and create knowledge. || [[Design Criteria in Sustainability Scienc|1]]\n|-\n| Complex Systems / Complexity || Complex systems are composed of many components which may interact with each other in various ways and which are therefore difficult to model. Specific properties include non-linearity, emergence, adaptation and feedback loops. || [[Agency, Complexity and Emergence|1]]\n|-\n| Concept || Abstract mental representation of our world. || [[Levels of Theory|1]]\n|-\n| Consultation || A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. || [[Transdisciplinarity|1]]\n|-\n| Context || The specific settings and circumstances of any given system or group of people. These context specific factors can include historical, political, cultural and other circumstances, as well as the structure and culture of the research and/or stakeholder organisations involved. ||\n|-\n| Creativity || Forming something novel and valuable, including ideas, theories, inventions and art. ||\n|-\n| Credibility || The believability of a person, source or message based on trustworthiness and expertise, and also often-shared experience and/or identity. ||\n|-\n| Cultural Models || Cultural models are taken-for-granted understandings of the world that are shared by groups of people. Like a mental model (on an individual scale), a cultural model is a group\u2019s implicit representations of, and thought processes about their perceived reality. ||\n|-\n| Culture || Behaviours and norms shared by groups of people. When the group is a society, culture includes language, religion, cuisine, social habits, music and arts. When the group is an organisation, culture includes shared attitudes, values, goals, and practices. ||\n|-\n| Culture Shift || The process of changing beliefs, behaviours and outcomes, usually in an organisation or other constructed institutions. ||\n|-\n| Cross-cultural research || Investigating issues that involve two or more cultures. Also includes learning from other cultures. ||\n|-\n| Data || Quantitative or qualitative units of information that can be used for analysis. || [[Data formats|1]], [[Design Criteria of Methods|2]]\n|-\n| Debiasing || Accounting for and reducing biases, particularly in judgments and decision-making. || [[Bias and Critical Thinking|1]]\n|-\n| Decision context || The circumstances under which a decision is made and which influence the decision. ||\n|-\n| Decision-making || Selecting a course of action among several alternate possibilities. ||\n|-\n| Decision support || Use of analytical tools, which may be computerized, to assist individuals and groups in decision making. Decision support includes various kinds of modelling and mapping. ||\n|-\n| Deductive reasoning || Deductive reasoning builds on statements or theories that are confirmed by observation or can be confirmed by logic. || [[:Category:Deductive|1]]\n|-\n| Dialogue || Conversations to share understandings and, ideally, integrate them towards solutions. Such conversations are often centred around problem framing, mutual learning and joined consensus, resolving problems for action. The aim is not to convince others, but instead to mutually share openly and honestly. Dialogue can be unstructured, semi-structured or structured. Structured dialogues are helpful when groups get larger. ||\n|-\n| Dispositions || A person\u2019s innate or learned qualities and inclinations, including tendencies to act in specific ways. Dispositions are useful for research integration and implementation include humility, curiosity and flexibility. ||\n|-\n| Dualism || Also known as either/or thinking. A style of thinking that builds on a constructed meaning in the world by dividing ideas, people, objects, processes and so on into two contrasting fundamental categories, e.g. good or evil, subject or object, and quantity or quality. || [[Data formats|1]], [[Design Criteria of Methods|2]]\n|-\n| Emergence || The incurrence of a characteristic or behaviour of two or more entities that could not be anticipated based on the individual parts. || [[Agency, Complexity and Emergence|1]]\n|-\n| Empowerment || The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership and solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. || [[Transdisciplinarity|1]]\n|-\n| Endogenous view || Approaches a problem searching for its causes and cures within the system boundary. ||\n|-\n| Facilitation || Planning, guiding and managing a group process and environment, by a facilitator. Facilitation is a composite term that may include:: full participation, mutual understanding, shared purpose and responsibility, and high-quality decisions. There may also be other aims depending on the purpose of the group process, and since professional facilitation is currently emerging, this definition may change. ||\n|-\n| Feedback Loops || A feedback loop is a process in which an output of a system is circled back and used as one or more inputs, through direct or indirect causal links. Feedback loops can be reinforcing (positive) or balancing (negative). || [[System Thinking & Causal Loop Diagrams|1]]\n|-\n| Fragmentation || Existing and functioning in separate parts, usually referring to the research \u2018community\u2019 with expertise in research integration and implementation. ||\n|-\n| Framework || A real or conceptual basic structure that supports or guides practical applications. || [[Levels of Theory|1]]\n|-\n| Funding || The provision of money, usually by agencies associated with government, philanthropy, or business, to support research on complex problems. ||\n|-\n| Hypothesis || A preconceived idea about the world that guides the research process and is to be falsified by it. || [[Experiments and Hypothesis Testing|1]]\n|-\n| Incommensurability || Ideas, theories, methods, standards, values and more, often from different disciplines, that have no common basis and are therefore unable to be integrated (for example in interdisciplinary research). ||\n|-\n| Inductive reasoning || Inductive reasoning draws conclusions based on data or observations. || [[:Category:Inductive|1]]\n|-\n| Innovation || Implementing something novel, including a new idea, method, technology or product. ||\n|-\n| Institutionalisation || Embedding research integration and implementation into the academic mainstream, e.g. by establishing departments of research integration and implementation, centres of interdisciplinarity, relevant journals and professional associations, funding streams, promotion criteria etc. ||\n|-\n| Interactional expertise || The ability to understand disciplines, professional practice and community experience without being trained in those disciplines or professions or having lived in those communities. ||\n|-\n| Journals || Academic or scholarly periodicals where knowledge about theories, methods and topics are published. || [[Staying on top of research|1]]\n|-\n| Knowledge governance || Formal and informal rules that shape knowledge production, research agenda setting, research financing, sharing and protecting knowledge, implementation of knowledge and other knowledge-based activities. Rules range from social expectations to intellectual property law. ||\n|-\n| Knowledge synthesis || Pulling together what is known about a problem from either or both of academic research and non-academic knowledge. || \n|-\n| Knowledge systems || The people, practices and institutions (including universities) involved in producing, transferring and using knowledge. ||\n|-\n| Leadership || Being in charge of, guiding, encouraging, organising and/or directing other individuals, teams or organisations or other constructed institutions ||\n|-\n| Legitimacy || What is accepted as proper (for researchers and stakeholders) in conducting research including knowledge, concerns, processes and authorisation. For stakeholders, legitimacy also includes whether representatives of stakeholder groups are nominated in a generally acceptable way. ||\n|-\n| Leverage Points || Places in systems where a small shift in one element can change or tilt the behaviour of the whole system or significant parts of the system. ||\n|-\n| Meeting Protocols || Explicit expectations and ground rules for meetings, aiming to make them run better. Meetings involve two or more people, occur in many environments and serve multiple purposes, often involving sharing information and/or joint decision making. Conferences are included. ||\n|-\n| Mental models || Mental models are representations of reality of individuals, based on the individuals perceptions, and guides their actions. Mental models are a combination of the surrounding people\u2019s minds are their private images (or other representations) of, and thought process about, what things are and how things work in the real world. These subjective, incomplete and sometimes flawed simplifications of reality play a major role in how people think, reason and make decisions. ||\n|-\n| Networking || Developing and using a web of professional contacts who can, when needed or requested, provide various forms of support including information, resources, insights, feedback, advice, contacts for others, and assistance with dissemination of research findings. Some network connections may develop into relationships. ||\n|-\n| Non-linearity || Relationships where changes in inputs do not lead to proportional changes in outcomes. Outcomes may be chaotic, unpredictable, or counterintuitive. || [[Agency, Complexity and Emergence|1]]\n|-\n| Open Science || A movement to make research processes, data and findings transparent and accessible to all. It includes access to research papers that is open, rather than behind a paywall, open reviewing where the reviewers\u2019 names and comments are made public, and making research processes public e.g. making researcher notebooks and raw data available online. ||\n|-\n| Paradigm || A universally recognized scientific achievement that provides theoretical and practical foundations for a specific scientific community. || [[Levels of Theory|1]]\n|-\n| Participation || A general term for a range of interactions both among researchers and other university staff with different expertise and between researchers and stakeholders. || [[Transdisciplinarity|1]]\n|-\n| Patterns || Patterns are regularities, where the elements repeat in predictable ways. Examples are standard ways of approaching a problem, standard sub-processes in modelling, standard layouts for organising research publications (eg introduction, methods, results, discussion). Patterns can be explicit or tacit. ||\n|-\n| Perseverance || Persistence or continued effort in doing something in order to achieve success, often despite difficulties, delay, failure and opposition. Also a Mars rover. ||\n|-\n| Policymaking || Setting the course of action to be pursued, especially by government, business or nongovernmental organisations. For governments, policy making includes making or changing laws and regulations, and setting budget priorities. ||\n|-\n| Power || Possession of control, authority or influence over others and how it impacts the conduct and communication of research, as well as research implementation and change. ||\n|-\n| Power asymmetry || Differential ability to exert control, authority or influence over others, within science especially in deciding what research will be conducted and how. ||\n|-\n| Problem-framing || Problems are defined differently by different disciplinary experts and stakeholders. Addressing any problem requires taking these different understandings of the problem into account in developing an agreed (or at least acceptable) statement of the problem, which will then determine how it is tackled. Coming to a shared problem framing will not always be possible, especially for complex problems. || [[Transdisciplinarity|1]]\n|-\n| Processes || Series of actions or steps taken in order to achieve particular ends. ||\n|-\n| Productive disagreement || Turning discomfort, tension, arguments or conflict into dialogue that broadens perspectives and aids learning and creativity. ||\n|-\n| Qualitative research || Qualitative research focuses on the human dimensions of the observable or conceptual reality, often linking observational data or interpretation of existing data directly to theory or concepts. || [[:Category:Qualitative|1]]\n|-\n| Quantitative research || Quantitative research focuses on the statistical and mathematical analysis of data, as well as the general analysis and often interpretation of data that consists of numbers. || [[:Category:Quantitative|1]]\n|-\n| Researcher || Someone who works actively in research. ||\n|-\n| Research ecosystem / environment || Different layers and interconnections which affect research conduct, including individuals, teams, organisations, funding and the communities in which research may be embedded. Ecosystems operate within and across institutions. ||\n|-\n| Research impact || Change that can be attributed to research. This includes making a difference in policy or practice, or in skills, attitudes, relationships or thinking. Research implementation is the process, research impact is the outcome, although impact may not be able to be unequivocally linked to specific implementation activities. ||\n|-\n| Rules || Accepted principles or instructions about the way things are or should be done, including norms, practices, taboos, regulations, legislation, treaties and ordinances. ||\n|-\n| Scaffolding || Using temporary structures, techniques, ideas, spaces etc to help those new to aspects of research integration and implementation understand and use concepts, methods and processes that are hard to grasp. Scaffolding is often provided by an educator or facilitator. ||\n|-\n| Scale || The unit of analysis, usually geographical region for spatial scale, time period for temporal scale and institutional level for organisational scale. || [[Design_Criteria_of_Methods|1]], [[Methods|2]]\n|-\n| Scientist || Someone who has gone through a scientific education. ||\n|-\n| Scientific Method || Scientific methods create knowledge in accordance with certain principles and rigour. || [[Design_Criteria_of_Methods|1]], [[Methods|2]]\n|-\n| Scoping || The process of identifying all aspects of a problem that are important, including discipline experts and stakeholders who should be involved in developing understanding and action. This is followed by a process of boundary setting, ie setting priorities for the approach that will be taken. ||\n|-\n| Sense-making || An on-going process of refinement of plausible understandings and effective actions in situations of high complexity and uncertainty. ||\n|-\n| Storytelling || A social and cultural activity for sharing and interpreting knowledge and experiences, and for education. || [[Narrative Research|1]]\n|-\n| System || Any number of individuals or elements that interact. || [[Agency, Complexity and Emergence|1]], [[System Thinking & Causal Loop Diagrams|2]], [[System Boundaries|3]]\n|-\n| System Dynamics || Focuses on the interactions and dynamic relationships between system elements, with feedback as the central concept. System dynamics are often modelled, e.g. with Causal-Loop Diagrams, which enables the researcher to observe and measure the behavior of the system. || [[System Thinking & Causal Loop Diagrams|1]]\n|-\n| Tacit knowledge || Tacit knowledge or \u2018unknown knowns\u2019 is knowledge that individuals, groups and organisations are largely unaware that they have. ||\n|-\n| Theory || A systematic ideational structure of broad scope, conceived by the human imagination, that encompasses a family of empirical (experiential) laws regarding regularities existing in objects and events, both observed and posited. || [[Levels of Theory|1]]\n|-\n| Three types of knowledge (System Knowledge, Target Knowledge, Transformation Knowledge) || Three types of knowledge that are relevant to provide solutions to a problem, and foster change. As defined by Brandt et al. (2013), System knowledge refers to the observation of the context of a given system and interpretation of the underlining drivers and buffers that causes and determine the extent of change. Target knowledge refers to the scope of action and problem-solving measures given by the natural constraints, social laws, norms and values within the system, and the interests of actors and their individual intentions. Transformation knowledge refers to the practical implications that can be derived from target knowledge to change existing habits, practices and institutional objectives. Typically conceptualized in Sustainability Science, famously proposed by ProClim (1997). ||\n|-\n| Tipping Points || Thresholds that, when exceeded, lead to large irreversible changes in systems. ||\n|-\n| Toolkits || Collections of resources for undertaking various aspects of research integration and implementation. They are often, but not always, collections of methods and processes. || [[Skills & Tools|1]]\n|-\n| Transdisciplinarity || Transdisciplinarity is a mode of research that is based around the understanding that certain types of problems cannot be defined from a single discipline's perspective. Instead, Transdisciplinarity aims to already integrate different types of knowledge, both academic and non-academic, in the problem definition phase. These jointly defined problems are then addressed by integrating knowledge, often with the goal to develop solution strategies to these problems.  || [[Transdisciplinarity|1]]\n|-\n| Trust || To have confidence in attributes such as the integrity, ability and reliability of someone (e.g. other researchers) or something (e.g. a model). ||\n|-\n| Vision || A vision provides \u201ca key reference point for developing strategies to transition from the current state to a desirable future state\u201d (Wiek & Iwaniec 2014,). A vision can take the form of qualitative or quantitative goals and targets, e.g. concerning the outcome of a research project, or societal change. || [[Visioning & Backcasting|1]]\n|-\n| Visualisation || Any technique for communicating ideas (abstract or concrete), information, situations etc through creation of some kind of image, diagram, map, animation or game. || [[Graphic Recording|1]]\n|-\n| Window of opportunity || Favourable opportunity when taking immediate action is likely to achieve a desired outcome. If the opportunity is missed, the possibility of action is lost. ||\n|}"
                    },
                    "sha1": "eid56lgdyh7hltltork3kjxvlpjb4po"
                }
            },
            {
                "title": "Graphic Recording",
                "ns": "0",
                "id": "303",
                "revision": {
                    "id": "4476",
                    "parentid": "4475",
                    "timestamp": "2021-03-17T10:21:09Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Links & Further reading */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4033",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nEver lost the golden thread in listening to a lengthy talk or very abstract discussion? <br>\n'''[[Glossary|Visualizations]] help!''' <br>\n\nGraphic Recording is the process of LIVE translating complex ideas from a spoken event into drawn words and pictures. It is used in meetings, discussions, talks and conferences to help participants engage with, comprehend and remember the contents better by making them graphically visible.\n\n[[File:Graphic Recording 2.jpg|600px|thumb|center|'''Graphic Recording helps visualise complex ideas.''' Source: [http://theasideblog.blogspot.com/2012/05/imagethink-graphic-facilitation-big.html The Aside Blog, ImageThink]]]\n\n== Goals ==\n* ENGAGE - get people to think creatively about what they hear \n* COMPREHEND - illustrate complexity and interconnections for better comprehension\n* REMEMBER - people remember visuals more easily than words\n\n[[File:Graphic Recording.jpg|600px|thumb|center|'''Graphic Recording: How To''' Source: [https://www.think-in-colour.com.au/wp-content/uploads/2012/05/00_WHat-is-Graphic-Recording_2019.jpg Think in Colour]]]\n\n== Getting started ==\n==== What you need ==== \n'''Graphic library'''<br>\nYour graphic library is your mental (and paper) collection of visuals and drawing ideas that you tap on when you do a live recording. While some visuals are easily imagined, more complex and abstract ideas need some reimagination. Get to know the key words for your recording session and find visuals beforehand. That way, you feel more confident in drawing when contents get difficult.<br>\n\n'''Recording station'''<br>\nThe graphic facilitator is usually placed somewhere to the side where she is visible to the audience but not distracting. Graphics can be drawn on boards, paper, flipchart etc. There are very good (and very expensive) special markers for graphic facilitation. Test which ones flow easily and suit your drawing style and technique.<br>\n\n'''Courage to draw''' (because everyone CAN do it!) ...<br>\nThe most important thing you need is courage to draw in front of others. The good thing is, that you don't need to be an artist, because graphic facilitation focuses on structuring content rather than artistic aesthetics. However, practice boosts confidence!<br>\n''... or you hire a professional graphic facilitator for your event''\n\n==== How to start ====\n'''Practice essentials'''<br>\nAsking Mike Rohde (2012) the Essential Eight are: lines, bullets, boxes, arrows, lettering, people, colors, and shade. \n''(insert pictures here)''<br>\n\n'''Use sketchnotes'''<br>\nTry taking notes from your classes, seminars, lectures, or use TED talks to practice your visualization skills. Sketchnotes are essentially the private form of a graphic recording in your own notebook. It's the ideal (because purposeful) way to experiment and build your graphic library.\n\n== Links & Further reading ==\n* Learning Graphic Facilitation part I: https://www.youtube.com/watch?v=S5DJC6LaOCI<br>\n* Learning Graphic Facilitation part II: https://www.youtube.com/watch?v=H0QZbwqp4lg<br>\n* Mike Rohde (2012): The Sketchnote Handbook https://rohdesign.com/<br>\n* Anna Lena Schiller (2017): Graphic Recording: Live Illustrations for Meetings, Conferences and Workshops<br>\n* Martin Haussmann (2014): UZMO: Denken mit dem Stift<br>\n* Bikablo (visual libraries) https://bikablo.com/en/\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Dagmar M\u00f6lleken."
                    },
                    "sha1": "exbdo4ym14uiz40ljxmyampfy6glmew"
                }
            },
            {
                "title": "Grounded Theory",
                "ns": "0",
                "id": "253",
                "revision": {
                    "id": "6480",
                    "parentid": "6473",
                    "timestamp": "2021-11-25T14:28:11Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17701",
                        "#text": "[[File:ConceptGroundedTheory.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Grounded Theory]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''In short:''' Grounded Theory is the inductive approach of developing concepts or theories from observed data without pre-determined assumptions, based on coding of information.\n\n== Background ==\nIn the 1960s, empirical social science was stuck between a rock and a hard place. The critique of positivism was at its peak, all the while the recognition of social science as an empirical science was dominated by a lack of suitable methodological approaches. Classical surveys and other methodological approaches were criticised by the anti-positivists, and the lack of methods led to a problematic view of on social science form other disciplines. This problem can be strongly associated to the dominating reign of [[Glossary|deductive]] approaches during this time, which was a suitable line of thinking in psychology, physics, agricultural research, biology or other fields, yet did not only pose an epistemological shortcoming, but was in addition not suitable for parts of science and knowledge production that built on inductive reasoning. '''In this context of the history and theory of science, the proposal of Grounded Theory can be seen as nothing short of a revolution.''' \n\n[[File:Grounded theory.png|400px|thumb|right|'''SCOPUS hits per year for Grounded Theory until 2019.''' Search term: 'grounded theory' in Title, Abstract, Keywords. Source: own.]]\n\nFollowing the increasing focus on the individual, Grounded Theory can be seen as a main stepping stone for a science that is interactive with non-scientists, and even can be interactive even within itself. From the perspective of philosophy of science, this is a clear commitment to the fact that there are branches of knowledge that do not build on theories, but instead derive theories from observation. This concluded a pendulum swing from Bacon and the inductive approach to the positivists and back, enabling a more pluralistic and diverse science that could approach new frontiers. What is more important, this approach paved the road for a thinking in science that was less absolute and pretending to be objective, but instead acknowledged that scientists look at parts of the picture, which are highly important, but should not be seen as objective facts.\n\n\n== What the method does ==\nGrounded Theory is a systematic approach to derive hypotheses and theories that are open and more general, starting with research questions that may lead to the development of theories, often through qualitative analysis and coding. '''Researchers utilising the Grounded Theory approach thus typically formulate broad research questions that are widely unaimed at specifics, but instead try to observe and generate new insights.''' This is often done in a systematic sense through coding of information, thereby allowing for code items that are similar or at least allow for certain similarities to be identified and to be grouped in clusters. In the next step, such clusters of code items allow for the conceptualisation or formulation of a theory. More specifically, the Grounded Theory approach involves three main steps of coding: open coding, axial coding, and selective coding (Images from [https://delvetool.com/blog/openaxialselective DELVE]).\n\n[[File:Grounded Theory - Delve 1.png|400px|frameless|right|Open Coding - Image from https://delvetool.com/blog/openaxialselective]]\n1) In the '''Open Coding''', all the available data - e.g. interview transcripts - is broken down into smaller units such as sentences. All of these data units are then labelled as individual \"codes\" that summarize the data in a more abstract way. Two data units that belong to the same topic, idea, person or place receive the same code. This coding step may be influenced by what the researcher assumes might be relevant for the data, for example based on previous knowledge or available literature, but generally, this step should be done as openly as possible so that no bias is imposed by the researcher's assumptions.\n\n[[File:Grounded Theory - Delve 2.png|400px|frameless|right|Axial Coding - from DELVE (https://delvetool.com/blog/openaxialselective)]]\n2) In '''Axial Coding''', all codes that have been created in the first step are now aggregated into overarching categories. This is done by reading the data and the codes again and again, and searching for connections between the codes, based on which they are organized by the researcher. These connections may be causal relations between codes, or additional context that one code provides for another code, and all codes are categorized accordingly. The researcher verifies the emerging categories by checking them against the data repeatedly to make sure that the categorization is true to the data. In the end, the original data has been reduced to overarching, interrelated categories, to which the original codes are now sub-categories.\n\n[[File:Grounded Theory - Delve 3.png|400px|frameless|right|Selective Coding - from DELVE (https://delvetool.com/blog/openaxialselective)]]\n3) Lastly, in '''Selective Coding''', all categories are again abstracted so that one encompassing result emerges. This result can be a new theory, or an alteration of an existing theory. In any case, all categories and codes should be combined to one final narrative that explains all data. To do so, the researcher repeatedly considers all the connections between all the categories and codes, inductively develops new conceptualizations, and deductively verifies these in view of the original data.\n\nAn important tool in these three steps are '''memos'''. Memos are small notes that the researcher writes during the coding process. They can contain information about any kinds of issues the researcher encountered with the data, or their thoughts on the coding process, and notes for results of this process. The memos are an essential source of knowledge in the iterative and abductive process of Grounded Theory, where conceptual ideas are developed and neglected again and again. By checking previously written memos during later stages of the process, the researcher makes sure not to forget thoughts that they had initially. To this end, memos can be extended through diagrams that visually help make sense of and organize the codes and categories during the coding procedure.\n\nThis inductive coding approach can be seen as methodologically diverse, as it can build and integrate both quantitative and qualitative information. '''To this end, Grounded Theory is not really a clear and rigid regime on how to approach data gathering and analysis, but closer to a mode of research.''' Grounded Theory demands openness and being undetermined from previous assumptions. Instead it puts the test subjects into the center of the initial research question, and allows for an open minded conceptualisation of the data itself. While this takes experience, as it is widely building on the coding skills and generalisation choices of the [[Glossary|researcher]], it surely proves a counterpoint to the rigid [[Glossary|hypothesis]]-driven deductive research approaches. A strong emphasis in Grounded Theory is on the documentation of the research process itself, as it is clear that the coding process is potentially rooted in preconcious recognition of the researcher.\n\n\n== Strengths & Challenges ==\nThe coding process in Grounded Theory-driven research strongly builds on the experience of the researcher, for better or worse. Very experienced researchers may look for mere confirmations of preconscious ideas and concepts, and may even [[Bias and Critical Thinking|flaw]] the coding process altogether. This criticism is also deeply rooted in the minds of many positivists, who reject inductive approaches altogether. While it should be noted that Grounded Theory is surely different from the hypothesis-driven research approaches, the emergence of Grounded Theory definitely solved a clear gap in research and science. It did not only enable open processes and theory-development with a more contextual focus, but more importantly, more diverse [[To Rule And To Measure|data formats]] were now considered relevant, as qualitative data often takes center stage in Grounded Theory. This surely unlocked new dimensions of knowledge altogether at a time when this knowledge was not only increasingly recognised, but many would agree also necessary. However, this created a pronounced challenge within social science, as it divided the community into the empiricists and the others, and faned the raging fire of the positivism dispute. The ethics of research itself were at stake for many, proving that Grounded Theory opened up another domain of knowledge, yet could not solve the overarching problem of critical rationalist, whose relationship with positivism was intertwined and questionable to many. This proves how scientific developments are not unconnected from societal developments, which in the 1960s were equally radical and fundamental. \n\nAnother source of criticism on Grounded Theory is that it is not a rigid and transparent method, but a diversity of approaches. While this plurality can be clearly seen as a strongpoint, it may make it difficult to approach this method without any experience or already known approaches. This can be seen as a tautological problem because the promised methodological plurality is nothing but micro-dogmas of individual researchers or their schools of thinking, who proclaim they know the best approach. \n\nYet another criticism could already be anticipated: We are not independent of pre-existing theories. The original postulation of Grounded Theory was well aware of that, and there was a clear recognition that we need to clearly indicate any form of [[Bias and Critical Thinking|bias]] or other influence. This, in turn, is of course hard to do if we are not aware of our own shortcomings. The positivist deducers consider this to be a severe flaw, yet I would highlight this as completely different forms of knowledge. '''The deductive and the inductive are not only different categories, but altogether different approaches to knowledge.''' Whoever aims at comparing them for a better or worse is bound to lose plurality, knowledge, and any debate about philosophy of science. \n\n\n== Normativity ==\nThe greatest achievement of Grounded Theory is the opening up of the rigid tracks of positivism to a more inductive line of thinking. Grounded Theory is surely one step towards a more open scientific world beyond the often arrogant and expectation-driven deductive approaches that are laced with theories. In terms of history of science, this is a serious development, as it opened up a necessary debate, and contributed to the building of a bridge between the positivists and [[Bias and Critical Thinking|critical theory.]] While it could not altogether close this gap between these opposed lines of thinking, it most notably is also part of a movement that opened science for more qualitative inquiry, and a deeper look into societal structures, power, [[Agency, Complexity and Emergence|agency]] and the actions of individuals embedded in society and their millieu. Consequently, the openness of the method creates challenges not only when approaching the method for the first time, which is why building on the aforementioned established sequential coding procedure can be beneficial. As much as comparability between studies building on Grounded Theory can be a problem in the eyes of conservative researcher that rely on deduction, this comparison is obviously a mistake altogether. '''Every study building on Grounded Theory should be seen as a new proposal in the canon of knowledge'''. We can be sure that the context, settings and structures of each specific case will have been taken into account. As such, Grounded Theory paved the road to a science that is more interactive with its objects of inquiry, and thus created one of the first and deepest link to society. \n\n\n== Outlook ==\nGlaser and Strauss, who originally developed the methods in the 1960s, had a clash in the 1990, with Strauss (and Juliet Corbin) creating a proposal for a more systematic approach. Glaser then even proposed not to read previous literature altogether as not to be influenced or biased by it, and called for an inductive openness and emergence. Just as any methodological approach is sooner or later bound to diverge into progressive and conservative lines of thinking, Grounded Theory now becomes nested into smaller schools of thought, where procedures, approaches and the context of theory of science differ. The future may show whether these differences can be overcome and whether this is even necessary. Critical realism may offer a solution to this end, as the difference between everything that is real, everything that actually is, and all that is empirical allows the researcher to divide knowledge production according to these three categories. This can be quite relevant, as it allows for a systematic inquiry into social structures and phenomena, where the differentiation between that which can be inquired, and that which cannot be inquired, constitutes a safe haven for an ontological objective view of the world, that is all the while subjective in terms of epistemology. This is clearly a pronounced progress in the eyes of many when it comes to positivism in comparison. '''Objective knowledge is not only overrated and problematic, but may ultimately be inappropriate and unnecessary'''. Time will tell if Grounded Theory can contribute to the contextual and inductive knowledge needed to overcome the rifts that positivism created in society and science. \n\n\n== An exemplary study ==\n[[File:Grounded Theory - exemplary study Kornilaki & Font (2019) - Title.png|600px|frameless|center|The exemplary study on Grounded Theory by Kornilaki & Font (2019)]]\nKornilaki & Font (2019) used (Straussian) Grounded Theory to identify and explain \"how socio-cultural and industrial norms influence the intentions and behavious towards sustainability of owner-managers of small tourism firms\" in Crete, Greece (p.2). Grounded Theory was chosen since the inductive approach promised novelty in the theoretical perspectives on the topic, and in order to be more flexible to adapt to the responses by the participants in the study. \n\n'''In a first step - and ongoing throughout the process -, the researchers consulted literature''' on the topic to \"enhance their theoretical sensitivity and to compare the emerging theory with other work in the field\" (p.184). '''Then, they selected 23 owner-managers from small tourism companies to engage with in an [[Open Interview]]'''. After a first broad analysis of the gathered data, key issues were identified, upon which another 16 partipants, including 14 of the first selection, were chosen for a '''second round of open interviews'''.\n\nThe interviews were recorded and transcribed, and key words or sentences were highlighted in the transcripts. Based on these key items, codes were developed openly and iteratively (open coding). The codes were then abstracted by grouping them into categories, with relationships emerging between the categories and codes (axial coding), which led to the development of hypotheses (selective coding).\n\nIn their results, the researchers claim that \"[t]hrough constant comparisons of the data and of incidents, the researchers were able to see variations in the owner-managers\u2019 behaviour depending on their circumstances, the environment, their different prioritisation of actions and strategic responses to events and problems, and the different consequences of those actions.\" (p.6). Participants were categorised based on their behavioral profiles, and the researchers were able to infer \"a nuanced account of how social influence and social norms affect human behavior in relation to sustainability actions\" (p.9).\n\nOverall, the Grounded Theory approach provided in-depth insights into the owner-managers' perceptions of their own and others' behavior, and helped the researchers identify which influences determine this behavior.\n\n\n== Key Publications ==\nCorbin, J.M. & Strauss, A. 1990. ''Grounded theory research: Procedures, canons, and evaluative criteria.'' Qualitative sociology 13(1). 3-21.\n* In this paper, Corbin & Strauss introduced the steps of open, axial and selective coding.\n\n\n== References ==\nKornilaki, M. Font, X. 2019. ''Normative influences: how socio-cultural and industrial norms influence the adoption of sustainability practices. A grounded theory of Cretan, small tourism firms.'' Journal of Environmental Management 230. 183-189.\n\nDELVE. ''How To Do Open, Axial and Selective Coding in Grounded Theory.'' Last accessed 16.08.2021. https://delvetool.com/blog/openaxialselective\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "dn0r9i4umnxndh4skumnzw4jphkceo7"
                }
            },
            {
                "title": "Group Concept Mapping",
                "ns": "0",
                "id": "842",
                "revision": {
                    "id": "6877",
                    "parentid": "6063",
                    "timestamp": "2023-01-14T13:46:20Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26115",
                        "#text": "[[File:ConceptGroupConceptMapping.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Group Concept Mapping]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br>\n\n'''In short:''' Group Concept Mapping is a participatory mixed-methods approach to generate, structure and visualise conceptual information.\n\n== Background ==\n[[File:Group Concept Mapping scopus plot.png|400px|thumb|right|'''SCOPUS hits per year for Group Concept Mapping until 2022.''' Search term: 'Group Concept Mapping' in Title, Abstract, Keywords. Source: own.]]\nThe general idea of Concept Mapping was developed by Joseph Novak and colleagues at Cornell University in the 1970s (Brown). The idea was to structure and visualize knowledge about a topic in an educational context. Concept Mapping is typically still used in education until today. '''In 1989, William M.K. Trochim, also from Cornell University, expanded on this idea and introduced Group Concept Mapping''' as a structured methodological approach (Trochim 1989). He originally presented it as a way to create conceptual frameworks for planning and evaluation purposes. At the same time, the results of a Group Concept Mapping process provide insights into specific topics that are of strong interest to scientific research, which is why the method may be seen by some researchers as both a research method and a planning and evaluation tool (cf. Dare & Nowicki 2019, p.3). Thematically, the method can find application in any field and is '''often used in health and social work''' (see for example McCaffrey et al. 2019, Trochim et al. 1994, Trochim et al. 2003)\n\n\n== What the method does ==\nGroup Concept Mapping is a '''structured, linear group-based approach of gathering conceptual elements, arranging and rating them, and creating visual representations'''. The end result is typically a map or similar form of visual representation, which displays the ideas and how they relate to each other. This visualisation, as suggested by Trochim 1989, can serve as a baseline for a subsequent planning or evaluation process. An important feature is the focus on the group of actors who create the concept map on their own, supported by the methodological process and a facilitator. The end result is based on their own contributions, represents their own understanding of the issue at hand, and can thus more easily be used by them for the following processes.\n\n=== Differences to Concept Mapping and Mindmaps ===\n[[Concept Maps]] refer to visual representations of conceptual ideas, as introduced by Novak and colleagues in the 1970s (Trochim 1989). Information (words or short sentences) is put into boxes or circles, which are connected in a network-like structure through arrows. This is in a way comparable to a mindmap. However, [[Mindmap|Mindmaps]] do not serve the purpose of presenting a final, comprehensive overview over a specific focus on a topic, but rather an unstructured, spontaneous collection of general related ideas around one key term. Both processes - Concept Maps and Mindmaps - can be done by a single person or in a group. For example, Brown (2003) shows how group-based creation of a concept map can enhance biology teaching.\n\nBy comparison, '''Group Concept Mapping is not a mere group-based creation of a Concept Map''', but an established methodological approach which includes qualitative and quantitative elements, and can be seen as a mixed methods approach in itself, with both elements of data gathering and analysis. It is a structured form of framework development, in which ideas, thoughts, goals, needs and resources relevant to a specific topic are collected, conceptually linked, quantitatively evaluated and presented in a map-like visualisation. \n\n=== The process of Group Concept Mapping ===\nGroup Concept Mapping typically consists of six linear steps (see figure): Preparation, Statement Generation, Structuring, Representation, Interpretation, Utilization. Below, we will elaborate on each step based on the original methodological design by Trochim (1989), and present the process as done in the exemplary study by McCaffrey et al. (2019), who attempted to generate a conceptual model of good health care from the perspective of patients.\n\n[[File:Group Concept Mapping - Visualisation-2.png|600px|thumb|center|'''The process of Group Concept Mapping'''. Source: adapted from Trochim 1989, p.3]]\n\n==== 1 - Preparation ====\n'''a) Selecting the participants'''<br>\nIn the preparation phase, first, '''the group of actors is gathered''' that will contribute to the process. The individuals are selected by the facilitator, who can be a member of the relevant group itself, or an outsider. The selected group, \"[d]epending on the situation, (...) might consist of the administrators, staff or members of the board of an organization; community leaders (...); academicians or members of the policy making community; (...) representatives of relevant client populations\" or other individuals that are relevant to the topic at hand, and whose perspectives should be included in the resulting framework visualisation (Trochim 1989, 2). When selecting the participants, \"[b]road heterogeneous participation helps to insure that a wide variety of viewpoints will be considered and encourages a broader range of people to \"buy into\" the conceptual framework which results\". (Trochim 1989, 3). Individuals may be selected by hand, or by the use of stratified or purposive sampling. Trochim (1989) recommends a group to include 10-20 individuals, but larger or smaller groups are still imaginable. Further, it is possible for the group size to vary in the different steps of the process, whereas continuity along all steps of the process might support a better understanding of the result for all participants (Trochim 1989).\n\n'''b) Developing the Focus for Brainstorming'''<br>\nIn the second part of the preparation, the underlying \"rules\" of the process are constituted. This involves, first, the creation of clearly defined, mutually agreed-upon '''prompts or questions which define the thematical focus of the latter brainstorming''' (in Step 2). The brainstorming might be intended to revolve around goals, outcomes, involved individuals or activities, problems, or other conceptual elements, and so the prompts or questions should target these. Whatever the group decides to focus on, the sentences or questions resulting from this first step should clearly define which kind of ideas and elements are gathered later, and every involved participant should agree on them. \n\n'''c) Developing the Focus for Rating'''<br>\nAdditionally, but not necessarily, a rating scheme can be developed. For this, the group decides on '''dimensions along which the collected ideas and elements from the brainstorming will be rated''' (in Step 3). This rating can revolve around the statements' importance, complexity, likeliness or such, and the group decides which kind of scale will be used to rate these elements. (Trochim 1989). For example, if we take the question of reasons for students to enroll, it could be reasonable to rate these reasons according to their importance on a 1-5 Likert Scale. As a second example, it could be interesting to rate how easy or likely the problems of the youth in the neighborhood might be to solve.\n\n'''Our example'''<br>\nIn our exemplary study, McCaffrey et al. developed three focus prompts based on previous insights from Interviews:\n# Please give us statements that describe 'good health care'.\n# What does 'good health care' look like?\n# Think about the health care that you have received. What aspects of your care have you liked?\n\nAs a rating scheme, the researchers later asked participants to rate the items generated as answers to these prompts by importance.\n\nIn terms of partipants, the researchers recruited individuals from two groups through an online health research network of 600,000 members: 157 patients with six different health conditions, as well as 17 health stakeholders (six patient representatives, six health providers, one researcher, two purchaser groups, two individuals from measure development).\n\n\n==== 2 - Generation of statements ====\nNow, the '''recruited group brainstorms all elements (= \"statements\") that come to mind in accordance with the pre-defined brainstorming focus''' prompt or question. So if the group defined to focus on goals of their organization, they should collect whichever potential, desirable, or current goals come to mind. None of these ideas are rated or discussed at this point. The process can be done anonymously if necessary, digitally as well as analogously. Trochim (1989) recommends to limit the brainstorming outcome to ~100 terms. If more terms arise, it might be necessary to reduce this number by deleting synonyms, or (jointly) grouping terms. Generally, the responses should be edited for clarity, and irrelevant responses should be deleted (Dare & Nowicki 2019). In the end, all participants should be informed and on the same page regarding all mentioned items. It is worthwhile mentioning that sometimes, the brainstorming phase is not necessary, and the elements can be derived from the prevalent organizational structure, existing documents, or other sources (Trochim 1989).\n\n'''Our example'''<br>\nMcCaffrey et al. provided the selected individuals digitally with the three focus prompts, and obtained 1564 statements in response. Further, they conducted a literature review on patient health care priorities, from which they extracted further 146 statements; and further 69 statements from Interviews that were done in preparation to their study. The researchers reduced this pool of almost 2000 statements through an iterative coding and cleaning process to a final selection of 79 statements.\n\n\n==== 3 - Structuring of Statements ====\n'''a) Sorting of Statements'''<br>\nThe generated '''statements are now structured in relation to each other'''. In the original proposition by Trochim (1989), this is done by writing down each statement individually onto cards, and providing these cards to each participant. They are then asked to group statements onto piles as they consider reasonable. After everyone did so, a matrix is created in which for every combination of two statements, a \"1\" indicates that these two statements were grouped together by a participant, while a \"0\" indicates that they were not (see figure). Next, all matrices are combined into one so that the number in each field represents how many participants combined a given pair of statements. This way, it can be identified which similitude or relationship the group assigns to the gathered statements. Today, this process can still be done manually, but is more commonly conducted digitally, for example with the groupwisdom\u2122 software (see Further Links)\n\n[[File:Group Concept Mapping - Visualisation.png|700px|thumb|center|'''The procedure for computing the binary, symmetric similarity matrix for one person from the card sort.''' Source: adapted from Trochim 1989, p.7.]]\n\n'''b) Rating of Statements'''<br>\nSecondly, '''every participant is asked to rate each statement on the pre-defined rating dimensions'''. This is typically be done using a 1-5 or 1-7 Likert Scale. The means are calculated and assigned to the statements.\n\n'''Our example'''<br>\nMcCaffrey et al. recruited new participants for this step in accordance with the diversity criteria for Step 1, including members of the previously used health network, local community members, and health stakeholders. Overall, they obtained 123, 27 and 15 complete sorting and rating results, respectively. They used the CSGlobal MAX software, which is a precursor to groupwisdom\u2122, and asked their participants to rate the 79 previously gathered items on a scale from 1 (Not Important) to 5 (Extremely Important).\n\n\n==== 4 - Representation of Statements ====\nAt this point, the group has created a set of max. 100 statements which describe a specific topic (e.g. goals of their organization), and a matrix exists that highlights both the relationship between these statements and each statement's rating in regard to a specific rating focus. \n\n\n'''Point Map'''<br>\nNow, '''each statement is placed on a blank, typically two-dimensional map''', with more closely related statements being placed more closely to each other. This is done using multidimensional scaling (MDS), a multivariate statistical approach, which accomplishes the complex task of placing all data points in the relative distance to all other data points, as defined by the values of relatedness indicated in the matrix. \n\n[[File:Group Concept Mapping Point Map.png|500px|frameless|center|]]\n'''An exemplary Point Map of 95 statements from Trochim 1989, p.11.''' Each point represents a statement that was gathered by the group, and is placed in a way that represents relatedness to the other statements.\n\n\n'''Cluster Map'''<br>\nIn a second step, '''all statements (= data points) are grouped into clusters''' \"which represent higher order conceptual groupings of the original set of statements.\" (Trochim 1989). This is done - as proposed by Trochim (1989) - by using the X-Y coordinates of each statement after the multidimensional scaling as input for a Ward's hierarchical cluster analysis (HCA), which \"partitions the multidimensional scaling map into any number of clusters\" (Trochim 1989, 8). The analyst has to decide how many clusters the statements should be grouped into, based on what makes sense with regards to the topic and statements. You will find more on cluster analysis in [[Clustering Methods|this entry]].\n\nConcerning the software for the conduction of this clustering, Dare & Nowicki (2019, p.9) highlight that multidimensional scaling as well as hierarchical cluster analysis can be performed using statistical software such as R, SAS, or SPSS, or with the aforementioned groupwisdom\u00a9 software.\n\n[[File:Group Concept Mapping Cluster Map.png|400px|frameless|center|]]\n'''The Cluster Map, which puts the statements (data points) into groups by means of Cluster Analysis.''' The visual representation of these clusters is in shapes instead of points. The amount of clusters is adjustable based on what makes sense for the data. Each cluster will be given a sensible name by the group (Step 5). Source: adapted from Trochim 1989, p.12\n\n\n'''Rating Maps'''<br>\nAt the end of this step, the group has a (two-dimensional) map that includes all statements as data points, placed according to their relatedness (the \"Point Map\"); and one map that also groups these data points into clusters (the \"Cluster Map\"). Two more maps can be created based on these. The ratings that were assigned to each statement in the matrix in Step 3 are placed in the respective position on the map, resulting in the \"Point Rating Map\". The same is done for each cluster, with the Point Ratings being averaged within each cluster.\n\n[[File:Group Concept Mapping Point Rating Map.png|400px|frameless|center|]]\n'''The respective Point Rating Map.''' For each data point, the mean rating as assigned by the group in Phase 3 is indicated, based on a 1-5 Likert Scale in this case. Source: Trochim 1989, p.13\n\n[[File:Group Concept Mapping Cluster Rating Map.png|400px|frameless|center|]]\n'''And finally, the (still unlabeled) Cluster Rating Map.''' Here, the point ratings of all data points (on a 1-5 Likert Scale) within each cluster are averaged, leading to a rating for each cluster, indicated by the \"levels\". With this map at hand, it is easy to see which groups of ideas, problems, goals, or individuals are most \"important\", \"difficult\", or \"likely\" for the group, which is of great help for their planning or evaluation process. Source: adapted from Trochim 1989, p.14\n\n'''Our example'''<br>\nMcCaffrey et al. first considered the three participant groups' responses individually to see if there were diverging conceptual understandings of what constitutes good health care, but eventually combined their results into one conceptual model. They created a point maps, and evaluated different numbers of clusters before eventually deciding on a 10-cluster result, with which they developed a cluster map. Each cluster represented a different aspect of good health care. They calculated ratings for all data points and clusters and created point and cluster rating maps, where a higher rating equals a more important aspect of good health care. They found that there are only small differences in their highest and lowest rated clusters, which they stated highlights that the initial selection of statements already included very important aspects of good health care. They further investigated if there were significant differences between different participant groups' clustering results, which they did not find.\n\n[[File:Group Concept Mapping - Cluster Rating Map - Example.png|700px|thumb|center|'''The resulting (yet unlabeled) Cluster Rating Map for McCaffrey et al. 2019''' (p.89).]]\n\n\n==== 5 - Interpretation of Maps ====\nNow, '''the group is asked to assign names to the clusters'''. Each participants looks at each cluster and the statements included, and suggests a name (e.g. a phrase, or a word) to describe the cluster, and the group negotiates until consensus is reached for each cluster. If there are a lot of clusters, it may be sensible to further develop names for groups of clusters - \"regions\" - but this depends on the map at hand. In any case, the names of the clusters should represent the statements included as well as the conceptual relation to other clusters which are close. This labeled Cluster Map is the main outcome of the Group Concept Mapping process. It can be re-arranged by the group if necessary, since they should feel comfortable with the conceptual framework it represents.\n\n'''In the end of the process, the group has the following results:'''\n* a statement list\n* a point map\n* a point rating map\n* a cluster list (listing all labeled clusters including the respective statements)\n* a labeled cluster map, \n* and a labeled cluster rating map.\n\n'''Our example'''<br>\nMcCaffrey et al. named the clusters themselves, based on \"cluster names provided by participants whose\nsorting produced results similar to the final cluster content, and (2) by reviewing statements within each cluster\" (p.89).\n\n[[File:Group Concept Mapping - Cluster List (first half).png|600px|frameless|center]]\n[[File:Group Concept Mapping - Cluster List (second half).png|600px|frameless|center]]\n\nThe final list of clusters in McCaffrey et al. (2019, p.90f) The clusters (left) are presented in order of importance (right), with a description and exemplary statements for each cluster in the center.\n\n\n==== 6 - Utilization of Maps ====\nThe group is now done with the Group Concept Mapping process, and can use either of the maps (preferable the Cluster Map, or Cluster Rating Map) as a baseline for their further work in many diverse ways. '''The maps show the most important elements they need to pay attention to,''' which can be used to coordinate future actions, prioritize tasks, and structure the process. The clusters can serve as the organizational foundation, or as groups of topics to work on, either when implementing measures, or developing an evaluation scheme.\n\n'''Our example'''<br>\nFor example, the results of the study by McCaffrey et al. (2019) may be of value for health care providers to evaluate and improve their services, and for health researchers to identify relevant aspects for further investigation.\n\n\n== Strengths & Challenges ==\n* Group Concept Mapping is a systematic and highly structured process with clear procedural steps, that helps a diverse group of people gather and structure their thoughts into a coherent and consensual set of maps.\n* '''The process of Group Concept Mapping is empowering''': all content that is included in, and leads to the final maps is created by the group itself, in their own language and based on their own perspectives. The participants will feel more ownership for the conceptual framework that results from the process, and the framework is more likely to be actively used by all involved actors than a framework that is imposed without involvement.\n* The end result of the process is a visual representation which introduces all important ideas at a glance in a structured manner. '''The maps can be easily communicated''' and presented without any knowledge about the methodological process.\n* Group Concept Mapping is versatile in that it can work with all kinds of statements, gathered from workshop sessions as presented above, or from documents, organisational structures etc. As Jackson and Trochim (2002) present, it can also be a useful approach to analyze open-ended survey responses, which are transformed into single statements and sorted by the researchers. Then, they can be analyzed using the multidimensional scaling and cluster analysis steps as presented above.\n* A challenge lies in the organisation of the process. For example, the number and selection of participants will influence the outcome and must therefore be done carefully. Further, the number of statements that are gathered, the way they are reduced if necessary, as well as the number of clusters are in the hands of the researchers. These decisions shape the end results and require thoughtful consideration of the topic and data at hand.\n\n\n== Normativity ==\n* Group Concept Mapping may be of interest in application-oriented research and transdisciplinary research. It allows for non-scientific actors to contribute their perspectives, actively engage with an issue and (research) question, and it empowers them to approach the problem with the self-developed conceptual framework.\n* The method is an interesting mix of qualitative research, akin to workshop-based or interview-like research approaches; and quantitative multivariate statistical analysis. Therefore, '''this single methodological process is a mixed methods approach in itsel'''f, highlighting the power of a sensible combination of diverse methods.\n* To assess the reliability and consistency of the process, Trochim et al. (1994) suggest using the contingency coefficient for all pairs of sorts from the matrix (Step 3). This way, it can be analyzed how the individual participants' sorts are interrelated, and if they sorted consistently. They further propose to split the group and correlate the resulting matrices and the results of the multi-dimensional scaling.\n* As McCaffrey et al. (2019) highlight, the results of Group Concept Mapping are mostly based on self-reported perception, which might differ from external analyses of the issue at hand. In their case, it was insightful to learn about patients' perspective on good health care, but medical reports offer another valuable perspective that should be taken into account.\n\n\n== Outlook ==\nMethodologically, Group Concept Mapping has been established for decades, and although elements of the process have found re-iterations and diversification, the general process seems unlikely to change drastically. However, as more transdisciplinary and practice-oriented research, as well as mixed methods approaches, take place, one can assume that Group Concept Mapping may find more diverse and frequent application in different fields of research in the future.\n\n\n== Key Publications ==\nTrochim, W.M.K. 1989. ''AN INTRODUCTION TO CONCEPT MAPPING FOR PLANNING AND EVALUATION.'' Evaluation and Program Planning 12. 1-16.\n\nKane, M., & Trochim, W. M. K. (2007). ''Concept mapping for planning and evaluation.'' Sage Publications, Inc.\n\n\n== References ==\n(1) Trochim, W.M.K. 1989. ''AN INTRODUCTION TO CONCEPT MAPPING FOR PLANNING AND EVALUATION.'' Evaluation and Program Planning 12. 1-16.\n\n(2) McCaffrey, S.A. Chiauzzi, E. Chan, C. Hoole, M. 2019. ''Understanding 'Good Health care' from the Patient's Perspective: Development of a Conceptual Model Using Group Concept Mapping.'' The Patient - Patient-Centered Outcomes Research 12. 83-95.\n\n(3) Brown, D.S. 2003. ''High School Biology: A Group Approach to Concept Mapping.'' The American Biology Teacher 65.3. 192-197.\n\n(4) Jackson, K.M. Trochim, W.M.K. 2002. ''Concept Mapping as an Alternative Approach for the Analysis of Open-Ended Survey Responses.'' Organizational Research Methods 5(4). 307-336.\n\n(5) Dare, L. Nowicki, E. 2019. ''Engaging Children and Youth in Research and Evaluation using Group Concept Mapping.'' Evaluation and Program Planning 76.\n\n(6) Trochim, W.M.K. Cook, J.A. Setze, R.J. 1994. ''Using Concept Mapping to Develop a Conceptual Framework of Staff's Views of a Supported Employment Program for Individuals With Severe Mental Illness.'' Journal of Consulting and Clinical Psychology 62(4). 766-775.\n\n(7) Trochim, W.M.K. Milsein, B. Wood, B.J. Jackson, S. Pressler, V. 2003. ''Setting Objectives for Community and Systems Change: An Application of Concept Mapping for Planning a Statewide Health Improvement Initiative.'' Health Promotion Practice. 1-12.\n\n\n== Further Links ==\n* [http://www.billtrochim.net/mapping/mapping.htm billtrochim.net] - Bill Trochim's own overview website on the method.\n* groupwisdom\u2122, which is an official software for the conduction of Group Concept Mapping processes [https://groupwisdom.com/buy](https://groupwisdom.com/buy)\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "87gxaunccr853cmj9l0zyh0r5he88ls"
                }
            },
            {
                "title": "Handling Missing Values in Python",
                "ns": "0",
                "id": "1064",
                "revision": {
                    "id": "7269",
                    "parentid": "7135",
                    "timestamp": "2023-07-04T15:21:21Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an overview over the different kinds of missing values, their distribution, and how to handle them.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6640",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Missing values: types and distributions==\n\nMissing values are a common problem in many datasets. They can occur for a variety of reasons, such as data not being collected or recorded accurately, data being excluded because it was deemed irrelevant, or respondents being unable or unwilling to provide answers to certain questions (Tsikriktsis 2005, 54-55).\n\nIn this text, we will explore the different types of missing values and their distributions and discuss the implications for data analysis.\n\n==Types of missing values==\n\nThere are two main types of missing values: unit nonresponse and item nonresponse missing values. Item nonresponse occurs when an individual respondent is unable to provide an answer to a specific question on a survey or questionnaire (Schafer and Graham 2002, 149).\n\nUnit nonresponse occurs when an entire unit, such as a household or business, is unable to provide answers to a survey or questionnaire (ibid.).\n\nNext, we will look at how missing values can be distributed and what the implications of such distributions are. Generally, both types of missing values can occur in any distribution.\n\n==Distributions of missing values==\n\nThe distribution of missing values in a dataset can be either random or non-random. This can have a significant impact on the analysis and conclusions drawn from the data. Three common distributions of missing values are missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) (Tsikriktsis 2005, 55).\n\n===Missing completely at random===\nMissing completely at random (MCAR) is a type of missing data where the missing values are not related to any other variables in the dataset, and they do not follow any particular pattern or trend. In other words, the missing values are completely random and do not contain any necessary information (Tsikriktsis 2005, 55).\n\nThe implications of MCAR for data analysis are relatively straightforward. Because the missing values are completely random, they do not introduce any bias into the analysis. Therefore, it is generally safe to impute the missing values using statistical methods, such as mean imputation or multiple imputations. However, even if the missing values are MCAR, there may still be other factors that can affect the analysis. It is important to consider the number and proportion of missing values (Scheffer 2002, 156). The larger the proportion of missing values in your overall dataset the less reliable is the use of the data. Imagine you had many unit nonresponse missing values across many different individuals, which results in having no variable without any missing value. This might affect the quality of your dataset. If and how this is the case, needs to be decided case by case.\n\n===Missing at random===\nMissing at random (MAR) is a type of missing data where the missing values are not related to the missing values themselves, but they might be to other variables in the dataset. In other words, the missing values are not completely random, but they are not systematically related to the true value of the missing values either (Tsikriktsis 2005, 55). For example, imagine you conduct a survey to analyze the relationship between education and income and there are missing values concerning income. If the missing values depend on education, then these missing values are missing at random. If they would depend on their actual income, they would not.\n\nThe implications of MAR for data analysis are more complex than those for MCAR. Because the missing values are not completely random, they may introduce bias into the analysis if they are not properly accounted for. Therefore, it is important to carefully consider the underlying reasons for the missing data and take these into account when imputing the missing values. One common approach to dealing with MAR missing values is to use regression or other statistical methods to model the relationship between the missing values and the other variables in the dataset (Tsikriktsis 2005, 56). Once the relationship is clear, other methods can be used to approximate to correct the variables for the bias due to the missing values missing at random.\n\n===Missing not at random===\nMissing not at random (MNAR) is a type of missing data that is related to both the observed and unobserved data. This means that the missing data are not random and are instead influenced by some underlying factor. This can lead to biased results if the missing data are not properly accounted for in the analysis (Tsikriktsis 2005, 55).\n\nThe implications of MNAR for data analysis are more complex than those for MCAR or MAR. Because the missing values are systematically related to the true values of the missing data, they can introduce bias into the analysis if they are not properly accounted for. In some cases, this bias may be difficult or impossible to correct, even with advanced statistical methods (Tsikriktsis 2005, 55).\n\n==Determining the randomness of missing data==\n\nThere are two common methods to determine the randomness of missing data. The first method involves forming two groups: one with missing data for a single variable and one with valid values for that variable. If significant differences are found between the two groups regarding their relationship to other variables of interest, it may indicate a non-random missing data process. The second method involves assessing the correlation of missing data for any pair of variables. If low correlations are found between pairs of variables, it may indicate complete randomness in the missing data (MCAR). However, if significant correlations are found between some pairs of variables, it may be necessary to assume that the data are only missing at random (MAR) (Tsikriktsis 2005, 55 - 56).\n\nOverall, the treatment of missing values should be tailored to the specific distribution of missing values in the dataset. It is important to carefully consider the underlying reasons for the missing data and take appropriate steps to address them in order to ensure the accuracy and reliability of the analysis.\n\n==References==\n\nSchafer, Joseph L., and John W. Graham. \"Missing data: our view of the state of the art.\" Psychological methods 7, no. 2 (2002): 147.\n\nScheffer, Judi. \"Dealing with missing data.\" (2002).\n\nTsikriktsis, Nikos. \"A review of techniques for treating missing data in OM survey research.\" Journal of operations management 24, no. 1 (2005): 53-62.\n\n\nThe [[Table of Contributors|author]] of this entry is Finja Schneider. Edited by Milan Maushart.\n[[Category:Statistics]]\n[[Category:Python basics]]"
                    },
                    "sha1": "h1oshrq9v63k61cplu45hci86vghexo"
                }
            },
            {
                "title": "Heatmap",
                "ns": "0",
                "id": "927",
                "revision": {
                    "id": "6537",
                    "parentid": "6534",
                    "timestamp": "2022-03-04T19:47:12Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7898",
                        "#text": "'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.\n\n==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nIt also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.\n\n==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=\"R\" inline>heatmap()</syntaxhighlight> function and '''mtcars''' dataset.\nIt is important to note that the <syntaxhighlight lang=\"R\" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=\"R\" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=\"R\" line>\ndata(\"mtcars\")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.\n\n<syntaxhighlight lang=\"R\" line>\nfullcolnames <- c(\"Miles per Gallon\", \"Number of Cylinders\",\n                  \"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                  \"Weight\", \"1/4 Mile Time\", \"Engine\", \"Transmission\",\n                  \"Number of Gears\", \"Number of Carburetors\")\n</syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = \"column\", labCol = fullcolnames, \n        margins = c(11,5))\n</syntaxhighlight>\n\n== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let\u2019s look at the feature <syntaxhighlight lang=\"R\" inline>\u201cNumber of Carburetors\u201d</syntaxhighlight>. We can see that '''Maserati Bora''' has the darkest color, hence it has the largest number of carburetors, followed by '''Ferrari Dino''', which has the second-largest number of carburetors. While other models such as '''Fiat X1-9''' or '''Toyota''' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.\n\n==Explanation of used arguments==\n* <syntaxhighlight lang=\"R\" inline>Colv = NA</syntaxhighlight> and <syntaxhighlight lang=\"R\" inline>Rowv = NA</syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.\n* <syntaxhighlight lang=\"R\" inline>scale = \u201ccolumn\u201d</syntaxhighlight> is used to normalize the columns of the matrix (to absorb the variation between columns). As it was stated previously, normalization is needed due to the algorithm by which the colors are set. Here in our dataset, the values of features \u201cGross horsepower\u201d and \u201cDisplacement\u201d are much larger than the rest. Therefore, without normalization, these two columns will be all marked approximately equally high and all the other columns equally low. Normalizing means that we keep the relative values in each column but not the real numbers. In the interpretation sense it means that, for example, the same color of features \u201cMiles per Gallon\u201d and \u201cNumber of Cylinders\u201d of Mazda RX4 does not mean that the actual values are the same or approximately the same (placed in the same bin). It only means that the relative values of each of these cells in corresponding columns are the same or are in the same bin.\n* <syntaxhighlight lang=\"R\" inline>margins</syntaxhighlight> is used to fit the columns and rows names into the graph. The reason we used it here is because of the renaming of the columns, which is resulted in longer names that did not fit well by themselves.\n\nColoring options for the heatmap\nThe choice of color for the heatmap is one of the most important aspects of creating an understandable and nice-looking representation of the data. If you do not specify the color (as in the example above) then the default color palette will be applied. However, you can use the argument <syntaxhighlight lang=\"R\" inline>col</syntaxhighlight> and choose from a wide variety of palettes for coloring your heatmap.\n\nThere are two options of setting a color palette for the heatmap:\n* First option is to use the palettes from R: <syntaxhighlight lang=\"R\" inline>cm.colors()</syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>heat.colors()</syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>rainbow()</syntaxhighlight>, <syntaxhighlight lang=\"R\" inline>terrain.color()</syntaxhighlight>  or <syntaxhighlight lang=\"R\" inline>topo.colors()</syntaxhighlight> \n* The second option is to install color palettes packages such as <syntaxhighlight lang=\"R\" inline>RColorBrewer</syntaxhighlight> \n\n==Additional materials==\n* [https://www.r-graph-gallery.com/heatmap Other functions for building a heatmap]\n* [https://www.datanovia.com/en/blog/how-to-normalize-and-standardize-data-in-r-for-great-heatmap-visualization/ How and why we should normalize data for a heatmap]\n* [https://vwo.com/blog/heatmap-colors/ How to choose the color palette for a heatmap]\n* [https://blog.bioturing.com/2018/09/24/heatmap-color-scale/ Do's and Dont's in choosing a color palette for a heatmap]\n* [https://www.displayr.com/what-is-dendrogram/ What is a dendrogram]\n* [https://sustainabilitymethods.org/index.php/Clustering_Methods More about clustering methods and how to build a dendrogram in R]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Evgeniya Chetneva."
                    },
                    "sha1": "f4bpsvnx6ny4ebig686zeltkrqxpsod"
                }
            },
            {
                "title": "Hermeneutics",
                "ns": "0",
                "id": "359",
                "revision": {
                    "id": "5140",
                    "parentid": "3527",
                    "timestamp": "2021-04-26T08:22:25Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Objective Hermeneutics */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16050",
                        "#text": "[[File:ConceptHermeneutics.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Hermeneutics]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''Annotation:''' Hermeneutics describes two things: first, a scientific method that will be described in this entry. Second, a branch of philosophy that revolves around the ontological question of how to understand not only texts and symbols, but life in general. It is a field that deals with the characteristics of knowledge. For more information see sources (1) and (2) in the References.\n\n'''In short:''' As a scientific method, Hermeneutics describes the process of analyzing and interpreting written text and imagery to understand the text and the creator's underlying intentions.\n\n== Background ==\n[[File:Hermeneutics.png|400px|thumb|right|'''SCOPUS hits per year for Hermeneutics until 2019.''' Search term: 'hermeneutics' in Title, Abstract, Keywords. Source: own.]]\nThe roots of Hermeneutics reach back to antiquity. The word 'Hermeneutics' derives from the Greek word ''hermeneuein'', which basically means \u201cto explain\u201d. The messenger-god Hermes brought messages from the gods to the humans and explained their meaning to them (4). There has been a highly developed practice of interpretation in Greek antiquity, aiming at diverse interpretanda like oracles, dreams, myths, philosophical and poetical works, but also laws and contracts. The beginning of ancient Hermeneutics as a more systematic activity, however, goes back to the exegesis of the Homeric epics. \n\nHermeneutics was further developed in the Middle Ages: \"In its earliest modern forms, hermeneutics developed primarily as a discipline for the analysis of biblical texts. It represented a body of accepted principles and practices for interpreting an author's intended (and inspired) meaning, as well as providing the proper means of investigating a text's socio-historical context.\" (5). Then, \"Hermeneutics took on a new and widening importance in the early modern period with the application of philological techniques for the validation and authentication of classical texts. With the Reformation, hermeneutic approaches were further deployed in intense exegetial exercises devoted to the resolution of contested meanings in scriptural texts.\" (4) The ambition of early modern hermeneuts was to assign correct meanings to a text and generate 'truth'. These aspects remained the focus of Hermeneutics until the period of Enlightenment.\n\nDuring the last decades and centuries, Hermeneutics has emerged into a field of philosophy, whereas its origins lie rather in a methodological approach. \"It has recently emerged as a central topic in the philosophy of the social sciences, the philosophy of art and language and in literary criticism - even though its modern origin points back to the early ninteenth century.\" (2) As a method, Hermeneutics is nowadays mostly used in the Social Sciences and Humanities, including theology, law, psychology, philosophy and history, with diverging methodological characteristics.\n\n=== Key Figures ===\n'''Schleiermacher''', Friedrich Daniel Ernst (1768-1834): After finishing his studies in theology, philosophy and philology, Schleiermacher worked as a private teacher, preacher, and later professor for theology. In Berlin he became friends with Friedrich Schlegel, he joined the academy of sciences and became its secretary of philosophy.  Schleiermacher can be seen as the founder of methodological \u2013 also called systematic \u2013 Hermeneutics, the new branch that dropped the traditional view of texts as keepers and producers of truth. He instead highlighted the importance of differentiating between grammatical and psychological interpretation.<br>\n  \n'''Dilthey''', Wilhelm (1833-1911): Dilthey lived from 1833 to 1911 and was a German philosopher. He took up the theory of the hermeneutic circle which was first mentioned by Schleiermacher and Friedrich Ast: Each individual part is revealed through the whole, and the whole through the individual. This points out that every fact, observation, or statement is always already connected to certain preconceptions. In Dilthey\u2019s opinion this was true not only for humanities but also for the theories of natural sciences. Dilthey influenced many other famous philosophers such as Husserl, Heidegger, or Cassirer.<br>\n\n'''Heidegger''', Martin (1889-1976): For Martin Heidegger, a German philosopher, hermeneutics is the existential basis of our human experience - according to him, being is itself shaped by understanding and interpretation. His philosophical concept of hermeneutics was and is important for the following theories and analyses.<br>\n \n'''Gadamer''', Hans-Georg: Gadamer (1900-2002) was one of the most influential German philosophers of the 20th century. Highly influenced by Martin Heidegger, he turned away from Schleiermacher's and Dilthey's methodological hermeneutics. Instead, he had a universal approach to the topic of understanding meaning. Hermeneutics for him is not only a method but the basis of human existence. In his main work \u2013 Wahrheit und Methode (truth and method) \u2013 Gadamer points out that to interpret, one needs to be open, willing to reflect, and conscious of one's own prejudices and presumptions.<br>\n \n'''Habermas''', J\u00fcrgen and '''Oevermann''', Ulrich: The German philosopher and sociologist Habermas criticizes Gadamer\u2019s universial view on hermeneutics. Oevermann, who is a German sociologist, used to be an assistant of Habermas at the Frankfurt School. He coined the term Objective Hermeneutics, a method that is predominantly used in social sciences and sociology.\n\n\n== What the method does ==\n\nHermeneutics as a method \"(...) offers a toolbox for efficiently treating problems of the interpretation of human actions, texts and other meaningful material.\" (1). It helps with making sense of the relationship between an author, his text, and the reader. Depending on the various theorists, the major goal of interpretation is either decoding and understanding the author's intention at the time of writing, understanding a text only through itself, or through the social and cultural context (1). Generally speaking, Hermeneutics aims at having a clear distinction between interpretation as an activity directed at the meaning of a text and textual criticism as an activity that is concerned with the significance of a text with respect to different values (1) (Meaning is what is represented by a text or what the author meant by his use of a particular sign sequence. Significance names a relationship between that meaning and a person, a conception, or situation.)\n\nAs the information on key figures show, there are many different approaches to using Hermeneutics as a method. However, here are some steps and principles you can follow if you choose a hermeneutic strategy to deal with texts. None of them are wrong, they just serve different purposes and are suitable for different questions, disciplines, or media:\n\n==== The Hermeneutic Process ====\nThese three steps are a basic guideline of the hermeneutic process:<br>\n\n#  ''elocutio'' (or understanding): the studying of the object of interpretation itself, e.g. the pre-given text\n#  ''interpretatio'' (or interpretation): the act of interpreting, analysing and understanding the given object\n#  ''explicatio'' (or explanation/judgment): the result of the interpretation or the text written by the interpretor as a result of the interpretation\n\nAccording to Schleiermacher, a given text should be seen in perspective with the author's internal world. This method has been criticized and altered by later philosophers like Gadamer, who does not regard the author's intention to be a critical point in the interpretation process. However, the principle of psychological interpretation is still a very important step in historical disciplines for example, where understanding the author's/creator's intention is crucial to evaluate the significance of a source or object. Consequently, one may distinguish between two steps in the hermeneutic interpretative process:\n\n# Grammatical interpretation (objective): This is the first step of interpretation according to Schleiermacher. Grammatical interpretation is all about understanding the relations between words and the sentences they appear in, sentences and paragraphs, paragraphs and chapters and finally their relation to the whole text. This is important, because \"(...) the meaning of a complex expression is supposed to be fully determined by its structure and the meanings of its constituents\". (1)\n# Psychological interpretation (subjective): The second step asks you to interpret the text as a part of the author's internal world or soul. it reveals the author's subjective intention as well as his thoughts and feelings towards the subject. The historical, societal, cultural, political and religious background of an author influences the meaning of his work.\n\n==== Objective Hermeneutics ====\n\nThe qualitative empirical research method of [https://www.objective-hermeneutics.com/ ''Objective Hermeneutics''] was introduced by Ulrich Oevermann. It is currently one the most prominent approaches in qualitative research in German-speaking countries (6). Objective hermeneutics, in contrast to conventional hermeneutics, tries to work out not only the psychologically unconscious, but above all the socially unconscious in language. In this context Oevermann speaks of \"latent social structures of meaning\". In the objective-hermeneutic interpretation the interpreter compares the extent to which his own expectations of a linguistic interaction, based on everyday [[Glossary|communication]] structures, apply or differ from those presented in the text or interview.\n|}\n<br>\nHere are five of the core principles or rules of Objective Hermeneutics (7):<br>\n  \n* Context-freedom: The initial interpretation of a text should not be influenced by its context. The interpretor should create his own contextual image based on the text only and can compare this image with the context afterwards and then reevaluate it. \n* Literality: The text speaks for itself and should not be judged at first (e.g. do not judge spelling or grammar mistakes as 'wrong' - they might have been made consciously to convey a specific message).\n* Sequentiality: A text should be analysed chronologically without skipping passages or jumping back and forth between chapters.\n* Extensivity: The extensivity principle makes clear that every little detail of a text must be included in the interpretation, no matter how irrelevant it seems.\n* Frugality: This requires the interpreter to only create readings that are enforced by the text and, accordingly, to only allow case structure hypotheses that can be proven using the text.\n\n===== The Hermeneutic Circle =====\nThe term \"Hermeneutic Circle\" was coined by Friedrich Ast, Wilhelm Dilthey and Georg Gadamer and is one of the key terms of Hermeneutics. The principle means that a whole text can only be understood if you understand its individual parts, while the individual parts can only be understood if you understand the whole. This shapes the process of interpretation into a circular structure. \nThe theory of the Hermeneutic Circle identifies three stages and one preliminary stage (8):\n* Preliminary stage: development of a pre-understanding, mastery of the language, idea of the external conditions of a text\n* The hermeneutical draft (first stage): The horizon of understanding and the horizon of meaning merge\n* The hermeneutic experience (second stage): pre-understanding is expanded and corrected\n* The improved draft (third stage): deeper understanding, maturation of the prior understanding\nWith this revised pre-understanding, the process of understanding can be restarted so that the previous stages are run through again. In principle, this circle can be repeated endlessly. Other theories choose the term \"hermeneutic spiral\" instead or reject the concept altogether.\n\n=== Strengths & Challenges ===\n* The different hermeneutic approaches and principles can be applied to studies in almost all disciplines - especially all Humanities, Social and Natural sciences - which makes it a universial method of understanding.\n* By trying to understand the meaning of an object, as well as the intention behind its creation, one is able to get a deeper understanding of the relations between the author as a person, the text and its historical/cultural/social background, and the reader with his or her own background.\n* There are so many different approaches and opinions on hermeneutic methodology that it can be difficult to stay on top of things.\n* One aspect of Hermeneutics that is still widely debated is the question which element should be emphasized: The author' intention, the cultural-historical context of the text or the reader's culturally conditioned way of understanding the text. This can create an imbalance or incomplete interpretation (5).\n* A central challenge to most forms of Hermeneutics is the subjectivity of the interpreter. As Gadamer proposes, it is crucial that the interpreter is conscious of his own prejudices and expectations before studying a text.\n* It has been a challenge to determine - assuming that authorial intention is the goal of interpretation - factors that can track or measure this intention.\n\n\n== Key Publications ==\n* Aristotle (1938). ''On Interpretation''. Harold P. Cooke (trans.). In Aristotle, Vol. 1 (Loeb Classical Library). William Heinemann, p. 111\u2013179.\n* Gadamer, H.-G. (2013). ''Truth and Method''. Bloomsbury.\n* Gadamer, H.-G. (1975). \"Hermeneutics and Social Science\". ''Cultural Hermeneutics''. 2 (4). p.307\u2013316.\n* Heidegger, M. (1962) [1927]. Being and Time. Harper and Row.\n* Oevermann, U. et al. (1987). ''Structures of meaning and objective Hermeneutics''. In: Meha, V. et al. (eds.). ''Modern German Sociology.'' ''European Perspectives: a Series in Social Thought and Cultural Criticism''. Columbia University Press, p. 436\u2013447.\n* Seebohm, T. (2007). ''Hermeneutics: Method and Methodology''. Springer.\n* Zimmermann, J. (2015). ''Hermeneutics: A Very Short Introduction''. Oxford University Press.\n\n\n== References ==\n(1) Stanford Encyclopedia of Philosophy. 2016. *Hermeneutics.* Last accessed on 15.07.2020. Available at [https://plato.stanford.edu/entries/hermeneutics/#Intr]<br>\n(2) Bleicher, Josef (2017): Contemporary Hermeneutics: Hermeneutics as Method, Philosophy and Critique. Routledge.<br>\n(3) Seebohm, Thomas (2004): Hermeneutics. Method and Methodology. Kluwer Academic Publishers.<br>\n(4) Gardner, Philip (2010): Hermeneutics, History and Memory. Routledge.<br>\n(5) Porter, Stanley; Robinson, Jason (2011): Hermeneutics. An Introduction to Interpretive Theory. Eerdmans Publishing.<br>\n(6) Reichertz, Jo (2000): Objective Hermeneutics and Hermeneutic Sociology of Knowledge. In: Flick, Uwe et\nal. (Eds.). Companion to Qualitative Research. Sage. Summary available at [https://uni-due.de/imperia/md/content/kowi/hermeneutikenglisch.pdf]<br>\n(7) Wernet, Andreas (2000): Einf\u00fchrung in die Interpretationstechnik der Objektiven Hermeneutik. Leske + Budrich.\n(8) Gadamer, Hans-Georg (2013). Truth and Method, Bloomsbury.\n\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Katharina Kirn."
                    },
                    "sha1": "1v39g2qqyiyy4zjiz2nohqnnu74nok5"
                }
            },
            {
                "title": "Histograms and Boxplots",
                "ns": "0",
                "id": "712",
                "revision": {
                    "id": "5370",
                    "parentid": "5367",
                    "timestamp": "2021-05-21T09:49:44Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "Replaced content with \"This page was updated and moved to [[Barplots,_Histograms_and_Boxplots|Barplots, Histograms and Boxplots]] entry.\"",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "113",
                        "#text": "This page was updated and moved to [[Barplots,_Histograms_and_Boxplots|Barplots, Histograms and Boxplots]] entry."
                    },
                    "sha1": "7p98xml9efbsv5qy6zhx12l5ekh16cb"
                }
            },
            {
                "title": "History of Methods",
                "ns": "0",
                "id": "392",
                "revision": {
                    "id": "6718",
                    "parentid": "6493",
                    "timestamp": "2022-06-15T20:25:05Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20658",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[History of Methods (German)]]\n\n'''In short:''' This entry provides an overview on the history of scientific methodology through the ages. Where did science come from, and which influences and changes occurred until now?\n__TOC__\n<br/>\n==== Antiquity - ''Observe And Understand'' ====\n'''With the rise of language, and much later with the invention of writing, we witnessed the dawn of human civilisation.''' Communicating experience and knowledge is considered to be one of the most pivotal steps that led to the formation of societies. Once we moved from being hunters and gatherers into larger and more complex [[Glossary|cultures]] that lived in cities, a surplus in food production allowed for some privileged people to preoccupy themselves with other goals than the mere safeguarding of their daily survival. This led to the blossoming of early cultures across the globe, many of which with tremendous developments in terms of agriculture, engineering and architecture. The rise of urban cultures East and West led to a new line of inquiry and ultimately also a new line of thinking, most notably in civilizations such as the Vedic, the Zhou period, early Persian culture, and - often most recognised in the West - Ancient Greece. \n\n[[File:1200px-Aristotle_Altemps_Inv8575.jpg|300px|thumb|'''Aristotle (384 - 322 BC).''' Source: [https://de.wikipedia.org/wiki/Aristoteles Wikipedia]]]\nGreece is often in the focus because we consider it the birthplace of modern democracy, despite only a small privileged elite actually being considered citizens. Though these privileged were only few, we owe [https://plato.stanford.edu/entries/aristotle-politics/ Aristotle], [https://plato.stanford.edu/entries/socrates/ Socrates] and [https://plato.stanford.edu/entries/plato/ Plato] and many others the foundation of Western Philosophy. '''Empirical inquiry did not weigh down thinking yet, hence much of the thinking of Greek philosophy was lofty but free from the burden of real world inquiry.''' There were connections between philosophical thinking and the real world - you might remember [https://plato.stanford.edu/entries/pythagoras/ Pythagoras] from school - yet much later philosophy and the rest of science would be vastly disconnected. Early accounts such as the scriputures of Herodot give testimony of the history of this age, and represent one of the earliest accounts of a systematic description of geography and culture. Early mathematics paved the way for more fundamental approaches, and [[Survey|surveys]] were a common part of the governance at the time. Hence many approaches that we would consider scientific methods were already being utilised, but not so much for scientific inquiry as for a direct purpose of benefit that was not necessarily associated with scientific knowledge production.\n\nEastern cultures often had a comparably early development of philosophies, with [https://plato.stanford.edu/entries/confucius/ Confucius]' work as well as the Vedas and the Pali canon as testimony of the continuous cultural influence that is comparable to Greek philosophy in the West. Equally did many Eastern empires use methods such as census and survey as measures of governance, and many other notable approaches that would later contribute to the formation of scientific methods. '''Law was an early testimony of the necessity of rules and norms in human societies.''' Consequently, [[Legal Research]] can be seen as one of the earliest forms of inquiry that translated systematic inquiry and analysis directly to the real world.\n\nBetween the ancients and the medieval times there a somewhat blurry gap, with the fall of the Roman Empire in the West, the expansion of the Mongol empire in the East, and the occupation of much of India by Islam as notable elements of [[Glossary|change]]. All this triggered not only an enormous cultural change, but more importantly an increasing exchange, leading to schools of thoughts that became increasingly connected through writing, and also often in thinking. '''Logic is an example of a branch of philosophy that linked the ancients (Plato, Confucius, [https://plato.stanford.edu/entries/buddha/ Buddha]) with the philosophy of the enlightenment.''' This was an important precondition in medieval times for a systematic line of thinking, triggering or further developing pragmatic, analytic and sceptic approaches, among others. Logic as a part of philosophy provided the basis for a clear set of terms and rhetoric within inquiry, and would later become even more important with regards to rationality. The early need for a clear wording was thus deeply rooted in philosophy, highlighting the importance of this domain until today. \n\nMany concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''\n\n==== Before Enlightenment - ''Measure And Solve'' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|'''Mercator world map.''' Source: [https://de.wikipedia.org/wiki/Mercator-Projektion Wikipedia]]] \n'''Another breakthrough that was also rooted in geometry was the compilation of early trade maps.''' While many European maps were detailed but surely not on scale (Ebstorfer Weltkarte), early maps still enabled a supra-regional trade. The real breakthrough was the [[Geographical Information Systems|Mercator map]], which was - restricted to the knowledge at the time - the first map that enabled a clear navigation across the oceans, enabling colonialism and Western dominions and domination of the world. This is insofar highly relevant for methods, because it can be argued that the surplus from the colonies and distant countries was one of the main drivers of the thriving of the European colonial rulers, their economies and consequently, their science. A direct link can be made between the [[Normativity of Methods|inequalities]] that were increased by colonialism and the thriving of Western science, including the development of scientific methods.\n\n[[File:printing-press-2.jpg|300px|thumb|left|'''Invention of the printing press'''. Source: [[https://www.ecosia.org/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \nWith a rising number of texts being available through the [https://www.britannica.com/biography/Johannes-Gutenberg invention of printing], the long known method of [[Hermeneutics]], which enabled deep and systematic analysis of writing, was one of the first methods to grow. A link can be made to translations and interpretations of the Bible, and many other religious discourses are indeed not different from deep text analysis and the derivation of interpretations. Hermeneutics go clearly a step further, and hence are one of the earliest and to date still most important scientific methods. [[Thought Experiments]] were another line of thinking that started to emerge more rapidly. By focussing on ''What if'' questions, scientists considered all sorts of questions to derive patterns of even law of nature ([https://plato.stanford.edu/entries/galileo/ Galileo]) and also directed their view into the future. Thought experiments were in an informal way known since antiquity, yet during medieval times these assumptions became a trigger to question much of what was supposedly known. Through experiments they became early staring points for subsequent more systematic experimentation. \n\n[[File:Somer_Francis_Bacon.jpg|thumb|300px|'''Francis Bacon.''' Source: [https://de.wikipedia.org/wiki/Francis_Bacon Wikipedia]]] \nIt was [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon] who ultimately moved out of this dominance of thinking and called for the importance of empirical inquiry. '''By observing nature and its phenomena, we are able to derive conclusion based on the particular.''' Bacon is thus often coined to be the father of 'the scientific method', an altogether misleading term, as there are many scientific methods. However, empiricism, meaning the empirical investigation of phenomena and the derivation of results or even rules triggered a scientific revolution, and also a specialization of different scientific branches. While Leonardo da Vinci (1452-1519) had been a polymath less than a century ago, Bacon (1561-1626) triggered an increase in empirical inquiry that led to the deeper formation of scientific disciplines. Many others contributed to this development: [https://plato.stanford.edu/entries/locke/ Locke] claimed that human knowledge builds on experience, and [https://plato.stanford.edu/entries/hume/ Hume] propelled this into skepticism, casting the spell of doubt on anything that is rooted in belief or dogma. By questioning long standing knowledge, science kicked in - among other things - the door of god himself, a circumstance that is too large to be presented here in any sense. What is important however is that - starting from the previous age of diverse approaches to knowledge - the combination of empiricism on the one end, and the associated philosophical developments on the other end, the age of reason had dawned.\n\n[[File:Timeline of methods V2.jpg|600px|frame|center|'''A timeline of major breakthroughs in science.''' Source: own]]\n\n==== Enlightenment - ''Pathways To Scientific Disciplines'' ====\nThe age of reason triggered - among many other changes - the development of scientific disciplines much further as ever before. Scientific inquiry was enabled by an increasing influx of resources from the colonies, and biology, medicine, mathematics, astronomy, physics and so much more prospered. '''Early inquiries into these disciplines had often existed since centuries if not millennia, but now knowledge got explored at an exponential pace.''' Much of it was descriptive at first, including what would be considered natural history today. Yet, the early inquiries into the systematical determination and differentiation of organisms ([https://www.britannica.com/biography/Carolus-Linnaeus Linn\u00e9]) laid the foundation of many other concepts. It was again the colonies that triggered a surplus, this time in basic material to generate more insight. Darwins theory, after all, would have been impossible without specimen from his journeys. While some thus looked into the natural world, other branches of science investigated society, and the first truly systematic case studies were designed in medicine. Hence we moved from experiments as a means of observation into [[Experiments|experimental inquiry]], paving the road to 20th century hypothesis testing. This demanded the testing of the observational outcome, a phenomenon that equally challenged scientists in astronomy. There, just as in experiments, it was recognized that although they tried to be diligent and use the latest available optical tools, there could still be an error that needed to estimated. This constituted a true application opportunity for statistics, leading to the rise of [[Category:Quantitative|quantitative]] methods. \n\n[[File:860-header-explainer-correlationchart.jpg|500px|left|thumb|'''Correlations can be deceitful.''' Source: [http://www.tylervigen.com/spurious-correlations Spurious correlations]]] Probability was the core underlying principle, or in other words, the statistical answer to the question whether something was truly by chance, or if there was a statistical relation. Under the growing economies, more and more numbers surfaced, and building on the Dutch resolution of double book keeping, questions arose whether there were patterns to be found in the data. Could crop outcome of cotton predict its market value later in the year? And would crop failures be clearly linked to famines, or could this be compensated? '''Statistical [[Correlations|correlations]] were able to relate to variables, and to find if one is related to the other, or if the two are unrelated.''' This triggered a hefty move of utilitarianism from philosophy into economics, which is one of the core disciplines up until today. Much can be said about the calculation of utility, [[Big problems for later|but this is beyond the point here]]. In science, learned academies grew everywhere, and universities thrived, also because of the economic paradigm of a growth-focused and consumption-orientated baseline that emerged, and more and more knowledge emerges to criticize and question this [[Glossary|paradigm]]. This triggered what is often called the age of reflection, when empiricism became basically untamed, and the depth of inquiry led to disciplines that started to entrench themselves into even deeper sub-disciplines. The impact on societies was severe. Mechanization, new approaches in agriculture, the rise of modern medicine, an increasingly refined biology and the developments in chemistry are testimony of the physical world being ever more in the focus of science. The general lines of philosophical thinking - reason, social contract and utilitarianism - became partly uncoupled from philosophy, and developed for better or worse to become distinct disciplines such as psychology, social science, political science, cultural studies and economics. We thus observe a deviance from empirical sciences from the previously all-encompassing philosophy. Hence empiricism was on the rise, and with it a pronounced shift in science and society.\n\n==== After the Wars - ''The Rise Of [[Agency, Complexity and Emergence|Agency]]'' ====\nOut of societal, economic, technological and other developments, there emerged almost consequently a counterpoint. This was already rooted in [https://plato.stanford.edu/entries/kant-reason/ Kant] and his assumption of the priority of reason, and [https://plato.stanford.edu/entries/marx/ Marx] made a vital step towards a critical perspective. He developed philosophy from an inquiry of views of the world towards an agenda to create change, which was mainly focussed on the economical system. '''All this cascaded into the tremendous changes of the 20th century, which are almost too much to comprehend.''' It could however be concluded that the disaster of the two World Wars, the recognition of the inequalities of the colonial system, and the focus on the injustices is associated with emancipation, discrimination and racism. Critical theory can be seen as a landmark development, where the structures and conditions of societies and [[Glossary|culture]]s are taken into the focus.\n\nThis triggered two general developments. First, there was a general and necessary increase in the questioning of society and science in a sense of critical theory, which needs to play a central role at least until these problems are solved. Second, the critical perspective triggered a recognition of the need to differentiate the creation of knowledge, which ultimately translated into severe methodological developments of taking people deeper into the methodological focus ([[Open Interview|Interviews]], [[Delphi]], [[Ethnography]]). The rising number of approaches in statistics led to more and more disciplines (social science, ecology, psychology) and sub-disciplines. A prominent example is psychology, which utilized the [[ANOVA]] as the main analysis tool for the psychological experiment. Interviews emerged in the 1960 as one of the main tools of deep social inquiry. [[Scenario Planning]] allowed for the systematic creation of different futures. [[Grounded Theory]] unlocked a more inductive view of the world. [[System Thinking & Causal Loop Diagrams|Network thinking and system theory]] emerged to take interaction and complexities of the world into account. And satellites and the recognition of [https://www.bfi.org/about-fuller/big-ideas/spaceshipearth 'spaceship earth'] unlocked a new global perspective on our planet. [[Meta-Analysis]] allowed for a ''supra'' perspective to integrate the diversity of different results, and Kuhn even proclaimed a perspective how science emerges as a whole. [https://plato.stanford.edu/entries/feyerabend/ Feyerabend] rejected the Dogma of science altogether, which was a vital step towards a critical perspective on methods. '''All the while, disciplines branched into ever smaller disciplines, fields, sub-disciplines etc.'''\n\n==== Internet and Computers - ''The New Science Of Interconnectedness'' ====\n[[File:disciplinary.png|thumb|600px|right|'''Different Disciplinary Approaches'''. Source: [http://makinggood.design/thoughts/tasty/ Jo Bailey makinggood.design]]] \nOut of the general recognition that novel paradigms need to be developed or identified because of the flaws of the current system, and equally out of the recognition of the importance of new forms of knowledge, a new mode of science emerged, calling for a critical reflection and an integration of science and society. With an increasing recognition of the complexity of the problems humankind faces, it became clear that one single discipline would not be able to approximate the necessary solutions. '''Instead, the disciplines would need to collaborate, which is no small thing to ask.''' Opposing lines of thinking, different foci, incompatible language, and after all a competition for limited resources and attention, and schools of thinking that claim superiority in their importance - all these are no building blocks of scientific collaboration. Yet all these problems are negligible compared to the challenge to create a joined knowledge production between science and society. Science kept a partly arrogant distance from society, or at least much of science avoided a direct interaction. Interviews, observations, maybe interaction were already familiar approaches. Joined problem framing and mutual learning from science and society posed a radical new mode of research, and we are only starting to shift the underlying paradigms that formed science since centuries. Trandisciplinary research thus emerged as a new mode of research, and an inclusive, reflexive and solution-oriented path that transcends trenches in science, and also between science and society. \n\nThis radical development coincides with yet another revolution that shattered science, namely the digital age. '''Computers allowed for faster calculation and novel methodological approaches.''' The internet fueled new sources and forms of knowledge, and the associated new forms of communication triggered an exchange between researchers at an unprecedented pace. All means of electronic communication, online [[Glossary|journals]] and the fact that many researchers today have their own computer led to an exponential increase in scientific collaboration. While this sometimes also breads opportunism and a shift to quantity instead of quality in research, it is undeniable that today much of scientific information is not further away from us than the click of a mouse. Technology cannot be an end in itself, but as a means to an end it enables today an exponential pace of research, which manifested itself most illustratively in the Corona crisis. The global community of researchers united in their utmost strength, and the speed and diversity of knowledge creation is unprecentented in the history of our civilisation. Never before was more interaction between the latest scientific inquiry or results and the society.\n\n== Additional Information ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html More information] on Kuhn's theory of scientific paradigm shifts.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "fvnw8i5wjqx62oamv3c2w6y6s4sljlz"
                }
            },
            {
                "title": "History of Methods (German)",
                "ns": "0",
                "id": "492",
                "revision": {
                    "id": "6531",
                    "parentid": "6530",
                    "timestamp": "2022-02-28T22:48:43Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Internet und Computer - Die neue Wissenschaft der Vernetzung */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23729",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[History of Methods]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag bietet einen \u00dcberblick \u00fcber die Geschichte der wissenschaftlichen Methodik im Wandel der Zeitalter. Woher kam die Wissenschaft und welche Einfl\u00fcsse und Ver\u00e4nderungen gab es bis heute?\n__TOC__\n<br/>\n==== Antike - ''Beobachten und Verstehen'' ====\n'''Mit dem Aufkommen der Sprache und viel sp\u00e4ter mit der Erfindung der Schrift wurden wir Zeug*innen des Anbeginns der menschlichen Zivilisation.''' Die schriftliche Vermittlung von Erfahrungen und Wissen gilt als einer der wichtigsten Schritte hin zur Bildung von Gesellschaften. Als wir von J\u00e4ger*innen und Sammler*innen zu gr\u00f6\u00dferen und komplexeren Kulturen \u00fcbergingen, die in St\u00e4dten lebten, erlaubte ein \u00dcberschuss in der Nahrungsmittelproduktion einigen privilegierten Menschen, sich mit anderen Zielen als der blo\u00dfen Sicherung ihres t\u00e4glichen \u00dcberlebens zu besch\u00e4ftigen. Dies f\u00fchrte zum Aufbl\u00fchen fr\u00fcher Kulturen auf der ganzen Welt, viele davon mit enormen Entwicklungen in Landwirtschaft, Technik und Architektur. Der Aufstieg der st\u00e4dtischen Kulturen in Ost und West f\u00fchrte zu einer neuen Forschungsrichtung und letztlich auch zu einer neuen Denkweise, vor allem in Zivilisationen wie den Veden, der Zhou-Periode, der fr\u00fchen persischen Kultur und - im Westen oft am meisten anerkannt - im antiken Griechenland. \n\n[[File:1200px-Aristotle_Altemps_Inv8575.jpg|300px|thumb|'''Aristoteles (384 - 322 v. Chr.).''' Quelle: [https://de.wikipedia.org/wiki/Aristoteles Wikipedia]]]\nGriechenland steht oft im Mittelpunkt, weil wir es als Geburtsort der modernen Demokratie betrachten, obwohl nur eine kleine privilegierte Elite tats\u00e4chlich als B\u00fcrger*innen betrachtet wird. Obwohl diese Privilegierten nur wenige waren, verdanken wir [https://plato.stanford.edu/entries/aristotle-politics/ Aristotle], [https://plato.stanford.edu/entries/socrates/ Socrates] und [https://plato.stanford.edu/entries/plato/ Plato] und vielen anderen die Gr\u00fcndung der westlichen Philosophie. '''Empirische Forschung belastete das Denken noch nicht, daher war ein Gro\u00dfteil des Denkens der griechischen Philosophie erhaben, aber frei von der Last der Untersuchung der realen Welt.''' Es gab Verbindungen zwischen philosophischem Denken und der realen Welt - Sie erinnern sich vielleicht an [https://plato.stanford.edu/entries/pythagoras/ Pythagoras] aus der Schulzeit - doch viel sp\u00e4ter w\u00fcrden Philosophie und der Rest der Wissenschaft in hohem Ma\u00dfe voneinander getrennt sein. Fr\u00fche Berichte wie die Schriften des Herodot geben Zeugnis von der Geschichte dieses Zeitalters und stellen einen der fr\u00fchesten Berichte \u00fcber eine systematische Beschreibung von Geographie und Kultur dar. Die fr\u00fche Mathematik ebnete den Weg f\u00fcr grundlegendere Ans\u00e4tze, und [[Survey|Umfragen]] waren damals ein \u00fcblicher Teil der Staatsf\u00fchrung. Daher wurden bereits viele Ans\u00e4tze, die wir als wissenschaftliche Methoden betrachten w\u00fcrden, genutzt, jedoch weniger f\u00fcr wissenschaftliche Untersuchungen als vielmehr f\u00fcr einen direkten Nutzen, der nicht unbedingt mit der wissenschaftlichen Wissensproduktion verbunden war.\n\n\u00d6stliche Kulturen hatten oft eine vergleichsweise fr\u00fche Entwicklung von Philosophien, wobei das Werk des [https://plato.stanford.edu/entries/confucius/ Confuzius] sowie die Veden und der Pali-Kanon von einem kontinuierlichen kulturellen Einfluss zeugen, der mit der griechischen Philosophie im Westen vergleichbar ist. Ebenso nutzten viele \u00f6stliche Reiche Methoden wie Volksz\u00e4hlung und Vermessung als Ma\u00df der Regierungsf\u00fchrung und viele andere bemerkenswerte Ans\u00e4tze, die sp\u00e4ter zur Herausbildung wissenschaftlicher Methoden beitragen sollten. '''Das Recht war ein fr\u00fches Zeugnis f\u00fcr die Notwendigkeit von Regeln und Normen in menschlichen Gesellschaften.''' Folglich kann [[Legal Research|Rechtsforschung]] als eine der fr\u00fchesten Formen der Untersuchung angesehen werden, die systematische Untersuchungen und Analysen direkt in die reale Welt \u00fcbertrug.\n\nZwischen der Antike und dem Mittelalter klafft eine etwas verschwommene L\u00fccke, wobei der Fall des R\u00f6mischen Reiches im Westen, die Expansion des Mongolischen Reiches im Osten und die Besetzung eines gro\u00dfen Teils Indiens durch den Islam als bemerkenswerte Elemente des Wandels gelten. All dies l\u00f6ste nicht nur einen enormen kulturellen Wandel aus, sondern, was noch wichtiger ist, einen zunehmenden Austausch, der zu Gedankenschulen f\u00fchrte, die durch das Schreiben und oft auch durch das Denken immer enger miteinander verbunden wurden. '''Die Logik ist ein Beispiel f\u00fcr einen Zweig der Philosophie, der die Alten (Platon, Konfuzius, [https://plato.stanford.edu/entries/buddha/theBuddha Buddha]) mit der Philosophie der Aufkl\u00e4rung verband.''' Dies war im Mittelalter eine wichtige Voraussetzung f\u00fcr eine systematische Denkweise, die u.a. pragmatische, analytische und skeptische Ans\u00e4tze ausl\u00f6ste oder weiterentwickelte. Die Logik als Teil der Philosophie bildete die Grundlage f\u00fcr eine klare Begriffsbildung und Rhetorik in der Forschung, die sp\u00e4ter im Hinblick auf die Rationalit\u00e4t noch an Bedeutung gewinnen sollte. Das fr\u00fche Bed\u00fcrfnis nach einer klaren Formulierung war also tief in der Philosophie verwurzelt, was die Bedeutung dieses Bereichs bis heute verdeutlicht. \n\nViele konkrete Schritte brachten uns der konkreten Anwendung wissenschaftlicher Methoden n\u00e4her, darunter - insbesondere - der Ansatz des kontrollierten Testens durch den arabischen Mathematiker und Astronomen [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]). Aus der Mathematik der Antike hervorgegangen und diese mit der allgemein aufkommenden Erforschung der Physik verbindend, war Alhazen der erste, der [[Field experiments|Versuchsbedingungen]] in einem systematischen Sinne manipulierte und damit den Weg zu der wissenschaftlichen Methode ebnete, die Jahrhunderte sp\u00e4ter aufkommen sollte. Alhazen ist auch deshalb relevant, weil er als Universalgelehrter betrachtet werden kann, was den Aufstieg von mehr Wissen unterstreicht, das solche Charaktere erm\u00f6glichte, aber immer noch zu weit von der wahren Bildung des vielf\u00e4ltigen Kanons der [[Design Criteria of Methods|Designkriterien f\u00fcr Methoden]] wissenschaftlichen Disziplinen entfernt ist, die ihn wahrscheinlich als Experten auf dem einen oder anderen Gebiet begr\u00fc\u00dft h\u00e4tten. Nat\u00fcrlich steht Alhazen hier nur als einer von vielen, die den Aufstieg der Wissenschaft \u00fcber '''die islamische Welt im Mittelalter, die als Wiege der westlichen Wissenschaft angesehen werden kann, und auch als eine Kontinuit\u00e4t von den Eskapaden, als in Europa viel vom unmittelbaren griechischen und r\u00f6mischen Erbe verloren ging''.\n\n==== Vor der Aufkl\u00e4rung - ''Messen und L\u00f6sen'' ====\n[[File:Normal_Mercator_map_85deg.jpg|thumb|300px|left|'''Die Mercator-Weltkarte.''' Quelle: [https://de.wikipedia.org/wiki/Mercator-Projektion Wikipedia]]] \n\n'''Ein weiterer Durchbruch, der auch in der Geometrie wurzelte, war die Erstellung von fr\u00fchen Handelskarten.''' W\u00e4hrend viele europ\u00e4ische Karten zwar detailliert, aber sicher nicht ma\u00dfstabsgetreu waren (Ebstorfer Weltkarte), erm\u00f6glichten fr\u00fche Karten dennoch einen \u00fcberregionalen Handel. Der wirkliche Durchbruch war die [[Geographical Information Systems|Mercator-Karte]], die - beschr\u00e4nkt auf das damalige Wissen - die erste Karte war, die eine klare Navigation \u00fcber die Ozeane erm\u00f6glichte und damit Kolonialismus und westliche (Gewalt-)Herrschaft erm\u00f6glichte. Dies ist insofern von hoher methodischer Relevanz, als man argumentieren kann, dass der \u00dcberschuss aus den Kolonien und fernen L\u00e4ndern eine der Haupttriebkr\u00e4fte f\u00fcr das Gedeihen der europ\u00e4ischen Kolonialherren, ihrer Wirtschaft und damit auch ihrer Wissenschaft war. Es kann eine direkte Verbindung zwischen der [[Normativity of Methods|Normativit\u00e4t der Methoden]], die durch den Kolonialismus verst\u00e4rkt wurde, und dem Gedeihen der westlichen Wissenschaft, einschlie\u00dflich der Entwicklung wissenschaftlicher Methoden, hergestellt werden.\n\n[[File:printing-press-2.jpg|300px|thumb|left|''' Die Erfindung des Buchdrucks'''. Quelle: [[https://www.ecosia.org/images?q=history.com+printing+press#id=9035447589536EF73753D35837F1AE4C604B80A1 history.com]]]] \n\nMit einer steigenden Zahl von Texten, die durch die [https://www.britannica.com/biography/Johannes-Gutenberg Erfindung des Buchdrucks] verf\u00fcgbar wurden, war die seit langem bekannte Methode der [[Hermeneutics|Hermeneutik]], die eine tiefe und systematische Analyse der Schrift erm\u00f6glichte, eine der ersten Methoden, die sich entwickelte. Es kann eine Verbindung zu \u00dcbersetzungen und Interpretationen der Bibel hergestellt werden, und viele andere religi\u00f6se Diskurse unterscheiden sich in der Tat nicht von einer tiefen Textanalyse und der Ableitung von Interpretationen. Die Hermeneutik geht eindeutig einen Schritt weiter und ist daher eine der fr\u00fchesten und bis heute wichtigsten wissenschaftlichen Methoden. [[Thought experiments|Gedankenexperimente]] waren eine weitere Denkrichtung, die sich schneller herauszubilden begann. Indem sie sich auf \"Was-w\u00e4re-wenn\"-Fragen konzentrierten, betrachteten die Wissenschaftler*innen alle m\u00f6glichen Fragen, um sogar Muster in Naturgesetzen abzuleiten ([https://plato.stanford.edu/entries/galileo/ Galileo]) und richteten ihren Blick auch in die Zukunft. Gedankenexperimente waren seit der Antike auf informelle Weise bekannt, doch im Mittelalter wurden diese Annahmen zum Ausl\u00f6ser, vieles von dem, was angeblich bekannt war, in Frage zu stellen. Durch Experimente wurden sie zu fr\u00fchen Ansatzpunkten f\u00fcr sp\u00e4tere systematischere Experimente. \n\n[[File:Somer_Francis_Bacon.jpg|thumb|300px|'''Francis Bacon.''' Quelle: [https://de.wikipedia.org/wiki/Francis_Bacon Wikipedia]]] \n\nEs war [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon], der sich letztlich aus dieser Dominanz des Denkens herausbewegte und die Bedeutung der empirischen Untersuchung einforderte. '''Indem wir die Natur und ihre Ph\u00e4nomene beobachten, sind wir in der Lage, Schlussfolgerungen aus dem Besonderen abzuleiten.''' Bacon wird daher oft als der Vater der \"wissenschaftlichen Methode\" bezeichnet, ein v\u00f6llig irref\u00fchrender Begriff, da es viele wissenschaftliche Methoden gibt. Der Empirismus, d.h. die empirische Untersuchung von Ph\u00e4nomenen und die Ableitung von Ergebnissen oder gar Regeln, l\u00f6ste jedoch eine wissenschaftliche Revolution, aber auch eine Spezialisierung verschiedener Wissenschaftszweige aus. W\u00e4hrend Leonardo da Vinci (1452-1519) noch vor weniger als einem Jahrhundert ein Universalgelehrter gewesen war, l\u00f6ste Bacon (1561-1626) eine Zunahme der empirischen Untersuchung aus, die zu einer tieferen Herausbildung wissenschaftlicher Disziplinen f\u00fchrte. Viele andere trugen zu dieser Entwicklung bei: [https://plato.stanford.edu/entries/locke/ Locke] behauptete, dass menschliches Wissen auf Erfahrung aufbaut, und [https://plato.stanford.edu/entries/hume/ Hume] trieb dies in die Skepsis und zog alles in Zweifel, was in Glauben oder Dogmen wurzelt. Indem die Wissenschaft langj\u00e4hriges Wissen in Frage stellte, trat sie - unter anderem - die T\u00fcr zu Gott selbst ein, ein Umstand, der zu gro\u00df ist, um hier in irgendeiner Weise dargestellt zu werden. Wichtig ist jedoch, dass - ausgehend von der vorangegangenen Epoche der unterschiedlichen Zug\u00e4nge zum Wissen - durch die Kombination von Empirie auf der einen Seite und den damit verbundenen philosophischen Entwicklungen auf der anderen Seite - das Zeitalter der Vernunft angebrochen war.\n[[File:Timeline of methods V2.jpg|600px|frame|center|'''Eine Zeitlinie wichtiger wissenschaftlicher Durchbr\u00fcche.''' Quelle: eigene Darstellung]]\n\n==== Aufkl\u00e4rung - ''Wege zu wissenschaftlichen Disziplinen'' ====\nDas Zeitalter der Vernunft hat - neben vielen anderen Ver\u00e4nderungen - die Entwicklung der wissenschaftlichen Disziplinen viel weiter vorangetrieben als je zuvor. Wissenschaftliche Forschung wurde durch einen zunehmenden Zustrom von Ressourcen aus den Kolonien erm\u00f6glicht, und Biologie, Medizin, Mathematik, Astronomie, Physik und vieles mehr gediehen. '''Fr\u00fche Untersuchungen in diesen Disziplinen gab es oft schon seit Jahrhunderten, wenn nicht Jahrtausenden, aber jetzt wurde das Wissen in einem exponentiellen Tempo erforscht.''' Vieles davon war anfangs beschreibend, auch das, was man heute als Naturgeschichte bezeichnen w\u00fcrde. Doch die fr\u00fchen Untersuchungen zur systematischen Bestimmung und Differenzierung von Organismen ([https://www.britannica.com/biography/Carolus-Linnaeus Linn\u00e9]) legten den Grundstein f\u00fcr viele andere Konzepte. Wieder waren es die Kolonien, die einen \u00dcberschuss ausl\u00f6sten, diesmal an Grundlagenmaterial, um mehr Erkenntnisse zu gewinnen. Schlie\u00dflich w\u00e4re die Darwin'sche Theorie ohne Exemplare von seinen Reisen nicht m\u00f6glich gewesen. W\u00e4hrend einige auf diese Weise in die Natur blickten, untersuchten andere Wissenschaftszweige die Gesellschaft, und in der Medizin wurden die ersten wirklich systematischen Fallstudien entworfen. Daher gingen wir von Experimenten als Beobachtungsmittel zu [[Experiments|experimentelle Untersuchung]] \u00fcber und ebneten damit den Weg f\u00fcr Hypothesentests im 20. Jahrhundert. Dies erforderte die Pr\u00fcfung der Beobachtungsergebnisse, ein Ph\u00e4nomen, das die Wissenschaftler*innen in der Astronomie gleicherma\u00dfen herausforderte. Dort erkannte man, genau wie bei den Experimenten, dass es, obwohl sie versuchten, sorgf\u00e4ltig zu sein und die neuesten verf\u00fcgbaren optischen Hilfsmittel zu verwenden, immer noch einen Fehler geben konnte, der abgesch\u00e4tzt werden musste. Dies stellte eine echte Anwendungsm\u00f6glichkeit f\u00fcr die Statistik dar und f\u00fchrte zum Aufstieg [[Category:Quantitative|quantitativer]] Methoden.\n\n[[File:860-header-explainer-correlationchart.jpg|500px|left|thumb|'''Korrelationen k\u00f6nnen t\u00e4uschen.''' Quelle: [http://www.tylervigen.com/spurious-correlations Spurious correlations]]] \nWahrscheinlichkeit war das zugrunde liegende Kernprinzip, oder mit anderen Worten, die statistische Antwort auf die Frage, ob etwas wirklich zuf\u00e4llig war oder ob es einen statistischen Zusammenhang gab. Unter den wachsenden Volkswirtschaften tauchten immer mehr Zahlen auf, und aufbauend auf der niederl\u00e4ndischen L\u00f6sung der doppelten Buchf\u00fchrung stellte sich die Frage, ob in den Daten Muster zu finden waren. Konnte das Ernteergebnis der Baumwolle ihren Marktwert sp\u00e4ter im Jahr vorhersagen? Und w\u00fcrden Ernteausf\u00e4lle eindeutig mit Hungersn\u00f6ten in Zusammenhang stehen, oder k\u00f6nnte dies kompensiert werden? '''Statistische [[Correlations|Korrelationen]] waren in der Lage, sich auf Variablen zu beziehen und herauszufinden, ob die eine mit der anderen in Beziehung steht oder ob die beiden nicht miteinander verbunden sind.''' Dies l\u00f6ste einen heftigen Wechsel des Utilitarismus von der Philosophie in die Wirtschaft aus, die bis heute zu den Kerndisziplinen geh\u00f6rt. \u00dcber die Berechnung von Utilitarismus l\u00e4sst sich viel sagen, [[Big problems for later|aber das werde ich sicher nicht hier tun]]. In der Wissenschaft wuchsen \u00fcberall gelehrte Akademien, und die Universit\u00e4ten bl\u00fchten auf, auch wegen des wirtschaftlichen Paradigmas einer wachstums- und konsumorientierten Grundlinie, die sich herausbildete, und es entsteht immer mehr Wissen, um dieses Paradigma zu kritisieren und in Frage zu stellen. Dies l\u00f6ste das oft als Zeitalter der Reflexion bezeichnete Zeitalter aus, in dem die Empirie im Grunde ungez\u00e4hmt wurde und die Tiefe der Untersuchung zu Disziplinen f\u00fchrte, die sich in noch tieferen Teildisziplinen zu verankern begannen. Die Auswirkungen auf die Gesellschaften waren schwerwiegend. Mechanisierung, neue Ans\u00e4tze in der Landwirtschaft, der Aufstieg der modernen Medizin, eine immer raffiniertere Biologie und die Entwicklungen in der Chemie zeugen davon, dass die physische Welt immer mehr in den Fokus der Wissenschaft geriet. Die allgemeinen Linien des philosophischen Denkens - Vernunft, Gesellschaftsvertrag und Utilitarismus - l\u00f6sten sich teilweise von der Philosophie ab und entwickelten sich auf Gedeih und Verderb zu eigenst\u00e4ndigen Disziplinen wie Psychologie, Sozialwissenschaft, Politikwissenschaft, Kulturwissenschaften und Wirtschaftswissenschaften. Wir beobachten also eine Abweichung der empirischen Wissenschaften von der zuvor allumfassenden Philosophie. Der Empirismus war also auf dem Vormarsch, und mit ihm eine ausgepr\u00e4gte Verschiebung in Wissenschaft und Gesellschaft.\n\n==== Nach den Kriegen - ''Der Aufstieg von [[Agency, Complexity and Emergence|Agency]]'' ====\nAus gesellschaftlichen, wirtschaftlichen, technologischen und anderen Entwicklungen entstand fast konsequenterweise ein Kontrapunkt. Dieser wurzelte bereits in [https://plato.stanford.edu/entries/kant-reason/ Kant] und seiner Annahme der Priorit\u00e4t der Vernunft, und [https://plato.stanford.edu/entries/marx/ Marx] machte einen entscheidenden Schritt hin zu einer kritischen Perspektive. Er entwickelte die Philosophie von einer Untersuchung der Weltbilder hin zu einer Agenda zur Schaffung von Ver\u00e4nderungen, die sich haupts\u00e4chlich auf das Wirtschaftssystem konzentrierte. '''All dies kaskadierte in die gewaltigen Ver\u00e4nderungen des 20. Jahrhunderts, die fast zu viel sind, um sie zu begreifen.''' Es k\u00f6nnte jedoch der Schluss gezogen werden, dass die Katastrophe der beiden Weltkriege, die Anerkennung der Ungleichheiten des Kolonialsystems und die Konzentration auf die Ungerechtigkeiten mit Emanzipation, Diskriminierung und Rassismus verbunden ist. Die Kritische Theorie kann als eine richtungsweisende Entwicklung angesehen werden, bei der die Strukturen und Bedingungen von Gesellschaften und Kulturen in den Mittelpunkt ger\u00fcckt werden.\n\nDies l\u00f6ste zwei allgemeine Entwicklungen aus. Erstens gab es eine allgemeine und notwendige Zunahme der Befragung von Gesellschaft und Wissenschaft im Sinne einer kritischen Theorie, die zumindest bis zur L\u00f6sung dieser Probleme eine zentrale Rolle spielen muss. Zweitens l\u00f6ste die kritische Perspektive eine Anerkennung der Notwendigkeit aus, die Schaffung von Wissen zu differenzieren, was letztlich zu schwerwiegenden methodischen Entwicklungen f\u00fchrte, die Menschen tiefer in den methodischen Fokus zu r\u00fccken ([[Open Interview|Interviews]], [[Delphi]], [[Ethnography|Ethnographie]]). Die steigende Zahl der Ans\u00e4tze in der Statistik f\u00fchrte zu immer mehr Disziplinen (Sozialwissenschaft, \u00d6kologie, Psychologie) und Teildisziplinen. Ein prominentes Beispiel ist die Psychologie, die die [[ANOVA]] als Hauptanalysewerkzeug f\u00fcr das psychologische Experiment verwendete. In den 1960er Jahren erwiesen sich Interviews als eines der Hauptinstrumente f\u00fcr tiefe soziale Untersuchungen. Die [[Scenario Planning|Szenarioplanung]] erm\u00f6glichte die systematische Schaffung verschiedener Zuk\u00fcnfte. Die \"[[Grounded Theory]]\" erm\u00f6glichte eine induktivere Sicht auf die Welt. Das [[System Thinking & Causal Loop Diagrams|Systemdenken]] und die Netzwerk- und Systemtheorie entstanden, um die Interaktion und Komplexit\u00e4t der Welt zu ber\u00fccksichtigen. Und Satelliten und die Anerkennung von [https://www.bfi.org/about-fuller/big-ideas/spaceshipearth 'Raumschiff Erde'] erschlossen eine neue globale Perspektive auf unseren Planeten. Die [[Meta-Analysis|Meta-Analyse]] erm\u00f6glichte eine 'supra'-Perspektive, um die Vielfalt der unterschiedlichen Ergebnisse zu integrieren, und Kuhn verk\u00fcndete sogar eine Perspektive, wie Wissenschaft als Ganzes entsteht. [https://plato.stanford.edu/entries/feyerabend/ Feyerabend] lehnte das Dogma der Wissenschaft ganz und gar ab, was ein entscheidender Schritt hin zu einer kritischen Methodenperspektive war. '''Dabei verzweigten sich die Disziplinen immer weiter in immer kleinere Disziplinen, Bereiche, Teildisziplinen usw.'''\n\n==== Internet und Computer - ''Die neue Wissenschaft der Vernetzung'' ====\n[[File:disciplinary.png|thumb|600px|right|'''Unterschiedliche Formen disziplin\u00e4rer Zusammenarbeit'''. Quelle: [http://makinggood.design/thoughts/tasty/ Jo Bailey makinggood.design]]] \nAus der allgemeinen Erkenntnis heraus, dass aufgrund der M\u00e4ngel des gegenw\u00e4rtigen Systems neue Paradigmen entwickelt oder identifiziert werden m\u00fcssen, und ebenso aus der Erkenntnis der Bedeutung neuer Wissensformen entstand ein neuer Wissenschaftsmodus, der eine kritische Reflexion und eine Integration von Wissenschaft und Gesellschaft erfordert. Mit zunehmender Anerkennung der Komplexit\u00e4t der Probleme, mit denen die Menschheit konfrontiert ist, wurde klar, dass eine einzige Disziplin nicht in der Lage sein w\u00fcrde, die notwendigen L\u00f6sungen anzun\u00e4hern. '''Stattdessen w\u00fcrden die Disziplinen zusammenarbeiten m\u00fcssen, was keine Kleinigkeit ist.''' Gegens\u00e4tzliche Denkrichtungen, unterschiedliche Schwerpunkte, eine unvereinbare Sprache und schlie\u00dflich ein Wettbewerb um begrenzte Ressourcen und Aufmerksamkeit sowie Denkschulen, die in ihrer Bedeutung \u00dcberlegenheit beanspruchen - all dies sind keine Bausteine wissenschaftlicher Zusammenarbeit. Doch all diese Probleme sind vernachl\u00e4ssigbar im Vergleich zu der Herausforderung, eine gemeinsame Wissensproduktion zwischen Wissenschaft und Gesellschaft zu schaffen. Die Wissenschaft hielt eine teilweise arrogante Distanz zur Gesellschaft, oder zumindest ein Gro\u00dfteil der Wissenschaft vermied eine direkte Interaktion. Interviews, Beobachtungen, vielleicht noch Interaktion, waren bereits bekannte Ans\u00e4tze. Die gemeinsame Problemdefinition und das gegenseitige Lernen von Wissenschaft und Gesellschaft stellten einen radikal neuen Forschungsmodus dar, und wir stehen erst am Anfang eines Paradigmenwechsels, der die Wissenschaft seit Jahrhunderten gepr\u00e4gt hat. So entstand die trandisziplin\u00e4re Forschung als ein neuer Forschungsmodus und ein inklusiver, reflexiver und l\u00f6sungsorientierter Weg, der die Gr\u00e4ben in der Wissenschaft, aber auch zwischen Wissenschaft und Gesellschaft \u00fcberwindet. \n\nDiese radikale Entwicklung f\u00e4llt mit einer weiteren Revolution zusammen, die die Wissenschaft ersch\u00fcttert hat, n\u00e4mlich dem digitalen Zeitalter. '''Computer erm\u00f6glichten schnellere Berechnungen und neuartige methodische Ans\u00e4tze.''' Das Internet trieb neue Wissensquellen und -formen an, und die damit verbundenen neuen Kommunikationsformen l\u00f6sten einen Austausch zwischen Forscher*innen in einem beispiellosen Tempo aus. Alle Mittel der elektronischen Kommunikation, Online-Zeitschriften und die Tatsache, dass viele Forscher*innen heute \u00fcber einen eigenen Computer verf\u00fcgen, f\u00fchrten zu einer exponentiellen Zunahme der wissenschaftlichen Zusammenarbeit. W\u00e4hrend dies manchmal auch Opportunismus und eine Verschiebung hin zu Quantit\u00e4t statt Qualit\u00e4t in der Forschung mit sich bringt, ist es unbestreitbar, dass heute viele wissenschaftliche Informationen nicht weiter von uns entfernt sind als ein Mausklick. Technologie kann kein Selbstzweck sein, aber als Mittel zum Zweck erm\u00f6glicht sie heute ein exponentielles Forschungstempo, das sich am deutlichsten in der Corona-Krise manifestiert hat. Die globale Gemeinschaft der Forscher*innen ist in ihrer gr\u00f6\u00dften St\u00e4rke vereint, und die Geschwindigkeit und Vielfalt der Wissenssch\u00f6pfung ist in der Geschichte unserer Zivilisation beispiellos. Nie zuvor gab es mehr Interaktion zwischen den neuesten wissenschaftlichen Untersuchungen oder Ergebnissen und der Gesellschaft.\n\n== Weitere Infos ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html Mehr Infos] \u00fcber Kuhns Theorie der Paradigmenwechsel.\n----\n[[Category:Normativity_of_Methods]]"
                    },
                    "sha1": "rlrw2kcvf19h3qw8v5fxdqgk2yt86q6"
                }
            },
            {
                "title": "History of Methods in Sustainability Science",
                "ns": "0",
                "id": "453",
                "revision": {
                    "id": "6362",
                    "parentid": "5141",
                    "timestamp": "2021-09-10T12:03:20Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17197",
                        "#text": "'''Annotation:''' This entry focuses especially on Sustainability Science. For a more general overview, please refer to the entry on the [[History of Methods]]\n\n'''In short:''' Here, you will learn more about how scientific methods evolved and shaped the discourse in environmental and sustainability science.\n\n== ''The History of Methods in Sustainability Science'' ==\n[[File:Silent Spring.jpg|300px|thumb|right|'''Rachel Carson's Silent Spring''' initiated a broad discourse on the role of pesticides for ecosystems. Source: [https://liber.post-gazette.com/image/2014/04/12/ca0,3,1253,1708/20140413nextsilentspring.jpg|Post Gazette]]]\n\nOriginating in the [https://www.wissenschaft.de/umwelt-natur/die-entdeckung-der-nachhaltigkeit-3/ long-term perspective of forest management], many cultures had recognized sustainability in its original meaning since a long time. With the rise of colonialism and the industrialisation, the impact of human societies onto the environment propelled into previously unseen dimensions, and the negative effects of pollution were an early indicator that modern factories and coal plants were not only drivers of economic growth, but equally harming livelihoods and the environment. First accounts of the environmental damage were localised, and Carson\u2018s [https://www.britannica.com/topic/Silent-Spring \"Silent Spring\"] is an early account of a systematic yet widely descriptive account of the negative impact that human activities had on the environment. This localised perspective was quickly altered onto a global scale through the [https://clubofrome.org/publication/the-limits-to-growth/ Club of Rome]. Their discourse around Climate Change was a first step from a problem-orientated perspective (Reports 1-3) to a solution-oriented perspective. The [https://www.millenniumassessment.org/en/index.html Millennium Ecosystem Assessments] were equally hinting at the shift from describing the system towards a more transformative agenda by the follow-up institution [https://ipbes.net/ IBPES]. These global reports also outlined a pronounced shift in the focus towards a regional agenda, while the scientific landscape of Sustainability Science increasingly explored case studies. While these were increasingly rooted in the recognition and description of local aspects of a crisis, more and more studies implemented a solution-oriented agenda, indicating yet another shift in the mode of Sustainability Science. This frontier was most deeply explored by the [[Transdisciplinarity|transdiscplinary]] approaches developed over the last decades, which illustrated the importance of joined problem framing of science together with society, and the opportunities that arise out of such a mutual learning approach. '''All in all, Sustainability Science has shifted repeatedly in terms its underlying paradigms in order to arrive at its current agenda,''' and is - following [https://www.simplypsychology.org/Kuhn-Paradigm.html Kuhn] - truly emerging out of the recognition of a crisis. This article provides a brief overview on the developments that contribute to the methodological approaches that are prominent within the wide arena of Sustainability Science today. Since this arena is emerging, this overview cannot be conclusive, and is certainly not comprehensive.\n\n== ''The Revolutionary Start'' ==\n[[File:Limits to Growth Graphic.png|400px|thumb|left|'''\"Limits to Growth\" used computer simulations to illustrate potential future developments.''' Source: Limits to Growth, p.140.]]\nThe first or at least earliest relevant recognition of the global scale and the combination with inequality came from Karl Marx. While Marx did not elaborate on a methodological approach, his work is a foundation that opened a new empirical dimension. Economic data gained prominence out of the global trade rooted in colonialism, with the Dutch double book-keeping serving as a initial basis for a rising amount of money and numbers. [[Geographical Information Systems|Maps and surveys]] led to a global spatial understanding, and an early combination of economic purpose and spatial mapping was deployed in forestry, which integrated a long term perspective with a resource focus. Another contribution that slowly emerged as a necessary reaction to the industrialisation was the observation of pollution. An early prominent example was the Great Smog of London, which killed thousands due to specific weather conditions that trapped the pollution over the city. The description of the consequences of pesticides for the environment was yet another important step, as it already integrated many diverse obervations of researchers into one coherent overview, combining the role of pesticides regarding environmental damage with a better understanding of the effects on human health. Such systematic observations were made possible by the [[Why_statistics_matters#A_very_short_history_of_statistics|rise of modern statistics]] in the early to mid-20th century, triggering systematic approaches in research that first contributed to such different fields as astronomy and agricultural studies, but quickly dispersed into other research fields as well. At the time of Carson\u2018s \"Silent Spring\", information on the environmental impact of human actions was rising, partly also due to the often catastrophic effects of the [https://www.thoughtco.com/green-revolution-overview-1434948 Green Revolution]. These local observations were subsequently amended by the more global perspective of the Club of Rome and its report on \"The Limits to Growth\". Computer simulations played a central role to generate the main message of this book, highlighting an at this time highly innovative approach where modelling supported the critique on unlimited growth. With to the [[System Thinking & Causal Loop Diagrams|System Thinking]] approach behind the model, a new perspective - that is the interaction between the different problems addressed in the report - was taken into account in a complex model. '''The Club of Rome thus provided a first vital approach to address the problem of unknown futures through scientific methods.''' To this end, [[Scenario Planning]] provided another milestone, as it allowed to generate different perspectives about the future through strategic planning. These two methods - Computer Simulation and Scenario Planning - are defining breakthroughs in the methodological canon that takes a deeper focus on different futures. \n\n== ''A Change of Methods'' ==\nWhile the recognition of systems and the global increased, there was equally a revolution happening on the methodological scale of the [[:Category:Individual|individual]], since especially Psychology and Social Science started to look more deeply into the behaviour and attitudes of individuals. By trying to understand human behaviour on an individual level, severe societal debates were filed, and in the long run these would play a vital role in the unraveling of normative knowledge. [[Open Interview|Interviews]] and other methods to engage with the knowledge and perceptions of people were rapidly increasing, and diverse approaches enabled a broader understanding of the values, beliefs and goals of people. The feedback loops that this created with politics are prominent, and elections and political influence were whole new spheres of knowledge unlocked by these methodological approaches. Since the 1960s, science thus engaged more and more in normative knowledge of individuals, and cultural studies, [[Ethnography|Ethnography]] and Political Science contributed on a wider level. Observational studies and more diverse Ethnographic approaches grew equally in importance, and due to their long tradition, their application to the current problems was a logical next step. '''Investigating modern society thus became almost like a new perspective in itself'''. From a methodological perspective, many such approaches became highly creative, yet the harmonisation with the existing classical lines of thinking was and still is a methodological challenge. The field of studies focussing on indigenous people is an example of a stratum of knowledge that was started to get unlocked already a long time ago, yet novel developments showcase that there is still a lot of knowledge waiting to get unraveled.\n\n== ''Evolving to Connect'' ==\n[[File:Ostrom framework.jpg|400px|thumb|right|'''The Ostrom framework by Elinor Ostrom provided a system perspective on resource issues.''' Source: Ostrom 2009 ]]\nAn equally normative dimension, however on a different part of the system, is the development of Ecology and Conservation Biology out of Biology. Ecology and the pattern-oriented understanding of Biology emerged during the 1960s and 1970s into the new discipline of Ecology, focussing - among other things - on the patterns and mechanism of the accelerating biodiversity loss. Methodologically, this research built strongly on statistical models, however on the basis of often extensive data. This was also often combined - or at least not independent of - local interests to protect biodiversity. This goal to protect local and regional parts of the environment led to the ripening of Conservation Biology, which grew at least [https://www.jstor.org/stable/1310054?seq=1 since the 1980s] into an own branch of science. What charaterizes this field is the strong interchange between science and practical applications, since the results were often directly fed into management of ecosystems. '''Conservation biology thus provides an early testimony of a shift from a problem focussed towards a solution-orientated agenda''', yet this would only emerge later as a new paradigm within Sustainability Science as such. A similar shift can be described for a more resource-oriented, or even economic, perspective. Recognition of the [[Life Cycle Analysis|life cycle]] of products become a new baseline, and economic assessment became not only more holistic, but also gained a more pronounced recognition of global supply chains, potential harm to the environment, an emphasis on labour conditions, and a longer-term perspective on the product at hand. The recognition of a more circular economy tilted the previous production [[Levels of Theory|paradigms]], and novel paradigms such as the ''[https://c2c.ngo/ Cradle to Cradle]'' concept offer a completely new line of thinking. Elinor Ostrom designed a shift away from the [https://www.britannica.com/science/tragedy-of-the-commons Tragedy of the Commons], and developed a [http://www.dpi.inpe.br/gilberto/cursos/cst-317/papers/ostrom_sustainability.pdf framework] that moved away from classical economical models when it came to resource management. From a methodological standpoint, this was a dramatic development, since it allowed for the integration of global responsibility with a [[:Category:System|system]] scaled analysis. By combining different parts of the system, a more holistic perspective was enabled, and this remarked the necessity to combine knowledge from [[Design Criteria of Methods|different schools of thinking]]. This indirectly enabled other approaches such as the social-ecological system approach. Knowledge about ecosystem dynamics became more integrated with the emergence of the [https://www.britannica.com/science/ecosystem-services ecosystem service concept], which focuses on the benefits people perceive from nature. Conservation, ecosystem services research and system-based approaches are all prominent examples of areas of science that often build on spatial data and analysis. Satellites and the availability of data from spatial planning enabled interlinkages between scientific analysis and management. This also enabled perspectives that had previously been concealed to our observation, such as the spatial spread of the [https://earthobservatory.nasa.gov/world-of-change/Ozone Ozone hole].\n\n== ''A Solution-Oriented Agenda'' ==\nBy putting the environmental concerns more strongly onto the political agenda, the report from the [https://sustainabledevelopment.un.org/content/documents/5987our-common-future.pdf Brundtland commission] was another vital stepping stone towards the field of [https://science.sciencemag.org/content/292/5517/641.summary Sustainability Science], and a broader recognition of the endangered environment. The [https://www.ipcc.ch/ IPCC] provided an upscale by focusing on the global dynamics of climate change, building on complex models, and the development of potential future scenarios under different pathways. Climate modelling - deeply rooted in the well-established meteorology - went much further in time, building on complex simulations that enabled a better understanding that the impact of climate change might unravel onto the world. '''What is concerning is that almost all IPCC models were less drastic than the reality that we faced decades after the first reports.''' In other words: many of the future models were more conservative than actual reality. From a methodological standpoint, the shift from a more descriptive to a more actionable focus in Report 4 indicated the most dramatic shift, as it highlighted that adaptation is now - according to the IPCC \u2013 one important step to cope with climate change in addition to lowering emissions etc. This led to a shift from a more global approach towards a more localised agenda. At this scale, chemistry was operating already with a stronger focus on the environment and a better understanding of pollutants. Measurements and the modelling of pollution hence gained ever more levels of detail, with an increased detection, and a continuously improving understanding of the interactions of pollutants in ecosystems, and the effects of these pollutant on people. \n\n== ''Emergence of a New Arena in Science'' ==\n[[File:Millenium Development Goals.png|250px|thumb|left|The '''UN Millenium Development Goal'''s provided a systematic, global and solution-oriented perspective on social-ecological issues. They have been replaced by the Sustainable Development Goals in 2015. Source: United Nations, [https://www.alliancemagazine.org/wp-content/uploads/2017/11/MDGs.svg_-1.png Alliance Magazine]]]\nSustainability Science thus implemented into its repertoire a diversity of data sources and methodological approaches, with a clear focus on enabling solutions and a large recognition of harmonising diverse data and results. The Millennium Ecosystem Report provided an already deep focus on diverse levels of information, yet the [https://www.un.org/millenniumgoals/ Millenium Development Goals] were the first approach where all topics and disciplines were enabled into a united perspective through the sense of justice. Such a Herculean effort was at least partly enabled by the computer revolution, since the means of [[Glossary|communication]] and data analysis and interpretation led to an exponential growth of research. [[Machine Learning|Machine learning]] and artificial intelligence are just two methodological approaches that may generate more research in the long run, and prove how digitalisation is way beyond being a mere tool or just more advanced typewriters. Science was however still mostly interacting with itself: different fields of science were collaborating, but what was widely missing was the recognition of science and society [[Transdisciplinarity|working together]]. '''The new mode of transdisciplinary research consists of a deep regime shift in science, as it builds on the framing of joined knowledge exchange between science and society.''' This mode of research is not a method, but certain methods such as [[Actor Network Analysis]], Stakeholder Workshops and Scenario Planning are characteristic parts of many transdisciplinary projects. On a [[:Category:Global|global scale]], an equally integrative approach is more and more emerging, with new global initiatives such as the IPBES as certainly more inclusive and diverse compared to previous institutions. This is not so much a criticism of previous approaches - which were after all embedded in their time - but an optimistic sign that times can change, if slowly. The IPBES is a good example how the broad knowledge from the scientific canon is combined to represent the current state of the art, but also to design future research. Such [[Meta-Analysis|meta-analytical]] approaches already evolved in medicine and psychology decades ago, yet the key challenge will be now to combine diverse case studies into such an analytical framework. An equally important but completely different challenge arises out of the diversities of data on the one end, and the different perception that diverse individuals (or groups) have of individuals. [[Glossary|Art]]-based approaches, [[Research Diary|research diaries]], theatre workshops and accompanying research are just some few examples that illustrate the long but exciting journeys that were already started, with much knowledge that waits at the horizon. Unlocking the potential of new and existing pathways to knowledge will not only define the frontier of science, but may ultimately enable a way out of ignorance, and to a truly common future.\n----\n[[Category:Normativity of Methods]]"
                    },
                    "sha1": "bn43ul2eew820ksfzczq1tey8ccnkhz"
                }
            },
            {
                "title": "How long do you store data?",
                "ns": "0",
                "id": "82",
                "revision": {
                    "id": "5765",
                    "parentid": "5733",
                    "timestamp": "2021-06-13T13:47:59Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3247",
                        "#text": "'''10 years.''' This is the most common answer I  get whenever I ask someone what they think is an appropriate amount of time for data storage.\n__NOTOC__\nIn most cases, there are no set rules for how long you should store [[Glossary|data]] for \u2014 there are only rules of thumb. While 10 years might seem a very long period of time,  the decision on the duration for storage of data requires some consideration. Below is a non-exhaustive list of factors that should be considered.\n\n=== Legal ===\nAs data plays an increasingly important role in our day-to-day life not only as researchers but also as individuals, the issue regarding data privacy and security has become more sensitive. As such, governing bodies (of countries or union of countries) have been taking more active steps to address this issue. Specifically in EU (as of November 2019) GDPR mandates that in case of personal data only the dataset that is immediately usable is stored, and only for the duration of time that the data is used for. There are further intricacies that are important to have in mind. As such, keeping yourself aware of the data privacy and security laws of the area where you and/or your stakeholders operate in is very important.\n\n=== Organizational Policy ===\nSome organizations have a strict policy that determines how long the data should be stored for. These policies generally already account for the current legal landscape and hence make deciding how long to store data for quite straightforward \u2014 you store the data for as long as the organization mandates that you store them for.\n\n=== Nature of Research and Data Set ===\nSome researches are more sensitive than others. For examples, consider that you are performing research in medicine industry where you work with patients' data. In this case, the data you have access to, what you do with that data, and the next steps your organization takes based on your work are all incredibly sensitive. Compare that to another situation where you are an engineer that wants to learn some process and are work with simulated data. Here too, your work is important and carries real consequences in the future. However, one of the biggest differences in the two aforementioned contexts is the data involved. You would naturally have to treat data on peoples' health issues and behaviors more importantly than you would a simulated data.\n\n=== Method of Storage of Data ===\nIf you can ensure that you can keep the data you use for your research private and secure for however long you have to store the data for, then adhering to the organizational policy (if available) or the 10-years-long heuristic (if no other guidelines are available) are fine. However, if you lack skills or resources to ensure data privacy and security, then you should reconsider long-term storage of data. Either you have to use services that ensure data security for you, or you will have to purge the data as soon as its purpose has been fulfilled (which is not ideal).\n\nAs mentioned earlier, there might be more factors that one needs to consider depending on the situation.\n\n----\n[[Category:Statistics]]\n[[Category:Skills and Tools]]\n[[Category:Software]]\n\nThe [[Table of Contributors|author]] of this entry is Prabesh Dhakal."
                    },
                    "sha1": "eli5xsjki49w0ml930thsx4t30dhftt"
                }
            },
            {
                "title": "How to PhD",
                "ns": "0",
                "id": "552",
                "revision": {
                    "id": "5957",
                    "parentid": "5852",
                    "timestamp": "2021-06-30T20:34:47Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* First, your supervisor */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "55007",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n'''In short:''' This entry provides guidance for anyone who's planning or already engaging in a doctorate position, based on experiences made by Prof. von Wehrden throughout the last years and decades.\n\n== I - To PhD or not to PhD? ==\n====Considering your options ====\n'''First of all, it is difficult to know whether you really want to become a PhD student.''' Back in the day, I did not want to do a PhD. I was basically done with University. Then my future supervisors gave me a call, with the words: \"This is a call from fate\". I feel he was totally on point, spot on. For me, doing a PhD was serendipity. Not everybody is always totally clear whether doing a PhD is the right way. Others are clear about it already while doing their Bachelor, and always claim they want to do a PhD. Consequently they do not need to ponder much about it. What is however tricky is the group of people who do not really know what to do in general, and just do a PhD, because, why not? While it is save to say that everybody needs to find their way in doing a PhD, it is difficult if you start a PhD because you do not have any alternatives. Do not misunderstand me: many of the best PhDs will come out of this group of people who lack alternatives. However, some will realise halfway that maybe a PhD is not the main goal in their life. This is perfectly alright. However, ideally you should make up your mind as early as possible. I wish I could offer some suggestion on who should ideally opt for doing a PhD, yet I feel that academia is built on diversity, and hence there is no universal recipe for a PhD, let alone for a successful PhD student, that is. \n\n==== Funding and positions ====\nWhat is however relevant for your decision to do a PhD is that you get [[Glossary|funding]]. If you apply for a project position, you will have funding, but there is also a clear expectation of what you are supposed to be working on. Quite often, you are part of a bigger team, and this demands not only a will to cooperate, but also a clear responsibility and reliability towards your other fellows. In my experience, there are quite some positions out there in exciting projects, but the collaboration in such projects is then part of your PhD, and you will constantly have to negotiate between your goals, and the goals of the team. While this sounds easy in theory, it is often a hole in which many work hours are buried under a pile of emotions. On the other hand, you may be the only PhD in a team working on a specific topic. There are for instance quite some scholarships available, and while I never understood the decisions about which PhDs get funding, and which are not funded, these scholarships have at least some sort of a stochastic chance to gain you funding. These scholarships are more independent, which then becomes a problem down the road. Many people believe that academia is about being part of a larger team. While this is in some sense true, because academics can work in teams, this exchange in teams will not take away the necessity that it is you who needs to shoulder the bulk of the workload. This workload is at the heart of any PhD, even if you want to contribute to a larger narrative, you will have to mostly work alone. This is something that almost all PhDs struggle with: to accept that it is them who will have to do the work, and no one else. As a supervisor, I am willing to go all the way with you, but I cannot go the way for you. You have to learn to walk this path alone, a supervisor is just for the general direction. '''A PhD is ultimately about learning to overcome yourself, not for others to help you overcome yourself.''' Others - your fellow students and supervisors - may be catalystic, but the learning is all yours. Embrace this early on. Everybody struggles with it, so I felt it is only fair if I tell you this early on.\n\n==== What makes a successful PhD student? ====\nHowever, there are some character traits that I observed in the past that created successful cases. First and foremost, I think that previous PhDs I know were often thrilled and excited about their research. This showed in many different appearances, but overall, many of these folks were basically quite driven in terms of their focus. You know it when you see it as an external observer. However, you do not know it if you are one of those driven people themselves. Instead, most potential PhD students are full of doubts, focus on their wannabe limitations, and are often never sure whether they are on the right track. Therefore, it is often a good indicator if successful researchers consider it a good choice for you to pursue a PhD, and are willing to support you. A PhD is more about what you could become compared to what you are. Learning is relative. Taken together, if an experienced researcher is willing to support you, and you feel you want to commit the next years of your life to work deeply on one topic, and mostly for yourself, then you may be the right person for a PhD. This brings us to the next point.\n\n\n== II - Finding a supervisor ==\n[[File:MSDEMST_EC002_H.jpg||thumb|right|'''Find a good supervisor.''' Source: [WIRED https://www.wired.com/2015/02/teaching-students-like-yoda-taught-luke/]]]\n\nMost of the time, you do not find a supervisor, ideally you find each other. I feel that a good supervisor knows that you are the right choice, or how my PhD supervisor summarised our first encounter: \"At this point, I knew I would not get rid of him\". I believe, doing a PhD is about learning, and equally is the supervision of a PhD about learning, that is, learning for the supervisor. There are several steps that you can take to pursue your academic life so that you and your future supervisor have an encounter leading to a PhD. First and foremost, leaving a good impression in a course is always a good idea. Many of my PhD students were students in our Bachelor or Master programs beforehand. Memories of good students are always vivid, and in my experience I can tell quite quickly whether a person is a good fit for our team, and a PhD. However, I also frequently hire external PhD students, and finding a way from outside is a bit harder. I believe that most of my colleagues and me get several dozen PhD applications form external students per year, often from foreign countries and with the question for a position. Unfortunately, this is not possible as most positions are related to specific projects, where there exist already clear ideas about the focus and goals of the projects, and positions are written out that people can apply for. In this case, you apply for a very specific context, and you should be clear what is expected from you. \n\nWhile I personally encourage my PhD students to give a position their own specific touch, there are clear exceptions from funding bodies and other project partners, with time frames, deliverables and milestones. If you apply for a position, try to learn about these things, and also learn about the team that you apply for. How are interactions facilitated, what about joint identity, and how is the supervision organised? All these things are valuable to know, and need to match your expectations. '''Many potential PhD students envision that they will sit with their professor all day, and will discuss things that sound important, and somewhat scientifically.''' This is not how reality looks like, I think it is closer to a cognitive apprenticeship, with a higher frequency of exchange in the beginning. In most cases, professors have ideas, and want their PhD students to pursue these ideas concretely. There may be regular interactions, but as long as you are not the only PhD student, there will be hardly daily active exchanges. People I admire meet their team on a weekly basis, yet professors have teaching duties, admin work and after all often apply for the next funding, and also have larger teams beside you. Finding a supervisor is hence often about managing your own expectations. However, in my experience, there is more. \n\n'''I think the most important thing a supervisor can do is to motivate and to inspire.''' I have been very inspired by my supervisors and mentors. I think to me it was most important that my supervisors gave me direction and advice, not necessarily general advice, but more tailor-made for me. I have a rather old-fashioned idea of a supervisor being like the teacher in a Kung-Fu monastery: relaxed, effortless, but incredibly versatile if challenged. An inspirational supervisor makes mastery look easy, effortless and incredibly elegant. The moment of bafflement when the learner attempts to do the same is the main inspiration, challenging you to become a better version of yourself. Beside this inspiration a supervisor also provides a role model when it comes to structure in life (or at least that happened for me). More experienced people somewhat got their life quite in order, and the mixture of experience plus more structure and established habits is a big boost for productivity. I admire experienced researchers and teachers in how they make much look effortless, while especially today it seems like a badge of honor that people are busy. It was my supervisors and my idols that showed me that most people are not super-busy, but badly structured and unable to focus. \n\nThe last but not least thing a supervisor can bring to your development is specific advice. We all have our little twerks and insecurities we need to adjust, and there may be even bigger hurdles to overcome. My first professor basically took one good look at me and addressed my biggest flaw, quite bluntly. I am still thankful to him - at the time it certainly was not easy, but necessary. My supervisor was equally directly showing me not only flaws and blunders in my thinking, but also explained the academic system to me. Understanding hierarchies, norms and structures in academia takes quite some time. Young people are often impatient, yet it is also highly relevant and overdue to change the academic circus. A progressive supervisor will know how to help you on this path, but equally realise that the main goal of supervision is to aid not only your students to become better, but for the whole academic system to become better. \n\nMost people these days are driven by topics, and want to make a difference with their work. While this is understandable and admirable, I think it is only a secondary goal compared to the main goal of any given PhD, and that is overcoming yourself. Looking back at my PhD, I loved my topic; it was about conservation, rangeland dynamics, and the Gobi desert. It was incredibly cool to do, and even had a real impact on the ground. I loved my PhD topic. But coming to think of it, my PhD was mainly about learning to overcome myself, and to learn key academic skills, which were in my case revolving around methods. I had interest in my topic, and my supervisors and colleagues were pivtotal to this end as well. But looking back at it 10 years later, it was more about learning to work deeply on a topic, any topic. Ever since, my focus has shifted. Many researchers continue to work on their PhD topics for their whole careers, which is kind of standard in the old days and in the normal sciences. Today, this seems to be vastly different. Your PhD may ramp up your live goal, but more often than not, people change their focus, sometimes repeatedly. \n\nTo summarise, you should try to find your way into the limelight of a supervisor early, and try to find for yourself what you expect from a supervisor, and what a supervisor can expect from you. In this way, you get a clear understanding about roles and expectations. These will change later on for sure, but can be an important door opener in an interview, and may enable you to find your way in a new role as a PhD student. '''Earning a PhD is among the greatest privileges of academia, and is for me only surpassed by the privilege to supervise a PhD.''' A PhD is about years of a life of a person, and that investment of a person counts for something, and then some, I would say.\n\n\n== III - Reading and designing ==\nNo research starts from scratch, but instead all research is standing on the shoulders of giants. We are part of a multiverse and a continuum, and no matter how small our own contribution may be, it is part of a bigger picture. This is the most important point of departure for any PhD, because it highlights the importance of reading the literature your work is built upon. ''What are the foundations of the topic? Who are central figures in the debate? What are the central theories of your work?'' All this you should read up upon, because it is expected that you become the expert on this small turf of current research. The goal of a PhD is to become an expert in one very specific part of current research, and to move it forward. '''This is a problem for many, because I believe that you hardly ever feel like an expert.''' I certainly do not feel an expert in anything, and most PhDs feel that they are predominately gigantic experts in procrastination. We have all been there. \n\nStill, consider the topic and theories that you read your way into, and try a little visualisation technique. Image a circle around you, maybe of about 100 kilometres. How many people would you think are surpassing your expertise regarding the very specific topic? Coming to think of it, quite few, probably. You become an expert. I for once became an expert about the vegetation of the southern Mongolian Gobi, and its dynamics. Granted, a very singular area of expertise, but there are a few handful of experts to this end, and I had to learn to acknowledge this in my PhD. More importantly, one needs to own it, and this takes time. Try to develop a strategy how you read papers, make a plan, and first and foremost try to learn to crossroad papers fast. You need to become able to grasp the main points of a paper quickly, especially the points that are important for you. Learning to get an overview of a field of research is a pivotal skill for any researcher. \n\nLearning to read is a means to an end, and this end is to inform your own research direction and design. By building on previous knowledge you learn to identify gaps, and become able to verbalise existing questions that remain unanswered. Then, you can start to frame what you want to do and how you want to do that correctly. No one is an island. Other people will have done some parts of what you want to do before - it is often the recombination of information that is the really new and innovative step. Try to understand where the frontier is in your research area, and be bold enough to push this frontier a bit further. I often advise people that if your topic is innovative, you may want to have your underlying theory and the methods you utilise to be maybe not on the bold end of the spectrum. If the theory is bold, maybe a solid topical foundation can make it more tangible. If everything about your research is the unexplored country - to build on Shakespeare - then it will be very hard to build something tangible. In order to be bold and innovative in some parts of your research, I often feel it is helpful if other parts are solid and somewhat established. In a PhD, it can be good to depart from the state-of-the-art, and then progressively move this forward. The next section will deal with the potential architecture of your thesis.\n\n\n== IV - Getting the first paper done ==\nThe first paper is the hardest, always. I remember how much effort I put into my first paper, and it felt as if it would never be done. I feel that here the supervisor can potentially help a bit more, while towards the end of a thesis the candidate should ideally become more independent. What is safe to say from my viewpoint is that we all need help at some point.\n\n'''I think all PhDs need three types of skills: Getting data, analysing data, and interpreting the results.''' Ideally, a PhD is more or less able to master at least two of these steps, and may need a bit more support by one of the other steps. Most PhDs are excited about getting data, a step that is often easy to begin and hard to end. This is why it is helpful to have clear designs and to fixate these in more or less binding written outlines. Writing a clear structure of your thesis in the beginning is helpful. This outline and structure will often be altered, but in my experience is it helpful to force your way through this, and this comes from someone who did not do this, which was clearly a mistake. I would say give it a try, have it critiqued, and then be open-minded to change it. But see it like a band aid: you want to rip this off fast.\n\nThis brings you to the first paper. In many institutions it is vital to have at least one paper published, and the typical three years you have for a PhD fly past fast. Since peer-review takes time, it is vital to get the first paper done rather quickly, and submit it. It may circulate for some months, or in severe cases even more than a year at the journal with editors and reviewers, so go figure what that means for the three year timeline of your PhD. Therefore, I often recommend that the first paper could be a [[Systematic Literature Review|systematic review]]. You anyway have to read all the available literature, so why not do that in a systematic sense. I acknowledge that I like reviews a lot, and approach many topics through reviews, yet in your PhD it also has the benefit that no one can take all this knowledge away from you. It can be quite comfortable to stand on the shoulders of giants, and in your defense you will be able to have references woven into your replies with ease, which people will appreciate. I also think it is quite helpful to write a systematic review to define your point of departure. All things considered, the outlook of the review may ramp up your agenda for the PhD. \n\nAn alternative way is to make a pilot study or a pretest of you next paper, and make this a small but precious first paper in your PhD. Such a small empirical paper can be an equally good starting point, as it is often quite easy to structure such a paper, and to get into a flow in terms of writing it. Here again, an outline is ideal, because many people are tempted to make such a paper more than it actually is. Remember that you start, and more can be pursued in the next papers. \n\n'''What is also important to consider for your first paper is to identify a target journal.''' ''Which journals harbour the most literature on the topic you are working on? Which journals seem appealing in terms of the style, focus and audience?'' And maybe you want to consider a small gambit, and aim a bit higher? Depending on your time line, you might make the first submission a little bit higher, knowing that you have a sure shot at another journal that is slightly less prestigious? To this end, it is good to rely on the experience of your supervisor, and fellow PhDs and other colleagues. Some [[Glossary|journals]] demand a specific format and structure, and this should be considered early on. \n\n, in order to get this first paper done, you need to develop a mindset where you want to get this paper done. This is what the first paper is all about: starting to learn to overcome yourself. Try to keep it simple, try to establish writing as a habit, and also try to gain experience. At this state, many people claim to be bad at writing. I think this is simply not true, they are only untrained in writing. Writing takes practice, and I am not only talking about putting words on paper. I am talking about creating a structure and taking decisions. Writers take decisions. Hence you will need to practice, and this is what the first paper is for. You will have to learn to get your work into smaller chunks. If you write a quarter of the introduction of a paper in a day, then you should be done in two weeks if you keep up the pace. Since structure and momentum are key challenges, try to make a bullet point outline, or discuss the structure of the paper with your supervisors or your peers. If you ask me, try not to focus on the flaws, but try to get a full first version onto the paper. This is the hardest thing, but being in the limbo of iterations is something that will happen anyway, so try to write the first version with your heart, and then try to get it criticised. This is not about being perfect. This is about finding a way to start, which is exactly what the first paper should focus on.\n\n== V - The empirical gap and/or trap ==\nWhile reading and designing the potential studies is one vital first step for any given PhD, getting empirical data (or engaging with conceptual depth in case of a non-empirical PhD) is the real deal. Time to get your hands dirty. The key challenge for any empirical work is that there are some things that can be planned and designed, and some other things where you have to embrace a healthy form of improvisation. The gap between these two is the source of endless stories of countless researchers across the globe. Planning what you want to work on needs to build widely on experience and previous research, hence consulting expert researchers, discussing your design with your supervisor, and first and foremost reading the relevant literature is the most important basis. \n\nNow imagine you started gathering data, for example you are interviewing actors in a system. In my experience, most researchers will now enter the ''what if'' stage, where they either start to amend their design with all sorts of additional modifications, or where they start beating themselves up about what they could have done differently, or what they would do differently next time. This is normal. First of all, negative results are also a scientific results, although of course this is way less thrilling, and most research is biased toward positive results. Second, no results explain 100 %, a model or outcome that explains everything is certainly flawed. It is in the very nature of empirical research that it has flaws, and variance and complexity add to lessen the results. Within qualitative research, it may surface that with more samples - for instance more interviews - the results may be different, and many researchers frame this different as \"better\" in their heads. Again, this is only partly true, as we have to acknowledge that any type of empirical research is only looking at parts of reality. Still, I sometimes wonder whether most empirical researchers beat themselves up about their data approximately as long as it took them to gather the respective data. This process is sometimes refered to as \"analysis\". While experts in methodology would agree that proper analysis of empirical data - both quantitive and qualitative - takes as long as the data gathering, most of this time is dominated by doubts, regrets and the predominant wish to be able to travel back in time to do everything again. Learning that your data is flawed and could have been better is an important lesson, and the lesson is not as much about improving your knowledge to avoid this specific mistake in the future. '''Instead, this is about learning to live with the imperfections of the empirical'''. For conceptual work, this can be equally framed. Often, you discover the most important source towards the end, or even better, after you finished your work. Conceptual work has its own demons, and I think it is overall much easier to work empirical first, and with a focus on conceptual contributions later, but here I may be mistaken. As a philosopher, you will probably work conceptually from the very beginning. I admire this work, but is demands a high level of coherence, which is why it can be relevant for any given PhD student - including empirical workers - to derive a glossary of your most important words early on in your PhD. I wish I would have done this, as it would have saved me from iterations and inconsistencies. Better to reflect about a term once and deeply, and then settling on a compromise that you stick to for the rest of your thesis. Building such consistency is I think an underrated skill in most PhD works. \n\nTo summarise, you should consult experts on your empirical design, because nothing is worse than a failed design. However, do not expect something magic out of that: empirical sampling is work, harder work, and failing. Just try to get at it, and enjoy it while you can. The head of my PhD lab always claimed what a privilege we all had, since us PhD students only focus on one thing, and one thing only. I think this is so incredibly true coming to think about it now, but at the time it did not feel like it. Life as a PhD students is often stressful, but always remember that is serves as a basis for great stories later in your life. Glorious days!\n\n\n== VI - Gaining flow vs procrastination ==\nNow let us assume you have your data, you are on your way towards results, and then you get stuck. What if you make just another round of fieldwork? How about one more campaign - a short one - just for a few more interviews? And did you see this great summer school that was advertised? It is not exactly your focus, but it sure sounds exciting, right? Maybe it would help you to gain some friction on your analysis, and then you could already join this years round of conferences to present some results. The one in Prague sounded really interesting! '''Welcome to the world of procrastination.''' May this stage be a short one, but it never is. You kind of started, technically you know what to do, and now you feel there are so many exiting opportunities that you need to seize as a researchers. And there are some plants that seem to need watering as well, right? Or how about a little snack? \n\nI believe that the [[Glossary|creativity]] of most researchers I admire is maxed out when they try to procrastinate. I have been there, and I am still there, sometimes. The world is full of exciting alternative opportunities when you should be actually writing your PhD thesis. What is most important now is that you learn to manage yourself, and this form of self-improvement starts with building consistent habits. Try to develop a daily rhythm that works for you. Me, I like starting rather early, doing some Yoga, writing the first text before 8, making it a rather energetic morning, taking a break at midday, preferring a nap for a lunch break, and setting meetings into the afternoon when energy levels are lower. In the evening, I might take another round of concentrated writing for some hours, or maybe some sort of mechanical tinkering such as some statistical analysis. My day was not always like this, but today it is planned down to 15 minute blocks, focussing on balancing my energy levels, and trying to build a pendulum between motivation for challenging tasks vs gratification with some of the nicer tasks. Keeping my energy levels constant is I think my main goal these days, preferring to avoid both extreme peaks as well as total slumber. '''Gaining flow in writing is a great challenge, and takes livelong practice.''' I like to start with a clear designation that I start writing in this moment. Either I make myself a coffee, have a location change, or I have the time clearly designated in my calendars (a trick I only learned recently). Most writers have deeply ritualised habits, and are often working to improve them even further. For a long time now, I have been writing with music or background sounds. I have almost a dozen nature sound apps on my phone, and Philip Glass, Dawn of Midi and Max Richter are my staples when writing. These days I get hardly distracted by this music, but instead is actively prevents my mind from wandering, and allows me to create text for long stretches of time. Another helpful step is to carve out the general structure, and then fill this up. Me, I like to structure my texts in bits of about 200-300 words. Everything more detailed is taking too long, and everything less detailed does not to justice to catching the overall structure. I encourage you to try things out and to try to develop your own habits and strategies. \n\nIn order to be able to do this, you need to practice. Writing and gaining flow at your work is not something that you learned in school, I guess, and most people have not produced a few hundred thousand words yet in their life. Academia communicates widely through writing, and learning to structure your thoughts in writing is one of the essential goals in a PhD. If you start by writing about 1000 words per day, you will see how this can ultimately bring you into a state where you are quickly able to structure your thoughts and fixate them on paper. Many prefer actual paper, yet most use their computer. I learned that a comfortable hour in a garden chair with my mobile phone can well produce 2000 words that propel me up for a good start. I often write a lot in the very early hours of the day, just as now. Late at night is another sweet spot, as there are less distractions. Find these niche spots, and write. Get at it. You are not a bad writer, you are just an untrained writer. '''No one would expect to grab a musical instrument for the first time, and play music right away.''' You need to practice in all you do, but I think the reason why the hurdle is higher in writing is because the actual form of scientific papers seems somewhat distant from us and our everyday life. Yet in order to do a PhD, and to gain recognition for your future career path, you need to ''publish or perish''. There is - in the current system - no way around it, and in my experience PhD students who question this simple path dependency are literally doomed to perish. To spin it less dramatic: I would argue that other people deserve to read your work. You are being paid to do research, and your results should contribute to the greater good of science. Typically, your salary is not so bad - if you write 3-4 papers in your PhD, consider the costs of one of those papers. Scientific papers are certainly not cheap, but they are necessary to make your work transparent, and to enable other researchers as well as society to learn form your insights. I think it is safe to say that peer reviewed publications will be the baseline of science for quite some time to come, hence they should be the baseline of your PhD as well. \n\nThis bring us to the next point: the role of your team. Most PhD students are part of a team, and that is a good thing. While I already said that it is you who is writing your PhD, and no one else, I think that getting feedback from your fellow PhDs, and learning how you can collaborate with others, is actually a core academic skill. Still, I think every paper should have someone who is in control, literally leading the others. The skill of mastering a multi-authored paper is not about others taking the workload from you, but you designating smaller parts to people with a complementary expertise, and you integrating their chunks. The bulk of the workload is with the lead author. Equally, a paper you co-author can be no excuse to not work on your papers first and foremost. Your papers should always be your main focus and only when these are done you can shift your energy to something else. '''Focus!''' I know it is so tempting to work on something else, but you may want to consider to have a clear plan how to divide your energy. Working for four days on your PhD and one day on a co-authored paper would be a reasonable division of your label force. Then the co-authored paper can become a reward, but make no mistake: You will have to shoulder the hardest part of your work by yourself, there is no way around it. Learning to gain flow in your own work is an essential academic skill. Try to put yourself in the shoes of your supervisor. I for once had to learn how to gain flow in my work in order to become able to try and inspire others, and to help them on the way. I can only write these lines now, because I learned to focus on my work at some point. In order to be able to supervise and inspire others in the future, you need to become able to gain flow and to prioritise your work above all else. In order to achieve this, focus is pivotal, which brings us to the next points.\n\n== VII - Get rid of (almost) everything ==\nYou know a lot now, right? You read all the literature. You are pondering about all sorts of theories and frameworks, and then some. You learned your system inside out. The model of your working area that is in your head is at leats ten times as complex as reality itself. No other human has ever comprehended what you learned about all this during the first part of your PhD. Now you need to explain it to others. \nThose who know me understand how fond I am of Occam's razor. ''Everything needs to be as simple as possible and as complex as necessary.'' Empirical research is about models of reality. Hence it is your work to reduce the full model of your results into a tangible version to follows Occam's razor. Every piece of information that you reduce is like a small death, to give this line of thinking a dramatic spin. What is important, what is not important? You are the judge. Whoever made you the judge will not be there to help you, because no one is supposed to understand this stuff as good as you. You are the expert, yet you feel a million miles away from understanding anything. How to proceed, you ask? Let us go through it point by point.\n\n==== The theoretical foundation ====\nScience often builds on theories, paradigms, framworks or concepts. While all these operate on different levels, it is still common that you identify one or several conceptual foundations that your work resolves around. Most PhD students are quickly falling in love with several concepts, and encounter problems when deciding which concepts are valuable for their work, and which concepts are less important. Theories are simplifications of reality, and hence imperfect. If you deductively try to falsify or confirm a theory, the theoretical foundation is very simplified, but easier to handle. Research over the last decades has had a general tendency to be more inductively, or even abductive. To this end, a theoretical foundation may give clear boundaries within which you work inductively. Take for instance the stakeholder theory, which you can test in all sorts of different settings or systems, but you always integrate a resource-based as well as a market-based view in your research. '''What is sure about theoretical foundations to this end is that several of them together are difficult to integrate.''' If you work on ecosystem services, focussing on biodiversity, building on neutral theory, investigating resilience, to investigate climate change adaptation, then you got yourself into a deep mess. Theoretical foundations have a tendency to exponentiate each other. If you have two theoretical foundations, you may get a vague one concrete, if the other one is concrete. Personally, I would leave integration of more theoretical foundations to philosophers, or at least to conceptual papers. In empirical research, more than two theoretical foundations are more often than not confusing. Still, many PhDs have some sort of a fear of missing out concerning theoretical foundations, the more the merrier seems to be where many get stuck. Never forget, they they are all only models of reality, and equally you will have to compromise which theoretical foundation(s) work best in your case. \n\n==== The empirical results ====\nSo you have your data, and now you dug yourself a hole in the world of analysis. May it be the never ending coding in a [[Content Analysis|content analysis]], or a statistical abyss - there is a long tradition to bury yourself in your own empirical results and their analysis. '''Remember that you are looking for the most important information now.''' Not everything, but the main patterns. What do you think the world can learn from your data. Many will iterate themselves though all stages, from ''do I got anything at all'', ''how can I every make sense of this mess'', ''it is dead, Jim'', ''A New Hope'' to finally ''how about I leave it like that and hate myself forever for it?''.\n\n'''Empirical research is messy.''' There is no mincing words here. Yet it is learnable. You can learn to live with it. Experience in empirical research almost never feels good, I think. It is either a total challenge, or being absolutely bored. Strangely, there is hardly anything in between, only the feeling of being totally overwhelmed, or the weird querkiness of thinking that is just cannot be that simple, and all this is super trivial. Get over it. What you should learn in the long run is that you may want to embrace your results, but still be critical. Results may vary. Even patterns are hard to be reproduced these days. Remember that we unlock [[Bias and Critical Thinking|parts of reality]]. Stand by what your data can do, but always keep an open mind to the simple truth that it will change probably in the (very) long run, because hardly anything is constant. You work is a contribution in the here and now, and I feel this should be enough to get it done in the here and now.\n\n==== The contribution to the topic and the state of the art ====\nThese days, much of science is driven by topics. People want to make a difference. To me, topics are mere samples. Do not misunderstand me:  some topics are timely, and some are more important than others. But when you look at your PhD one or two decades later, you will know what I mean. Your interest will necessarily shift, yet what will remain is the skills your learned and the experience you gained. Taking your thesis to find your way into a proper Philosophy of Science is something that will surely stick. Developing your work ethic will surely stick. But this section is not about all that. It is about your topic. You have to become the expert on the respective topic, so much we already agreed upon. You will be the expert, and you need to break the topic down in order for others to understand it. What are the most important pieces of information that build as much as a whole picture? How can you frame your knowledge into a narrative? Do you have an [[Elevator Pitch|elevator pitch]] to tell someone the really short version of your thesis? And what if you get stuck in an elevator with the president of your University, do you also have a longer version? Developing all this takes time, but you need to get at it, and test your expertise then and again. \n\nMost importantly, are you also able to share your excitement about your PhD with people outside of academia? Many people within academia will measure your impact by the journals you publish in, but these days impact is measured in many facets, and driving change in society is of growing importance, and rightly so. What is however difficult to grasp are the outer margins of your topic. Try to work hard to define what you are working on, and what you are not working on. Many people get dragged into a rabbit hole where they try to shoulder everything, but it is equally important to know what you are not working own, as to know what you are working on. Ideally, you can choose and reframe along the way. The typical PhD student changes their concrete focus quite a lot along the way. What is always sad to observe is how PhD students are continuously worried that someone did exactly what they did, but half a year before. While this can be the case in some cutting edge branches of medicine or genetics, where the trajectories are following a clearer path, I hardly encountered that somebody really did exactly the same thing as you were planning to do in your branch of soft science. There are always differences, and quite often pretty substantial ones. Just be relaxed, in the end it is all going to be alright. And because no one will relax about this point any day soon, let us move to the next point, and quickly.\n\n\n== VIII - Iteration circus ==\nAt this stage you have your data, you have a vague idea how you are going to contribute to your specific part of science, and you have a menu of what theories you can utilise in your thesis. This is the stage where many struggle to master their spirit and get instead lost in self-doubt and endless questioning everything inbetween the smaller details and - well - everything. The best way to countermeasure this is to start early to have a structure that allows you to have early success with your first paper. Building on Ali Abdaal here, we got motivation mixed up. It is not our ideas that motivate us, but instead smaller success that motivates us towards larger challenges that lead to larger success, and so on. If you do not manage to reach early success, the larger challenges of work will become impossible to overcome. This is why I think that you need to learn writing first, and then use this skill to write what you actually want to write about. Writers block is an altogether different story, I think. Many people who think they have writers block have either not even started to learn how to write, and are just lacking a routine, or are living in the believe that writing should feel great. Science made a dramatic mistake when is starting selling itself as a ''Heureka''-generating euphoria - that simply does not match reality. Writing in science is hard work, and in any other field of labour, if nothing else will work, ''work will work''. The other challenge is taking decision, which I extensively outlined above. Again this takes practice, you need to learn to become constant in your life habits. There are too many things beyond your control that you need to take into account, hence the more stability you can add to the process, the better.\n\n==== First, your supervisor ====\nTry to establish a clear protocol early on. What is expected from you, and what would you like your supervisor to do? Remember that your supervisor could probably write your thesis in a matter of months if not weeks, but this is not what it is all about. Some things you can only learn by yourself. Yet if there is one thing that was absolutely fantastic during my PhD, it was the fact that the door of my supervisor was always open. Whenever I had a question, I could ask him. Looking back at it, I do not know how he managed this, and coming to think of it, much must have been quite annoying. I can consider myself lucky that I had a supervisor with an incredibly high level of patience. Looking back today, I think that I can be grateful, yet as a supervisor I would clarify the roles clearer, which is another nod to the patience of my supervisor. It is however the role of a supervisor to give a frank and honest evaluation of the current stage of the thesis, and to motivate the student if need be. This is easier said then done, because this is different for every PhD student. Therefore, I think the most important thing one should try to develop is [[Glossary|trust]]. There is no magic portion or spell that can help tu to develop trust, but it should be the goal from both sides, I feel.\n\n==== Second, your co-authors ====\nManaging co-authors is a pivotal academic skill, and most PhD students learn hardly more than under these constellations. I exclude the supervisor here, as I dealt with this relation before. The rest of the co-authors can be divided into three groups: The Peers, the superiors, and the minions.  \n\n===== The peers =====\nThe peers are the most important group for you. Find peers with a similar interest and work ethic. And with 'interest' I do not mean 'topic'. It is not important what they work on, this can be different, but how they work on it should be allowing to build a link, and the means of their work should be to allow for the creation of a reflexive space. You learn the most about yourself through the reflection of your peers. Working together with peers can give you energy, and help you to master hurdles together. Yet make no mistake: Your peers will not write your thesis, and in the end you will have to take the decisions they cannot take for you. I had three fantastic peers during my PhD, and a wonderful wider team at our institute. The closer peers - and later also some other fellows - were active collaborators. This was highly relevant for me, because I learned about methods through the data of other people. I think that it is ideal if you can use a peer-to-peer exchange to develop yourself. The wider circle is important as a social buffer, where you can share the experience of academia, and lighten the load of the smaller and larger challenges all PhDs face. This is after all also a time one should enjoy.\n\n===== Superiors ===== \nPublishing with other superiors is a privilege. If someone with less time than you is available to you wants to commit their dense schedule to your work, then you should try to leave a positive impression. Be clear in your communication, discuss openly who is doing which tasks, but try to have the others maybe first outline their role, and then try to match it. Peer-review also led to the rise of some opportunists, but remember that the line between a real contribution and some helpful thoughts is a blurry one. Still, for a PhD it is often difficult to access the contributions of a professor or postdoc, hence ideally you rely on your supervisor, who may decide together with your supervisor which contribution merits which authorship. I had the honour of having many co-authors as a PhD that were on a much higher level, both in hierarchy as well as experience, and I learned very much from them, and for this I am truly grateful. \n\n===== Minions =====\nIf on the other hand you have minions working for you, and by that I mean student or technical assistants, then you should extend the same learning experience and gratitude to the people that you work with. Students will look at you for inspiration, and you could be a mentor for them through which you learn in turn as well. Working well with technicians will help you to learn about their specific expertise, and in my experience there is hardly anyone more motivated and kind than a technician who feels valued. Again, with students and technicians it is important to cleary communicate your expectations and make sure that everybody is on the same page. Try to lead by example, if you are hard working and committed, the others will be motivated and empowered. \n\n==== Third, peer review ====\nThe system of peer review is better than its reputation. Impressions of reviews are highly reproducible, and the system creates hundreds of thousand of papers every year. However, sometimes it takes some patience, and it has some challenges the one needs to learn. The first rejection will feel like a total devastation, and it will be hard not to take the comments of reviewers and editors personal, especially if these comments are at the lesser end of the spectrum. Yet peer-review is not about every little detail, but it is more of a lump sum process. Some comments are less important than others. It is more about the general pattern of a review, and not you escalating yourself about piecemeal. Yet, peer review takes time. Submitting a paper, hearing back, then submitting it elsewhere after a rejection or working on a revision takes months, and for each step a few months easily adds up to a year. Learn to see this as a process that is established - criticising it will help you very little. You may want to ask your supervisor if you could do a review of a paper, given that you are up for it, then you will learn that reviewers are also people. Peer-review is something that you learn, and this principle works for both receiving criticsm, but also for handing it out. \n\n\n== IX - You have to let go ==\nYou did your empirical work and analysis, you have your papers happening, maybe presented at a conference, and you can see a light at the end of the tunnel. Or so you should. For many PhDs, the final phase is dense, stressful, just an altogether twice-baked turmoil lacking any sugar coating. After the papers you should start early on the red tape, which is the overall part of the thesis that in the end will have woven all parts together. This will be hard for most, because it means that your time as PhD student will end. You have to decide that it is done. You will have to accept that you become a part of the academic merit that so many passed before you. You will have to acknowledge that you did what you could. You have to stand up to your supervisor, your peers, the whole institute. You should see this as a part of a process that is overall well established. Why should your case be any different? In my experience it helps to make a rather clear structure of the steps you have to take to finalise your PhD, and to stick by this list. Try to share it with others and build up peer pressure. Discuss it with your supervisor. '''And most importantly, build a continuous timeline of the phase after your PhD.''' What do you want to do? If you do not get this right, at least in terms of the general direction, than you will probably procrastinate. You do not need to have a postdoc position waiting for you, but you should know which next steps you want to pursue. \n\nAlso, try not to be obsessive about finalising your thesis in terms of typesetting and layout. Try to learn the necessary skills early, I certainly failed this lesson. Become a Word ''pro''. Maybe this is not the most exciting skill, but surely helpful when you need to set up a few hundred pages of text, you text, your thesis. Get help, if you need it. Have people read the pre-final version, the penultimate version, and help you with the printout. Once the whole thing is handed in, never look at it again. Ever. There will be flaws and mistakes. Most people will not recognise them, even your reviewers may not find them. Hence you may want to keep this opportunity to harvest anxiety out of your defence preparation.\n\n\n== X - X marks the spot ==\nDay X, the day of your thesis defence is approaching. You need to start your presentation slides. How could you ever boil down all these papers into the few minutes you have?! You need to choose, maybe you drop one paper altogether. You need to focus on the main points. Try to attend defenses early on, learn what is expected, who did well, and why? Learn about the questions the committee asks, and try to prepare yourself mentally for similar settings. Practice way early by presenting at some conferences or in front of your peers. Have your slides critiqued by your peers. Build a narrative. '''It should not be magic, it should be robust, and it should follow the existing conventions'''. Giving at least some mockup presentations is crucial, as you need to learn the timing and the drama of the whole thing. Do not worry about being repetitive: once the real thing happens, adrenalin will kick in, and whip you through the whole thing in no time. \n\nTo prepare for a defense means that everything that can be asked was already in your head once. You need to become so deeply embedded into your stuff, that you become everything that can be asked. In my experience, this is mostly not about knowledge, but about confidence and structure. You need to overcome the doubts you have, listen to the questions people may ask, breath once - shortly, to think - and then give a crisp answer. Do not ramble.  Say the most important things, and then find an endpoint. Just like in writing, you need to take a decision. \nA good friend once told me that an outstanding PhD student to him is someone, who can afterwards independently conduct research in the respective field alone, or as a supervisor. I think this is what you should be thriving for. This is what a PhD is about. You follow a path of learning that many went on before. You become part of an academic tradition, and the key goal of this tradition is that you become an innovator. You learn to overcome yourself, and become an academic. Depending how you go on, you can become an expert in civil society, do research in society or academia, and in academia you may later educate others and inspire them to follow in your footsteps. To me, this is my greatest privilege I have as an academic. Thank you very much for this.\n\n\n== (XI) Celebration ==\nThe end of any successful PhD is a celebration. Personally, I am very bad at celebrating, but it is part of the academic circus. For most people the celebration is important, and it is clear that it is well deserved. You have to give a speech. Hardly any surprises ever happened at such speeches, as it is widely scripted by the circumstances. There is often food and drinks. Your supervisor will probably talk with your parents, always the greatest pleasure for me, and almost always an embarrassment for the PhD student. This is a moment of true emancipation of the PhD student, both from the parents and the supervisor(s). The PhD student is however often too shellshocked to process anything that is happening. Often it is quite a turmoil of emotions, and it will take days to realize that it is done. '''From then on, you will go into a phase of reflection that will take years.''' I closely follow when my fellow professors talk about their supervisors, you often hear gratitude, subtle criticism, or even downright tendencies of Stockholm syndrome. This is all perfectly normal. Coming to think of it, there are hardly any relationships in life outside of our family and partners that are closer. Also, learning can be stressful. I know that all I am in academia was seeded by my mentors and supervisors, and their patience and experience. It took me years to figure this out, since we do not only learn how to do things, but also how to do things our way. Academia is about development, and doing a PhD is about personal transformation.  You engage in a journey that technically ends with your defense, but actually it never ended. The latest stepping stone I reached was when I understood that I am not a teacher, but a learner. Having passed the rites of a PhD and now being a professor, the whole endeavor is about learning and teaching, intertwined, woven, twisting. This is what I came to love the most about academia, as I know that academia has a lifetime of learning to offer to me.\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "mnff0njstenvwrm0e4pky5g49n0o9up"
                }
            },
            {
                "title": "How to dean",
                "ns": "0",
                "id": "972",
                "revision": {
                    "id": "6907",
                    "parentid": "6727",
                    "timestamp": "2023-01-28T18:11:58Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "62640",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n'''In short:''' This entry provides guidance for anyone who's planning or already engaging to be a dean, and might also help others to understand the position of a dean better, and how academia might work in general. Naturally, this is the perspective of one person only.\n\n\n'''Being the Dean \u2013 a manifesto reflected and revised'''\nIn this post I will reflect about the time of being the dean of the Faculty of Sustainability. After some thinking and recent events, I wrote down 10 rules in the very beginning that I consider relevant to me. You might wonder now, why you should read them. Are you a dean? Do you consider becoming a dean? Even if not, the following might be of relevance for you, maybe not to learn something I learned, but to learn something about the academic system, and your part in it. I can say I learned some lessons. The guidelines I defined early on are written in bold. The revisiting of the guidelines six months into the deanship is written in plain text. The revisiting of these texts 2 months after my six year term as a dean are also highlighted accordingly. This is a highly personal account, obviously. However, since I am not aware of such a knowledge resource being available quite often, I felt it might still give people some value.\n\n==Developing the Faculty==\n'''The dean works on the coherent triangle of management of the Faculty, research in the Faculty and teaching of the Faculty, together with the other deans.'''\n\n'''Management:''' \nMany consider that management is the central theme of being the dean. It is simply boring admin, and why would a true researcher or teacher bother at all? Well, it is admin work, I will not fool you here, but maybe we can change the admin system, if only step by step. I learned a lot about admin tricks during the last 6 months, but more importantly, I altered the mode of admin surrounding me. How I did that you ask? First, I observed for 2-3 months. During this time, only emergency changes were made, beside that, I sticked quite to the structure I found, which was after all very good.\nAfter 2-3 months, I increasingly asked people, what they would change. Then I took the ratio and rational out of the intel and started implementing changes. It was actually fun, not only for me, but also (I hope) for others. Fact is, that most people want responsibility and trust. I think it is vital to help people reach their peak, and through the efforts of my tremendous surrounding, I actually feel I get closer to my own peak. Admin is actually much much much better than its reputation. I think people do take the time to reflect on how good the system actually is. If baffled by admin and procedures, it is best -I learned- to take a step back and look at the whole picture. With some context, I believe to have learned that hardly anyone is harmful, and no one is evil in admin. People are just overworked and misunderstood. Structured and solution-orientated suggestions for improvement are what\u2019s working best.\n\n'''2022 revisit:'''\nManagement within constructed institutions means playing a long game. If you want to change things, negative emotions or pressure will bring you nowhere. You need to work with the system, not against it. In my experience, the vast majority of the people working in administration and management are doing a tremendous job. Those few who seemingly bother you actually may have the most challenging job in the whole institution, as they need to enforce principles or mechanisms that they did not choose. We are at a point in time where \u201ethe machine\u201c tries to control any given outlier point through a new rule. To this end, we pile rules and rules on top of each other. In my experience this is not specific for any given institution, but simply the Zeitgeist. Only time will tell how we can change this. However, we can only do this together, because the machine with its mechanisms and principles is quite tight. We should never forget that we came a long way, and I can personally say that I feel to be in a very privileged situation. Also, I am in a very powerful situation, there are no mincing words there. To this end, it should be ok that there are some underlying mechanisms that control our power. However, there are definitely parts of the management that need changing. After Utilitarianism widely failed due to the rightful complaints of minorities the protection of smaller groups now often outweighs the needs of the many, yet this protection of minorities is of the most utmost importance, and should be. Consequently, we need time and patience, because there is no doubt that the rights of minorities need to be protected. All this is directly related to the management of constructed institutions. In my experience, we need to be open and clear in our communication, yet always consider why certain people in the administration enforce specific rules, and how this helps and protects people.\n\n'''Research:'''\nLeuphana University is a tremendous place, I think. We have some truly inspiring research happening, and it is among the duties of the dean to aid the creation of a coherent and bold research narrative. This is really fun, since it is a true team effort. There are so many inspiring and fascinating researchers at our Faculty and university and beyond, and it is very nice to be one of the central hubs in this structure. I surely still have my own interests and research (I hope), but creating ties between an array of people is challenging yet fun. What I think is even more fascinating is the mode we can have as a dynamic and thriving Faculty. Whenever a new research call comes in, we are prepared, and as a team can consider the best options to approach a potential project. In addition, we build on a tremendous experience from the wider University, especially from the leaders of the University.\n\n'''2022 revisit:'''\nHaving an overview concerning the research happening at the Faculty is essential in order to tell the story of the Faculty. This is a tremendous challenge in a Faculty of Sustainability, where you have many diverse backgrounds focussing on different aspects of research. While this is on the one hand a challenge, it is naturally also an opportunity. Research collaboration in such a diverse place needs to build on a higher level of integration as well as trust. At the Faculty level, broad coherence is the biggest obstacle. You need a coherent recognition in terms of research in order to work in a joined direction. Much good if not excellent research is built on joined experience, which is basically researchers collaborating together and learning from each other. This is a bigger struggle if the backgrounds are more diverse. What is more, having different goals is a huge barrier. Imagine someone is strongly focused on English peer-reviewed publications, while another researcher only publishes books? How can these two collaborate, do they write both, or each their individual publications? Working towards joined goals is one of the biggest obstacles to gain coherence. In an interdisciplinary setting, and with a transdisciplinary ambition, the trade-offs can be huge, hence it is best to have an early joined reflection on the goals within a research project. What is more, in my experience it is best to work within larger collaborative research projects, because this gives you a good level of experience on where the other people are standing.\n\n'''Teaching:'''\nThe reason why I started at Leuphana, to begin with, was that a position with teaching seemed more attractive as compared to a pure research position. Still, teaching seems somewhat to play a smaller role in today\u2019s academia. I think this is wrong, and as the dean I can often add to the discussion and strategic planning of the teaching program. Just as with research, we have a great portfolio of people, and designing and improving such cutting-edge programs (no kidding) as ours is surely fun. Also, the exchange with the student representatives as a dean is a real pleasure, as you get great feedback that you can implement. Most importantly, sustainability is a surely dynamic field, and parts of our programs are more than dynamic. What is tricky though is quality. Before I always said that I want to forget all bad info on specific courses and lectures. Now I have to implement this, and this is surely a continuous process. What to do with mediocre teaching? I myself have a course that just has not ripened into a digestible form (yet!). This takes time and clear strategic planning. Being the dean is also to work with people on a longer time scale, and build trust. This can lead to lasting and sustainable change, at least this is my current hypothesis.\n\n'''2022 revisit:'''\nTeaching is often one of the most underestimated building blocks of academia. Many admirable researchers I know told me that things started moving in their heads in a more constructive direction as soon as they had to explain their research -or research in general- to others. Teaching how to become a scientist creates a deeper level of reflection that cannot be replaced by anything else, I believe. I know teaching is not for everybody, and that is ok. Different people have different focal points. However, for me it is unimaginable how to test new thoughts other than through my teaching. If I am able to explain a new thought to students, then I know I am onto something. This works in both directions. If I fail to explain something, then I guess the thought is not there yet. Teaching at a non-normal University is so great just because it is at the leading edge of research, but this is also constantly leaking into the study programs. It is admirable how many of my colleagues are using their own research as example cases in their teaching. We came a long way from academic teachers using decades-old material, which is how I was sometimes educated. Yet I had also many teachers that were entertaining us students with their latest thoughts and thus transported our learning through their excitement. One cannot overestimate the responsibility of a teacher, because many people look at you for learning and often guidance, and sometimes nothing short of inspiration. I believe teaching future scientists is the biggest honour one can have.\n\n'''The team:'''\nThe greatest part of being the dean is being a team. There are typically several vice deans, one on teaching, and one on research. In addition, there is the former dean, who is now designated advisor to the president on sustainability. The four of us make a surely great team. What I like best is the speed of our communication. Our telephone calls, meetings, slack chats and whatever mean of communication we use are among the densest I ever participated in. All this really brought teamwork for me to the next level.\n\n'''2022 revisit:'''Team constellations may change over time, yet the most important thing is that all functions are still covered. While it is definitely better to work in a team with a high redundancy in case of emergencies, this is often not possible. In my experience, within a good team you have a smaller part of overlap and a larger part of individual work. You need to exchange about the important points, and you need to be able to anticipate the decisions the others might take, and be able to explain why you deviate from their anticipated decision. However, within a good team my experience is that most decisions are coherent or at least understandable for everyone. It would not be feasible to have this cohesion for all decisions, yet I believe it can be achieved. This is again a matter of trust. In a good team -and I had the privilege to work within incredible teams- you learn to trust others and hence have the possibility to lay much of the work in their hands. However, never forget, that it is ultimately your responsibility if something does not work out. Pragmatically, I can surely say that this hardly ever happened. In this case, you need to step up and say that you should have seen it coming, or that no one could have seen this coming. It does not help to blame others for mistakes that were within the realm of your leadership. These days we often speak about people of power and their privilege. Acknowledging responsibility is one of these privileges, especially when things went wrong. Naturally, this is easier said than done, yet we may be able to thrive towards it.\n\n==Creating a vision==\n'''The dean tries to jointly create a vision for the Faculty. Integrating the different ideas of professors and other Faculty, as well as our colleagues both within and outside of Leuphana demands recognition of the bigger picture, as well as being able to compromise. Please help the dean to create this vision.'''\n\nSince half a year I am the dean. Building on books I read about academic deans and my own observation, I reflected from the very start on how to create a joined vision. \nWe are a Faculty of more than 250 people, with more than 30 professors. We are diverse. We have sustainability as a joined goal. Naturally, we have different ideas, strategies and approaches to academia. I see this as our joined strength. We are also linked within the University, and also to colleagues outside of Leuphana. What are now the key steps to create a vision? Is it even possible to create a vision? Our former chancellor Helmut Schmidt famously claimed that \u201che who has vision should go see a doctor\u201d. While it is true that we should not waste our resources thriving for some unachievable vision, I think one key aspect of vision is to push beyond our imagination, regardless of initial limitations. To achieve this, recognition of diversity is pivotal. By combining different entities, such as knowledge domains, methods, approaches, frameworks etc. may enable us to create something novel. This demands a proper setting, enabling exchange and multiplication of the different Faculty. Surely this poses a key challenge, as it requires initial investments and lots of diplomacy. This is one of the key goals of me being the dean. Bringing together diversity and excellence is a great challenge, but it is also very rewarding. The group of people at the Faculty is not only diverse but also large. Diplomacy and efficiency in creating exchange is one of my key goals.  However, creating linkages demands knowledge and insight. This can be knowledge of the rest of the Faculty, but also includes knowledge of people outside of the Faculty. One key goal of the dean is then to try and integrate all these different types of knowledge. Creating a coherent and novel picture out of this knowledge is then -potentially- a vision. Here, another vital step is necessary to be noted. Creating a vision should not be restricted to foresight. While it is vital to envision potential future trajectories, I consider it even more important to understand a goal -even a soft goal- that you want to target. We need to create visions not only by foresight, but also by a clear and bold anticipation of what we want to achieve. Therefore, the dean needs to be able to communicate this vision and make it a participatory exercise of the Faculty, university and other players. I am glad to help integrate this vision out of the coherent canon of knowledge and goals that unites us all.'\n\n\n'''2022 revisit:''' Humans life through storytelling. Being able to transport a story about the Faculty is certainly a challenge, but also a really good practice. Therefore, everybody needs to have a keen eye on what the others are doing, first and foremost the dean. Being in a Faculty of Sustainability can be overwhelming, but also fantastic. Often discussions resolve about disciplines, identities and origin stories. It reminds me a bit of the discussion of the different schools in Buddhism, where many claim to have the true, original or best way of the Dharma. Indeed there are many perspectives and ways which can lead to enlightenment, and sustainability is to me yet another contribution to this. Negotiating between the different fractions is often an absurd act, and nothing short of funny at more than one time. I remember several times that I would meet one colleague from the natural science complaining that the colleague is ok if the Faculty is only doing social science in the future, and if this is the way it is, so be it. Literally, the next colleague scheduled for a meeting was from social science and said that now this very colleague is ok if we only do natural science from now on, but at least we should be honest about it. I wish both could have seen each other, yet this is sometimes only the privilege of the dean. Hence creating a vision is often a form of integration of diverse views, and the daring act of alienating no one. During my time as a dean I tried to be as open and transparent about all these views as I could be under the given circumstances that sometimes demand confidentiality. I had the experience that creating a vision means first and foremost being coherent. To this end, it is less important that everybody understands your vision, but that they at least face a familiar narrative that they can anticipate. Having a vision is more important than convincing every last person. This is not because I would not have loved to confine each and every single person and honor their viewpoint. However, I believe that in a constructed institution this is impossible, for two reasons at least. First, for the time being, each and every single constructed institution suffers from limited resources. Thoughts on how to allocate these resources differ often, since most people only know the viewpoint of the specific institute, but not always of the whole Faculty. The second challenge is that some people just question decisions at random. In academia, you have sometimes such characters that question maybe not everything, but almost everything. If you trigger such a person, I do not know how to convince this colleague that you only have the best intention. They may question you no matter what. Ignorance and greed are old news in the struggle of human existence, yet they also pose a threat to a united vision, and clear communication and coherence are the best antidote, but will not cure all. \n\n\n==Acting fast and slow==\n'''The dean is action-orientated. Solutions are key to the dean. While exploring solutions may take time, we need to focus on how we reach a goal best. This demands a sensible measure and a transparent procedure, and a clear recognition of futile tasks.'''\n\nUniversities are just as any form of organisation built on the constant exchange between people that are goal orientated, and people that are process orientated. While certain diligence is key in creating solutions, many processes are endless, depending on the people involved. Therefore, it is key for me to find the right balance between these two extremes, the sheer endless maelstrom of admin trying to eat our time, and the head in the cloud professor who \"just wants to work\u201d. I feel for both groups. I think we have to learn where our strengths are (mine is not in filling out forms), and have to cope with our weaknesses (again I am weak at filling out forms). While this is trivial fortune cookie wisdom, much of the frictions we have in daily academia are still rooted in this simple fact. I think it is very easy to process a problem endlessly, while it is quite hard to make the first step towards a solution. On the other hand, it is clear that creativity needs time, and also needs repeated failure. Well, maybe it does not need failure, but it may certainly build on it. I might now continue telling you how we need speed, or creativity, or creativity, or speed. I have however a different take on this. I think engaging in action orientation takes experience, yet also depends on your mindset. Let's start with the latter - the mindset. I think while some people seem to be born for reflection, some others are born for action. Some people seem to tick more top-down, while others are more bottom-up. Where these two mechanisms clash, there is often tension, yet also moving forward. Experience is more tricky. I think people tend to become more effective over time, building experience. This is very helpful, yet can also create disharmony between experienced and inexperienced people. When people become very experienced, they seem to accumulate knowledge on such an epic scale, that they create action almost by reflex. This is actually the time when it is most pleasant to be the dean. Working with these fast-thinkers is an extreme privilege and a great pleasure. I learn a lot and hope to become more efficient myself. I can highly recommend to lower ranks to observe experienced thinkers and build enough trust in themselves to just observe how these fast tinkers create action. To me, this is one of the core levers in how we can move academia forward. Let\u2019s build experience to empower fast thinkers, or at least let\u2019s try to learn how we can create action.'\n\n'''2022 revisit:''' Changing institutions takes time. Due to the limited resources, we have in institutions we are often in a stalemate between development and the machine trying to prevent all progress. Academic institutions can feel like a Rube Goldberg machine at times. I can only extrapolate from our institution that a mere 99% of our colleagues only have the best intentions. I am actually very grateful for how much our colleagues in administration try to help us all the time, and mostly do not get the gratitude they deserve. The same holds true for the collaboration between the deans office and the presidential floor. These colleagues are to me the unsung heroes that despite all hardships try to evolve the institution further. And then there is the office of the dean. It is these colleagues that a dean owes the most. The solutions that these close colleagues created over my six years as a dean are the biggest workload I have ever seen evolving. Leading a Faculty is like steering a huge ship; without such a team like the one we have, everything would be futile. There is a lesson learned here because in order to have such a team move mountains I believe you have to trust the team. Equally with the presidential floor of the administration, if you suspect failure or mischief, this is probably what you are also going to find. This does not mean it's literally there since it is you who sees it, but it may not have been the intention. Yet there is a flip side, and I learned this during the pandemic. At the very beginning of the pandemic, there were a lot of wrong assumptions and often naive actions that led to danger and challenges. Sometimes being the dean then means to take the responsibility and create action even if people are unhappy with your decision at the time. Leading institutions does not always mean that you have to win a prize for the most beloved leader, quite the opposite. I was facing severe backlashes and completely baffled colleagues at times, yet I can clearly say that I stand by my actions also in retrospect. I hope that some of the colleagues remember, not because this is about right or wrong -it is not- but just that people understand that it was not about them or me, but about the right action, and protecting others. Being a person of power can mean that you are sometimes very much alone in your decisions, yet this may not make some decisions less right.\n\n\n\n==Transparency and communication==\n'''The dean believes that transparency and communication are key in our daily interaction. Therefore, the dean does not want to tolerate power relations and violent communication. Academia just as any other branch of society is often still hampered by the lack of non-violent communication. Hierarchy should be about taking decisions and responsibility, not about dominating others. I believe all live is precious, and violence is futile.'''\n\nOne could assume that administration processes would sooner or later reach some sort of equilibrium, where all processes and information flows were optimal to make the system flow best. Unfortunately, this assumption is in my experience wrong. Instead, it seems we miss most of the important information quite often. This is what administration should be all about: getting information where it is needed the most. Administration should mostly be about matching supply and demand of information. In my experience, much frustration in larger organizations originates in people not knowing the relevant piece of information, often through some flaws in the system. It could be worse, for instance when information is actively hidden. Some people might think they can manage a system best by not providing the relevant information to the people who might benefit from it. This is often a question of hierarchy and power. I believe that transparency works best in these cases. Hiding information can be seen as a form of violence. Having power over information can make you in fact superior, and consequently makes others inferior. Hierarchies are part of the way that many systems and organizations work. Yet should these entitle us to monopolize information? I think we might conclude that as long as legal issues are not violated, we might opt for transparency. This argument might not convince all-powerful people, but the truth will come out eventually, in my experience. Will the truth make us free? Hm. Sometimes. But it can also burden us. Some people are indeed very much burdened by the truth. Should we then decide to hide the truth from these burdened individuals? I think not. We should not make others less free, is what I hypothesize to be true. However, I did not start by talking about the truth. I started talking about information. Information could be neutral. Truth is not. Truth may instead be normative, many might argue. Imagine Case #1: The all-powerful professor. A professor knows the solution to a problem. The solution is knowledge of certain information. The professor does not want to reveal this information. Hence the problem cannot be solved. Obviously, this act is wrong.\nImagine Case #2: The cluttered assistant. The information to solve a problem exists but was misplaced. Should we judge the cluttered assistant who misplaced it as less strong compared to the powerful professor? Probably not. The harm was not inflicted intentionally. Yet it represents a problem as we still cannot approximate a solution.\nImagine Case #3: The transparent secretary. A secretary knows all solutions and can tell you all solutions, even when you only ask for a specific one. This influx of information makes you miss the important solution. The problem remains unsolved. Is this better or worse than case 1 or case 2? After all, no one solved the case. All cases are realistic. Indeed, I can say they are very realistic. However, the person inflicting power over others (=the professor) would be ranked by many as the least moral character, and it is certainly the person with the highest privilege. The problem with power relations and communication is that we often confuse these three cases. We think someone has power over us, or that the person with the solution is simply cluttered. Or we avoid a person that has the solution because we cannot understand what the person says or do not have the time to listen to all answers. In other words, I think that many of these problems are a reflection of us. The way we approach people might directly reflect back on us. Hence if we approach and judge these situations will make a true difference. As long as we are open and positive, and show our respect for a person\u2019s work, and make sure that we highlight that we are willing to jointly approximate a solution, we may solve these challenges better, I think.\n\n'''2022 revisit:''' There exists information in the head of a dean that is confidential, and if such information was shared with this ignitions then it has to stay confidential. However, this is very rare. Instead, information usually travels fast in constructed institutions. You can catch almost all information as it happens, and much can be anticipated once you know the respective people that are involved. Yet most people have a limited understanding of the system. This creates very negative emotions in many people. I feel that in constructed institutions this naive ignorance is the greatest root of suffering. Therefore, being the dean means to often come back to and share information that is long known to you. Also, it may not be enough to simply share the information, but instead, you have to reveal it like a magic trick. Different people understand different information in different ways. Quite often people think they had a great idea when they actually understood the idea of somebody else. The negative assumptions about institutions still prevail. People speak of backroom hush-hush debates, clandestine deals and secret circles. It is almost ridiculous that in almost all cases they are completely wrong, and lost in some weird conspiracy scheme that tells more about their trust in themselves and others than about their willingness to understand the system. I think that most information is readily available and quite accessible. You just need to invest the time to get it and connect the dots. Yet people have singular information and extrapolate to the whole network based on their imperfect understanding of the net of information. This makes transparency a challenge, as it is often about the imperfect understanding of individuals. To me, this is quite sad. I wish I could explain better why I believe that all is good, or at least almost all and everybody. Sometimes people are misguided, yes. And there is still the rare breed of people wanting power. Still, most people I met have altruistic intentions, or at least they believe they have good intentions. Who would get up in the morning and say \"today I will be all evil\"? Hence if I can give one piece of advice: Ask people about their aims and intentions. It is our own imperfections and impatience that hamper transparency. Now more than ever, we need to invest time to communicate, exchange and learn from each other. I have no better way how to explain this, I only know that it took me a long time to understand this. However, I am sure that I had this time.\n\n\n==Leadership and vision==\n'''The dean integrates processes and moves the vision of the Faculty forward. While this demands leadership, it, more importantly, demands hierarchical planning. Even if some tasks are moved forward by the dean, the whole Faculty needs to stand up to the role to move the Faculty forward, since we all are the Faculty.'''\n\nThe dean integrates processes and moves the vision of the Faculty forward. This demands leadership and, even more importantly, it demands hierarchical planning. While some tasks are moved forward by the dean, the whole Faculty needs to stand up to the role to move the Faculty forward, since we all are the Faculty. Now wait a minute. Just in #4, I talked about how transparency is key, and now I talk about hierarchy? Is this not a contradiction? Well, I think not. Let's see. First of all, the dean reports and collaborates with the president and his team. Hence the system of Universities has a constructed hierarchy that is long-standing and well-established. A lot of information flows in this system, and not all information can be known by all. Hence a hierarchical system demands that not all information flows towards the top. Coming to think of it, this gives lower parts of the hierarchy actually more possibility for action, which I think is a good thing. A hierarchy does to me not only focus on the top but enables also the broader basis. So far so good. Why are the higher levels now labelled as leaders by me? This is first and foremost because of the necessity to take responsibility. I consider leaders to be simply people who do not only act but also provide feedback on whether a certain line of action by people lower in the hierarchy of the system is ok. By doing this, a leader takes responsibility for their actions. These decisions can be often controversial, and sometimes even wrong. Yet inaction is not an option for most challenges. Many people want to be leaders, yet at their heart, they would need to ask themselves whether they can take responsibility and live with the consequences. I can for instance say that I was often wrong as a leader, and can only hope that my decisions were more often for the better than for the worse. Personally, I stand by my actions. I believe that a system becomes problematic when people fail to or cannot stand by their decisions, or cannot even take the necessary decisions. While this may seem trivial, it is certainly not rare. Another thought raised, in the beginning, is vision. As Helmut Schmidt famously said, whoever has visions should go see a doctor. This reflects his partly admirable stoic logic, yet I think that a certain vision of the bigger picture can be helpful as a goal and motivator. Often this is controversial since some people typically cannot identify with any vision, hence visions are prone to random critique. Still, I believe visions can reflect the focus on the main challenges, which can be helpful in hierarchical systems. The last point that I raised was about the contribution of all to help the Faculty. Most faculties show a Pareto distribution when it comes to contribution to the greater good, where 80% of the work for the Faculty is made by 20% of the people. I can understand this. People have different goals, and not all have the same goals as the dean. Some focus on their individual research, which they consider most important. I could now at length discuss the reasons for doing one or the other, thereby pitching self-interest vs. altruistic motives. I think this dimension is too complex to be answered here, if at all. However, if we stick to the empirical fact that few people shoulder most work in the Faculty, the core question is if we want to change this. Being the dean, I think our Faculty is much better than most Faculties I know, with the workload resting on more shoulders than average. Yet there is room for improvement. Instead of losing myself now in arguments on why you should contribute to your institution, I simply reduce my argument to the question: \u201cWhat would be your gain if you contribute?\u201d If I get you to reflect upon this question, I think we are one step further.\n\n'''2022 revisit:''' Hierarchical planning and mechanisms may sound almost like a contradiction to the previous points, yet the last thing any leader has is clearly time. Leadership within institutions is therefore often still who can achieve the most with the least amount of time. While this is a sad reality, it is still a reality. We could solve this, I believe. If we would diminish hierarchies and build more on trust, then leaders would have more time which they could use more wisely. After all, we are still surrounded by institutions that demand hierarchies. Members of the ministry will want to meet presidents and deans, which they rightly perceive as the designated leaders of universities and faculties. Yet if more people would shoulder and share leadership within institutions then we would have a more integrated vision. I noted before that less and less people are willing to do this, and some may even only do this with the wrong intentions. I propose a radically different approach. Assuming that we acknowledge that leadership is still needed in a world of hierarchies, we should trust and support our leaders. Instead, I feel we often mistrust and amplify the workload. If we do not want to lead, we can at least support our leaders, after all, sharing is caring. Within our institutions, there is at least a lot of work to be shared. I believe only through that we can come to a truly shared vision. The people actively moving our institutions forwards are already all maxed out in terms of workload, yet there are many who are frustrated or even resigned. We can only rise above our current level if we integrate these people, yet our current visions only allow for a certain breed of people, a certain way of mechanisms how to conduct academia, and only for certain patterns how we evaluate academics. We somewhat became stuck in a certain direction and lost all track of diversity. Only if more people are integrated, we can have a shared vision, and increase our workload. Right now, most people still do now recognise that contributing to the long-term development is a work that may be cumbersome and sometimes futile, yet it is clearly worth it. Yet why should the many divisions we witness in society spare academia? Academia is riddled by divisions because it is a system that breads and builds upon strong identities. One colleague once put it exactly right, having 30 professors in a faculty is like herding cats. And then there are many other Faculty members to consider as well. I have no solution to offer here, but I am sure we need to radically redesign our constructs of leadership and vision, otherwise, we will be stuck in limited systems with much frustration. Strong hierarchies should be a thing of the past, yet it will take us a long way to learn how to integrate diverse perceptions for the greater good. Maybe sustainability science would be a great place to start such a line of thinking in academia? We are after all quite diverse and very normative indeed. Where could you have greater ambitions and knowledge to start such an endeavour?\n\n==Taking decisions==\n'''There are committees in the Faculty and university that have the role to take decisions. Please consider whether the institution is served best if you approach the dean about a decision that the dean will not and can not decide on.'''\n\nIt is in the nature of the position of the dean to take an uncountable number of decisions every day. While much is purely day-to-day business, it is still a challenge how to act in a fair and just way, while also being compassionate. Personally, I think many of these decisions are not literally decisions, but actions that have a path dependency that is rooted in the possibilities and opinions of the Faculty. Ideally, everybody with an equal amount of knowledge and experience would come to the same conclusion. Most people are not in the position to combine the diversity of opinions as the dean tries to do. Through partial knowledge and not having relevant information people often come to different conclusions. On the other hand, no one can know everything, hence communication is key in order to approximate just and fair decisions. It is understandable that most people have a rather subjective perspective, just as I can hardly claim at all to be objective. I cannot be. And yet I will keep trying. The ethics of my actions are certainly not easy to depict. I think one should act rational, reasonable, and try to maximise the utility for all. Any single of these approaches alone is bound to fail. Yet by reflecting on each decision from all three sides I will try my best. My underlying ethical paradigm is that I try to help everybody if I can. While most decisions are -I think- path dependencies of the possibilities available, some will be controversial. The ideal future I can imagine for me personally is the one where I can stand by all my decisions.\n\n'''2022 revisit:''' In my experience, it is good to act following some sort of a codex. While one would want to take all decisions on a case by case basis, this may create an incoherence down the road that creates huge perceptions of injustices. Showing compassion then becomes essential, because sadly, the rules within institutions can also change often only over a longer time. Then you have long debates on your hand about how things were possible in the past that are now not allowed anymore. To this end, I think it is central to verbalise why rules change and what you think about it, or even whether you tried to stop it. This is while a coherent and holistic code of conduct for yourself is of such immanent importance, yet one also needs to be able to understand when you are wrong and explain why this was the case. Since the dean ideally has the most knowledge about the Faculty you once more need to integrate this knowledge into clear guidelines. An example of such a guideline could be that all people follow the same rules. You cannot make an exception for one person, only if all people then do not need to follow the rule. Another example would be that you can ignore a rule if no damage will come from this. Now, this may feel like quite a stretch, but we are all aware of rules that are clear relics of a different time. I once heard of a rule at another university that you need to have your key on a keychain. Clearly, no one had their keys on a keychain. And then there can be rules where you trade time against damage control. If you need to check every contract you sign as a dean, then this is a lot of contracts that need checking. One could now say that you never ever check a contract of people that have a lot of funding and that what a contract signed that is below 5000 \u20ac. Effectively, through this conduct, you do not check about three-quarters of all contracts you ever sign. This saves a lot of work, and if something goes wrong, these folks anyway have enough funding to cover it through some other pot. Of course, I would have never followed such a silly rule, but if I would have, I would have found out that in six years no contract ever failed. Being fast in taking decisions is pivotal. During my tenure as a dean we often joked about blitz chess, and indeed many decisions do not only feel like this, but it is all also a bigger game with moves being connected, and a blunder at one point creates ripples down the road. This is the last argument for why you need a codex, simply you will not have the time for longer ethical reflections as it happens. Hence develop a codex on how to act early on, and try to adapt it. Lightning reactions must be carefully trained.\n\n==Being not corrupt==\n'''It is in the nature of the position of the dean that the dean cannot be corrupted. The dean will not yield to pressure or violence, and will despite his compassion not yield to negative emotion. I try to be the dean I want to see in the world.'''\n\nI think that in all forms of constructed governance the danger of corruption is always there, as no one can always avoid being corrupt. What does it mean to me to be corrupt? I think whenever you misuse your power to gain a benefit for yourself or someone else close to you, you are corrupt. Consequently, no one would claim to be corrupt. All people would argue that they only had the best intentions, or were even true altruists. Following this thought, can we truly judge whether we have been corrupt? I think not, at least not if we are personally involved in the matter. Others need to be the judge, and these people need to be independent and as objective as possible. In a world that is interdependent, this is hard to achieve. However, I think this is what we have governance systems for, and there need to be institutions within these systems that can judge whether a certain act or decision might be seen as corrupt. Still, it does not end there. If corruption is the benefit certain people claim, then corruption is also about the negative and biased effects one can have on other people. I think that in any case of doubt a potential bias needs to be checked. I dare say that I have been observing corruption in the past, and often it is a quite slippery slope. I grew up with it, hence I know it when I see it. This is why I often insist on checking whether actions are corrupt or not. Still, I also know that some people consider some of my actions to be corrupt, though I dare say these are not many. I am aware that people sometimes perceive reality in this way, also since we all may have different perceptions of reality. Often, this different perception is rooted in a lack of information, which is typical in hierarchical systems, and communication may resolve the issue. Nevertheless, in rare cases opinions can differ. I think on these occasions it can be hard to convince people of my perception of reality. It may not work, and may not be possible. Sometimes, when people think you act wrong, it is very hard to make them understand that you act right. In these cases, I think it is best to try to continue to act right, or at least not corrupted. Long term, the person might reconsider, and only time will tell whether some truths may be reflected in a different light in the future.\n\n'''2022 revisit:''' Power corrupts. This insight should be the guiding principle for anyone holding an office. There is no way how the human mind cannot be compromised by power because it is this taking of the office that leads to one's own mind being corrupted. This corruption can only be broken by knowing that there is this corruption on the one end, and by everybody else having full trust in their elected leaders. However, such philosopher-kings are not only a rare breed but also more of a ghost of the past than something we should thrive for. Still, I have the greatest respect for all leaders in this world, because their critics may never know how lonely the office can make a person. Yet these are all lofty goals, and the reality is often worse. Many people try to compromise the integrity of the system, as they are corrupt themselves, and try to bribe or otherwise compromise elected leaders. This is a bad action indeed, and a direct attack on the whole system. Some people do this for their own good, some for their respective group of people, yet I believe all these are deceived. Again, no one gets up and decides to act in an evil way. Just as only everybody can liberate themselves, people also corrupt themselves. A dean or any other leader needs to be aware of that, and make these misdeeds transparent. After all, this is the only way to face a wrong action, by making the actor aware of your perception, thereby rejecting it. Even if these people then disagree with you, you may have to move on. Just as I mentioned before, I still believe that often you cannot convince people that you believe that your action is right, and that they are wrong. This is perfectly alright. We cannot convince everybody of everything, and this is again a hard lesson learned for me. Some people seem to have no other goal than to corrupt the system, and they do this proudly. While we may never understand why these people do that, they often have a regrettable existence, because these are the most lonely ones. They cannot even fall back on power, because the only power they have is to question the power of others, yet everybody else may become increasingly annoyed by this. If we continue to ridicule ourselves by these power games, we will continue to be wasteful and not worthy of understanding each other. Therefore leaders sometimes need to lead the way, yet not all may follow. Systems are often governed by majorities, and not by everybody agreeing. While this is a trivial detail of democracy, it is still a lesson learned. Making sure that all voices are heard is the most important conduct one has to safeguard.\n\n\n==Representing the Faculty==\n'''The dean is a representative of the Faculty. The dean is glad to represent the Faculty, however, due to his role being focussed on a wide array of tasks please realise that the dean cannot stay for longer time stretches at all events where he is representing the Faculty. The dean often has to leave for other tasks.'''\n\nIt is a large honour to represent the Faculty as a dean. While honour is hard to quantify, I am constantly aware that currently, I am there for the Faculty, whenever a face, a speech or a welcoming is needed. Knowing as much as possible about the Faculty's facts, all the while having these lined up in a nice canvas of stories and anecdotes is essential to connect to others and explain the Faculty to them. In my case, I am very glad that this honour is shared among other people, hence I am currently focussing on representing the Faculty within Leuphana. Representing the Faculty has to be balanced, for at least two reasons. First, there is a high number of requests, asking for you - or better the Faculty- to be represented somewhere. Here one needs to focus, and only attend the ones which are most necessary and helpful to a high number of people. While the dean can welcome or integrate people or give a larger perspective, the dean certainly has also time constraints and cannot participate in time-intense activities that are relevant to a smaller circle of people. Where to draw this line, you ask? I do not know. This is the time when you can give the position your very own tone and style. I try to walk the fine line between diversity and utility. The second reason why representing the Faculty can get out of hand is the ego. It is not about you, it is about the office and the Faculty. When representing one should as much as possible step out of your own ego, since power enables, but also corrupts. This is again the point to realise how much power people associate with the office. However, I think this has next to nothing to do with me as a person. Another thing that is worth noticing is that some occasions are closer to my interest than others. Here, one should remember the bigger picture in balancing the Faculty. I sometimes wonder whether all time representing is well spent, even when I know I have to be there. On some occasions, the dean is a bit like a Jack-in-the-box, being put somewhere to represent. Still, I think that this is not only a necessity in constructed institutions but an honour. By now I enjoy these occasions, as there is always something to be learnt. When being a dean, you have to be the Jack-in-the-box you want to see in the world and make this role come to life. Evolve your style, and try to become good at it. The constructed function of the dean demands you to serve in this role.\n\n'''2022 revisit:''' Many people consider representational work to be a charade, yet we should remember it is an honour. A dean represents the whole faculty, which is quite a burden, but also a great responsibility. This highlights once more the importance to have a code of conduct and be transparent about it. In addition, you need to be able to integrate the diverse aspects of the different people in the Faculty and use the most suitable information at the respective moment. Often this is closely associated to diplomacy, because you need to anticipate your audience. To this end, it is vital to find your own voice and to generate a bag of narratives that you can use at the respective appropriate time. This is almost like a small repertoire of magic tricks that you need to rehearse and practise. Me, I build several narratives that worked over time, and thus you can adapt more and more to the context of any given situation. Still, while much can be improvised, preparation is key. Ideally, you pre-prepare a short script which you then if possible adapt to the atmosphere of the room shortly before your presentation. I always tune into the room and try to feel the vibes, but also to build on information that was shared before. What I still consider to be most important is to make all representation work about the office, and not about your own interests. All professors can surely talk a lot about their own focus, but the challenge in representing the Faculty is to be impartial in your work. While this is a great idea in theory and in practice probably impossible, I believe still one needs to thrive for it. Nothing devalues an office more than the respective leader misusing their power for their own good. However, this principle can also be trouble in the other direction. Very often when you need to represent the Faculty as a dean, many people assume that after a welcome speech you can just stay for the whole day because everything is so exciting and interesting. This is indeed a very naive assumption, since the daily schedule of a dean may hold well more than a dozen appointments and schedules meetings. During peak times, this may easily double, plus additional phone calls and constant chat communication. Hence it is best to make clear from the beginning that you are glad to represent the Faculty, yet then have to leave for other duties. Some people may still be baffled, but most people appreciate clear communication. Always remember what an honour it is to meet so many people who are interested in the Faculty.\n\n==The Dean is a hub of information==\n'''The dean is a hub of information.  Please bring everything you consider relevant for the Faculty to my attention. The dean will try and communicate information to the necessary people in order to approximate solutions.'''\n\nI think that while knowing builds understanding, ignorance may breed suffering. Therefore, it is the role of the dean to bring information together to understand the Faculty best, and to explain the Faculty and all his decisions to others. Many people now wonder what best can actually mean. I think we can try to have the best knowledge at any given point in time, to base our decisions on. Things change, this is trivial. However, at some point in time, we should try to get all relevant information together. This is especially important to help others understand. In my experience, this is sadly not always possible. This wealth of information is needed to take especially controversial decisions. All people are biased by their own view of things. As the dean -hence as a head of a constructed institution- it is important to try to have a view on the diversity of opinions, and then look at all the checks and balances. However if people would know all the diversity of opinions and information about the Faculty, I think most decisions would neither be bold nor controversial. I like to think that most are mere path dependencies intermingled with innovations. And this is what I mean by best. All the diversity of information is often leading to several pathways, one of which is best. Many people criticise this best, but I am a true believer in Occam\u2018s razor (surprise!). However, within a constructed society, some information is also confidential. These secrets are necessary -often for legal reasons- or also since some people trust the dean with information they do not want to share with everybody. While I think that the smallest part of information I receive is confidential, it is very important to me that this information is a well-kept secret that can still be influential in my decisions. Funnily, many people also share confidential information with me that they also share with many others. While I still guard this information, it is amusing to hear confidential hush-hush information from many different sources. Anyway. If you want to understand something about the Faculty that is not confidential, I say ask the dean. He might know, or is glad to hear what he does not know, and will make efforts to improve his knowledge. Thank you for your contribution!\n\n'''2022 revisit:''' The stream of information you need to integrate as a dean can be at times overwhelming, yet I can clearly say that integrating all the diverse information was one of the greatest learning points for me personally. The key about information as a dean is indeed diversity. There are many different forms of information. Some important, some maybe less so. Some very confidential, some best spread widely. Yet most important, there is a strong information monopole, where much information arrives at the dean, but also much more information needs to find its way back to the Faculty. Explaining is a key trade as a dean, and often people want to somewhat connect the dots themselves. Hence spreading information while allowing people to gain deeper insights into the connection of information is a key goal for a dean, I think. Perfecting patience is a virtue not to be taken lightly, because once you start to see patterns much information can be indeed anticipated. Warning people about immanent problems might feel difficult if all people believe that your assessment is wrong. This is yet another point concerning information, I feel the office is served best once you reached a stage where not a lot of information is able to surprise you. This was for me a certain feeling of safety, because this was when I understood how reliable the system actually is. It works mostly like a clock, with clear precision and mechanism that can indeed be understood. The last point I would like to share when it comes to information is confidentiality. Never break a vow of confidentiality unless the information is spread from somewhere else. What was given to you in confidence stays in confidence. Only if the information starts dripping through other channels, you can start spreading the information elsewhere. To this end, it is however highly relevant that within the team of the deans office, even such information needs to be shared if preparations need to be done to deal with changes. I had a very good experience with the team being well able to deal with different levels of confidentiality. Alas, information travels fast in any constructed institution. Usually one does not need to worry about this, because it seems that spreading confidential information is a key human trait. Still, within our deans team we were always very clear and explicit about which information stays within our meetings, and which can travel far and wide. I had equally good experiences with the presidential floor and other people of power in our institution. Outsiders often assume backroom agendas and restriction of information as a means of power, yet I can say with confidence that this is hardly ever a fact of reality. Instead, I believe that all leaders I know are confident where it is needed, yet open-minded and willing to share information if this is helpful. I am indeed impressed by how well the spread of information works at our institution.\n\n\n==How to feel about being the dean==\n'''I am grateful to be the dean.''' \n\nOh, how glad I am to be the dean. Being the dean is the greatest honour of my work life. To me, the position is not about power, but about trust. The Faculty trusts me to do my work best. I am so happy for this trust, as it brings me to the current setting closest to my main goal in life, which is helping others. Being responsible for 1000 students, some 150 Faculty staff, some 140 PhDs and our about 28 professors -right now- is fantastic. Being the dean to me basically means opening a box of duct tape every morning and start wrapping duct tape around problems. In the evening the box is empty, and there is a box for the next day already waiting. Duct tape is very versatile, you see. The first Moon lander was mainly built out of duct tape. Duct tape holds things together, blocks leaks, seals surfaces, it can soundproof, it is fantastic stuff. WD40 helps as well. And a hammer comes in handy at times. Every day the office holds something new, this is what I learned early on. Indeed, this office is the best learning opportunity ever. This is also due to the splendid team in the deans office, who catch problems to solve them really swiftly. These people are a constant influx of the most versatile solutions and approaches, that they never cease to amaze me, what a great team! Hence the core of the office for me is not about giving, it is instead a non-stopping flow of learning opportunities. In private life the greatest honour to me was becoming and being the father of my children, as I also learn so much from them. But as a dean the learning is -almost- equally amazing. However, with the office as a dean there are many constructed roles and traditional expectations. It is so great to learn these, and at times to change these. The role of the dean in general needs to shift from a role of power to a role of trust, I think. I saw many deans -also at Leuphana- being on this very same track. Being a dean you are the head of a completely constructed institution, an institution that is made by people. Hence I see the position as an opportunity to change the institution of the dean, and also to change the institution per se through this. This is the most wonderful thing about this office - it is all about driving change. I am so glad to be the dean. I thank you so much for this opportunity!\n\n'''2022 revisit:''' Being the dean was the greatest honour of my life, with the only exception of being the father of my children. The learning experience of being the dean can not be overstated. I learned so much about institutions, other people, but first and foremost about myself. I was glad to serve the Faculty and the University. Staying calm under stress is one of the biggest lessons learned out of the office, and a lesson indeed learned several times through failure. I count five occasions. Learning to act under pressure and to remain calm in front of violence is yet another key take-home message. Integrating diversity and thriving to minimise injustices coherently is yet another challenge. While I cannot claim that I did not fail regularly considering all these deeds, I would have never thought that my thinking on these terms would have gotten ever that far, and hopefully also some of my actions. Yet the best thing was working in a team, with my fellow deans, the whole dean team, the presidential floor, and basically the whole university, and beyond. I believe that often I was trusted, but more importantly, I trusted other people. This is the greatest achievement one can have in any constructed institution, and I will always look back at my six years as a dean with the fondness of being trusted, and having trusted other people.\n\n\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "kb9ejozpvy20omwc0frf8bgx5x5m1ze"
                }
            },
            {
                "title": "How to read an empirical paper",
                "ns": "0",
                "id": "910",
                "revision": {
                    "id": "6431",
                    "parentid": "6430",
                    "timestamp": "2021-11-01T09:05:17Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4624",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n|[[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]'''|| '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n'''In short:''' This entry provides tips on how to read an empirical, scientific paper.\n\n'''Just as much else, reading scientific papers is a matter of practice'''. When I started reading scientific papers, it took me hours to get through one paper, and I can very safely assume from my perspective today that I understood next to nothing back then. In between 'beginning to engage with science' and the position I find myself in today, there is a long but continuous journey of learning and gaining experience. I cannot tell you exactly how to learn to read scientific papers, but here are some suggestions that may help you in the beginning. These tips focus on empirical papers - for conceptual papers however, much is similar, and you will realize the differences as you read more papers of all kinds.\n\n1) '''Read the abstract first.''' The abstract is a systematic from to condense information, and what the authors feel is most important. Therfore, grasping the abstract should reveal what the paper is all about. Reading the abstract twice can be a good first step to learn what is interesting about the paper.\n\n2) '''Cross-read the introduction'''. After all, it leads to the research questions or hypotheses. However, it often contains deep knowledge that is not necessary to understand the bigger picture. Also, do not go down the rabbit hole of following the referenced literature for now - you may come back to it when you've gotten the bigger picture. First, take it step by step.\n\n3) '''Do not try to understand every word.''' In the beginning, much is overwhelming, because much is new. Try to hold on to what you understand. Each discipline has its own words, and sometimes different words even mean the same in different scientific cultures. Over time you will learn more of these big words, and what begins as a puzzle will form a picture. \n\n4) '''Note down the main points''' that the paper is focussing on. Many papers are well structured, and you can easily trace down the structure throughout the text. Your notes will reveal the methodological and theoretical logic of the paper, which is a good thing to come back to when you feel lost in the details.\n\n5) '''Do not preoccupy yourself with undressing the methods''' if you have no knowledge about these methods. Peer-review should have usually safeguarded that the applied methods are OK. So if you have no knowledge about the specifics of these methods, just accept that something was done, and try to understand what you can.\n\n6) '''The results are the heart of every paper.''' Read these in detail, and try to write down the main points. Also, be on the lookout how the authors qualify the individual results. Is a result more generalisable, or rather specific? \n\n7) In the discussion, '''look for the mentioning of limitations''' of the results. Then, try to see how the authors connect their specific results to the wider research picture. Lastly, check whether the authors give some recommendations for future research.\n\n8) '''Learning to read papers takes time.''' In the beginning, read a lot, but be patient with yourself. You may understand very little. The first 100 papers are more about familiarising yourself with the general form, not the content as such. Reading papers needs to become a habit, not a pain.\n\n9) Last piece of advice: '''Make your peace before reading.''' Instead of dreading towards the time where you feel you have to have to have to read the paper, just take one or two minutes, breathing in, and breathing out. Clear your mind, to prepare it for reading, just like you would stretch a bit before going running.\n\n10) '''Exchange with others''', and use peer pressure to your advantage. If you sit together and read, and then discuss what you got from the paper, then you can learn to understand how other people read papers, which can be quite a beneficial skill. Learn together, and nudge  yourselves through the papers. You can do it.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]] \n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "52vq74g37335f620kp7xpcp52t1p3td"
                }
            },
            {
                "title": "How to write a thesis",
                "ns": "0",
                "id": "379",
                "revision": {
                    "id": "7138",
                    "parentid": "6854",
                    "timestamp": "2023-05-03T10:29:15Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "comment": "/* Ending the design phase */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "47339",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\nThe following entry shall guide you through the process of writing your Bachelor's or Master's thesis - or any bigger research endeavour, really. Reading it takes some time, but so does writing your thesis, right?\n\n__TOC__\n== 1) Orientation ==\nFor most students, a thesis is the largest piece of work they ever created in their life, although there can be exceptions. '''Writing your thesis can be stressful,''' which is why it is helpful to start with a clear orientation phase that allows you to plan your timeline, get the right tools, align topic, method and theory and - last but not least - get your work mode in order. I would argue that the last point is the most important one. From my perspective, I want people to write a thesis to learn to structure themselves and overcome all barriers to create a contribution to the scientific community and ideally to society as well. In the broader context, the latter parts are very important to most students. Yet, as a teacher, I should highlight that from a purely educational perspective, the goal to catalystically learn to structure yourself and create such a thesis is more important than the content itself. As a researcher, I can however clearly state that it is not only very desirable, but also rewarding if a thesis contributes to the wider development of science, if only as a small contribution. Being a sustainability scientist, I would like to say that it is our privilege to study, and hence we should see it as our responsibility to contribute to society as well. \n\nHere are some concrete steps that may help you to orientate yourself in order to start with your thesis:\n\n==== The science ====\nWhich topics, theories and/or methods are you interested in? Which courses got you motivated to engage deeply with something, triggered a lasting motivation and interest, and enabled you to become versatile within this branch of science? The best starting point to me is to go through your past academic life, and see which parts thrilled you or kept resonating in your head. If you ask me, it is not a particularly good idea to search for something that is not tangible within the setting and environment you are in. If you want to make a topic because you feel that this particular topic was never touched upon in your study program, you may have difficulty finding a supervisor. I think it is equally a challenge to work on something where next to no references exists. Do not misunderstand me: I think it is good to look for engaging and energising topics, ideally linked to creating an impact beyond science. Even very specific cases or settings that are white spots on the landscape of science can be looked upon in a specific sense or be approached in a specific mode, may it be as a [[Glossary|transdisciplinary]] approach, a case study setting, or through the testing of a specific theory. If however your peers and potential supervisors do not seem to resonate with this specific topic you are so eager to engage with, maybe you could find something else? Remember that no human is an island, and science depends on peer exchange, and may benefit from a supervisor who is able to give you feedback. I suggest, you do not work on a topic that you literally work on completely alone.\n\n==== The supervisor ====\n[[File:Obi-wan-qui-gon-padawan.jpg||thumb|right|'''Many people have wrong assumptions about their supervisors''' Source: [https://www.starwars.com/news/what-is-a-padawan]]]\nCan you identify a specific gap in knowledge or contribution to existing research that resonated with you, and made you return to it again and again? Who from the faculty raised this to you, or may supervise it? Since not everybody can come up with a topic by themselves, maybe a suggestion of a potential supervisor might help. The more structured you approach your potential supervisor with your request, the happier they will be to help, at least in my experience. People are often inspired by projects that potential supervisors conduct, and would like to integrate their thesis into the large project. To this end, I would give some cautionary and sobering advise: Most projects that exist are already underway, and most of the time a thesis is an add-on, and not exactly a pivotal part of the project. This should not be a reason to feel that the work is irrelevant. More often than not additional questions arise through your contribution to a project. Yet, you might need to acknowledge that the project was designed a long time ago by someone who probably was not you. Hence the demand in such a project is often quite precise and the questions are already rather specific, and I would suggest that you are clear about what is expected from you. However, this may be a good opportunity to be under the wing of a PhD or somebody else in the project, and get a closer interaction that you would get with most supervisors.\n\n==== Past experience ====\nTake stock of your previous work. Which setting worked best, where were you stuck? Analyse your past work systematically in order to make an informed decision to commit to a specific approach in this phase in your life. Try to understand which parts of these great things stuck with you. Sometimes it was not the topic at all, but the way it was discussed in the group. Try to analyse what specific part resonated with you. Make a mind map of the different components you see in past parts of science that resonated with you. Also, these days many people are guided by joy. I think overall, this is a good thing. However, we have to acknowledge that in science not everything is always joyful. While I enjoy writing these lines, writing my own thesis was certainly not joyful all the time. I think this is important to remember, since I would question whether we really want to maximise our joy, or whether we want to balance it. This is for each and every single one of us to decide.\n\n==== Designing the workspace ====\n[[File:pexels-serpstat-572056.jpg|thumb|Designing your personal Workspace. Source: pexels.com]]\nFinally, how do you need to adapt your life to enable a committed focus to a thesis? We are talking about hundreds of hours of your life, and you may need to harmonise this with other interests and duties in your life. The head of the lab where I did my PhD always said how nice it is to focus only on one thing, i.e. your thesis. While from her perspective I think this is very true, this singular focus is often also a curse, as you may get stuck. This is why I think her remark was so clever. Design your life that your thesis is in focus, but you also have enough distraction in order to stay continuously motivated. This may include getting a desk, or a committed workspace that was previously unnecessary. I never had a desk at home, but the COVID-19 crisis changed that. I sit on this desk right now, writing these lines. My first mentor had the slogan: ''\u201eIf a cluttered desk represents a cluttered head, what does an empty desk represent\u201d'' written on the side of his desk. His place was a mess, but it was his mess, and he thrived in it. Design your space how you can work best.\n\n==== The peers ====\nAssemble a team. How did previous students orientate themselves? Ask around in your peer-group, and start to engage early with people who write their thesis. The insights and experience that they already have may allow you to orientate yourself on a different level later. You need to find people that are either complementary to or aligned with your work style and ethics. This is the most important point since you need to exchange with peers on a very regular basis. This is not so much about the goals, but more about the way. Peers can become an important reflexive space, with more time than your supervisors, and more knowledge of your specific needs. In addition, your peers can help you to think out loud on the progressive track that you are ideally on and give you emotional support. This is what friends are for. Most people we know as scientific geniuses were embedded into a larger group, and exchanged frequently and regularly with their peers. Peers often share a similar set of goals (i.e. writing a thesis), and also a comparable rhythm.\n\n== 2) Topical iteration and triangulation ==\nThis is the time when you use the broader topical aim that you have, and try to make it concrete. Now you should contact your supervisor and also intensify iterating your topic with your peers. I sometimes get the feeling that people these days think that finding a topic is like finding your ultimate purpose in life: both are absolutely energising moments, and you have the feeling that suddenly everything falls into its place. In my experience, both is highly unlikely. Instead, '''finding your topic is a long process''' that may start with a vague idea, may be heftily trimmed into place by a potential supervisor or someone else experienced in the topic, and may ultimately require a lot of iterations. Here are some tips that helped me in the past:\n\n==== Keep on writing ====\nWriting is like sports, it takes training to get good at it. People often claim to be bad writers, but I would say they are untrained writers, and I can only say that I came a long way in terms of writing that will never end. What is most important hence is to practice. Write at least 500 words per day, which may sound strange at first - considering that your thesis might have a few thousand words in lengths, you would think that after a few days it is over, so all good! This is unfortunately not what I meant. You should write not to write your thesis, but to practice writing to become able to write your thesis. Start ideally with something that engages you. This might be your thesis, but this might also be something completely different. Just start with something that you cannot get out of your head. For me, it is often a thought that I keep coming back to, that can be altogether trivial, but requires more than one sentence. An example would be why I think that scientific communication is often unprecise. I could ramble on for thousands of words about this, but bringing it to the point in - say - 500-1000 words is I think really helpful. What I learned once I started writing daily is that I get way more swift and precise, and my writing enables deeper analytical thoughts. Therefore, start engaging in whatever setting works best. Some write on the computer. Others need pen an paper. I often write these few hundred words on my phone, which can be quite convenient. Find the setting that works best, and for that, try out diverse settings. Some people need to change the location to get energised. Try things out to find the setting that works best for you.\n\n==== Read enough, but not too much ====\n[[File:pexels-pixabay-256431.jpg|thumb|Read enough, but not too much. Source: Pexels.com]] A suitable topic should be built on scientific literature, maybe not to the really precise inner core, but at least in terms of previous studies going into a similar direction. If a topic is flooded in literature, I would stay away from it, but this is only my preference, because I like emerging topics. If so much was said before, what could you specifically contribute? If this is not clear, I would advise you to avoid the dead chewed topics where your contribution may not add anything new anyway. However, it can be equally difficult if no literature exists about a specific topic. How do you approach the topic, which theory or conceptual approach can you build upon? All this makes it very hard to engage with something totally new, no matter how appealing it may be. I have a rule of thumb: from the trilogy of science - topic, theory and method - ideally two are rather clear and the third is then in the focus. If you want to work on a specific topic that is vague, the conceptual basis and the methodological approach should be rather clear. Working on methods empirically demands a well understood topic and a good command of the conceptual foundation. Researchers often tend to hopefully become more innovative over time, which could lead them to combine a rather new method with a vague topic, but this can create problems at such an early stage of your career. \n\nPeople often get dragged down by all the details they read in the literature. This is why it is so important to decide on what you think would work in your specific case. I think this decision can be best taken by someone with experience, but remember that failure is an important part of science. Failure is not the opposite of success, but a steppingstone! Pragmatism is also important, since in my experience it is very easy to start a thesis, but remarkably difficult to end. Hence do not get dragged down on what could potentially work in your thesis, but instead try to focus on what will most likely work, and is also feasible.\n\n==== Create an outline that you get critiqued ====\nA German proverb reads \"''Paper is patient''\". Writing what you want to do is hence vastly different from telling someone what you want to do. While talking you can adapt to the reactions of the other person, but writings need to be clear, concise and consistent. Therefore, writing an outline is one of the most pivotal steps of any emerging research. Experienced researchers may take a shortcut after they wrote dozens of papers, but I think that anyone with less experience under their belt should start with an outline. This also has the benefit that you can approach your peers or people you trust to give you honest [[Giving Feedback|feedback]]. When writing an outline, I build on a blueprint that I have used in years. Many people used it before, and it has proven valuable over time. '''You can find this blueprint here:''' [[File:Outline blueprint - Henrik von Wehrden.pdf|thumb|This is Henrik von Wehrden's blueprint for a scientific research outline.]]. It includes some remarks on typical pitfalls for each section.\n\n==== Have a bin next to your desk ====\nWhat is sadly relevant when developing your research is having a bin readily standing next to your desk. More often than not, initial ideas do not translate into a definitive scientific result. Most ideas die even before they are even tested, or to quote Thomas Edison: \"''I got a thousand ideas that did not work out.\"'' This is truly the case in my own research as well. I create most ideas for the bin, by a rough estimate I would say that one out of hundred ideas becomes my actual active research. Everything else is illogical, flawed, dull, impossible or gets swamped away by the tides of everything else. Research is iteration, and failure. This is probably the reason why many researchers I admire have a rather stoic attitude when it comes to failure - most of them failed at an uncountable rate. The question is now: Is this truth depressing? Some would certainly say yes, but - and I am transcending Derek Parfit here - I think it is liberating. You just need to learn this art of iteration, and not get too deeply attached to an early idea. Just like art, science often thrives as an almost never-ending pursuit, and learning to translate an initial idea into a clear outline is an art that you should practice. Do not feel bad because your pile of unused ideas is growing! I am often joyful when I see that someone had the same idea that landed in my bin, but instead made it into something wonderful. Is it not great to know that you had great ideas, and that other great people made these into a reality? It took me a while to figure this out, but I think today that this is a good thing indeed if more knowledge is out there. It is nice to know that these ideas became real contributions - though from somebody else - to the scientific community and society as a whole. We are all in this together, after all.\n\n== 3) Design ==\n[[File:How To Write A Thesis Outline.jpeg|thumb|Thesis Outline Source: www.thesishelper.com]] An outline gives an overview of what you want to do. Ideally you have an idea which theory you are building on or which conceptual approach you use, which methods you want to utilise, and what your thesis is all about - roughly. The design is where you want to develop all the precise details on how you want to conduct your research. This is the time when people often drown in papers and want to combine about 70 ideas and approaches from previous papers while ideally avoiding roughly 200 errors from other previous studies. Here, '''the main question is what is really important for your case''', and you need to learn to ignore what could be ''potentially'' important for your case. Remember the tree in the forest that no one is there to hear? This is the time when you should not hear this tree falling when you are not there. This is the tree right now. Can you hear it? No? Then move on!\n\n==== The prototype ====\nTrying things out is awesome, and in research it is almost essential to do prototyping whenever possible. Whether it is checking if interview questions work, make a test run of a workshop to see if the time would be well-balanced, or growing some seeds in a pot to see if your ecological design will be feasible. If you can prototype your design, or even better parts of it, and this can be done with a rather small effort, go for it. You can also prototype your work flow, trying different settings and check how you make progress. Your peers are the ideal reflexive space to test your prototype. Team up early with people that share a similar interest and work ethic, and exchange about each others' theses. You may find that their thesis is a welcoming distraction from your thesis, yet the feedback you can give to each other is one step forward to the joined goal of finishing your own thesis. \n\n==== The conceptual design ====\nTheory and the conceptual basis of the thesis are often the maelstrom that drags people down into limbo. The first and foremost rookie mistake I observe frequently - and that I made myself - is to try to combine too many concepts in one thesis. Working on ecosystem services, focusing on biodiversity, building on resilience while focusing on climate change adaptation? This is not going to work. Ideally, you build on one concept, [[Glossary|framework]] of theory. You may work on two, but then one of the two needs to be super concrete to tackle the other rather vague. I know how happy you are about all the deeper insights you gained over the last years concerning all sorts of concepts, and I know that our safe haven or bubble of sustainability science is only emerging, and hence tinkering with quite many concepts. In much of current research, theory plays a central role. There are approaches such as data science that explicitly do not focus on theory but instead on data, and this showcases that we live in exciting times. Still, I advise you to be rather conservative in your conceptual design, and to not bring yourself into a theoretical orbit from which you will never make it down to the ground on mother earth. After all, we should not forget that theory is helpful to construct a specific model of the world. Models are defined by compromises and imperfections, and we sometimes tend to forget this if we are all exited in the midst of it. \n\nAnother word to all of you who consider to write a conceptual work. From my specific perspective I would say: Don't! Do not write a conceptual work. While it may be exiting and some consider it feels very scientific to write a conceptual work, you need to be sure that your education and your personality allows you to do this. If you are a philosopher, or come from the wider circle of this hemisphere, then your education may be the foundation for a conceptual work. If you then also feel that in the past you were able to conceptualise complicated thoughts into an analytical and coherent line of thinking, fine. But do not underestimate the slumber you can basically build for yourself if you get entrenched in often century old debates about this theory or that concept. Many people in the past thought a conceptual work is easy, and learned the hard way that this is not true. Also, do not underestimate the experience of a solid empirical contribution. This may be your first scientific paper, if only in a smaller journal - it can be a start. Your results may matter deeply to a specific case, and thus contribute to bridge the gap between science and society. Last but not least, I have the hypothesis that empirical work trains you to see the world through the specific sense of your work: a precise focus, a defined perspective, a compromise in your field of view. The experience of empirical work should not be underestimated, as it can teach you how to look at something through a specific theory or conceptual framework.\n\n==== The methodological design ====\nI often prototype my sampling designs in [[Lego Serious Play|Lego]], deciding how many samples to take on which levels, how they are connected and so on. Thinking things through with the haptic power of Lego always helps me to visualise the bigger picture and allow for interactions with iterations I attempt when revising the methodological design. A more conventional approach to visualise and iterate a sample design is a whiteboard. I urge you to get a whiteboard by all means, and use it as often as possible. Making a clear figure that summarises your sample design is essential. Apart from the overall sampling, it is also good if you have compiled your analysis methods. Be as specific as possible to this end. Lately, many people write they will use a mixed methods approach, which is not specific at all. If you use different methods, you have to be as specific for each and every single method as you would be if you would only use one method. When creating a methodological design, I would advise you to prefer exchanging with experienced people as compared to relying on textbooks and papers. While especially the latter may give you some insights into your possibilities, the diversity of approaches that you may find in papers is often overwhelming and difficult to contextualise without experience. Remember that each of these papers has a history and a context. Do not be fooled by the assumption that all this is hyper-objective and super-reproducible. Recent trends in research advice us to be careful to this end, what works in one context might not work in another context. More importantly, most empirical papers are heavily biased based on the preferences and experiences of the authors. Textbooks are a different story. Many authors go through great lengths to provide a good introduction, and this is often very valuable. But one should not confuse such an introduction with the true power of experience. Textbooks are typically schematic superficialities and simply too shallow to enable an informed and sound contextualised design, but they can be a good starting point. However, this also depends on the grade of ripening of a specific method. If you want to rely on a interview campaign, this type of knowledge is somewhat available in textbooks. If you want to statistically analyse such data, there is also a fairly long canon of knowledge available. However, I would still say that presenting your pen-ultimate methodological design to someone with experience is the best way to gain some friction. Until you have your pen-ultimate design: iteration is the key to compensate for lack of experience (write that on a teabag).\n\n==== Your work life design ====\nI am truly fascinated by the context, setting and design in which famous people worked. Albert Einstein managed to pull off his best work as a patent officer. Maryam Mirzakhani's impact was short but stellar in so many ways. Franz Kafka wrote while working in the insurance business. In comparison, many of us are in a really privileged position. However it is not only the societal and cultural context that thrills me when wondering how influential people work. It is also the design of their work place. People have all sorts of diverse settings, but people I admire typically have workplaces that they designed to best fit their needs. While this may sound trivial at best, it showcases that these people designed places that give the impression that these people arrived. They are in the right place. Often this may not be a real place, but more of a workplace setting. I worked for years wherever I wanted, I just flapped my computer open. Now I learned that having a desk at home can be helpful in times of Covid 19. However, sometimes I change the setting, and write about a thousand words in the garden on my phone. I am however very mindful about these settings, and use the right setting at the right time. I detest for instance all travel by train. I love trains, but I cannot work there. I either fall asleep or my mind wanders. Trains are bad working places for me. I also need to cut distractions. This is indeed a very important design principle. Distractions are not benign by design, yet being distracted may sometimes clear the mind. Sometimes the best motivation can be to finish the draft in a cafe. Many colleagues I admire do this alll the time, and I learned from them. Again I have to be conscious in how a distraction does not ripe into a full blown procrastination. Here are a few things I think a good workplace might link like:\n\n* ''A desk.'' Separate work place and the rest of your place, even if all this is in the same room. You need to get your brain into work mode as soon as you switch places. \n* ''End work at times.'' Find breaks, do not work into the night no matter how cool you feel (no, you are not a night owl). Structure your day, write down what you want to achieve in the morning, and review it at the end of the day.\n* ''Clean up after yourself''. This can be a nice ritual to end the day. Maybe you want to review all open browser windows? Maybe shut the computer down all together? Clear all the cups away? \n* ''[[Research Diary|Journal]] and/or write on paper''. Or better, find different ways to reflect. This is crucial as you may want to track your progress and develop some consciousness about your work procedures. While this may be the hardest part, in the long run it is more important than what you work on right now. \n* ''Find a peer group with whom you can exchange''. This is what friends are for, at least partly. Help others, and learn from each other. \n\n==== Ending the design phase ====\nBeing done with the methodological design is incredibly hard for some. Letting go and deciding to start with the actual works a daring act, and many keep iterating and changing things. This is a fine line, since you naturally want to adapt if it needs to be, but you also want your planning and design to be worth something. You can always make things better, but this is about ending. You need to call it a day. I witnessed many people being admirably pragmatic to this end, yet for others it is hard. I think a good rule of thumb is the question whether you can anticipate for a fact that would change the patterns of your results. If the patterns do not change, then do not change anything. Too much has been lost already in our endless quest for all the ''what ifs'' in the world (write that on a teabag).\n\n== 4) Getting it done ==\n\n==== Brace yourself, reality is out there ====\nNow you read all this scientific literature, you had all these big BIG thoughts, and then you hit reality. Nothing works. It's complicated. '''You start worrying if this will ever end.''' And worst of all, nothing went as planned. Welcome to the reality of a researcher. From where I am standing, this is the greatest learning achievement for many. There are these few who just smoothly steer through this, but the empirical reality, or the conceptual desert for the theorists among you, hits many people the hardest. Yet this is the most important part about research, as research tries to understand parts of our reality through a constructed perspective. We are looking at a world from a certain direction, angle and through a lens, or better, looking glass. Progress is gained step by step, inching forward. Ideally you get yourself into a mindset to not exactly check every little step. While some people rely on a clear plan, other just try to get it done. I think it is in principle good to make a daily plain that is not too ambitious, and have a larger plan that can be adapted. I quit relying on deadlines in 2006, partly thanks to Douglas Adams: ''\"I love deadlines! I love the whooping noise they make as they go by.''\" As soon as I stopped relying on deadlines, I did not work up until the deadline and a wee bit further, but instead just developed the urge to get everything done - whenever, wherever. Many people do not work like this. This is another problem we face: Other people are different compared to us, hence other people work differently compared to us. Do not compare yourself with others, this will not help, I think. Instead try to focus solemnly on the task ahead of you. This is where you want to excel, want to get the data, finish your fieldwork, and go on. You can do it - just say no to everything else (write that on your wall). \n\n==== Finding a narrative, finding citations ====\nOnce you have your data, you should start to develop a narrative. If your research is very hypothesis-driven, then the narrative would have been clear from the very beginning. These are, however, muddy waters, as since the availability of larger data and more complex investigation, not all hypothesis-driven papers are clearly planned. Instead, with [[Why statistics matters|the rise of statistics,]] data started becoming available for everybody to sometimes tinker with it until the best pattern emerges. This more or less inductive approach is of course different from a hypothesis driven approach, but the border between inductive and deductive became more blurry during the last decades. This poses a problem for emerging researchers, as this iterative procedure gives a less clear structure, and make the analysis - may it be inductive or deductive - less straightforward. Also, the vast availability of research makes it often confusing for many young researchers [[Staying on top of research|to decide which publications to consider]] in their own research. Often, the ambition in people almost forces them to quote all papers they ever read, and construct meandering passages of text that bring us from one shore to the next, crossing large oceans full of all sorts of things. Also, many young researchers touch upon things in their emerging narrative that got next to nothing to do with how their research should be framed. Instead you should focus on the initial hook, the main point where you want to continue. Everything else should be logically assigned around this one point, but please not too much. A good introduction can have 3-5 parts, but not 10. Make sure to pitch your story in way that is understandable to others, which is the next point.\n\n==== Build on your peers often, and on your supervisor only when necessary ====\nThere is an almost general misconception of the role of a supervisor. Supervisors could make all the research you do probably in a blink of an eye. Any problem you might want to solve, they could solve, and much faster. If they would do that, they would deprive you however of the most important goal any thesis has: Making you an independent researcher. With a thesis you prove that you are independently able to create a scientific work. Science builds on collaboration, and yes, they are called supervisors. To me, all this is however trumped by the fact that you need to become independent in being able to conduct research. If you are not able to prove this at the end of your study program, then it would be difficult to acknowledge that you are a scientist. Therefore, I think, supervisors may give guidance and general directions, and they may even support you on a meta level as well as considering the broader embedding of your research into the wider context. What they should however not do is make the researcher with you, or make the research for you. They should not solve every second problem you have. This would - to me - be particularly bad because then you become not independent if you are pampered. It is not all about being able to conduct the research, it is - more importantly - about finding your own voice, fostering your own agency as a researcher. Otherwise, most researchers would be clones of their supervisors, something that I would consider to be altogether undesirable. \n\n[[File:pexels-startup-stock-photos-7096.jpg|thumb|Meet with your peers. Source: pexels.com]] Since research lives through collaboration and interaction, it may however be essential to interact with your peers. Their lived reality is so much closer to yours, and they can help you with all sorts of things. They may have experiences with certain theories or methods, may know the literature about certain aspects, and they may be a good mirror to reflect on your research. They can also motivate you, give you a little push, and support you along the way. Having a small team of peers around you is I think one of the most important building blocks of the early stage of most researchers I know. It is important to find people with a similar interest, both in terms of the topics but more importantly in terms of the modus operandi. Being interactive increases the ''Serendipity space'', that is the space in which something good may happen to you. This is almost never more important than in your early career, where you might want to maximise this space. Finding peers to interact and learn together is ideal to jointly develop into independent researchers. This is especially important since this may be your first step into being a teacher and a mentor, albeit for your peers. Building tolerance, learning from others and being nevertheless ruthless in your feedback if necessary is essential, I think. I learned so much from my peers along the way, and I owe them so much. I encourage you to try the same.\n\n==== Research is study and practice ==== \nMany people told me they want to become researchers over the years. This is all fine to me, but we should never forget that research is a craft, it is years of practice that make you a researcher. Instead of understanding that you better start with years of study and practice now, many believe they will have some sort of an epiphany. In my experience there will be however no shining light from heaven that will tell you that now you are a researcher. We make such things up in retrospect quite often, the famous ''and in that moment I knew I wanted to become a researcher''. When I was young I wanted to be all sorts of things, including fire fighter and train conductor. All this did not happen. Do not misunderstand me, I was always thrilled by science, our planet, how people live, but I was also thrilled by playing guitar, material arts and playing video games. Do not put too much expectations into the process of conducting active research. In my experience, it will take time to like it, and it will be a process that is also build around a lot of failures and frustration. Yet, I could not think of anything better for me. It is your own challenge to find out if it works for you as well.\n\n== 5) Reflection and iteration ==\nAt some point in your work you will have something written at every section that you planned from the beginning, or that subsequently developed while working on your thesis. '''Your work is done, or so you think'''. This is the starting point of your iteration. You already achieved something, no doubt. I always have trouble looking at a part of a work, because it could go on in all sorts of directions. Hence having a whole work done once is an ideal setting for an iteration. Give the work to one of your peers, and all the while start working on consistency. More often than not a typical problem of beginners in research is either inconsistency on working, or too much repetition of certain words. Be consistent in concepts and theories, but still write vividly and engaging, not overly relying on the same phrases. This is quite a challenge, which is why I suggest every writer to write 500-1000 words per day outside of the normal writing process - as long as you are not a total pro, meaning you make your money with writing. Did you ever hear of a professional dancer who never practices dancing or a musician who does not practice their instruments for hours per day? Just like them, you need to practice your skills in writing. This enables you to develop the experience to rewrite your thesis draft, and make it better. \nWhat can also be helpful is to deconstruct your thesis. How is the argumentation, what do you want to say in which section? Is the flow of arguments straightforward? Is the structure of the introduction reflected in the discussion? Do you only deal with concepts that are relevant for the thesis? Is all relevant literature quoted, but not too much? Is the style of writing readable? Do you provide clear take home messages? And most of all, did you decide? Writers make decisions. Quite often I read texts of young researchers, and you can see that they could not decide which direction to go. They wanted to put everything in, overburdening the text, and making the hook almost impossible to find. You need to decide what you want to keep in, and what is just an additional thought you may have considered necessary on the way, but coming to think of it not relevant.\nAllow me to say a word on time management regarding this stage. In my experience, most students writing a thesis work right up to the deadline, becoming increasingly frantic towards the end, and quite some are even extending the deadline if at all possible. All planning that was done by so many people almost never paid off. In most cases I am aware of, people just completely ripped their timeline, became exhausted, repeatedly pulled all-nighters, and would have liked to improve their work even more. What does this tell us? I think it is normal that this is a stressful and not always fulfilling time. After all, it is often the largest hurdle people faced so far. So let us be honest with ourselves. This can be stressful.\nAs a supervisor, it is also important for me to read a thesis 1-2 weeks before it is finalised. It is just sad if a good piece of work is getting a worse grade because of some error that could have been prevented.\n\n== 6) Finalising ==\nNow all the content is there, all you need to do is '''print the whole thing and be done with it for real'''. Alas, once more the warning should have been written in the beginning. Formatting and printing a text is often one of the most stressful step. There are at least 3 types of common problems:\n\n==== Technical problems ====\nIt is remarkable how bad Word can be, yet 99 % of us have to use it. Formatting on Word can be a real horror, and certainly took me a lot of time during my thesis. Ideally, familiarise yourself with the settings before the thesis, and make an online course. This may be the time that your geek friend becomes your life saver. However you solve the technical problems in writing a thesis, ideally try all things beforehand so you know what's coming. In addition, use a citation software. Creating citations by hand is not only a lot of work, but also bound to create errors. Go for Zotero, Citavi, whatever works for you. Last, make a pdf of your final work, so you can share it. Make sure that everything is correct in this fine pdf file. This will not only be the file you share with the printer shop, but also the file you may want to share with anyone interested in your work. Me, I prefer to read a thesis online. \n\n==== Not being able to let go ====\nYes, you put so much work into it. I understand you feel like it is not finished yet. This is perfectly normal. Any work can always still be improved; at least in research, there is hardly something perfect. Still, there are many contributions that are very good, and I invite you to become part of this. It often works to tell others when you hand in, and to have them support you. I saw it that often that people want to go on and continue, yet I advise you to not do that. Quite often people just keep iterating in the end, but not in a good way. They are just unable to take the final decisions. However, when it comes to smaller mistakes I would advice you to focus on your supervisor. Me, I do not care about one or two small errors, and I am not good at spotting these. Other supervisors are different. Out the last mile into the things that are important to your supervisor. \n\n==== Administrative problems ====\nMake sure that you know early on how to hand in, considering where to give paper versions. Who needs a digital version, and which other administrative steps are necessary to bring the whole thing finally to an end? Regarding administration, it is best to ask people who went through the whole process what to do when. Also, make sure the supervisors know when you hand in, and already book a defense date early with them. Ask them whether the time in between is enough for them to read your thesis. Also, make sure how you can end so that you do not need to pay tuition fees for the next semester. Often I need to make a lot of defenses right before this. Planning ahead helps you also with letting go, because then you really have to let go, since your defense date is set.\n\n==== Never look at it again once you printed it ==== \nMost people look at the thesis a day after they printed it, and find a bunch of mistakes. Again, this is perfectly normal. To this end, it is relevant that your supervisor is either not so keen to find mistakes (like me), because otherwise you need a good editor for the final check. However, I would advise you not to look at your work before the defense, at least not reading it so deeply that you find mistakes. This makes you only feel bad, and this cannot help, right? Therefore, once you have no chance of changing things, do not look at your thesis again. What is more important is to consider flanks in your argumentation, weak points to consider for the defense, which brings us to the next point.\n\n== 7) Defense ==\n\n==== Preparing ====\nThe best way to prepare for a defense is to cumbersome '''anticipate which open questions arise from your work''', especially from the view of your supervisors. You typically need to give a short and concise summary of your work, focusing on the more interesting aspects, but also giving the necessary overview. Many consider the 10-15 minutes you have to be not enough. I understand this, but then again consider that most Ted talks are equally long (or short). Be brief and to the point. Also, prepare lots of additional slides that anticipate potential questions, and master your repertoire. Many people feel that practicing the presentation too often feels kind of dull, but do not worry: The adrenaline during the real thing will up your pulse, and ideally also up your game. \n\n==== The presentation ====\nYou need to be to the point. Do not have too many slides. No text deserts. Self explanatory figures. Do not talk too fast. Try to look at your audience. Get the essence of your work, but also be critical. You need to present the main results and discuss these. There is so much things to consider. The most important one: Find your own voice. Try to get inspired by others, and tinker with the way you present things. This ideally leads you to developing your own genuine presentation mode, which should be your best in this situation.\n\n==== The questions ====\nMany supervisors have a lot of questions, hence you should be prepared. Always think for a second before you answer, or best while the question you are being asked is elaborated on. There are many rhetorical tricks how to answer. The most common mistake is that people answer too long and build in too many caveats. Be brief, but to the point. Also be positive, and ideally energized. People should feel that you are confident, which you can be, but not arrogant. Most importantly, do not be thrown off balance by a super hard question. Many supervisors push the limit, including me. You need to stand by your work, but also see its limitations. There is a difference between these two things. Understanding this is at the heart of the academic agenda, and often also a good defense.\n\n==== Accepting your grade ====\nSupervisors may differ in grading but are typically at least internally consistent. Hence, I would suggest to be confident that the grade you get is the one you deserve. I had counterexamples as a student, but even then, I could not do anything about it. Hence, I advise you to accept the grade you get. More often than not, you deserve it, and hopefully can be glad about it. Celebrate it. \n\n\n== 8) Celebration ==\nI often witness peers throwing a small party after the defense right in front of my office. Such testimony of friendship is very moving, yet the person being celebrated often looks shell-shocked, even with the best grades. Quite often they do not feel like celebrating. Yet I think, having such a small party is not only a nice academic tradition, but also demarks a good starting point to move on. \nI would also encourage you to celebrate at graduation day, as this is an important academic tradition, and as a professor I have to say that it is quite an honor to meet the parents of the students that often grew so dear to my heart. I think having this exchange is an important commitment and opportunity for both sides, as these brief encounters seal the trust that could hopefully be built between parents and the teachers through their children. I owe so much to my parents and my academic teachers and think that a meeting of them really ended my PhD officially. The students are often slightly ashamed by what is being said, which is quite entertaining, I have to say. Endure it, these will become nice memories, I hope, and an important steppingstone in your coming-of-age.\n\nWhat is most relevant is to '''look back at that experience and take stock'''. If you are done with your thesis, then you should not only celebrate with the outside world. Be cheerful, you made something remarkable for yourself, and contributed if only a wee bit to something so much larger. Personally, I remember how long ago I finished my Diploma thesis, and how many helped me along the way. Looking back at it, it seems like a small step now, but it was a big step then. As my supervisor said when I had my thesis handed in: \u201c''A small step for you, but a big step for [hu]mankind''.\u201d\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "qxddpmxuvtpksg8prjtcdsikmny3osj"
                }
            },
            {
                "title": "Iconology",
                "ns": "0",
                "id": "822",
                "revision": {
                    "id": "6875",
                    "parentid": "5946",
                    "timestamp": "2023-01-14T13:44:45Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16229",
                        "#text": "[[File:ConceptIconology.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Iconology]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br><br>\n'''In short:''' Iconology is a research method that is traditionally used to detect an overall worldview of a past decade or location by interpreting a painting of that time or area.\n\n== Background ==\n[[File:Iconology scopus plot.png|400px|thumb|right|'''SCOPUS hits per year for Iconology until 2022.''' Search term: 'Iconology' in Title, Abstract, Keywords. Source: own.]]\nFrom Greek \u201cIcon\u201d = picture, \u201cLogos\u201d = word, thinking, speech, reason (4, 3)\nIconology is a research method originating in the field of art history and was founded by the German art historian Erwin Panofsky in cooperation with Aby Warburg in the 1930s (6, 3). '''To this day it is one of the groundworks for all visual analysis done in art, media, theatre, and performance analysis''' (4, 8). Contemporarily, it is also used as an analysis method in research fields like architecture, literary studies and even politics (4, 8). This entry focuses on the basics of the traditional Iconology. However, <artwork> and <artist> can also be replaced by any other object and practitioner and can be especially effective in experimental, inter-, and [[Glossary|transdisciplinary]] research.\n\n== What the method does ==\nGenerally, Iconology works as a three-step model through which different questions are formulated and the according insights are put together subsequently in a complex manner (10). The three steps are: the pre-iconographic step, the step of interpretation (or iconography) and the iconological synthesis (10). Panofsky advised to not consider the steps as absolutely separated, because the verbal articulation of the artwork can only really be usefully read in its complete meaning after the analysis and interpretation (7). \n\n==== 1 - Pre-iconography ====\nThe pre-iconographic step involves the analysis of all formal aspects of the artwork (1a) and the articulation of the emotional reception of the work (1b) (10). Usually, the formal aspects are described upfront (1, 10). However, sometimes the emotional reception is formulated first and positioned ahead of the rest of the analysis as a \u201cpercept\u201d (zu Deutsch: \u201cPerzept\u201d), because it involves the researcher\u2019s subjectivity and is therefore a non-scientific insight which some writers and readers like to separate from the scientific text. Nonetheless, the percept is an important part of the iconological analysis and should not be completely excluded, because it can situate the analysis historically and culturally and lead to fruitful insights for the analysis itself and for foreign or future audiences.\n\n'''1a - The formal analysis'''\nThis step should include naming the artist and the date and place of production as well as the ownership or place of exhibition, identifying the type of artwork (i.e. painting, sculpture, performance etc.), the scale of the artwork, all materials used, genre and epoch, the form of composition, the colour composition (saturation, light and shadow, forms of contrasts, colour schemes) and all depictions which refers to the recognizability of all things depicted, i.e. the scale of abstraction, the structure, and the arrangement/positioning of things portrayed (fore, mid and background, size of things, left or right) (1, 10).\n\n'''See below for a list of all aspects of the formal analysis in more detail:\n'''* Name of the artwork: note the name of the artwork.\n* Naming the type of artwork: make sure to note the type of artwork. Is it a sculpture? A painting? Mixed Media?\n* Name the scale and format of the artwork.\n* Naming the artist: note the artists full name, date of birth and death.\n* Name the date and place of production.\n* List all materials used during the production of the artwork.\n* Name the genre and epoch of the artwork only if you are absolutely sure it is correct. Especially analyses that were conducted before the cultural and pictorial turn of postmodernism, which included cultures and pictures of everyday life as scientifically significant and actively socio-politically produced (7), should be paid attention to. These analyses often ascribed genres and epochs to artworks solely because of the culture and time they were produced in, ignoring the fact that some paintings are not common but in opposite very unusual for their date and place of production.\n* Identify the form of composition: this step is quite hard to recognize without training, but there are a few tricks on how you can tell the composition by the contours and placements of the artwork\u2019s contents. These are the traditional compositions: symmetrical and asymmetrical (vertical, horizontal), dynamic linear (diagonally from left upper side to right corner or vice versa, the former is precepted as soothing in European culture, the latter as arousing), square, oval, round, triangular and the golden section. Of course, combinations are possible, but there is in almost all cases one compositional form that dominates the arrangement of objects and colours within an artwork.\n* Identify the composition of colours: the composition of colours includes the most dominant colours of the painting, including questions like: which colour has been used most dominantly? Which colours are unusually bright? It is best to start with identifying the contrasts within the painting, because it already tells a lot about what colours are used the most and what are more significant than others. The most important contrasts are light and dark contrast, pure and impure/muted colours, and the complementary contrast. Be careful about identifying pure and muted colours, since pure colours can be mixed colours and muted colours are not just mixed colours but those mixed with either black or white. Additionally, sometimes harmonies and colour schemes are helpful to conclude the part about colour, but the presence and absence of contrasts are always necessary to mention.\n* Identify the depictions, scale of abstraction, size of objects and arrangement: it is important to be careful about interpreting certain objects of depiction as specific objects or beings, especially if the painting or parts of it are abstract. However, if objects are clear to you, it is important to name them. Sometimes it is easier to first note the depictions and then explain the colour composition, but both steps can also be combined, as well as mentioning the scale of abstraction, and the arrangement and size of the objects and the logic of organization, including the gradation of the objects in the fore-, middle-, and background and the probable absence of one of the three. (cf. 1)\n\nDisclaimer: In general, if something is unknown or unrecognizable, always make sure to state that after referring to what is known and seen about a category.\n\n'''1b - The emotional reception/the percept''' \nThis is traditionally structured in a way that leads from the overall perception to that of single objects or colours within the artwork. '''Usually, the atmosphere is perceived through emotionality and therefore described during the percept.''' It can however also be part of the formal analysis if the atmosphere is very obvious and therefore rather part of the common sense than a personal perception. It is important to not only state why a feeling about the artwork occurs but to make assumptions about why this feeling occurs. Contemporary iconological analysis also often refers to assumptions made about how one\u2019s own culture and generation might react to the object. It is very important to state that any universalizing claim is clearly formulated as an assumption or probability (10).\n\n==== 2 \u2013 Interpretation/Iconography ==== \n'''This step involves the consideration of the cultural-historic environment of the artist, the theme and style (10).''' Primary as well as secondary literature is used for research here (21). It is noteworthy that some artworks, specifically European landscape, still-life, and genre paintings as well as non-representational art do not require this step (6). For those who do however, a first interpretative conclusion is derived only based on the cultural-historic and biographic background of the artist. The artwork can be discussed as a whole and as specific to a genre or epoch or opposingly as unusual. Discussing detailed aspects of the formal analysis is specifically effective when discussing icons and symbolic contents that are clearly relatable to the time of production. However, as with the percept, it is important to consider that sometimes icons, symbols and colour choices were meant to portray the opposite of the traditional understanding of the displayed. Some artists consciously chose to ironically engage with their culture\u2019s, genre\u2019s, and epoch\u2019s traditions. \n\n==== 3 - Iconological synthesis ==== \nThe final step of the iconological analysis is to '''connect the insights from the pre-iconographic analysis and interpretation''' to unravel the deeper meaning of the artwork and the artist's probable intention. The percept can be helpful in this step to clarify findings and to approve that the artist's intentions were put to work effectively. Motifs and single items are put into relation with one another to understand the narrative of the artwork. Colours and shapes are now not only set within an historical background, but also comprehended as an entire tone of expression, representing specific moods or feelings. Similar to the overall atmosphere, single settings within an artwork are put into relation. Finally, all those smaller settings are being interconnected or juxtaposed with each other and with the historical and biographical background of the artist to conclude the probable intended meaning of the artwork and moreover the worldview of the time (10, 1, 6, 8).\nThe following scheme (see Fig. 1) is the original overview of Panofsky\u2019s iconological analysis:\n \n[[File:Iconology.jpg|700px|thumb|center|'''The steps of interpretation''' by Erwin Panofsky in \"Iconology and Iconography\". [https://visual-studies.com/images/no2/fornacciari/02.jpg Source].]]]\n\n==== Contemporary Iconology ====\nContemporary Iconology, compared to traditional Iconology, also includes other fields in which Iconology has become an effective method of analysing present and past events. Political iconography for example is capable of analysing artistic and medial staging of governments and ruling classes (4). Such visual representations can lead to new findings about political history or reverse national ideas about their own history which reveal possible \u2018false histories\u2019 by the means of Iconology (4). Spatial Iconology or architectural Iconology engage with the organization of rooms, their interior structure, its historic symbolical meaning, and how it influenced perception and usage of space by certain groups of society. These forms were an adaption by G\u00fcnter Bandman of Panofsky\u2019s traditional iconography which focused on paintings (4, 8). Also, archaeology is a prominent field in which Panofsky\u2019s Iconology is in constant use to determine pottery, paintings, and sculptures (162).\n\n\n== Strengths & Challenges ==\nPanofsky\u2019s iconographic method is a great way to gain an overview about visual materials. The last step is especially fruitful on gaining insights on the side of the production, may it be the environment of production or the producer/artist as such. Iconology is ideal to research a certain worldview expressed through visual materials. However, it is very important to also accept the possibility of a painting to express the opposite of what information the iconographic step has provided. If done with caution and attention, the pre-iconographic step and the iconological synthesis will draw attention to such discordancies.\n\nThe iconographic turn is an attempt of some postmodern thinkers to redefine the way in which images are understood (2). Other than Panofsky\u2019s approach, which focused on reading the worldview transmitted by pictures (9), Brock wanted to add the term \u201cBilderwelten\u201d (Brock in Burda, p.118) (\u2018pictorial worlds\u2019) which means to additionally uncover and interpret any internally ordered unities within a picture (cf. Brock in Burda, p.118). From then on, Panofsky\u2019s iconological model has been decreasingly developed itself but has since become more specialized by new aspects being added rather than changed (3). \n\nAdditionally, there is the issue that Iconology often leads to different and even opposing findings than other art research methods of which Iconology is always the more traditional approach (3). One prominent attempt to widen Panofsky\u2019s method was William Mitchells \u201caim [\u2026] to further generalize the interpretive ambitions of Iconology by asking it to consider the idea of the image as such\u201d (5). He thus asked two key questions: \"What is an image?\" and \"What is the difference between images and word?\u201d (1). He continues by defining an image generally as a thing that represents objects in the world with a certain \u201clikeness\u201d, \u201cresemblances\u201d, or \u201csimilitude\u201d (10). Topper argues however, that this is still not enough and adds that \u201cmaps, diagrams and d\u00e9cor\u201d do not always meet any of Mitchell\u2019s criteria, yet need to be included in the definition of what an image is (9). Nevertheless, none of the two specify how Iconology should adapt to make an interpretative analysis of the more generalized term \u2018image\u2019 possible.\n\n\n== Outlook ==\nIt is very probable that the rise of inter-, multi-, and transdisciplinary studies and the ongoing postmodern criticism on the sole acceptance of the scientific method of the natural sciences will contribute to the progress of widening and complementing traditional Iconology and hence make a greater implementation of the method possible.\n\n\n== Key Publications ==\n* Mitchell, W.T.J. 2009. ''Iconology: Image, Text, Ideology''. University of Chicago Press.\n* M\u00fcller & Geise. 2015. ''Grundlagen der Visuellen Kommunikation''. utb Verlag. \n* Panofsky, E. 2006. ''Ikonographie und Ikonologie - Bildinterpretation nach dem Dreistufenmodell''. DuMont. \n* Petersen & Schwender. 2018. ''Die Entschl\u00fcsselung der Bilder. Methoden zur Erforschung visueller Kommunikation. Ein Handbuch.'' 1st Edition, Herbert von Halem Verlag.\n\n\n== References ==\n(1) Bambach-Horst, E. 2012. ''Bildanalyse und Bildinterpretation.''. In: ''Kunstgeschichte. Von der Antike bis zum 21. Jahrhundert.'' Dudenverlag, Berlin.<br>\n(2) Burda et al. 2010. ''In medias res. Zehn Kapitel zum Iconic Turn.'' Fink, Paderborn.<br>\n(3) Irsigler, F. 2006. ''M\u00f6glichkeiten und Grenzen der Ikonologie als kunsthistorische Methode. Panofsky und seine Kritiker''.<br>\n(4) Leben, L. 2017. ''Ikonographie und Ikonologie''. In: \"Methoden der Kunstvermittlung\".<br>\n(5) Mitchell, W.T.J. 2009. ''Iconology: Image, Text, Ideology''. University of Chicago Press.<br>\n(6) Morosow, A. 2014. ''Kunstmittel der Ikonologie''.<br>\n(7) Sardar & van Loon. ''Introducing Cultural Studies.''<br>\n(8) Sch\u00e4dler, U. 1993. ''Ikonologie und Arch\u00e4ologie''.<br>\n(9) Topper, D. 1989. ''Image, Text, Ideology by W.J.T. Mitchell (review).'' Leonardo 22 (3/4). 449\u2013450.<br>\n(10) Wirth et al. 2011. ''Kunst und Bilder richtig verstehen?''. Kompaktwissen Oberstufe, 3rd edition. Cornelsen Scriptor, Mannheim.<br>\n----\nThe [[Table of Contributors|author]] of this entry is Mona H\u00fcbner.\n\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]"
                    },
                    "sha1": "g7ag9ytrmn21vqw7ecbn4xej5kmh9m9"
                }
            },
            {
                "title": "Interviews",
                "ns": "0",
                "id": "752",
                "revision": {
                    "id": "6425",
                    "parentid": "6410",
                    "timestamp": "2021-10-28T05:42:50Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5164",
                        "#text": "'''This page provides an overview on diverse facets of Interview methodology.'''\n__NOTOC__\n=== How to find the right Interview method ===\nYou may have decided that you want to ask people things for your research, but you don't quite know which approach to choose.<br>\n\n'''We thought about this, and we decided that in the end, it comes down to two things:''' \n* ''Do you want qualitative or quantitative insights?'' \n** Qualitative means that you plan to go into depth with fewer individuals, and understand their worldviews. Qualiltative Interviews lead to results that are hard to compare, but can be of great help when a topic shall be openly and inductively investigated.\n** Quantitative means that you are interested in larger numbers of responses which you can calculate with, or in gathering quantitative responses right away, e.g. on a Likert Scale. Quantitative Interviews typically lead to more comparable results, and are often useful for more deductive approaches when your theoretical assumptions are clear.\n* ''Do you want interactive or non-interactive Interviews?''\n** Interactive means that you maintain control over the Interview conduction, with pre-determined questions and a strong impact on the questions that individuals respond to.\n** Non-interactive means that you rather observe and listen. You have some things you are interested in, but you also want to let the Interviewees speak freely.<br>\n\n'''If you can decide on these two questions, choose the appropriate Interview method from the following table'''. You can also have a quick look into the respective entry to check if this is really what you are looking for.<br>\n{| class=\"wikitable\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! Interview Method !! Short description !! Qualitative or Quantitative? !! Interactive or non-interactive?\n|-\n| [[Open Interview]] || Open, unstructured conversations with individuals || Qualitative || Non-interactive\n|-\n| [[Narrative_Research#Narrative_Interviews|Narrative Interview]] || Open Interviews with a focus on the creation of narratives through the Interviewee || Qualitative || Non-interactive\n|-\n| [[Ethnography#Observing_&_Interviewing|Ethnographic Interview]] || Open Interviews conducted in the field during Ethnographic Research || Qualitative || Non-interactive\n|-\n| [[Focus Groups]] || Group discussions with a focus on the participants\u2019 viewpoints and group interaction and dynamics. || Qualitative || Interactive\n|-\n| [[Semi-structured Interview]] || Loosely pre-structured conversations with Interviewees. || Quantitative & Qualitative || Interactive\n|-\n| [[Survey Research|Structured Interview]] || Strongly pre-structured conversations with Interviewees. || Quantitative || Interactive\n|-\n| [[Survey Research|Questionnaire]] || Written documents used to gather responses from pre-determined options. || Quantitative || Interactive\n|}\n\n\n=== What you need to know for your Interview research ===\n'''After choosing an appropriate method, there are a few things to consider.''' <br>\nWe created the following diagram to show you which other Wiki entries we think you should read when you plan to conduct research based on one of the Interview methods. These entries will help you get a better overview of what is important to your research, and will hopefully help you design and shape your methodological approach. The diagram is clickable - just choose a topic that you are interested in!\n* The left area shows the different types of Interviews. Select one and follow the respective arrows to the right.\n* The central area shows you methods of data analysis that are relevant for the respective Interview methods.\n* The yellow boxes highlights aspects of normativity that you should pay attention to.\n\n<imagemap>Image:Interview Level 2 Sankey.png|center|1000px\nrect 14 62 186 108 [[Focus Groups|Focus Groups]]\nrect 14 139 191 182 [[Open Interview|Open Interview]]\nrect 11 185 101 201 [[Ethnography#Observing_&_Interviewing|Ethnographic Interview]]\nrect 106 186 194 201 [[Narrative_Research#Narrative_Interviews|Narrative Interview]]\nrect 16 231 191 2752 [[Semi-structured Interview|Semi-structured Interview]]\nrect 14 305 188 347 [[Survey Research|Structured Interviews]]\nrect 14 377 187 420 [[Survey Research|Questionnaires]]\nrect 282 208 423 252 [[Transcribing Interviews|Interview Transcription]]\nrect 465 21 608 69 [[Grounded Theory|Grounded Theory]]\nrect 445 150 632 201 [[Content Analysis|Qualitative Content Analysis]]\nrect 453 282 636 331 [[Content Analysis|Quantitative Content Analysis]]\nrect 464 382 608 433 [[Statistics for Surveys|Linguistic Statistics for Surveys]]\nrect 190 469 344 491 [[Bias in Interviews]]\nrect 41 13 161 38 [[Sampling for Interviews]]\nrect 656 409 796 457 [[An_initial_path_towards_statistical_analysis#Univariate_statistics|Univariate Statistics]]\nrect 648 228 788 276 [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|Multivariate Statistics]]\nrect 844 331 959 360 [[Design_Criteria_of_Methods#Deductive_vs_Inductive|Deductive research]]\nrect 833 124 953 150 [[Design_Criteria_of_Methods#Deductive_vs_Inductive|Inductive Research]]\n</imagemap>"
                    },
                    "sha1": "pduw3jqverq6luxu12xqc9pc12t7qsy"
                }
            },
            {
                "title": "Introduction to statistical figures",
                "ns": "0",
                "id": "704",
                "revision": {
                    "id": "6613",
                    "parentid": "6602",
                    "timestamp": "2022-04-13T12:31:38Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20514",
                        "#text": "'''In short:''' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically '''barplots'''. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.\n\n'''Correlation plots''' ('xyplots') are the next staple in statistical graphics and most often the graphical representation of a correlation. Further, often also a regression is implemented to show effect strengths and variance. Fitting a [[Regression Analysis|regression]] line is often the most important visual aid to showcase the trend. Through point size or color can another information level be added, making this a really powerful tool, where one needs to keep a keen eye on the relation between correlation and causality. Such plots may also serve to show fluctuations in data over time, showing trends within data as well as harmonic patterns.\n\n'''Boxplots''' are the last in what I would call the trinity of statistical figures. Showing the variance of continuous data across different factor levels is what these plots are made of. While histograms reveal more details and information, boxplots are a solid graphical representation of the Analysis of Variance. A rule of thumb is that if one box is higher or lower than the median (the black line) of the other box, the difference may be signifiant.\n\n[[File:Xyplot.png|250px|thumb|left|'''A Correlation plot.''' The line shows the regression, the dots are the data points.]]\n[[File:Boxplot3.png|250px|thumb|right|'''Boxplots.''']]\n[[File:2Barplots.png|420px|thumb|center|'''Barplots.''' The left diagram shows absolute, the right one relative Barplots.]]\n\n\n[[File:Histogram structure.png|300px|thumb|right|'''A Histogram.''']]\nA '''histogram''' is a graphical display of data using bars (also called buckets or bins) of different height, where each bar groups numbers into ranges. They can help reveal a lot of useful information about numerical data with a single explanatory variable. Histograms are used for getting a sense about the distribution of data, its median, and skewness.\n\nSimple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]\n[[File:Introduction to Statistical Figures - Bipartite Plot example.png|300px|thumb|right|'''A bipartite plot showing the affiliation of publication authors and the region where a study was conducted.''' Source: Brandt et al. 2013. A review of transdisciplinary research in sustainability science.]]\n[[File:Introduction to Statistical Figures - Structural Equation Model.png|400px|thumb|center|'''A piecewise structural equation model quantifying hypothesized relationships between economic and technological power, military strength, biophysical reserves and net imports of resources as well as trade in value added per exported resource item in global trade in 2015.''' Source: Dorninger et al. 2021. Global patterns of ecologically unequal exchange: Implications for sustainability in the 21st century.]]\n\n\nMultivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.\n\n[[File:Introduction to Statistical Figures - Ordination example.png|450px|thumb|left|'''An Ordination plot (Principal Component Analysis) in which analyzed villages (colored abbreviations) in Transylvania are located according to their natural capital assets alongside two main axes, explaining 50% and 18% of the variance.''' Source: Hanspach et al 2014. A holistic approach to studying social-ecological systems and its application to southern Transylvania.]]\n\n[[File:Introduction to Statistical Figures - Circular Network Plots.png|530px|thumb|center|'''A circular network plot showing how sub-topics of social-ecological processes were represented in articles assessed in a systematic review. The proportion of the circle represents a topic's importance in the research, and the connections show if topics were covered alongside each other.''' Source: Partelow et al. 2018. A sustainability agenda for tropical marine science.]]\n\n'''Descriptive Infographics''' can be a fantastic way to summarise general information. A lot of information can be packed in one figure, basically all single variable information that is either proportional or absolute can be presented like this. It can be tricky if the number of categories is very high, which is when a miscellaneous category could be added to a part of an infographic. Infographics are a fine [[Glossary|art]], since the balance of information and aesthetics demands a high level of experience, a clear understanding of the data, and knowledge in the deeper design of graphical representation.\n\n\n'''Of course, there is more.''' While the figures introduced above represent a vast share of the visual representations of data that you will encounter, there are different forms that have not yet been touched. '''We have found the website [https://www.data-to-viz.com/#connectedscatter \"From data to Viz\"] to be extremely helpful when choosing appropriate data visualisation.''' You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.\n\n\n== How to visualize data in R ==\n'''The following overview includes all forms of data visualisation that we consider important.''' <br>\nBased on your data, have a look which forms of visualisation might be relevant for you. Just hover over the individual visualisation type and it will show you its name. It will also show you a quick example which this kind of visualisation might be helpful for. '''By clicking, you will be redirected to a dedicated entry with exemplary R code.'''<br>\n\nTip: If you are unsure whether you have qualitative or quantitative data, have a look at the entry on [[Data formats]]. Keep in mind: categorical (qualitative) data that is counted in order to visualise each category's occurrence, is not quantitative (= numeric) data. It's still qualitative data that is just transformed into count data. So the visualisations on the left do indeed display some kind of quantitative information, but the underlying data was always qualitative.\n\n<imagemap>Image:Statistical Figures Overview 27.05.png|1050px|frameless|center|\ncircle 120 201 61 [[Big problems for later|Factor analysis]]\ncircle 312 201 61 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA, TREE SPECIES IN AMERICA as three colors, with joint species in the overlaps]]\ncircle 516 190 67 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA as two colors, with joint species in the overlaps]]\ncircle 718 178 67 [[Stacked Barplots|Stacked Barplot, e.g. count data of different species (colors) for the variable TREES]]\ncircle 891 179 67 [[Barplots, Histograms and Boxplots#Barplots|Barplot, e.g. different kinds of trees (x) as count data (y) for the variable TREES]]\ncircle 1318 184 67 [[Barplots, Histograms and Boxplots#Histograms|Histogram, e.g. the variable EXAM POINTS as count data (y) per interval (x)]]\ncircle 1510 187 67 [[Correlation_Plots#Line_chart|Line Chart, e.g. TIME (x) and BITCOIN VALUE (y)]]\ncircle 1689 222 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), POPULATION (bubble size)]]\ncircle 1896 238 67 [[Big problems for later|Ordination, e.g. numeric variables (AGE, INCOME, HEIGHT) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\ncircle 202 326 67 [[Treemap|Treemap, e.g. FORESTS (colors) and count data of the included species (rectangles)]]\ncircle 410 323 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. different species of trees (absolute count data per color) for the variables TREES in ASIA, TREES IN AMERICA, TREES IN AFRICA, TREES IN EUROPE (x)]]\ncircle 608 295 67 [[Stacked Barplots|Proportions Stacked Barplot, e.g. relative count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 812 277 67 [[Pie Charts|Pie Chart, e.g. different kinds of trees (relative count data per color) for the variable TREE SPECIES]]\ncircle 1015 308 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE HEIGHT (y) for beeches]]\ncircle 1228 287 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS per point (x)]]\ncircle 1422 294 67 [[Correlation_Plots#Scatter_Plot|Scatter Plot, e.g. RUNNER ENERGY LEVEL (y) per KILOMETERS (x)]]\ncircle 1574 379 67 [[Big problems for later|Heatmap with lines]]\ncircle 1788 401 67 [[Correlation_Plots#Correlogram|Correlogram, e.g. the CORRELATION COEFFICIENT (shade) for each pair of the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 297 441 67 [[Wordcloud|Wordcloud]]\ncircle 516 434 67 [[Big problems for later|Spider Plot, e.g. relative count data of different species (shape) for the variables TREES IN EUROPE (green), TREES IN ASIA (blue), TREES IN AMERICA (red)]]\ncircle 710 402 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x)]]\ncircle 1323 410 67 [[Regression Analysis#Simple linear regression in R|Linear Regression Plot, e.g. INCOME (y) per AGE (x)]]\ncircle 392 558 67 [[Chord Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites]]\ncircle 621 521 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of FAVORITE SNACKS (colors) with the connections connecting shared favorites (if connections are valued: 3 variables)]]\ncircle 853 496 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. different TREE SPECIES (x) and TREE HEIGHT (y)]]\ncircle 1014 521 67 [[Stacked Area Plot|Stacked Area Plot, e.g. INCOME (x) and count data (y) of BOUGHT ITEMS (colors) (if y is EXPENSES: three variables)]]\ncircle 1174 502 67 [[Kernel density plot|Kernel Density Plot, e.g. count data (y) of EXAM POINTS IN MATHS (blue) and EXAM POINTS in HISTORY (green) per point (x)]]\ncircle 1438 521 67 [[Big problems for later|Multiple Regression, e.g. INCOME (y) per AGE (x) in different COUNTRIES]]\ncircle 1657 554 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER, WEIGHT]]\ncircle 517 648 67 [[Sankey Diagrams|Sankey Diagram, e.g. count data of VOTER PREFERENCES (colors) with movements from Y1 to Y2 to Y3]]\ncircle 755 621 67 [[Stacked Barplots|Simple Stacked Barplot, e.g. absolute count data (y) of different species (colors) for the variables TREES in ASIA & TREES IN EUROPE (x), with numeric PHYLOGENETIC DIVERSITY (bar width)]]\ncircle 912 679 67 [[Barplots, Histograms and Boxplots#Boxplots|Boxplot, e.g. TREE SPECIES (x), TREE HEIGHT (y), COUNTRIES (colors)]]\ncircle 1095 693 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color)]]\ncircle 1267 645 67 [[Heatmap|Heatmap, e.g. TREE SPECIES (x) with FERTILIZER BRAND (y) and HEIGHT (colors)]]\ncircle 1509 696 67 [[Big problems for later|Network Plot, e.g. calculated connection strength (line width) between actors (nodes) based on LOCAL PROXIMITY, RATE OF INTERACTION, AGE, CASH FLOWS (nodes may be categorical)]]\ncircle 622 812 67 [[Clustering Methods|Cluster Analysis, e.g. car data points are grouped by their similarity according to the numeric variables KILOMETERS PER LITER, CYLINDERS, HORSEPOWER and categorical BRAND]]\ncircle 733 759 67 [[Bubble Plots|Bubble Plot, e.g. GDP (x), LIFE EXPECTANCY (y), COUNTRY (bubble color), POPULATION SIZE (bubble size)]]\ncircle 782 875 67 [[Big problems for later|Factor analysis]]\ncircle 1271 825 67 [[Big problems for later|Structural Equation Plot]]\ncircle 1394 771 67 [[Big problems for later|Ordination, e.g. numeric and categorical variables (AGE, INCOME, HEIGHT, PROFESSION) are transformed into Principal Components (x & y) along which data points are arranged and explained]]\n</imagemap>\n\n\n=== Other statistical figures ===\nFor further data types and visualisation exploration, we have found the website [https://www.data-to-viz.com/#connectedscatter \"From data to Viz\"] to be extremely helpful when choosing appropriate data visualisation. You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.\n\n'''Further visualisation examples on this Wiki:'''\n* [[Clustering Methods|Clustering methods]] \n* [[Big problems for later|Ordination and Network Analysis]]\n* [[Chord Diagram]]\n* [[Kernel density plot]]\n\n== Graphical etiquette \u2013 a rough guide how to make scientific quantitative figures ==\nThere is an almost uncountable amount of scientific figures out there. While diversity is great, it can often be overwhelming to consider which graphics to do with data, and which other types of figures can be beneficial. In addition, there are some general norms and conventions to consider when making graphics. Let us start with these. While these are mere suggestions, and reflect my own style and experience, they may serve as a reflection basis for others. \n\n'''1) Only show data in a figure that contains enough information to justify a figure'''\n<br>\nMany barplots I have seen contain 2 values. Could these not be fit into a sub-sentence? A graphic is to this end downright trivial, and wastes a lot of space. The same is also true for a piechart, but even worse, because it can look like a Pacman, and pie-charts are just generally misleading. Hence consider how many values you want to show, and if these really justify the journal space. \n\n\n'''2) One should not show plots that violate the statistics'''<br>\nA common example are barplots. There can be good reasons to show barplots with error bars, however the majority of data is better represented by a boxplot. Boxplots show more values, and are sensible for data that contains a range of integers. A typical borderline case is the [[Likert Scale]], which is often shown in boxplots. While this is not entirely wrong, it is a bit strange, as the scale contains five values, and a boxplot is constructed of five parts at least.  Another example is the standard fitted [https://www.statisticshowto.com/lowess-smoothing/ loess line] in a ggplot. If you show non-linear statistics, there should be a reason for it. Some of these trends show nothing at all, and follow no assumption beside adding some graphical interest. Scientific figures are however not only about aesthetics, but also about soundness and validity. \n\n\n'''3) Avoid empty spaces in plots. After all, your graphics are no Zen garden'''<br>\nSpeaking of aesthetics, your plots should be balanced, and should not contain open spaces. Try to set the x and y axes in a way that empty space is avoided. Such space could be ideal for a legend, or maybe some other additional information. Often people use weird short names in the legend, and then explain them in the figure caption. This should be avoided at all costs. Try to create legends that are self-explanatory. Make sure that the density of the information that is being shown is balanced, at least as long as the data and analysis allows for this. If figures contain much open space you can still definitely try to make them smaller, and maybe even combine several of these in a panel plot. In certain graphics such as clearcut correlations empty space is hard to avoid. This is a good indicator that maybe this figure is not needed at all, but instead can be replaced by some relevant values (e.g. p-value, R<sup>2</sup> ) in the text. \n\n\n'''4) Use diverse, but also non-confusing colours'''<br>\nI am colourblind, so I am in the privileged position to judge on the diversity of colours in a plot. Actually, no test ever picked up any colour blindness, but I definitely have some problem there. Hence I would say that colours should be diverse, and ideally brightness and hue allow also to differentiate groups. Figures become problematic if there are too many colours, and I define this threshold for me at around 8. In addition, consider using colours that have proven their value when combined. I can endorse weandersonpalette as a really cool package that gives you a pleasing combination of colours. \n\n\n'''5) Label orientation goes a long way'''<br>\nMost label orientations in most plots are wrong. Try to bring your labels -if space permit- into a horizontal orientation. quite often tables are too densely packed on the x-axis then, yet you could also consider making them in a 45 \u00b0 fashion. As much as a 90 \u00b0 label orientation can offer a little stretch for the neck, it is also a one-sided exercise. Hence consider flipping the labels whenever possible.\n\n\n'''6) Compose the figure right to its intended size'''<br>\nOne of the most common mistakes in the creation of figures is the proportion of different parts of the figure. Often the axes labels are really small, but the heading is massive. Ideally when designing a figure, one should consider to size with which this figure should be printed. If you know that a figure is only 6x6 centimetres, you need to make the labels sufficiently large and sparse to be readable, and orderly. A larger figure has different proportions. Hence design your figures in the right composition interns of the different text sizes.\n\n\n'''7) Use one font only'''<br>\nOk, font types are like borderline religion, but still I guess we can all agree that one should only use one font in a figure. If you want to make a typesetter cry you may use one font with serifs and another one without serifs, but otherwise do not do that. It can make sense to use a narrow font (iei. Arial narrow) if you do not have enough space in your figure. \n\n\n'''8) Occam's razor applies to scientific figures, too'''<br>\nA good figure is balanced, and ideally contains as much information as possible, but not more. You can however also decide to make figures that contain more information, and are borderline like mandalas. This is really cool if you have complex information to show for, and maybe the result basically says that it's complex. Other figures may be graspable in a split second, which is very cool if you want people to understand something quickly. In between is the Occam's razor sweet spot, where you need a few seconds with the figure, but then you kind of got it. To this send, let it be known that this sweet spot is different for different people. \n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "39hj4glihsbuqxqbc745hw867tbzr15"
                }
            },
            {
                "title": "Kanban",
                "ns": "0",
                "id": "382",
                "revision": {
                    "id": "3259",
                    "parentid": "2344",
                    "timestamp": "2020-11-04T10:22:22Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2348",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nWhen you're thinking about how to manage your tasks, you might consider a Kanban board. A Kanban board is a tool that provides you with an overview of all tasks you (and your team) are currently working on and highlights the progress of each task. It especially helps when you feel you're loosing track of how things are progressing and/or losing oversight.\n\n== Goals ==\n* Visualize work and keep track of tasks.\n* Manage tasks efficiently for yourself or in a team\n* Limit amount of work simultaneously done, thereby reducing complexity, increasing efficiency\n\n[[File:Kanban.jpg|350px|thumb|right|Organize your tasks in 'To Do', 'Doing' and 'Done' to keep track of them. Source: [https://www.proofhub.com/articles/kanban-apps proofhub]]]\n\n== Getting Started ==\nIn it's simplest form, a Kanban board is just a simple table with three columns (To Do, Doing, Done). You can easily draw it on e.g. a whiteboard and start adding post-it's. If you want to do it digitally, a tool such as Trello (see link below) might be your way to go.\n\nIf you want to get more sophisticated, you may add columns such as \"On Hold\" or \"For Review\" if need be.\n\nDo be careful when your projects are getting bigger. Managing a Kanban board also means constantly keeping it tidy if you want to keep a good overview.\n\n== Links & Further Reading ==\n* [https://en.wikipedia.org/wiki/Kanban_board Overview on Wikipedia]\n* [https://www.atlassian.com/agile/kanban/boards Exhaustive description]\n\n'''Software you may want to consider:'''\n\n* [https://trello.com/ Trello]\n* [https://asana.com/de Asana]\n* [https://monday.com/1/ Monday]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "eu2iqt6gqazbvbi77in5o3cxy1a5hsx"
                }
            },
            {
                "title": "Kernel density plot",
                "ns": "0",
                "id": "659",
                "revision": {
                    "id": "5597",
                    "parentid": "4824",
                    "timestamp": "2021-05-28T11:57:22Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Kernel density plots */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3398",
                        "#text": "'''Note:''' This entry revolves specifically around Kernel density plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n== Kernel density plots ==\nThis entry aims to introduce kernel density plot and its visualization using R\u2019s ggplot2 package. '''Density plot is used to plot the distribution of a single quantitative variable.''' It allows to see which score of a variable is more frequent and which score is relatively rare. The x-axis represents the values of the variable whereas the y-axis represents its density. The area under the curve equates to 1.\n\nPackages used : gapminder, ggplot2\n\n<syntaxhighlight lang=\"R\" line>\n# Install and load the gapminder and ggplot2 packages\ninstall.packages(\"gapminder\")\nlibrary(gapminder)\nlibrary(ggplot2)\n#A glimpse of the gapminder dataset\nhead(gapminder)\n</syntaxhighlight>\n[[File:File_2021-03-01_at_18.36.14.png|500px|frameless|center]]\n\n<syntaxhighlight lang=\"R\" line>\n#?gapminder\n#View(gapminder)\n#Using the basic plot function of R to view the distribution of GDP per capita\n\nplot(density(gapminder$gdpPercap))\n</syntaxhighlight>\n[[File:File_2021-03-01_at_18.42.26.png|500px|frameless|center]]\n\n'''Bandwidth''' determines the smoothing and detail of a variable. The bandwidth can be changed in the <syntaxhighlight lang=\"R\" inline>aes</syntaxhighlight> parameter of <syntaxhighlight lang=\"R\" inline>gemo_density()</syntaxhighlight> function. The default bandwidth can be viewed as:\n<syntaxhighlight lang=\"R\" line>\nbw.nrd0(gapminder$lifeExp)\n\n#Output: [1] 2.624907\n</syntaxhighlight>\n\nA basic density plot of life expectancy with ggplpot2() over the years can be viewed as:\n<syntaxhighlight lang=\"R\" line>\nggplot(gapminder, aes(x = lifeExp))+\n   geom_density(fill = \"red\", bw = 1)+\n   labs(title = \"Life expectancy over the years\")\n</syntaxhighlight>\n[[File:FIle_2021-03-01_at_18.49.00.png|500px|frameless|center]]\n\nRepresentation of life expectancy for every continent can be further seen with using the \"continent\" variable for the fill parameter.\n\n<syntaxhighlight lang=\"R\" line>\nggplot(gapminder, aes(x = lifeExp))+\n    geom_density(aes(fill = continent, color = continent), alpha = 0.5)+\n    scale_fill_discrete(name = \"Continent\")+\n    scale_color_discrete(name = \"Continent\")+\n    labs(title = \"Life expectancy over the years\")\n</syntaxhighlight>\n[[File:File_2021-03-01_at_18.52.47.png|500px|frameless|center]]\n\n===Faceting===\nWith facetting, the variable can be split into groups and viewed side-by-side for a better comparison. The code for viewing the plot below is the following:\n\n<syntaxhighlight lang=\"R\" line>\nggplot(gapminder, aes(x = lifeExp))+\n    geom_density(aes(fill = continent, color = continent),alpha = 0.5)+\n    scale_fill_discrete(name = \"Continent\")+\n    scale_color_discrete(name = \"Continent\")+\n    labs(title = \"Life Expectancy over the years\")+\n    facet_wrap(continent ~.)\n</syntaxhighlight>\n[[File:File_2021-03-01_at_18.57.00.png|500px|frameless|center]]\n\n'''Refernces:'''\n# Lecture slides.\n# [https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0 \"Histograms and Density Plots in Python\" by Will Koehrson] \n# Kabacoff, R. (2018). Data visualization with R. EEUU: Wesleyan University.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Archana Maurya."
                    },
                    "sha1": "7cfsmgv0bap563078pyyvwlv61fgwlv"
                }
            },
            {
                "title": "Learning for exams",
                "ns": "0",
                "id": "293",
                "revision": {
                    "id": "6716",
                    "parentid": "6715",
                    "timestamp": "2022-06-14T09:19:56Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "minor": null,
                    "comment": "/* Smartphone use */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14108",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== Why & When ==\nWell, when you have an exam coming up \ud83d\ude09 However, most of the ideas below are generally targeted at learning, be it in academic or non-academic settings.\n\n== Goals ==\n* Spend your learning time efficiently & effectively\n* Retain knowledge & understanding long-term\n* Enjoy learning\n\n== What to do ==\nLearning for exams is a vast field. The following can thereby only be a fraction of possible insights on the topic and is not meant to be completely exhaustive or 100% scientific. It is however largely backed by scientific evidence and the personal experience of the author. If you think anything is inaccurate or missing, do feel free to contact us.\n\nThe following is structured into separate paragraphs, starting with a more general note on learning approaches before diving deeper into techniques which you may find helpful. Furthermore, a section on what is usually '''not''' effective when learning and some general tips are included. The body of reference is meant to give you the opportunity to dive deeper into selected topics, yet if you follow what is collected below, you should be very much good to go. \n\n=== Planning & Consistency ===\nThe first thing to note is that exam success hardly depends on the few weeks before the exam itself, but rather on your whole semester. Starting to learn early and consistently will, in the long run, save you a lot of time and stress and will also improve your exam success.<br>\nPart of this is having a more or less clear overview on what is going to be covered or assessed and what it is you want to learn. This should also include a clear idea on what is not interesting or too important, either for yourself or the exam. Try to get an overview of topics and contents via the syllabus or ask the teacher / lecturer for it. <br>\nAnother part is always being more or less up to date with lectures or seminars. It doesn't take more than 30 minutes after a lecture for you to forget most of what you just heard. If at all possible, take 10 to 15 minutes after each session to work with your notes, reformulate them and throw them into your knowledge base (whatever that is). A good idea is also to directly transfer them to a flashcard tool such as Anki (see below).\n\n=== Techniques ===\nThere are several learning techniques that have been scientifically proven to yield better results than what is commonly used by many students and learners. Try to stick to these as you learn!<br>\n\n'''Spaced-Repetition'''<br>\nOne important concept is spaced repetition. This means that you should repeat content in increasingly longer intervals to ensure long-term retention. So, concretely, if you've just learned something, repeat it first on the next day, then after 3 days, after 7 days, after two weeks, and so on. Repetition intervals should depend on how well you were able to remember something on a given day (e.g. when you had lots of trouble after 7 days, maybe throw in another repetition 2 days later). The aforementioned overview of learning content can be combined with this, i.e. you can make a table with all the content for a given course and track when you repeated which topic to help you keep an overview.<br>\n\n'''Active Recall'''<br>\nActive Recall means recapitulating everything you know on a topic or question you are learning '''without checking your notes or sources'''. There is a significant learning edge to this over re-reading notes or trying to immediately fill in your gaps. Try to remember everything you know for as hard as possible until you are certain that you will not remember anything else. Only then should you check back with your notes to see if you have forgotten anything. This will then subsequently show you on which gaps you need to focus more.\n\nUseful tools can be using MindMaps or SpiderDiagrams, but also writing down bullet points, full texts or talking to a friend about it. If you happen to have an interim outage of friends, it is absolutely fine to talk to your chair or any other attentive listener that can't escape.<br>\n\n'''Elaboration'''<br>\nElaboration is the act of thoughtfully laying out a topic, talking about associations, explaining concepts, drawing connections to other knowledge and in general being very verbose about answering a question or talking about a topic. This can very much be combined with active recall (i.e. not using your notes for recapitulation). <br>\n\n'''Interleaving'''<br>\nInterleaving means to disperse the learning you're doing in a certain amount of time across several topics. It has been shown that this can lead to better learning outcomes than block or massed learning. As an example, you might be inclined to learn one whole week for one topic, another whole week for another topic and so on. Research indicates that interleaving those topics over two weeks can be more effective. Of course, you should not overdo it and switch topics every thirty minutes. Try to find some balance that works for you!<br>\n\n'''Breaks'''<br>\nProbably the most important and undervalued in learning (and work in general)! '''Take. Breaks.''' A lot. \n\nYour ability to understand and retain information will benefit from you being on top of your game, and you will likely be more on top of it if you take breaks. A good rule of thumb is to take a 5 minute break every 25 minutes when learning (as that has been shown to be the average time a student can fully concentrate). Of course, this will vary from person to person, but do remember that people are generally pretty bad at accurately assessing their concentration level and learning effectiveness at a given point in time. If in doubt, take the break.\n\nAlso, '''several shorter breaks are way more effective than fewer long breaks'''. Studying for two and a halve hours and taking a thirty minute break will typically be much worse than doing six 25/5 blocks - especially if you plan to continue after the initial three hours.\n\nIf you're interested, see e.g. [https://www.sciencedaily.com/releases/2011/02/110208131529.htm Ariga & LLeras (2011)]\n\n=== Supportive Tools ===\n'''[https://apps.ankiweb.net/ Anki]'''<br>\nA really good tool for active recall and spaced repetition! A small app available for Computers, Macs, iPhones and Android Smartphones, giving you all the things you would want from a flashcard app and having a really good spaced repetition algorithm. Alternatives can be [https://en.wikipedia.org/wiki/Leitner_system analogue flashcard boxes] or [https://quizlet.com/latest quizlet].\nThe huge benefit of Anki is that you'll have it with you all the time. Waiting for the bus? Answer a few cards. Bored before going to sleep? Answer a few cards!<br>\n\n'''(Libre) Office'''<br>\nGoing hand in hand with the planning section above, it can be very useful to pour your learning plan into a table or worksheet, e.g. in Excel or Word. Any other software such as Libre Office or Google Docs will do just as well!\n\nTry to write down the topics you have to learn in a [https://en.wikipedia.org/wiki/MECE_principle MECE way] (mutually exclusive, collectively exhaustive) so you have everything neatly in front of you. You can now, at every point in time, write down an assessment of your perceived learning strength per topic, keep an overview of when you learned which topic last or when you'll learn it next (see \"Spaced Repetition\" above) and so on. If you're lucky, your lecturer will provide you with something that you can use as a solid base for this - asking for it also doesn't hurt!\n\nAll of this can of course also be done on paper or in a notebook - it might just get messy if you're not very disciplined.\n\n=== Further remarks ===\n==== Notes on group work ====\nGroup work is great, but can also be largely ineffective. If you gather 5 people who know next to nothing about a topic, there is very little chance you'll create insight from nothing just by being together. Try to have a clear idea on what you want out of your group work and assign or rotate roles accordingly. One idea is to, each week, have one person in the group explain a recent topic to everyone else (see also \"Elaboration\" and \"Active Recall\"). You can also have sessions within which one person tries to actively recall and elaborate on a topic, and the other(s) ask questions regarding gaps they might be seeing or remark inaccuracies. This combines active recall, elaboration and having an incredible amount of fun together! \ud83e\udd73\n\n==== Restructuring your environment ====\nIt can be a good idea to make use of your environment. Build in cues into your rooms, have different stations for different topics, and so on. Maybe hang a mindmap of your learning topics next to your bathroom mirror so you can recap 1 or 2 topics while brushing your teeth. Throw up sticky notes with central questions around your house.\n\nAlso, it can be very useful to have a dedicated place for your learning activities. Don't learn in your bed, don't learn at your dining room table. If both happen to be the same thing, try to have something around that you can alter when you start learning and switch back as you stop learning. The easiest way to do this is getting a Learning Lamp that you only turn on while learning!\n\n==== Smartphone use ====\nSmartphones can be useful for learning (see Anki), but they are also distracting. It has been shown that only keeping a smartphone in the same room without direct eye contact messes with your concentration level (cf. [https://www.journals.uchicago.edu/doi/abs/10.1086/691462 Ward et al., 2017]). So, when learning, shut it off and get it as far away from you as possible!\n\n==== A note on \"learning types\" ==== \nThere's a persistent belief that people have different learning types, e.g. being a Visual, Aural, Read/Write or Kinesthetic (VARK) learner. As of today, we know of no research that supports this thesis (cf. [https://anatomypubs.onlinelibrary.wiley.com/doi/abs/10.1002/ase.1777 Husmann et al., 2018] / [https://pubmed.ncbi.nlm.nih.gov/27620075/ Knoll et al., 2017]). What is true however is that using multiple types of media and modes supports understanding and retention. So don't stick to only watching videos because \"you're a visual learner\", but do switch it up - it's also more fun!\n\n== What NOT to do ==\nThese are several techniques that are widely used by learners that are pretty ineffective. Typically, you'll want to avoid these.<br>\n\n'''Summarizing'''<br>\nThere's almost no learning effect when summarizing slides, syllabi or textbooks while having them openly available next to you. Of course, if you're doing it with Active Recall, you're good to go! It might nevertheless be helpful to reformulate concepts in your own words to improve understanding.<br>\n\n'''Underlining & Highlighting'''<br>\nTypical study technique - next to no effect. As long as you just use it to underline/highlight important passages without recapitulating them later by memory, you can stop doing this.<br>\n\n'''Re-Reading'''<br>\nRe-Reading is probably the opposite of active recall. Don't do it, it will only make you feel as if you get things better (because you get better at recognizing the content), yet will do next to no good in terms of application and recall in exams.<br>\n\n'''Cramming'''<br>\nThis goes hand in hand with taking breaks and interleaving. Don't spend massive amounts of time without taking breaks or looking into other topics every once in a while!<br>\n\n== Links & Further reading ==\n=== Videos ===\n* [https://www.youtube.com/watch?v=ukLnPbIffxE Ali Abdaal - How to study for exams]: There are numerous videos and guidelines on how to study effectively. Yet, Ali Abdaal has managed to narrow it down to the essentials that have scientific backing.\n* [https://www.youtube.com/watch?v=IlU-zDU6aQ0 Marty Lobdell - Study Less, Study Smart]: An entertaining yet insightful overview of pitfalls and useful techniques for studying effectively.\n=== Books ===\n* [https://www.hup.harvard.edu/catalog.php?isbn=9780674729018 Brown, P. C., Roediger, H. L., &amp; McDaniel, M. A. (2014). Make it stick: The science of successful learning. Cambridge, MA: The Belknap Press of Harvard University Press.]\n=== Tools ===\n* [https://apps.ankiweb.net/ Anki] (also see our [[Anki|Wiki article]])\n* [https://quizlet.com/latest Quizlet]\n=== Papers ===\n* [https://www.journals.uchicago.edu/doi/abs/10.1086/691462 Ward, Adrian F., Duke, Kristen, Gneezy, Ayelet, Bos, Maarten W., \"Brain Drain: The Mere Presence of One\u2019s Own Smartphone Reduces Available Cognitive Capacity,\" Journal of the Association for Consumer Research 2, no. 2 (April 2017): 140-154.]\n* [https://anatomypubs.onlinelibrary.wiley.com/doi/abs/10.1002/ase.1777 Husmann, P. R., &amp; O'loughlin, V. D. (2018). Another Nail in the Coffin for Learning Styles? Disparities among Undergraduate Anatomy Students\u2019 Study Strategies, Class Performance, and Reported VARK Learning Styles. Anatomical Sciences Education, 12(1), 6-19. doi:10.1002/ase.1777]\n* [https://pubmed.ncbi.nlm.nih.gov/27620075/  Knoll, A.R., Otani, H., Skeel, R.L., Van Horn, K.R. (2017). Learning style, judgements of learning, and learning of verbal and visual information. Br J Psychol. 2017;108(3):544-563. doi:10.1111/bjop.12214]\n* [https://www.sciencedaily.com/releases/2011/02/110208131529.htm Atsunori Ariga, Alejandro Lleras. Brief and rare mental 'breaks' keep you focused: Deactivation and reactivation of task goals preempt vigilance decrements. Cognition, 2011; DOI: 10.1016/j.cognition.2010.12.007]\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "foakvhyjmb5p5iaqjd5yg3hjfuyoh98"
                }
            },
            {
                "title": "Legal Research",
                "ns": "0",
                "id": "357",
                "revision": {
                    "id": "6212",
                    "parentid": "5830",
                    "timestamp": "2021-08-10T07:15:07Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "24687",
                        "#text": "[[File:ConceptLegalResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Legal Research]]]]\n\n<br>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br>\n__NOTOC__\n<br>\n'''Annotation''': Due to the special role of Law as a scientific discipline, this entry will focus more on legal research as a whole as opposed to one specific method.\n\n'''In short:''' Legal Research subsumes different approaches to understand and interpret legal documents and/or cases.\n\n== Background ==\nThe field of Law has a long-standing history. \"Roman legal doctrine developed since the second century before Christ, and reached a very high level as from the third century after Christ. Its rediscovery and renewed study in Bologna in the eleventh century was the start for the creation of universities.\" (van Hoecke 2011, p.1). The emergence of Law and its eventual [[Glossary|institutionalisation]] as a scientific discipline was fundamentally based on the reading and interpretation of (Roman) legal documents (2). \"During the whole of the Middle-Ages, legal doctrine was highly thought of and considered as a 'scientific discipline', as in those times 'authoritative interpretation', not 'empirical research', was the main criterion for the scientific status of a discipline.\" (van Hoecke 2011, p.1). '''The discipline subsequently spread from the first universities in Italy throughout Europe''' (2). In this time, legal work was mostly restricted to each national context due to the differences in the respective legal systems (2). This early approach to Law as a scientific discipline represents the so-called 'doctrinal approach' which is still the most common understanding of legal work (see What the method does).\n\nHowever, a second approach developed from the 17th Century on, but mainly from the 19th Century until today. With [[History of Methods|the rise of positivism]] - which implies that the only valid knowledge arises from scientific inquiry and falsification of scientific theory -, a new understanding of science emerged. Here, the use of empirical data, the testing of hypotheses and the development of theories, independent from geographical limitations, became the new scientific ideal. In consequence, Law as a scientific discipline changed (van Hoecke 2011, p.1). '''The research focus in Law partly shifted, both in terms of research questions and methodology''': \"[i]n the 1960s and 1970s, legal realists and socio-legal scholars started the law and society movement, and pointed to the importance of understanding the gap between 'law in books' and 'law in action', and the operation of law in society. They were interested in examining the legal system in terms of whether legal reform brings about beneficial social effects and protects the interests of the public. Similarly, in the 1980s critical legal studies integrated ideas and methods found in disciplines such as sociology, anthropology and literary theory.\" (McConville & Chui 2007, p.5). With Law being a discipline between the Sciences and the Humanities, this new movement borrowed heavily from the Social Sciences in terms of methodology. New adjacent disciplines emerged, including legal sociology, legal psychology and law and economics, among others (2, 5). This 'Law in context' - also called 'socio-legal' - approach includes new topics and methodological directions with a stronger focus on empirical research and theory building and has been universally taken up in academic institutions.\n\nFrom this contextualized approach to Law, research emerged on the relationship between Law and Society and questions of power relations, more specificially on aspects such as gender, social class, ethnicity and religion (5). ''''Socio-legal research' may be seen as an alternative or a supplement, but not necessarily as a substitute to traditional legal analysis''' (2, 5). The openness towards a contextualization of Law acknowledges the fact that Law is influenced by thoughts and discourses from other disciplines such as sociology, economics, political science, history and psychology (4, 7). In addition, \"understanding of the modern patent law, environmental law, and information technology law presupposes adequate study of biotechnology, ecology, and information technology.\" (4, p.18). New approaches to and combinations of methods have emerged and continue to develop. The common 'doctrinal' approach to legal research, i.e. collecting, reading and interpreting (legal) texts, is increasingly scrutinized as the primary methodological approach (2, 3). According to critics, legal publications and teaching in academic institutions do not sufficiently reflect upon the methodological approaches of traditional legal research (2, 5). This critique stems from a comparison to other established disciplines in the (Social) Sciences which have been using a diverse set of defined methodological approaches for decades (2). It has also emerged in view of fundamental changes in academia in the last decades, notably in terms of methodological advancements and trends of internationalisation (2).\n\n\n== What the method does ==\nGenerally, legal research aims at understanding the law and the legal order. The methodological approach can be seen as concentring circles: the smallest circle is about understanding a particular provision, then this provision within the respective legal order (i.e. national legal order, European legal order), then within the specific societal context, then within the world. As explained before, there are different approaches to this: 'Doctrinal' research views Law on a theoretical basis and as a self-sustaining set of principles that are accessible through (legal) texts. This represents the approach by Kelsen (6), according to whom legal research is not about the process behind or content of Law, but rather about its structure, independent from any context. 'Socio-legal' research, by comparison, predominantly revolves around Law as a social phenomenon with its social, economic and political implications which the researcher often attempts to analyze using empirical methods in an interdisciplinary manner (4; 5, see Background). While the majority of academic publications and practical work in Law falls under the first category, the second type has significantly gained traction in academic discourse during the last decades (5c, see Background). Both approaches may be pursued for different purposes: to understand and describe the current system, to explain causes and effects or explore new ideas, to aim for reforms and improvements, to find practical solutions to existent problems and to provide foresight into future scenarios (4).\n\n\n==== Classical doctrinal legal research ==== \nTraditional legal research is often embedded in a specific case study with a certain problem concerning legal norm, policy, institutions or systems (4). The research aims to bring out the problem's background and functioning, assess the underlying principles' outcome and efficacy and suggest reforms or solutions to legal decision-makers answer the problem (4, 7). The results of the analysis may offer solutions to individual legal conflicts or serve as reference points for future decisions and further the development of legislation (2). This type of 'research' must not be seen in the purely academic sense of 'research', but also represents the classical process in practical Law (e.g. for judges). In this regard, Rubin (1997) distinguishes academic and non-academic legal work in that the former often ends with prescriptions for policy-makers, while the latter focuses predominantly on solving legal conflicts (7).\n\nThe research process entails \n# locating the source of law in primary (legal and sub-legal) as well as secondary texts (e.g. journal articles or commentaries on legislation) and collecting these documents,\n# systematically interpreting, analyzing and clarifying the texts (also by contrasting texts and considering comparable cases) and \n# finding a conclusive synthesis and infering general principles to answer a legal question or problem (1, 2, 4). \n\nIndependent from the sphere of application, the research process is predominantly text-based, focuses on theoretical discussions of the legal text itself, and takes the societal context of the legal problem only into consideration for the purpose of understanding the object and purpose of the Law (5). Overall, this classical method of legal analysis is comparable to [[Hermeneutics]]: Legal scholars read text in detail and argue about diverging interpretations (3). The basic procedure to this end is based on syllogisms: ''If A is B, and A is C, then B must be C.'' This is based on Sokrates, and when applied to law (normative syllogism after David Hume), it means that:\n* the Law prescribes \"If element(s) of the norm ''A'' happen(s), legal consequence(s) ''B'' must follow\",\n* legal scholars analyze whether their real-life situation ''C'' is what the element of the norm A entails, \n* and if it is, the legal consequence(s) ''B'' must follow.\n\n'''Let us take the example of \u00a717 Animal welfare act:''' \"Anyone killing a vertebrate without a reasonable ground shall be punished with up to three years imprisonment or a fine\".\n* The ''A'' - the elements of the norm - would be \"anyone killing a vertebrate without a reasonable ground\". Actually, there are four elements here: The \"anyone\", the \"killing'', the \"vertebrate\", and the \"without a reasonable ground\". All of these elements will need to be considered when assessing whether the situation ''C'' fulfills ''A''. \n* If it does, ''B'' - the legal consequence - would be \"shall be punished with up to three years imprisonment or a fine\". \n* Now, we look at our case ''C'' - let's say, a man kills a cow with a knife. It is rather straightforward to see that the \"anyone\" and the \"vertebrate\" are fulfilled. The \"killing\" might be a matter of debate, but if it is clear that the man actively caused the cow's death by using the knife, this is also quite clear. But the \"without a reasonable ground\" is a matter of interpretation. The legal scholar needs to give meaning to this element so that they are able to decide whether it is fulfilled.\n\nIn order to give meaning to \"reasonable ground\", legal research typically uses four different methods of interpretation:\n\n* the actual meaning of individual legal terms (grammatical interpretation);\n* the historical background of the text (historical interpretation)\n* the systemic position of the legal provision, for example when several adjacent laws have to be seen in connection to make sense of each (systematic interpretation);\n* and the object and purpose of the text (object and purpose interpretation)\n\nFurther, the interpretation of the documents is supported ''argumentatively''; the analysed laws are ''explained'' by the existence of a higher norm or general principle based on real-world contexts; the texts can be analyzed ''axiomatically'' and ''logically'' and there are ''normative'' elements in understanding the underlying norms of the law as well as normative positions to take during their interpretation (3).\n\nSo let us apply these four main steps to our example of \"reasonable ground\":\n* a grammatical interpretation would translate it as a sensible or acceptable reason for acting,\n* a systematic interpretation would show that other adjacent laws further define what is  unreasonable behavior towards animals, which makes it clearer what is \"reasonable\",\n* a historical interpretation would highlight that there has been increasingly more attention towards the rights and protection of living creatures, and therefore increasingly narrow margins for \"reasonable killing\", and\n* the object and purpose of the law state that there are circumstances under which humans need to kill vertebrates (e.g. for food production), and others where they don't need to.\n\nAs we can see, these four steps do not point to one clear definition of what is a \"reasonable ground\". So the situation can be analyzed in light of a higher law. This happened in Germany when it was discussed whether male chicks should be allowed to be killed during the production of chickens. The German Federal Constitutional Court stated that the economic purpose of killing male chicks does not (anymore) outweigh the responsibility towards animal life, so it was banned.\n\n'''As this description of the classical legal research process shows, there is indeed a distinct methodology to classical Law''' (4). It is based on the adoption of methods from the Humanities (historical, analytical and philosophical methods) and generally from the Social Sciences - as Dobinson and Johns (5b) highlight: the identification and collection of relevant documents in law can be seen as \"analogous to a social science literature review\" (p.22). Traditional legal research also entails theory- and hypothesis-building. As Van Hoecke (3) states: \"Legal scholars collect empirical data (statutes, cases etc.), word hypotheses on their meaning and scope, which they test, using the classic canons of interpretation. In the next stage, they build theories (...) which they test and from which they derive new hypotheses.\" (p.11). Also, \"[t]he level of systematisation and concept building is the level of theory building in legal doctrine.\" (p.17). However, the prescriptive element of legal scholarship - the suggestion of iterations and guidance for policy-makers - differentiates it from the aforementioned disciplines which mostly engage in descriptive accounts of the world (7).\n\n'''An interesting notion is the question of how 'empirical' legal research is understood.''' While several scholars imply the usage of 'empirical' data, they most commonly refer to the legal and non-legal documents that are gathered, compared and analysed for a specific case (see e.g. (3)). Methods of social science field research, such as [[Open Interview|interviews]], are acknowledged, but are admitted to play a minor role in the gathering of data (3). Because of this, Dobinson and Johns (2007) highlight that 'doctrinal' legal research is neither quantitative or qualitative, since it does not apply classical empirical methods. However, they declare that \"(...) such research is a process of selecting and weighing materials taking into account hierarchy and authority as well as understanding social context and interpretation\" (Dobinson & Johns 2007, p.40). Because of this, they still frame doctrinal legal research as a qualitative method. Thus, from a methodological perspective, it may be categorized as a form of data gathering as well as analysis, as qualitative and as both inductive (when focusing on wording as well as object and purpose of the text) and deductive (when analysing systematics and historical background) with regards to the process of document analysis.\n\nThe 'doctrinal' approach to legal research has been critized (see Background). As Van Hoecke (3) puts it: \"(...) it is often too descriptive, too autopoietic, without taking the context of the law sufficiently into account; it lacks a clear methodology and the methods of legal doctrine seem to be identical to those of legal practice; it is too parochial, limited to very small scientific communities, because of specialisation and geographical limits; there is not much difference between publications of legal practitioners and of legal scholars.\" (p.2). The 'socio-legal' approach may be seen as an answer to this critique.\n\n\n\n==== Socio-legal research ====\n'''Socio-legal research attempts to broaden the picture of the legal situation and takes into consideration factors and data other than legal and sub-legal documents exclusively'''. The sociological, historical, economic, geographical and/or cultural context of legislation is acknowledged and data is gathered empirically on these elements in order to better understand the underlying issues. Qualitative and quantitative methods of empirical data collection include [[Ethnography|observation]], questionnaires, [[Content Analysis|content analysis]], interviews, [[Survey|surveys]], [[Experiments and hypothesis|experiments]] and others (5b, 5c). Qualitative methods can help contextualize the subsequent quantitative analysis of the gathered data. The final selection of methods is dependent on the research questions, aims and contexts (2, 4). Like the 'doctrinal' approach, the research may still happen under the umbrella of a case study, where all methods of inquiry and analysis focus on one particular situation (1). Socio-legal research may serve the same goals as classical legal research (improving legislation, suggesting solutions), but is generally more interested in understanding the impact of the legal system on society. \n\n\n== Strengths & Challenges ==\n* Especially in 'doctrinal' legal research, there is a fine line between practical and academic work on legal issues which are similar in many aspects. However, scientific work in Law is generally more complex (3, 4). Academics want to understand through a comprehensive analysis and conclusion why the law exists and how it works in different (national) contexts in order to provide prescriptive guidance (7), often for the purpose of publication. By comparison, practitioners focus on what the Law is in the given context for the purpose of finding immediate solutions for their clients (4). The term 'legal research' applies to both groups, but the differences are crucial for the methodological discussion.\n* The international differences in legal systems make it difficult to develop an internationally accepted consensus on theories, terms and approaches (3). At the same time, international jurisdictional instruments such as Conventions more strongly influence national systems. Both insights may highlight the need of a more homogenous, international approach to Law (5).\n* McConville & Chui (2007) declared a trend in legal training towards empirical research in legal work: \"Law students are now more research-based than ever before, and research is an integral part of the undergraduate curriculum, no longer the preserve of postgraduate students. This means, at the least, that legal research and scholarship is much more pervasive, complex and demanding than ever before and those engaging in research have more possible pathways to travel and require a greater range of skills and competences than their law-focused predecessors.\" (McConville & Chui 2007, p.2). This demand meets with the aforementioned lack of reflexive debate about legal methodology, which raises questions for the future of teaching in Law.\n\n\n== Normativity ==\n==== Connectedness ====\n* As described above, the basic interpretation of legal documents may be expanded by further empirical methods to gather additional contextual data, especially in 'socio-legal' research.\n\n==== Everything normative to this method ====\n* Maybe the overall challenge with Law as a \"created\" object by any given society at the time, which is in and of itself liquid, changing and changeable, is that a scientific approach that could be inferred from the object itself is lacking. '''Law is a social construct and serves society:''' it is \"(...) a product of conscious decision by decision-makers (...) in bold terms, law is created not discovered.\" (Rubin 1997, p.525). Law and traditional legal methodology are inherently prescriptive instead of descriptive, which makes them strongly normative (7).\n* It has thus been claimed that the acknowledgement of socio-ecological, socio-technical, cultural and political developments should be furthered in the field of Law (see (3), Background). Socio-legal researchers argue that \"[a] precondition for legal research in any form has become that the researcher should not only have knowledge about the traditional elements of the law, but also about the quickly changing societal, political, economic and technological contexts and, possibly, other aspects of relevance.\" (Langbroek et al. 2017, p.1). This claim supports the socio-legal research approach, but raises new issues since it highlights the blurred lines between Law as a scientific discipline and Law as a political object that is influenced by the societal context (see (6)).\n* The question of objectivity is raised especially when considering the different purposes of classical legal research. When the goal of research is to find solutions for a client, the analysis process is influenced by the client's ambition. As Dobinson and Johns (5b) highlight: \"Epstein and King contrast the approach of a lawyer and a science PhD where the lawyer is encouraged to research from the perspective of the client whereas the science PhD has to acknowledge contrary positions. 'An attorney who treats a client like a hypothesis would be disbarred; a PhD who advocates a hypothesis like a client would be ignored'.\" (Dobinson & Johns 2007, p.22)\n* The predominant lack of empirical methods, especially in traditional legal research, imposes a challenge in terms of quality criteria. Objectivity is difficult to achieve due to the interpretative nature of the analytical process of legal documents as well as a lack of clear definitions of core terms (2, 3). Dobinson & Johns (2007) claim there there is no 'objective truth' for Law outside of the legal documents, which is why - as with qualitative analysis in general - the validity and objectivity of the analysis depend on the rigour of the applied analysis methods (5b). This uncertainty in terms of quality criteria also complicates the review process for publications (2).\n\n== Outlook ==\n* According to Langbroek et al (2017), there are two lines of development for the methodological debate in Law: a) exchange between scholars in Law and social sciences on normative and methodological approaches and b) an ongoing debate between traditional academic lawyers and legal researchers on the methods used for legal analysis and research. \"(...) [W]hat to date has been seen as 'traditional legal methodology' will show itself as not having been self-evident at all.\" (p.3)\n\n== Key Publications ==\n* Zippelius, R. ''Juristische Methodenlehre'' (11. Auflage, Beck 2012)\n* Rubin, E.L. 1997. ''Law and and the Methodology of Law''. Wisconsin Law Review 3. 521-566. HeinOnline.\n\n== References ==\n(1) Webley, L. 2016. ''Stumbling Blocks in Empirical Legal Research. Case Study Research.'' Law and Method xx(x). 1-21. Available at [http://www.lawandmethod.nl/tijdschrift/lawandmethod/2016/10/lawandmethod-D-15-00007](http://www.lawandmethod.nl/tijdschrift/lawandmethod/2016/10/lawandmethod-D-15-00007) \n\n(2) Langbroek, P. van den Box, K. Thomas, M.S. Milo, M. van Rossum, W. 2017. ''Methodology of Legal Research: Challenges and Opportunities.'' Utrecht Law Review 13(3).\n\n(3) Van Hoecke, M. ''Legal Doctrine: Which Method(s) for What Kind of Discipline?'' In: Van Hoecke, M. (ed) 2011. ''Methodologies of Legal Research: Which Kind of Method for What Kind of Discipline?'' Bloomsbury Publishing. 1-18.\n\n(4) Bhat, P.I. 2019. ''Idea and Methods of Legal Research. Part I: General.'' Oxford University Press.\n\n(5) McConville, M. Chui, W.H. ''Introduction and Overview.'' In: McConville, M. Chui, W.H. (eds) 2007. ''Research Methods For Law''. Edinburgh University Press. 1-15.\n\n(5b) Dobinson, I. Johns, F. ''Qualitative Legal Research.'' In: McConville, M. Chui, W.H. (eds) 2007. ''Research Methods For Law''. Edinburgh University Press. 16-47.\n\n(5c) Chui, W.H. ''Quantitative Legal Research''. In: McConville, M. Chui, W.H. (eds) 2007. ''Research Methods For Law''. Edinburgh University Press. 48-71.\n\n(6) Kley, A. Tophinke, E. 2001. ''Hans Kelsen und die Reine Rechtslehre.'' Juristische Arbeitsbl\u00e4tter Heft 2. 169-174.\n\n(7) Rubin, E.L. 1997. ''Law and and the Methodology of Law''. Wisconsin Law Review 3. 521-566. HeinOnline.\n\n== Further Information ==\n\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "3yvjqtgbj0hyqb264n5eqr2v90r3fzo"
                }
            },
            {
                "title": "Lego Serious Play",
                "ns": "0",
                "id": "301",
                "revision": {
                    "id": "5932",
                    "parentid": "5794",
                    "timestamp": "2021-06-30T17:43:37Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Example 1: Lego to build a better business */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8408",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When == \nLego can be a tool in a [[Design Thinking]] approach to prototype research ideas, haptically test setups and explore team settings and interactions. The strength of Lego Serious Play lies in group settings, where Lego can serve as a [https://www.sciencedirect.com/science/article/abs/pii/S0921800914001219?via%3Dihub boundary object] to allow team members to interact, allowing for a reflexive interaction. '''Lego is playful, many remember it from childhood, and it can trigger people to challenge their underlying assumptions.''' Through the haptic interaction people are enabled to interact more than if they would be just talking. Originating in management, Lego Serious Play can be used in any sort of research setting, and is suitable for disciplinary design settings yet particularly helpful in transdisciplinary research modes. The structure of Lego can help structure the research. Lego can be thus be used as a catalyst for a team to develop research together.\n\n== Goals ==\nLego Serious Play can be seen more as a way and not a goal. The goal can be bluntly put to <br>\n1) facilitate people to brainstorm in a smaller (3-15) group setting about research<br>\n2) integrate knowledge in a room in a non-hierarchical and enabling space<br>\n3) create research designs, workshop settings or the architecture of larger research projects.<br>\n\nThe goal to this end is not so much the final Lego model, but more the way that leads to it.\n\n== Getting started ==\nFirst of all, you need some Lego. Personally, I think the Lego Serious Play sets are not really necessary. Instead, any collection of Lego will do, as long as you do not have a Lego expert who will be missing this specific piece from his childhood. Find a large table or a clean ground space (blanket?) where everybody can sit comfortably for a long time. '''Put the Lego in the center, and make it clear, that everybody can and should be equally engaged.''' Ideally, there is a moderator who only intervenes in times of crisis or may engage people by asking specific questions. The process is still often led by hierarchies, yet this should ideally be avoided. \nHere I present three approach show I used Lego in the past in research settings. \n\n==== Example 1: Lego to build a better business ====\nThe first example shows how Lego can be used in a Team setting, for instance in management. The original Lego Serious Play approach focuses on business settings, where hierarchical management structures, habitual thinking and rigid systems often prevent change. Lego Serious Play has proven to be valuable to transform such [[Glossary|systems]] in a playful way. It serves both as a prototyping tool as well as a reflexive setting, allowing for system change and the [[Glossary|empowerment]] of the individual.\nA good starting point follows three steps (in accord with Kristiansen and Rasmussen 2014):\n1) Get the knowledge from the room. Everybody should contribute their perspective, along other to understand their knowledge and perspective. \n2) Construct a system. Try to bring the different parts of the knowledge together, so that people will jointly construct a model of the system, showing what is relevant and where connections and interactions are. \n3) Link the system and the individual. By trying to create a connection between the individual perspective and the system a new team hierarchy as well a a reflection of the role of individuals in the team or system can be created.\nThis approach was the original proposal of Lego Serious Play, and aims to build better business. More concretely, it is about creating more reflexive and interactive settings in the world of business.\n\n==== Example 2: Lego to prototype projects ====\n[[File:Lego Serious Play.jpg|300px|thumb|right|'''Lego can bring people together in a non-hierarchical manner.''' Source: [https://www.youtube.com/watch?v=2TbTa31ACB4 seriousplayeducation, YouTube]]]\nThe second example builds on the creation of a research design for a larger project.\nLarge research projects often contain several PhD students, which within most empirical research - as an example - write at least three scientific papers. Hence a project with three PhD students creates in a utilitarian calculation knowledge in the form of nine papers. Of course, other forms of knowledge will also be created, but this is at Leuphana in the Faculty of Sustainability what students have to deliver to be able to hand in their dissertation. \n\nI typically build two models in such a setting. One model for the PhDs, where each color represents one PhD student. There, links can be created between different papers of the PhDs, visualising where interaction are, and how the larger project may make sense. I would consider a project where you have three stacks of bricks that are completely unconnected a failure. There should be a justification and logic on why these PhDs are together in a larger project. \n\nThe second model I often create differentiates between data, analysis and interpretation, showing a methodological differentiation. This is helpful if certain data feedbacks into later papers, or if two papers use the same analysis, but with different data. This can help to identify synergies in the project. Creating such models of projects can especially be also good at the beginning of a project to create a joint recognition of the different milestones - or Lego bricks - of the project. \n\n==== Example 3: Lego to create a research design ====\nThe third example is for planning one specific sampling within a research project. [[Stakeholder Mapping]] can be aided by Lego figures, and out of such a stakeholder system, several bricks can be located to show how many interviews are conducted, and where. These bricks can then be combined, and the interview campaign can be logistically planned. This can also be amended by a prototyping of the analysis, checking whether the sample number is large enough and if the knowledge saturates are a specific point. The Lego bricks can thus also represent units of knowledge, that can be integrated into a three dimensional plots during the interview campaign to visualise not only the process, but also the progress and the information that was already gathered. \n\n==== Lego as a facilitation tool ====\nLego can be a serious boundary object within research settings, and can help to reflect within a team, and integrate the knowledge and perspectives in the team. It is important that the team agree that they want to try this out, thus that people do not feel pressured into this format. In addition, it has to be acknowledged that this approach creates a [[Bias and Critical Thinking|bias]] towards people that are familiar with Lego, which is why an inclusive and encouraging [[Glossary|facilitation]] is relevant. The possibilities of the Lego Serious Play approach are endless, and it is up to the team members how Lego is being utilised. Try it out!\n\n== Links & Further reading ==\n* Kristiansen, P., & Rasmussen, R. (2014). ''Building a better business using the Lego serious play method.'' John Wiley & Sons.\n* McCusker, S. (2020). ''Everybody\u2019s monkey is important: LEGO\u00ae Serious Play\u00ae as a methodology for enabling equality of voice within diverse groups.'' International Journal of Research & Method in Education, 43(2), 146-162.\n* A [https://www.youtube.com/watch?v=rld-gUrp-iw video] on Lego Serious Play by Liquid Agency \n* [https://www.youtube.com/watch?v=2TbTa31ACB4 Lego Serious Play for Education] - in three minutes\n* [https://www.strategicplay.de/lego-serious-play/?gclid=EAIaIQobChMIlfWvxsTl6gIVyu7tCh09cgPSEAAYASAAEgJIsfD_BwE Online Training] to become a Lego Serious Play Facilitator\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "mc8nzjah5hhxfsgs05g8slk5kua8iep"
                }
            },
            {
                "title": "Levels of Theory",
                "ns": "0",
                "id": "429",
                "revision": {
                    "id": "7240",
                    "parentid": "7239",
                    "timestamp": "2023-06-28T17:01:47Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7709",
                        "#text": "'''In short:''' This entry structures 'Theory' and how its levels relate to each other.\n\n'''[[Glossary|Theory]] is at the heart of science.''' Scientists come up with theories, test them, confirm them, falsify them, derive them from observation - theories are one of the staples of science. Hence, many scientists love discussing theories. Within the following lines, I want to show you a conceptualisation of different levels of theory, building on examples and common definitions of the respective terms.\n\nWhen reading a book about inequalities, I was at some point getting the feeling that this different forms of inequalities were like an enumeration to me. While each and every one of these inequality had a clear footing in reality, they felt more like a composite of examples of inequality. What was lacking to me was a more general level of thinking about inequality. Hence I derived a system of levels within theory, which I will present here. As the quoted definitions highlight, there is often not one single understanding for these terms despite their utter importance in scientific discourse.\n__NOTOC__\n==== ''Theory'' ====\n'Theory' is not so much the first level as it is the general idea of the following terms. Theory is defined as a \"systematic ideational structure of broad scope, conceived by the human imagination, that encompasses a family of empirical (experiential) laws regarding regularities existing in objects and events, both observed and posited. A scientific theory is a structure suggested by these laws and is devised to explain them in a scientifically rational manner.\" (Britannica). Wikipedia understands it as \"an explanation of an aspect of the natural world that can be repeatedly tested and verified in accordance with the scientific method, using accepted protocols of observation, measurement, and evaluation of results.\" Theories can be tested either through experiments or through principles of abductive reasoning. An example of a theory would be the theory of evolution, which has been tested empirically and abductively so often that we came to call it accepted knowledge on how this part of the world works. In theory, however, this theory could be rejected, and a recent epigenetic renaissance in Lamarkism proves that such theories are continuously explored. \n\n==== 1st Level: ''Concept'' ====\nMost commonly, 'Concepts' are understood as mental representations of the world. Wikipedia defines them as \"abstract ideas or general notions that occur in the mind, in speech, or in thought. They are understood to be the fundamental building blocks of thoughts and beliefs\". According to Merriam-Webster, a concept is \"an abstract or generic idea generalized from particular instances\". Concepts are the highest unit of theoretical thought. Our mind develops concepts to make sense of more specific elements of the world, such as the concept of 'nations' that helps subsume different groups of people and enables us to speak about these on a more generalized level. Other examples are Reason, Logic or Justice, which are abstract ideas of how a variety of elements from the world may be aligned, and related to each other. Philosophy is most relevant for this level of theory, and it is most relevant to generalisable knowledge.\n\n==== 2nd Level: ''Paradigm'' ====\nIn scientific discourse, the term 'Paradigm' was coined by Kuhn in the 1960s. Kuhn described scientific paradigms as \"universally recognized scientific achievements that, for a time, provide model problems and solutions for a community of practitioners\". A paradigm includes the concepts and theories, laws and thought patterns, generalizations and assumptions, and basically guidelines and standards on how to conduct research in the respective scientific school or discipline (Merriam-Webster, Wikipedia). Kuhn called the successive transition from one paradigm to the next the maturation process of science. You may have heard the term 'Paradigmenwechsel' in German public discourse - it means that everthing that was assumed about a field of knowledge is questioned and re-developed. The [[Glossary|emergence]] of sustainability itself may be seen as such a paradigm shift because it changed the way we think about scientific and societal sub-systems. In this regard, paradigms are composites or connections of several concepts: sustainability connects topics such as justice and equality, livelihoods, resources and other things, and by doing so influences the way we think about, work with and research these topics. Gender equality is another example, which consists of two combined concepts. Paradigms make concepts graspable. Not everybody is a philosopher, but paradigms can represent and encompass the large narratives that build and drive societies and cultures. Paradigms are thus more tangible, yet with enough blurriness to keep you thinking.\n\n==== 3rd Level: ''Framework'' ====\nA 'Framework' is a real or conceptual basic structure that supports or guides practical applications (Merriam-Webster, whatis.com). Frameworks pave the road to connect paradigms to the real world. The [https://www.un.org/sustainabledevelopment/sustainable-development-goals/ 17 Sustainable Development Goals] are an example, or the [http://www.dpi.inpe.br/gilberto/cursos/cst-317/papers/ostrom_sustainability.pdf Ostrom framework]. Frameworks imply, explore or test causality between their single elements or have the explicit focus to make paradigms applicable in planning and management. Hence frameworks link science to the real world.\n\n==== The bottom levels ====\nThe '''4th level''' are Cases. Cases are simply the bread and butter of empirical work, allowing us to apply frameworks. On the '''5th level''' you have actors. Actors are diverse, may be together part of institutions, and thus typically have identities, knowledge - ideally - and -finally - they act. No actor is an island. Actors are connected and interacting through actor-networks. Lastly, actors are non-static, they are changeable in space and time.\n\n\n[[File:Levels of Theory.jpg|800px|thumb|left|Source: own]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow many discussions you see and hear start on one level. You have the occasional discussion about sustainability. Somebody says: \u201eto me it is about justice\u201c, so you find yourself on the concept level. Then suddenly somebody says \u201ebut what about the vegan Amish\u201c, and you go down quite some levels to the cases. Within the debate, you make a jump from a very high to a very low level. This often creates confection and unconnectedness in the debates. All world religions revolve around paradigms. Take the Holy Trinity or the Four Noble Truths, which are paradigms, but then find their way into frameworks, for instance through the Ten Commandments or the Noble Eightfold Path. These were ideas about early frameworks, but one could have a hard time to take such frameworks all to literal, since they are more guidelines than actual rules, just as in [https://www.youtube.com/watch?v=6GMkuPiIZ2k Pirates of the Caribbean]. \n\n'''Within science, it is quite good to understand where you have the strongest footing.''' Few today are philosophers, many work with the conceptual linkages of paradigms, and even more utilise the applied power of frameworks. It is good to locate not only yourself, but also the level on which a discussion is running, or jumping. Paradigms and Frameworks often get taxed by a lack of clarity on the conceptual level. Only by verbalising these problems, we shall be able to move our theoretical thinking forward to this end.\n\n----\n[[Category: Normativity of Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden and Christopher Franz."
                    },
                    "sha1": "jgcdqaop3qshigrnkq63pxd9rjxyctl"
                }
            },
            {
                "title": "Life Cycle Analysis",
                "ns": "0",
                "id": "286",
                "revision": {
                    "id": "5904",
                    "parentid": "5150",
                    "timestamp": "2021-06-28T08:38:30Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "13859",
                        "#text": "[[File:ConceptLifeCycleAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Life_Cycle_Analysis]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}'''\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' A Life Cycle Analysis attempts to analyze the (socio-)environmental impact of a product through its lifespan.\n\n==Background==\n[[File:LCA.png|400px|thumb|right|'''SCOPUS hits per year for Life Cycle Analysis until 2019.''' Search terms: 'life cycle analysis', 'life cycle assessment' in Title, Abstract, Keywords. Source: own.]]\n\nLife Cycle Analysis, or Life Cycle Assessment (LCA), dates back to the late 1960s/early 1970s when the first (partial) LCAs were conducted: \"The scope of these studies was initially limited to energy analyses but was later broadened to encompass resource requirements, emission loadings and generated waste. LCA studies in this period mainly focused on packaging alternatives.\" (Guin\u00e9e 2011, p.9; 5) Here, a study done for the Coca-Cola company set the foundation for subsequent life cycle analyses (4). Yet, the method was not unified at all, but existed as a broad variety of theoretical frameworks and methodological approaches. This led to a lack of comparability between the results and hindered the method from becoming an accepted analytical tool until the 1980s (1).\n\nDuring the 1980s, the Green Movement in Europe strengthened public interest in the method (4). In the 1990s, its image and usefulness improved through the engagement of SETAC (Society of Environmental Toxicology and Chemistry) and ISO (International Organization for Standardization) in the harmonization and standardization of LCAs. At this time, the method also first appeared in scientific publications (1).\n\nSince the year 2000, LCAs have been increasingly adopted by policy actors (such as the EU, US and UN). Alongside a publicly increasing role of life-cycle-thinking, more diverse forms of the LCA have been been developed that differ in terms of their system boundaries, allocation methods and the considered impacts (1).\n\nMore recently, the Life Cycle Sustainability Assessment (LCSA) has been suggested to better connect LCAs with the sustainability discourse. The LCSA broadens the scope of the LCA both in terms of criteria involved (from purely environmental to economic, environmental and social impacts) and regional scope (from the product to the industry) while deepening the perspective on the elements involved in the Life Cycle (1, 2, 3). While this approach is still in development, it may be seen as one/the future of the Life Cycle approach (see Outlook).\n\n\n== What the method does ==\nThe underlying idea of the Life Cycle Analysis is that the environmental impact of a product. activity or process is not to be found in the use, but (mostly) in the production, transportation and disposal of this product. '''The LCA thus attempts to gather a quantitative overview on the resources used and the waste / emissions produced during the whole 'life' of any given product''' - 'from cradle to grave' - to provide a more accurate account of its environmental footprint (1, 4, 5). Based on this, comparisons between similar products may be made or a company (or policy-maker) is enabled to decide which [[Glossary|processes]] or materials might reasonably minimize the environmental impact of a product in order to subsequently act on these insights (4, 5). A LCA is also often conducted internally for a company to learn about potential energy - and thus cost - savings and in order to substantiate marketing claims about the environmental compatibility of a product (4).\n<br/>\n[[File:LCA Elements.png|600px|thumb|center|'''Life Cycle of a product.''' Source: (Ciambrone 1997, p.7)]]\n<br/>\n'''A LCA consists of four steps''' (4, 5).\n* First, the '''scope of the analysis''' is defined. This includes the elements (processes, inputs and outputs) that are to be considered and the boundaries of the analysis (see Figure 3). The boundaries are crucial, since they do not only define what is to be studied, but also which elements are left out of the analysis - e.g. socio-economic impacts of the product (4). Each LCA may define the system's boundaries and individual steps differently, which is why the proper documentation of the specific system definition is relevant to allow for comparability with other studies (4). The same is true for the in- and outputs which need to be defined according to specific standards (5; see Further Information for ISO standardization). This harmonization and unambiguous definition and documentation further assures a valid interpretation of the data (4).\n\n[[File:LCA Input Output.png|500px|thumb|right|'''The Inventory step of the LCA.''' Source: (Ciambrone 1997, p.19)]]\n\n* Second, the '''inventory analysis'''. During this step, quantitative data on the direct and indirect in- and outputs that arise during the life-cycle of the analyzed product are gathered (materials, energy, emissions, waste). Caution is advised in many aspects of this step to properly synthesize the data (see Strengths & Challenges).\n* Third, the '''impact analysis'''. This includes first the selection of categories for the environmental impacts - e.g. climate change, ecotoxicology - of the analyzed product with appropriate indicators - e.g. occurrence of natural disasters, quality of aquatic life. This is followed by the assignment of the inventory results to these impact categories, and concluded with the calculation of the environmental impacts (7). The latter is done by multiplying the inventory emission of a specific compound (e.g. CO2 per unit of measurement) with a characterisation factor that is provided in the literature or an official database. The product of this multiplication represents the impact of this emission on the specific category indicator. As an example, the amount of carbon dioxide emitted in the life cycle of a product can be converted (by the use of the characterisation factor) into a level of contribution to climate change as measured by the increased occurrence of natural disasters.\n* Lastly, the '''improvement analysis''' which highlights the possible changes to be made by policy actors or companies to the life-cycle processes of the product. These actors may use the conclusions of this step to prioritize their potential future decisions.\n\n[[File:LCA example.png|600px|thumb|center|'''US normalization of cradle-to-grave impacts for the consumption of 3,7 kg cheddar cheese.''' Source: Kim et al. 2013, p.1030]]\n\n== Strengths & Challenges ==\n* For companies, conducting LCAs to their products may lead to \"(...) better and less-polluting products that are less expensive and provide a marketing edge over the competition\" (Ciambrone 1997, p.3).\n* LCAs can also be a powerful tool for policy-making and the development of new technologies and products, especially with a focus on sustainability. They can also help the customer and the public get a better understanding of the environmental impacts of products (see Normativity).\n* Whenever a LCA is conducted with the intent of publishing the data, but based on company-internal data, the question of confidentiality must be addressed (4). Confidential data may be erroneous and can hardly be verified by external readers, which may disguise analytical errors (5).\n* The appropriate use of inventory data is of special importance for the LCA, and a variety of pitfalls may arise in the inventory phase.\n** The researcher must ensure the recentness of the data and take into the account the (temporal) specificity of the data as opposed to average numbers. A sufficiently long time period of data is advisable to smooth out individual variations (4).\n** The researcher must make sure that the data is normalized for each product (4).\n** Some data may not be obtainable, in which case it is interesting to ask why it is not, and how important this data is to the overall LCA results. If an emission is not (easily) measurable, a qualitative discussion of its amount might be reasonable, which should then be properly reflected upon (4).\n** When analyzing and synthesizing the data from individual steps of the life cycle, the researcher must be aware of uncertainty of the respective datasets. If the uncertainty of one - or several combined - datasets becomes too high, the overall result might not be valid enough to serve the intended purpose. In this case, a test for sensitivity of the overall result with regards to the exclusion or inclusion of said data is advisable to check the relevance of the data for the general conclusion (4).\n** In terms of analysis, the researcher must be aware of potential differing conditions within an industry (deviating regulations and standards, other technologies, materials and processes) if the purpose of the LCA is a product comparison (4).\n\n'''Quality criteria for the LCA''' are (4):\n* quantitative data that is synthesizable and comparable to the results of other LCAs,\n* replicable information with sufficient documentation,\n* a scientific approach to the data gathering and analysis,\n* a comprehensive assessment of all significant resources and emissions,\n* sufficiently detailed data with regards to the study purpose,\n* peer-reviewed quality assurance in case of public use and\n* usefulness of the results for further integration.\n\n\n== Normativity ==\n* Often, products are hardly comparable due to varying product characteristics whose (dis-)advantages depend on choices that are not in the hands of the producer. Thus, \"(...) in many - perhaps most - cases LCA can only expose the trade-ofs. It can only rarely point unambiguously at the 'best' technological choice.\" (Ayres 1995, p.201)\n* It should be noted that the results of LCAs can serve not only as information on how to decrease manufacturing costs of a product, but also as marketing material for companies. These two elements of the LCA are market-related side effects of the environmental impact reduction that is - or should be - the main goal of the LCA. Therefore, when reading a company-led publication of LCA results, one should be aware of potential selectivity in the data that is intended to proclaim a specific image of the company or product (4).\n* In addition, knowledge about the environmental (and/or social) footprint of any product has potential implications for consumer behaviour. A good example for this is the plastic vs paper bag discussion, which acknowledges the carbon footprint during the production of both products as well as the environmental consequences of their (inappropriate) disposal, indicating a differentiated perspective on the issue (see (4)). Another example is the discussion of electric vs non-electric vehicles, where the energy consumption during the production and the usage of the car are taken into consideration, but also the social costs of battery manufacturing. The assessment of all relevant impacts of the product and thus the (non-)purchase decision is in the hands of the consumer. In this regard, the LCA allows for more informed decisions but can also be misleading if not conducted and communicated properly.\n\n\n== Outlook ==\nAs mentioned before, the LCSA approach, including considerations of sustainability into Life Cycle Assessments, may constitute the future direction of the field. As of now, this approach is not fully developed and important challenges persist, but the discourse about acknowledging the wider impacts of products is ongoing and can be expected to further shape the method (for more on the LCSA, see (2) and (3)).\n\n\n== Key publications ==\n* Ciambrone, D.F. 1997. ''Environmental Life Cycle Analysis.'' CRC Press.\nA detailed description of the LCA process.\n\n\n== References ==\n(1) Guin\u00e9e, J.B. 2011. ''Life cycle assessment: past, present and future.'' International Symposium on Life Cycle Assessment and Construction, Nantes, France.\n\n(2) Zamagni, A. Pesonen, H.-L. Swarr, T. 2013. ''From LCA to Life Cycle Sustainability Assessment: concept, practice and future directions.'' International Journal of Life Cycle Assessment 18. 1637-1641.\n\n(3) Guin\u00e9e, J. ''Life Cycle Sustainability Assessment: What Is It and What Are Its Challenges?'' In: Clift, R. Drucklman, A. (eds.) ''Taking Stock of Industrial Ecology.'' Springer Cham. 45-68.\n\n(4) Ciambrone, D.F. 1997. ''Environmental Life Cycle Analysis.'' CRC Press.\n\n(5) Ayres, R.U. 1995. ''Life cycle analysis: A critique.'' Resources, Conservation and Recycling 14. 199-223.\n\n(6) Kim, D. Thomas, G. Nutter, D. Milani, F. Ulrich, R. Norris, G. 2013. ''Life cycle assessment of cheese and whey production in the USA.'' International Journal of Life Cycle Assessment 18. 1019-1035.\n\n(7) Pennington, D.W. et al. 2004. ''Life cycle assessment Part 2: Current impact assessment practice.'' Environment International 30. 721-739.\n\n\n== Further Information ==\n* Standards for LCAs are maintained by ISO who publish new iterations regularly. The current version of the LCA standard is ISO 14044 which is accessible [https://www.beuth.de/de/norm/din-en-iso-14044/279938986 here].\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "9olqxxq7pctm295mhfriiamfgpgb5tn"
                }
            },
            {
                "title": "Likert Scale",
                "ns": "0",
                "id": "569",
                "revision": {
                    "id": "5804",
                    "parentid": "4956",
                    "timestamp": "2021-06-14T07:48:56Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* 3. Inference Statistics */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "10986",
                        "#text": "'''In short:''' Likert Scales are categorical ordinal scales that are used in surveys to measure attitude. They are widely used in Social Sciences, but also in employee surveys when asking about satisfaction in the job or in customer surveys where products/services can be rated. \n\n== Background ==\nThe name of the scale comes from the American psychologist Rensis Likert. In 1932, Likert developed a 7-point agreement scale out of interest in measuring people\u2019s opinions and attitudes. His initial scale is very similar to the ones used today. \n\n== What do Likert Scales look like? ==\nThe most common Likert Scale asks for attitude on 5 different levels: Strongly Agree, Agree, Neutral/Undecided, Disagree, and Strongly Disagree. Other options include 3 or 7 answer options. Odd numbers are commonly used to include a neutral or undecided option in the middle. If no such neutral option is added, we call the scale a forced-choice method, because respondents are forced to decide if they are leaning more against the agree or disagree side of the scale. Sometimes, opt-out responses are also used, such as \u201cI don\u2019t know\u201d or \u201cNot applicable\u201d. Those are then placed outside the scale. \nApart from attitude, Likert scales can also be used to measure other variations such as quality, frequency or likelihood to give some examples. In online surveys, a variant with a kind of slider has been used more and more recently: In this variant, respondents can indicate their answer on a scale from zero (minimum) to 100 (maximum). \n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"dplyr\")\ninstall.packages(\"kableExtra\")\ninstall.packages(\"ggplot2\")\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf <- data.frame( \n  Attitude = c(\"Strongly Agree\", \"Agree\", \"Neutral\", \"Disagree\", \"Strongly Disagree\"), \n  Quality = c(\"Excellent\", \"Good\",\"Fairly Good\", \"Poor\", \"Very Poor\"), \n  Frequency = c(\"Always\", \"Often\", \"Sometimes\", \"Rarely\", \"Never\"), \n  Likelihood = c(\"Definitely\", \"Probably\", \"Possibly\", \"Probably not\", \"Definitely not\")) \n\nkbl(df, booktabs = T, caption = \"Likert Scale Examples\") %>% \n  kable_styling(latex_options = c(\"striped\", \"hold_position\"), \n                full_width = F)\n</syntaxhighlight>\n\n[[File:Likert Scale - Examples.png|400px|frameless|center|'''Examples of a Likert Scale'''. Source: own.]]\n\n\n== How to analyze Likert Scales & Visualization in R ==\nEven though one may find that analyzing Likert Scales looks fairly easy and straight forward, there is a huge debate on how to analyze these scales correctly. The biggest point of conflict in the discussion is whether the scale can be considered ordinal or interval. While Likert Scales technically are ordinal data, because there clearly is a grading: \"Strongly Agree\" is graded higher than \"Agree\", strictly speaking, the assumption of equal distance between categories is not valid. That is, the distance between \u201cAgree\u201d and \u201cNeutral\u201d theoretically is not the same as the distance between \u201cNeutral\u201d and \u201cNot Agree\u201d. That is why, we would strictly not consider Likert Scales as interval scaled, where each distance should be equal. \n\nHowever, in some areas, it is accepted to apply parametric statistics for Likert Scales, such as calculating a mean, doing an [[ANOVA]] or [[Simple_Statistical_Tests#One_sample_t-test|t-test]], which means treating them as an interval. Cases in which Likert Scales can be treated as intervals are if there are a high number of response options per question, when response options are assumed to be equally spaced, or when respondents mark their answer on a line so that the precise location of the mark can be measured. Making a decision about which method to apply, one should look at the literature of its discipline, reflect the assumption for each method and apply the method which is widely accepted in its field. \n\n==== 1. Coding the Responses ====\nStatements in a survey are often differently framed. While some are framed positively, for instance, the participant may be asked for its attitude to the statement \u201cI am satisfied with what I learned this semester\u201d, others are framed negatively, like \u201cI am not satisfied with the course offer this semester\u201d. The different framing is used to check for respondents who do not read the question and just click the same answer every time. In a statistical analysis, it may be useful to recode the responses, so that each response is going in the same logical direction when 0 is \"Strongly Disagree\", 1 is \"Disagree\", and so on. Let\u2019s see how that works with an example dataset I will construct here. \n\n<syntaxhighlight lang=\"R\" line>\n#construct a random dataset \nset.seed(5) # random number will generate from 5 \nsatisfaction_learning <- floor(runif(100, min=0, max=5)) #construct 100 random responses of range 0 to 100\ndissatisfaction_semester <- floor(runif(100, min=0, max=5)) \nsex <- floor(runif(100, min = 0, max = 2)) \n\n# We want 100 responses with satisfaction_learning being coded 0 = Strongly Disagree, 1 = Disagree... \n# We recode dissatisfaction_semester to satisfaction_semester by making the opposite: 4 = Strongly Disagree,\n#3 = Disagree, etc.\n\ndf <- data.frame(id = 1:100, \n                 sex = case_when(sex == 0 ~ \"man\", \n                                 sex == 1 ~ \"woman\"), \n                 satisfaction_learning = case_when(satisfaction_learning == 0 ~ \"Strongly Disagree\", \n                                                   satisfaction_learning == 1 ~ \"Disagree\", \n                                                   satisfaction_learning == 2 ~ \"Neutral\", \n                                                   satisfaction_learning == 3 ~ \"Agree\", \n                                                   satisfaction_learning == 4 ~ \"Strongly Agree\"), \n                 satisfaction_semester = case_when(dissatisfaction_semester == 4 ~ \"Strongly Disagree\", \n                                                   dissatisfaction_semester== 3 ~ \"Disagree\", \n                                                   dissatisfaction_semester == 2 ~ \"Neutral\", \n                                                   dissatisfaction_semester == 1 ~ \"Agree\", \n                                                   dissatisfaction_semester == 0 ~ \"Strongly Agree\")) \n\n# Create a function to calculate the mode of the given responses\ngetmode <- function(v) { \n  uniqv <- unique(v) \n  uniqv[which.max(tabulate(match(v, uniqv)))] \n} \nprint(mode_satis_learning <- getmode(df$satisfaction_learning))\nprint(mode_satis_semester <- getmode(df$satisfaction_semester))\n</syntaxhighlight>\n\n==== 2. Descriptive Statistics ====\nTo analyze Likert Plots, one can have a look at the results in a descriptive way at first. That means, looking at the mode (which is the most frequent response) and plotting the percentages of people who disagree, agree etc., in a stacked barplot, a graph or one bar for each response. \n\nIn our example data, \"Strongly Agree\" is the mode for the statement of being satisfied with what one learned this semester and \"Agree\" is the most frequent answer for the satisfaction of the course offered this semester. Let\u2019s plot the learning satisfaction ratings of all the participants in one stacked barplot with percentages per gender. \n\n<syntaxhighlight lang=\"R\" line>\n# We need the label Agreement for our colors later \ndf$satisfaction_learning <- ordered(df$satisfaction_learning,\n                                    levels = c(\"Strongly Disagree\", \"Disagree\", \n                                               \"Neutral\", \"Agree\", \"Strongly Agree\"))\n\n# We have to put the labels in order \ndf$satisfaction_learning <- ordered(df$satisfaction_learning, \nlevels = c(\"Strongly Disagree\", \"Disagree\", \"Neutral\", \n\n# Create a unique color palette (I like to use colors from Wes Anderson movies) \ncol_pal_satisf <- tibble(\n  agreement = c(\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"),\n  colors = c(c(\"#D1362F\", \"#F24D29\", \"#FCD16B\", \"#CECD7B\", \"#2E604A\")))\nsum_satisfaction_learning <- df %>%\n  group_by(sex, satisfaction_learning) %>%\n  summarise(n = n()) %>%\n  group_by(sex) %>%\n  mutate(proportion = n/sum(n))\nsum_satisfaction_learning %>%\n  ggplot(aes(sex, proportion, fill = satisfaction_learning)) +\n  geom_col() +\n  scale_fill_manual(values = setNames(as.character(col_pal_satisf$colors), col_pal_satisf$agreement))\n</syntaxhighlight>\n\n[[File:Likert Scale - Diagram.png|500px|frameless|center|'''The visualisation of the Likert Scale data above.''' Source: own.]]\n\n==== 3. Inference Statistics ====\nThe last step now really depends if we would treat the scale as ordinal or interval. Here are some analysis options for when treating the scale as ordinal or reducing it to a binary variable. If it is treated as an interval, any parametric test ([[ANOVA]], [[Regression Analysis|Linear Regression]], etc.) can be computed. \n\n'''If treating scale as ordinal''' \n* Ordinal Regression \nHere, the dependent variable is ordinal in the regression. The ordinal package in R provides a powerful and flexible [[Glossary|framework]] for ordinal regression. It can handle a wide variety of experimental designs, including those with paired or repeated observations. Look [https://www.r-bloggers.com/2019/06/how-to-perform-ordinal-logistic-regression-in-r/ here] for more on Ordinal Regressions in R.\n\n* Tests for ordinal \nTests for ordinal data are arranged in a contingency table form. These include the linear-by-linear test, which is a test of association between two ordinal variables, or the Cochran-Armitage test. This is a test of association between an ordinal variable and a nominal variable. One has to consider though, that these tests are limited to data arranged in a two-dimensional table and they require spacing between ordinal categories. More information can be found [https://rcompanion.org/handbook/H_09.html here]. \n\n* Permutation tests \nOne can use the coin package in R to run permutation tests with ordinal dependent variables. It can handle models analogous to a one-way analysis of variance with stratification blocks, including paired or repeated observations. For more information on these tests visit this [https://rcompanion.org/handbook/K_01.html link]. \n\n'''If reducing scale to a binary variable''' \n<br>\nSometimes, it is acceptable to reduce the responses of a Likert Scale to only two responses: summing up for instance \u201cStrongly Agree\u201d and \u201cAgree\u201d to one category, and \u201cStrongly Disagree\u201d and \u201cDisagree\u201d to another category and dropping the neutral response. In this case, lots of information will get lost, but if there is a strong argumentation for this step in the research, it is valid to do so. In this case, one can do a Logistic Regression if the dependent variable is the Likert Scale or just add a dummy variable in a Linear Regression if the independent variable is ordinal. \n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Nora Pauelsen."
                    },
                    "sha1": "cnvf5ojov94c8g7ql00gstfommm4ph5"
                }
            },
            {
                "title": "Limitations of Statistics",
                "ns": "0",
                "id": "805",
                "revision": {
                    "id": "7209",
                    "parentid": "5902",
                    "timestamp": "2023-06-12T12:02:41Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* Limitations of statistics within science */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15476",
                        "#text": "'''In short:''' This entry revolves around what Statistics cannot do, and where its limits are - if you are interested in what Statistics CAN do, have a look at the [[Statistics]] overview page. You might also be interested in the topic of [[Bias in statistics]].\n\n== Statistics and the where it may fail us ==\n[[Statistics]] provide a specific viewpoint with which you can look at the world, and it can help you understand parts of what you're seeing. '''If more people understood statistics and the insights we can gain through it, these people would become more empowered to make their decisions based on their own analysis and insight.''' However, we do not only need to recognise the benefits that can be gained through statistics, but we also need to be aware of the limitations of statistics. After all, it is a view through numbers and models, and can therefore only offer parts of the complete picture. In addition, we have to accept that statistics are also riddled by diverse biases, which we also have to take into our focus and conscious recognition as much as possible. More about [[Bias in statistics]] can be found here.\n\nBeyond problems directly associated to statistics as such, there are also problems associated to the relative position of statistics within science. Statistics were feeling the flame of positivism, and a critical perspective on statistics is beyond a doubt of pivotal importance for the recognition of the role that statistics can play to produce knowledge, but also to understand where statistics may fail us. '''In other words, we need to be clear about what statistics can do for us as scientists, and what it cannot do.''' The other major problem that statistics face within science is the normativity of statistics. There are diverse norms and schools of thinking that got established over time, and within this entry we will take a look at some examples on how differences, co-evolutions and interchanges influence the role of statistics within science. To this end, a third point may also be a historical perspective, which can be by no means exhaustive, but should enable us to once more to a look at the role of statistics in science.\n\nThe question of the limitations of statistics in the current societal debates - and the possibilities and opportunities which statistics may offer for the great transformation - are in the focus of the last part of this entry. Why are many societal debates so disconnected from available results derived through statistics? And how can we improve the literacyy regarding statistics within society? Lastly, we want to examine with a critical eye how the limitations of statistics may contribute to some of the problems we face in society today, and how we may ultimately overcome these.\n\n\n== Limitations of statistics ==\nLimitations of statistics can be divided into several parts.\n\n==== The dependent variable ====\nFirst, and many would argue most importantly, are the questions of plausibility and validity. Are the questions we are testing plausible, and are the predictors we use to access these models valid? We often test constructs: for instance, GDP is commonly used as a proxy for many other things. While this had some surely some merits in the past, it also draws an increasing amount of criticism. Another example would be the Intelligence Quotient, which may tell you something about people, but more often than not is misleading and judgmental. Have a look at the entry [[To Rule And To Measure]] to learn more, and also remember the role that [[Data_distribution | data distributions]] can play.\n\n==== The independent variable ====\nEqually, predictors may vary in their [[Experiments_and_Hypothesis_Testing#Validity | validity and reliability]]. This may be related to the way the predictors are measured, how constructed specific predictors are, or if they can actually serve as proxies to answer the questions we want to answer. Money can be a predictor for happiness, but it has clearly its limitations, and the amount of money needed to have a decent life may differ between countries. Also remember that different [[Data_formats | data formats]] are highly relevant in order to understand the nature of your data.\n\n==== Random factors ====\nThis brings us to the next factor that can affect our model validity. There are many things we might want to know when creating a statistical model, but there may also be things that we do not want to know. Statistically speaking, these are ''random factors'' or ''random variables''. Regarding these, we explicitly exclude the variance that is created by these parts of the dataset, because we want to minimise their effects. An example would be a medical study that wants to investigate the effect of a certain drug on the recovery rate of some diseased patients. In such studies, the information whether a patient is a smoker or not is often included as a random factor, because smoking negatively affects many diseases and the respective recovery rates. We know that smoking makes many things worse (from a medical standpoint) and this is why such variables are excluded. More on random factors can be found [[ Field_experiments#Fixed_effects_vs._Random_effects | here]].\n\n==== The model ====\nThe next part of a [[An_initial_path_towards_statistical_analysis | statistical model]] that needs to be examined when discussing the limitations of statistics is the statistical test or model itself. Models differ in terms of their mathematical foundations. This can lead to some models being more complicated than others, relying on different measures of model performance, and are also sometimes differently utilised within different scientific communities or disciplines. Data visualisations showcase how different branches of sciences have different ways to approach how to [[Introduction_to_statistical_figures | show data in figures]], which can be diverse and often follow specific norms within different scientific disciplines.  \n\nThis is difficult to explain in a nutshell, but let me try: basically, different historical developments in the diverse disciplines, as well as the specific relation of a discipline towards the respective theory and topics, enable differences in how statistics are being utilised by different disciplines. Yet, there are also examples of exchanges and even coevolutionary patterns. For instance, the [[ANOVA]] started within agricultural science, but equally unleashed its potential in psychology, medicine and ecology, as these all rely strongly on the [[Experiments and Hypothesis Testing|testing of hypotheses.]] Econometrics and engineering build more strongly on [[Correlations | correlative]] testing, which is again related to the specifics of these disciplines. Most simple tests are [[Simple_Statistical_Tests#Normativity | hardly being used these days]], a phenomena that unites almost all disciplines, yet sometimes even very [[Back_of_the_envelope_statistics | simple calculations]] can offer robust answers. However, the difference between scientific disciplines regarding their statistical use does not always make sense, and is not necessarily rooted in reasonable decisions.\n\n== Limitations of statistics within science ==\n'''Equally grave (and not necessarily always meaningful) is the [[Design Criteria of Methods|great divide]] between quantitative and qualitative branches within science.''' While we should acknowledge that there are clear limitations of knowledge driven from statistics, it is equally difficult to reject knowledge derived from quantitative methods altogether. It is well deserved that with the rise of positivism - which widely cemented its reign through quantitative methods and particularly statistics - a qualitative counter-culture established itself. However, this does by far not do justice to the highly creative and so desperately needed deeper unlocking of qualitative dimensions in research. [[Interviews]], [[Grounded Theory]], [[Ethnography|Ethnographic Observations]] and [[Content Analysis]] are examples of important methods being established that go way beyond previous methods, and some of these approaches have a [[History of Methods|long history]] of highly relevant applications in science. However, statistics is nevertheless embedded into a judgemental perspective, creating a dichotomy that has not always been of benefit for science. Some would argue that the positivist wars - and the subsequent science war and culture war - are on a long chain of interwoven conflicts that may have been solved by [[Bias and Critical Thinking|critical realism]]. \n\n'''We have to acknowledge that the knowledge we create is subjective''', which one of the main claims of critical realisms. Still, our way of using this knowledge to make sense of the world and derive heuristics and concepts that tell us how we ought to act may be objective. For instance, I might acknowledge that our knowledge about the validity of a vaccine to prevent me from getting a certain disease is a snapshot in time, and governed by conservative and robust norms in research. Hence I might decide to trust this knowledge in order to not only protect myself, but also to contribute to the protection of society. The decision to protect myself can be seen as hedonism, while the decision to protect my surrounding and ultimately society as a whole can be declared my contribution to the social contract. This showcases that my intentionality can be rooted in different principles, and the way we conduct [[Case_studies_and_Natural_experiments | experiments]] showcases how these areas of science are still under development. My actions may be informed by statistics, but statistics can only serve as a basis for my ontology, not be ontology in itself.\n\n== Limitations of statistics within society ==\n'''This brings us to the complicated relationship between statistics and society.''' Through its contribution to agricultural science, medicine, economics and many other fields, statistics have served as one of the main methoetodological approaches to [[Scientific methods and societal paradigms|contribute to a growth-focused economy]]. It can be clearly argued that this led to manifold benefits, but also to problems, including destruction, degradation and inequalities. All the while, science became increasingly glorified in the middle of the 20th century. However, due to the many negative effects that statistics indirectly contributed to, a critical perspective on statistics increased within the societal debate. What in the past was perceived as factual knowledge was increasingly questioned, and even altogether rejected.\n\n'''It is clear that the knowledge derived from statistics has value, but also limitations for societal discourse.''' Within an increasing rise of dichotomies as part of the culture wars, science was often called out, or simply taken as an emotional counterpoint. The debate on climate change showcases that both sides argue strongly from an emotional perspective (if we think in terms of the extreme points within the debate). At the same time, both sides also make selective arguments, having different focus points in terms of the goals they are trying to achieve. Climate change activists try to argue based on scientific evidence to protect the planet. Climate change deniers often take a limited or blurry understanding of bits and pieces of information to create a counterpoint that focuses, for instance, on the rights of some few people to continue their current strategies. Both sides seemingly argue about so-called 'facts', which showcases the deep entrenchment and differences that currently riddle societal debates. Statistics, to this end, is often devalued by one side or the other, and becomes a collateral due to criticism that emerges from false understandings or implausible claims. \n\n'''This highlights that statistics in itself may not be normative if all people share the same understanding about it.''' Sadly, this is not the case, and hence the limitations of statistics within societal debates are not much more than a mirror of the lack of education and ignorance that many people have to endure. Statistics is thus a privilege that some people can benefit from, but to use this educational effort as a devaluation of people who do not share this privilege is a difficult point. It would be naive to say that if all people shared the same knowledge, we'd all come to the same conclusions. But would it not be wonderful if this was the case?\n\nI believe that ''power to the people'' is still one of the best idea we ever had. Yet, with it comes the responsibility to provide an education that enables all people to use their [[Glossary|power]] to take decisions. Ethics suggests that these decisions ought to be rooted in principles. These principles can be informed by information that can originate from - among many other sources - [[Why_statistics_matters | statistics]]. During the pandemic, for instance, the interest in numbers increased among the broader public. At the same time, it also became clear that different numbers and changes in these numbers represented different aspects of the pandemic at different times. Sometimes it was the amount of spread of the disease (r-value), sometimes the number of occupied hospital beds, at other times the mortality, and sometimes the increase in cases. This created confusion, and even the experts were not agreeing which number was relevant at different times, or which combination of values. This showcases that statistics are still emerging, especially when it comes to taming wicked problems, and also highlights the gap between science and society. It does not make any sense to blame people that were never educated in statistics for not understanding it. It becomes clear that science still has a long way to go to explain knowledge from statistics in a sense that it becomes a contribution to the great transformation, yet statistics should seize this opportunity and learn to contribute in a critical way.\n\n==External Links==\n===Articles===\n* [https://lifereconsidered.com/2018/04/25/six-major-limitations-of-statistics/ A nice take on statistics and its limitations]\n\n===Videos===\n* [https://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen The classic Hans Rosling Ted talk]\n* [https://www.ted.com/talks/mark_liddell_how_statistics_can_be_misleading A Ted Talk about how statistics can be misleading]\n* [https://www.ted.com/talks/sebastian_wernicke_1_000_ted_talks_in_six_words The meat Ted talk]\n* [https://www.ted.com/talks/sebastian_wernicke_lies_damned_lies_and_statistics_about_tedtalks Ted Talk - Lies, dammned lies, and statistics]\n* [https://www.ted.com/talks/mona_chalabi_3_ways_to_spot_a_bad_statistic Ted Talk - Some suggestions to spot mistakes in stats]\n\n===Podcasts===\n* [https://www.wnycstudios.org/podcasts/radiolab/episodes/91697-numbers Radiolab Episode about numbers] \n* [https://www.wnycstudios.org/podcasts/radiolab/episodes/91684-stochasticity Radiolab Episode about stochasticity]\n* [https://www.wnycstudios.org/podcasts/radiolab/articles/love-numbers another Radiolab Episode about numbers 3]\n* [https://www.thisamericanlife.org/88/numbers A this american life episode on numbers]\n* The [https://www.bbc.co.uk/programmes/p02nrss1/episodes/downloads more or less] podcast\n\n----\n[[Category:Normativity of Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "4q92rai0yy6ox168av1n67nb8697pup"
                }
            },
            {
                "title": "Lists, Tuples, Sets, and Dictionaries in Python",
                "ns": "0",
                "id": "1027",
                "revision": {
                    "id": "7012",
                    "timestamp": "2023-03-31T08:10:39Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article provides information on how to store and use compound data types.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9303",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Introduction==\nIn this tutorial, we will learn more about different ways to store data in PytHon. We will specifically look at lists, tuples, sets, and dictionaries (so-called compound data types) that can store multiple integers, strings, or even another set or tuple inside it. We will look at the advantages and disadvantages of the different compound datasets and when it's best to use which. When coding, it is hard to get around them since they are very useful when you clean and prepare your dataset.\n\n==Tuples==\nTuples are an ordered sequence. As a sequence, it also has an index of its values, just like a string has an index for its characters. It is represented by **brackets** ().\n\nExample: a Tuple with multiple data types has values \u201cinside\u201d\n\n<syntaxhighlight lang=\"Python\" line>\nmy_tuple = (\u201dnagisa\u201d, 7, 1.1)\nmy_tuple[0] #\u201dnagisa\u201d\nmy_tuple[1] #7\nmy_tuple[2] #1.1\n</syntaxhighlight>\n\nTo get the **length** of the tuple (how many elements inside a Tuple), we can use the function \u201clen()\u201d\n\nExample: \n<syntaxhighlight lang=\"Python\" line>\nlen(my_tuple)\n</syntaxhighlight>\n\nThe unique feature of Tuple is that it is **immutable** which means that it is not changeable after it is created. \n\nExample: if you don't believe me, you can try it \n\n<syntaxhighlight lang=\"Python\" line>\nwrong_tuple = (1,2)\nwrong_tuple[0] = 2\n</syntaxhighlight>\n\nPython will give you an error code since you have tried to change the tuple \"wrong_tuple\" by replacing the 1 with a 2.\n\n==Lists==\nLists are also an ordered sequence, just like tuples. \n\nThe differences are: \n1. It is represented by squared brackets []\n2. It is mutable, which means it can be changed after being created\n\nExample: Changing the first element of a list\n\n<syntaxhighlight lang=\"Python\" line>\nmy_list = [\u201dcharmander\u201d, 1, 1.2]\nmy_list[0] = \u201ccharizard\u201d \n</syntaxhighlight>\nmy_list now becomes [\u201dcharizard\u201d, 1, 1.2]\n\nEverything else is the same compared to tuples! \n\n=List - Aliasing=\n\nWhat happens when you create a list A = [\u201dGoku\u201d, 7]\nand then assign B = A ?\n\nTry it yourself, what happened to List A when you change B[0] = \u201cVegeta\u201d?\n\nList A[0] will also changed to \u201cVegeta\u201d. \n\nWhen multiple names are referring to the same object, it is known as Aliasing. \n\n=List - Cloning=\n\nSometimes, aliasing creates more problem than solving things. So remember, there is another method similar to liasing, but without referencing the same list, called Cloning. \n\nTo clone, simply use: \n\n<syntaxhighlight lang=\"Python\" line>\nB = A[:] \n</syntaxhighlight>\n\nThe code above will clone List A, and name it B.\n\nHow to choose tuples or lists?\nTuples and lists can be very similar in their function. However, some things need to be considered if you have to decide between lists and tuples.\n1.) Order of the data: If the order of the data is very important, it is advisable to use tuples since they are immutable. This way you can guarantee, that the order does not change accidentally over time. If you want to change the order, you can just convert the tuple to a list, make the changes and convert it back.\nFor example:\n<syntaxhighlight lang=\"Python\" line>\ntuple_1 = (1, \"A\", 3, 4, 5)\nlist=list(tuple_1)\nlist[2]=6\ntuple_1=tuple(list)\ntuple_1\n(1, \"A\", 6, 4, 5)\n</syntaxhighlight>\nBut of course, converting it back and forth takes time and might be unhandy if you know that you might have to change your data compound from time to time. Then, a list might be better\n2.) Data size and processing efficiency: Tuples are faster stored and processed than lists. Therefore, I might be useful to use tuples when you have a large dataset or know that other steps in your research demand a lot of time. For smaller datasets, it is not so important what you choose.\n\n==Nesting==\nIn programming, nesting means having a group inside a group. Since tuples can contain tuples inside it as an element, it is called tuple nesting. You can imagine it as a folder inside of a folder. \n\nExample: I want to save my biography as a tuple. It includes my name, my age, any 2 of my favourite colors as a tuple.\n\n<syntaxhighlight lang=\"Python\" line>\nmy_bio = (\u201dSteph\u201d, 22, (\u201dRed\u201d,\u201dOrange\u201d) )\n</syntaxhighlight>\n\nHow to access \u201cRed\u201d then? It can be accessed in 2 steps: \n\n1. my_bio[2] \u2192 will give me (\u201dRed\u201d, \u201cOrange\u201d)\n2. my_bio[2][0] \u2192 will give me \u201cRed\u201d specifically.\n\nThis means that I can choose the elements in my nested tuple with these squared brackets. The order of the brackets follow the same order as the nesting. The first bracket determines the element within the larger tuple, and the second one within the chosen tuple.\n\n==Dictionaries==\nDictionaries are similar to lists, but it has unique indexes. Let\u2019s start with an example of list and dictionary below:\n{| class=\"wikitable\"\n|-\n! List Index !! List Element\n|-\n| 0 || \"Gohan\"\n|-\n| 1 || \"8\"\n|}\n\n{| class=\"wikitable\"\n|-\n! Dictionary Key !! Header text\n|-\n| \"name\" || \"Gohan\"\n|-\n| \"age\" || 8\n|}\n\nAs we can see above, instead of just numbers 0 and 1, dictionaries have a special unique \u201ckey\u201d.\n\nHow to create Dictionary? \n\n1. Use curly brackets {}. \n2. The keys have to be immutable and unique\n3. The values can be immutable, mutable, and duplicates (anything)\n4. Each key and value pair is separated by a comma\n\nExample: \n<syntaxhighlight lang=\"Python\" line>\nmy_dict = {\n\u201dnum\u201d: 1, \n\u201cname\u201d: \u201cBulma\u201d, \n\u201cchildren\u201d: (\u201dalice\u201d, \u201cbatty\u201d, \u201ccarry\u201d), \n\u201cage_children\u201d:[4,5,6]  }\n</syntaxhighlight>\n\nHowever, above is just to give you an example of the flexibility of dictionary. Actually, in real world, people use dictionary, just like real-world people use dictionaries. \n\n\"Real Example\"\n<syntaxhighlight lang=\"Python\" line>\nmy_phone_book = { \u201cChancellor\u201d: 0123456789, \u201cBeyonce\u201d: 0987654321 }\n</syntaxhighlight>\nSo that you can just call the key : my_phone_book[\u201cChancellor\u201d] or \u201cBeyonce\u201d to get their phone numbers. \n\nHere are some special commands that might be useful to get you started using dictionaries:\nDictionary special commands: \n\n1. **del**: to delete an entry\nExample:\n<syntaxhighlight lang=\"Python\" line>\ndel(my_phone_book[\u2019dad\u2019])\n</syntaxhighlight>\n \u2192 it will delete the \u201cdad\u201d entry from the dictionary. \n2. **in** : to verify if a key is in the dict\nExample: \n<syntaxhighlight lang=\"Python\" line>\n\u2018mom\u2019 in my_phone_book \n</syntaxhighlight>\n3. **keys()** : method to get all the keys in the dict\nExample:\n<syntaxhighlight lang=\"Python\" line>\nmy_dict.keys()\n</syntaxhighlight>\n4. **values()** : similar to keys(), but we get values instead\nExample:\n<syntaxhighlight lang=\"Python\" line>\nmy_dict.values()\n</syntaxhighlight>\n\n==Sets==\nSets are a type of collections, just like lists and tuples. However sets only have unique elements. \n\nExample: \n<syntaxhighlight lang=\"Python\" line>\nmy_set= {\u201dgoku\u201d, \u201cvegeta\u201d, \u201ckrillin\u201d, \u201c**goku**\u201d} \n</syntaxhighlight>\nThe duplicates will be automatically removed when the set is created.\n\nSets are therefore especially useful if you want to make sure that there are no duplicates in the data you work with. For example, if you want to know how many households there are with children, you can just create a set of all individuals in households with children, and the set will only choose one individual per household. You then know how many households with children there are.\n\nHere are some additional commands to get you going using sets:\n1. set() : to cast the list into a set. \nExample:\n<syntaxhighlight lang=\"Python\" line>\nset(my_list)\n</syntaxhighlight>\n2. add() : to add a new element into the set\nExample:\n<syntaxhighlight lang=\"Python\" line>\nmy_set.add(\u201dmajinbu\u201d)\n</syntaxhighlight> \n3. remove() : to remove an element in the set\nExample: \n<syntaxhighlight lang=\"Python\" line>\nmy_set.remove(\u201dkrillin\u201d)\n</syntaxhighlight> \n\nMathematical operations:\n\n1. AND (&) : intersection of two sets \u2192 find the similar element\nExample:\n<syntaxhighlight lang=\"Python\" line>\nyour_set = {\u201dgoku\u201d, \u201cnaruto\u201d, \u201cnatsu\u201d}\nour_set = my_set & your_set \u2192 it will contain \u201cgoku\u201d\n</syntaxhighlight> \n2. union()\nExample:\n<syntaxhighlight lang=\"Python\" line>\nmy_set.union(your_set)\n</syntaxhighlight> \n3. issubset() : to check if a set is a subset of a bigger set. \nExample: \n<syntaxhighlight lang=\"Python\" line>\nsmall_set.issubset(big_set)\n</syntaxhighlight> \n\n==Quiz==\n1. Supposed you have a Tuple A = (11,22,33,44), print the last 2 element of the tuple. \n2. What happens when you add 2 tuples together? \ntuple_1 = (\u201dred\u201d, 1)\ntuple_2 = (\u201dblue\u201d, 2)\ntuple_3 = tuple_1 + tuple_2\n3. Is it possible to put Tuples inside Lists? How about the other way around? \nlist = [ (\u201dtuple\u201d, 1), (\u201dtuple\u201d, 2) ]\ntuple = ([\u201dlist\u201d, 1 ], [\u201dlist\u201d , 2] )\nHow to access the first element of the list and tuple? \n4. How to get the length of a set = {\u201dA\u201d,\u201dB\u201d,\u201dC\u201d} \n5. You want to analyze how siblings interact with each other based on age difference and there relative position to their siblings (if they are the oldest/youngest child or in the middle). What kind of compound data would you use if you set up your data for the analysis?\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "d1vhtp3zhxlpjsb2268p4jog5idh87i"
                }
            },
            {
                "title": "Living Labs & Real World Laboratories",
                "ns": "0",
                "id": "422",
                "revision": {
                    "id": "5959",
                    "parentid": "5944",
                    "timestamp": "2021-06-30T20:39:26Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "24357",
                        "#text": "[[File:ConceptLivingLab.png|450px|left|frameless|[[Sustainability Methods:About|Method categorization for Delphi]]]]<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n\n'''Annotation:''' This entry deals with two concepts: Living Labs as well as Real-World Laboratories. While these are not strictly scientific methods as much as they are settings and modes, they shall be presented here in order to highlight their unique approach to scientific inquiries. Further, while both concepts are closely related, they are distinct in various aspects. Because of this, their description will be done separately where necessary and jointly where appropriate.\n\n'''In short:''' Living Labs (and Real-World Laboratories) are research, [[Glossary|innovation]] and learning environments that facilitate solution-oriented research.\n\n== Background ==\n[[File:Living labs.png|400px|thumb|right|'''SCOPUS hits per year for Living Labs and Real World Laboratories until 2019.''' Search terms: 'living laboratory', 'living lab', 'real world laboratory', 'real world lab' in Title, Abstract, Keywords. Source: own.]]\n\n'''Both the Living Lab and the Real-World Laboratory approach are rather new research areas''', emerging from the beginning of the 21st century.\n\nThe Living Lab (LL) started outside of academia, revolving around testing new technologies in home-like constructed environments in order to promote entrepreneurial innovation (1). In 2006, the European Network of Living Labs (EnoLL) was founded by the Finnish government, focusing on information and communication technologies and institutionalizing the concept (6, 9). The [[Glossary|concept]] is therefore common predominantly in Europe but has also expanded to other geographical contexts during the last years (see Further Information). Living Labs have also emerged as a [[Glossary|transdisciplinary]] scientific method, its applications going beyond product and service development for the sake of business: Living Labs have also been used in the fields of, among others, eco-design (4), IT (1), urban development (2), energy or mobility for the purpose of sustainable development.\n\nThe concept of Real-World Laboratories ('Reallabore') emerged around the same time, but was a scientific field from the beginning. It is most prevalent in German-speaking research communities that focus on sustainability and transdisciplinary transformative research (10). A broad set of RWLs have been set up in and supported by the German state of Baden-W\u00fcrttemberg since 2015 (14).\n\n== What the method does ==\n\n'''Living Labs and Real-World Laboratories are research, innovation and learning environments.''' They share a variety of traits, but differ in crucial elements. Their characteristics will be described below. \n\nThe '''Living Lab''' \"(...) has become an umbrella concept for a diverse set of innovation milieus (...)\" (1, p.357) and has been described as a \"rapidly diffusing phenomenon (...)\" (6, p.139), indicating a lack of a distinct methodological conceptualization (9). The Living Lab has often been defined as both a methodology and the environment in which this methodology is being applied (1, 3, 6, 9). Generally, LLs revolve around the testing and implementation of new, marketable and standardized products and services that provide social or technological innovation (10). They experiment in settings with limited [[Glossary|participation]] with these innovations and attempt to create generalizable insights. They do not necessarily contribute to transformative change.\n\n'''Recurring fundamental elements of Living Labs include:''' \n\n* ... the creation and testing of technological and/or social innovation (a service, a product, societal infrastructure) with the goal of solving real-world problems (1, 4, 6, 8)\n* ... in a real-life test and experimentation environment (e.g. a house, a school, a city, a region or a virtual network) (1, 3, 6, 7, 8, 9)\n* ... through the involvement and collaboration of diverse stakeholders with a common interest in the respective domain, including actors from business, government and academia as well as citizens (1, 7, 8, 9). The active, open and conscious co-involvement of 'users' of the respective service  or product in the innovation process, equally among other stakeholders, constitutes a central idea of the Living Lab as opposed to more passive approaches where they are seen as subjects whose behavior is to be studied (1, 3, 5, 6, 7, 9).\n* ... with a technological component that allows for standardized, comparable feedback on the innovation process (1, 7, 9)\n* A Living Lab can take between months and years (1, 9)\n\nWithin this co-creation approach, users can be involved on different stages of the innovation process: the ideation and conceptualization stage (co-creation), the implementation of the product or service or the evaluation (2, 7, 9). For each of these stages, a diverse set of potential methods exists that can be used to include the users' and stakeholders' perspectives (see Normativity). To provide the best results of the Living Lab, involvement on all of these stages should be combined in accordance with the specific goals and [[Glossary|vision]] of the Living Lab (2, 9). However, an analysis of four Living Lab projects by Menny et al. (2018) indicates that the stage of co-creation is rarely achieved - most often, users are only included in the implementation or evaluation phase (2).\n\n'''Real-World Laboratories''', too, \"(...) exhibit a broad and not clearly defined research format.\" (Sch\u00e4pke et al. 2018, p.94). They are generally understood as research environments that complement transdisciplinary and transformation-oriented sustainability research by offering a real-world environment for experimentation and reflexive learning. RwLs attempt to close the gap between research and practice by combining scientific research with contributions to societal change in order to solve societally relevant problems (12, 13). This may help solve scientific problems, but also practical issues and fulfill educational purposes (13). RwLs are both interdisciplinary and transdisciplinary in that they strongly favor the involvement of various stakeholders from different scientific and non-scientific backgrounds that are relevant to the problem at hand. RwLs and Living Labs share the characteristics of stakeholder involvement and real-world experimentation, but focus more on the process of researching, learning and testing of new structures and transformative [[Glossary|processes]] rather than on the implementation of a specific innovation.\n\n[[File:Living Labs example visualisation.png|600px|thumb|center|The Real-World Laboratory approach. Source: Bernert et al. 2016, p.257]]\n\n'''Core characteristics of Real-World Laboratories''' are that (10, 13)\n\n* ... they contribute to transformation by experimenting with potential solutions and support transitions by providing evidence for the robustness of solutions\n* ... they deploy transdisciplinarity as the core research mode in order to \"(...) integrate scientific and societal knowledge, related to a real-world problem\" (Sch\u00e4pke 2018 p.87). They \"(...) can build on previous transdisciplinary process of, for instance, co-designing a shared problem understanding and related vision (...) or they can inclue these steps.\" (Sch\u00e4pke 2018, p.87)\n* ... they establish a culture of sustainability around the laboratory, stabilize the cooperation between the actors and empower the involved practitioners\n* ... they therefore have a strong normative and ethical component and pursue to contribute to the common good\n* ... they include experiments as a core research method which they provide concrete settings for, and which stakeholders are actively involved in (co-design and co-production)\n* ... they attempt to create solution options that \"(...) have a long-term horizion, potentially going beyond the existence of the lab\" (Sch\u00e4pke 2018, p.87)\n* .... they have a strong educational aspect and support three levels of reflexive learning: individual competency, social learning and learning with regard to transdisciplinary collaboration\n* ... learning outcomes are used to evaluate the research procedure, applied to improve the research process, and transferred to other transformation processes\n* ... they create knowledge on and for transformative processes which consists of system knowledge, target knowledge and process knowledge\n\n'''Apart from LL and RWL, there are further related approaches which should be mentioned:''' \n\n* (Urban) Transition Labs are based on transition management and focus on developing new ideas, practices and structures to support transition processes (10). Guiding principles and future visions for the transition process are developed and translated into experiments through backcasting (10, p.88). They focus neither on transdisciplinary research processes nor on specific socio-technical innovations, but revolve around the conceptualization of broader socio-technical change, including social learning and [[Glossary|empowerment]] processes.\n* Transformative Labs focus on systemic human-environment connections that are neglected in other transition management approaches. They strongly acknowledge environmental feedbacks in the innovation process and attempt to change the system dynamics that created existent problems in the first place (10). They \"(...) build on extensive pre-studies and collective system-analysis to develop prototypes of systemic innovations and awareness amongst participants that they are part of a system (reflexivity)\" (Sch\u00e4pke et al. 2018, p.94).\n\n== Strengths & Challenges == \n\n'''Living Labs'''\n\n* The central element of the Living Lab - the co-involvement of 'users' at all stages of the development process - constitutes a distinctiveness of the method and provides specific advantages to the process (7). Instead of designing the product around the alleged needs of the users, the ideas and knowledge contributed by the users enable the developer (e.g. the company, the researchers) to more properly design their product / service according to the user's needs and desires (4, 6). The contextual knowledge gained in the real-world environment further facilitates the design process of the respective product or service (6, 7).  This way, the risk of failure is decreased while the result better fits the user and supports sustainability, e.g. by reducing the rebound effect in eco-design (4).\n* At the same time, this element poses challenges. First, it may be difficult to develop effective methods and business models to motivate individuals to participate in the Living Lab process during which they provide work and knowledge that should be compensated (1, 9). This collaboration between all the different stakeholders ought to be continuous to make the Living Lab successful (1, 7, 8). Also, the participants should depict a sufficiently diverse and realistic representation of the eventual group of 'users', which raises the question of how to gather participants for the Living Lab (1, 2, see Normativity). In this regard, a socially inclusive, high level of involvement may lead to higher engagement and better results of the overall process (2).\n\n'''Real-World Laboratories'''\n\n* RwLs highlight the need for profound participation and cooperation (11). Doing justice to the diverse roles, perspectives and intentions of the involved stakeholders requires a differentiated procedure, structure and choice of methodological approaches. Further, \"[u]ndertaking collaborative, real-world experiments (...) often raises particular challenges regarding ownership, transparency, knowledge integration, and conflict management.\" (Sch\u00e4pke 2018, p.87)\n* The various kinds of intended knowledge raise a need for a highly reflexive application of methods. Another challenge in this regard is to balance the descriptive-analytical and the prescriptive research focus (10). \"RwL approaches are subject to major challenges. These include high expectations (e.g., delivering evidence-based knowledge and governing societal change), blurring of boundaries and responsibilities due to the engagement of researchers in societal actions, and a lack of analytical distance in the research process between the researchers and their objects of investigation.\" (Sch\u00e4pke et al. 2018, p.94)\n* \"A major challenge regarding research quality concerns the generation of generic and transferable insights from experiments in specific contexts, with many factors that are difficult to control\" (Sch\u00e4pke 2018, p.87) In order to provide transferable and scalable solutions, feasibility studies, comparisons of experiments between labs and the involvement of key actors from different scales are beneficial. However, this \"(...) requires adequate project architectures and longer-term fuding, allowing for continuous experimentation and longitudinal evaluation. Transferability and scalability are particularly challenging and potentially limited due to the situatedness of RwLs\" (Sch\u00e4pke et al. 2018, p.87)\n\n\n== Normativity ==\n\n==== Connectedness ====\n\nAs shown in the diagram below, the incorporation of user and stakeholder perspectives in Living Labs can take various methodological forms, including traditional scientific methods such as interviews or ethnographics as well as tools and methods originating from marketing and business.\n\n[[File:Living Lab Connectedness.png|600px|thumb|center|Possible methods to apply in a Living Lab. Source: Feurstein et al. 2008, p.4]]\n\nRwLs can employ a variety of methods and combinations thereof, partly ones are developed specifically for the Real-World Laboratory context (13). Distinct methods of transdisciplinary research are listed in the [[Transdisciplinarity|TD entry.]]\n\n==== Everything normative ====\n* As with all transdisciplinary research methods, Living Labs and Real-World Laboratories have a highly normative component that relates to collaborating with real-world actors (see the [[Transdisciplinarity|TD entry]]). For the Living Lab, this is the case since it revolves around creating products and services that provide maximum suitability to the needs, desires and ideas of individuals. Living Labs are situated in the middle of the user involvement spectrum ranging from the user as the main creator to the user as a passive subject whose insights are introduced into the innovation process (3, 6)\n* For RwLs, the transformative purpose raises questions of which kind of future scenario is desired by whom: \"RwLs [Real-World Laboratories] become immersed in political and normative issues that science traditionally attempts to avoid. Correspondingly researchers take on new roles in addition to what is traditionally seen as research (i.e., producing knowledge), including acting as facilitators of the process, knowledge brokers, and change agents\" (Sch\u00e4pke et al. p.87)\n* The selection of the participating individuals and stakeholders is a normative endeavour. An important notion in this regard is the question of an Open vs Closed format of user involvement - i.e. is everyone allowed to participate or only selected users and stakeholders? A closed format allows for more focused and in-depth feedback but requires the capacity to select individuals and limit access to the research environment. An open format, on the other hand, is simpler to implement and provides more diversity, but requires the capacity to filter results and manage the greater number of users (6).\n* Both approaches serve as empowering environments for the individuals involved (1, 8, 10, 13). They can have educational impacts and enable citizens to participate in processes that influence their daily lives.\n\n\n== Outlook ==\n* '''Being a rather young methodological approach''', the Living Lab still lacks clear definitions and supporting concepts as well as robust and homogenous methodologies, methods and tools (1, 4, 9). The majority of theoretical publications include thoughts on how to locate the method within participatory design approaches, or descriptions of individual projects. Few peer-reviewed studies exist that provide an instructional, clear illustration of the method's elements (6, 9). This impedes its more wide-spread use of the approach but may also be seen as flexibility, displayed by the various applications and adaptations of the approach in an increasing amount of diverse contexts (5, see Further Information).\n* Real-World Laboratories are yet to be established in the realm of the other comparable approaches. They are not \"(...) unique or completely new in what they pursue and in the ways they proceed. They are part of a larger development: the emergence of a family of transdisciplinary and experimental approaches to transformative research. (...) Current trends in [[Glossary|funding]] programs and research collaborations provide space to further explore the potential of experimental approaches in transformative research. Long-term evaluation and comparisons will show which approaches or combinations are most promising, in terms of real-world sustainability transformation and acceleration in given contexts. (...) [T]he diverse emerging approaches may complement each other, rather than compete for being the \u201cbest\u201d approach.\" RwLs \"(...) have a lot to learn from the other approaches. (...) Their open approach may provide a suitable [[Glossary|framework]] for combining different components of experimental transformative research. Thus, RwLs might contribute to building bridges between different styles of transformative research, the design and implementation of experiments, as well as the evaluation fo processes of learning and reflexivity.\" (Sch\u00e4pke et al. 2018, p.95)\n* \"Despite the dynamic development of lab approaches in the last decades, '''their diffusion is still limited'''. The contribution of such approaches to societal transformation largely depends on them being embedded into a broader policy commitment to systemic change, as well as on the development of mechanisms to accelerate learning.\" (Sch\u00e4pke et al. 2018, p.95) If their long-term establishment is supported, RwLs may provide an institutionalized frame for systematic transdisciplinary and transformative research and experimentation. In this regard, RwLs have the potential to support the further development of science-society collaborations and provide new ideas for academia (11). \"On their own, lab approaches risk to have limited real-world impact. Creating some isolated space for experimentation, the significance of labs for societal transformation might remain limited by their own borders. It is thus important to complement lab approaches with broader policy commitments, if we want to harness the transformative potential of these approaches in the real world.\" (Sch\u00e4pke et al. 2018, p.95)\n\n== Key Publications ==\n* Veeckman, C. Schuurman, D. Leminen, S. Westerlund, M. 2013. Linking Livinb Lab Characteristics and Their Outcomes: Towards a Conceptual Framework. Technology Innovation Management Review 3(12). 6-15.\n\nA more recent explanation of the concept and an analysis of four Living Labs with a focus on how their characteristics influenced their results.\n\n* Sch\u00e4pke, N. et al. 2018. Jointly Experimenting for Transformation? Shaping Real-World Laboratories by Comparing Them. GAIA 27(1). 85 \u2013 96.\n\nPresents and compares diverse approaches to transformative research.\n\n* Defila, R. Di Giulio, A. Reallabore als Quelle f\u00fcr die Methodik transdisziplin\u00e4ren und transformativen Forschens - eine Einf\u00fchrung. in: Defila, R. Di Giulio, A. (eds). 2018. Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung. Springer VS.\n\nA German summary of learnings and experiences made with Real-World Laboratories.\n\n* Schwartz et al. 2015. What People Do with Consumption Feedback: A Long-Term Living Lab Study of a Home Energy Management System. Interacting with Computers 27(6). 551-576.\n\nA Living Lab study on the topic of energy efficiency in households, conducted for 18 months in Germany. Data was collected through energy logs, interviews, surveys, observation and Grounded Theory\n\n* Bernert, P. et al. 2016. Towards a Real-world Laboratory. A Transdisciplinary Case Study from L\u00fcneburg. GAIA 25(4). 253-259.\n\nA case study that presents the establishment of a RwL in L\u00fcneburg.\n\n\n== References ==\n(1) Bergvall-Kareborn, B. Stahlbr\u00f6st, A. 2009. Living Lab: an open and citizen-centric approach for innovation. International Journal of Innovation and Regional Development 1(4). 356-370.\n\n(2) Menny, M. Palgan, Y.V. McCormick, K. 2018. Urban Living Labs and the Role of Users in Co-Creation. GAIA 27. 68-77.\n\n(3) Almirall, E. Lee, M. Wareham, J. 2012. Mapping Living Labs in the Landscape of Innovation Methodologies. Technology Innovation Management Review 2(9). 12-18.\n\n(4) Liedtke, C. Welfens, M.J. Rohn, H. Nordmann, J. 2012. LIVING LAB: user-driven innovation for sustainability. International Journal of Sustainability in Higher Education 13(2). 106-118.\n\n(5) Niitamo, V.P. Kulkki, S. Eriksson, M. Hribernik, K. 2006. State-of-the-art and good practice in the field of living labs. Proceedings of the 12th International Conference on Concurrent Enterprising: Innovative Products and Services Through Collaborative Networks.\n\n(6) Dell'era, C. Landoni, P. 2014. Living Lab: A Methodology between User-Centered Design and Participatory Design. Creativity and Innovation Management 23(2). 137-154.\n\n(7) Hribernik, K. et al. 2008. Living Labs - A New Development Strategy. in: Schumacher, J. Niitamo, V.P. (eds.) 2008. European Living Labs - A New Approach for Human Centric Regional Innovation. Wissenschaftlicher Verlag, Berlin, Germany. 1-14.\n\n(8) van der Walt, J.S. Buitendag, A.A.K. 2009. Community Living Lab as a Collaborative Innovation Environment. Issues in Informing Science and Information Technology. Volume 6. 421-436.\n\n(9) Veeckman, C. Schuurman, D. Leminen, S. Westerlund, M. 2013. Linking Living Lab Characteristics and Their Outcomes: Towards a Conceptual Framework. Technology Innovation Management Review 3(12). 6-15.\n\n(10) Sch\u00e4pke, N. et al. 2018. Jointly Experimenting for Transformation? Shaping real-world laboratories by Comparing Them. GAIA 27(1). 85 \u2013 96.\n\n(11) Defila, R. Di Giulio, A. Reallabore als Quelle f\u00fcr die Methodik transdisziplin\u00e4ren und transformativen Forschens - eine Einf\u00fchrung. in: Defila, R. Di Giulio, A. (eds). 2018. Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung. Springer VS. 9-38.\n\n(12) Arnold, A. Piontek, F. Zentrale Begriffe im Kontext der Reallaborforschung. in: Defila, R. Di Giulio, A. (eds). 2018. Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung. Springer VS. 143-155.\n\n(13) Beecroft, R. et al. Reallabore als Rahmen transformativer und transdisziplin\u00e4rer Forschung: Ziele und Designprinzipien. In: Defila, R. Di Giulio, A. (eds). 2018. Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung. Springer VS. 75-100.\n\n(14) Ministerium f\u00fcr Wissenschaft, Forschung und Kunst Baden-W\u00fcrttemberg. Baden-W\u00fcrttemberg f\u00f6rdert Reallabore. Available at https://mwk.baden-wuerttemberg.de/de/forschung/forschungspolitik/wissenschaft-fuer-nachhaltigkeit/reallabore/\n\n\n== Further Information ==\n* The [https://enoll.org/ EnoLL] (European Network of Living Labs) connects, lists and supports hundreds of Living Lab projects in mostly Europe, but also internationally. \n* A good overview on projects, literature and actors for Real-World Laboratories for sustainability can be found on https://www.reallabor-netzwerk.de/.\n\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "sfdi6kvdlgzcnpb8lvmpgnxdaer3jje"
                }
            },
            {
                "title": "Loom",
                "ns": "0",
                "id": "733",
                "revision": {
                    "id": "5102",
                    "parentid": "5101",
                    "timestamp": "2021-04-16T09:47:40Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2712",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nLoom is a '''video recording software which lets you easily record your screen and yourself at the same time''' right in your browser. This way, you can show your work or ideas to colleagues or others, and not waste time and harddrive space on recording videos right on your computer.\n\n== Goals ==\n* Communicate your work to colleagues, collaborators, students in a way that is precise and engaging.\n* Better enable others to prepare for meetings.\n\n== Getting started ==\n[[File:Loom Logo.png|400px|thumb|right|'''Loom logo.''' Source: [https://www.loom.com/ Loom]]]\nFor some situations, it might be practical to record your screen and talk over what is shown. Loom lets you record your browser, Excel file, presentation or anything you open on your computer, and record your face and voice while clicking through the applications. For browser recordings, you can simply enable a Loom extension; for anything else, you download the software; and there is also a mobile app. You just record the video and share the link and others can watch it online. There are voice transcripts, and you can share the video alongside a call-to-action, if you like.\n\n'''We found Loom to be helpful to convey a point to colleagues so that they can prepare for a meeting, or to quickly summarize our work for colleagues and others''', so that they can engage with the video whenever they find the time. Loom is easier than recording the video with external software, and sharing or re-uploading the created .mp4-file. As opposed to video-call screen-sharing, Loom can be used asynchronously.\n\n[https://www.loom.com/ Loom] is free for anyone and allows for 5-minute videos, up to 100 videos in total. There are paid options that allow for recordings of up to 45 minutes, and these are [https://www.loom.com/education free for educational accounts].\n\n== Links & Further reading ==\n* [https://www.youtube.com/watch?v=3PY6v9s1MU8 An introductory video to using Loom] \n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "3y6rkjhsmllfrgn6rx015brzsrq03qv"
                }
            },
            {
                "title": "Loops in Python",
                "ns": "0",
                "id": "1029",
                "revision": {
                    "id": "7245",
                    "parentid": "7015",
                    "timestamp": "2023-06-29T05:30:02Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to for and while loops in Pyhton.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3976",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n===Repetition===\nSometimes, you want to repeat (read: iterate) an operation many times. Although you can achieve that using copy-pasting skills, sometimes it is not enough. A simple example where copy-pasting will take so much time is when we have to print the same thing 777 times. Repeated operations in programming are performed by \u201c'''Loops'''\u201d. There are two ways of doing loops, using \u201cfor\u201d and \u201cwhile\u201d operations, which in my opinion are interchangeable. \n\nBefore we are going for the loops, let\u2019s talk about the range object. The range command tells Python where to start and where to end. We can tell the loop when to end, by determining the range. For example, the number 3 has a range from 0 to 2. We can also find this out with the command: range (3)\n\n<syntaxhighlight lang=\"Python\" line>\nrange(3)\n</syntaxhighlight>\n\n===For Loop===\nFor loop is one of the main 2 loops that we have in python programming. For example, you want to iterate the number 0-10\n\n<syntaxhighlight lang=\"Python\" line>\n#for loop example\nfor i in range(11): # this is when you initialized the variable i\n\tprint(i)          # this when you use it by printing it\n</syntaxhighlight>\n\nAs we can see above, we initialized the variable \u201ci\u201d and it will iterate 11 times. Every time the code prints, the value of \u201ci\u201d increased by 1. \n\nWe can also use for loop to see and print the element of our list. As an example, I have a hand of cards and I want to show my friend my cards.\n\n\n<syntaxhighlight lang=\"Python\" line>\n#print all the card in my hand\n\ncards = [2,5,7,'jack','king']\ncard_num = len(cards)\n\nfor i in range(card_num):\n\tprint(cards[i])\n</syntaxhighlight>\n\nSince combining for loop to do array operations is very often used, Python has a simpler shortcut for that. \n\n<syntaxhighlight lang=\"Python\" line>\n#shortcut for printing list\ncards = [2,5,7,'jack','king']\nfor card in cards:\n\tprint(card)\n</syntaxhighlight>\n\nSince combining for loop to do array operations is very often used, Python have a simpler shortcut for that.\n\n===While Loop===\nThe main difference between the \u201cwhile\u201d and \u201cfor\u201d loop is that the while loop is used when we don't know when we want to loop a certain algorithm. The \u201cwhile\u201d loop will keep on executing loops until a given logical condition.\n\n\n<syntaxhighlight lang=\"Python\" line>\nnumber = 0\nwhile (number < 11):\n\tprint(number)\n\tnumber = number + 1\n</syntaxhighlight>\n\nTo bridge our knowledge regarding for loop, we can see the code above is very similar to what we have created before. However, while loop shines when we want to search for something inside the list. \n\nAs an example, in our hand, we want to know in which position of the card is our jack.\n\n<syntaxhighlight lang=\"Python\" line>\ncards = [2,5,7,'jack','king']\n\ni = 0\ncard = cards[0]\n\nwhile(card != 'jack'):    \n    print(card)\n    i = i + 1<<\n    card = cards[i]\nelse:\n    print(\"Jack is in the position\", i+1)\n</syntaxhighlight>\n\n===Quiz===\n# Write a for loop that prints out all elements from range -7 until 7\n# Write a for loop that prints out all elements in this list=[9,4,5, \u2019ace\u2019, \u2018jack\u2019]\n# You think that a proper fruit salad needs to have a banana. This is the order in which fruits are added to your salad: ingredients= [\"strawberry\", \"apple\", \"pear\", \"orange\", \"blueberry\", \"banana\", \"lime\", \"raspberry\"]. Make a while loop in which the respective fruit added to the salad is printed until the banana is added. Then print \"this is a proper fruit salad\".\n# You have a bad hand. To win, you want to change your card to be [10, \u2018jack\u2019, \u2018queen\u2019, \u2018king\u2019, \u2018ace\u2019]. Implement a for loop to change your hands to the winning hands!\n\n<syntaxhighlight lang=\"Python\" line>\ncards = [2,3,4,5,6]\nfor i in range (len(cards)): \n\t# TODO insert your code here\n</syntaxhighlight>\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "4q2ln12v8bo7k4k4r6bvjyuw0rvftc1"
                }
            },
            {
                "title": "Machine Learning",
                "ns": "0",
                "id": "421",
                "revision": {
                    "id": "5912",
                    "parentid": "5316",
                    "timestamp": "2021-06-28T08:49:03Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Background */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18681",
                        "#text": "[[File:ConceptMachineLearning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Machine Learning]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"|  '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>\n\n__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and/or the nature of the outputs is well understood. \n\nHowever, in situations where the inputs can be ''noisy'' or the outputs are expected to be different in each case, you cannot hand-craft the \"[[Glossary|rules]]\" that account for ''every'' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: '''Machine Learning'''. The core idea behind Machine Learning is that instead of being required to hand-craft ''all'' the rules that take inputs and provide outputs in a fairly accurate manner, you can ''train'' the machine to ''learn'' the rules based on the inputs and outputs that you provide. \n\nThe trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].\n\n== What the method does ==\nThe term \"Machine Learning\" does not refer to one specific method. Rather, there are three main groups of methods that fall under the term Machine Learning: supervised learning, unsupervised learning, and reinforcement learning.\n\n=== Types of Machine Learning Tasks ===\n\n==== Supervised Learning ====\nThis family of Machine Learning methods rely on input-output pairs to \"learn\" rules. This means that the data that you have to provide can be represented as <syntaxhighlight lang=\"text\" inline>(X, y)</syntaxhighlight> pairs where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> is the input data (in the form of vectors or matrices) and <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ..., y_n)</syntaxhighlight> is the output (in the form of vectors with numbers or categories that correspond to each input), also called ''true label''. \n\nOnce you successfully train a model using the input-output pair <syntaxhighlight lang=\"text\" inline>(X, y)</syntaxhighlight> and one of the many ''training algorithms'', you can use the model with new data <syntaxhighlight lang=\"text\" inline>(X_new)</syntaxhighlight> to make predictions <syntaxhighlight lang=\"text\" inline>(y_hat)</syntaxhighlight> in the future.\n\nIn the heart of supervised Machine Learning lies the [[Glossary|concept]] of \"learning from data\" and from the data entirely. This begs the question what exactly is learning. In the case of computational analysis, \"a computer program is said to learn from experience '''E''' with respect to some class of tasks '''T''' and performance measure '''P''', if its performance at tasks in '''T''', as measured by '''P''', improves with experience '''E'''.\"[3]\n\nIf you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc.\n* '''Performance measure (P)''' is a metric, or a set of metrics, that is used to evaluate the efficacy of the learning process and the \"learned\" algorithm. Usually, we want the learning process to take as little time as possible (i.e. we want training time time to be low), we want the learned algorithm to give us an output as soon as possible (i.e. we want prediction time to be low), and we want the error of prediction to be as low as possible.\n\nThe following figure from the seminal book \"Learning from Data\" by Yaser Abu Mostafa [1] summarizes  the process of supervised learning summarizes the concept in a succinct manner:\n\n[[File:Mostafa Supervised Learning.png|thumb|The Learning Diagram from \"Learning from Data\" by Abu-Mostafa, Magdon-Ismail, & Lin (2012)]]\n\nThe ''target function'' is assumed to be unknown and the training data is assumed to be based on the target function that is to be estimated. The models that are trained based on the training data are assessed on some error measures (called ''metrics'') which guide the strategy for improving upon the Machine Learning algorithm that was \"learned\" in the previous steps. If you have enough data, and adopted a good Machine Learning strategies, you can expect the learned algorithm to perform quite well with new input data in the future.\n\n'''Supervised learning tasks can be broadly sub-categorized into ''regression learning'' and ''classification learning''.''' \n\nIn ''regression learning'', the objective is to predict a particular value when certain input data is given to the algorithm. An example of a regression learning task is predicting the price of a house when certain features of the house (eg. PLZ/ZIP code, no. of bedrooms, no. of bathrooms, garage size, energy raging, etc.) are given as an input to the Machine Learning algorithm that has been trained for this specific task.\n\nIn ''classification learning'', the objective is to predict which ''class'' (or ''category'') a given observation/sample falls under given the characteristics of the observation. There are 2 specific classification learning tasks: ''binary classification'' and ''multiclass classification''. As their names suggest, in binary classification, a given observation falls under one of the two classes (eg. classifying whether an email is spam or not), and in multiclass classification, a given observation falls under one of many classes (eg. in ''digit recognition'' task, the class for a picture of a given hand-written digit can range from 0 to 9; there are 10 classes).\n\n==== Unsupervised Learning ====\nWhereas supervised learning is based on pairs of input and output data, unsupervised learning algorithms function based only on the inputs: <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> and are rather used for recognizing patterns than for predicting specific value or class.\n\nSome of the methods that are categorized under unsupervised learning are ''[https://en.wikipedia.org/wiki/Principal_component_analysis Principal Component Analysis]'', ''[[Clustering_Methods|Clustering Methods]]'', ''[https://en.wikipedia.org/wiki/Collaborative_filtering Collaborative Filtering]'', ''[https://en.wikipedia.org/wiki/Hidden_Markov_model Hidden Markov Models]'', ''[https://brilliant.org/wiki/gaussian-mixture-model/ Gaussian Mixture Models]'', etc.\n\n==== Reinforcement Learning ====\nThe idea behind reinforcement learning is that the \"machine\" learns from experiences much like a human or an animal would [1,2]. As such, the input data \"does not contain the target output, but instead contains some possible output together with a measure of how good that output is\" [1]. As such, the data looks like: <syntaxhighlight lang=\"text\" inline>(X, y, c)</syntaxhighlight> where <syntaxhighlight lang=\"text\" inline>X = (x_1, x_2, ..., x_n)</syntaxhighlight> is the input, <syntaxhighlight lang=\"text\" inline>y = (y_1, y_2, ... , y_n)</syntaxhighlight> is the list of corresponding labels, and <syntaxhighlight lang=\"text\" inline>c = (c_1, c_2, ..., c_n)</syntaxhighlight> is the list of corresponding scores for each input-label pair. The objective of the machine is to perform such that the overall score is maximized.\n\n=== Approaches to Training Machine Learning Algorithms ===\n\n==== Batch Learning ====\nThis Machine Learning approach, also called ''offline learning'', is the most common approach to Machine Learning. In this approach, a Machine Learning model is built from the entire dataset in one go.\n\nThe main disadvantage of this approach is that depending on the computational infrastructure being used, the data might not fit into the memory and/or the training process can take a long time. Additionally, models based on batch learning need to be retrained on a semi-regular basis with new training examples in order for them to keep performing well.\n\nSome examples of batch learning algorithms are ''[http://pages.cs.wisc.edu/~jerryzhu/cs540/handouts/dt.pdf Decision Trees]''(C4.5, ID3, CART), ''[https://www.datacamp.com/community/tutorials/support-vector-machines-r Support Vector Machines]'', etc.*\n\n==== Online Learning ====\nIn this Machine Learning approach, data is ordered and fed into the training algorithm in a sequential order instead of training on the entire data set at once. This approach is adopted when the dataset is so large that batch learning is infeasible, or when the nature of the data makes it so that more data is available over time (eg. stock prices, sales data, etc.)\n\nSome examples of online learning algorithms are ''[https://en.wikipedia.org/wiki/Perceptron Perceptron Learning Algorithm]'', ''[https://en.wikipedia.org/wiki/Stochastic_gradient_descent stochastic gradient descent] based classifiers and regressors'', etc.\n\n=== What about Neural Networks? ===\nNeural networks are a specific approach to Machine Learning that can be adapted to solve tasks many different settings. As a result, they have been used for detecting spams in emails, identifying different objects in images, beating humans at games like Chess and Go, grouping customers based on their preferences and so on. In addition, neural networks can be used in all three types of Machine Learning tasks mentioned above - supervised, unsupervised, and reinforcement learning.\n\nPlease refer to this [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi video series from 3blue1brown] for more on Neural Networks.\n\n\n== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.\n\n\n== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=\"text\" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=\"text\" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].\n\nHowever, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, ''k-means clustering'', ''hierarchical clustering'', various approaches to performing ''regression'', ''principle component analysis'' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other' expense [7]. \n\nThe ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called ''Differential Privacy'' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.\n\n\n== Key Publications ==\n* Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n* LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n* Cortes, Corinna, and Vladimir Vapnik. \"Support-vector networks.\" Machine Learning 20.3 (1995): 273-297.\n* Valiant, Leslie G. \"A theory of the learnable.\" Communications of the ACM27.11 (1984): 1134-1142.\n* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n\n\n== References ==\n(1) Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4). New York, NY, USA:: AMLBook.\n\n(2) Russell, S., & Norvig, P. (2002). Artificial intelligence: a modern approach.\n\n(3) Mitchell, T. M. (1997). Machine Learning. Mcgraw-Hill.\n\n(4) Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199\u2013231.\n\n(5) [https://www.vodafone-institut.de/aiandi/5-things-machines-can-already-do-better-than-humans 5 things machines can already do better than humans (Vodafone Instut)]\n\n(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305\u2013306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255\u2013260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP\u201906 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1\u201312).\n\n== Further Information ==\n* [https://www.datacamp.com/community/tutorials/introduction-machine-learning-python Introduction to Machine Learning in Python]\n* [https://www.datacamp.com/community/tutorials/machine-learning-in-r Machine Learning in R for Beginners]\n* [https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU Machine Learning Lecture from Andrew Ng (Stanford CS229 2018)]\n* [https://www.youtube.com/watch?v=0VH1Lim8gL8 Lex Fridman - Deep Learning State of the Art (2020) [MIT Deep Learning Series<nowiki>]</nowiki>]\n* [https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe How does Spotify know you so well?]: A software engineer tries to explain this phenomenon\n* [https://www.repricerexpress.com/amazons-algorithm-a9/ The amazon algorithm]: A possible explanation\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Past]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Prabesh Dhakal."
                    },
                    "sha1": "7ik3xgv6n10anw67we73x6iy40be8ro"
                }
            },
            {
                "title": "Macroinvertebrates",
                "ns": "0",
                "id": "1041",
                "revision": {
                    "id": "7088",
                    "parentid": "7087",
                    "timestamp": "2023-04-18T14:45:59Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "12372",
                        "#text": "[[File:ConceptMACROINVERTEBRATES.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[MACROINVERTEBRATES]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n\n\n'''In short:''' Netting, kick sampling and taking benthic cores are methods for collecting macroinvertebrate samples to obtain information about the water quality of a river, lake or pond.\n{| class=\"wikitable\"\n|-\n|  ||  '''Netting''' || '''Kick sampling''' || '''Taking benthic cores'''\n|-\n| '''Zone''' || nekton|| riffles in nekton|| benthal\n|-\n| '''Speed of water'''|| slow|| fast-flowing|| still or slow-moving\n|-\n| '''Depth of water'''|| shallow (pond nets), deep (tow nets)|| shallow|| shallow/ deep (using a boat)\n|}\n\n\n==Netting==\n=== What the method does ===\n[[File:Fig1 TeichkescherMACROINVERTEBRATES.jpg|thumb|right|Figure 1. Pond net for catching invertebrates. It is important that the collecting bag is long enough to throw it over the brim to prevent invertebrates from escaping. Source: https://www.der-gartenteich.com/teichkescher-klappbar-art.nr.-84296?sPartner=google-de-ts&c=172&gclid=Cj0KCQiA1sucBhDgARIsAFoytUvZORDGHrnxv01LJzM4d4JWxv9RoA5PRK0qP6bboR64KUU2WktN_dgaArTnEALw_wcB ]]\n\nTo standardize pond netting to enable comparisons between different sites or different times at the same site, two points in the water are marked with canes and the net is moved in between the poles for a certain number of times at a constant speed. \nAfter netting, the net is emptied into a white tray such as a photographic developing tray. The net should be washed very carefully to remove all the specimens. Some specimens such as nymphs often strongly cling to the net.\nFor tow netting, a net is pulled behind a driving boat to estimate the number of zooplankton in open water. While driving the boat, it is important to keep the opening of the net perpendicular to the water surface to keep the opening of the net at the same size throughout each tow. To measure the distance of towing, it is helpful to add a flowmeter to the net. After towing, the content of the collecting bottle is emptied into a container. To wash off all the zooplankton from the inside of the net, water is splashed from the outside. It can take a considerable amount of time to transfer all the invertebrates from the net into the container. The sample is preserved with 95% ethanol.\n\n=== Strengths and Challenges ===\nPond nets can be used for quick surveys of the species that occur in a pond or stream. Within only three minutes of netting, you can find 62% of families and 50% of species that you can catch during 18 minutes of netting (Furse et al. 1981). However, this method is hard to standardize as the net-hauls always need to have the same length and the same speed and these are hard to keep up. This means that this method can be used to get an overview on the number of species, but not to compare species richness and abundances between different sites or points in time.\nTow nets can be used in deep, open and weed-free waters. They are suitable for large areas of water, especially for watching zooplankton as this is often distributed in patches. They require more work as at least two people are needed, of which one steers the boat and the other one looks after the net. A major problem is to drive the boat with constant speed to standardize the method. Moreover, turbulence caused by the boat might disturb the position of the net. The nets can become clogged by sediment and phytoplankton, so that they lose their catching efficiency.\n\n\n==Kick sampling==\n===What the method does===\n[[File:Fig2 KicksamplingMACROINVETEBRATES.jpg|thumb|right|Figure 2. A student kick-samples a river stretch. Foto: John Rostron. Source: https://upload.wikimedia.org/wikipedia/commons/0/04/Sampling_in_the_River_Roding_-_geograph.org.uk_-_1430581.jpg]]\nKick sampling is a method that is applied to collect invertebrates from fast-flowing streams, especially from riffles, to assess the water quality. Therefore, a net is held on the ground of the river with its opening facing towards the water flow direction. The substrate is disturbed by kicking and the invertebrates are dislodged and collected in the net a short distance downstream (Figure 2). \n\nThe standardized version of this method consists of sampling all the micro-habitats within a stretch of a stream by kick-sampling for a total of three minutes by using a 0.9-mm-mesh pond net.\nInvertebrates are stored in a white tray. The sample can be preserved with 95% ethanol. After determination of the taxa, individual scores are calculated that are based on the tolerance towards oxygen depletion of each taxon. These scores are summed up to produce a biotic score for this section of the river. This score gives an estimate on the oxygen content of the river stretch and therefore, of its water quality.\n\n===Strengths and Challenges===\nKick-sampling is a quick and straight-forward method to collect invertebrates from rivers. Moreover, it is standardized and allows to calculate a score that can be used to estimate the water quality. However, only a small proportion of invertebrates from a streambed are caught, so that it cannot be used to calculate population densities.\n\n==Taking benthic cores==\n===What the method does===\n[[File:Fig3 WormsMACROINVERTEBRATES.png|thumb|right|Figure 3. a) Polychaete worm. b) Oligochaete worm. Sources: https://commons.wikimedia.org/wiki/File:Polychaeta_%28no%29_2.jpg https://commons.wikimedia.org/wiki/File:Oligochaeta.jpg]]\nTaking benthic cores is a simple way to collect invertebrates such as polychaete and oligochaete worms (Figure 3). \nFor taking benthic cores, a corer (Figure 4) is sunk into the substrate. If the substrate is firm, the corer is simply pulled from the substrate. If the substrate is soft, a thin piece of metal is slid across the opening of the corer to prevent the sand from falling out. \nThe same method can be conducted in deep water as well, using a boat. In this case, the corer need to be longer and have a bung that fits tightly into its top end. The corer is pushed into the substrate and the bung is pushed into the top end of the corer. This creates a vacuum that prevents the substrate from falling out. The depth of the corer should be around 10 to 15 cm for a sample of 5 cm depth. The corer is then pulled out of the substrate. The benthos is then slowly removed from the corer to not injure the invertebrates. The unwanted part of the sample can be discarded now. \nAfterwards, invertebrates are extracted by wet sieving with meshes of e.g. the sizes 2 mm, 1 mm and 0.5 mm for invertebrates of down to 1 mm length. To make the sorting process quicker and more efficient, a 1% solution of Rose Bengal dye might be added to stain the translucent invertebrates pink. If the sorting process should take place later on, the sample can be preserved with 95% ethanol.\n\n===Strengths and Challenges===\nIn general, this is a relatively quick and easy method to sample benthos. However, sampling in deep water is more challenging as a boat is needed.In this case, the main challenge is to steer the boat into the correct position.\n\n==Normativity==\nKick sampling underestimates the number of invertebrates attached to stones as these might not let go of the stones and therefore, are not caught in the net. This applies for instance to caddisflies. While taking benthic cores, some larger invertebrates might detect the disturbance in the ground and therefore, retreat deeper into the ground. Other invertebrates such as chironomids might be lost from the corer during sampling (Euliss et al. 1990). In both cases, some taxa get lost from the sample which causes a bias towards other taxa.\nThe taxa that are caught by netting will be strongly determined by the speed of the net. Slow netting causes fast swimming macroinvertebrates to avoid the net whereas fast netting pushes the water in front of the net which also causes some invertebrates to not get caught in the net. Moreover, invertebrate fauna differs with the depth of the water and it might be hard to always keep the net on the same depth during different rounds of netting. This makes catches from nets hard to compare. As netting tends to underestimate benthic invertebrates, it is suggested to combine this method with taking benthic cores. \n\n==Outlook==\nKick sampling and taking benthic cores can be easily standardized by taking the same number of kicks or cores at different locations. For netting, this is more difficult as the taxa caught in the net strongly depend on the depth of the water and keeping the net at the same height each time can be difficult .\nAll these methods for sampling macroinvertebrates are relatively quick and straight-forward. The time-consuming part is to afterwards sort the individuals into taxa and to determine each individual as far as possible/ needed. At the moment, it is still the standard procedure to determine macroinvertebrates using an identification key and a binocular microscope.\nHowever, molecular methods for species determination of macroinvertebrates are recently getting more and more precise. DNA metabarcoding enables species determination based on characteristic DNA sections without sorting the sample into taxa. The advantages are that this method is less time-consuming and enables determination up until species level. It prevents typical mistakes that come from sorting and determining individuals (2). As experts for certain groups of macroinvertebrates are rare and their work is expensive, meta-barcoding can even help to save money as it is expected to get cheaper in the future (3).\nPreserving and storing the samples for metabarcoding does not require special treatment. Samples are still suitable for DNA metabarcoding even after one year of storage in 95% ethanol (4). Nichols et al. (2020) show that all of the more abundant macroinvertebrate families that were identified by morphological examination were also detected through metabarcoding. However, the authors were unable to assign all macroinvertebrates to species as there is still a great lack of reference DNA barcodes in the DNA barcode libraries. As a result, more individual barcoding is required to fill the gaps in these libraries before metabarcoding is ready to be widely used in practical applications (3).\n \n==Key publications==\nTheoretical: Sutherland, W. J., Editor (1996). Ecological Census Techniques - a Handbook. P. 178-180. Cambridge University Press.\n\nEmpirical: Euliss Jr, N. H., Swanson, G. A., & MacKay, J. (1992). Multiple tube sampler for benthic and pelagic invertebrates in shallow wetlands. The Journal of wildlife management, 186-191.\n\n==References==\n(1) Furse, M. T., Wright, J. F., Armitage, P. D., & Moss, D. (1981). An appraisal of pond-net samples for biological monitoring of lotic macro-invertebrates. Water Research, 15(6), 679-689.\n\n(2) Baird, D. J., & Hajibabaei, M. (2012). Biomonitoring 2.0: a new paradigm in ecosystem assessment made possible by next\u2010generation DNA sequencing.\n\n(3) Nichols, S. J., Kefford, B. J., Campbell, C. D., Bylemans, J., Chandler, E., Bray, J. P., ... & Furlan, E. M. (2020). Towards routine DNA metabarcoding of macroinvertebrates using bulk samples for freshwater bioassessment: Effects of debris and storage conditions on the recovery of target taxa. Freshwater Biology, 65(4), 607-620.\n\n(4) Stein, E. D., White, B. P., Mazor, R. D., Jackson, J. K., Battle, J. M., Miller, P. E., ... & Sweeney, B. W. (2014). Does DNA barcoding improve performance of traditional stream bioassessment metrics?. Freshwater Science, 33(1), 302-311.\n\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau."
                    },
                    "sha1": "acz6cm5bnhm1r027j2jzw7g1ynxv382"
                }
            },
            {
                "title": "Main Page",
                "ns": "0",
                "id": "1",
                "revision": {
                    "id": "6489",
                    "parentid": "6470",
                    "timestamp": "2021-12-06T16:31:55Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2332",
                        "#text": "__NOTITLE__\n== Welcome to the ''Sustainability Methods Wiki!'' ==\nThe aim of this Wiki is to present and explain fundamental methods, terms and tools relevant to (Sustainability) Science and discuss underlying questions. The Wiki is composed of several sub-wikis:\n\n{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = [[Courses]] | class = center |\n|content =\n<nowiki></nowiki>\nThis section revolves around '''curated selections of Wiki entries (and more)''' as introductions to specific topics.\n}}\n\n{{InfoCard\n|heading = [[Methods]] | class = center |\n|content =\n<nowiki></nowiki>\nMethods are at the heart of scientific research. '''Learn about the most important methods in Sustainability Science - and beyond.'''\n}}\n\n\n}}\n\n{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = [[Skills & Tools]] | class = center |\n|content =\n<nowiki></nowiki>\nEvery type of work can be facilitated through appropriate Skills & Tools. '''Dive in and learn something new!'''\n}}\n\n{{InfoCard\n|heading = [[Normativity of Methods]] | class = center |\n|content =\n<nowiki></nowiki>\nThe choice of methods influences the knowledge we produce. '''Here you can learn more about this relation.'''\n}}\n\n}}\n\n== First time visitor? ==\n'''Please watch this short introduction to the Wiki.'''\n{{#evu:https://www.youtube.com/watch?v=MjlJTjzLg6M\n|alignment=center\n}}\n<br/>\n* Have a look at the '''[[Sustainability_Methods:About|About]] page.''' Here, you will find more information on what the Wiki is all about. It also contains a FAQ section with answers to some general questions concerning the Wiki, and a contact mail in case you would like to provide feedback or ask further questions.\n\n* Now you can get started! '''Pick a section above''' and choose one or more entries to learn about. <br/>\n\n* If you do not know which methods to start with, have a look at the '''[https://sustainabilitymethods.org/method_recommendation_tool Method Recommendation Tool]'''!\n\n* You can also generate a random entry on top of the page or read one of the 5 newest Wiki entries:\n{{Special:NewestPages/-/5}}\n\n__NOTOC__"
                    },
                    "sha1": "0y1mjqej33ggdfakkk6ynntcbuoj2v9"
                }
            },
            {
                "title": "Making a research design",
                "ns": "0",
                "id": "995",
                "revision": {
                    "id": "7137",
                    "parentid": "7129",
                    "timestamp": "2023-05-02T13:43:37Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "comment": "/* Methodological designs as a critical part of reality */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14117",
                        "#text": "'''In short:''' This entry revolves around the basics of a methodological design when planning empirical research.\n__TOC__<br/>\n\nA methodological design is one of the three central stages of any scientific endeavor that builds on a methodological approach. While analysis and interpretation are conducted after the empirical data has been gathered, designing a study is what hardwires the type of knowledge that is being gathered, and thus creates a path dependency on the overall scientific output, for better or worse. It is next to impossible to reduce the essence of all the diversity of different approaches to methodological designs into a text, yet there are some common challenges scientists face, and also mistakes that are made frequently to this day. Hence nothing written here can serve as one size fits all solution, yet it may serve as a starting point or safeguarding in order to get most empirical research on track. Let us start with the basics.\n\n== No researcher is an island ==\nAny given research builds on what happened so far. In order to create a tangible and tested design, you need to read the literature within the specific branch of science. Read the highly cited papers, and the ones that match your specific context most precisely, and build on a ration from there. While many people start with textbooks, this should only be done by absolute beginners. Methodological textbooks are usually too static and constructed to do justice to the specific context of your research, yet they may be a good starting point. The examples in textbooks are however often rather lifeless and hard to connect to what you actually want to do. This is why you need to read papers that connect more clearly with your specific research project. If you believe that no one focussed on this specific part of knowledge, then you are probably mistaken. Other researchers may not have worked within your specific location, but they may have worked within systems that are comparable regarding the patterns and dynamics you want to investigate. Context matters. Try to find research that is comparable in the respective context, or at least embeds your context into their approaches and results. This is a craft that takes time to master. Beginners in the craft of science usually need quite some time to read papers, which is mainly because everything is new and they read all of it. With time, knowledge becomes familiar and you will learn to read papers faster. However you can only create a proper scientific design if you build on previous literature. If other researchers point out that your research is flawed because you ignored or missed previous research, then all is in vain. This brings us to the next point.\n\n== Stand on the shoulders of giants ==\nIt is perfectly normal if you do not know how to approach the design of a complex research design. There are however several workarounds. The most simple one is to start with a simple research design. Not all research needs to be complex or complicated, and indeed many questions that we have towards new knowledge are based on simple methodological designs. Hence it can be more than enough to start with a simple and tested research design that you find well established within science. Take the example of interviews: We can probably agree that science has not asked enough people yet in order to integrate the knowledge they have. There are exiting opportunities out there to ask actors and stakeholders about their knowledge and experience. <br>\nThe next workaround is obvious: Ask an expert. The most important advise to this end is to be well prepared, honor the time of the expert, and trust in their opinion. People with expertise in research mounted usually thousands or hour of experience, and are knowledge brokers that are quite in demand. Hence, make sure to be to the point and also be aware that you may have to read up on what the researcher pinpoints you at. Yet asking experienced researchers only makes sense if your design is advanced enough to merit the input of an experienced researcher. If such an expert explains to you a robust yet simple approach, go for it. Such a talk may not take longer than 10 minutes, hence do not expect two hours of your time if the matter is solved with simple solutions. Be trustful, remember that they usually had all the troubles you faced already, if not worse. This brings us to the next point.\n\n== Designs are safeguards ==\nWe create scientific designs in order to make sure that our gathering of data ultimately works and produces the knowledge we potentially want to analyze and interpret. Hence, we create our design based on past experience of research and researchers. We need to be realistic concerning the potential problems we may face. There are often aspects within research that are prone to errors or unforeseen circumstances and a good research design creates a certain failsafe and guard-rails against unforeseen circumstances. A low rollback in an interview campaign or moulded samples in a soil chemistry lab are a known example where your research faces challenges that a good design should fortify you against. Thus, a research design is not only about what can go right and how it is done, but also about what could go wrong and how do you deal with unforeseen circumstances. This is nothing to obsess about, but indeed something that we should simply implement in order to be prepared and ready if all else fails. Research is often failure, and then we pick ourselves up. Yet there are also more concrete safeguards.\n\n== Knowledge saturation and process understanding ==\nWhen you have enough information on a given empirical phenomenon and more sampling does not add more knowledge, one can speak of saturation. We have to be aware that from a philosophy of science standpoint, this type of knowledge saturation is impossible, because nothing is permanent and just like this, new knowledge may emerge and replace old knowledge. Yet, for a snapshot in time, within a given system, and considering that maybe not all systems change dramatically within one and the same day, or the time frame of our research, saturation is a helpful concept. If you kept for example interviewing people, yet did not gain more knowledge since several interviews, then you probably reached saturation. Within quantitative science this is easier to achieve, which is why within qualitative branches of science saturation is also a question of experience. However, within a research design process, saturation is useful because it allows you to plan your maximum sample, yet of course then one has to consider also the absolute minimum that is necessary.\n\n== Sample effort ==\nAn important factor in any given scientific research design is the sample size in relation to the effort and resource put into the consequential research. To this end, one has to be careful not to waste resources due to sampling or analysis that do not add the knowledge we are looking for. Anticipating the resources needed for a certain research project can also simply boil down to such a simple factor as time. Will we be able to conduct qualitative interviews with thousands of people? Probably not, at least not if you do not have a massive number of interviewers at your disposal. Hence, a sampling is always a fine balance between anticipated knowledge and available resources. Yet, for most research methods there is ample experience not only how to sample, but also how much to sample.\n\n== Induction or deduction ==\nWithin the deductive science, the experience concerning sample intensity is often rather clear, because new research always adds a piece of the puzzle to already existing knowledge. Since deductive approaches test a specific hypothesis and are usually designed to be reproducible, such research is clearly more easy to plan. What sounds like a benefit is likewise also the biggest criticism, at least from a stance of philosophy of science. What seems robust can also be static, and instead of adding knowledge is merely the same knowledge added, or maybe becoming more precise. It is beyond this text to discuss this criticism in depth, yet this underlines that part of any deductive design has always to be a discussion of the biases and limitations. This is equally true for inductive research, yet at a different scale and focus. Inductive designs are more about modes and conduct than about sample size and variance. An example is the setting and preparation of a qualitative interview, including ethical guidelines and clear guidelines concerning bias. Inductive research is less concerned with being reproducible which makes documentation a key issue. Especially within qualitative research an extensive documentation is key in order to enable that the path from raw data to interpreted knowledge is understandable. Since inductive research is less static, a clear documentation can also contain information that is not clearly relevant from the get-go, which may especially be important if other researchers re-examine inductive research later on.\n\n== Pre-plan the analysis ==\nYet both inductive and deductive researchers should already be aware of the initial design of the sampling and how the research is going to proceed into analysis, this is also were the [[Framing a research question|framing of a research question]] becomes important. Most methodological approaches used in analysis -both concerning quantitative and qualitative approaches- have been used hundreds if not thousand of times. There is research that can pinpoint you on how to plan your analysis. Again, it is central to strike a balance between a critical perspective and knowledge that is well established. Mayring is a typical example of a methodological approach that has proven its worth concerning myriads of analysis, yet is also notoriously underestimated concerning the necessary experience a researcher needs. Reading up on studies applying Mayring is a good starting point, yet it does not stop there, since you additionally should read up the major criticism that exists concerning Mayring. Otherwise you will not be able to defend your research against known problems that could also affect your specific approach and context. Another example would be an ecological green house experiment. In such a case a respective sample and manipulation design automatically translates into the respective statistical analysis. This leads so far that in quantitative deductive research such as in much of psychology, medicine or ecology the sample design is completely determined by the statistical analysis, and vice versa. Such methods are well established and, for the respective knowledge they produce, have stood the test of time.\n\n== For bold topics, be conservative, yet test bold methods with established topics ==\nConservative methods can be indeed helpful for the case of of bold topics, because bold topics are often deeply contested and not yet deeply investigated. Hence, a robust method can help to add credibility and robustness to an overall research design. However some research also advances methods and develops these further. In these cases tested topics can help to maintain an element of stability if a method is developed or developed further. Hence the methodological design within scientific research strongly depends on the topic as well as the underlying theories or concepts. To this end, it is clear that the interplay of these three components is a matter of deep experience, which is why it is so important to not only rely on the literature, but also build on the expertise of experienced researchers. Therefore, early career scientists may consider to build on more established procedures, while it does not come as a surprise that most scientific innovations are created by already emerging or established researchers. While one would wonder if more bold ideas emerge at a younger age and evolvements based on deeper experience originate later in life, this remains to be tested. Still, command of the literature is vital in order to move a specific branch of science forward or even evolve a new one.\n\n== Methodological designs as a critical part of reality ==\nKnowledge of the current state of the art is a precondition in order to move a specific part of science forward. Empirical reality presents an imperfect reality to us that we will probably never fully understand from an epistemological stance. Consequently, research adds pieces to a puzzle, yet we have to be aware that we may never see a full-blown epistemological reality as a whole. Reality is messy, imperfect and deceives us. Consequently, a critical perspective is a vital prerequisite for any given empirical researcher. This in turn demands a clear recognition of the literature, both concerning the empirical approaches that were being utilized, but also how the respective knowledge is limited in a broader stance. A critical perspective can hence range from simple and known biases towards broader questions of philosophy of science. Usually more established branches and procedures in science are more tamed when it comes to biases, while more emerging areas in science often contain new challenges. Interactions with stakeholders are a good example of research that is hard to tame in a rigid methodological design, and right now we rely heavily on this horizon evolving in order to gain the knowledge necessary for deep transformations. Our epistemological research being pieces of the puzzle means also that our research is a small contribution, and we should never forget this. Our piece of the puzzle will hardly be a big contribution, because after all, what is this? Yet sometimes, in rare occasions, when you have a piece of the puzzle in your hand, you know exactly where it fits, and it snugs right into the empty space connecting all the surrounding pieces. Just as one gains from practice and experience in a puzzle, one can equally gain from experience in research designs.\n\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.\n\n[[Category:Normativity_of_Methods]]"
                    },
                    "sha1": "eezx2b4wiue1k27c1cv0qcuhw0ilghi"
                }
            },
            {
                "title": "Markdown",
                "ns": "0",
                "id": "988",
                "revision": {
                    "id": "6942",
                    "parentid": "6857",
                    "timestamp": "2023-03-02T07:44:28Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article provides an introduction to Markdown, which allows to create standardized formatted text many platforms support.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4281",
                        "#text": "== What is Markdown? ==\nMarkdown is an easy-to-use markup language that is used with plain text to add formatting elements (headings, bulleted lists, URLs) to plain text without the use of a formal text editor or the use of HTML tags.\n\n=== Why Markdown? ===\n* Can be used for everything (websites, documents, notes, books, presentations, email messages, and technical documentation).\n* Portability: Files containing Markdown-formatted text can be opened using virtually any application.\n* Platform independent: You can create Markdown-formatted text on any device running any operating system.\n* Future proof: You\u2019ll always be able to read Markdown-formatted text using a text editing application.\n* It is everywhere: Websites like Reddit and GitHub support Markdown, and lots of desktop and web-based applications support it.\n\n=== How does Markdown work? ===\n1. Create a Markdown file using a text editor or a dedicated Markdown application. The file should have an .md or .markdown extension.\n\n2. Open the Markdown file in a Markdown application.\n\n3. Use the Markdown application to convert the Markdown file to an HTML document.\n\n4. View the HTML file in a web brower or use the markdown application to convert it to another file format, like PDF.\n\n\n=== Pros & Cons ===\n* It simple, fast and easy to learn which made it very popular\n* All features of HTML can be used in Markdown and it is more readable than HTML\n* Markdown is not able to map different element types to each other, so it is less useful as a semantic tool\n* Creation of table of contents, reusing content, mixing parts together and managing larger documents are not possible\n\nIn the following, the basics will be presented. You can find more [https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax here]\n\n== Formating examples ==\n=== Headings ===\nCreate a hierarchically nested system in your text, and consider balance to this end. Most larger headings should contain smaller headings, yet not too many of these. Find a balance.\n<syntaxhighlight lang=\"R\" line>\n`# The largest heading`\n`## The second largest heading`\n`##### The smallest heading`\n</syntaxhighlight>\n\n=== Text styles ===\n<syntaxhighlight lang=\"R\" line>\n##### *Styling text*:\n`**This is bold text**`\n`*This text is italicized* or _This text is italicized_`\n`**This text ist bold and _partly italicized_**`\n`***The entire text is bold and italicized***`\n\n**Like this bold text**\n</syntaxhighlight>\n\n=== Quoting ===\n<syntaxhighlight lang=\"P\" line>\n##### *Quoting*:\n\n`> Text is a quote`\n> Like this  quote\n`Use backticks (``) to code quote `\n```\n\\```  without backslash (\\)\nThis is a \ncode quote block\n\\````\n```\nA code block like this\n```\n</syntaxhighlight>\n\n=== Links ===\n<syntaxhighlight lang=\"P\" line>\n##### *Links*:\n`This normal text includes the website [website text](https://docs.github.com)`\n\n##### *Relative Links*:\n`We can link a relative to the current file by [text](path/file.md)`\n\n##### *Images*:\n`We can display an image using ![image text](image_link)`\n</syntaxhighlight>\n\n=== Lists ===\n<syntaxhighlight lang=\"P\" line>\n##### *List*:\n```\n[A link Like this to run Markdown ](https://stackedit.io/app#)\n![test imge](https://picsum.photos/200/300)\n```markdown\n- George Washington\n- John Adams\n- Thomas Jefferson\n```\n- We can also created\n- unordered lists\n1. or create \n2. ordered lists\n```\n\n##### *Nested Lists*:\n```\n1. First list item\n   - either (-) or (*) needs to be under the first character of the previous item\n     * this would be the third nested list item\n```\n</syntaxhighlight>\n\n=== Mentions and footnotes ===\n<syntaxhighlight lang=\"P\" line>\n##### *Mentioning people and Teams*:\n`@name Do you understand how it works?`\n\n```\n@https://github.com/teslamotors\n\n##### *Footnotes*:\n```\nSimple footnote[^1]\nFootnote with several lines[^2]\nIt is also possible to use words[^note]\n\n[^1]: First reference\n[^2]: Second reference\n   with multiple lines\n\n[^note]: words are still converted to numbers but makes it more readable for you as you edit.\n```\n</syntaxhighlight>\n\n\n\n----\n[[Category:Statistics]]\n[[Category:Markdown]]\n[[Category:R]]\n[[Category:Python]]\n\nThe [[Table_of_Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "a93t8hw924toz5rjpuk2v1y8x648lzj"
                }
            },
            {
                "title": "Mathematical Functions in Python",
                "ns": "0",
                "id": "1012",
                "revision": {
                    "id": "7260",
                    "parentid": "7250",
                    "timestamp": "2023-06-30T05:45:48Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an overview over mathematical functions in Pyhton with examples.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6483",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n== Introduction ==\nPython has a built-in module math which defines various mathematical functions. In addition to the math module, there is a fundamental package for scientific computing in Python named NumPy (Numerical Python). It is an open-source Python library and contains multidimensional array and matrix data structures. In this section, examples of both methods will be presented.\n\nFor using math, we must first import this module:\n\n<syntaxhighlight lang=\"Python\" line>\nimport math\n</syntaxhighlight>\n\nFor using `NumPy`, we must first install it. There is no prerequisite for installing NumPy except Python itself. We can use `pip` or `conda` for this purpose:\n\n'pip'\n\n<syntaxhighlight lang=\"Python\" line>\npip install numpy\n</syntaxhighlight>\n\n'conda'\n\n<syntaxhighlight lang=\"Python\" line>\nconda install numpy\n</syntaxhighlight>\n\nWe must import `numpy` to access it and its functions. We also shorten the imported name to `np` for better readability of code using NumPy:\n\n<syntaxhighlight lang=\"Python\" line>\nimport numpy as np\n</syntaxhighlight>\n\n== Mathematical Functions ==\n=== Basic Functions ===\nSome basic functions for my fellow students. Some functions need the module `math`. Please check out the introduction at the top. :)\n\n{| class=\"wikitable\"\n|-\n! math !! Description\n|-\n| math.ceil(x) || Rounds the number x up to the next integer\n|-\n| math.floor(x) || Rounds the number x down to the next integer\n|-\n| math.com (n,k) || Binomial Coefficient: number of possible k choose n without order\n|-\n| math,factorial(n) || Returns n factorial if n >= 0\n|-\n| abs(x) || Returns the absolute value of x\n|}\n\n<syntaxhighlight lang=\"Python\" line>\nmath.ceil(5.3)\n\n6\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nmath.floor(173.123)\n\n173\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nmath.factorial(4)\n\n24\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nabs(-17.2)\n\n17.2\n</syntaxhighlight>\n\n=== Power Functions ===\n\n{| class=\"wikitable\"\n|-\n! built-in function !! math !! numpy !! Description\n|-\n| pow(x, y, mod) || math.pow(x, y, mod) || np.power(x1, x2,...)|| x to the power of y. x1, x2 array_like\n|}\n\n''Examples''\n\n<syntaxhighlight lang=\"Python\" line>\npow (2,2)=4\n##Square\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\npow (2,3)=8\n\n##=Cube\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\npow (3,4, mode: 10)\n\nThe value of (3**4) % 10 is = 1\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n##The exponents of two 1-D arrays\n\nar1 = [3, 5, 7, 2, 4]\n\nar2 = [6, 2, 5, 3, 5]\n\narr = np.power(ar1,ar2)\n\narr: array([  729,    25, 16807,     8,  1024], dtype=int32)\n\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n##The power values of a 2-D array\n\nar1 = np.array([[3,4,3],[6,7,5]])\n\nar2 =np.array([[4,2,7],[4,2,1]])\n\narr = np.power(ar1,ar2)\n\narr: array([[  81,   16, 2187],\n\n       [1296,   49,    5]], dtype=int32)\n\n</syntaxhighlight>\n=== Root Functions ===\n\nTo implement root functions in python we can use the built-in power function. Alternatively, we can use 'math' or 'numpy'.\n\n{| class=\"wikitable\"\n|-\n! built-in power function !! math !! numpy !! Description\n|-\n| x**(1/2) || math.sqrt(x) || np.sqrt(x) || Returns the square root of x\n|-\n| x**(1/3)|| math.powe(x, 1/3) || np.cbrt(x) || Returns the cube root of x\n|-\n| x**(1/n) || math.pow(x, 1/n)|| np.power(x, 1/n) || Returns the nth root of x\n|}\n\n''Examples''\n\n* 'built-in function'\n\n<syntaxhighlight lang=\"Python\" line>\n0** (1/2)\n\n0.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n9** (1/3)\n\n2.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n120** (1/10)\n\n1.6140542384620635\n</syntaxhighlight>\n\n* 'math'\n<syntaxhighlight lang=\"Python\" line>\nmath.sqrt(0)\n\n0.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nmath.pow(9, 1/3)\n\n2.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nmath.pow(120, 1/10)\n\n1.6140542384620635\n</syntaxhighlight>\n\n* 'numpy'\n\n<syntaxhighlight lang=\"Python\" line>\nnp.sqrt(0)\n\n0.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nnp.cbrt(9)\n\n2.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nnp.power(120, 1/10)\n\n1.6140542384620635\n</syntaxhighlight>\n\n=== Exponential Functions ===\n{| class=\"wikitable\"\n|-\n! math !! numpy !! Description\n|-\n| math.exp(x) || np.exp() || The math.exp() method returns E raised to the power of x (Ex).\u2018E\u2019 is the base of the natural system of logarithms and x is the number passed to it, return value: Float \n|}\n\n''Examples''\n\n<syntaxhighlight lang=\"Python\" line>\nmath.exp(66) or np.exp(66)\n\n4.607186634331292e+28\n</syntaxhighlight>\n\nDELETE?\n<syntaxhighlight lang=\"Python\" line>\nmath.exp(66) or np.exp(66)\n\n214643579785916.06\n</syntaxhighlight>\n\n=== Log Functions ===\n{| class=\"wikitable\"\n|-\n! math !! numpy !! Description\n|-\n| math.log(x, base) || np.log(x) || The natural logarithm log is the inverse of the exponential function. The math.log() method returns the natural logarithm of a number, or the logarithm of number to base. Base value is optional and the default is e.\n|}\n\n''Examples''\n\n<syntaxhighlight lang=\"Python\" line>\n# get the log value of an array with base 2\n\narr = np.array([1, 4, 6])\n\narr1 = np.log2(arr)\n\nprint(arr1)\n\n\u200b\n\n# Output :\n\n# [0.        2.        2.5849625]\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n# get the log value of an array with base 10\n\narr = np.array([1, 4, 6])\n\narr1 = np.log10(arr)\n\nprint(arr1)\n\n\u200b\n\n# Output :\n\n# [0.         0.60205999 0.77815125]\n</syntaxhighlight>\n\n=== Trigonometric Functions ===\n\n{| class=\"wikitable\"\n|-\n! math!! numpy !! Description\n|-\n| math.cos(x) || np.cos(x) || Return the cosine of x radians. \n|-\n| math.sin(x) || np.sin(x)|| Return the sine of x radians. \n|-\n| math.tan(x)|| np.tan(x) || Return the tangent of x radians.\n|}\n\n''Examples''\n\nWe use math.pi or np.pi methods for defining pi(\u03c0) in Python.\n* math\n\n<syntaxhighlight lang=\"Python\" line>\nmath.cos(math.pi)\n\n-1.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nmath.sin(math.pi/2)\n\n1.0\n</syntaxhighlight>\n\n* numpy\n<syntaxhighlight lang=\"Python\" line>\nnp.cos(math.pi)\n\n-1.0\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nnp.sin(math.pi/2)\n\n1.0\n</syntaxhighlight>\n\n== References ==\n1.https://docs.python.org/3/library/math.html\n\n2.https://numpy.org/doc/stable/reference/routines.math.html#extrema-finding\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "gf4ebd04y24vk8yrqir4n6urtb6f08d"
                }
            },
            {
                "title": "Mathematics of the t-Test",
                "ns": "0",
                "id": "960",
                "revision": {
                    "id": "6612",
                    "timestamp": "2022-03-24T16:01:50Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "comment": "Created page with \"'''In short:''' The (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution...\"",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5477",
                        "#text": "'''In short:''' The (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on the mathematics behind T-tests and covers one-sample t-tests and two-sample t-tests, including independent samples and paired samples. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]]. For more information on t-testing in R, please refer to this [[T-Test|entry]].\n\n__TOC__\n\n==t-Distribution==\nThe (Student\u2019s) t-distributions ares a family of continuous distributions. The distribution describes how the means of small samples of a normal distribution are distributed around the distributions true mean. The locations ''x'' of the means of samples with size n and ''\u03bd = n\u22121'' degrees of freedom are distributed according to the following probability distribution function:\n[[File:prbdistribution.png|700px|frameless|center]]\nThe gamma function:\n[[File:prbdistribution1.png|700px|frameless|center]]\nFor integer values:\n[[File:prbdistribution2.png|700px|frameless|center]]\nThe t-distribution is symmetric and approximates the normal distribution for large sample sizes.\n\n==t-test==\nTo compare the mean of a distribution with another distributions mean or an arbitrary value \u03bc, a t-test can be used. Depending on the kind of t-test to be conducted, a different t-statistic has to be used. The t-statistic is a random variable which is distributed according to the t-distribution, from which rejection intervals can be constructed, to be used for hypothesis testing.\n[[File:prbdst3.png|450px|thumb|center|Fig.1: The probability density function of the t-distribution for 9 degrees of freedom. In blue, the 5%, two-tailed rejection region is marked.]]\n\n==One-sample t-test==\nWhen trying to determine whether the mean of a sample of ''n'' data points with values ''x<sub>i</sub>'' deviates significantly from a specified value ''\u03bc'', a one-sample t-test can be used. For a sample drawn from a standard normal distribution with mean ''\u03bc'', the t-statistic t can be constructed as a random variable in the following way:\n[[File:prbdst4.png|700px|frameless|center]]\nThe numerator of this fraction is given as the difference between \u2002''x'', the measured mean of the sample,\nand the theorized mean value ''\u03bc.''\n[[File:prbdst5.png|700px|frameless|center]]\nThe denominator is calculated as the fraction of the samples standard deviation ''\u03c3'' and the square-root of the samples size ''n''. The samples standard deviation is calculated as follows:\n[[File:prbdst6.png|700px|frameless|center]]\nThe t statistic is distributed according to a students t distribution. This can be used to construct confidence intervals for one or two-tailed hypothesis tests.\n\n==Two-sample t-test==\nWhen wanting to find out whether the means of two samples of a distribution are deviating significantly. If the two samples are independent from each other, an independent two-sample t-test has to be used. If the samples are dependent, which means that the values being tested stem from the same samples or that the two samples are paired, a paired t-test can be used.\n===Independent Samples===\nFor independent samples with similar variances (a maximum ratio of 2), the t-statistic is calculated in the following way:\n[[File:prbdst7.png|700px|frameless|center]]\nwith the estimated pooled standard deviation\n[[File:prbdst8.png|700px|frameless|center]]\nIn accordance with the One-sample t-test, the sample sizes, means and standard deviations of the samples 1 and 2 are denoted by ''n<sub>1/2</sub>'', \u2002''x<sub>1/2</sub>'' and ''\u03c3<sub>x<sub>1/2</sub></sub>'' respectively.\nThe degrees of freedom which are required for conducting the hypothesis testing is given as ''\u03bd = n<sub>1</sub> + n<sub>2</sub> \u2212 2''.\nFor samples with unequal variances, meaning that one sample variance is more than twice as big as the other, Welch\u2019s t-test has to be used, leading to a different t-statistic t and different degrees of freedom ''\u03bd'':\n[[File:prbdst9.png|700px|frameless|center]]\nAn approximation for the degrees of freedom can be calculated using the Welch-Satterthwaite equation:\n[[File:prbdst10.png|700px|frameless|center]]\nIt can be easily shown, that the t-statistic simplifies for equal sample sizes:\n[[File:prbdst11.png|700px|frameless|center]]\n\n===Paired Samples===\nWhen testing whether the means of two paired samples are differing significantly, the t-statistic consists of variables that differ from the ones used in previous tests:\n[[File:prbdst12.png|700px|frameless|center]]\nInstead of the independent means and standard deviations of the samples, new variables are used, that depend on the differences between the variable pairs. ''x<sub>d</sub>'' is given as the average of the differences of the sample pairs and ''\u03c3<sub>D</sub>'' denotes the corresponding standard deviation. The value of ''\u03bc<sub>0</sub>'' is set to zero to test whether the mean of the differences takes on a significant value.\n\n----\n[[Category:Statistics]]\n[[Category:Methods]]\n[[Category:Quantitative]]\n\nThe [[Table_of_Contributors| authors]] of this entry is Moritz Wohlstein."
                    },
                    "sha1": "acd5cr07i9p3xlfdng3o24czrsd09ji"
                }
            },
            {
                "title": "Meta-Analysis",
                "ns": "0",
                "id": "580",
                "revision": {
                    "id": "6309",
                    "parentid": "6288",
                    "timestamp": "2021-09-01T07:01:14Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "12398",
                        "#text": "[[File:Concept Meta-Analysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Meta-Analysis]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:'''In a Meta-Analysis, statistically comparable results from a range of scientific studies are integratively summarized to gain aggregated, quantitative insights into scientific knowledge on a specific issue.\n\n== Background ==\n[[File:SCOPUS results Meta-Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Meta-Analysis until 2019.''' Search term: 'meta-analysis' in Title, Abstract, Keywords. Source: own.]]\nStatistician Karl Pearson had a strong focus on openly integrating knowledge into overarching results, and in this spirit he created one of the first systematic Meta-Analyses by integrating results from several comparable studies on typhoid into one of the first Meta-Analyses in 1904 (1, 2). It was more than 35 years later that within psychology the next relevant Meta-Analysis was published: the book-length \"Extrasensory Perception After Sixty Years\", authored by Pratt et al. in 1940, summarizing experimental results from 1882 to 1939 (1). Despite some few studies being compiled over the next decades (most based on clinical research), more sophisticated statistical analysis tools emerging after WW2 ultimately sealed the breakthrough of the method. In the 1970s, Gene V. Glass - together with many other statisticians - developed what he called the 'analysis of the analyses' (2). By integrating statistical results into a meta-analytical frame, it became possible to combine results from the at the time already rising number of publications. Another key development was the availability of modern computers, which allowed for the conducting pf the more sophisticated analyses, along with more studies becoming available online with the rise of the Internet. Meta-analysis thus represents a method where the demand for knowledge of a different order became possible because the general data basis increased, and advanced analyses were made possible by developments in statistics and the wider availability of technology. Today, the method is widely established within all branches of normal science, and generates knowledge way beyond individual studies, yet new challenges emerge concerning which studies can be integrated, and how.\n\n== What the method does ==\nRegarding methodological design criteria, it is important to note that meta-analyses are deductive, as they try to integrate and test results from designed studies. While these results are clearly, quantitative, it is important to notice that there is a trend of so called qualitative meta-analysis, which we will not include here. The spatial scale depends on the scale of the studies that are integrated here. While hence a meta-analysis can be global in its focus, more often than not do the studies that a meta-analysis is based on an individual scale, such as in medicine and psychology. It is a matter of debate whether meta-analyses are a snapshot of the current state of the art, or a look into the past. To this end, it is important to recognize that the development or changes over time can be implemented into the analysis, and thus be a relevant part of the hypothesis.\n\n==== How it works ====\nThe Meta-Analysis is a well established method that revolves around the systematic identification of relevant studies, integration into a meta-analytical design, and interpretation based on established norms. This process is most established within medical research and psychological science, with extensive protocols being available, while in other scientific disciplines the procedures are more diverse. Meta-Analyses most commonly take place in a [[Systematic Literature Review]], allowing for a quantitative analysis of the results of the gathered publications. The Meta-Analysis is thus not the same as a Systematic Literature Review, but the quantitative process of integrating data within such a process. \n\nIn a strict sense, a Meta-Analysis summarizes statistics from scientific studies, with estimates, sample size and levels of significance as an important basis for the meta-analytical [[Glossary|framework]]. Meta-Analyses are thus able to correct for different sample sizes from the respective studies. Equally, other information can be included, such as different sub-groups or random factors. For instance, a meta-analytical scheme in medicine is able to differentiate between studies that included children, adults, or both. Through such random factors, a Meta-Analysis can hence include and integrate information that was not analyzed in the original studies, but that only emerges from the wider analysis of multiple studies. Typical models for such an analysis are [[Mixed Effect Models]], and [[Statistics|many advanced statistical software solutions (R, SPSS)]] are able to conduct a Meta-Analysis. Another important staple of Meta-Analyses are summarizing tables or figures that give an overview of the diversity of studies that were included, as well as both overall effects and mean effects for sub-groups.\n\n== Strengths & Challenges ==\nMeta-Analyses are at least as strong (and weak) as the multiple studies that they are based on. Great care needs to be taken when identifying studies that are suitable for a meta-analytical approach, since all studies should be as comparable as possible in terms of their study design. In other words, unexplained variance between studies because of factors that are not taken into account should be avoided.\n\nThe analysis of data within a Meta-Analysis demands a clear utilization of both the available data and the usage of a proper statistical approach. Many scientific disciplines have established standards that are robust, but can also be rigid. In the long run, a greater exchange of knowledge on the available approaches would be helpful.\n\nWithin rigid [[:Category:Deductive|deductive]] ranges of science, Meta-Analysis can serve as a framework to integrate quantitative results. However, it remains a challenge to take more qualitative aspects of research into account in these kinds of analyses. In addition, the integration of [[:Category:Inductive|inductive]] studies is often a challenge, as Meta-Analyses build on comparability of studies above all else. However, many Meta-Analyses are not rigid to this end, and there is a tendency to not build on clear hypotheses, but instead apply an inductive statistical data crunching ''post hoc'' which is subsequently translated into a pseudo-deductive pretending. Such abductive approaches are clearly a challenge and question the quality of the research in some areas of science. Meta-Analyses need to be safeguarded against scientific misconduct, as these few bad examples endanger the reputation of this method, and science in general.\n\n\n== Normativity ==\nRegarding the interpretation of the results of a Meta-Analysis, many studies are often somewhat prone to not properly indicate the extent and limitations of their results. While this is again a standard in some disciplines, other disciplines integrate studies that are not based on comparable designs, and thus create a [[Bias and Critical Thinking|Type I error]] by integrating studies that are not based on the same assumptions. It is less often that studies are excluded that should be included (Type II error), and this is also less relevant since it does not increase the error in the meta-analytical result. The main challenge of Meta-Analysis is hence that the overall procedure is not careful enough, and many Meta-Analyses literally compare apples with oranges. Since the method is considered to be relevant, allowing for the integration of knowledge, studies using Meta-Analyses are increasingly published. It remains to be seen if the knowledge that is thus created is always sound, and whether some branches of science did not overplay their hand. After all, the origins of this method lie in psychology and medicine, where study designs are often more comparable, although the reproducibility crisis psychology even calls this into question.\n\nThe prestige of a meta-analytical study is often well rooted in the long-standing career of an expert researcher, and this is probably an ideal case. If an expert in a branch of research oversees the integration of knowledge in a Meta-Analysis, then the balanced and nuanced evaluation of the results is safeguarded. However, sometimes Meta-Analyses are more opportunistic and lack expertise that is traded off for a highly cited paper. Ideally, experts on a topic and statisticians should team up to jointly create a blended piece of work that combines the long experience about a topic with the experience in the application of statistical analysis. Recognizing [[Glossary|bias]] and relevant categories in the baseline studies and taking it into account in the analysis are both of importance, as this decides more than anything about the quality of a Meta-Analysis.\n\n== Outlook ==\nOver the last decades, meta-analytical approaches protruded into diverse branches of research. Recognizing the context of individual studies and integrating them into a proper meta-analytical framework and analysis scheme is probably one of the key challenges of science in the 21st Century. The availability of more and more studies, and the development of analysis schemes in statistics call for larger and more diverse research groups that can cover all aspects needed when conducting a Meta-Analysis. Taking the context of individual studies into account is an already recognized challenge in Meta-Analysis, and with the availability of more and more studies as well as data, meta-analytical approaches are likely to grow both in terms of their extent and complexity of the analysis and interpretation in the future. \n\nAfter an early hype in many quantitative branches of science, sobering revision should try to focus on rigor both in terms of comparability of studies as well as statistical analysis. Meta-Analysis will play a pivotal role in the integration of knowledge in the future, yet if the results are flawed and the wish for a meta-analytical framework is larger than the actual data that can be included, then the research landscape will suffer in the long run. Since Meta-Analyses inform future research and thus play a vital role in terms of agenda setting of future research, the highest ethical standards need to be upheld, as the results and responsibility of a Meta-Analysis can emerge way beyond the sum of the studies that are included. Just as medical research continuously updates the meta-analytical knowledge on relevant topics, this may become a standard procedure in other areas of science as well. Establishing standards in reporting schemes, safeguarding comparability in analysis, and creating the norms that are needed for nuanced interpretations will take a long time. This highlights the importance of a higher level of research way beyond the ever-finer atomization of scientific disciplines. If future research is interconnected, Meta-Analysis can become a guiding principle for knowledge integration.\n\n== Key Publications ==\nto be added\n\n== References ==\n(1) Wikipedia. ''Meta-analysis.'' https://en.wikipedia.org/wiki/Meta-analysis <br/>\n(2) Booth, A. Sutton, A. Papaioannou, D. 2016. Systematic approaches to a successful literature review. Second Edition. SAGE Publications.\n\n== Videos ==\n[https://www.youtube.com/watch?v=H9GY49bsp9o Grimme prize winner Mai Thi Nguyen-Kim explains Meta-Analysis] in German\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "ii0bvwi54ed7xd9ji9nqkb18a5j8bi7"
                }
            },
            {
                "title": "Methods",
                "ns": "0",
                "id": "352",
                "revision": {
                    "id": "6302",
                    "parentid": "6286",
                    "timestamp": "2021-08-30T12:31:14Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3247",
                        "#text": "'''This sub-wiki deals with scientific methods.''' <br/>\n\n=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps. \n* Many specific methods can be differentiated into different schools of thinking, and many methods have finer differentiations or specifications in an often hierarchical fashion. These two factors make a fine but systematic overview of all methods an almost Herculean task, yet on a broader scale it is quite possible to gain an overview of the methodological canon of science within a few years, if you put some efforts into it. This Wiki tries to develop the baseline material for such an overview, yet can of course not replace practical application of methods and the continuous exploring of empirical studies within the scientific literature. \n\n\n=== What can you learn about methods on this Wiki? ===\n'''This Wiki describes each presented method in terms of''' \n* its historical and disciplinary background,\n* its characteristics and how the method actually works,\n* its strengths and challenges,\n* normative implications of the method,\n* the potential future and open questions for the method,\n* exemplary studies that deploy the method,\n* as well as key publications and further readings.\n\nAlso, each scientific method that is described on this Wiki is categorized according to the Wiki's underlying [[Design Criteria of Methods]].<br/>\n'''This means that each method fulfills one or more categories of each of the following criteria:'''\n<br/>\n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scales: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scales: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]\nYou can click on each category for more information and all the entries that belong to this category.\n<br/>\n\n\n=== Which methods can you learn about? ===\nSee all methods that have been described on this Wiki so far:\n<categorytree mode=\"pages\" hideroot=\"on\">Methods</categorytree>\n<br>\nWe also have what we call '''Level 2''' overview pages. <br>\nOn these pages, we present everything that is necessary for a specific field of methods in a holistic way. So far, Level 2 pages exist for:\n* '''[[Statistics]]''': Here, you will find guidance on which statistical method you should choose, help on data formats, data visualisation, and a range of R Code examples for various statistical applications.\n* '''[[Interviews]]''': Here, we help you select the proper Interview method and provide further Wiki entries on Interview methodology you should read."
                    },
                    "sha1": "pbkor280tvgmlakij9j8ukf91nhkian"
                }
            },
            {
                "title": "Methods of Environmental Sciences",
                "ns": "0",
                "id": "394",
                "revision": {
                    "id": "3877",
                    "parentid": "3824",
                    "timestamp": "2021-01-20T10:41:38Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2189",
                        "#text": "The course '''Methods of Environmental Sciences''' covers a broad cross-section of those scientific methods and approaches that are central to sustainability research as well as further Wiki entries that frame the presented methods in the light of the Wiki's conceptual perspective. \n\n__TOC__\n<br/>\n==== Definition & History of Methods ====\nWithin this lecture we deeply focus on the formation of a new arena in science that is including not only system knowledge, but also normative knowledge as well as transformative knowledge. In order to create solution for the problems we currently face, a solution orientated agenda is necessary.  This may demand the creation of novel methodological pathways to knowledge creation. Here, we give a tentative overview on the developments up until now.\n* [[History of Methods in Sustainability Science]]\n\n==== Design Criteria of Methods in Sustainability Science ====\nThere are several design criteria that allow you to systematise methods. Many of these criteria are part of the \u201cusual suspects\u201d in normal science ''sensu strictu'' Kuhn. Here, we discuss further design criteria and knowledge types that can be relevant to systematise knowledge production through methods for sustainability science.\n* [[Design Criteria of Methods in Sustainability Science]]\n\n==== [[Thought Experiments]] & [[Legal Research]] ====\n\n==== Quantitative Methods in the Humanities ====\n* [[Causality and correlation]]\n\n==== [[Geographical Information Systems]] ====\n\n==== [[Grounded Theory]] ====\n\n==== Interviews ====\n* [[Semi-structured Interview]]<br/>\n* [[Open Interview]]\n\n==== The ecological experiment ====\n* [[Experiments and Hypothesis Testing]]\n\n==== Causal Loop Diagrams ====\n* [[System Thinking & Causal Loop Diagrams]]\n\n==== Questioning the status quo in method-driven research ====\n* [[Questioning the status quo in methods]]\n\n==== [[Social Network Analysis]] ====\n\n==== Meta-Analysis ====\n* [[Meta-Analysis]]\n* [[Systematic Literature Review]]\n\n==== Mixed Methods in transdisciplinary research ====\n* [[Transdisciplinarity]]\n* [[Visioning & Backcasting]]\n* [[Scenario Planning]]\n* [[Living Labs & Real World Laboratories]]\n\n----\n[[Category: Courses]]"
                    },
                    "sha1": "i2x0xy21cfaomj286gyivzz63rm8t17"
                }
            },
            {
                "title": "Microsoft Excel",
                "ns": "0",
                "id": "977",
                "revision": {
                    "id": "6772",
                    "parentid": "6771",
                    "timestamp": "2022-09-02T12:53:24Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "comment": "/* Shortcuts */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6777",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || '''[[:Category:Software|Software]]''' || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nThis article is about using spreadsheets in general and Microsoft Excel in particular. We\u2019ll show you how to conceptualize problems and use spreadsheets so solve them, as well as a host of other things. The article mostly revolves around a video tutorial, so there\u2019s not so much to read.\n\nThe goal is to enable you to confidently be able to use spreadsheets for things from everyday tasks to work to scientific research.\n\n== Getting Started ==\nTo get started, you\u2019ll need a spreadsheet software. Here, we\u2019re using Microsoft Excel because it\u2019s the most widely used and the richest in features. If you're a member of Leuphana University, you can order a cheap Microsoft Office license here:\u00a0https://www.leuphana.de/services/miz/service-support/beschaffung/software/ms-office-365.html\n\nMost of the things we\u2019re showing are also doable in Google Sheets or LibreOffice Calc, so if you prefer those, don\u2019t worry. The software will look and feel a bit different, but the underlying ideas are the same.\n\nAs stated above, this article mainly revolves around a set of videos, which we link to below. All the content and explanations can be found there. Here, will only tell you the contents of the video and give accompanying information and resources so that you can a) choose which one is interesting to you and b) follow along (which we recommend you do, because that\u2019s the only way you\u2019ll learn).\n\n== The Videos ==\n=== Introduction ===\nThis video is an overview of the other four tutorials and discusses some caveats and hints when learning Excel. \n{{#ev:youtube|https://www.youtube.com/watch?v=N7mqi8auu88%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\n=== 1) The Flatshare Cost Calculator ===\nIn this introductory video, we\u2019re building an Excel model that allows you to calculate how to split the rent in a flatshare based on room size. The final model can be downloaded here for reference. It is rather basic, so if you already have some solid Excel knowledge, you can probably skip it entirely or just skim through it if you\u2019re interested in some particular thing. We provide the final file below for reference.\n\n[[File:Flat Share Fair Cost Calculator Final.xlsx|thumb|Flat Share Fair Cost Calculator Example File]]\n{{#ev:youtube|https://www.youtube.com/watch?v=OCbW3EiyQ0E%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\nThings that we cover:\n* Navigating in Excel\n* Basic formulas & calculations (including VLOOKUP/SVERWEIS)\n* Basic formatting\n* Custom formatting\n* Creating of basic charts (we also recommend reading our [[Introduction to statistical figures]]\n\n=== 2) Data Analysis ===\nThis tutorial is supposed to put you in a position to import a dataset as a csv into Excel and efficiently build an analysis of the data. The dataset to follow along can be downloaded below.\n\n[[File:Videogame Sales Dataset.csv|thumb|Videogame Sales Data]]\n{{#ev:youtube|https://www.youtube.com/watch?v=1N1Z7wBAVmY%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\nThings that we cover:\n* CSV Import\n* Making a table\n* Format columns\n* Filter & Sort\n* Make a pivot table\n* Make a pivot chart\n* How to play around with data\n\n=== 3) Making Decisions ===\nThis video is about building a cost-benefit analysis in Excel that might help you to make reasoned decisions. Whilst it is primarily about how to use Excel to help with real-life problems, the method itself is also explained. You may download the final Excel file below.\n\n[[File:Excel Decision Helper.xlsx|thumb|Excel Decision Helper]]\n{{#ev:youtube|https://www.youtube.com/watch?v=LOVQIqQ2IxI%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\n\nThings that we cover:\n* Using tables\n* Advanced formulas (weighted average, sum-if)\n* Conditional formattin\n* Advanced charts (point chart & stacked barchart)\n* Using data across worksheets\n\n=== 4) Collaborating in Excel ===\nIn this last video, we talk about online collaboration in Excel via Microsoft OneDrive. We\u2019ll show you how to setup everything, invite people and get going. The case we present is a common case for collaboratively tracking something, be it interview partners, tasks or anything else. We\u2019re not going to cover many Excel functions, but just show you how you can use what you might have already learned to set this up. Again, you may download the final Excel file below.\n\n[[File:Interviews Excel Collaboration.xlsx|thumb|Interviews Excel File]]\n{{#ev:youtube|https://www.youtube.com/watch?v=xW4YKNbM9UQ%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\nThings that we cover: \n* Setup\n** OneDrive\n** Create and share a table (directly, indirectly, online)\n* Data validation\n* Pivot Tables & Charts\n\n== Shortcuts ==\n\nHere's some of the most useful shortcuts in Excel!\n\n{| class=\"wikitable\"\n|-\n! Shortcut !! Keyboard Combination !! Usage\n|-\n| Add row or column or cell || CTRL + '+' || Adds a row, column or cell above or before the selected row, cell or column.\n|-\n| Remove row or column or cell || CTRL + '-' || Removes the selected row, cell or column.\n|-\n| Fill downwards/rightwards || CTRL + U, CTRL + R || Fills the first the in a selection either downwards or rightwards, i.e. puts the same value into all the selected cells.\n|-\n| Undo / Redo || CTRL + Z, CTRL + Y || If you messed up, press CTRL + Z, if you then noticed you actually didn't, press CTRL + Y.\n|-\n| Format Window || CTRL + 1 || This opens up the formatting window, which comes in handy surprisingly often, for example when you want borders (only in Excel, nowhere else).\n|-\n| Mark range of cells to copy/change || CTRL + Shift + arrow keys || Allows you to quickly, precisely and without using a mouse, select a range of cells. \n|-\n| Quickly format text || Ctrl + Shift + F (U, K) || Either boldens, underlines or italicizes your text.\n|-\n| Snap to grid while sizing and aligning elements || Hold down Alt || When you hold down alt while moving or resizing an object (such as a chart), the object will snap to Excel's gridlines, making everything look nice and orderly.\n|}\n\n== Links & Further reading ==\nOf course, our own videos cover everything you need to know ;)\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Software]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table of Contributors|author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "tgf17na11xolv7syp4w2ml7i0y50lj0"
                }
            },
            {
                "title": "Microsoft Word For Academic Writing",
                "ns": "0",
                "id": "922",
                "revision": {
                    "id": "6868",
                    "parentid": "6505",
                    "timestamp": "2022-12-17T12:51:48Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "comment": "Added the template",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17273",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || '''[[:Category:Software|Software]]''' || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nMicrosoft Word is and probably will remain the number one tool for writing, be it in academia, business, non-profits or wherever you want to look. There are cases to be made for free alternatives such as LibreOffice, and some will insist that LateX is always the better option. We will leave these considerations aside for now and concern ourselves with what for some is a good friend, and for some an arch enemy: Microsoft Word. Our goal here is to make you part of the former group and reduce the size of the latter. \n\nIf you're only here for a good template, here you go: [[File:Academic Word Template.docx|thumb]]. But be warned: You might not understand the template if you don't watch the video below!\n\n== Getting Started ==\nAs stated above, this article mainly serves the purpose to support academic writing with Word. However, most tips are probably useful in a variety of other fields, so feel free to transfer your freshly acquired knowledge!\n\nWe're going to cover a short, a medium and a larger topic. You don't need to have read any in order to read the others, so feel free to skip to the most interesting parts.\n\n1) Technological concerns: We will keep this short as this is not a tech course, but a few aspects are worthwhile considering.\n\n2) Process: This is important to save you trouble down the road. We will talk about a few things in regards to sequence of activities that make sense.\n\n3) Becoming friends with Word: This part is going to cover basics and fundamentals that we think are very useful in not starting to hate the world in general and Bill Gates in particular. We're going to go over very important DOs and also some DON'Ts. This article is not going to make you a Word Professional, but we will try to nudge you towards the things you'll want to consider or familiarize yourself with. If you are lucky enough to be part of Henrik von Wehrden's Bachelor Forum, you might get the chance to participate in a Workshop that will go over these points in practice. And:\n\n'''BIG NOTICE:'''\n\nLearning a software by reading text is usually a) boring and b) ineffective. That is why your favourite Wiki team spared neither work nor sacrifice to produce a '''Word Video Tutorial''' alongside this article. You can find it here: https://www.youtube.com/watch?v=ItXOGe7kVhY \n\n{{#ev:youtube|https://www.youtube.com/watch?v=ItXOGe7kVhY%7C1000%7Cright%7COur YouTube Tutorial|frame}}\n\nIn the video, we're building a template, based on this document: [https://docs.google.com/document/d/1b_6tf5drOX3McTLqyfbFdTcCW4eIe-aO/edit?usp=sharing&ouid=101353147119951336675&rtpof=true&sd=true Word Basic Template] (just download it via file -> download and then you should be able to work with it, or just copy it into your own Word). \n\nYou might notice that it has a whooping 59 minutes of length, which might seem daunting at first. Luckily, you don't need to watch the whole thing in one go. If you're short on time, just watch the fundamentals and maybe return for the advanced section whence you have the time. You can also make use of the time stamps in the video description to jump to your favorite parts.\n\nSmall disclaimer: This article was written with Word 2016 in mind. There might be some changes in future or previous versions of Word, but the general gist should remain the same.\n\nOkay, ready?\n\n== Technological concerns ==\n\n=== Word Versions ===\n\nIn regards to the version of Word you are using, you can usually assume that newer is better. Overall, any version is fine (maybe it should be 2010+) and won't make a crucial difference in writing. It is however noteworthy that newer versions get (security) upgrades and will interact better with other tools such as OneDrive. This does however not mean that you need to ditch your older version and pay 130\u20ac for the newest product. As long as it's working for you, don't buy into the hype!\n\nIf you're a member of Leuphana University, you can order a cheap Microsoft Office license here: https://www.leuphana.de/services/miz/service-support/beschaffung/software/ms-office-365.html\n\n=== OneDrive (or any other cloud storage really) ===\n\nOneDrive is Microsoft's cloud storage, but you can really use whichever you want - we do not recommend any particular one. The important thing is that you do USE A CLOUD STORAGE. You do not want to be part of the sad group of individuals that lost their thesis half-way through or even later due to a computer crash or some other unfortunate incident. Put your very first version into a folder in a cloud storage software and don't ever leave that comfy place. It allows you to jump back to earlier versions, automatically backs up your files and - if need be - opens up the option for collaborating on the same document. In the last point, OneDrive has an edge over other cloud storage software because it can directly access Word files and allows you to work simultaneously on the same Word document with others.\n\n=== Citavi (or any other citation software) ===\n\nWe do recommend using a citation management software. When you're working on Windows, Citavi is a good option, but you can always go with Zotero, Mendeley, or whichever one you come across and which works for you. Citavi has a pretty good Word Integration, so that's a plus. You do not need to use every function of Citavi, but at least the generation of the reference list is a really big plus in terms of saving you time and mistakes that occur from manually typing it.\n\n== Process ==\n\nAs stated above, your process in working with Word is valuable to pay attention to because it may safe you time and frustration down the road. The points below are by no means meant to give you a 100% fool-proof instruction, but are generally worthwile considering from our experience. \n\n=== Build a solid framework ===\n\nYou'll want to setup everything nice and clean in the beginning. Nothing fancy, but so that it satisfies all the formal requirements and doesn't hurt your eyes. Set up the font size and styles, line spacing, page margins, front page information and your basic structure (abstract, intro, main parts, conclusions, references and reference format, indices, glossary, ...). Check 3 times and then with your advisor if your basic setup satisfies all the formal requirements.\n\nIf you use a citation software, set that one up as well and make sure everything works to your liking.\n\n=== Write everything, don't format ===\n\nThis is the part where you basically don't want to have to care about the software you're using. Write plain text. If you have to format, **use styles** (see instructions below), and don't ever play around with picture or table positioning. Anything fancy you do here will make you despair when revising and reformatting later.\n\n=== Finalize formatting ===\n\nHave you written everything? I mean, everything? Like, there is nothing you're going to change? Okay. But have you sent it to someone you trust and let them check? Yes? Okay, you may go ahead.\n\nThis is the part where you can really finalize your formatting and make everything more fancy if you desire. If your initial setup was good, this is probably not going to take you too much time. You can alter some styles if you wish (e.g. for block quotes or subheadings) and format pictures and tables (e.g. let the text flow around them or something similar). You might also want to go over the whole document and check for odd pagebreaks (which should be a lot less likely to occur if you follow the instructions below). \n\n== Word Fundamentals ==\n\nOkay, here are the very basic things you should consider when setting up your word document. As stated before, this is not meant to tell you exactly how you can do it, but rather that you should do it. Have a look at our tutorial in order to find out how to do it exactly.\n\n=== DONT's ===\n\nLet's start with some things you never ever want to do. Like, really, never. \n\n'''First''', do not, under any circumstances, manually format any piece of text, table or picture in your document. Always, always use Word Styles (\"Formatvorlagen\"). There are many reasons why it is a good idea, but the main one is that you are guaranteed to miss some font style change that happened due to you copy/pasting stuff in and you will spend roughly 3 weeks in your document to try and spot where your font style changed. Use styles instead, and adjust those when necessary.\n\n'''Second''', do not ever use multiple line-breaks to get to the next page. You can either circumvent this with styles (see below) or at least use the CTRL+Enter shortcut to insert a pagebreak. This will prevent things from going all wonky when you change something above the page where you foolishly tried to page-break in the first place. \n\n'''Third''', do not play around with picture or table formatting before your work is finished (see \"Process\" above). If you want to insert such elements during writing, paste them, let them stay where they are in a normal line and don't try fancy stuff such as having text float around it. You will with 100% certainty change something above or below the picture and mess up the page design. Do this only once you're done writing.\n\n=== DO's ===\n\nOkay, now that we got these out of the way, let's concern ourselves with what you should do. \n\n'''Styles'''\n\nFirst, styles. Styles (or \"Formatvorlagen\" in the elegant German language) are a way of telling Word how a block of text should be formatted, and to apply that formatting to all blocks of text that share the same type of content. They usually appear to the top right of your Word document. You can think of them as instructing Word to always treat different kinds of content (e.g. normal text, headings, quotations, subheadings) differently, and once you tell word that a block of text belongs to some category, it knows how to format that block of text. This will also give you the flexibility to change the formatting of all same kinds of text in an instant throughout the whole document.\n\n'''Headings'''\n\nOne of the most important aspects of this is using **heading styles**. This allows you to automatically generate a table of contents, navigate within your document more quickly and have an array of options to better format your document. For example, you can tell Word to always insert a page break before your level 1 headings, which is saving you the trouble of either putting in a dozen line-breaks (which you should never ever do) or using CTRL+Enter (which is better but sometimes still suboptimal). If you use heading styles, you can easily insert an **automatically generated table of contents** by going to \"References\" \u2192 \"Table of Contents\" and select one of the options.\n\nYou can also use this to quickly navigate your document through using Word's **navigation bar**. Go to \"View\" (\"Ansicht\") and look for the checkbox \"Navigation bar\" (\"Navigationsbereich\"). This might seem minute but is a big comfort plus when working with larger documents like a thesis.\n\n'''Copy & Pasting'''\n\nAnother aspect comes into play when you copy in external content. Always make sure that you only **copy the text, not the formatting**. Word gives you that option when you paste something in: a little box appears next to the pasted text, where you can select to only paste the text without formatting. This will automatically format the pasted text into default style and save you the trouble of manually formatting it.\n\n'''Track Changes & Comment'''\n\nYou should definitely familiarize yourself with the inner workings of the \"track changes\" and \"comment\" functions of word. They allow you to clarify changes between versions as well as give and receive feedback in a way that is clearly intelligible and does not mess with your documents. To find these in Word, go to \"Review\" (\"\u00dcberpr\u00fcfen\"). You will find the comment button, as well as the option to activate \"track changes\". Track changes will highlight all the changes you have done while it is active, and will allow you or others to either accept or decline them.\n\n=== Tips, Tricks and Shortcuts ===\n\nThe following is just a compendium of tips & tricks of whose existence you should be aware of. Again, explaining this in text form is rather tedious and you'll be much better served to look this up in a tutorial or other context.\n\n'''Page Numbers'''\n\nPage numbers are great, you should always use them. Usually the easiest way is to go to \"Insert\" \u2192 \"Page number\" and then select where you'd like to have it.\n\nThere are some things you'll want to pay attention to. First, the \"first page\" issue. You'll usually want your (arabic numeral) page numbering to start on your first page of content. You can do this by inserting a section break (\"Abschnittswechsel\") before your first page of content. You can then, on your first page of content right click the page number in your header or footer, select \"format page numbers\" and select \"Begin with...\" and insert a 1. \n\nThere is more stuff you can do such as alternating sides or using latin numerals on the pages before your main content. This is shown in the accompanying tutorial as well.\n\n'''Headers and Footers'''\n\nWhile this is usually not necessary, it is nice for reader guidance: you can automatically insert the current chapters title into the header or footer row of your document. To do so, go into the header or footer row (by double clicking into the area), in the \"header and footer tools\" tab, go to \"Insert\" \u2192 \"Document Information\" \u2192 \"Field\" and select \"StyleRef\" under field names. In the selection menu, you can then select \"Heading 1\" (or \"\u00dcberschrift 1\") and click \"OK\" This will insert a field that automatically includes the name of the current heading in your header (or footer).\n\nGenerally, it might make sense for you to familiarize yourself with the different mechanics of headers and footers such as the \"link to previous\", \"orientation tab stop\" (\"Ausrichtungstabstopp\"), \"first page differently\" and \"alternating odd and even pages\".\n\n'''Figures and Tables'''\n\nAs alluded to above, you'll want to only format these when you finished writing. Another point is labelling them properly, which allows you to include automatic numbering and, similar to the table of contents, automatically generate lists of figures and tables.\n\nWhen you have a picture (and this works the same for tables), right-click it and select \"Add label\" (\"Beschriftung einf\u00fcgen\"). In the pop-up, you will need to select the right type (i.e. figure, table, formula) and click \"OK\". This will insert a text field below your figure which is automatically numbered and which's text you can extend in order to add a description.\n\nIf you want to create a list of figures, go to the \"References\" (\"Referenzen\") tab \u2192 \"Labels\" (\"Beschriftungen\") and click on \"Add list of figures\" (\"Abbildungsverzeichnis hinzuf\u00fcgen\"). Don't get irritated that it only says figures, it works just as well for tables. In the pop-up, you'll need to select the kind of element you want to create the list for (i.e. figures in our example) in the bottom-left. That's it, hit \"OK\" and enjoy all the work you did not have to do manually.\n\n=== Shortcuts ===\n\nOkay, here's some very useful shortcuts, descendingly ordered by subjective importance.\n\n{| class=\"wikitable\"\n|-\n! Shortcut !! Keyboard Combination !! Usage\n|-\n| Undo & Redo || CTRL + Z, CTRL + Y || The bread and butter shortcuts for everyone who messes up sometimes (which is everyone). This works in almost all software ever created by the way.\n|-\n| Copy Commands || CTRL + C, CTRL + X, CTRL + V || The C copies, the X cuts, and the V pastes. I hope you knew this already, but if you didn't, make use of it!\n|-\n| Fancy keyboard navigation || CTRL and Arrow Keys || Instead of trying to move your mouse exactly behind that word, try to get used to pushing your cursor around with the arrow keys. If you use CTRL and the arrow keys (to the \u2190 left or the right \u2192), you jump between words instead of letters. This works everywhere by the way.\n|-\n| Make heading || Alt + 1, Alt + 2, Alt + 3 || This makes the selected piece of text a level 1 heading. Can be used with 2 and 3 as well. Very useful for quickly structuring your document!\n|-\n| Show paragraphs and stuff || CTRL + Shift + * || This shortcut shows you linebreaks, tabs, pagebreaks and formatting. Very useful when you're trying to figure out why your document is a mess.\n|-\n| Page Break || CTRL + Enter || Inserts a pagebreak, saves you the trouble of inserting to many line breaks.\n|}\n\n== Links & Further reading ==\nOf course, our own video covers everything you need to know: https://www.youtube.com/watch?v=ItXOGe7kVhY\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Software]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "41ktm122mv9njz932l58pisp2m13e0m"
                }
            },
            {
                "title": "Mindfulness",
                "ns": "0",
                "id": "469",
                "revision": {
                    "id": "5964",
                    "parentid": "5315",
                    "timestamp": "2021-07-01T09:46:08Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5741",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n[[File:Noble Eightfold Path - Mindfulness.png|350px|thumb|right|'''The Noble Eightfold Path, with Mindfulness being the seventh practice.''' Source: [https://en.wikipedia.org/wiki/Noble_Eightfold_Path Wikipedia], Ian Alexander, CC BY-SA 4.0]]\nMindfulness is a [[Glossary|concept]] with diverse facets. In principle, it aims at clearing your mind to be in the here and now, independent of the normative [[Glossary|assumptions]] that typically form our train of thought. Most people that practice mindfulness have a routine and regular rhythm, and often follow one of the several schools of thinking that exist. Mindfulness has been practiced since thousands of years, already starting before the rise of Buddhism, and in the context of many diverse but often religious schools of thinking.\n\n== Goals ==\nSince the goal of mindfulness is basically having \"no mind\", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free. \n\n== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen \u2013 sitting meditation \u2013 is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|'''[https://www.headspace.com/de Headspace] is an app that can help you meditate''', which may be a way of practicing Mindfulness for you. Source: Headspace]]\n\nDuring the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.\n\nMindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around \u2013 often referred to as the monkey mind \u2013, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.\n\nIf you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions \u2013 just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!\n\n== Links & Further Reading ==\n* [https://www.headspace.com Headspace] \n* [https://www.youtube.com/watch?v=ZToicYcHIOU A YouTube-Video for a 10-Minute Mindfulness Meditation]\n* [https://thichnhathanhfoundation.org/be-mindful-in-daily-life Thich Nhat Hanh Foundation - Be Mindful in Daily Life]\n* A [https://en.wikipedia.org/wiki/Zen_Mind,_Beginner%27s_Mind Wikipedia] overview on ''Zen Mind, Beginner's Mind'' is classic introduction to Zen.\n* [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 Forbes. Science-based tricks for quieting the monkey mind.]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Katharina Kirn."
                    },
                    "sha1": "7mo9s8m306bl8wnaffopo48z0x0kka0"
                }
            },
            {
                "title": "Mindmap",
                "ns": "0",
                "id": "727",
                "revision": {
                    "id": "5961",
                    "parentid": "5667",
                    "timestamp": "2021-06-30T20:44:41Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Getting started */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4379",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan.\n\n[[File:Mindmap Example 2.jpg|600px|thumb|right|'''MindMaps can take the form of trees, with the words on the branches, or clusters/bubbles, as in this example.''' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https://www.thetutorteam.com/wp-content/uploads/2019/07/shutterstock_712786150.jpg thetutorteam.com]]]\n\n'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.\n\nThe central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). '''This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.''' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. '''Elements may be subordinate to other elements, indicating nestedness of the information.''' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.\n\n== Links & Further reading ==\n''Sources:''\n* [https://www.mindmapping.com/de/mind-map MindMapping.com - Was ist eine Mindmap?]]\n* Tony Buzan. 2006. MIND MAPPING. KICK-START YOUR CREATIVITY AND TRANSFORM YOUR LIFE. Buzan Bites. Pearson Education.\n* [http://methodenpool.uni-koeln.de/download/mindmapping.pdf Uni K\u00f6ln Methodenpool - Mind-Mapping]]\n* [https://kreativit\u00e4tstechniken.info/problem-verstehen/mindmapping/ Kreativit\u00e4tstechniken.info - Mindmapping]]\n* [https://www.lifehack.org/articles/work/how-to-mind-map-in-three-small-steps.html Lifehack - How to Mind Map to Visualize Ideas (With Mind Map Examples)]]\n* [https://www.thetutorteam.com/blog/mind-maps-how-they-can-help-your-child-achieve/ The Tutor Team. MIND MAPS: HOW THEY CAN HELP YOUR CHILD ACHIEVE]]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "ooducrm55lxnntq3dbd8pthycuf59qp"
                }
            },
            {
                "title": "Miro",
                "ns": "0",
                "id": "738",
                "revision": {
                    "id": "5158",
                    "parentid": "5157",
                    "timestamp": "2021-04-26T11:35:36Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Getting started */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2387",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nMiro is an online collaboration tool which allows you to hold virtual workshops, develop common ideas and designs, have a digital workspace or design presentations in a different way.\n\n== Goals ==\n* Work better together online\n* Integrate many different functions (project management, online collaboration, presentation) into one\n\n== Getting started ==\nMiro is an online-tools which allows you to collaborate with others (but also work by yourself). '''It is basically an infinite canvas onto which you can put all kinds of elements:''' shapes, text, videos, documents, interactive elements such as a [[Kanban]] Board or sticky notes. As collaboration happens in real-time, it can very much substitute the classical whiteboard or brown-paper and allow you to collaboratively brainstorm ideas, design a presentation, or manage your team.\n\nWe recommend using it for digital workshops, organizing your teamwork or as an alternative to traditional presentation formats such as Prezi, PowerPoint or Keynote.\n\nIt can be a bit overwhelming at first, but once you get the hang of it, it becomes really natural to use and hopefully will make your teamwork more productive and fun.\n\nTo get started, create an account on [https://miro.com the Miro website]. If you are eligible (e.g. when you're a member of Leuphana University), you can apply for a free Miro Education account which comes with all the premium features [https://miro.com/education-whiteboard/ here].\n\n== Links & Further Reading ==\nHere's a video that covers (almost) everything you need to know to get started working with Miro:\n{{#evu:https://www.youtube.com/watch?v=pULLAEmhSho\n|alignment=center\n}}\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "bykoqtahak6x4l7r8rltd5ty38uysvw"
                }
            },
            {
                "title": "Mixed Effect Models",
                "ns": "0",
                "id": "648",
                "revision": {
                    "id": "6502",
                    "parentid": "6267",
                    "timestamp": "2021-12-30T22:00:52Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* Outlook */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15739",
                        "#text": "[[File:ConceptMixedEffectModels.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Mixed Effect Models]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"|''' [[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n'''In short:''' Mixed Effect Models are stastistical approaches that contain both fixed and random effects, i.e. models that analyse linear relations while decreasing the variance of other factors.\n\n== Background ==\n[[File:Mixed Effects Model.png|400px|thumb|right|'''SCOPUS hits per year for Mixed Effects Models until 2020.''' Search terms: 'Mixed Effects Model' in Title, Abstract, Keywords. Source: own.]]\nMixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as psychology, social science, physics, and ecology.\n\n\n== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable. \n\nTypically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.\n\nOften, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models. \n\n\n== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know. \n\nTake the example of many studies in medicine that investigate how a certain drug works on people to cure a disease. To this end, you want to know the effect the drug has on the prognosis of the patients. What you do not want to know is whether people are worse off if they are older, have a lack of exercise or an unhealthy diet. All these single effects do not matter for you, because it is well known that the prognosis often gets worse with higher age, and factors such as lack of exercise and unhealthy diet choices. What you may want to know, is whether the drug works better or worse in people that have unhealthy diet choice, are older or lack regular exercise. These interaction can be meaningfully investigated by Mixed Effect Models. '''All positive factors' variance is minimised, while the effect of the drug as well as its interactions with the other factors can be tested.''' This makes Mixed Effect Models so powerful, as you can implement them in a way that allows to investigate quite complex hypotheses or questions.\n\nThe greatest disadvantage of Mixed Effect Models is the level of experience that is necessary to implement them in a meaningful way. Designing studies takes a lot of experience, and the current form of peer-review does often not allow to present the complex thinking that goes into the design of advanced studies (Paper BEF China design). There is hence a discrepancy in how people implement studies, and how other researchers can understand and emulate these approaches. Mixed Effect Models are also an example where textbook knowledge is not saturated yet, hence books are rather quickly outdated, and also often do not offer exhausting examples to real life problems researchers may face when designing studies. Medicine and psychology are offering growing resources to this end, since here the preregistration of studies due to the reproducibility crisis offers a glimpse in the design of scientific studies.\n\nThe lack of experience in how to design and conduct Mixed Effect Models-driven studies leads to the critical reality that more often than not, there are flaws in the application of the method. While this got gradually less bad over time, it is still a matter of debate whether every published study with these models does justice to the original idea. Especially around the millennium, there was almost a hype in some branches of science regarding how fancy Mixed Effect Models were considered, and not all applications were sound and necessary. Mixed Effect Models can also make the world more complicated than it is: sometimes a regression is just a regression is just a regression.\n\n==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.\n\n'''Mixed Effect Models are built on a litany of preconditions, most of which most researchers choose to conveniently ignore.''' In my experience, this is more often than not ok, because it does not matter. Mixed Effect Models are - bless the maximum likelihood estimation - very sturdy. It is hard to find a model that does not have some predictive or explanatory value, even if hardly any pre-conditions are met. Still, this does not mean that we should ignore these. In order to sleep safe and sound at night, I am almost obsessed with model inspection, checking variance across levels, looking at the residuals, and looking for gaps and flaws in the model's fit. We should be really conservative to this end, because by focusing on fixed and random variance, we potentiate things that could go wrong. As I said, more often than not, this is not the case, but I propose to be super conservative when it comes to your model outcome. In order to get there, we need yet another thing: Model simplification.\n\nMixed Effect Models lead the forefront of statistics, and this might be the reason why the implementation of AIC (Akaike Information Criterion) as a parsimony-based evaluation criterion is more abundant here when compared to other statistical approaches. P-values fell widely out of fashion in many branches of science, as did the reporting of full models. Instead, model reduction based on information criteria approaches is on the rise, reporting parsimonious models that honour [[Why_statistics_matters#Occam.27s_razor | Occam's razor]]. Starting with the maximum model, these approaches reduce the model until it is the minimum adequate model - in other words, the model that is as simple as possible, but as complex as necessary. The AIC is about the equivalent of a p-value of 0.12, depending on the sample size, hence beware that the main question may be the difference from the null model. In other words, a model that is better from the Null model, but only just so based on the AIC, may not be significant because the p-value would be around 0.12. This links to the next point: Explanatory power.\n\nThere has been some sort of a revival of r<sup>2</sup> values lately, mainly based on the suggestion of r<sup>2</sup> values that can be utilised for Mixed Effect Models. I deeply reject these approaches. First of all, Mixed Effect Models are not about how much the model explains, but whether the results are meaningfully different from the null model. I can understand that in a cancer study I would want to know how much my model may help people, hence an occasional glance of the fitted value against the original values may do no harm. However, r<sup>2</sup> in Mixed Effect Models is - to me - a step into the bad old days when we evaluated the worth of a model because of its ability to explain variance. This led to a lot of feeble discussions, of which I only mention here the debate on how good a model needs to be in terms of these values to be not bad, and vice versa. This is obviously a problem, and such normative judgements are a reason why statistics have such a bad reputation. Second, people are starting again to actually report their models based on the r<sup>2</sup>  value, and even have their model selection not independent of the r<sup>2</sup> value. This is something that should be bygone, yet it is not. '''Beware of the r<sup>2</sup> value, it is only deceiving you in Mixed Effect Models.''' Third, r<sup>2</sup> values in Mixed Effect Models are deeply problematic because they cannot take the complexity of the random variance into account. Hence, r<sup>2</sup> values in Mixed Effect Models make us go back to the other good old days, when mean values were ruling the outcomes of science. Today, we are closer to an understanding where variance matters, and why would we embrace that. Ok, it comes with a longer learning curve, but I think that the good old reduction to the mean was nothing but mean.\n\nAnother very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.\n\n== Outlook ==\nIn terms of Mixed Effect Models, language barriers and the norms of specific disciplines are rather strong. Explaining the basics of these advanced statistics to colleagues is an art in itself, just as the experience of researchers being versatile in [[Semi-structured Interview|Interview]]s cannot be reduced into a few hours of learning. Education in science needs to tackle this head on, and stop teaching statistics that are outdated and hardly published. I suggest that at least on a Master's level, in the long run, all students from the quantitative domain should be able to understand the preconditions and benefits of Mixed Effect Models, but this is something for the distant future. Today, PhD students being versatile in Mixed Effect Models are still outliers. Let us all hope that this statement will be outdated rather sooner than later. Mixed Effect Models are surely powerful and quite adaptable, and are increasingly becoming a part of normal science. Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever.\n\n== Key Publications ==\n----\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "lmesv9qv9parrs1apwvgx0xqh0t0e08"
                }
            },
            {
                "title": "Mixed Methods",
                "ns": "0",
                "id": "574",
                "revision": {
                    "id": "6347",
                    "parentid": "6345",
                    "timestamp": "2021-09-10T11:50:53Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18845",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Mixed Methods (German)]].<br/>\n\n'''Note:''' This entry revolves around mixed methods in general. For examples of mixed methods research designs, have a look at [[Mixed Methods Examples]]. For more on mixed methods and statistics, please refer to the entry on [[Statistics and mixed methods]].\n\n'''In short:''' This entry revolves around the combination of scientific methods and what to be aware of when doing so.\n__TOC__<br/>\n\n== What are Mixed Methods? ==\n\"Mixed Methods refers in the broadest sense to the combination of elements of a qualitative and a quantitative research approach within one investigation or several investigations related to each other. The combination can refer to the underlying scientific theoretical position and the research question, to the methods of data collection or analysis, or to the procedures of interpretation and quality assurance\" (Schreier & Odag, p.263, definition based on Johnson, Onwuegbuzie & Turner 2007, p.123).\n\nMixed methods have become almost like a standard reply to any given problem that researchers define to be outside of the domain of normal science. These days, mixed methods are almost like a mantra, a testimony of beliefs, a confirmation of openness and the recognition of diversity. One could speak of a confusion concerning mixed methods, where researchers speak about ontology (how we make sense of what we know about the world), when they should actually speak about epistemology (what we know about the world and how we create this knowledge); the talk of mixed methods is drifting into a category mistake that then becomes a categorical mistake. Mixed methods are one of the moon shots of modern science, they are proclaimed, envisioned and continuously highlighted, but the question is now: ''How do we get to the moon of mixed methods?''\n\n\n== Knowledge integration - epistemological problems ==\nThe first set of challenges when trying to bring mixed methods into reality are epistemological problems. Among the most profane yet widely unsolved questions is the integration of different data formats. While we can code data into qualitative and quantitative information into tables, this can hardly do justice to the diversity of knowledge within science. Even the integration of data within tables poses many unsolved problems. For instance, different methods can have not only a different understanding but also diverging indications of validity and plausibility. While these are pivotal in the realms of quantitative data but also in logic, validity and plausibility may be totally wrong criteria regarding much knowledge from qualitative domains where context and transferability - or the lack thereof - are more relevant. Some forms of knowledge are important to many, while other forms of knowledge are important to few. Yet, these forms of knowledge are still important, and science often ignores diversity more strongly than we should. \n\nWhile this is a trivial statement as such, it highlights how different form of discourse and knowledge production are historically rooted and more or less consolidated in schools of thought. '''There is an increasing tendency of research trying to create connections between diverse domains of knowledge, which is often a basis for methodological innovation'''. This does not only demand more efforts from researchers attempting this, but may also lead to researchers being rejected by both communities, because they mingle with 'the others'. This readily highlights the importance of a history of ideas within which we locate our epistemological identity, and within which we can clearly indicate and reflect about it. This further highlights the importance of bridging divides. Otherwise, [[Glossary|communication]] becomes more difficult if not impossible. We need to understand that all parts of science look at parts of the problem, but [[Agency, Complexity and Emergence|emergence]] is something that is ideally built on integration between different parts of science.\n\n\n== Facilitation - the gap between the epistemological and the ontological ==\nIn order to enable successful integration and bridge the epistemological challenges with the ontological problems, facilitation becomes a key part of mixed method research. This demands first and foremost the willingness to be critical about one's own positionality and reflection concerning the critical positioning in the [[History of Methods|history of science]]. Otherwise, the limitations of the knowledge in the respective area of science is often ignored, leading to a wrong recognition of these limitations. '''Many conflicts between different disciplines and their methodological epistemologies are nothing but an unreflected rejection of the unknown.''' Consequently, it takes time and effort to not only locate yourself within the canon of knowledge production, but to also value and appreciate other forms of knowledge. A simple division into a ''better or worse'' highlights the shortcomings of our understanding concerning the [[Normativity of Methods|normativity of methods]]. \n\nAn example of a tangible problem when facilitating diverse researchers that use different scientific methods is language. Misunderstanding is sometimes simply rooted in a lack of understanding, quite literally in the wording and jargon. In extreme cases, some disciplines even use the same methods but do not even realise this, because the individual methodological developments led to different wordings for the same details of the respective methodologies. Imagine how these challenges are growing exponentially if you even talk about different methodologies. This leads to the most important point in facilitation: [[Glossary|trust]]. If everyone wanted to understand every method that every branch of science utilises, this would be at a tremendous cost of resources. Consequently, a lack of understanding can alternatively be overcome by trust, which is often rooted in joint experience. There is currently an increasing recognition of the importance of facilitation, yet this also needs to recognise the ontological challenges scientists face when attempting research built on mixed methods approaches.\n\n\n== Interpretation and beliefs - ontological problems ==\nMuch of current research is embedded into specific theories of science, and many scientists are not even aware to which theory of science they are counted. Despite a growing [[Bias and Critical Thinking|criticism of positivism]], most of the current knowledge production still falls into this domain, and this creates also problems of mixed methods research. Positivism does not only create a ranking where some forms of science are - at least indirectly - considered of different value. Positivism also claims to generate objective truths, which - not surprisingly - creates a problem if other domains of science also claim to create objective truths about the very same mechanisms, entities and patterns. Modern science needs to integrate an active and reflexive theory of science, and some would argue that this demands an integration of ethics as well. How else would we evaluate the knowledge that science produces if we do not try to evaluate through different ontological lenses if it actually makes sense? '''There is a pronounced disconnection between the scientific knowledge production and how we make sense of it.''' Recent claims highlighted that there are some dimensions of knowledge that may be restricted to ethics, which could be seen as an argumentation that we need active knowledge about ethics in order to claim responsibility for our research results. Empirical research is currently mostly far away from claiming ethical responsibility for its research results, let alone the consequences of these. Time will tell if the disconnection between the epistemological and the ontological may be overcome. To this end, mixed methods research may pose a vital and effective cornerstone, as it can facilitate reflection through active research. By using research as a boundary object to bridge diverse domains of science, we may overcome our differences, and focus instead on our joint goals. We will have to see how quickly this may emerge. \n\n\n== Concrete design problems of mixed models ==\nBeside the theoretical considerations highlighted above, the [[Design Criteria of Methods|design criteria of methods]] allow for a clearer identification of concrete challenges that mixed method research faces. By going through the design criteria step by step, I would like to highlight some known problems, yet have to indicate that right now, the challenges seem endless, as mixed methods research is only starting to emerge. \n\nThe great divide between [[:Category:Quantitative|quantitative]] and [[:Category:Qualitative|qualitative]] research is probably the largest hurdle science faces, with a long established traditional distrust between these two domains in most branches of science. Data formats were already mentioned as a mere mechanical problem. However, there are more problems from the perspective of quantitive science. The larger challenges lie in a different direction: ''How can we translate quantitive knowledge into qualitative knowledge, or at least connect the two?'' '''The current distrust some people have in scientific results is a complex problem that highlights how quantitative knowledge exists, but how some things are probably best understood through qualitative knowledge.''' Contextualisation, perceptions and transformational knowledge are challenges that we only start to unravel, and longer conceptual boundary objects such as [[Agency, Complexity and Emergence|agency or emergence]] highlight the difficulties when we try to investigate phenomena and mechanisms that probably demand a mixed method agenda. Even the [[Glossary|analogy]] of pieces of the puzzles we look at seems insufficient, because it implies a materialistic dimension, and much knowledge we lack is in fact vague and [[Glossary|tacit]].\n\nThe divide between [[:Category:Inductive|inductive]] and [[:Category:Deductive|deductive]] knowledge is equally severe, yet probably less recognised. Today, much research that claims to work deductively is however shifting towards a more abductive agenda, while still being embedded into schools of thought that claim to be deductive. This creates a deep rift in terms of the validity of knowledge, since it is exactly the positivist and their deductive approaches that claim the highest validity to the knowledge they create. One might even suggest that positivist knowledge encompasses a strong system of valuation, if not judgment, regarding individual forms of knowledge, where one might wonder what the purpose of these evaluations actually is? Despite these conflicts, is it increasingly clear that deductive knowledge faces [[Questioning the status quo in methods|severe problems (e.g. reproducibility crisis)]], and in addition, it becomes increasingly clear that many of the challenges we face cannot be solely answered through deductive knowledge production. '''The rift between deductive and inductive knowledge become deeper over time, yet the bridging between the two through abductive knowledge creates a connection that is slowly widening.''' Time will tell if both domains can get down their high horses, and value each other's strengths while acknowledging one's own weaknesses. \n\nIntegration of spatial scales is an obvious problem in mixed methods research, and the examples are almost too numerous to even find any starting point that makes sense. Global supply chains may serve as an example where the needs and wishes of individuals create ripple effects on a global scale, with organisations, countries and cooperations being examples of important mediators between these two scales. These dynamics create a complexity that is only slowly unfolding, and the persisting divide between Microeconomics and Macroeconomics showcases that many forms of knowledge need to be integrated, and the relations between diverse methods are as of yet widely unclear. Another prominent example is the relation between the global biodiversity crisis and local conservation measures. While there is an emerging agenda, and even policy efforts are underway, it is still a long way from an encapsulated system towards an active knowledge exchange across spatial scales. \n\nThe last design criterion mentioned here is time. For many reasons, science focuses strongly on knowledge about the present. While a stronger recognition of knowledge about the future emerged over the last decades, and there is also a long tradition to investigate knowledge about the past, all these domains are typically disconnected. This is - again - rooted in the different knowledge domains that we have, but in addition also showcases the isolation of certain disciplines that investigate either [[:Category:Past|past]], [[:Category:Present|present]] or [[:Category:Future|future]]. It is clear that a historian would not necessarily team up with a policy maker, yet is it seems necessary considering the challenges we face. There is work to do to learn how we can integrate these diverse knowledge domains and agendas.\n\n\n== Mixed method metaphors ==\n==== Bokeh ====\n[[File:Bildschirmfoto 2020-06-14 um 19.24.14.png|right|frameless]]\nThe picture on the right allows us very clearly to depict a person. We can see a lot of details about this person, such as the clothes, a not too cheap umbrella, and much more. We can also see that it is obviously in a part of the world where is rains, where you have public transportation (both tram and buses), we see high buildings, but also low building, traffic, street lights - hence a lot of information that can be depicted despite the blurriness.\n\nOne of the nicest metaphors of interlinking methods is the Japanese word ''Bokeh''. ''Bokeh'' means basically 'depth of field', which is the effect when you have a really beautiful camera lens, and keep the aperture - the opening of the lens - very wide. Photographs made in this setting typically have a laser sharp foreground, and a beautifully blurry background. You have one very crisp level of distance, and the other level is like a washed watercolour matrix. A pro photographer or tech geek will appreciate such a photo with a \"Whoa, nice Bokeh\". Mixed method designs can be set up in a similar way. '''While you have one method focusing on something in the foreground, other methods can give you a blurry understanding of the background.''' Negotiating and designing each method's role in a mixed method setting is central in order to clarify which method demands which depth and focus, to follow with the allegory of photography. Way too often, we demand each method to have the same importance in a mixed method design. More often than not, I am not sure if this is actually possible, or even desirable. Instead, I would suggest to stick to the ''Bokeh'' design, and harmonise what is in the foreground, and what is in the background. Statistics may give you some general information about a case study setting and its background, but deep open interviews may allow for the focus and depth needed to really understand the dynamics in a case study.\n\nAn example of a paper where a lot of information was analysed that is interconnected is Hanspach et al 2014, which contains scenarios, causal-loop analysis, GIS, and many more methodological approaches that are interlinked (see Additional Information).\n\n==== Paintings ====\nAnother example of a metaphor that may help to understand a mixed method approach is classical landscape painting. You try to include all the details from a landscape in a painting, leaving out uncountable details, and generalising some information into something altogether different. The setting can be quite designed, with a canvas, a certain perspective, some brushes and a pallet of colours. Yet already considering the diversity of colours showcases the different ways how the natural colours can be approximated. Before the chemical industry thrived, painters went through great efforts to get specific colours, often at a high price. Yet analyses of such paintings show us the superiority of these paints, and indeed how many of such paintings stand the test of time. Another thing to consider is the way how many painters paint, maybe sketching out ideas first, creating charcoal sketches and experimenting with perspectives. Paint was often not only mixed but also applied in layers. Brush techniques were often an essential part of the specific character of a painting. Now consider all these details when looking at scientific methods. Just as landscape painting, scientific methods can be a combination of a canvas, some brushes and a few colours. Yet consider the diversity of painting we know today, and compare this to the diversity of knowledge we may gain if we combine a slightly different methods with some other perspective. What if a specific technique to document data may lead to novel insights? And how can we alter our pallet to gain a different representation of reality? And, last but not least, how does our knowledge integrate into previous knowledge? Many painters were strongly influenced by other painters, and schools of thought can be traced in paintings quite well, and are in the focus of diverse forms of research.\n\n'''Integration, reflection and normativity are components of methodology that are slowly starting to emerge.''' While deep focus will probably remain the main strategy of most researchers, the wicked problems we currently face demand novel approaches in order to enable a transformation, and to investigate the associated mechanisms and structures. Much is still unknown, and we need to overcome the focus on resources, the claims of scientists to rank and judge knowledge of other scientists, and last but not least the deep entrenchment of scientific disciplines when it comes to their diverse methodologies. \n\nTo quote Nietzsche: ''There has never been such a new dawn and clear horizon, and such an open sea.''\n\n\n== Further Information ==\n* Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to southern Transylvania''. Ecology and Society 19(4): 32.\nA paper that illustrates the combination of various methods (including Scenario Planning, GIS, and Causal-Loop Diagrams) in an empirical study.\n\n* Schreier, M. & Odag, \u00d6. ''Mixed Methods.'' In: G. Mey K. Mruck (Hrsg.). ''Handbuch Qualitative Forschung in der Psychologie.'' VS Verlag f\u00fcr Sozialwissenschaften | Springer Fachmedien Wiesbaden GmbH 2010. 263-277.\nA good introduction to the Mixed Methods approach.\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.\n\n[[Category:Normativity_of_Methods]]"
                    },
                    "sha1": "64t8mgwhegj7y7dlcogxhk4de3g04ao"
                }
            },
            {
                "title": "Mixed Methods (German)",
                "ns": "0",
                "id": "575",
                "revision": {
                    "id": "6526",
                    "parentid": "6523",
                    "timestamp": "2022-02-14T22:29:54Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Konkrete Design-Probleme von Mixed Methods */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "21603",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Mixed Methods]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' In diesem Eintrag geht es um die Kombination unterschiedlicher wissenschaftlicher Methoden und was dabei zu beachten ist.\n\n__TOC__\n\n== Was sind Mixed Methods? ==\n\"Mixed Methods (...) bezeichnet im weitesten Sinne die Kombination von Elementen eines qualitativen und eines quantitativen Forschungsansatzes innerhalb einer Untersuchung oder mehrerer aufeinander bezogener Untersuchungen. Die Kombination kann sich dabei auf die zugrunde liegende wissenschaftstheoretische Position und die Fragestellung, auf die Methoden der Datenerhebung oder der -auswertung oder auch auf die Verfahren der Interpretation und der Qualit\u00e4tssicherung beziehen.\" (Schreier & Odag, p.263, Definition in Anlehnung an Johnson, Onwuegbuzie & Turner 2007, S.123)\n\nMixed Methods sind fast so etwas wie eine Standardantwort auf jedes beliebige Problem geworden, das Forscher*innen als au\u00dferhalb des Bereichs der normalen Wissenschaft liegend definieren. Heutzutage sind Mixed Methods fast wie ein Mantra, ein Glaubensbekenntnis, eine Best\u00e4tigung der eigenen Offenheit und die Anerkennung von Vielfalt. Man k\u00f6nnte von einer Verwirrung in Bezug auf Mixed Methods sprechen, wo Forscher*innen \u00fcber Ontologie (wie wir unser Wissen \u00fcber die Welt deuten) sprechen, wenn sie eigentlich \u00fcber Epistemologie (wie wir Wissen \u00fcber die Welt erzeugen) sprechen sollten; die Rede von gemischten Methoden driftet in einen Kategorienfehler ab, der dann zu einem kategorialen Fehler wird. Mixed Methods sind die Mondfahrt der modernen Wissenschaft, sie werden proklamiert, in Aussicht gestellt und immer wieder hervorgehoben, aber die Frage ist nun: ''Wie kommen wir zum Mond der gemischten Methoden?''\n\n== Wissensintegration - epistemologische Probleme ==\nDie erste Reihe von Herausforderungen beim Versuch, Mixed Methods in die Realit\u00e4t umzusetzen, sind epistemologische Probleme. Zu den profansten, aber weitgehend ungel\u00f6sten Fragen geh\u00f6rt die Integration unterschiedlicher Datenformate. Wir k\u00f6nnen zwar Daten in qualitative und quantitative Informationen in Tabellen kodieren, aber das kann der Vielfalt des Wissens innerhalb der Wissenschaft kaum gerecht werden. Auch die Integration von Daten innerhalb von Tabellen wirft viele ungel\u00f6ste Probleme auf. Zum Beispiel k\u00f6nnen unterschiedliche Methoden nicht nur ein unterschiedliches Verst\u00e4ndnis, sondern auch divergierende Angaben zu Validit\u00e4t und Plausibilit\u00e4t haben. W\u00e4hrend diese im Bereich quantitativer Daten, aber auch in der Logik von zentraler Bedeutung sind, k\u00f6nnen Validit\u00e4t und Plausibilit\u00e4t wiederum v\u00f6llig falsche Kriterien sein, wenn es um Wissen aus qualitativen Bereichen geht, wo Kontext und \u00dcbertragbarkeit - oder deren Mangel - relevanter sind. Einige Formen des Wissens sind f\u00fcr Viele wichtig, w\u00e4hrend andere Formen des Wissens f\u00fcr Wenige wichtig sind. Dennoch sind diese Formen des Wissens wichtig, und die Wissenschaft ignoriert die bestehende Vielfalt oft st\u00e4rker, als sie es sollte. \n\nDies ist zwar an sich eine triviale Aussage, aber sie verdeutlicht, wie unterschiedliche Formen des Diskurses und der Wissensproduktion historisch verwurzelt und mehr oder weniger in Denkschulen verfestigt sind. '''Es gibt eine zunehmende Tendenz in der Forschung, zu versuchen, Verbindungen zwischen verschiedenen Wissensdom\u00e4nen herzustellen, was oft eine Grundlage f\u00fcr methodologische Innovationen ist'''. Dies verlangt nicht nur mehr Anstrengungen von Forscher*innen, die dies versuchen, sondern kann auch dazu f\u00fchren, dass Forscher*innen von beiden Gemeinschaften abgelehnt werden, weil sie sich mit 'den Anderen' vermischen. Dies macht deutlich, wie wichtig eine [[Scientific methods and societal paradigms (German)|'Geschichte von Ideen]]' ist, innerhalb derer wir unsere epistemologische Identit\u00e4t verorten, sie klar benennen und dar\u00fcber reflektieren k\u00f6nnen. Damit wird auch deutlich, wie wichtig es ist, Gr\u00e4ben zu \u00fcberbr\u00fccken. Andernfalls wird die Kommunikation schwieriger, wenn nicht gar unm\u00f6glich. Wir m\u00fcssen verstehen, dass alle Teile der Wissenschaft Teile des Problems betrachten, aber [[Agency, Complexity and Emergence (German)|Emergenz]] ist etwas, das idealerweise auf der Integration zwischen verschiedenen Teilen der Wissenschaft aufgebaut ist.\n\n== Moderation - die L\u00fccke zwischen dem Epistemologischen und dem Ontologischen ==\nUm eine erfolgreiche Integration zu erm\u00f6glichen und die epistemologischen Herausforderungen mit den ontologischen Problemen zu \u00fcberbr\u00fccken, wird Moderation zu einem zentralen Bestandteil der Mixed-Methods-Forschung. Dies erfordert vor allem die Bereitschaft zur kritischen Auseinandersetzung mit der eigenen Positionalit\u00e4t und die Reflexion \u00fcber eine kritische Positionierung in der [[History of Methods (German)|Wissenschaftsgeschichte]]. Andernfalls werden die Grenzen des Wissens im jeweiligen Wissenschaftsbereich oft ignoriert, was zu einer falschen Anerkennung dieser Grenzen f\u00fchrt. '''Viele Konflikte zwischen verschiedenen Disziplinen und deren methodischen Erkenntnistheorien sind nichts anderes als eine unreflektierte Ablehnung des Unbekannten.''' Es kostet also Zeit und M\u00fche, sich nicht nur innerhalb des Kanons der Wissensproduktion zu verorten, sondern auch andere Wissensformen zu sch\u00e4tzen und zu w\u00fcrdigen. Eine einfache Einteilung in ein ''besser oder schlechter'' zeigt die Unzul\u00e4nglichkeiten unseres Verst\u00e4ndnisses bez\u00fcglich der [[Normativity of Methods|Normativit\u00e4t von Methoden]] auf. \n\nEin Beispiel f\u00fcr ein greifbares Problem bei der Moderation zwischen verschiedenen Forschenden, die unterschiedliche wissenschaftliche Methoden verwenden, ist die Sprache. Missverst\u00e4ndnisse beruhen manchmal einfach auf mangelndem Verst\u00e4ndnis, ganz w\u00f6rtlich in der Wortwahl und im Jargon. Im Extremfall verwenden manche Disziplinen sogar die gleichen Methoden, ohne dies zu bemerken, weil die einzelnen methodischen Entwicklungen zu unterschiedlichen Bezeichnungen f\u00fcr die gleichen Details der jeweiligen Methodik gef\u00fchrt haben. Stellen Sie sich vor, wie diese Herausforderungen exponentiell zunehmen, wenn man \u00fcberhaupt von unterschiedlichen Methodologien spricht. Das f\u00fchrt zum wichtigsten Punkt in der Moderation: Vertrauen. Wenn jeder jede Methode, die jeder Wissenschaftszweig anwendet, verstehen wollte, w\u00e4re dies mit einem enormen Aufwand an Ressourcen verbunden. Mangelndes Verst\u00e4ndnis kann also alternativ durch Vertrauen \u00fcberwunden werden, das oft in gemeinsamen Erfahrungen wurzelt. Gegenw\u00e4rtig wird die Bedeutung von Moderation zunehmend anerkannt, doch muss dies auch die ontologischen Herausforderungen ber\u00fccksichtigen, denen sich Wissenschaftler*innen gegen\u00fcbersehen, wenn sie versuchen, mit gemischten Methoden zu forschen.\n\n== Interpretation und \u00dcberzeugungen - ontologische Probleme ==\nEin Gro\u00dfteil der aktuellen Forschung ist in bestimmte Wissenschaftstheorien eingebettet, und viele Wissenschaftler*innen sind sich nicht einmal bewusst, zu welcher Wissenschaftstheorie sie gez\u00e4hlt werden. Trotz zunehmender [[Bias and Critical Thinking|Kritik am Positivismus]] f\u00e4llt ein Gro\u00dfteil der aktuellen Wissensproduktion immer noch in diese Dom\u00e4ne, und das schafft auch Probleme der Mixed-Methods-Forschung. Der Positivismus schafft nicht nur eine Rangordnung, in der einige Formen der Wissenschaft - zumindest indirekt - als unterschiedlich wertvoll angesehen werden. Der Positivismus erhebt auch den Anspruch, objektive Wahrheiten zu erzeugen, was - nicht \u00fcberraschend - ein Problem schafft, wenn andere Bereiche der Wissenschaft ebenfalls den Anspruch erheben, objektive Wahrheiten \u00fcber dieselben Mechanismen, Entit\u00e4ten und Muster zu erzeugen. Die moderne Wissenschaft muss eine aktive und reflexive Wissenschaftstheorie integrieren, und einige w\u00fcrden argumentieren, dass dies auch eine Integration der Ethik erfordert. Wie sonst sollten wir das Wissen, das die Wissenschaft produziert, bewerten, wenn wir nicht versuchen, durch verschiedene ontologische Linsen zu bewerten, ob es tats\u00e4chlich Sinn macht? '''Es gibt eine ausgepr\u00e4gte Diskrepanz zwischen der wissenschaftlichen Wissensproduktion und der Art und Weise, wie wir ihr einen Sinn geben.''' In j\u00fcngster Zeit wurde hervorgehoben, dass es einige Dimensionen des Wissens gibt, die sich auf die Ethik beschr\u00e4nken, was als Argumentation daf\u00fcr gesehen werden k\u00f6nnte, dass wir ein aktives Wissen \u00fcber Ethik brauchen, um Verantwortung f\u00fcr unsere Forschungsergebnisse zu \u00fcbernehmen. Die empirische Forschung ist derzeit meist weit davon entfernt, ethische Verantwortung f\u00fcr ihre Forschungsergebnisse zu beanspruchen, geschweige denn f\u00fcr die Folgen dieser Ergebnisse. Die Zeit wird zeigen, ob die Entkopplung zwischen dem Epistemologischen und dem Ontologischen \u00fcberwunden werden kann. Hierf\u00fcr kann die Mixed-Methods-Forschung einen wichtigen und effektiven Eckpfeiler darstellen, da sie eine Reflexion durch aktive Forschung erm\u00f6glichen kann. Indem wir die Forschung als Grenzobjekt nutzen, um verschiedene Bereiche der Wissenschaft zu \u00fcberbr\u00fccken, k\u00f6nnen wir unsere Unterschiede \u00fcberwinden und uns stattdessen auf unsere gemeinsamen Ziele konzentrieren. Es wird sich zeigen, wie schnell sich dies entwickeln kann.\n\n== Konkrete Design-Probleme von Mixed Methods ==\nNeben den oben hervorgehobenen theoretischen \u00dcberlegungen erlauben die [[Design Criteria of Methods (German)|Gestaltungskriterien von Methoden]] eine klarere Identifizierung konkreter Herausforderungen, denen sich die Mixed-Methods-Forschung gegen\u00fcber sieht. Indem ich die Gestaltungskriterien Schritt f\u00fcr Schritt durchgehe, m\u00f6chte ich einige bekannte Probleme hervorheben, muss aber auch darauf hinweisen, dass die Herausforderungen im Moment unendlich erscheinen, da die Mixed-Methods-Forschung erst im Entstehen begriffen ist. \n\nDie gro\u00dfe Kluft zwischen [[:Category:Quantitative|quantitativer]] und [[:Category:Qualitative|qualitativer]] Forschung ist wahrscheinlich die gr\u00f6\u00dfte H\u00fcrde f\u00fcr die Wissenschaft, mit einem seit langem etablierten traditionellen Misstrauen zwischen diesen beiden Bereichen in den meisten Wissenschaftszweigen. Datenformate wurden bereits als ein rein mechanisches Problem erw\u00e4hnt. Aus der Perspektive der quantitativen Wissenschaft gibt es jedoch mehr Probleme. Die gr\u00f6\u00dferen Herausforderungen liegen in einer anderen Richtung: ''Wie k\u00f6nnen wir quantitatives Wissen in qualitatives Wissen \u00fcbersetzen, oder zumindest die beiden miteinander verbinden?'' '''Das derzeitige Misstrauen mancher Menschen gegen\u00fcber wissenschaftlichen Ergebnissen ist ein komplexes Problem, das verdeutlicht, dass es zwar quantitatives Wissen gibt, dass aber manche Dinge wahrscheinlich am besten durch qualitatives Wissen zu verstehen sind.''' Kontextualisierung, Wahrnehmungen und Transformationswissen sind Herausforderungen, die wir erst beginnen zu entwirren, und l\u00e4ngere konzeptionelle Grenzobjekte wie [[Agency, Complexity and Emergence|Agency oder Emergenz]] verdeutlichen die Schwierigkeiten, wenn wir versuchen, Ph\u00e4nomene und Mechanismen zu untersuchen, die wahrscheinlich eine gemischte Methodenagenda erfordern. Selbst die Analogie von Puzzleteilen, die wir betrachten, scheint unzureichend zu sein, weil sie eine materialistische Dimension impliziert, und viel Wissen, das uns fehlt, ist in der Tat vage und implizit.\n\nDie Kluft zwischen [[:Category:Inductive|induktivem]] und [[:Category:Deductive|deduktivem]] Wissen ist ebenso gravierend, aber wahrscheinlich weniger anerkannt. Heutzutage verlagert sich jedoch ein Gro\u00dfteil der Forschung, die behauptet, deduktiv zu arbeiten, in Richtung einer eher abduktiven Agenda, w\u00e4hrend sie immer noch in Denkschulen eingebettet ist, die behaupten, deduktiv zu sein. Dies schafft einen tiefen Riss in Bezug auf die G\u00fcltigkeit von Wissen, da gerade die Positivist*innen und ihre deduktiven Ans\u00e4tze die h\u00f6chste G\u00fcltigkeit f\u00fcr das von ihnen geschaffene Wissen beanspruchen. Man k\u00f6nnte sogar behaupten, dass positivistisches Wissen ein starkes System der Bewertung, wenn nicht gar des Urteils, \u00fcber einzelne Wissensformen beinhaltet, wobei man sich fragen k\u00f6nnte, was der Zweck dieser Bewertungen eigentlich ist? Trotz dieser Konflikte wird immer deutlicher, dass deduktives Wissen mit [[Questioning the status quo in methods (German)|schwerwiegenden Problemen (z.B. der Reproduzierbarkeitskrise)]] konfrontiert ist, und dar\u00fcber hinaus wird immer deutlicher, dass viele der Herausforderungen, vor denen wir stehen, nicht allein durch deduktive Wissensproduktion beantwortet werden k\u00f6nnen. '''Die Kluft zwischen deduktivem und induktivem Wissen wird mit der Zeit immer tiefer, doch der Br\u00fcckenschlag zwischen beiden durch abduktives Wissen schafft eine Verbindung, die sich langsam verbreitert.''' Die Zeit wird zeigen, ob beide Dom\u00e4nen von ihrem hohen Ross herunterkommen und die St\u00e4rken des jeweils anderen sch\u00e4tzen und gleichzeitig die eigenen Schw\u00e4chen anerkennen k\u00f6nnen. \n\nDie Integration von r\u00e4umlichen Skalen ist ein offensichtliches Problem in der Mixed-Methods-Forschung, und die Beispiele sind fast zu zahlreich, um \u00fcberhaupt einen sinnvollen Ansatzpunkt zu finden. Globale Lieferketten k\u00f6nnen als Beispiel dienen, wo die Bed\u00fcrfnisse und W\u00fcnsche von Individuen Welleneffekte auf einer globalen Skala erzeugen, wobei Organisationen, L\u00e4nder und Kooperationen Beispiele f\u00fcr wichtige Vermittler zwischen diesen beiden Skalen sind. Diese Dynamik schafft eine Komplexit\u00e4t, die sich nur langsam entfaltet, und die anhaltende Kluft zwischen Mikro- und Makro\u00f6konomie zeigt, dass viele Formen von Wissen integriert werden m\u00fcssen und die Beziehungen zwischen verschiedenen Methoden noch weitgehend unklar sind. Ein weiteres prominentes Beispiel ist die Beziehung zwischen der globalen Biodiversit\u00e4tskrise und lokalen Schutzma\u00dfnahmen. Zwar gibt es eine sich abzeichnende Agenda, und auch politische Bem\u00fchungen sind im Gange, aber es ist noch ein weiter Weg von einem gekapselten System hin zu einem aktiven Wissensaustausch \u00fcber r\u00e4umliche Skalen hinweg. \n\nDas letzte hier genannte Design-Kriterium ist die Zeit. Aus vielen Gr\u00fcnden konzentriert sich die Wissenschaft stark auf das Wissen \u00fcber die Gegenwart. W\u00e4hrend in den letzten Jahrzehnten eine st\u00e4rkere Anerkennung von Wissen \u00fcber die Zukunft aufkam und es auch eine lange Tradition gibt, Wissen \u00fcber die Vergangenheit zu erforschen, sind alle diese Bereiche typischerweise nicht miteinander verbunden. Dies ist - wiederum - in den unterschiedlichen Wissensdom\u00e4nen begr\u00fcndet, zeigt aber auch die Isolation bestimmter Disziplinen, die entweder [[:Category:Past|die Vergangenheit]], [[:Category:Present|die Gegenwart]] oder [[:Category:Future|die Zukunft]] erforschen. Es ist klar, dass sich Historiker*innen nicht unbedingt mit einem Politiker*innen zusammentun w\u00fcrde, dennoch scheint es angesichts der Herausforderungen, denen wir gegen\u00fcberstehen, notwendig. Es wird noch zu erarbeiten sein, wie wir diese verschiedenen Wissensbereiche und Agenden integrieren k\u00f6nnen.\n\n== Metaphern f\u00fcr Mixed Methods ==\n==== Bokeh ====\n[[File:Bildschirmfoto 2020-06-14 um 19.24.14.png|right|frameless]]\nAuf dem Bild rechts k\u00f6nnen wir eine Person sehr deutlich erkennen. Wir k\u00f6nnen viele Details dieser Person erkennen, wie z.B. die Kleidung, einen nicht zu billigen Regenschirm und vieles mehr. Wir k\u00f6nnen auch sehen, dass sie sich offensichtlich in einem Teil der Welt befindet, wo es regnet, wo es \u00f6ffentliche Verkehrsmittel gibt (sowohl Stra\u00dfenbahn als auch Busse), wir sehen hohe Geb\u00e4ude, aber auch niedrige Geb\u00e4ude, Verkehr, Stra\u00dfenlaternen - also viele Informationen, die trotz der Unsch\u00e4rfe dargestellt werden k\u00f6nnen.\n\nEine der sch\u00f6nsten Metaphern f\u00fcr die Verkn\u00fcpfung von Methoden ist das japanische Wort ''Bokeh''. ''Bokeh'' bedeutet im Grunde 'Tiefensch\u00e4rfe', was der Effekt ist, wenn man ein wirklich sch\u00f6nes Kameraobjektiv hat und die Blende - die \u00d6ffnung des Objektivs - sehr weit h\u00e4lt. Fotos, die mit dieser Einstellung gemacht werden, haben typischerweise einen laserscharfen Vordergrund und einen sch\u00f6n verschwommenen Hintergrund. Sie haben eine sehr scharfe Entfernungsebene, und die andere Ebene ist wie eine verwaschene Aquarellmatrix. Ein Profifotograf oder Technikfreak wird ein solches Foto mit einem \"Wow, sch\u00f6nes Bokeh\" w\u00fcrdigen. Designs mit gemischten Methoden k\u00f6nnen auf \u00e4hnliche Weise aufgebaut werden. '''W\u00e4hrend Sie eine Methode haben, die sich auf etwas im Vordergrund konzentriert, k\u00f6nnen Sie mit anderen Methoden den Hintergrund unscharf erkennen.''' Das Aushandeln und Erarbeiten der Rolle jeder Methode in einem Mixed-Methods-Setting ist zentral, um zu kl\u00e4ren, welche Methode welche Tiefe und welchen Fokus verlangt, um dem Gleichnis der Fotografie zu folgen. Viel zu oft verlangen wir, dass jede Methode in einem Mixed-Methods-Design den gleichen Stellenwert hat. In den meisten F\u00e4llen bin ich mir nicht sicher, ob dies tats\u00e4chlich m\u00f6glich oder sogar w\u00fcnschenswert ist. Stattdessen w\u00fcrde ich vorschlagen, sich an das ''Bokeh''-Design zu halten und das, was im Vordergrund ist, mit dem, was im Hintergrund ist, zu harmonisieren. Statistiken k\u00f6nnen Ihnen einige allgemeine Informationen \u00fcber das Setting einer Fallstudie und deren Hintergrund geben, aber tiefgehende offene Interviews k\u00f6nnen den Fokus und die Tiefe erm\u00f6glichen, die notwendig sind, um die Dynamik in einer Fallstudie wirklich zu verstehen.\n\nEin Beispiel f\u00fcr eine Arbeit, in der viele Informationen analysiert wurden, die miteinander verkn\u00fcpft sind, ist Hanspach et al. 2014, die Szenarien, Kausalschleifenanalyse, GIS und viele weitere methodische Ans\u00e4tze enth\u00e4lt, die miteinander verkn\u00fcpft sind (siehe Additional Information).\n\n==== Malerei ====\nEin weiteres Beispiel f\u00fcr eine Metapher, die helfen kann, einen Mixed-Methods-Ansatz zu verstehen, ist die klassische Landschaftsmalerei. Man versucht, alle Details einer Landschaft in ein Gem\u00e4lde aufzunehmen, l\u00e4sst dabei unz\u00e4hlige Details weg und verallgemeinert einige Informationen zu etwas ganz anderem. Das Setting kann recht konstruiert sein, mit einer Leinwand, einer bestimmten Perspektive, einigen Pinseln und einer Palette von Farben. Doch schon die Betrachtung der Farbvielfalt zeigt, auf welch unterschiedliche Weise die nat\u00fcrlichen Farben angen\u00e4hert werden k\u00f6nnen. Bevor die chemische Industrie florierte, haben Maler gro\u00dfe Anstrengungen unternommen, um bestimmte Farben zu erhalten, oft zu einem hohen Preis. Doch Analysen solcher Gem\u00e4lde zeigen uns die \u00dcberlegenheit dieser Farben, und wie viele solcher Gem\u00e4lde den Test der Zeit bestehen. Eine andere Sache ist die Art und Weise, wie viele Maler malen, vielleicht indem sie zuerst Ideen skizzieren, Kohleskizzen anfertigen und mit Perspektiven experimentieren. Farbe wurde oft nicht nur gemischt, sondern auch in Schichten aufgetragen. Pinseltechniken waren oft ein wesentlicher Bestandteil des spezifischen Charakters eines Gem\u00e4ldes. Ber\u00fccksichtigen Sie nun all diese Details bei der Betrachtung der wissenschaftlichen Methoden. Genau wie die Landschaftsmalerei k\u00f6nnen wissenschaftliche Methoden eine Kombination aus einer Leinwand, einigen Pinseln und ein paar Farben sein. Doch betrachten Sie die Vielfalt der Malerei, die wir heute kennen, und vergleichen Sie diese mit der Vielfalt der Erkenntnisse, die wir gewinnen k\u00f6nnen, wenn wir eine etwas andere Methode mit einer anderen Perspektive kombinieren. Was, wenn eine bestimmte Technik zur Dokumentation von Daten zu neuen Erkenntnissen f\u00fchren kann? Und wie k\u00f6nnen wir unsere Palette ver\u00e4ndern, um eine andere Darstellung der Realit\u00e4t zu erhalten? Und, nicht zuletzt, wie f\u00fcgt sich unser Wissen in bereits vorhandenes Wissen ein? Viele Maler wurden stark von anderen Malern beeinflusst, und Denkschulen lassen sich in Gem\u00e4lden sehr gut nachvollziehen und stehen im Fokus diverser Forschungsformen.\n\n'''Integration, Reflexion und Normativit\u00e4t sind Komponenten der Methodologie, die sich langsam durchsetzen.''' W\u00e4hrend der Tiefenfokus wahrscheinlich die Hauptstrategie der meisten Forscher bleiben wird, verlangen die \"wicked problems\", mit denen wir derzeit konfrontiert sind, nach neuen Ans\u00e4tzen, um eine Transformation zu erm\u00f6glichen und die damit verbundenen Mechanismen und Strukturen zu untersuchen. Vieles ist noch unbekannt, und wir m\u00fcssen die Fokussierung auf Ressourcen, die Anspr\u00fcche von Wissenschaftlern, das Wissen anderer Wissenschaftler zu bewerten und zu beurteilen, und nicht zuletzt die tiefe Verankerung der wissenschaftlichen Disziplinen, wenn es um ihre unterschiedlichen Methoden geht, \u00fcberwinden. \n\nUm Nietzsche zu zitieren: \"Nie gab es eine so neue Morgenr\u00f6te und einen so klaren Horizont, und ein so offenes Meer.\"\n\n\n== Additional Information ==\n* Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to southern Transylvania''. Ecology and Society 19(4): 32.\nZeigt eine Kombination verschiedener Methoden in empirischer Forschung auf, darunter Scenario Planning, GIS, Causal-Loop Diagrams).\n\n* Schreier, M. & Odag, \u00d6. ''Mixed Methods.'' In: G. Mey K. Mruck (Hrsg.). ''Handbuch Qualitative Forschung in der Psychologie.'' VS Verlag f\u00fcr Sozialwissenschaften | Springer Fachmedien Wiesbaden GmbH 2010. 263-277.\nEine gute Einf\u00fchrung in das Mixed Methods-Konzept.\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.\n[[Category:Normativity_of_Methods]]"
                    },
                    "sha1": "ksxs7t9r3mzjrayh3kd2vgykx74x7ja"
                }
            },
            {
                "title": "Mixed Methods Examples",
                "ns": "0",
                "id": "833",
                "revision": {
                    "id": "6211",
                    "parentid": "6210",
                    "timestamp": "2021-08-09T07:03:26Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6032",
                        "#text": "'''This page presents examples of Mixed Methods research designs for sustainability research.''' The following texts are summaries of research proposals that were contributed by the credited students of M.Sc. (Global) Sustainability Science for the 2021 class \"Research Methods in Sustainability Science.\" For more on how to combine scientific methods, please refer to the entry on [[Mixed Methods]].\n\n\n== Solving the conflict between railroads, nomads and wildlife in Mongolia ==\nby Johanna Hofmann, Elena Kortmann, Chantal Krumm, Antonia Polheim, Theresa Seidel; <br>\nMiranda Kiefer, Moritz L\u00fcdemann, Svenja Meyer, Elias Schiafone, Jana Sperlich\n\nThe project focuses on the Trans Mongolian Railroad Corridor (TMR) between Ulaanbaatar and Erenhot, where fenced railroad - a consequence of a growing mining sector - creates a barrier for nomands and their herds, leads to habitat fragmentation for the movement of wildlife, and imposes a danger to both. This '''conflict between infrastructure, local populations and animals''' is in the center of this research proposal. Two research proposals are presented here, which both revolve around the idea of identifying potential spots for railway crossings for wildlife and nomads. Landsat [[Geographical Information Systems|'''GIS''']] data, as well as wildlife movement GIS data from previous studies, are proposed be analyzed to examine topographical preconditions for railway crossings, as well as vegetation changes and gazelle migration patterns to identify relevant spots for local wildlife. In addition, data from [[Semi-structured Interview|'''semi-structured interviews''']] with ten nomad herders shall be gathered and analyzed to assess if, and where, they would be interested in also using railway crossings in accordance with their traditional and personal requirements. In combination, the gathered data shall help identify potential spots for railway crossings, which can be installed subsequently.\n\n[[File:Mission 2 - Justice League - Visualisation.jpg|600px|frameless|center|Visualisation of the Research Proposal for the Mongolia Mission. Source: Johanna Hofmann, Elena Kortmann, Chantal Krumm, Antonia Polheim, Theresa Seidel]]\n\nOne of the proposals further suggests to set up two test railway crossings to investigate differences between partial fencing and warning signals to support safe railway crossings. Here, after suitable locations have been identified, two railway crossings shall be installed, one with and one without warning signals, with camera traps each. Based on the results of the '''analysis of camera trap photographs''', the more viable approach of railway crossing shall be further pursued. Nomads shall be involved in long-term maintenance of the crossings.\n\n[[File:Mission 2 - X-men - Visualisation.jpg|600px|frameless|center|'''Visualisation of the Research Proposal on the Mongolia Mission.''' Source: Miranda Kiefer, Moritz L\u00fcdemann, Svenja Meyer, Elias Schiafone, Jana Sperlich]]\n\n\n== Providing healthy and affordable food at universities ==\nby Lauren Anderson, Elena Kortmann, Chantal Krumm, Elias Schiafone\n\n[[File:Mission 3 - TMNT - Visualisation.jpg|600px|frameless|right|'''Visualisation of the research proposal on Diets.''' Source: Lauren Anderson, Elena Kortmann, Chantal Krumm, Elias Schiafone]]\n\nThis research proposal revolves around the '''problem of providing affordable, yet healthy and sustainable food at university cafeterias''' in Germany and the US. The suggested idea here is to, first, conduct a [[Survey|'''questionnaire''']] with 60 first semester university students at two universities, each, based on which the participants will be grouped according to their financial resources, which are of interest to this study. Then, students take part in four six-week long project phases, in which they are asked to document in a '''digital diary''' their food consumption and review their experience with one of '''three rotating interventions''', as well as their initial behavioral patterns. The interventions either include meal vouchers for vegan or vegetarian meals at dining halls; educational sessions on health, sustainability and affordable nutrition as well as cooking classes; and engagement with a newly set up greenhouse. After going through all four phases, participants are asked to rank all interventions in a questionnaire. All diary and questionnaire data is analyzed qualitative and quantitatively, and suitable recommendations are developed for both universities based on the insights.\n\n\n== Protecting the Sierras Nevadas in Argentina ==\nby Chantal Krumm, Magdalena Nordmeyer, Sabina Davletkildeeva, Luna Prince\n\n[[File:Mission 4 - Justice League - Visualisation.jpg|600px|frameless|right|'''Visualisation of the research proposal on Argentina.''' Source: Chantal Krumm, Magdalena Nordmeyer, Sabina Davletkildeeva, Luna Prince]]\n\nThis research proposal '''investigates the potential of volunteer tree planting''' in the Sierras Grandes in Argentina. To do so, it is proposed to establish a local science center in order to co-develop a future [[Scenario Planning|scenario]] with local stakeholders and subsequently ensure its long-term and inclusive implementation. In a first step, a literature review and [[Ethnography|'''ethnographic observation''']] of local actors are conducted to get an initial overview on the study area and field. Then, in [[Semi-structured Interview|'''semi-structured interviews''']], relevant stakeholders shall be identified, which shall be invited to four [[Scenario Planning|'''scenario planning''']] workshop. Here, with a focus on inclusive and collaborative development of the region, four scenarios are constructed, one of which is selected as the baseline for future activitites. First measures, with a focus on volunteer tree planting, shall be implemented and evaluated as a baseline for future plans.\n\n----\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this page is Christopher Franz, based on the research proposals by the credited students."
                    },
                    "sha1": "habaft7uvdr4nxhe613q9p5fbvyoiud"
                }
            },
            {
                "title": "Model reduction",
                "ns": "0",
                "id": "998",
                "revision": {
                    "id": "6869",
                    "timestamp": "2023-01-05T08:44:22Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "Created page with \"== '''Starting to engage with model reduction - an initial approach''' ==  The diverse approaches of model reduction, model comparison and model simplification within univaria...\"",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9408",
                        "#text": "== '''Starting to engage with model reduction - an initial approach''' ==\n\nThe diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics.  \n\nRegarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy. Lastly, statistical analysis based on probabilities has a flawed if not altogether boring view on maximum models, since probabilities change with increasing model reduction. Hence maximum models are nothing but a starting point. These may be informed by previous knowledge, since clear hypotheses are usually already more specific than brute force approaches. \n\nThe most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.\n\nInstead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values. \n\nThe simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word \"fairly\" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally unrelated. Whatever information the first axis contains,  the second axis contains exactly not this information, and so on. This allows to filter large datasets with many variables into few artificial axis while maintaining much of the information. However, one needs to be careful since these artificial axis are not the real variables, but synthetic reductions. Still, the PCA has proven valuable to check for redundancies, also since these can be graphically represented. \n\nThe last way to check for redundancies within concrete models is the variance inflation factor. This measure allowed to check regression models for redundant predictors. If any of the values is above 5, or some argue above 10, then the respective variable needs to be excluded. Now if you want to keep this special variable, you have to identify other variables redundant with this one, and exclude these. Hence the VIF can guide you model constructions and is the ultimate safeguard to exclude all variables that are redundant. \n\nOnce you thus created models that contain non-redundant variables, the next question is how you reduce the model or models that you have based on your initial hypotheses. In the past, the usual way within probability based statistics was a subsequent reduction based on p-values. Within each step, the non-significant variable with the highest p-value would be excluded until only significant variables remain. This minimum adequate mode based on a subsequent reduction based on p-values still needs to be tested against the Null model. However, p-value driven model reductions are sometimes prone to errors. Defining different and clearly defined models before the analysis and then compare these models based on AIC values is clearly superior, and inflicts less bias. An information theoretical approach compares clearly specified models against the Null Model based on the AIC, and the value with the lowest AIC is considered to be the best. However, this model needs to be at least 2 lower than the second best model, otherwise these two models need to be averaged. This approach safeguards against statistical fishing, and can be soldiered a gold standard in deductive analysis. \n\nWithin inductive analysis it is less clear how to proceed best. Technically, one can only proceed based on AIC values. Again, there is a brute force approach that boils the maximum model down based on permutations of all combinations. However, this approach can be again considered to be statistical fishing, since no clear hypothesis are tested. While an AIC driven approach failsafes against the worst dangers of statistical fishing, it is clear that if you have no questions, then you also have no answers. Hence a purely inductive analysis does not really make sense, yet you can find the inner relations and main patterns of the dataset regardless of your approach, may it bee inductive or deductive. \n\nDeep down, any given dataset should reveal the same results based on this rigid analysis pathway and framework. However, the scientific community developed different approaches, and there are diverse schools of thinking, which ultimately leads to different approaches being out there. Different analysts may come up with different results. This exemplifies that statistics are not fully unleashed yet, but are indeed still evolving, and not necessarily about reproducible analysis. Keep that in mind when you read analysis, and be conservative in your own analysis. Keep no stone unturned, and go down any rabbit hole you can find."
                    },
                    "sha1": "qkkr2647pvx9cf6mh81u8h5riwn46hs"
                }
            },
            {
                "title": "Modelling Initial Value Problems in Python",
                "ns": "0",
                "id": "1128",
                "revision": {
                    "id": "7265",
                    "parentid": "7228",
                    "timestamp": "2023-06-30T06:04:25Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to modelling and solving initial value problems in python.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6754",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Introduction==\nThis is a brief overview of modeling simple initial value problems in Python. Here, we will look into the mathematical concepts and the Python libraries involved. Therefore, a basic understanding of differential equations and programming (Python preferred), would be extremely helpful before diving into the guidelines that follow.\n\n==What are Initial Value Problems (IVP)?==\n\nBefore getting to Initial Value Problems, we describe Ordinary Differential Equations (ODE). An ODE involves unknowns of one(or more) functions of one variable along with its derivatives. A simple example would be:\n\ny' = 6x - 3\n\nwhere y' is the first derivative of the function y w.r.t. x. The solution to an ODE is not a numerical value but a set of functions. For the above example, the set of infinite solutions would be:\n\ny = 6x^2/2 - 3x + c\n\nwhere c is a constant. One can differentiate the above expression or integrate the ODE to verify.\n\nAn Initial Value Problem (IVP), is just an ODE with an initial condition, providing the value of the function at some point in the domain. This allows us to achieve a finite set of solutions for the ODE.\n\nFor instance, if we were given values y=4 at x=0 for the above ODE, we can substitute their values in the solution expression to get c = 4 and thus, to a single solution.\n\n==Modelling ODEs==\nODEs are used to model various practical scenarios. Here, we present an instance in the simplest of terms. These events are more complex in reality and require much more detail.\n\nWe take an instance from motorsport of a racecar braking before entering the pit lane. When entering the pit lane, a racecar needs to lower its speed under a designated limit before the entry line. Our aim here would be to predict if a racecar can stay under the regulations for any given speed and distance remaining after braking.\n\nFirst of the variables, we need to consider is the distance 's' from the entry line. The change of 's' over some interval of time, which is the derivative of 's' w.r.t. time, is the velocity.\n\nv = s'\n\nSimilarly, acceleration is the change in velocity over a certain period.\n\na = v'\na = s''\n\nWe can update distance by velocity, but we still need to determine how the acceleration will vary through the braking period to update the velocity accordingly.\n\nWe will consider the deceleration of the racecar to be incurred by a combination of the 'bkf', the braking force of the car, and 'dgf', the drag which is dependent on the velocity. The forces are negative as they act in the opposite direction.\n\na = -bkf - dgf*v\n\n===SciPy's solve_ivp===\nMoving onto the programming part, we discuss SciPy's 'solve_ivp' function. Ensure that you install the [https://scipy.org/install/ SciPy library] in your system before importing the 'solve_ivp' function as follows.\n\n<syntaxhighlight lang=\"Python\" line>\nfrom scipy.integrate import solve_ivp  \n</syntaxhighlight>\n      \nLooking at the [https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html documentation for 'solve_ivp'], the most important parameter required is the function to update the variables we are interested in. The 'updater' function below computes the changes in distance and velocity as we modeled before.\n\n<syntaxhighlight lang=\"Python\" line>\n# variable update function\ndef updater(t, y):\n\n  bkf = 12 # brakes\n  dwf = 0.3 # drag\n  \n  # variable values for the current iteration\n  s, v = y\n  \n  # computing change in variables for the current iteration\n  dsdt = v\n  dvdt = (-1)*(bkf + dgf*v) if v > 0 else 0\n\n  return dsdt, dvdt\n</syntaxhighlight>\n\nThe functional argument should have the first parameter as the independent variable, which is time in our case. The second parameter is an n-dimensional list containing the current state of all the dependent variables, which are distance and velocity in our case.\n\nThe function should return a list of the same dimension as the second parameter representing the changes in the input variables w.r.t. the independent parameter in respective order.\n\nOther required parameters for 'solve_ivp' include 't_span' & 't_eval' which take in the bounds and specific values for computation for the independent variable.\n\n<syntaxhighlight lang=\"Python\" line>\n# time bounds for 't_span'\nstart, end = 0, 5\n# time step\ndt = 0.25\n# list of desired time values for 't_eval' \ntimes = np.arange(start, end + dt, dt)\n</syntaxhighlight>\n\n'y0' is another required parameter that takes in initial values for our dependent variables.\n\n<syntaxhighlight lang=\"Python\" line>\n# initial values for distance & velocity\ns0 = 0\nv0 = 75\nivs = [s0, v0]\n</syntaxhighlight>\n\nFinally, 'solve_ivp' is called. The attribute 'y' from the return object provides the required solution lists in the input order, whereas attribute 't' gives you back the inputs provided for 't_eval'.\n\n<syntaxhighlight lang=\"Python\" line>\nvals = solve_ivp(fun=updater, t_span=(start, end), y0=ivs, t_eval=times)\n# gives you the list of values for the dependent variables\ns, v = vals.y\n# list of values for the independent variable\nt = vals.t\n</syntaxhighlight>\n\n===Visualization with Matplotlib===\nWe make use of the [https://matplotlib.org/ Matplotlib] library for visualizing our models for simplicity. The visualization we are interested in is between the distance and the velocity.\n\nIn addition to plotting 'v' vs. 's', we plot two lines indicating the distance to the pit entry line for the race car, and the velocity it should be under by that point.\n\n<syntaxhighlight lang=\"Python\" line>\n# plot v vs. s\nfig, axs = plt.subplots()\naxs.grid(False)\naxs.set_facecolor('black')\naxs.plot(s, v, 'bo-')\n\n# lines representing pit entry line and speed limit\naxs.plot([100, 100], [v[0], v[-1]], 'r--', label='Pit lane entry')\naxs.plot([0, max(s[-1], pl_dist + 10)], [20, 20], 'g--', label='Speed limit in pit lane')\n\naxs.set_xlabel('Distance travelled (m)')\naxs.set_ylabel('Velocity (m/s)')\naxs.set_title('Deceleration of a racecar from point of braking')\nplt.legend()\nplt.show()\n</syntaxhighlight>\n\n[[File:IVP.png]]\n\nOur initial values here for the velocity at the time of braking, the distance from the pit entry line, and the speed limit are 75 m/s, 100 m, and 20 m/s respectively. From the graph, we can conclude, the racecar driver will just be able to slow the car down under the speed limit before entering the pit lane.\n\nSticking to motorsport, one follow-up could be to model lap time degradation, which is crucial for a team to decide when to stop for a tyre change for the optimal race strategy.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is [http://spradha1.github.io/portfolio Sanjiv Pradhanang]. Edited by Milan Maushart."
                    },
                    "sha1": "qu6urv1s6y50rw2ksvf4c8yzqt5nkn9"
                }
            },
            {
                "title": "Multi-Criteria Decision Making in Python",
                "ns": "0",
                "id": "1059",
                "revision": {
                    "id": "7270",
                    "parentid": "7121",
                    "timestamp": "2023-07-04T15:33:06Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to multi-criteria decision making in Python, including an example.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11832",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Introduction==\nDecision-making processes are part of our everyday life. Some decisions require very little effort and thought. However, there are real-life problems, we would have to evaluate many criteria for making a decision. The situation becomes more difficult when these criteria conflict with each other!\n\nFor example, a student needs to consider different criteria to choose a subject for an undergraduate program at a university. These criteria include not only the student\u2019s interest but also prospects, location of the university, size of the study program, etc. But we should consider that each of these criteria would have different importance from our point of view.\n\n==Decision Making Process==\nIn general, a decision-making process can be divided into 8 steps as follows:\n\n1. '''Define the problem''': Accurate problem definition is very crucial for effective decision making and all the stakeholders must agree with its definition.\n\n2. '''Determine requirements''': Requirements indicate what conditions must be met for any acceptable solution. If we want to explain it in mathematical form, the constraints describing the set of feasible solutions are considered to be requirements of the decision problem.\n\n3. '''Establish goals''': The goals we define can be inconsistent and conflicting. This is very common in the real world. For this reason, various decision-making methods have been developed.\n\n4. '''Identify alternatives''': As we collect information about our problem, we would identify several possible alternatives which lead us to our desired conditions.\n\n5. '''Define criteria''': In this step, we determine the decision criteria that evaluate and compare our alternatives. Criteria should be chosen to represent the preferences of potential stakeholders. Criteria should have the following features:\n\n  * Ability to discriminate between alternatives,\n  * Comprehensive and determined based on our goals,\n  * Non-redundant (independent),\n  * Not be too large in number, just a few!\n6. '''Choose a decision-making tool''': To be able to choose the right tool and methods, we must consider the definition of the problem and also the goals of the decision-makers.\n\n7. '''Evaluate alternatives against criteria''': By using the decision-making tool, it is possible to rank the alternatives or get a subset of the possible solutions.\n\n8. '''Validate solutions against the problem statement''': In the last step, we must check whether the chosen options meet the requirements and goals of the decision-making problem.\n\n== Multi-Criteria Decision Making==\nWhen there is a complex problem and we must evaluate multiple conflicting criteria, multi-criteria decision making (MCDM) as a sub-discipline of operations research, leads us to more informed and better decisions by making the weights and associated trade-offs between the criteria. In general, MCDM is used to reduce the incidence of decision-maker bias as well as the lack of success in group decision-making.\n\nThe MCDM is divided into two categories:\n1. '''Multi-attribute decision making (MADM)''': This category concentrates on problems with discrete decision spaces, ranking the alternatives. In these studies, the set of alternatives has been predetermined.\n\n2. '''Multi-objective decision making (MODM)''': In this category, several competing objectives are required to be optimized simultaneously and decision space is continuous.\n\nFigure (1) below shows the classification of MCDM methods:\n\n[[File: MCDM.jpg|900px]]\n\n'''Figure (1)'''. MAV(U)T: multi-attribute value(utility) theory; SAW: simple additive weighting; AHP/ANP: Analytical hierarchy(network) process; TOPSIS: a technique for order of preference by similarity to ideal solution; ELECTRE TRE: elimination and choice expressing reality; PROMETHEE: preference ranking organization method for enrichment evaluation; LP: linear programming; ILP: integer linear programming; NILP: non-integer linear programming; MILP: mixed integer linear programming; GP: goal programming; CP: compromise programming; D_models: dynamic models; SA: simulating annealing; TS: tabu search; GRASP: greedy randomized adaptive search procedure; HPSO: hybrid particle swarm optimization; NSGA: non-dominated sorted genetic algorithm\n\n==Example==\nIn this part, we want to solve a MADM problem. Suppose you want to enroll in a fitness center. There are 4 centers in your city (A, B, C, D). The criteria that influence your decision are as follows:\n\nOpening hours(hours/day)\nLocation (Distance from your home (km))\nEquipment (Basic: 1, Intermediate:2, Advanced:3)\nMembership fee(fee/month)\nCleanliness (Excellent:4, Good:3, Sufficient:2, Inadequate:1)\n\nThe weight of each criterion is also as follows:\n\nOpening hours weight: 0.15\nLocation weight: 0.2\nEquipment weight: 0.35\nMembership fee weight: (0.1\nCleanliness weight: 0.2\nWe can create the decision matrix with pandas and specify the score/rate of each criterion for each alternative.\n\n<syntaxhighlight lang=\"Python\" line>\n# Create a Pandas DataFrame from a Numpy array and specify the index column and column headers\n\n# import required libraries\nimport numpy as np\nimport pandas as pd\n\n# creating a numpy array (score of each criterion)\nrate = np.array([[24, 5, 2, 20, 3],\n                [12, 2, 1, 13, 2],\n                [6, 3, 2, 15, 2],\n                [12, 7, 3, 19, 4]])\n\n# generating the Pandas data frame\n\ndecision_matrix = pd.DataFrame(data = rate,\n                        index = [\"A\", \"B\", \"C\", \"D\"],\n                        columns = [\"opening_hours\",\n                                \"location\", \"equipment\", \"membership_fee\",\"cleanliness\"])\n\n# printing the data frame\nprint(decision_matrix)\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n#Importance of each criterion\n\nimport plotly.express as px\nimport pandas as pd\nweights=[0.15, 0.2,0.35,0.1 ,0.2]\ndf = pd.DataFrame(dict(\n    w_vector=weights,\n    criteria=['Opening hours','Location','Equipment','Membership Fee','Cleanliness']))\nfig = px.line_polar(df, r='w_vector', theta='criteria', line_close=True,\n                   title=\"Importance of Each Criterion\")\n\nfig.update_traces(fill='toself')\n\nfig.show()\n</syntaxhighlight>\n\nThis code produces a polar plot showing the different weighting of the criteria. First, the weights are defined. Then, a dictionary with the two keys \"w_vector\" and \"criteria\" are created within a pandas data frame. This information is then used to create the polar plot, whereas r stands for radius and describes the distance of the line between the center and the outer boundary and theta divides the circle into five equal parts to depict each criterion.\n\nThe next step is applying a MADA method. We implement the Simple Additive Weighting (SAW) method for this problem. The basic concept of the SAW method is to find the weighted sum of ratings on each alternative on all attributes.\n\nIn this example, opening hours, equipment, and cleanliness are benefit criteria while location and membership fee are cost criteria. A benefit criterion means that the higher an alternative scores in terms of it, the better the alternative is. The opposite is considered true for the cost criteria.\n\nAll criteria must be either benefit or cost criteria in order to use the SAW method. For this reason, we reverse the values of membership fee and location.\n\n<syntaxhighlight lang=\"Python\" line>\ndecision_matrix['location'] = 1/decision_matrix['location']\ndecision_matrix['membership_fee']= 1/decision_matrix['membership_fee']\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndecision_matrix\n</syntaxhighlight>\n\nThe SAW method requires the process of normalizing the decision matrix. We use Max-Min techniques.\n\n<syntaxhighlight lang=\"Python\" line>\n# Normalization is a process for rescaling the real values of a numeric attribute into a range from 0 to 1\nfrom sklearn.preprocessing import normalize\ndecision_matrix_norm = normalize(decision_matrix, axis=0, norm='max')\ndecision_matrix_norm\n</syntaxhighlight>\n\nThe final result is obtained from ranking the sum of the normalized matrix multiplication by the weights:\n\n<syntaxhighlight lang=\"Python\" line>\nresult = np.dot(weights,decision_matrix_norm.transpose())\nresult_index = pd.DataFrame(data = result,\n    index = [\"A\", \"B\", \"C\", \"D\"],\n    columns = [\"value\"])\nresult_sorted = result_index.sort_values(by=['value'], ascending = False)\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nresult_sorted\n</syntaxhighlight>\n\nThe np.dot command multiplies matrices, in our case the two-dimensional matrix of the normalized values with the 1-dimensional matrix of the weights. The matrix of the normalized values also needed to be transposed so that the number of rows and columns of the two matrices are the same and can be multiplied. Then, we gave names to the rows (\"index\") and the column (\"value\") and sorted them by the values in ascending order. We can also plot this with the following code:\n\n<syntaxhighlight lang=\"Python\" line>\nresult_sorted['value'].plot(kind='bar', xlabel='Fitness Center', ylabel= 'Value',rot=0,colormap='coolwarm')\n</syntaxhighlight>\n\nIn this example, alternative D has the highest value.\n\nWe can also solve the above example with the \"mcdm\" package. The package can be installed from \"PyPI\" or from its \"GitHub\" repository. This package includes the different methods of scoring, weighting, correlation, and normalization.\n\n<syntaxhighlight lang=\"Python\" line>\nimport mcdm\nalt_names = [\"A\", \"B\", \"C\", \"D\"]\nmcdm.rank(decision_matrix_norm, alt_names=alt_names, w_vector= weights, s_method=\"SAW\")\n</syntaxhighlight>\n\nThe result shows that Fitness Center D has the highest value.\n\n==Advantages & Limitations==\nA well-designed MCDM provides an appropriate and cost-effective way for reducing a large number of options. It also increases transparency in decision-making. In this method, we can use both quantitative and qualitative evaluation criteria. However, if there are diverse alternatives, it would be difficult to find the same criteria that can evaluate all of these alternatives. Moreover, since the choice of criteria and scoring sometimes requires judgment by experts and decision-makers, this judgment is subject to bias, which can reduce the quality of decision-making.\n==References==\nAytekin, Ahmet. \"Comparative analysis of the normalization techniques in the context of MCDM problems.\" Decision Making: Applications in Management and Engineering 4.2 (2021): 1-25.\nAvailable online: https://pypi.org/project/mcdm/\n\n\u010cernevi\u010dien\u0117, Jurgita, and Audrius Kaba\u0161inskas. \"Review of multi-criteria decision-making methods in finance using explainable artificial intelligence.\" Frontiers in artificial intelligence 5 (2022).\n\nF\u00fcl\u00f6p, J\u00e1nos. \"Introduction to decision making methods.\" BDEI-3 Workshop, Washington . 2005 Available online: https://www.1000minds.com/decision-making/what-is-mcdm-mcda\n\nGebre, Sintayehu Legesse, et al. \"Multi-criteria decision making methods to address rural land allocation problems: A systematic review.\" International Soil and Water Conservation Research 9.4 (2021): 490-501.\n\nKazimieras Zavadskas, Edmundas, Jurgita Antucheviciene, and Samarjit Kar. \"Multi-objective and multi-attribute optimization for sustainable development decision aiding.\" Sustainability 11.11 (2019): 3069.\n\nMontibeller, Gilberto, and Detlof von Winterfeldt. \"Biases and debiasing in multi-criteria decision analysis.\" 2015 48th Hawaii International Conference on System Sciences. IEEE, 2015.\n\nTriantaphyllou, Evangelos, et al. \"Multi-criteria decision making: an operations research approach.\" Encyclopedia of electrical and electronics engineering 15.1998 (1998): 175-186.\n\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "9prfu7x7cek17qfjt67u1ozw01bquz3"
                }
            },
            {
                "title": "Multiple Regression in Python",
                "ns": "0",
                "id": "1065",
                "revision": {
                    "id": "7280",
                    "parentid": "7267",
                    "timestamp": "2023-08-29T09:36:38Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article goes through an example for preparing, running, and interpreting a multiple regression model.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16965",
                        "#text": "Multiple Regression in Python\n'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Introduction==\nIn this article, we will go through the whole process of running a multiple regression in Python. This includes the choice of the relevant packages, inspecting and cleaning data, creating descriptive data, writing the multiple regression model, and interpreting its results. Generally, a multiple regression analysis can be understood as a model that identifies a relationship between one dependent variable and several independent variables, while taking the effects of all independent variables into account. The prediction you receive regarding the change in the dependent variable by one unit increase of a dependent variable is assumed to be linear. That means that no matter how high the dependent or independent variable is, the change is always the same. An example of multiple regression analysis is an analysis of how the number of hours spent for domestic unpaid work is influenced by gender, the number of children in the household, the number of paid hours worked, marital status, and educational level. The dependent variable is the hours of domestic unpaid work, and all the other variables are independent variables. For this example, we use a different example looking at the likelihood to receive admission for a university based on several independent variables. The dataset has 400 entries.\nOur independent variables are: \n* GRE Scores ( out of 340 )\n* TOEFL Scores ( out of 120 )\n* University Rating ( out of 5 )\n* Statement of Purpose (SOP) and Letter of Recommendation (LOR) Strength (out of 5)\n* Undergraduate GPA ( out of 10 )\n* Research Experience ( either 0 or 1 )\n* Chance of Admit ( ranging from 0 to 1 )\n\nThe dataset can be found here: https://www.kaggle.com/datasets/akshaydattatraykhare/data-for-admission-in-the-university (retrieved: 17.12.2022)\n\n== First steps==\nFirstly, we have to import all relevant packages for analysis and visualization.\n\n<syntaxhighlight lang=\"Python\" line>\nimport pandas as pd ## needed for organizing our data\nimport matplotlib.pyplot as plt ## needed for data visualization\nimport seaborn as sns ## needed for data visualization\nimport statsmodels.api as sm ##needed to run the multiple regression\nimport scipy.stats ## for checking your multiple regression model\nfrom scipy.stats import jarque_bera ##for checking your multiple regression model\nimport statsmodels.api as sm ## for checking your multiple regression model\nfrom statsmodels.stats.diagnostic import het_breuschpagan ## for checking your multiple regression model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor ##needed to test for multicollinearity\n</syntaxhighlight>\n\n==Data Inspection and Cleaning==\nIn the next step, we load our dataset and print some general information about it, such as column names, shape, number of NAs, and the datatypes of the column.\n\n<syntaxhighlight lang=\"Python\" line>\ndf = pd.read_csv(\"C:/Users/Dell Latitude/Downloads/archive/adm_data.csv\")## make sure you fill in the correct working directory to let python know where the data is. I have here made an exemplary path.\nprint(df.columns) ###shows the columns\nprint(df.shape)##tells you the number of rows and columns (first rows, then columns)\nprint(df.isnull().sum()) ## tells you the number of missing values per column\nprint(df.dtypes)## tells you the data types\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nprint(df.head)## you can see here the first five and the last five rows to check the structure of the data\n</syntaxhighlight>\n\n==Data Visualization==\nWe can see that in this case, the dataset does not contain any NA values (\"not available\", values that are missing), which is why we can skip the step of dropping or filling in the missing values. In the next step, we are going to do a first visualization of our data. To keep it simple, we are going to leave out the categorical data (LOR, SOP & University Ranking). However, to use categorical data in the case of a regression, the variables could be coded into dummy variables containing 1 & 0's. You can find a wiki entry on this topic, here: https://sustainabilitymethods.org/index.php/Dummy_variables\n\n<syntaxhighlight lang=\"Python\" line>\nfig, axs = plt.subplots(2, 2, figsize = (12,15))\nsns.histplot(ax=axs[0, 0], data=df, x=\"GRE Score\", bins = 10).set(title=\"Amount of students with a specific GRE Score\")\nsns.histplot(ax=axs[1, 0], data=df, x=\"TOEFL Score\", bins = 10).set(title=\"Amount of students with a specific TOEFL Score\")\nsns.histplot(ax=axs[1, 1], data=df, x=\"CGPA\", bins = 10).set(title=\"Amount of students with a specific CGPA\")\nsns.histplot(ax=axs[0, 1], data=df, x=\"Chance of Admit \", bins = 10).set(title=\"Chance of Admittance of students\")\n\nfig, axs = plt.subplots(2,1, figsize = (10,10))\nsns.scatterplot(ax = axs[0], data = df, x = \"CGPA\", y= \"Chance of Admit \").set(title=\"Chance of Admittance of a student with a specific CGPA\")\nsns.scatterplot(ax = axs[1], data = df, x = \"GRE Score\", y= \"Chance of Admit \").set(title=\"Chance of Admittance of a student with a specific GRE Score\")\n\nfig, axs = plt.subplots(figsize = (5,5))\nsns.boxplot(data = df, x=\"Research\", y = \"Chance of Admit \").set(title=\"Chance of Admittance with Research experience (1) vs. no experience (0)\")\nplt.show()\n</syntaxhighlight>\n\nThe Histograms visualize the distribution of the continuous variables TOEFL Score, GRE Score, CGPA and the Chances of Admittance. Here, they are used to check whether our data is roughly normally distributed. \nIn the scatter plot, we can do a first analysis of the relationship between the variables CGPA and GRE Scores and the Chances of Admittance. The plots showcase that there could be a significant linear relationship between the variables. \nLastly, the boxplot is used to look at the only categorical variable left, which is Research. The variable is coded with 0 and 1, to indicate whether a student already has some research experience or not. Based on the boxplots, the variable Research might have a significant impact on the chance of admittance. Therefore, we include the variable in the regression later. \n\nTo test the continuous variables for possible correlations, we use the seaborn heatmap function to visualize the correlation between multiple variables. With this tool, we can also check for possible multicollinearity of the variables, so correlation among the independent variables. The scale on the right shows the correlation coefficient ranging from +1 to -1.\n\n<syntaxhighlight lang=\"Python\" line>\ndf_continous = df[[\"CGPA\",\"GRE Score\", \"TOEFL Score\", \"Chance of Admit \"]]\nsns.heatmap(df_continous.corr(), vmin=-1, vmax=1, annot=True, cmap = \"PiYG\")\nplt.show()\n</syntaxhighlight>\n\nWe can see in the heatmap that all the variables are highly correlated with each other. Another tool we can use to check for multicollinearity is the Variance Inflation Factor. To test for multicollinearity using the VIF score, we create a new table containing the variables and the VIF. To calculate the score, the variance_inflation_factor function from the statsmodels module is used.\n\n<syntaxhighlight lang=\"Python\" line>\ndata = df[[\"CGPA\", \"TOEFL Score\", \"GRE Score\"]]\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = data.columns\n\nfor i in range(len(data.columns)):\n    vif_data[\"VIF\"] = variance_inflation_factor(data.values, i)\n\nprint(vif_data)\n</syntaxhighlight>\n\nOne recommendation for the VIF score is interpreting a score higher than 5 on a variable as a sign that this variable is correlated with the other given variables.\nAs expected based on the heatmap, all of the variables have a high VIF score, which we interpret as high multicollinearity. Therefore, we drop 2 of the 3 variables for the linear regression. In this case, the GRE Score and the TOEFL Score are dropped. \n\n==Regression Analysis==\nNow we can do the regression analysis. We use the variables CGPA and Research as predictor or independent variables for the model and the Chance of Admittance variable as our dependent variable. If we get a meaningful and significant model in the end, it allows us to make predictions on the chances of admittance of a student based on their CGPA and whether they already have some research experience or not. \n\n<syntaxhighlight lang=\"Python\" line>\nx = df[[\"CGPA\", \"Research\"]]\nx = sm.add_constant(x) \ny = df[\"Chance of Admit \"]\n\nmodel = sm.OLS(y, x).fit()\nprint(model.summary())\n</syntaxhighlight>\n\n==Interpretation of the Regression Results==\n* '''P>|t|''' is the p-value. We can see, that the p-value of our variables is very close to 0, which corresponds to our model being highly significant. Therefore, we can reject the null hypothesis (our coefficients are equal to 0). \n\n* '''coef''' are the regression coefficients of our model. We interpret the coefficient as the effect the independent variable has on the dependent variable. The higher the coefficient, the bigger the magnitude of the effect of the variable. We would also conclude that the variables have a positive effect on the dependent variable, meaning that if the CGPA is higher, the chance of admission also tends to be higher. Furthermore, we can see that the CGPA seems to make a larger impact on the chance of admission than the Research variable. \n\n* '''R-squared and Adjusted R-squared''' evaluates how well our regression model fits our data and is always represented with values between 0 and 1. Therefore, a higher R-squared value indicates smaller differences between our data and the fitted values. In our example, an adjusted R-squared value of .775 means that our model fits our data reasonably well.\n\n* '''AIC''' The Akaike information criterion cannot be interpreted by itself. Normally, we could use the AIC to compare our model against other different models. One common case is a comparison against the null model. The formula of the AIC weighs the number of parameters against the Likelihood function, consequently favoring models that use the least amount of parameters to model our data the best. Therefore, it corresponds to the principle of Occam's razor, which you can read more about here: https://sustainabilitymethods.org/index.php/Why_statistics_matters#Occam.27s_razor. For evaluation and selection of models, it is often used as an alternative to p-values.\nConsequently, the regression formula can be written down as: \n'''Chance of Admittance = CGPA * 0.1921 + Research * 0.0384 - 0.9486''', which can now be used to predict the chance of admittance of an undergraduate based on their research experience and their CGPA. \n\nLastly, we need to make some checks to see if our model is statistically sound. We will check if the residuals are normally distributed, for heteroscedasticity, and serial correlation. \nTo check for normal distribution of the residuals, we take a look at the QQ-Plot to look at the distribution of the residuals. It should follow the shape of the red line roughly, to assume normal distribution of residuals. The QQ Plot compares the theoretical quantiles of the normal distribution with the residual quantiles. If the distributions are perfectly equal, meaning the residuals are perfectly normally distributed, the points will be perfectly on the line. You can find out more about QQ-Plots here: https://sustainabilitymethods.org/index.php/Data_distribution#The_QQ-Plot\n<syntaxhighlight lang=\"Python\" line>\nresiduals = model.resid\nfig = sm.qqplot(residuals, scipy.stats.t, fit=True, line=\"45\")\nplt.show()\n</syntaxhighlight>\n\nThis can also be tested using the Jarque-Bera test. A Jarque-Bera test compares the kurtosis und skewness of the distribution of your variable with the properties a normal distribution has.\nThe lower the value of the Jarque-Bera test, the more likely the residuals are normally distributed. If the p-value is above your chosen significance level (e.g., 0.05), you can assume that your residuals are normally distributed.\n<syntaxhighlight lang=\"Python\" line>\njarque_bera(df[\"CGPA\"])\n</syntaxhighlight>\n\nAs you can see, the value of the Jarque-Bera test is quite small and the p-value is even above 0.1. It can therefore not be said that the residuals do not follow a normal distribution.\nInstead, we assume that your variable CGPA follows a normal distribution. Note that you cannot do the test for binary variables since the Jarque-Bera test assumes that the distribution of residuals is continuous.\n\nNext, we should test for heteroscedasticity. In our regression model, we have assumed homoscedasticity which means that the variance of the residuals is equally distributed. The residuals are the difference between your observations from your predictions.\nIf this is not the case, you have heteroscedasticity. This is often the case because of outliers or skewness in the distribution of a variable. You can assess this visually by plotting the variance of your residuals against an independent variable. Again here, this only makes sense for continuous variables, which is why we look at GCPA.\n\n<syntaxhighlight lang=\"Python\" line>\n# Calculate the residuals\nresiduals = model.resid\n\n# Calculate the squared residuals ( to only have positive values)\nsquared_resid = residuals ** 2\n\n# Group the squared residuals by the values of each independent variable\ngrouped_resid = squared_resid.groupby(x['CGPA'])\n\n# Calculate the variance of the squared residuals for each group\nvar_resid = grouped_resid.var()\n\n# Plot the variance of the squared residuals against the values of each independent variable\nplt.scatter(var_resid.index, var_resid.values)\nplt.xlabel('CGPA')\nplt.ylabel('Variance of Squared Residuals')\nplt.show()\n</syntaxhighlight>\n\nWe can see that there are some outliers that might cause heteroscedasticity. We can also check this with the Breusch-Pagan test. \nIf the p-value is lower than your chosen significance level (e.g., 0.05), we need to reject the null hypothesis that we have homoscedasticity.\nWe would then treat the model to be heteroscedastic.\n\n<syntaxhighlight lang=\"Python\" line>\n##create the multiple regression model\nx = df[[\"CGPA\"]]\nx = sm.add_constant(x) \ny = df[\"Chance of Admit \"]\n\nmodel = sm.OLS(y, x).fit()\n\n# perform the Breusch-Pagan test\nbp_test = het_breuschpagan(model.resid, model.model.exog)\n\n# print the results\nprint(\"Breusch-Pagan test p-value:\", bp_test[1])\n</syntaxhighlight>\n\nThe test shows that we need to reject the assumption of homoscedasticity. We assume heteroscedasticity. \nBefore we adapt the model, we should also check for serial correlation.\nSerial correlation is the case in which error terms across time or observations are correlated (in our case across observations).\nIf we look at our regression output again, we can check for this using the Durbin-Watson test statistic. Generally speaking, if the \nvalue is around 2, there is no serial correlation, if it is 0, there is a perfect positive serial correlation, if it is 4, there is a perfect\nnegative correlation. The exact values for the number of independent variables and observations you can find [https://real-statistics.com/statistics-tables/durbin-watson-table/ here]. Our Durbin-Watson\ntest value is 0.839 and we therefore have reason to assume a positive serial correlation. We therefore need to correct for this telling \nthat there is serial correlation (covariance type of H3).\n\n## Now, we need to correct for heteroscedasticity and serial correlation\n<syntaxhighlight lang=\"Python\" line>\nx = df[[\"CGPA\", \"Research\"]]\nx = sm.add_constant(x) \ny = df[\"Chance of Admit \"]\n\nmodel = sm.OLS(y, x)\n\nnw_model = model.fit(cov_type='HAC', cov_kwds={'maxlags': 20})\n\n# print the results\nprint(nw_model.summary())\n</syntaxhighlight>\n\nHAC corrects for heteroscedastic and the additional information \"cov_kwds={'maxlags': 20)\" corrects for serial correlation. \nAs a rule of thumb for our data, you can set the lags to half of your dataset size or the square root. In our case, this makes \nno difference. You can check it out. If you have the case of serial correlation in another dataset (especially if you\nhave time series data), you might have to perform other analytical tasks to get the correction right. [https://towardsdatascience.com/advanced-time-series-analysis-in-python-decomposition-autocorrelation-115aa64f475e Here] is a good start for that.\nAs you can see in the results, nothing has really changed. Even though this seems odd and it seemed like a lot of unnecessary work,\nit is important to do these diagnostic checks and correct your model if needed. In another regression model, you might\nget completely different results after e.g., correcting for heteroscedasticity.\n\n==Bibliography==\n* \"Statsmodels.stats.outliers_influence.variance_inflation_factor.\u201d statsmodels, December 10, 2022. https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is Shirley Metz. Edited by Milan Maushart"
                    },
                    "sha1": "e8j2gjbb1cl5zytdb9249qb35mc04kb"
                }
            },
            {
                "title": "Narrative Research",
                "ns": "0",
                "id": "624",
                "revision": {
                    "id": "5931",
                    "parentid": "5929",
                    "timestamp": "2021-06-30T17:41:20Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Background */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20965",
                        "#text": "[[File:ConceptNarrativeResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Narrative Research]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Narrative Research describes qualitative field research based on narrative formats which are analyzed and/or created during the research process.\n\n== Background ==\n[[File:Narrative Research.png|400px|thumb|right|'''SCOPUS hits per year for Narrative Research until 2020.''' Search terms: 'Narrative Research', 'narrative inquiry', 'narrative analysis' in Title, Abstract, Keywords. Source: own.]]\n'''[[Glossary|Storytelling]] has been a way for humankind to express, convey, form and make sense of their reality for thousands of years''' (Jovchelovitch & Bauer 2000; Webster & Mertova 2007). 'Storytelling' is defined as the distinct tonality, format and presentation in which a story is told. The term 'narrative' includes both: the story itself, with its dramaturgy, characters and plot, as well as the act of storytelling (Barrett & Stauffer 2009). However, the term 'narrative' has been used used predominantly as a synonym for 'story' in academia for decades (Barrett & Stauffer 2009).\n\n'''Psychologist Jerome Bruner introduced the notion of 'narrative' as being one of two forms of distinct modes of thinking in 1984''' - the other being the 'logico-scientific' mode (Barrett & Stauffer 2009). While the latter is \"(...) more concerned with establishing universal truth conditions\" (Barrett & Stauffer 2009, p.9), the 'narrative' mode represents the broad human experience of reality. This distinction led to further investigation on the idea that 'narratives' are a central form of human learning about - and [[Glossary|sense-making]] of - the world. Scholars began to recognize the role of analyzing narratives in order to understand individual and societal experiences and the meanings that are attached to these. This led e.g. to the establishment of the field of narrative psychology.\n\n'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).\n\n== What the method does ==\n'''First, there is a distinction to be made:''' 'Narrative' can refer to a form of Science Communication, in which research findings are presented in a story format (as opposed to classical representation of data) but not extended through new insights. 'Narrative' can also be understood as a form of scientific inquiry, generating new knowledge during its application. This entry will focus on the latter understanding.\n\n'''Next, it should be noted that Narrative Research entails different approaches''', some of which are very similar to other forms of qualitative field research. In this Wiki entry, the distinctiveness of Narrative Research shall be accounted for, whereas the connectedness to other methods is mentioned where due.\n\nNarrative Research -  or 'Narrative Inquiry' - is shaped by and focussing on a conceptual understanding of 'narratives' (Barrett & Stauffer 2009, p.15). Here, 'narratives' are seen as a format of [[Glossary|communication]] that people apply to make sense of their life experiences. \"Communities, social groups, and subcultures tell stories with words and meanings that are specific to their experience and way of life. The lexicon of a social group constitutes its perspective on the world, and it is assumed that narrations preserve particular perspectives in a more genuine form\" (Jovchelovitch & Bauer 2000, p.2). '''Narratives are therefore not merely forms of representing a chain of events, but a way of making sense of what is happening.''' Through the telling of a story, people link events in meaning. The elements that are conveyed in the story, and the way these are conveyed, indicates how the story-teller - and/or their social surrounding - sees the world. They are a form of putting reality into cultural and individual perspective. Also, narratives are never final but change over time as new events arise and perspectives develop (Jovchelovitch & Bauer 2000, Webster & Mertova 2007, Squire et al. 2014, Moen 2006). \n\nNarrative Research is \"(...) the study of stories\" (Polkinghorne 2007, p.471) and thus \"(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience.\" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the 'narratology', i.e. the structure and grammar of a story; the 'narrative content', i.e. the themes and meanings conveyed through the story; and/or the 'narrative context', which revolves around the effects of the story (Squire et al. 2014).\n\n'''One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.''' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, 'narrative' is the underlying \"frame of reference\" (Moen 2006, p.57) for the research. An example for the latter understanding is the 'Narrative Interview'.\n\n==== Narrative Interviews ====\n[[File:Narrative Research Phases.png|400px|thumb|right|'''Basic phases of the narrative Interview.''' Source: Jovchelovitch & Bauer 2000, p.5.]]\n\nThe Narrative Interview is an Interview format that \"(...) encourages and stimulates an interviewee (...) to tell a story about some significant event in their life and social context.\" (Jovchelovitch & Bauer 2000, p.3). 'Narrative Interviewing' \"is considered a form of unstructured, in-depth interview with specific features.\" (Jovchelovitch & Bauer 2000, p.4) To this end, it is a form of the [[Open Interview]], which - compared to [[Semi-structured Interview|Semi-structured]] and [[Survey|Structured Interviews]] - is relatively free from deductively pre-developed question schemata: \"To elicit a less imposed and therefore more 'valid' rendering of the informant's perspective, the influence of the interviewer should be minimal (...) '''The [Narrative Interview] goes further than any other interview method in avoiding pre-structuring the interview.''' (...) It uses a specific type of everyday communication, namely story-telling and listening, to reach this objective.\" (Jovchelovitch & Bauer 2000, p.4) Here, it is central that the interviewer appreciate the interviewee's perspective, by using the subject's language and by posing \"as someone who knows nothing or very little about the story being told, and who has no particular interests related to it\" (ibid, p.5). The interview is then transcribed and analyzed using different forms of coding (see Content Analysis), with a focus on the narrative elements. For more information on the methodological foundations of conducting and analyzing narrative Interviews, please refer to Jovchelovitch & Bauer 2000.\n\n==== Narrative Inquiry as collaborative story-creation ====\nIn a third understanding of Narrative Research, which is most commonly referred to as 'Narrative Inquiry', 'narratives' are more than the underlying phenomenon - they are a central methodological element. ''''This approach dissolves the barrier between researcher and subject further, and the collaboration between both is central to the methodological design''' (Clandinin 2006, Moen 2006). This type of narrative research does not apply an 'outsider's perspective', but instead is \"(...) collaboration between researcher and participants, over time, in a place or series of places, and in social interaction with milieus. An inquirer enters this matrix in the midst and progresses in the same spirit, concluding the inquiry still in the midst of living and telling, reliving and retelling, the stories of the experiences that made up people's lives, both individual and social.\" (Clandinin 2006, p.20, citing Clandinin & Connelly 2000). To this end, Clandinin (2006) distinguishes between the re-telling of stories that Interview participants tell the researchers, and the telling of stories that the researchers experience themselves, e.g. in ethnographic studies. The difference is not so much of methodological nature as it is in the purpose and perspective of the research (Barrett & Stauffer 2009). In this understanding, 'Narrative Inquiry' is a reflexive and iterative process, with the researchers entering into a field of experiences, telling their own stories, telling the stories of other participants, and dialogically co-creating joined stories with them (Moen 2006). The created narratives serve as a way of presenting the research experiences, but also as forms of data for the analysis of the joint and conveyed experiences.\n\nIn terms of practical methodology, this form of narrative inquiry is very closely related to methods of [[Ethnography]], which are based on the active [[Glossary|participation]] in the field, i.e. the social situation of interest, and the creation of [[:Category:Qualitative|qualitative]] data in form of field notes. The distinctive component of Narrative Inquiry is the focus on narratives. Clandinin (2006, p.47f) explains the methodological approach as following: \"As we enter into narrative inquiry relationships, we (...) negotiate relationships, research purposes, transitions, as well as how we are going to be useful in those relationships. These negotiations occur moment by moment, within each encounter, sometimes in ways that we are not awake to\" or \"in intentional, wide awake ways as we work with our participants throughout the inquiry. As we live in the field with our participants, whether the field is a classroom, a hospital room or a meeting place where stories are told, we begin to compose (...) a range of kinds of field texts from photographs, field notes, and conversation transcripts to Interview transcripts. As narrative inquirers work with participants we need to be open to the myriad of imaginative possibilities for composing field texts. (...)  As we continue to negotiate our relationships with participants, at some points, we do leave the field to begin to compose research texts. This leaving of the field and a return to the field may occur and reoccur as there is a fluidity and recursiveness as inquirers compose research texts, negotiate them with participants, compose further field texts and recompose research texts.\" Data may be gathered in form of \"field notes; journal records; interview transcripts; one's own and other's observations; storytelling; letter writing; autobiographical writing; documents (...); and pictures\" (Moen 2006, p.61) and analyzed by forms of Content Analysis.\n\n== Strengths & Challenges ==\n* Narratives have their own inherent structure, formed by the narrating individual. Therefore, while narrative inquiry itself provides the benefits and challenges of a very open, reflexive and iterative research format, it is not non-structured, but gains structure by itself (see Jovchelovitch & Bauer 2000)\n* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research \"(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates\". Further, \"Narrative inquiry attempts to capture the 'whole story', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important 'intervening' stages\" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).\n* '''The insights gained through narratives are subjective to the narrator, which implies advantages and challenges.''' Compared to an 'objective' description of, e.g. a chain of events, the narration provides insights about the individual's interpretation and experience of the events, which may be inaccessible elsewhere, and shine light on complex social phenomena: \"Narrative is not an objective reconstruction of life - it is a rendition of how life is perceived.\" (Webster & Mertova 2007, p.3; see Moen 2006, p.62). However, this subjective representation of events or a situation may be distorted and differ from the 'real' world. Squire et al. (2014) refer to this distinction as different forms of 'truth' that researchers may be interested in: either representations of the physical world or of social realities which present the world through the lense of the narrator. Jovchelovitch & Bauer (2000, p.6) suggest that the researchers take both elements into consideration, first fully engaging with the subjective narrative, then comparing it to further information on the physical 'truth'. They should try \"(...) to render the narrative with utmost fidelity (in the first moment) and to organize additional information from different sources, to collate secondary material and to review literature or documentation about the event being investigated. Before we enter the field we need to be equipped with adequate materials to allow us to understand and make sense of the stories we gather.\" Moen (2006, p.63), by comparison, explains that \"(...) there is no static and everlasting truth\", anyway.\n* This [[Bias and Critical Thinking|conflict between different 'truths']] also has consequences for the quality criteria for Narrative Inquiry, especially for those research endeavors that create narratives themselves. To this end, 'usefulness' and 'persuasiveness' of the created narratives have been suggested as quality criteria (Barrett & Stauffer 2009) or, as Webster & Mertova (2007, p.4) put it: \"Narrative research (...) does not strive to produce any conclusions of certainty, but aims for its findings to be 'well grounded' and 'supportable' (...) Narrative research does not claim to represent the exact 'truth', but rather aims for 'verisimilitude'\". (For more thoughts on validity in Narrative Inquiry, see Polkinghorne (2007)). For the analysis of narratives, a 'trustworthy' set of field notes and Interview data may serve as a measure of quality, which the researchers created through prolonged engagement in the field, triangulation of different data sources and the active search for disconfirmation of one's research results (see Moen 2006, p.64). Also, researchers \"(...) need to cogently argue that theirs is a viable interpretation grounded in the assembled texts\" (Polkinghorne 2007, p.484).\n* Further challenges may arise during the active collaboration of the researcher in the field. For example, \"(...) the researcher and the research subjects interpret specific events in different ways or (...) the research subjects question the interpretive authority of the researcher\" (Moen 2006, p.62). Further comparable issues of qualitative field research are noted in the entry on [[Ethnography]].\n\n\n== Normativity ==\n* As mentioned before, Narrative Inquiry methodology is in many regards similar to methods in [[Ethnography]], and shares elements with [[Hermeneutics]], [[Content Analysis|Qualitative Content Analysis]], and [[Open Interview|Open Interviews.]] Generally, it can be seen as a purely qualitative and inductive approach that focuses on limited temporal and spatial scales.\n* \"(...) [A] distinguishing feature of Narrative Research is a \"(...) move away from an objective conception of the researcher-researched relationship\" (...) to one in which the researcher is deeply involved in the research relationship. (...) In this process, narrative inquiry, becomes to varying degrees a study of self, or self alongside others, as well as of the inquiry participants and their experience of the world.\" (Barrett & Stauffer 2009, p.11f, quote from Pinnegar & Daynes 2007, p.11). '''The autobiography of the researcher, as well as personal beliefs and practices and ethical positionality is brought into the research endeavor, which should be acknowledged in the process and results.''' \"Narrative inquirers cannot bracket themselves out of the inquiry but rather need to find ways to inquire into participants\u2019 experiences, their own experiences as well as the co-constructed experiences developed through the relational inquiry process.\" (Clandinin 2006, p.47)\n* Within this interaction in the field, the researcher must be aware of the consequences of creating new narratives in the studied subjects and situations, which is why Clandinin (2006, p.53) mentions \"(...) the importance of thinking in responsive and responsible ways about how narrative inquiry can shift the experiences of those with whom we engage\". In addition, Moen (2006, p.62) mentions that the narratives written down as a result of the research inquiry are themselves subject to interpretation: \"The story has been liberated from its origin and can enter into new interpretive frames, where it might assume meanings not intended by the persons involved in the original event. (...) [T]he narrative that is fixed in a text is thus considered an \u201copen work\u201d where the meaning is addressed to those who read and hear about it. Looking on narrative as an open text makes it possible to engage in a wide range of interpretations\".\n\n== Outlook ==\nBarrett & Stauffer (2009, p.16) claim that \"[n]arrative inquiry is still in its early stages of development (...). It will be subject to contestation over the years as the methodology develops, and other pathways are marked out.\" This can be substantiated by the dispersed literature with its diverse understandings (see References), and the rather recent development of the discourse around narratives.\n\n\n== Key Publications ==\nVeroff et al. 1993. ''NEWLYWEDS TELL THEIHR STORIES: A NARRATIVE METHOD FOR ASSESSING MARITAL EXPERIENCES.'' Journal of Social and Personal Relationships 10. 437-457.\n* An example study that applied Narrative Interviews.\n\n\n== References ==\n(1) Stauffer, S.L. Barrett, M.S. 2009. ''Narrative Inquiry: From Story to Method.'' In: Barrett, M.S. Stauffer, S.L. (eds). 2009. ''Narrative Inquiry in Music Education. Troubling Certainty.'' Springer VS. 7-17.\n\n(2) Clandinin, D.J. 2006. ''Narrative Inquiry: A Methodology for Studying Lived Experience.'' Research Studies in Music Education 27. 44-54.\n\n(3) Jovchelovitch, S. Bauer, M.W. 2000. ''Narrative interviewing'' [online]. LSE Research Online.\n\n(4) Webster, L. Mertova, P. 2007. ''Using Narrative Inquiry as a Research Method. An introduction to using critical event narrative analysis in research on learning and teaching.'' Routledge London, New York. \n\n(5) Squire, C., Davis, M., Esin, C., Andrews, M., Harrison, B., Hyden, L-C, and Hyden, M. 2014. ''What is narrative research?'' London: Bloomsbury\n\n(6) Moen, T. 2006. ''Reflections on the Narrative Research Approach.'' International Journal of Quantitative Methods 5(4). 56-69.\n\n(7) Polkinghorne, D.E. 2007. ''Validity Issues in Narrative Research.'' Qualitative Inquiry 13(4). 471-486.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "3bupfkx476uqljim2ga89s6x39a6i94"
                }
            },
            {
                "title": "Non-equilibrium dynamics",
                "ns": "0",
                "id": "760",
                "revision": {
                    "id": "5883",
                    "parentid": "5497",
                    "timestamp": "2021-06-27T12:11:20Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* The limits of equilibrium dynamics */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20739",
                        "#text": "'''In short:''' This entry revolves around (non-)equilibria in our world, and how we can understand them from a statistical point of view.\n\n== The limits of equilibrium dynamics ==\n'''Frequentist statistics widely builds on equilibrium dynamics.''' Generally, equilibrium dynamics are systems that are assumed to have reached a steady state, and whose variability is either stable or understood. The last difference is already of pivotal importance and marks a pronounced difference in how models evolved over the last decades. \n\nRegarding stable dynamics, steady state systems have been known for long in physics, and their dynamics have equally long been explored by mathematics. However, especially the developments in physics during the 20th Century made it clear that many phenomena in the natural world do not follow steady state dynamics, and this thought slowly inched its way into other fields beyond physics. Computers and the availability of measures and numbers raised questions on why certain dynamics remain unexplained. Such dynamics were not necessarily new to the researchers, yet they were never before in the focus of research. '''Weather models were one of the early breakthroughs that allowed for a view into dynamic systems'''. It became clear that beyond a certain amount of days, weather prediction models perform worse than predicting the weather based on long term average records. Hence we can clearly diagnose that there are long-term dynamics that dominate short-term changes, and allow for the calculation of averages. Nevertheless, local dynamics and weather changes lead to a deviation from these averages. These dynamics are so difficult to predict over longer time stretches, that today meteorologist rely on so-called 'ensemble models'. This approach looks at, say, one million runs of the same mathematical weather model which also contains a stochastic component and derives average weather conditions over a certain time period. In fact, the statement that there is a 60% probability of rain at a certain day and time follows from the fact that, in this example, 60% of all model runs resulted in rain at that day and time.\n\nKnowledge about complex or chaotic systems slowly inched its way into other branches of science as well, with cotton prices being a prominent example of [[Glossary|patterns]] that could be better explained by chaos theory, at least partly. There seemed to be unexplained variance in such numbers that could not be tamed by the statistical approaches available before. From our viewpoint today, this is quite understandable. Cotton is an industry widely susceptible to fluctuations by water availability and pests, often triggering catastrophic losses in the production of cotton that were as hard to explain as long-term weather conditions. Cotton pests and weather fluctuations can be said to follow patterns that are at least partly comparable, and this is where chaotic dynamics - also known as 'non-equilibrium dynamics' - become a valuable approach. \n\nThe same can be said for humanly induced climate change, which is a complex phenomenon. Here, not only the changes in the averages matter, but more importantly the changes in the extremes - in the long run, these may have the most devastating effects on humans and the ecosystem. We see already an increase in both flooding and droughts, which are often contradicting average dynamics, leading to crop failures and other natural disasters. El Ni\u00f1o years are among the most known phenomena that showcase that such dynamics existed before, but were seemingly amplified by global climate change, triggering cascades of problems across the globe. Non-equilibrium dynamics hence become a growing reality, and many of the past years were among the most extreme on record concerning many climatic measures. While this is a problem in itself, it is also problematic from a frequentists statistics viewpoint. The world is after all strongly built and optimised by statistical predictions. Non-equilibrium dynamics are hence increasingly impacting our lives more directly. '''Future agriculture, disaster management and climate change adaption measure will increasingly (have to) build on approaches that are beyond the currently dominating portfolio of statistics.'''\n\n== The chaos increases ==\nThis brings us now to the second point: what are current models that try to take non-equilibrium dynamics into account? The most simple models are [[Correlations]] and [[Regression Analysis|Regressions]], with 'time' as the independent variable in the case of regressions, or just look at simple temporal correlations concerning temporal dynamics. Since such models either build on linear dynamics of blunt rank statistics, it is clear that such approaches must fail when it comes to taking non-linear dynamics into account. '''Non-equilibrium dynamics are more often than not characterised by non-linear patterns, which need us to step out of the world of linear dynamics.''' \n\nOver the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer [[Causality|causality]] when explaining the patterns that are being modeled. '''In other words, GAMs are able to increase the model fit or predictive power, but in the worst case, we are throwing our means to understand or explain the existing relations out of the window.''' \n\nUnder this spell, over the last decades, parts of the statistical modelling wandered more strongly into the muddy waters of superior model fit, yet understood less and less about the underlying mechanisms. Modern science has to date not sufficiently engaged with the questions how predictive modelling can lead to optimific results, and which role our lack of explicit understanding play in terms of worse outcomes. Preventing a pandemic based on a predictive model is surely good, but enforcing a space shuttle start when there is some lack of understanding of the new conditions at launch day led to the Challenger disaster. '''Many disasters of human history were a lack of understanding the unexpected.''' When our experience was pushed into the new realms of the previously unknown, and our expactation of the likelihood of such an extreme event happening was low, the impending surprise often came at a high price. Many developments - global change, increasing system [[Agency, Complexity and Emergence|complexity,]] growing inequalities - may further decrease our ability to anticipate infrequent dynamics. This calls for a shift towards maximizing the resilience of our models that may be needed under future circumstances. While this has been highlighted i.e. by the IPCC, the Stockholm resilience center and many other institutions, policy makers are hardly prepared enough, which became apparent during the COVID-19 pandemic. \n\n\n== The world can (not) be predicted ==\nFrom a predictive or explanatory viewpoint, we can break the world of statistics down into three basic types of dynamics: Linear dynamics, periodical dynamics, and non-linear dynamics. Let us differentiate between these three.\n\n'''Linear dynamics''' are increasing or decreasing at an estimate that does not change over time. All linear relations have an endpoint which these patterns do not surpass. Otherwise there would be plants growing on the top of Mount Everest, diseases would spread indefinitely, and we could travel faster than the speed of light. All this is not the case, hence based on our current understanding, there are linear patterns, which at some point cannot increase. Until then, we can observe linear dynamics: you have a higher plant diversity and biomass under increasingly favourable growing conditions, illnesses may affect more and more people in a pandemic, and we can travel faster if we - simply spoken - invest more energy and wit into our travel vehicle. All these are current facts of reality that were broken down by humans into linear patterns and mechanisms. \n\n'''Periodical dynamics''' are recognized by humans since the beginning of time itself. Within seasonal environments, plants grow in spring and summer, diseases show increases and decreases that show up-and-down patterns, and travel vehicles do not maintain their speed because of friction or gravity. Hence we can understand and interpret many of the linear phenomena we observe as cyclic or periodical phenomena as well. Many of the laws and principles that are at the basis of our understanding are well able to understand such periodic fluctuations, and the same statistics that can catch linearly increasing or decreasing phenomena are equally enabling us to predict or even understand such periodic phenomena. \n\n'''Non-linear dynamics''' are often events or outlier phenomena that trigger drastic changes in the dynamics of our data. While there is no overarching consensus on how such non-linear dynamics can be coherently defined, from a statistical standpoint we can simply say that such non-linear dynamics do not follow the linear dynamics frequentist statistics are built upon. Instead, such phenomena violate the order of [[Data distribution|statistical distributions]], and are therefore not only hard to predict, but also difficult to anticipate. Before something happened, how should we know it would happen? Up until today, earthquakes are notoriously hard to predict, at least over a long-term time scale. The same is also true for extreme weather events, which are notoriously hard to predict over longer time frames. Another example is the last financial crisis, which was anticipated by some economists, but seemingly came as a surprise to the vast majority of the world. The COVID-19 pandemic stands out as the most stark example that hardly anyone expected before it happened. Yet it was predicted by some experts. Their prediction was based on the knowledge that during the last hundred years the parameters after which a specific disease spread were not adding up to a germ that actually leads to a global pandemic that threatens the whole world. At some point - the experts knew - the combination of traits of a germ may be hence unfortunate enough to enable a pandemic. Since the flu - and some may also say, HIV - there was no clear global threat occurring, yet it has happened before, and it may happen again. \n\nSuch rare events are hard to anticipate, and almost impossible to predict. Nevertheless, many of the world leading experts became alarmed when the rising numbers from Wuhan were coming in, and with the spread starting outside of China, it became clear that we were in a situation that would take a while and huge efforts to tame. The world is facing such wicked problems at an increasing pace, and while it is always under very individual and novel circumstances, we may become increasingly able to anticipate at least the patterns and mechanisms of such non-linear dynamics. Chaos theory emerged over the last decades as an approach to understand the underlying patterns that contribute to phenomena being unpredictable, yet which still follow underlying meachnisms that can be calculated. '''In other words, chaos theory is not about specific predictions, but is more about understanding how complex patterns emerge.''' [https://news.mit.edu/2008/obit-lorenz-0416 Edward Lorenz] put it best when he defined chaos: when the present determines the future, but the approximate present does not approximately determine the future. The concrete patterns in an emergent chaotic system cannot be predicted, because a later state of the system will always be difficult to foresee due to minute non-recognizable differences in the past. In statistical words, we are incapable to statistically predict chaotic systems because we are incapable to soundly measure the past conditions that lead to future patterns. Not only may our measures be not sensitive enough to measure past conditions sufficiently, but we are also unable to understand interactions within the system. '''In statistical terms, the unexplained variance in our understanding of a past state of the system translates into an inability to predict the future state of the system.''' \n\n\n== Approaches to understand the chaos ==\nLet's face it, all statistical models are imperfect. Chaos theory is testimony that this imperfection can add up to us completely losing our grip or our understanding of a system altogether. To any statistical modeler, this should not be surprising. After all, we know that ''all models are wrong, and some models are useful.'' It is equally clear that the world changes over time, not only because of Newton's law of physics, but because humans and life make the world a surely less normally-distributed place. Chaos theory does however propose an array of mathematical models that allow to understand the patterns that allow the repeating patterns and mechanisms that often explain the astonishing beauty associated with chaos theory. Many repeated patterns in nature, such as the geometry of many plants, or the mesmerizing beauty of turbulent flow can be approximated by mathematical models. While some branches of modelling, such as weather forecasts, are already utilizing chaos theory, such models are far away from becoming part of the standard array in predictive modelling. As of today, the difference between [[:Category:Inductive|inductive]] and [[:Category:Deductive|deductive]] models is often not clear, and there is a feeble understanding of what causal knowledge is possible and necessary, and which models and patterns are clearly not causal or explainable but instead predictive.\n \nWhat did however improve rapidly over the last decades are [[Machine Learning]] approaches that utilize diverse algorithmic approaches to maximize model fits and thus take a clear aim at predictive modelling. This whole hemisphere of science is partly strongly interacting and building on data originating in the Internet, and many diverse streams of data demand powerful modelling approaches that build on Machine Learning. 'Machine Learning' itself is a composite term that encapsulates a broad and diverse array of approaches, some of which are established since decades ([[Clustering Methods|Clustering]], [[Ordinations]], [[Regression Analysis]]), while other methods are at the frontier of the current development, such as artificial neural networks or decisions trees. Most approaches were already postulated decades ago, but some only gained momentum with the [[History of Methods|computer revolution]]. Since many approaches demand high computational capacity there is a clear rise in the last years, along with the rise of larger computer capacities. [[Bayesian Inference|Bayesian]] approaches are the best example of calculations that are even today - depending on the data - more often than not rather intense in terms of the demand on the hardware. If you are interested in learning more about this, please refer to the entry on [[Bayesian Inference]].\n\n== Bayes Theorem ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. However, this coin comes with a flip side\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data. \n\nThis does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands. \n\nThere is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Key Publications ==\n* Gleick, J. (2011). Chaos: Making a new science. Open Road Media.\n* Rohde, Klaus. Nonequilibrium ecology. Cambridge University Press, 2006.\n* Kruschke, John. \"Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.\" (2014).\n* Hastie, T. & Tibshirani, R. 1986. ''Generalized Additive Models''. Statistical Science 1(3). 297-318.\n\n==External Links==\n====Articles====\n[https://science.sciencemag.org/content/sci/199/4335/1302.full.pdf?casa_token=SJbEKSHs2gwAAAAA:iWho1AqZsznpL8Tt5vvaPHX2OVggZkP2NlUjEZ8I0avaMKTs6BlCMA7LGG0405x6l5LBY9hTAhag One of the classical papers] on non-equilibrium ecology\n[https://link.springer.com/content/pdf/10.1007/BF00334469.pdf Non-equilibrium theory in ecology]<br>\n[https://www.jstor.org/stable/pdf/1942636.pdf?casa_token=gYme29pwLTsAAAAA:eZBniNyPMJyFe5F7hkmy51EkBk3h0Bm6ap6nG2WWs8-n6EjuhJ16sDt5mJFXipvIIUBu9mzjI16EkLwCgMG70s-YayWTrlzAm63iX3iBk0zk-Mgk4g Classical account on equilibrium and non-equilibrium] dynamics in ecology<br>\n[https://link.springer.com/chapter/10.1007/978-3-319-46709-2_6 A balanced view on rangelands and non equilibrium dynamics]<br>\n[https://esajournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1890/11-0802.1 Non-equilibrium dynamics in rangelands]<br>\n[https://www.mdpi.com/2079-8954/7/1/4/htm A view on complexity]<br>\n[https://plato.stanford.edu/entries/chaos/ A deeper dive into chaos]<br>\n[https://theconversation.com/explainer-what-is-chaos-theory-10620 A nice take on Chaos]<br>\n[https://en.wikipedia.org/wiki/Chaos:_Making_a_New_Science The most definite guide to chaos] Note the synergies to the emergence of sustainability science<br>\n[https://www.r-bloggers.com/2019/05/bayesian-models-in-r-2/ Some intro into Bayesian statistics] in R<br>\n[https://www.wnycstudios.org/podcasts/radiolab/episodes/91684-stochasticity Stochasticity] just for kicks.\n\n\n====Videos====\n[https://www.youtube.com/watch?v=fDek6cYijxI Chaos]: The Veritasium explanation<br>\n[https://www.youtube.com/watch?v=5zI9sG3pjVU Laminar flow] is of course more awesome<br>\n[https://www.youtube.com/watch?v=ovJcsL7vyrk Random] is not random, as this equation proves<br>\n[https://www.youtube.com/watch?v=HZGCoVF3YvM One more take on Bayes]<br>\n\n\n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "pjc6iqqpojnsv4q0ghq0lbvu5ig06nz"
                }
            },
            {
                "title": "Normativity of Methods",
                "ns": "0",
                "id": "192",
                "revision": {
                    "id": "6285",
                    "parentid": "5926",
                    "timestamp": "2021-08-17T11:34:12Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "1235",
                        "#text": "'''This sub-wiki is dedicated to deliberations, open questions and new ideas about science.''' Often, these highlight how the choice and application of [[Glossary|scientific methods]] and the interpretation of scientific results relate to values and norms, the philosophy of science and the general question how science relates to the 'real world'.\n\n''''Normativity' can be defined as any form of evaluation by humans.''' Some things are evaluated to be better than others. Normativity of methods hence deals with the question of the evaluation of methods by entities such as disciplines, scientific communities, civil societies, schools of thinking etc. We try to link these reflections with a post-disciplinary agenda. In philosophy,normativity is often associated to actions or mental states and the question what we ought to do, which is naturally linked to ethics. We consider the choice of method within science to be normative because of the question of knowledge production, which is a normative choice. Therefore, the choice of method and any preference for a method is a normative choice.\n\n'''Choose one of the following entries to learn more:'''\n<categorytree mode=\"pages\" hideroot=\"on\">Normativity_of_Methods</categorytree>"
                    },
                    "sha1": "fs5wydfq1s1w76m460ixjm3dok2c4sy"
                }
            },
            {
                "title": "Notion",
                "ns": "0",
                "id": "728",
                "revision": {
                    "id": "6960",
                    "parentid": "6424",
                    "timestamp": "2023-03-06T17:57:25Z",
                    "contributor": {
                        "username": "Matteo",
                        "id": "13"
                    },
                    "comment": "/* Goals */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3375",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Notion is an all-in-one online workspace''' that allows you to take notes, organize your knowledge, manage yourself and collaborate on projects. If you feel overwhelmed by managing your lecture notes and connecting everything study- or work-related, Notion might be a good tool to integrate all your different activities.\n\n== Goals ==\nNotion is extremely useful if you need to organize yourself and your team(s) and have your very own knowledge database.\n\n== Getting started ==\nGetting started with Notion is easy. First off, you need to create yourself an account. If you're a student or an educator, you can have a pro-license for free:\n\n'''For students:''' https://www.notion.so/students <br/>\n'''For educators:''' https://www.notion.so/educators\n\nOnce you have done that, you can go to http://notion.so/ and start building your very own Notion workspace. While most features are pretty self-explanatory, here are two very good introductory videos that you may use to acquaint yourself with the platform:\n\n'''Notion Beginner Training''' (8 min): https://www.youtube.com/watch?v=aA7si7AmPkY <br/>\n'''Notion Advanced Training''' (13 min): https://www.youtube.com/watch?v=PxQjqYN23vc\n\n\n'''Some general features we found especially helpful'''\n* By typing \"/\" on any given page, you can do pretty much everything in Notion: create To Do Lists, new pages, tables, calendars, kanban boards, timelines and lists; or embed videos, pictures, documents, websites, tweets, maps, code... The opportunities are almost endless.\n* Through the use of \"@\", you can easily tag everyone that also uses your Notion space (e.g. in group projects) for any task, info, or question you type on any page. They get notified, and you don't have to notify them yourself.\n* You can make pages refer to each other: a table can grab information from another table, and a task that is created on one page pops up on another page.\n* There is a smartphone app for Notion which is actually very useful.\n* You can grant external users the rights to work on selected pages only if you want to restrict collaborative access.\n\nThat's it, you're ready to dive deep into Notion. See the \"Links & Further reading\" section for templates and tips!\n\n== Links & Further reading ==\n* [https://www.notion.so/matteoramin/Notion-Template-for-Students-02f3b1819666404d9506eea9113befd2 A template designed by us] for helping students organize their studies (and themselves).\n* [https://www.notion.vip/insights/pro-tips/ Notion Pro Tips]\n* [[https://www.youtube.com/watch?v=ONG26-2mIHU An introductory video]] by Ali Abdaal\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Software]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "l0b8e3m8f9lzwr991e3cmppbl4nn8y9"
                }
            },
            {
                "title": "Object Relational Mapping in Python",
                "ns": "0",
                "id": "1066",
                "revision": {
                    "id": "7277",
                    "parentid": "7262",
                    "timestamp": "2023-08-29T09:31:24Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article provides an introduction to Object Relational Mapping",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9858",
                        "#text": "Object Relational Mapping in Python\n'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Introduction==\nThis article introduces the advantages and disadvantages of Object Relational Mapping (ORM) and the popular Python ORM solutions. This article uses SQLAlchemy, one of the common packages for ORM, to demonstrate how to use ORM in Python.\n\n===What is ORM===\nORM is a programming technique for mapping objects to relational databases. There is a significant difference between object-oriented and relational databases. Relational databases are organized in rows and columns where each row has a unique key, and each column has a unique name. This way, you have a very structured database. An object-oriented database relies on objects which contain data and methods. For example, an object of Leuphana University might contain data about the staff employed, the total number of students, the number of courses offered, yearly budget spending, etc. But it can also contain methods, such as a formula to calculate the average monthly spending on chemistry laboratory equipment. These two datatypes therefore follow different logics and are not fully compatible, even though information from one database sometimes needs to be accessible from the other (for more information and comparison see [https://databasetown.com/relational-database-vs-object-oriented-database-key-differences/ here]). \n\n===Popular ORM Solutions in Python===\nORM creates an intermediate layer between the developer and the database, converting the data into object entities in Python, which shields the differences between different databases while making it very easy for the developer to manipulate the data in the database to use the advanced object-oriented features.\n\nMany components in Python provide ORM support, each with a slightly different application field, but the theoretical principles of database manipulation are the same. The following describes some of the more well-known ORM frameworks for Python databases.\n\n* '''SQLAlchemy'''is the most mature ORM framework in Python and has a lot of resources and documentation. As a result, SQLAlchemy is considered to be the de facto ORM standard for Python.\n* '''Django ORM''' comes with the Django framework for simple or moderately complex database operations. However, Django ORM is based on complex query operations transformed into SQL statements, which are more tedious than those written directly in SQL or generated with SQLAlchemy.\n* '''Peewee''' is a lightweight ORM, which is small and flexible. Peewee is based on the SQLAlchemy kernel development, and the entire framework consists of only one file. Peewee provides access to various databases, such as SQLite, MySQL, and PostgreSQL, for small sites with simple functionality.\n* '''Storm''' is a medium-sized ORM library that requires developers to write DDL statements for data tables and can not generate their table definitions directly from the data table class definition.\n*'''SQLObject''' is a Python ORM that maps objects between SQL databases and Python. Thanks to its Ruby on Rails-like ActiveRecord model, it is becoming increasingly popular in the programming community. It includes a Python-object-based query language, which makes SQL more abstract and thus provides substantial database independence for applications.\n\n==Pros and Cons of ORM==\n'''Pros:'''\n* Database details are shielded from developers so that developers do not have to deal with SQL statements, which improves development efficiency.\n* Facilitates database migration. SQL-based data access layers often spend much time debugging SQL statements when changing databases because of the nuanced differences in SQL statements between each database. ORM provides a SQL-independent interface, and the ORM engine handles the differences between databases, so no code changes are required when migrating databases.\n*Applying techniques such as cache optimization can sometimes improve the efficiency of database operations.\n\n'''Cons:'''\n* ORM libraries are not lightweight tools and require much effort to learn and set up\n* For complex queries, ORM is either inexpressible or less performant than native SQL.\n* ORM abstracts away the database layer, and developers need help understanding the underlying database operations and customizing some particular SQL.\n\n==Applying Methods in Python with SQLAlchemy==\nThe following sections introduce the basic syntax of ORM in Python, using SQLAlchemy as an example. SQLAlchemy ORM presents a method of associating user-defined Python classes with database tables and instances of those classes (objects) with rows in their corresponding tables. An application can be built using ORM alone.\n\n== Create a Model==\nSQLAlchemy ORM provides a way to relate user-defined python classes to tables in the database. The developer manipulates the database similarly to the python class (argument, method). Usually, the first step in ORM operations is to describe/declarative the database tables as python classes. A table (python class) must have at least the __tablename__ attribute and at least one column with a primary key field.\n\n<syntaxhighlight lang=\"Python\" line>\nfrom sqlalchemy import *\nfrom sqlalchemy.ext.declarative import *\nfrom sqlalchemy.orm import *\n\nengine = create_engine(\"sqlite:///test.db\")\nmetadata = MetaData()\nBase = declarative_base()\n\nclass Individ(Base):\n    __tablename__ = 'individ'\n    metadata = metadata\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    addresses = relationship(\"Address\", backref=\"individ\")\n\nclass Address(Base):\n    __tablename__ = 'address'\n    metadata = metadata\n    id = Column(Integer, primary_key=True)\n    email = Column(String)\n    individ_id = Column(Integer, ForeignKey('individ.id'))\n    individs = relationship(\"Individ\",backref =\"address\")\nmetadata.create_all(engine)\n</syntaxhighlight>\n\nFirst, we need to install the necessary packages. Then we create a database engine called \"test.db\" which is stored in the current working directory. We then settle the metadata to contain meta-information about the tables we will create. The Base command determines the structure of the database in a way that it is attributive. This means we can assign attributes to each column in our tables, such as a name. We then create the two tables, individ and address. For both, we first define the name of the table and settle the same metadata. We then create an id column as an integer for both tables. For the address table, we create another string column with the e-mail information and individ ID information which has its own key. We then set the relationship between the two tables and make sure with the \"backref\" specification that the relationship is also reversed. Lastly, we create the relational database using all the information from above. \nIn the following, we will look at how to create read, update, and delete connections between the database we have created and ORM.\n\n==Create, Read, Update, and Delete- the Four Elements of ORM==\n===Create===\nWe create an instance of individ and address, which are \"Hannah\" and \"Hannah@gmail.com\" and set the attribute name for an e-mail for individ and address respectively. We also set that the person with the given address belongs to the person with the individ ID 1 (which in this case is Hannah). We can therefore treat the instances in the relational database as objects. We then create a database using the sessionmaker function and add the attributes with \"session.add_all()\" (we could also only add one attribute with \"session.add()\"). With the commit command, we confirm the changes made to the database.\n\n<syntaxhighlight lang=\"Python\" line>\nindivid = Individ()\nindivid.name = \"Hannah\"\n\naddress = Address()\naddress.email = \"Hannah@gmail.com\"\naddress.individ_id = 1\n\nSession = sessionmaker(engine)\nsession = Session()\nsession.add_all([individ, address])\nsession.commit()\n</syntaxhighlight>\n\n===Read===\nWe first again start a session to connect to the database. Then we make the query to retrieve the address of the person with the ID 2, so the address of the second individ. The first command makes sure that the first result of the query is retrieved. Since we have only created one ID, we should get the result \"none\".\n\n<syntaxhighlight lang=\"Python\" line>\nSession = sessionmaker(engine)\nsession = Session()\nresult = session.query(Address).filter(Address.id==2).first()\n</syntaxhighlight>\n\n===Update===\nTo update the database, we again start the session to connect to the database. We again query for the address with ID 2. But this time, we update the e-mail address by adding \"update({\"email\": \"Hannah+1@gmail.com\"}. Note that e-mail is an attribute of the address we have defined above. We then finalize the update with \"session.commit\". This update only works when ID 2 already exists.\n\n<syntaxhighlight lang=\"Python\" line>\nSession = sessionmaker(engine)\nsession = Session()\nresult = session.query(Address).filter(Address.id==2).update({\"email\": \"Hannah+1@gmail.com\"})\nsession.commit()\n</syntaxhighlight>\n\n===Delete===\nTo delete an attribute, we again query an address, this with ID after, we have stated the session. This retrieved address we call the result. This result is then deleted using \"session.delete(result)\". Lastly, we confirm the changes made with \"session.commit\".\n\n\n<syntaxhighlight lang=\"Python\" line>\nSession = sessionmaker(engine)\nsession = Session()\nresult = session.query(Address).filter(Address.id==1).first()\nsession.delete(result)\nsession.commit()\n</syntaxhighlight>\n\n==References==\n* Copeland, Rick, and Jason Myers. Essential Sqlalchemy. O'Reilly, 2016.\n* \u201cThe Python SQL Toolkit and Object Relational Mapper.\u201d SQLAlchemy, https://www.sqlalchemy.org/.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "mhu9sk7zjzz0gmnpfoh07c4bm9yt7ok"
                }
            },
            {
                "title": "Objects and Classes in Python",
                "ns": "0",
                "id": "1032",
                "revision": {
                    "id": "7263",
                    "parentid": "7020",
                    "timestamp": "2023-06-30T05:52:36Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to classes and objects in Python.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4309",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Introduction==\nIn this tutorial, we will learn more about classes and objects in Python. Knowing how to use classes and objects is very handy because it allows you to bundle different entities in Pythn in a way that suits you. Have a look yourself!\n==Creating a Class==\nWhen talking about class, we can imagine it really as a class. For example, in many games, we can find classes in weapon types. A sword class has \u201cattributes\u201d such as material (can be iron, or adamantine). While a bow class has different \u201cattributes\u201d such as arrows, or string, it can also have the same attribute, which is material. Maybe you can already grasp here that we can treat the attribute as a variable here, with different values for sword (iron) and bow (wood).\n{| class=\"wikitable\"\n|-\n! Class !! Sword !! Bow\n|-\n| Attribute 1 || Material || Material\n|-\n| Attribute 2 || Shape || Arrow\n|}\n\n==Object==\nObjects can be understood as entities that belong to a class. What does that mean? I think it is easier to explain it through an example. \n\nUnder the class sword, we have different kinds of swords like Iron Katana, gold broadsword, etc. So now you can see that classes are categories bundling together things (objects) based on common properties (such as sharp weapons). Each of these objects then has attributes that they either share with other objects or not.\n{| class=\"wikitable\"\n|-\n! Sword !! Iron Katana !! gold broadsword\n|-\n| Material || iron || gold\n|-\n| Shape || katana || broadsword\n|}\n\n==Methods==\nMethods give you a way to interact with/change/call the object. The notation that is used in Python and many other Object Oriented Programming is a \u201cdot\u201d ( . ), which is the same as applying a function to the attribute in the object. \n\nExample: After a blacksmith upgrades your Iron Katana to Steel Katana, the blacksmith can then call a method .change_material(\u201dsteel\u201d). The below example is an abstraction of how a blacksmith would change the material of my katana.\n\n<syntaxhighlight lang=\"Python\" line>\nsteph_katana.material() # return iron\nsteph_katana.change_material(\"steel\")\nsteph_katana.material() # return steel\n</syntaxhighlight>\n\n==Code Examples==\n\n==Creating a Class==\nWe can simply create a class by using \u201cclass\u201d keyword: \n<syntaxhighlight lang=\"Python\" line>\nclass Sword:\n\tdef __init__(self, material, shape):\n\t\tself.material = material\n\t\tself.shape = shape\n\n\tdef change_material(self, new_material):\n\t\tself.material = new_material\n</syntaxhighlight>\n\nThe \u201c__init__\u201d constructor is used to initialize two attributes each object in a class can have (material and shape). The \"self\" refers to the object that is created in the sword class. So this means, that any object in the class sword consists of the object itself (self) and two attributes (material, shape). In the next step, we have created a method to change the material. It defines \"change_material\" to change the material from the old one to the new material.\n\n<syntaxhighlight lang=\"Python\" line>\nx = Sword(\"iron\", \"katana\")\nx.material # to check the material\nx.shape # to check the shape\n</syntaxhighlight>\n\nThe code above creates an iron katana object and stores it in variable x. Sometimes, you just forgot what is the attributes of an object, and that\u2019s totally fine. You can just invoke the attribute name again to ask python. In the code above, to check \u201cx\u201d\u2019s material and shape, you can just use x.material and x.shape.\n\n==Quiz==\n1. Create Bow Class that has material and arrow as attributes. \n<syntaxhighlight lang=\"Python\" line>\nclass Bow: \n\t# TODO\n</syntaxhighlight>\n\n2. Create your own oak bow with an iron arrow. And don't forget to double-check whether the material and arrow are correct.\n<syntaxhighlight lang=\"Python\" line>\nmy_bow = # TODO\n</syntaxhighlight>\n\n3. You gave your bow to an alchemist, and he turns it into gold. Add a method/ function in the class so that the chemist can use it to change the material. \n<syntaxhighlight lang=\"Python\" line>\nclass Bow: \n\t# TODO from Quiz 1\n\n\tdef change_material(self, new_material):\n\t\t# TODO \n</syntaxhighlight>\n\n4. Check again your material, is it successfully upgraded?\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "0gpslauwkqee9y6ld03i0punyittzr5"
                }
            },
            {
                "title": "Objects in Python",
                "ns": "0",
                "id": "1011",
                "revision": {
                    "id": "7243",
                    "parentid": "7130",
                    "timestamp": "2023-06-29T05:23:14Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This Article describes the different objects in Python and how to create them.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6776",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Introduction- What Objects are there?==\nAn object is simply a collection of data and methods. This data can be variables and functions. For example, if x=1, x is an object. \n\n==== Variables and Data Types ====\nVariables are just like a container for storing data values. For example, if y=2. It means 'y' is a variable assigned with an integer value of 2.\n\n{| class=\"wikitable\"\n|-\n! Variables !! Example\n|-\n| X || x=5\n|-\n| Y || y=2\n|-\n| Z || z=x+y\n|}\n\nData Types are simple variable definitions. Data types in Python are; primitive, abstract, and composite. [Explanation needed? Does this refer to the respective examples above?]\n\n==== Different Data Types in Python ====\n{| class=\"wikitable\"\n|-\n! Data Types!! Example\n|-\n| Primitive || integer, float, strings, boolean\n|-\n| Abstract || set, stack, tree, vector, map, queue\n|-\n| Composite || list[], tuple [], dict[]\n|}\n\n==Save, load, and declare Objects==\nYou can create a Python object by assigning a value to a variable. In subsequent lines of code, you can use that variable to access the designated value. For example, in the following code, we assign the value 1 to variables x and y and then use the variables to create a new object z:\n\n<syntaxhighlight lang=\"Python\" line>\nx = 1 \ny = 1\nz = x + y\nprint(z)\n>>> Output: 2\n</syntaxhighlight>\n\nNote that you will overwrite the value saved in an object if you assign something new to it. For example, you may re-assign another value to x in the example above. Note how what is stored in x changes:\n\n<syntaxhighlight lang=\"Python\" line>\nx = 1 \ny = 1\nz = x + y\nprint(x)\n>>> Output: 1\n\nx = \"gryffindor\"\nprint(x)\n>>> Output: gryffindor\n</syntaxhighlight>\n\n====Object Types====\nIn contrast to statically typed programming languages like C++ and Java, Python is dynamically typed, which means that you do not need to explicitly declare the data type. So, in the example above, Python automatically identifies that variable x contains the integer 1. The following table provides an overview of some of the most important Python objects and how to implement them in code.\n\n{| class=\"wikitable\"\n|-\n! Object Type !! Description !! Implementation\n|-\n| String || sequence of character || s = \"slytherin\"\n|-\n| Integer || whole number || i = 1\n|-\n| Float || decimal number ||  \tf = 1.0\n|-\n| Boolean || True or False || b = True\n|-\n| List || mutable*, ordered**, collection of values || l = [a, b]\n|-\n| Tuple || immutable, ordered collection of values|| t = (a, b)\n|-\n| Set || unordered collection of unique values || z = {a, b}\n|-\n| Dictionary || mutable, ordered mapping of unique keys to values || d = {\"key1\" : \"value1\", \"key2\": \"value2\"}\n|}\n\n''Mutable means that an object can be changed after it has been created. For example, you can add or delete elements to an already existing list.'' \n\n''Ordered means that the individual elements of an object have fixed indices that you can use to reference them. For example, `l[0]` returns the first element of the list l.''\n\nWhile Python generally identifies the data type of an object for you, you may find yourself wanting to change the data type of an object. There are multiple built-in functions that return a new object representing the converted value. For example, the following code allows you to perform addition on a string c and an integer d, which would otherwise result in an error:\n\n<syntaxhighlight lang=\"Python\" line>\na = 1\nb = \"1\" \nc = a + b\n>>> Output: TypeError\n\nb = int(b)\nc = a + b\nprint(c)\n>>> Output: 2\n</syntaxhighlight>\n\nSome useful conversions, besides `int()`, include `str()`, `tuple()`, and `list()`. The following code shows some examples of how to use them:\n\n<syntaxhighlight lang=\"Python\" line>\nm = 1\nm_string = str(1)\nprint(m_string, type(m_string))\n>>> Output: 1 <class 'str'>\n\nn = \"hufflepuff\"\nn_tuple = tuple(n)\nprint(n_tuple, type(n_tuple))\n>>> Output: ('h', 'u', 'f', 'f', 'l', 'e', 'p', 'u', 'f', 'f') <class 'tuple'>\n\nn_list = list(n)\nprint(n_list, type(n_list))\n>>> Output: ['h', 'u', 'f', 'f', 'l', 'e', 'p', 'u', 'f', 'f'] <class 'list'>\n\nn_set = set(n)\nprint(n_set, type(n_set)\n>>> Output: {'e', 'h', 'p', 'u', 'f', 'l'} <class 'set'>\n</syntaxhighlight>\n\nGenerally, conversion functions share the following same syntax:\n<syntaxhighlight lang=\"Python\" line>\n<required_datatype>(object)\n</syntaxhighlight>\n\nThe explicit type conversion of objects described above can be distinguished from implicit type conversion, where Python changes the type of an object automatically. For example, the result of adding up an integer and a float will be a float.\n\nIf you are ever unsure what type of object you are dealing with, you can pass it into the `type()` function as follows:\n<syntaxhighlight lang=\"Python\" line>\nl = [1, 2, 3]\nl_type = type(l)\nprint(l_type)\n\n>>> Output: <class 'list'>\n</syntaxhighlight>\n\n==Conversion and type() function==\n{| class=\"wikitable\"\n|-\n! Function !! Function !! What it does\n|-\n| int(x)|| float, str (like '25')|| Convert x to a int, parse a string as an int\nIn case of float, cut off the decimals.\nFor rounding float to int, use round(x)\n|-\n| float(x)|| int, str (like '10.4') || Convert x to a float, parse a string as a float\n|-\n| str(x) || any type || Get the string representation of x\n|-\n| list(x) || any iterable type (another list, set, map, zip, dict (keys only))|| Convert x to a list\n|-\n| dict(key=value) || key-value pairs as named arguments|| Create a dictionary with arguments as key-value pairs\n|}\n\n==Concatenating/Adding two Objects==\n\n{| class=\"wikitable\"\n|-\n! Object / Data Type !! number !! string !! boolean !! list\n|-\n| '''number''' || number 1 + 2.5 = 3.5 || unsupported || number1 + True = 2|| unsupported\n|-\n| '''string''' || unsupported || string \"Hello \" + \"world!\" = \"Hello world!\" || unsupported || unsupported\n|-\n| '''boolean''' || number False + 2 = 2|| unsupported || number True + False = 1|| unsupported\n|-\n| '''list''' || unsupported || unsupported || unsupported || list [1,2,3] + [\"one\",\"two\",\"three\"] = [1,2,3,\"one\",\"two\",\"three\"]\n|}\n\n==Multiplying two Objects==\n{| class=\"wikitable\"\n|-\n! Object / Data Type !! number !! string !! boolean !! list\n|-\n| '''number''' || number 2 * 2.5 = 5 || string 2 * \"Hello\" = \"HelloHello\" || number 5 * False = 0 || list2 * [1,2,3] = [1,2,3,1,2,3]\n|-\n| '''string''' || string \"200\" * 2 = \"200200\" || -| string<br/>\u201cHello\u201d * False = \u201c\u201d` || - || -\n|-\n| '''boolean''' || number True * 2 = 2 || string True * \"Hello\" = \"Hallo\" || number True * True = 1 || list False * [1,2,3] = []\n|-\n| '''list''' || list [\"one\",\"two\",\"three\"] * 2 = [\"one\",\"two\",\"three\",\"one\",\"two\",\"three\"] || - || list True * [1,2,3] = [1,2,3] || -\n|}\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "druoh4bb8vfjmafjqg92sep3klfx5bs"
                }
            },
            {
                "title": "Online Conduct",
                "ns": "0",
                "id": "471",
                "revision": {
                    "id": "5063",
                    "parentid": "3263",
                    "timestamp": "2021-04-09T15:14:31Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Goals */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3794",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThis article represents helpful guidance for personal conduct (i.e. how you should behave) in online spaces such as video conferences or group chats. While good manners should be mostly the same as in the analogue world, some things are different anyhow. For a more general guide on group behavior, please refer to the entry on [[Code of Conduct]].\n\n== Goals ==\n* Ensure good [[Glossary|communication]] online.\n* Foster a respectful environment.\n\n== Guidelines ==\nBelow you will find some guidance regarding personal conduct in video conferences and group chats. In general, do make sure to pay attention to your language. This includes not only being polite, but also being aware of who can read the language you're typing in.\n\n=== Video Conferences ===\nThe following guidelines should be respected in online video conferences with tools such as Zoom, Slack, Microsoft Teams or Skype. \n\n# '''Before or at the beginning of every video conference, a moderator is nominated'''. He*she is responsible for ensuring compliance with these rules. He*she is especially responsible to look for requests to speak.\n# '''All participants mute their microfone''' when joining a conference call. The microphone will solely be activated when the moderator gave allowance to speak and will be muted immediately afterwards.\n# '''To request to speak, use the possibilities of the video conferencing software'''. Typically, there is a chat or \"Raise hand\" function at your dispoal.\n# '''Audio transmission is to be prioritized'''. This means:\n## Use headsets with a built-in microphone.\n## Do not turn on video if unnecessary. \n# '''Good practice for offline meetings should especially be followed in online meetings''' due to the limited possibilities of interaction. This refers especially to:\n## Have an agenda including time-boxed agenda slots.\n## Nominate a keeper of the minutes at the beginning. He*she will summarize the meeting at the end.\n## Only discuss topics that are relevant for all participants. Individual consultations should happen before or after the meeting, or - if need be - using the private chat function or mail during the meeting.\n\n=== Group Chats ===\nThe following guidelines especially apply to tools such as Rocket.Chat, Slack or Microsoft Teams.\n\n# '''Be aware of the size of your audience.''' Many people might be able to read what you write and/or get notified.\n# '''Use the built-in thread function (!).''' I can't stress this enough. Using threads makes communication structured and helps everyone keep an overview. Mindlessly writing directly into the chat windows clutters the chat history and makes everyone sad. A thread is when you directly answer to a chat message so that answers get grouped beneath the original message. The button for a thread typically appears when you hover your mouse over the message you want to reply to. It's usually a small speech bubble.\n# '''It's okay to use punctuation and proper spelling in chat.''' It really helps your readers make sense of what you're writing.\n\n== Links & Further Reading ==\n\n__NOTOC__\n[[Category:Skills_and_Tools]]\n\n[[Category:Personal Skills]]\n\n[[Category:Team Size 2-10|Team Size 2-10]]\n\n[[Category:Team Size 11-30]]\n\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "fxferpijnonvxk8a8r6chnpjptfvrvb"
                }
            },
            {
                "title": "Open Interview",
                "ns": "0",
                "id": "249",
                "revision": {
                    "id": "6863",
                    "parentid": "6862",
                    "timestamp": "2022-11-28T22:58:44Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16882",
                        "#text": "[[File:ConceptOpenInterview.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for the [[Open Interview]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''In short:''' Open Interviews are a form of qualitative data gathering through open conversations with interviewees. For more Interview forms, and more on Interview methodology in general, please refer to the [[Interviews|Interview overview page]].\n\n== Background ==\n[[File:Open interviews.png|400px|thumb|right|'''SCOPUS hits per year for Open Interview until 2019.''' Search terms: 'open interview', 'unstructured interview' in Title, Abstract, Keywords. Source: own.]]\n\n\"Interview methodology is perhaps the oldest of all the social science methodologies. Asking interview participants a series of informal questions to obtain knowledge has been a common practice among anthropologists and sociologists since the inception of their disciplines. Within sociology, the early-20th-century urban ethnographers of the '''Chicago School''' did much to prompt interest in the method.\u201d (Hamill 2014) '''Barney Glaser and Anselm Strauss''' are central figures regarding this method-and interviews and other methods in general- ever since they developed the [[Grounded Theory]] approach to qualitative data analysis in the 1960s. Their 1967 landmark book paved the road for the integration of methodologically conducted interviews into sociological research. While Glser & Strauss built on several previously explored approaches, they widely triggered this branch of science, and set off a change considering the role that interviews play ever since in research, creating an altogether new arena in science (see Key Publications). Today, qualitative interviews are abundantly used in gender studies, social and political studies as well as ethnographics (2, 3), market research and other fields.\n\n\n== What the method does ==\n* '''Interviews are a form of data gathering.''' They can be conducted in direct conversation with individuals or groups. This is also possible online or via the phone (1, 2). It is possible for more than one researcher to conduct the interview together (2). The necessary equipment includes notes regarding topics of interest that help ask relevant questions, a recording device and paper & pencil or a computer for note taking during the interview.\n* A wide range of formats is available for qualitative interviews. They may be distinguished in terms of a) the extent to which the interview questions are pre-formulated (for a stronger pre-formulation, see [[Semi-structured Interview]]), b) how broad or topic-specific the questions are and c) the focus of the interview - letting the interviewee narrate freely or purposefully extracting meanings and interpretations expressed by the interviewee (3).\n* Among this variety, commonly used forms and fields of the open interview are clinical interviews to diagnose illnesses -anamnese- biographical interviews to \"gain access to life-histories\" (Hopf 2004, p.205) or ethnographical interviews (see [[Ethnography]]) (3).\n* \"Interview methodology is particularly useful for researchers who (...) are concerned with the way in which individuals interpret and assign meaning to their social world.\" (Hamill 2014) '''Interviews allow the researcher to gain insight into understandings, opinions, attitudes and beliefs of the interviewee''' (2).\n\n[[File:ResultVisualisationOpenInterview.png|500px|thumb|right|'''An exemplary interview transcript from an Open Interview.''' Source: own.]]\n\n\n==== How to do an open interview ====\n'''Conducting the interview'''<br/>\nFor an open interview - similar to the [[Semi-structured Interview]] - the researcher may prepare a set of questions ahead of the interview. These questions may result from the study design and research questions, but also from literature or prior studies on the respective research topic and specific characteristics of the interviewee. However, different from the [[Semi-structured Interview]], these questions are not strictly pre-formulated but rather represent general topics of interest to the researcher. \n\nBefore the start of the interview, the interviewer lets the interviewee sign a Voluntary Consent Form, guaranteeing voluntary participation in the research. The interviewer should provide Full Disclosure (information about the main goal of the research and further use of the information), Confidentiality of the Data as well as the right of the interviewees to review the results before publication and to withdraw from the interview without any need for an explanation.\n\nThe interviewer then opens the wider field around these topics by asking open questions, listening to the answers given by the interviewee and developing new questions that arise from these responses. The interviewer tries to understand, not to measure, what the interviewee is saying (2).\n\n'''Transcribing the interview'''<br/>\nThe interview should be video- or audio-recorded for later transcription. This transcription is preferably done by writing down the recorded speech either word by word (denaturalism) or including stutters, pauses and other idiosyncratic elements of speech (naturalism). The chosen approach depends on the research design. In any case, the form of transcription should not impose too much interpretation upon the interview data, and allow for intersubjective data readability (7). The text may be structured and punctuations may be added to improve legibility. For ethical reasons, the identity of the interviewee should be secured in the transcribed text (7). To allow for quotations, the lines of the transcript should be numbered or time codes should be added at the end of each paragraph (6). The subsequent analysis of the gathered data relies on this transcription. Quite often, the transcription is coded to map out a more systematic understanding of the interview. This may allow for a deeper qualitative analysis of the text, upon which a sequential analysis that is integrated later may build (see [[Content Analysis]]). For more on Transcription, please refer to the entry on [[Transcribing Interviews]].\n\n\n== Strengths & Challenges ==\n* Compared to the more restricted standardized format of [[Survey|surveys]], the qualitative interview allows for an open collection and investigation of self-interpretations and situational meanings on the part of the interviewees. This way, theories from psychology and sociology can more easily be empirically explored (3), while also keeping the option open to work independently from initial theories.\n* Open interviews may reproduce existing power structures less strongly, allowing for more open spaces where more information is shared. This highlights the importance of the overall settings, and a clarification of the respective roles, as well as the general research protocol, including ethical concerns. \n* At the same time, due to this focus on the subjective position of the interviewee with regards to his/her/their feelings, attitudes and interpretations, the results gathered in qualitative interviews represent the perspective and context of an individual and hence differ in many aspects from data gathered by quantitative surveys (2), often offering what many consider to be a deeper perspective.\n* Language barriers impose problems in understanding the interviewee's statements during the interview and in the transcription process, making it often necessary to hire native speakers, since due to the qualitative nature and the sensitive interview setting a deep understanding of the respective cultural and social context is desirable.\n* '''Pitfalls during the interview:''' It is crucial that the interviewer is well-acquainted with the conceptual approach and design of the research, which is crucial to maintain the general flow of the interview setting (8). This process imposes high demands on the interviewer. The interviewer must remain attentive and flexible throughout the interview in order to make sure all relevant aspects of the interview guide are profoundly answered. The quality and richness of the data depend on the proficiency in this process (1). In addition, the interviewer must make sure not to impose [[Bias and Critical Thinking|bias]] on the interviewee. Therefore, he/she/they should not ask closed yes/no-questions or offer answers for the interviewee to choose from. This open form can be lengthy, at time, which demands a lot of patience form the interviewer. Answers should not be commented or confirmed. The questions should not be judgemental, unexpected or incomprehensible to the interviewee (3, 5).  It is thus recommendable that the interview be rehearsed (as much as possible in view of the open structure) and the interview guide be tested before the first real interview so as to ensure the quality of the interview conduction.\n* The amount and depth of data that is gathered in long interviews prohibits a big sample size in terms of number of different interviews. This affects the feasible sample size. Also, the more extensive the interview is, the longer takes the transcription process. Especially longer interviews cannot be replicated endlessly, as opposed to the huge number results possible with standardized surveys. Open interviews therefore tend to rather small samples (1, 2).\n\n\n== Normativity ==\n==== Connected methods ====\n* * Qualitative Interviews can be used as a preparation for standardized quantitative Interviews ([[Survey]]), [[Focus Groups]] or the development of other types of data gathering (2, 3). To this end, they can provide an initial understanding of the situation or field of interest, upon which more concrete research elements may follow.\n* A Stakeholder Analysis may be of help to identify relevant interviewees.\n* The interview transcript ought to be analysed using a qualitative form of content analysis (e.g. with MAXQDA) (1). While quantifications of the gathered data are possible, this is uncommon - almost proscribed - in the case of open interviews (2).\n* The open nature of Open Interviews, adapting the process of data gathering to inductively as the first insights are evaluated, is comparable to [[Grounded Theory]], which was also developed by Glaser & Strauss (see Background).\n\n==== Everything normative related to this method ====\n* '''Quality criteria''': The knowledge gathered in qualitative interviews, as in all qualitative research, is dependent on the context it was gathered in. Thus, as opposed to standardized data gathering, objectivity and reliability cannot be valid criteria of quality for the conduction of qualitative interviews. Instead, a good qualitative interview properly reflects upon the subjectivity involved in the data gathering process and how it influences the results. It is therefore crucial to transparently incorporate the circumstances of the interview situation into the data analysis. The quality criteria of validity, i.e. the extent to which the gathered data constitutes the intended knowledge, must be approached by the principles of openness and unfamiliarity. An open approach to the interview ensures a valid understanding of the interviewee's subjective reality (4).\n* '''Ethics''': As indicated throughout the text, a range of ethical principles should guide the Interviewing process. These include that Interviewees should be informed about the goal of the research and the (confidential) use of their data and statements. The creation of recordings requires consent. Interviewees should be allowed to review and withdraw statements, and if desired, they should be credited for their participation in the Interviews.\n* The '''[[Sampling|sampling]]''' strategy may heavily influence the gathered data, with snowballing- or opportunistic sampling leading to potential [[Glossary|bias]] in the interviewee sample. The sample size depends on the intended knowledge in the first place. While more structured, systematic approaches need bigger sample sizes from which a numerical generalization may be done, more qualitative approaches typically involve smaller samples and generalisation of the gathered data is not the researcher's main goal (2). It often needs to be considered that fatigue can be an important factor both for the interviewer as well as the interviewee.\n* While interviewing concerns itself with the present situation and perspective of the individual interviewee, the meanings derived from the interviews can extend further, including the perception of a system or even a specific global context. However, since the information is sampled at the scale of an individual, it is important to note that the spatial scale is clearly restricted to the view of an individual. Something similar can be said about the temporal scale, as interviews can be not only about present views, but also about the memory an individual has regarding past events, and of course also what an individual expects the future to bring. While interviews can thus offer the perspective of individuals on past event as well as the future, we understand that the source of information is the individual at this point in time. Memories can be fleeting, and a wide array of research examines how inconsistent can create flaws within studies.\n\n\n== Key Publications ==\nFielding, Nigel G., ed. 2009. ''Interviewing II''. London: SAGE.\n* A four-volume collection of essays of which the wide-ranging contributions comprehensively cover all the theoretical and practical aspects of interviewing methodology\n\nGlaser, Barney G. Strauss, Anselm L. 1967. ''The discovery of grounded theory: Strategies for qualitative research.'' Chicago: Aldine.\n* The principles of grounded theory were first articulated in this book. The authors contrast grounded theories derived directly from the data with theories derived from a deductive approach.\n\nPatton, Michael Q. 2002. ''Qualitative research and evaluation methods''. 3d ed. Thousand Oaks, CA, and London: SAGE.\n* In chapter 7, Patton provides a comprehensive guide to qualitative interviewing. This chapter highlights the variations in qualitative interviews and the interview guides or schedules that can be used. It provides a very useful guide as to how to formulate and ask questions and offers practical tips about recording and transcribing interviews. The chapter also covers focus groups, group interviews, ethics, and the relationship between researcher and interview participants.\n\nArksey, H. Knight, P. 1999. ''Interviewing for Social Scientists. An Introductory Resource with Examples. Chapter 1: Interviews and Research in the Social Sciences''. SAGE Publications, London. 1-20.\n* An extensive description of the interview process, the differences between interview forms and the underlying assumptions of the interview methodology.\n\nBryman, A. 2012. ''Social Research Methods.'' 4th Edition. Oxford University Press.\n* An all-encompassing guide to the basics of social science research, including insights into interview methodology.\n\n\n== References ==\n(1) Hamill, H. 2014. ''Interview Methodology''. in: Oxford Bibliographies. Sociology.\n\n(2) Arksey, H. Knight, P. 1999. ''Interviewing for Social Scientists. An Introductory Resource with Examples.'' SAGE Publications, London.\n\n(3) Hopf, C. ''Qualitative Interviews: An Overview.'' In: Flick, U. von Kardorff, E. Steinke, I. (eds). 2004. ''A Companion to Qualitative Research.'' SAGE Publications, London. 203-208.\n\n(4) Helfferich C. ''Leitfaden- und Experteninterviews.'' In: Baur N., Blasius J. (Hrsg.) 2019. ''Handbuch Methoden der empirischen Sozialforschung.'' Springer VS Wiesbaden. 669-685.\n\n(5) Helfferich, C. 2011. ''Die Qualit\u00e4t qualitativer Daten. Manual f\u00fcr die Durchf\u00fchrung qualitativer Interviews.'' 4th edition. Springer VS Wiesbaden.\n\n(6) Dresing, T., Pehl, T. 2015. ''Praxisbuch Interview, Transkription & Analyse. Anleitungen und Regelsysteme f\u00fcr qualitativ Forschende.'' 6th edition.\n\n(7) Widodo, H.P. 2014. ''Methodological Considerations in Interview Data Transcription.'' International Journal of Innovation in English Language 3(1). 101-107.\n\n== Further Information ==\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "o8ga06ec5erp58d3ueidgv512gcsqz9"
                }
            },
            {
                "title": "Ordinations",
                "ns": "0",
                "id": "1004",
                "revision": {
                    "id": "6888",
                    "parentid": "6887",
                    "timestamp": "2023-01-26T08:39:22Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "13952",
                        "#text": "== Ordinations ==\n\nOrdination techniques evolved already more than a century ago in mathematics, and allowed fro a reduction of information that makes these analysis approaches timely up until today. Ordination techniques rely strongly on a profound knowledge of the underlying data format of the respective dataset that is being analysed. Since ordination allow for both inductive and deductive analysis, they often pose a risk for beginners, who typically get confused by the diversity of approaches and the answers these analysis may provide. This conduction is often increased by the model parameters available to evaluate the results, since much of ordination techniques allows for neither probability based assumptions, let alone more advanced information based techniques. What is more, ordinations are often deeply entangled in disciplinary cultures, with some approaches such as factor analysis being almost exclusive to some disciplines, and other approaches such as principal component analysis being utilised in quite diverse ways within different disciplines. This makes norms and rules when and how to apply different techniques widely scattered and intertwined with disciplinary norms, while the same disciplines are widely ignorant about other approach from different disciplines. Here, we try to diver a diverse and reflected overview of the different techniques, and what their respective strengths and weaknesses are. This will necessary demand a certain simplification, and will in addition trigger controversy within certain branches of science, and these controversies are either rooted in partial knowledge or in experiential  identity. Unboxing the whole landscape of ordinations is also a struggle because these analysis are neither discrete nor always conclusive. Instead they pose starting points that often serve initial analysis, or alternatively enable analysis widely uncoupled from the vast landscape of univariate statistics. We need to acknowledge to this end that there is a vast difference between the diverse approaches not only in the underlying mathematics, yet also how these may be partly ignored. This is probably the hardest struggle that you can fin neither in textbooks nor in articles. The empirical reality is that many applications of ordinations violate much of the mathematical assumptions or rules, yet the patterns derived from these analyses are still helpful if not even valid. Mathematicians can choose to live in a world where much of ordination techniques is perfect in every way, yet the datasets the world gives to ordinations are simply not. Instead, we have to acknowledge that multivariate data is almost always messy, contains a high amount of noise, many redundancies, and even data errors. Safety comes in numbers. Ordinations are so powerful exactly because they can channel all these problems through the safety of the size of the data, and thus derive either initial analysis or even results that serve as endpoints. However, there is a difference between initial or final results, and this will be our first starting point here.\n\nOrdination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through experience can you pick what serves your dataset best. This is associated to the reality that many analysis made with ordinations are indeed compromises. Ecology and psychology are two examples of disciplines why imagined ordinations deeply enough into the statistical designs to derive datasets where more often than not assumptions for statistical analysis of a respective ordination are met. However, many analyses based on ordinations are indeed compromises, and from a mathematical standpoint are real world analysis based on ordinations a graveyard of mathematical assumptions, and violation of analytical foundations that borderline ethical misconduct. In other words, much of ordinations are messy. This is especially true because ordinations are indeed revealing mostly continuous results in the form of location on ordination axes. While multivariate analyis based on cluster analysis are hence more discrete through the results being presented as groups, ordinations are typically nice to graphically inspect, but harder to analytical embedded into a wider framework. More on this point later. Let us now begin with a presentation of the diverse ordination types and their respective origins. \n\n=== Correspondence analysis ===\n\nThis ordination is one of the most original ordination techniques, and builds form its underlying mechanics on the principal component analysis. However, since it is based on the chi square test, it is mainly applied for categorical data, although it can also be applied to count data, given that the dataset contains enough statistical power for this. In a nutshell, the correspondence analysis creates orthogonal axis that represent a dimension reduction of the input data, thereby effectively reducing the multivariate categorical data into artificial exes, out of which the first contains the most explanatory power. Typically, the second and third axis contain still meaningful information, yet for most datasets the first two axis may suffice. The correspondence analysis is today mostly negotiable in terms of its direct application, yet serves as an important basis for other approaches, such as the Detrended Correspondence analysis or the Canonical Correspondence analysis. This is also partly related to the largest flaw in the Correspondence analysis, namely the so called Arch-effect, where information on the first two axis is skewed due to mathematical representation of the data. Still, the underlying calculation, mainly the reciprocal averaging approach make it stand out as a powerful tool to sort large multivariate datasets based on categorical or count data. Consequently, the basic reciprocal averaging was initially very relevant for scientific disciplines such as ecology and psychology. \n\n=== Detrended Correspondence analysis ===\n\nHill can be credited with eliminating the arch- or horseshoe effect by a move that mathematicians will probably criticise until the end of time. What this analysis does is that it simply takes the geometric space that comes out of a Correspondence analysis and bends it into an even shape. In other words, the loadings out of a CA are detrended. This has several benefits. For once, you look at a dimension reduction that is visually more easy to interpret, because it does not follow the horseshoe effect of the CA. The second benefit that the detrending has is that you can calculate a measure that is called a turnover. A full turnover in the data is defined as the two most extreme data points do not share ay joined information or data. This can be a highly useful tool to access the heterogeneity of a dataset, and is indeed a unique measure to approximate diversity. While this has found raving success in ecology, there are few areas outside of this domain that realised the value of calculating turnovers. Equally is the DCA clearly less abundantly used outside of ecology, which is a loss for everyone with multivariate presence/absence or count data on their hand. Instead other fields usually opt for other approaches, which is partly rooted in their data structure and may also be partly attributed to their specific philosophy of science (i.e. inductive vs. deductive), but mostly due to different methodological developments in the respective fields. \nAnother advantage of the DCA is the possibility to postdoc fit environmental parameters of the plots onto the axes of the ordination. This allows to interpret how the environmental patterns relate to the respective ordination axes as well as individual plots. Since this posthoc test is based on a permutation analysis, there is even a crude measure on whether relations between environmental parameters and ordination axes are significant. \n\n\n=== Canonical correspondence analysis ===\n\nThis ordination method became really big around the millennium, and flips the script in terms of the analysis. Instead of reducing the multivariate species/plot matrix into artificial axes, and postdoc fit environmental information onto these axes, thus reducing the multivariate information based on the environmental variables, which is also often called secondary matrix. Within these multivariate axes the species information-or primary matrix- is integrated. The CCA thus fits the species into the environmental space, or in other words, the secondary metrix is used to calculate the axes reductions, and the primary matrix is implemented into this in a second step. This was a highly promising approach at the turn of the millennium, yet has decreased in importance over the last years. While it clearly has its specific benefits, the original promise to \"put things into even better order\" (Palmer 1993 Ecol) has only been partly fulfilled. No one ordination is better than any other ordination, but they all have their specific preconditions and use cases. \n\n=== Non metric (multi)dimensional scaling ===\n\nThis type of analysis has clearly a huge benefit compared to other approaches, it is able to deal with non-linearity in data. NMDS can include non-parametric as well as euclidean data, and it's thus a powerful ordination algorithms that can not only deal with a a variety of data and its distribution, but also with missing data. Since it can be quite computer demanding, the NMDS is still on the rise with rising computer capacity, yet has not cracked the largest datasets yet, which are beyond its reach so far. Still, since it is more flexible and adaptable in its approach, it has found raving success, and is among the newer approach sin the pantheon of ordinations. The main difference to other techniques is that the use predefines the number of axes they expect from the analysis. NMDS then reduces the multivariate data and calculates a stress value, which is a a measure how well the analysis reduced the multivariate information into artificial axes. The NMDS aces many datasets when it comes to group differentiations, which is probably one of the main reasons why it has become a darling of data scientists who often prefer groups over gradients. Some users propose to measure the R2 value of the analysis, and there are rumours about thresholds that defines an excellent fit. Just as other such thresholds this depends on the data and aim of the analysis, no one cut-off level can help here. Still, the NMDs is versatile, flexible, and surpasses many of its alternatives especially when the analysis is more rigorous and strutted and not just some initial fishing.\n\n=== Principal component analysis ===\n\nThe PCAS is probably the most abundantly used ordination technique. Focussed on an Euclidiean approach, it is unmached in analysing linear continuous data. Whenever you have a larger multivariate datasets, such as a table with many continuous variables and are interested in the main gradients of the dataset, the PCA is your universal and versatile tool. It reduces multivariate data into artificial main components based on linear combinations, and this is a powerful approach to test any given dataset for redundancies. Therefore, if you want to analyse a larger dataset through univariate analysis, the PCA is s staple to test your predictor dates for redundancies. It shows the predictor variables as vector, where lengths shows the correlative power with the artificial axes, and direction shows with which axes it correlates. The first axes explains the most, and the others subsequently less. Often the first axis can explain up to 50 % of the variance of a fasste, and you shall see that the following axis explain way less at a certain point. This is called a broken stick approach, which is used to eyeball how many axes need to be considered from the model. More often than not, it is two or three. Within trait analysis, it can also be more, and there are other examples where more axes are need to meaningfully represent the dataset. All this is again based on how homogenous the dataset is. Hence the PCA can also serve as a dimension reduction that allows us to reduce highly collinear data into artificial axes, which is helpful i.e. in climate science, remote sensing or engineering, where multicollinearity is pathologically high. In such branches of sciences, linear models are not even made with individual predictions, but instead with the PCA axes themselves, which are implemented in the univariate model.s While this literally cures the problem of redundancy and collinearity, it can make interpretation of the results much harder, as one always has to think around the corner on what the individual axes actually meant, and with which predictor variables they correlated. Still, or maybe exactly because of this, the PCA is one of the most powerful ordination techniques, and has proven its value over time. \n\n\n\n=== Redundancy analysis ==="
                    },
                    "sha1": "82h35j8niwa4a0ey45an3i02fezvk7n"
                }
            },
            {
                "title": "Outlier Detection in Python",
                "ns": "0",
                "id": "1076",
                "revision": {
                    "id": "7258",
                    "parentid": "7253",
                    "timestamp": "2023-06-30T05:29:12Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to outlier detection using Python.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16680",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n\n'''Outlier Detection in a nutshell:''' Outlier Detection includes methods identifying data points that deviate a lot from the rest of the data set.\n==Background==\n'''What is an outlier?''' An outlier is a data point that is significantly different from the other data points in a dataset. Its value is mostly unexpected and deviates a lot from the mean of the dataset. The task of outlier detection is to detect these points. However, one has to consider the context and the dataset when selecting a specific method for detecting outliers. Some examples of outliers include: 1. In a dataset of housing prices in a city, a data point representing a house that is significantly more or less expensive than the others could be an outlier. 2. In a dataset of exam scores for a class, a student who scores significantly higher or lower than the rest of the class could be an outlier. 3. In a dataset of consecutive GPS coordinates of sailing activities, coordinates with high deviation from the data points beforehand could be an outlier.\n'''\nWhy is it important to detect outliers?''' Outlier detection is important because it helps to identify unusual or anomalous data points that may not fit with the rest of the data and therefore will have an impact on further analysis of the data. For instance, a linear regression model would be immensely affected by outliers and lose its ability to generalize well over the dataset.\n\n''Where is outlier detection located in the analysis process?''' Outlier detection is typically a part of the data cleaning and exploration phase of the data analysis process. This is typically the first step in the process of analyzing a dataset, and it involves identifying and addressing any issues or problems with the data. Once the data has been cleaned and prepared, it can be analyzed using a variety of methods to draw conclusions, make predictions, or inform decisions\n\n'''What are reasons for outliers?''' There are many possible reasons why outliers may occur in a dataset. Some common reasons include: 1. Measurement errors: Outliers can occur due to errors in the measurement process, such as incorrect readings or faulty equipment. 2. Data entry errors: Outliers can also occur due to errors in data entry, such as transposing numbers or mistyping data. 3. Natural variations: In some cases, outliers may occur naturally as a result of natural variations in the data. For example, extreme weather events or rare occurrences may result in data points that are significantly different from the rest of the data. 4. Fraud: Outliers can also occur as a result of fraudulent activity, such as attempts to manipulate data or hide certain transactions. 5. Sampling errors: In some cases, outliers may occur due to sampling errors, such as a sample that is not representative of the population being studied.\n\nIt is important to carefully consider the possible causes of outliers in a dataset, as this can help to identify and understand the underlying reasons for unusual data points. In some cases, it may be necessary to exclude outliers from analysis, while in other cases it may be important to include them to get a complete picture of the\n\n==Applying Methods in Python==\nn this section different methods for detecting outliers in datasets are presented using Python code. To demonstrate the methodology of different approaches, a dataset containing accelerometer data (captured during cycling activity) is used. More files like that can be found under the following Kaggle dataset: [https://www.kaggle.com/datasets/nilshmeier/bike-underground-detection Kaggle Dataset].\n\nIn the first place, the dataset is loaded and limited to the first 10 seconds to make the visualizations of the outlier detection results better understandable.\n\n<syntaxhighlight lang=\"Python\" line>\ndata = pd.read_csv('Accelerometer_Data.csv', index_col=0)\ndata = data[data.time <= 10]\n</syntaxhighlight>\n\n===Visual Inspection===\nAlready during the explorative data analysis with visualizations, you can make a first screening of the data for outliers using boxplots. For boxplots, the data is ordered from small to large to assess the distribution. It can then be seen how many data points lie outside the whiskers, i.e. deviate more than 1.5 from the inter-quartile range (IQR) from the mean value. The interquartile range represented by the box describes the range of 25-75% of the data distribution and the whiskers at the top and bottom the respective last 25%. Any data point outside the whiskers then is outliers. This methodology can be used as a basis for the subsequent quantile method to determine the corresponding thresholds/quantiles and to detect outliers from them.\n\n<syntaxhighlight lang=\"Python\" line>\nfig, axs = plt.subplots(2, figsize=(16, 8))\nfig.suptitle('Visual Inspection of the Data', size=20)\nsns.lineplot(data=data, x='time', y='z', ax=axs[0])\naxs[0].set(title='Lineplot over Time')\nsns.boxplot(data['z'], orient='h', ax=axs[1])\naxs[1].set(title='Boxplot of Z-Axis')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 1.png|900px]]\n\n''Figure 1: Visual Inspection of Outliers''\n\nWe first create a plot with two subplots, one for the lineplot, and one for the boxplot. The line plot shows the development of the value z over time, and the boxplot shows the distribution with respect to the IQR of Z. You can see here already that there are some outliers. There are more outliers with larger than with smaller values. In the following, we will look at different approaches to detect outliers. Which method to use needs to be decided case by case, but you can compare the results in this article. In general, it is always advisable to try several approaches to fully understand your dataset.\n\n===Quantile Based===\nQuantile-based outlier detection is a fairly simple procedure in which points outside a certain quantile are considered outliers. For example, for thresholds 2 and 98 (as in the following code), all values that fall outside of these quantiles are marked as outliers. This means that data points at the top and bottom of the distribution are considered to be outliers. However, this approach runs the risk of incorrectly marking points as outliers since it does not take any further information about the distribution, like the standard deviation, into account.\n\n<syntaxhighlight lang=\"Python\" line>\n# Set the quantiles and get the respective values\nlower_q, upper_q = 0.02, 0.98## determine the qyartule thresholds.\nlower_value = data['z'].quantile(lower_q)## apply the lower threshold to dataset.\nupper_value = data['z'].quantile(upper_q)## apply the upper threshold to dataset.\n\n# Plot the results\nmask = (lower_value <= data.z) & (data.z <= upper_value)## create mask that differentiates between outlier and no outlier.\nfig, ax = plt.subplots(figsize=(16, 6))## create a plot\nfig.suptitle('Quantile-Based Outlier Detection', size=20) ## create title\nsns.scatterplot(data=data, x='time', y='z', hue=np.where(mask, 'No Outlier', 'Outlier'), ax=ax) ## apply color to outlier/no outlier\nplt.tight_layout() ##makes sure that all variables and axes are readable\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 2.png|900px]]\n\n''Figure 2: Outliers detected with quantile-based method''\n\n===Distribution Based===\nIn distribution-based outlier detection, the assumption is made that data from a measurement are normally distributed. Based on the mean and standard deviation of the data, a probability results for each point of the recording that belongs to this normal distribution. Using the so-called Chauvenet criterion, a lower limit can be defined below which a point is marked as an outlier. If the Chauvenet criterion is set to 0.5, all points that belong to the normal distribution of the data with a probability of less than 0.5% are recognized as outliers. Depending on how high the criterion is set, the number of detected outliers changes accordingly (this is illustrated in figure 3). How high the optimal limit is, i.e. to which value the Chauvenet criterion must be set, depends on the further goal and is often only determined over time. This approach is closely related to the Z-score, which is a measure of the deviation of a point from the mean of a standardized normal distribution.\nThe Chauvenet criterion can be applied to both univariate and multivariate data, offering the possibility to apply outlier detection either to single attributes or to a combination of attributes.\n\n<syntaxhighlight lang=\"Python\" line>\n# Set the quantiles and get the respective values\nlower_q, upper_q = 0.02, 0.98## determine the qyartule thresholds.\nlower_value = data['z'].quantile(lower_q)## apply the lower threshold to dataset.\nupper_value = data['z'].quantile(upper_q)## apply the upper threshold to dataset.\n\n# Plot the results\ndef univariate_chauvenet(data_points: np.ndarray, criterion: float = None):\n    # Calculate mean and std of the data\n    mean = data_points.mean()\n    std = data_points.std()\n\n    # Calculate the criterion based on the number of points if not provided\n    if criterion is None:\n        criterion = 1.0/(2 * len(data_points)) ## criterion is set to 1 divided 2 times the length of the data set.\n\n    # Calculate the absolute deviation and scale by std\n    deviation = np.abs(data_points - mean) / std\n\n    # Calculate the probabilities using erfc and return the mask. Erfc is the error function calculating the probability of a data point (not) being an outlier.\n    # if the probability is lower than the criterion (so the likelihood that the data point is no outlier is smaller than the set criterion), it is an outlier.\n    probabilities = special.erfc(deviation)\n    return probabilities < criterion\n\n\nfig, axs = plt.subplots(ncols=3, figsize=(16, 5))##create plot with three sub-plots\nfig.suptitle('Univariate Chauvenet Criterion', size=20)\nfor i, c in enumerate([0.01, 0.005, 0.001]): ## creating a for loop to check for outliers for different criteria.\n    mask = univariate_chauvenet(data_points=data['z'].values, criterion=c)## create mask as above\n    sns.scatterplot(data=data, x='time', y='z', ax=axs[i],\n                    hue=np.where(mask, 'Outlier', 'No Outlier'),\n                    hue_order=['No Outlier', 'Outlier']) ##create scatter plots colored according to the mask.\n    axs[i].set(title=f'Criterion = {c} with {sum(mask)} Outliers')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n\n[[File:Outliers 3.png|900px]]\n\n''Figure 3: Outliers detected with the Chauvenet Criterion (univariate)''\n\nWe can also show the z value and the y value in one graph with multivariate data.\n\n\n<syntaxhighlight lang=\"Python\" line>\ndef multivariate_chauvenet(data_points: np.ndarray, criterion: float = None) -> np.ndarray:\n    # Calculate the mean and covariance matrix of data points\n    mean = np.mean(data_points, axis=0)##determine mean for each column\n    covariance = np.cov(data_points.T)## determine covariance between y and the value z. For this, the data points arrays need to be transposed.\n\n    # Calculate criterion if not provided\n    if criterion is None:\n        criterion = 1 / (2 * len(data_points))\n\n    def calculate_probability(x: np.ndarray) -> float:\n        p = 1 / (2 * np.pi * np.linalg.det(covariance) ** 0.5) * \\\n            np.exp(-0.5 * np.matmul(np.matmul((x - mean), np.linalg.inv(covariance)), (x - mean)))\n        return p ## calculate the probability of a data point X being in a multivariate normal distribution with a given probability density function\n\n    # Calculate probabilities and return the mask\n    probabilities = np.array([calculate_probability(x) for x in data_points])\n    return probabilities < criterion\n\n\nmask = multivariate_chauvenet(data_points=data[['y', 'z']].values##create mask\nfig, ax = plt.subplots(figsize=(8, 6))##create plot\nfig.suptitle('Multivariate Chauvenet Criterion', size=20)\nsns.scatterplot(data=data, x='y', y='z', hue=np.where(mask, 'No Outlier', 'Outlier'), ax=ax)##create plot with colors accroding to status of (no) outlier\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 4.png|900px]]\n\n''Figure 4: Outliers detected with the Chauvenet Criterion (multivariate)''\n\n===Neighbor Based===\nAnother approach to identifying outliers is the so-called Local-Outlier-Factor. It works by measuring the local deviation of a data point with respect to its neighbors, and it compares this deviation to the average deviation of the neighbors. A data point that has a significantly higher deviation than its neighbors is considered to be an outlier. By choosing the number of neighbors to consider, the number of outliers and their positioning within the data distribution is affected (see figure five).\n\n<syntaxhighlight lang=\"Python\" line>\nfig, axs = plt.subplots(ncols=3, figsize=(16, 5))## create plot with three sub-plots\nfig.suptitle('Neighbor-Based Outlier Detection', size=20)\nfor i, n in enumerate([5, 10, 20]):## the number determines how many neighbored data points are considered\n    lof = LocalOutlierFactor(n_neighbors=n)\n    mask = lof.fit_predict(data[['y', 'z']])\n    sns.scatterplot(data=data, x='y', y='z', hue=np.where(mask==1, 'No Outlier', 'Outlier'), ax=axs[i])\n    axs[i].set(title=f'Neighbors = {n} with {sum(mask == -1)} Outliers')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 5.png|900px]]\n\n''Figure 5: Outliers detected with the neighbor-based method'' \n\n==What to do with Outliers==\nThe methods presented provide information about which data points are very likely to be outliers and show strong differences from the majority of the data set. The next step is to decide how to deal with the outliers. There are essentially two different variants for this:\n\n1. Completely remove outliers from the data set. The corresponding entries in the data set are not considered further in the analysis. However, it is not recommended to remove all outliers from data sets with many attributes, since potentially quite a lot of entries are removed across the different attributes. \nIf you go back to the detection of outliers using IQR, you can easily identify the outliers and excluded them from the dataset (this works for all outlier detection methods, this one has been chosen as an example.\n\n<syntaxhighlight lang=\"Python\" line>\n# Remove outliers based on IQR method\nq1 = data['z'].quantile(0.25)\nq3 = data['z'].quantile(0.75)\niqr = q3 - q1\nupper_bound = q3 + 1.5 * iqr\nlower_bound = q1 - 1.5 * iqr\ndata_no_outliers = data[(data['z'] > lower_bound) & (data['z'] < upper_bound)]\n\n##check data\nfig, axs = plt.subplots(2, figsize=(16, 8))\nfig.suptitle('Visual Inspection of the Data, No Outliers', size=20)\nsns.lineplot(data=data_no_outliers, x='time', y='z', ax=axs[0])\naxs[0].set(title='Lineplot over Time')\nsns.boxplot(data_no_outliers['z'], orient='h', ax=axs[1])\naxs[1].set(title='Boxplot of Z-Axis')\nplt.tight_layout()\nplt.show()\n</syntaxhighlight>\n\n[[File:Outliers 6.png|900px]]\n\n''Figure 6: Boxplot and line plot after the outliers have been removed''\n\n2. Alternatively, outliers can be replaced by NaN values and thus converted into missing values. However, it is recommended to perform the detection of outliers only on single attributes. The missing values can then be filled by value imputation methods.\n\n<syntaxhighlight lang=\"Python\" line>\n# Calculate the IQR\nQ1 = data['z'].quantile(0.25)\nQ3 = data['z'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define the boundaries\nlower_boundary = Q1 - 1.5 * IQR\nupper_boundary = Q3 + 1.5 * IQR\n\n# Replace outliers with NaN\ndata_nan = data.copy()\ndata_nan.loc[(data_nan['z'] < lower_boundary) | (data_nan['z'] > upper_boundary), 'z'] = np.nan\n</syntaxhighlight>\n\n\n3. A third approach would be to keep the outliers and their respective values in the dataset but add a binary column that indicates whether a point is considered an outlier or not. When fitting models on the data, the model might learn to subtract a certain value for outliers. However, this approach is not the best-practice and should only be used if none of the above methods works well.\n\n==Key Publications==\n\n* P.A. MacMahon: The Expression of Syzygies among Perpetuants by Means of Partitions (1888) ([DOI](https://doi.org/10.2307/2369505 ) ) \n* Breunig et al.: LOF: identifying density-based local outliers (2000) ([DOI](https://doi.org/10.1145/335191.335388 ) ) \n* Prakhar Mishra: 5 Outlier Detection Techniques that every \"Data Enthusiast\" Must Know (2021) ([Link](https://towardsdatascience.com/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210))\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "ohbjmj26cd4ukk6nzn6suw8cgb8o3zv"
                }
            },
            {
                "title": "Overcoming Exam Anxiety",
                "ns": "0",
                "id": "462",
                "revision": {
                    "id": "3264",
                    "parentid": "2969",
                    "timestamp": "2020-11-04T10:25:45Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5044",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]]|| [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n==What, Why and When==\nIt is perfectly normal to feel some kind of nervousness or excitement before an important exam or presentation. This will actually help you to stay focused during the test because of higher adrenalin levels in your body. However, there are cases when the feeling of fear and worrying thoughts are so strong that they inhibit a persons ability to concentrate properly and create strong physical symptoms. These can include headaches, an upset stomach, feelings of fear and dread, sweating, shortness of breath, overthinking, black outs and more.\nTest anxiety most often occurs shortly before or during an exam or similar test situations. In severe cases it can negatively affect a students overall social life or emotional development, and - of course - the academic career. Therefore it is important to know how to handle feelings of anxiety that may occur in your life as a student at university.\n  \n==Goals==\n* Overcome feelings and symptoms of anxiety before an exam or test in order to perform successfully\n* Get to know strategies that help you stay calm and focused - at uni and in your day-to-day life\n* Strengthen your confidence and learn to trust your own abilities\n\n==Getting Started==\nThere are several strategies that can help you stay calm before or during an exam and you can practice them! This will help you to get out of the vicious cycle exam anxiety can create: Fear of failure and physical discomfort during an exam might lead to lower marks which can convince the student of his self-attributed inability to master the subject. In turn this can create even more anxiety.\nHere is what you can do to create positive change:\n* For some it might already help to pay more attention to their overall lifestyle. Getting enough sleep, proper nutrition, and regular exercise strengthen the body's resources and ability to deal with stressful situations, which is not only beneficial for exams but for life in general \n* Additional stress-management practices like meditation, mindfulness, breath work and study routines will enable you to actively calm your mind and body in your day to day life. Using this knowledge before a test can help you get back into a quiet, focused, and calm state.\n* Be prepared! Even though exam anxiety does not necessarily result from poor preapration or lack of knowledge, it can be helpful psychologically to know that you have revised and studied enough - maybe with a group of friends/peers or an advanced tutor.\n* Preparation also includes finding out about the location and procedure of the test, the material you have to bring, and, of course, the time frame.\n* Maybe you have already tried all these things and they did not offer you the relief you expected? Or your fear is to overwhelming to even get started? Exam anxiety is a very common fear among students of all backgrounds and it is not a fear to be ashamed of. It can already help to open up to someone you trust.\n\n==Links and Further Reading==\n====Where to learn more====\n'''Books'''\n* Walther, Holger (2015): Ohne Pr\u00fcfungsangst studieren, 2. Auflage. Konstanz: UTB.\n* Hafner, Bettina; Kronenberger, Ursula (2015): Entspannt Pr\u00fcfungen bestehen. Ein Manual f\u00fcr Studierende in Lern- und Pr\u00fcfungszeiten. Bern: Hans Huber Verlag.\n* Bernstein, Ben (2018): Crush Your Test Anxiety. How to Be Calm, Confident, and Focused on Any Test, 2nd ed. Sanger: Familius.\n* Weisinger, Hendrie; Pawliw-Fry, J.P. (2015): Performing Under Pressure: The Science of Doing Your Best When It Matters Most. Redfern: Currency.\n\n'''Videos'''\n* [https://www.youtube.com/watch?v=fHfHSq7PVDU Thomas Frank: How to Beat Test Anxiety]\n* [https://www.youtube.com/watch?v=bgtUjDcLIXc Peter Beer: Nervosit\u00e4t und Pr\u00fcfungsangst \u00fcberwinden]\n\n==== Where to get help ====\nThere are a lot of resources - online and offline - you can use to get professional help, even here at Leuphana:\n# The Studienberatung regularly offers workshops on all kinds of things related to stress and time management strategies and exam preparation that you can take part in: [https://www.leuphana.de/college/studienberatung/studierende/workshops.html Workshops at Leuphana]\n# You can make a free appointment at the Psychologische Beratungsstelle (Studentenwerk) where you can speak to a professional and learn more about helpful strategies and resources: [https://stw-on.de/lueneburg/beratung/pbs Psychologische Beratungsstelle]\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]][[Category:Personal Skills]][[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Katharina Kirn."
                    },
                    "sha1": "1suhsq1vs30g2feo1csu3bdu6g9b1zr"
                }
            },
            {
                "title": "Partial Correlation",
                "ns": "0",
                "id": "946",
                "revision": {
                    "id": "6593",
                    "parentid": "6592",
                    "timestamp": "2022-03-21T08:51:13Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Example for Partial Correlation using R */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "8047",
                        "#text": "'''Note:''' This entry revolves specifically around Partial Correlations. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n<br/>\n'''In short:'''Partial Correlation is a method to measure the degree of association between two variables, while controlling for the impact, a third variable has on both. When looking at correlations, they can often be misleading and must be treated with care. For example, this [https://www.tylervigen.com/spurious-correlations website] provides a list of strongly significant correlations that do not seem to have a meaningful and direct connection. This undermines that correlation is not necessarily related to [[Causality_and_correlation|causality]]. On the one hand, this may be due to the principle of null hypothesis testing (see [[Designing_studies#P-value|p-value]]). But in the case of correlation, strong significances can often be caused by a relationship to a common third variable and are therefore biased.\n__TOC__\n\n== Problem Set ==\n[[File:matthewsexample.png|300px|thumb|left|Fig.1: Stock populations and human birth across Europe. From Matthews (2001).]]\n[[File:partcorreltns.png|300px|thumb|left|Fig.2: Model of partial correlations.]]\nLet's consider this example from [https://onlinelibrary.wiley.com/doi/10.1111/1467-9639.00013 Matthews (2001)] on stork populations and human birth rates across Europe (Fig.1).\nA correlation between both variables indicates a statistically significant positive relation of r = .62. But can we really make sense of this correlation? Does it mean that storks deliver babies? It is very likely that the connection can be explained by one (or more) external variable(s), for example by how rural the area of measurement is (see Figure 2). In a more rural area, more children are born and at the same time there is more space for storks to build their nests. So how can we control for this effect in a correlation analysis?\n\n==Assumptions==\nTo use partial correlation and gain valid results, some assumptions must be fulfilled. No worries if these assumptions are violated! There is often still a work-around that can be done to bypass them.\n# Both variables and the third variable to control for (also known as control variable or covariate) must be measured on a '''[[Data_formats#Continuous_data|continuous]] scale'''.\n# There needs to be a '''linear relationship''' between all three variables. This can be inspected by plotting every possible pair of variables.\n# There should be '''no significant outliers'''. Partial correlation is sensitive to outliers, because they have a large effect on the regression and correlation coefficients. Determining if a point is really a significant outlier, is most likely based on personal experience and can therefore be ambiguous\n# All variables should be '''approximately normally distributed'''.\n\n== Notion of Partial Correlation ==\nTo check if a connection between two variables is caused by a common third variable, we test whether the relationship still exists after we have accounted for the influence of the third variable. This can be achieved by calculating two simple [Regression Analysis|linear regressions] to predict each of the two variables based on the expected third variable. Thus, we account for the impact the third variable has on the variance of each of our two variables.\n[[File:prtcrl1.png|450px|frameless|center]]\nThat way we can now consider the left-over (unexplained) variance - namely the residuals of the regressions. To better understand, why we are doing this, let\u2002s look at an individual data point of the regressions. If this point exceeds the predictions for both regressions equally, then there seems to be a remaining connection between both variables. So, when looking back at the broad picture of every data point, this means that if the residuals of both regressions are unrelated, then the relationship of the two variables could be explained by the third variable.\n[[File:prtcrl2.png|450px|frameless|center]]\nSo in conclusion, to control for the impact of a third variable, we calculate the correlation between the residuals resulting from the two linear regressions of the variables with respect to the third variable. The resulting formula for partial correlation is:\n[[File:prtcrl3.png|450px|frameless|center]]\n\nIt is also possible to add more than one control variable. We then use another, recursive [https://en.wikipedia.org/wiki/Partial_correlation#Using_recursive_formula formula]. However, it is not recommended to add more than 2 control variables, because more variables impair the [https://www.youtube.com/watch?v=m0W8nnupcUk reliability] of the test and the complexity of the recursive algorithm increases exponentially. When checking for the \u201cunbiased\u201d effect of multiple variables, it might be better to use a multiple linear regression instead.\n\n== Partial Correlation vs. Multiple Linear Regression ==\nWhen thinking about partial correlation, one might notice that the basic idea is similar to a multiple linear regression. Even though a correlation is bidirectional while a regression aims at explaining one variable by another, in both approaches we try to identify the true and unbiased relationship of two variables. So when should we use which method? In general, partial correlation is a building step for multiple linear regression. Accordingly, linear regression has a broader range of applications and is therefore in most cases the method to choose. Using a linear regression, one can model interaction terms and it is also possible to loosen some\nassumptions by using a [[Generalized_Linear_Models|generalized linear model]]. Nevertheless, one advantage, partial correlation has over linear regression, is that it delivers an estimate for the relationship which is easy to interpret. The linear regression outputs weights that depend on the unit of measurement, making it hard to compare them to other models. As a work-around, one could use standardized beta coefficients. Yet those coefficients are harder to interpret since they do not necessarily have an upper bound. Partial correlation coefficients, on the other hand, are defined to range from -1 to 1.\n\n== Example for Partial Correlation using R ==\nPartial Correlation can be performed using R. Let\u2002s consider the following (fictitious) data. The capital of the following sample of workers is correlated to the number of annual doctor visits (r \u2248 .895).\n\n<syntaxhighlight lang=\"R\" line>\n# Fig.3\ndf <- data.frame(doctor_visits, capital, age)\n</syntaxhighlight>\n[[File:prtcrl4.png|450px|thumb|right|Fig.3: \"doctor_visits\" data frame.]]\nOne way to perform a partial correlation on this data is to manually calculate it.\n\n<syntaxhighlight lang=\"R\" line>\n# First, we calculate the 2 simple linear regressions\nmodel1 <- lm(doctor_visits~age)\nmodel2 <- lm(capital~age)\n# Next, we save the residuals of both linear models\nres1 <- model1$residuals\nres2 <- model2$residuals\n# Finally, we can calculate the partial correlation by correlating the residuals\ncor(res1, res2)\n\n## Output:\n## [1] 0.08214041\n</syntaxhighlight>\n\nHowever, there are also a lot of pre-build packages for calculating a partial correlation. They provide an easier use and are especially more convenient when controlling for more than one variable. One example is the package <syntaxhighlight lang=\"R\" inline>ppcor</syntaxhighlight>.\n\n[[File:prtcrl5.png|450px|thumb|right|Fig.4: Partial-correlation matrix]]\n<syntaxhighlight lang=\"R\" line>\nif (!require(ppcor)) install.packages('ppcor')\nlibrary(ppcor)\n# The function generates a partial-correlation-matrix of every variable in the data frame!\n# Fig.4\npcor(df)$estimate\n</syntaxhighlight>\n\nWe can see that the result for doctor visits and capital is identical to our manual approach.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Daniel Schrader."
                    },
                    "sha1": "iyx4yeg0qh06g5yfn515mnwzwaf4xha"
                }
            },
            {
                "title": "PechaKucha\u2122",
                "ns": "0",
                "id": "336",
                "revision": {
                    "id": "5930",
                    "parentid": "5029",
                    "timestamp": "2021-06-30T17:40:17Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3896",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n===='''Motto''': ====\n ''\"PechaKucha\u2122 is what \u201cShow and Tell\u201d always dreamed of becoming.\"''<small> (as seen on the PechaKucha\u2122 website)</small> \n\n== What, Why & When ==\nPechaKucha\u2122 is a 20x20 format for short presentations consisting of exactly 20 slides on auto-timer, with each slide being displayed for 20 seconds. This results in a talk of 6 minutes and 40 seconds. \n\nA PechaKucha\u2122 presentation is a great alternative to the classical, plain slideshow, and it is especially useful for [[Glossary|storytelling]] or for presenting creative portfolios. More generally, it can be used for any subject when there is one clear idea to be communicated (e.g. a concise overview of your research results). The slides usually contain one full-width image or photograph each, which makes this format very engaging for the audience. Because of the limited duration and the auto-timer, the speaker is required to sharpen their message when preparing their talk, and he is forced to stay on track when delivering it.\n\n== Name and Origin ==\nPechaKucha\u2122 means \"chit chat\" in Japanese. This format was invented by Tokyo-based architects Astrid Klein and Mark Dytham because they thought people in their industry talked too much. Instead, they wanted to set an example by delivering presentations where the focus was on actually ''showing'' the work.\n\nSince then, PechaKuchas\u2122 became a global phenomenon, with regular \"PechaKuchas\u2122 events\" or nights being held in major cities from more than 140 countries. It is also used widely within the corporate world, as well as in schools as universities as an educational tool.\n\n== Goals ==\nPechaKuchas\u2122 are about helping you to:\n\n* Present something in exactly 6 minutes and 40 seconds \n* Be concise and clear\n* Stay on time\n* Engage your audience with compelling visuals\n* Put your spoken message at the center (vs. slides with too much text)\n\n== Getting started ==\n\nYou can prepare your PechaKucha\u2122 by using a conventional presentation software (e.g. MS Powerpoint), but also simply as a .pdf to be presented with Adobe Reader (yes, it seems Adobe Reader also has an auto-timer function, see [https://www.cedarville.edu/insights/blog/pdf-turn-into-presentation.aspx here] how to do that).\n\n==== Steps: ====\n# Decide on your key message; \n# Create a story line for 20 slides;\n# Choose 20 images for your slides;\n# Set the auto-timer so that it switches to the following slide after 20 seconds;\n# Practice your presentation (don't underestimate the importance of this step!);\n# ''(Optional)'' Instead of Step 5, you can also pre-record your voice and play it with the slideshow (or combine them into a movie).\n \nGood luck!\n\n== Links & Further reading ==\n* [https://www.pechakucha.com/ The official website of PechaKucha\u2122]. Find here: \n** information about events around the world \n** many examples of PechaKucha\u2122 talks\n** the online platform where you can create your own PechaKucha\u2122 as a video with pre-recorded audio (but possibly for a fee)\n* [https://www.cedarville.edu/insights/blog/What-is-a-Pecha-Kucha-Presentation.aspx  Technical tips] for making a PechaKucha in Powerpoint or as a .pdf\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Software]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Cristina Apetrei."
                    },
                    "sha1": "qgcp50l0dkfy6k9ewxsevrn0goxy41z"
                }
            },
            {
                "title": "Persona Building",
                "ns": "0",
                "id": "315",
                "revision": {
                    "id": "5772",
                    "parentid": "3266",
                    "timestamp": "2021-06-13T22:39:44Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Getting started */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6659",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nPersona Building is a tool that helps better understand the target group when designing a product or service. In Persona Building, a specific fictional user is imagined with his or her experiences, needs and characteristics, and the prospective product or service can be developed more specifically for this persona. This is mostly done in business for marketing purposes, where the personas are potential customers or people to pitch an idea to. It can nevertheless also be helpful for any design process that aims to fulfill the needs of a specific audience (see [[Design Thinking]])\n\n== Goal(s) ==\n* Understand your target audience's needs and wishes and establish a common reference point in your design team.\n* Facilitate and speed up design and decision-making processes.\n* Make the outcome more user-focused and distinct.\n\n== Getting started ==\nPersona Building starts with the '''gathering of data''' about the target groups. This can be [[Glossary|data]] that already exists, such as large-scale surveys, information on potential user groups such as the [https://www.sinus-institut.de/sinus-loesungen/sinus-milieus-deutschland/ SINUS milieus], or observations made in terms of the audiences of comparable products and services. It can also emerge from knowledge and experiences within the team, or be original data gathered for the specific design purpose, if the resources are available for that. \n\n'''Based on this data, overlappings and trends about the targeted audience are identified.''' The persona that is subsequently built should represent these identified elements. Importantly, a persona should not become a prototypical or stereotypical puppet. A persona is not the sum of survey data or a demographic group. Instead, one should attempt to imagine an archetype of a potentially real person with flaws, contradictions and different facets. No more than a handful (three to four) personas (if more than one at all) should be built at once in order to maintain a sharp idea.\n\nEach created persona should possibly include a '''drawing or picture''' of the imagined person and his or her '''name'''. This way, the persona becomes more vivid and recognizable within the project. Furthermore, the persona should be described in terms of his or her '''characteristics''': his or her background, hobbies and job, lifestyle, family situation, needs, problems, preferences and desires. This helps understand how and why the person would be interested in the to-be-designed product or service, and how it can be adapted and improved to better fit the persona's circumstances. The designers should try to understand their offer from the persona's perspective.\n\nLastly, the persona should be '''implemented into the design process'''. From this point on, all team members should know about the persona and speak in terms of his or her specific needs and actions instead of speaking about broad needs of a potential target audience. The persona can be thrown into imagined scenarios and situations to understand how he or she would react. This way, the design process becomes more user-focused. Also, all team members, which may come from different disciplines or departments, now have a common reference point to speak about.\n\n== Example ==\nLet us develop an example. Imagine you are attempting to develop a solution for smart mobility in the city. You may be working in the city planning department, or in a start-up that tries to develop new technologically or socially innovative solutions. You do know what the general service should be - something that makes cycling more attractive in cities -, but you are unsure of the specificities.\n\n[[File:Cyclists Persona Building.jpg|200px|thumb|left|'''Cyclist Lisa is a result of Persona Building.''' Source: Pixabay.]]\n\n'''Now Persona Building comes into play.''' You develop a handful of Personas based on the target audience you want to provide with new opportunities for a change in their mobility behaviour. So, first of all, we think of Lisa.\n\nLisa is 30 years old and has a six year old daughter, Nele. Lisa works at an insurance firm in a middle-sized town and rides the bike to work. She likes cycling, also with her daughter, but the city only has bikelanes for some parts of her commute and into town. So, Lisa is frustrated about the stressful interactions with cars, as well as concerned about her own and her daughter's safety. Lisa would be ready to pay some money for a good solution, but she doesn't want to give up cycling.\n\nWe must make sure not to make Lisa a hippie-eco-cyclist-stereotype, but her situation is surely archetypical for many people in cities nowadays. You could think of more details in terms of Lisa's personality, her hobbies, her background and future, but the most important points are set: You know Lisa's situation, her problems, desires and circumstances - at least those that concern your design process.\n\nSo, Lisa is now your first persona. The solution that might fit her needs and circumstances might be different depending on your design context (More bike lanes? A product that makes cycling safer on streets?), but focusing on Lisa and her daughter makes it easier for you to make decisions and find a solution that fits her needs. By developing even more personas and incorporating their needs into your design process, the product will become more and more graspable and finally, you have a solution that is hopefully attractive for your target audience.\n\n== Links & Further reading ==\n''Sources''\n* [https://uxplanet.org/how-to-create-personas-step-by-step-guide-303d7b0d81b4 UX Planet.] How to create Personas, a step by step guide.\n* [https://medium.com/ignation/your-guide-to-successful-persona-building-f3dae3f4762d Medium]. Your guide to successful persona building. \n* [https://uxpressia.com/blog/how-to-create-persona-guide-examples UXPRESSIA.] How to Create a Persona in 7 Steps - A Guide with Examples.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "gs43m6bzlvyv6yo2lfzvw7n38b2r3ci"
                }
            },
            {
                "title": "Pie Charts",
                "ns": "0",
                "id": "768",
                "revision": {
                    "id": "5598",
                    "parentid": "5460",
                    "timestamp": "2021-05-28T12:00:24Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Description */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3795",
                        "#text": "'''Note:''' This entry revolves specifically around Pie charts. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n__TOC__\n<br/>\n\n== Pie chart ==\n===Description ===\nPie charts represent parts of a whole in a circle. This is done by dividing the circle into multiple slices. The area of each slice is proportional to the quantity it represents. These slices of the circle are reminiscent of a cake or a pie hence the name. 1801 was the oldest known pie chart published by William Playfairs in the book The Statistical Breviary.\n<br/>\n<br/>\n'''Why pie charts shouldn\u2019t be used?'''\n<br/>\nPie charts are frequently used in non-scientific journals. However, the use of pie charts is especially in the scientific context discouraged. Often they contain so little information that one sentence in the text would have been sufficient instead. In other cases where they could contain more information, (stacked) bar charts have proven to be superior.\n\nCompared to stacked bar charts, pie charts are slower to interpret and more mistakes are made when interpreting them. One explanation for this is the following: only the length of the individual parts of the bar chart have to be considered to determine their share of the whole while for pie charts the area of the slices has to be considered. Since the human eye and brain can more easily determine the lengths than the size of areas, especially when one side of the area is curved as it is the case with pie charts, stacked bar charts are superior.\n\nIn a nutshell: The use of pie charts should be well considered and avoided in most cases.\n\n===R Code===\nThe R documentation is not found of the pie chart either and advises against using it. The pie charts in R are not actual circles but polygons (by default with 200 edges) which approximate a circle. You can see by yourself that this circle does not look smooth.\n[[File:Cylindersmtcars.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=\"R\">\n#Fig.1\n#read in the dataset\ndata(\"mtcars\")\n#count how often each cylinder type appears in the data set\ncylinders <- table(mtcars$cyl)\n#creating a pie chart that displays the number of cylinders\npie(cylinders,\n    labels = c(\"4 Cylinders\", \"6 Cylinders\", \"8 Cylinders\"),\n    main = \"Number of cylinders in the mtcars data set\")\n#This gives us a really basic pie chart\n</syntaxhighlight>\n\n[[File:Cylindersmtcars2.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\">\n#Fig.2\n#Now we create a more sophisticated pie chart\n#read in the dataset\ndata(\"mtcars\")\n#count how often each cylinder type appears in the data set\ncylinders <- table(mtcars$cyl)\n#calculate % values\npercent<- round(100*cylinders/sum(cylinders), 0)\n#create % labels\npercent_labels <- paste(percent, \"%\", sep = \"\")\n#add the % labels\n#add a colour scheme\n#increase the size of the pie chart\n#increase the smoothness of the circle\n#change the order of the pieces to clockwise\npie(cylinders,\n    labels = percent_labels,\n    col = heat.colors(3),\n    radius = 1,\n    edges = 1000,\n    clockwise = TRUE,\n    main = \"Number of cylinders in the mtcars data set\")\n#adding a legend on the right side of the pie chart with the same colors\n#cex scales the size of the legend\nlegend(\"right\", c(\"4 Cylinders\", \"6 Cylinders\", \"8 Cylinders\"),\n       cex = 0.8, fill = heat.colors(3))\n#This pie chart looks a lot better than the previous one.\n#But the problem remains that this pie chart only shows 3 different categories.\n#A sentence or a representation as a bar chart would be more appropriate.\n</syntaxhighlight>\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Oskar Cybulski."
                    },
                    "sha1": "rioyg9tmsxn4p674ibetbhut7yosobz"
                }
            },
            {
                "title": "Poisson Distribution in Python",
                "ns": "0",
                "id": "1088",
                "revision": {
                    "id": "7259",
                    "parentid": "7254",
                    "timestamp": "2023-06-30T05:42:48Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to the Poisson distribution theoretically and in Python.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "13029",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==1. Introduction to Probability Distribution==\nA probability distribution summarizes the probabilities for the values of a random variable. The general properties/features that define a distribution include the four mathematical moments: \n\n1.\t'''Expected value E(X)''': This is the outcome with the highest probability or likelihood or an average or mean value for the variable X.\n \n2.\t'''Variance Var(X)''': Variance denotes the spread of the values from the expected value. The standard deviation is the normalized value of variance obtained by taking a square root of the variance. The covariance summarizes the linear relationship for how the two variables change with respect to each other. \n\n3.\t'''Skewness''': Skewness is the measure of asymmetry of a random variable X about the mean E(X) of a probability distribution. It can be positive (tail on the right side), negative (tail on left side), zero (balanced tail on both sides) or undefined. Zero skewness does not always mean symmetric distribution as one tail can be long and thin and the other can be short and fat. \na\n4.\t'''Kurtosis''': Kurtosis is the measure of \u2018 *peaked-ness* \u2019 of distribution. It can be differentiating tool between distributions that have the same mean and variance. Higher kurtosis means a peak and fatter/extended tails. \n\nEach random variable has its own probability distribution that may have a similar shape to other variables, however, the structure will differ based on whether the variable is discrete or continuous, since probability distributions of continuous variables have an infinite number of values and probability functions of discrete variables assign a probability to each concrete data point. \n\n==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=\"wikitable\"\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}\n\nPoisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https://studiousguy.com/poisson-distribution-examples/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===\n\n''The Poisson distribution probability mass function (pmf) gives the probability of observing k events in a period given the length of the period and the average events per time.''\n\n[[File:1 equation.PNG|center|250px]]\nT = Time interval \ne = natural logarithmic base \nk = number of events \nThe K! means K factorial. This means that we multiply all integers from K down to 1. Say K is 4 then K!= 4* 3* 2* 1= 24\n\nIntroducing lambda, \u03bb, as a rate parameter (events/time)*T into the equation gives us \n[[File:2 equation.PNG|center]]\n\nSo, \u03bb is basically the expected number of event occurrences in an interval, a function of '''number of events''' and the '''time interval'''. Changing \u03bb means changing the probability of event occurrence in one interval.\n\nFor example, Ladislaus Bortkiewicz calculated the deaths of soldiers by horse kicks in the Prussian Army using Poisson Distribution in the late 1800s. He analyzed two decades of data from up to 10 army corps, equivalent to two centuries of observations for one army corps.\n[[File:Horse.png|center|500px]]\nFigure 1: Poisson Distribution for deaths by horse kick by Ladislaus Bortkiewicz. Source: [https://www.scribbr.com/statistics/poisson-distribution/#:~:text=You%20can%20use%20a%20Poisson,days%20or%205%20square%20inches. Scribbr]\n\nAccording to his observation, an average of 0.61 soldiers died every year. However, the deaths were random, for example, in one year four soldiers died and for most years no deaths were caused by horses. Figure 1 shows the probability mass function graph.\nIn Poisson Distribution terms,\n* Death caused by a horse kick is an \u2018event\u2019\n* Mean per time interval, represented by \u03bb, is 0.61\n* The number of deaths by horse kick in a specific year is k.\n\n[[File:Pmf.png|center|500px]]\nFigure 2 is the probability mass function of the Poisson distribution and shows the probability (y-axis) of a number of events (x-axis) occurring in one interval with different rate parameters. You can see the most likely number of event occurrences for a graph is equivalent to its rate parameter (\u03bb) as it is the expected number of events in one interval when it is an integer. When it is not an integer, the number of events with the highest probability would be nearest to \u03bb. The Lamda, \u03bb, also represents the mean and variance of the distribution.\n\n==4. Features and Properties==\nSome properties of Poisson distribution are:\n\n1. As seen in figure 2, if a quantity is Poisson Distributed with rate parameter, \u03bb, its average value is \u03bb.\n\n2. Variance of the distribution is also \u03bb, implying when \u03bb increases the width of the distribution goes as square root lambda \u221a\u03bb.\n\n3. A converse in Raikov\u2019s theorem says that if the sum of two independent random variables is Poisson distributed, then so are each of those two independent random variables Poisson distributed.\n\nConsider the example of radioactive decay for long-lived isotopes, in a radioactive sample containing a large number of nuclei, each of which has a tiny probability of decaying during some time interval, T. Let\u2019s say the rate of decay is 0.31 decay/second and is monitored for 10 seconds. That gives us the \u03bb1 0.31 * 10 = 3.1 which means the probability equals,\n[[File:3 equation.PNG|center]]\nConsider another example where \u03bb2 is 2.7, the probability is\n[[File:4 equation.PNG|center]]\n\nTherefore,\n'''If we look at the total number k = k1 + k2 of a radioactive decay in a time T, the result is also a Poisson distribution with \u03bb = \u03bb1 + \u03bb2 -> \u03bb = 3.1 + 2.7 = 5.8 : P(k, \u03bb1 + \u03bb2 )'''\n[[File:5 equation.PNG|center]]\n\n''If we have two independent Poisson-distributed variables, their sum is Poisson distributed too.''\n\n4. The skewness is measured by 1/\u221a\u03bb\n\n5. Excess kurtosis is measured by 1/\u03bb. See the difference between excess kurtosis and kurtosis [https://www.investopedia.com/terms/e/excesskurtosis.asp here]. In a nutshell, excess kurtosis compares the kurtosis of the distribution with the kurtosis of a normal distribution and can therefore tell you if an (extreme) event is more or less likely in this case than if the distribution followed a normal distribution.\n\n6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> \u221e and \u03bb = np, rate parameter, \u03bb is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.\n\n7. A Poisson distribution with a high mean \u03bb > 20 can be approximated as a normal distribution. However, as normal distribution is a continuous probability distribution, a continuity correction is necessary. It would exceed the scope to discuss in detail here what this correction is. In short, it adds or substracts 0.5 to the value in question to increase the accuracy of the estimation when using a continuous probability approach for something that has discrete probabilities.\nFor example, a factory with 45 accidents per year follows a Poisson distribution. A normal approximation would suggest that the probability of more than 50 accidents can be computed as follows:\n\nMean = \u03bb = \u03bc =45\n\nStandard deviation = \u2202 = \u221a\u03bb = 6.71 P(X>50.5) -> after continuity correction =\n[[File:6 equation.PNG|center]]\nusing Z score table, see more [https://builtin.com/data-science/how-to-use-a-z-table here].\n\n==5. Poisson Distribution in Python==\nThe following example generates random data on the number of duststorms in the city of Multan. The lambda is set at 3.4 storms per year and the data is monitored for 10,000 years.\n\n<syntaxhighlight lang=\"Python\" line>\n#import libraries\n\nfrom scipy.stats import poisson ## to calculate the passion distribution\nimport numpy as np ## to prepare the data\nimport pandas as pd ## to prepare the data\nimport matplotlib.pyplot as plt ## to create plots\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\n#Random Example: Modeling the frequency of the number of duststorms in Multan, Pakistan \n\n#creating data for 10000 years using scipy.stat.poisson library\n#Rate lamda = 3.4 duststorm in Multan every year\n\nd_rvs = pd.Series(poisson.rvs(3.4, size=10000, random_state=2)) #random_state so we can reproduce it #turning into panda series\nd_rvs[:20] # first 20 entry, so the first 20 years, with the number of storms on the right\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nd_rvs.mean() # mean of 10000 values is 3.3879, approximately what we set as the average number of duststorm per year.\n</syntaxhighlight>\nOutcome: 3.3879\n\n<syntaxhighlight lang=\"Python\" line>\n#showing the frequency of years against the number of storms per year and sorting it by index. \ndata = d_rvs.value_counts().sort_index().to_dict() #storing in a dictionary\ndata\n## You can see that most years have 2-4 storms which is also represented in the calculated average and our lambda.\n</syntaxhighlight>\nOutcome: \n 0: 357,\n 1: 1122,\n 2: 1971,\n 3: 2144,\n 4: 1847,\n 5: 1253,\n 6: 731,\n 7: 368,\n 8: 126,\n 9: 54,\n 10: 24,\n 11: 2,\n 12: 1\n\n<syntaxhighlight lang=\"Python\" line>\nfig, ax = plt.subplots(figsize=(16, 6))\nax.bar(range(len(data)), list(data.values()), align='center')\nplt.xticks(range(len(data)), list(data.keys()))\nplt.show()\n</syntaxhighlight>\n[[File: plot 1.png|center|700px]]\n\nThe resulting pmf confirms that the closest integer value to lamba i.e., 3 has the most number of years out of 10,000 meaning most years will have 3 duststorms. You can also see that the data is slightly skewed to the right since there is a larger variance to the right than to the left. Looking at the distribution, it looks fairly normally distributed. However, the low lambda does not allow to use the Poisson distribution as an approximation for a normal distribution. Most probably, the large dataset allows us to see it as a normal distribution, since most distributions converge to a normal distribution with increasing sample size.\n==6. References==\n* Brownlee, Jason: \"A Gentle Introduction to Probability Distributions\", 14.11.2019. Retrieved from: https://machinelearningmastery.com/what-are-probability-distributions/#:~:text=A%20probability%20distribution%20is%20a,properties%20that%20can%20be%20measured, last checked: 21.05.2023\n* Koehrsen, Will: \" The Poisson Process and Poisson Distribution Explained (With Meteors!), 28.10.2022. Retrieved from: https://builtin.com/data-science/poisson-process, last checked: 21.05.2023\n* Papoulis, Athanasios; Pillai, S. Unnikrishna: \"Probability, Random Variables, and Stochastic Processes\" (4th ed.).\n* Raikov, Dmitry (1937): \"On the decomposition of Poisson laws\". Comptes Rendus de l'Acad\u00e9mie des Sciences de l'URSS. 14: 9\u201311.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "spv8v55ytw0294occ0cu2hca1eyy5d3"
                }
            },
            {
                "title": "Pomodoro",
                "ns": "0",
                "id": "287",
                "revision": {
                    "id": "3267",
                    "parentid": "2986",
                    "timestamp": "2020-11-04T10:28:09Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2423",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to ''25 minutes = 1 \"Pomodoro\"'').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15\u201330 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https://play.google.com/store/apps/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https://apps.apple.com/us/app/focus-keeper-time-management/id867374917 Focus Keeper]\n\n\n__NOTOC__\n----\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "42u0aqlp3chpdm9h7qarq0qgujzczp2"
                }
            },
            {
                "title": "Poster Design",
                "ns": "0",
                "id": "308",
                "revision": {
                    "id": "5464",
                    "parentid": "5252",
                    "timestamp": "2021-05-25T13:23:37Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6973",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n== Why & When ==\nPoster, usually a large sheet of paper with a designed combination of text, graphs, color, etc., is one of the cost-effective means used to convey information to a large audience. There are different types of poster for various settings, such as advertising, event, politics, for motivational and scientific uses and many more. You may encounter the need to create a poster for a presentation at University or at a Conference. Informative posters are oftentimes commercial ones of a product or services hung in front of stores, whereas formative ones are to raise awareness or speak for a good cause.\n\n== Goal(s) ==\n* Crisp-and-clear conveyance of desired information\n* Visually creative and appealing\n* Ability to stand out from the background and attract attention\n\n== Essential elements of a good poster ==\n===== '''1. Clear message & Call-to-Action''' =====\nIf you design a poster for an event, there are questions that should be clearly and concisely answered:\n\n* What is the event about?\n* Why should I attend this?\n* When is it taking place?\n* Where is the venue?\n* How to attend the event (tickets info, etc.) / How to keep up with the event (social media portals, etc.)\n\n===== '''2. Know your audience''' =====\nThe target audience should be at the center when designing a poster, as not only the content itself but other elements in the poster can be tailor to best reach its recipients. [https://venngage.com/blog/poster-design/#Consider-your-target-audience An example] compares two different posters both advertising for a fundraising event.\n\n[[File: PosterComparison.png|thumb|center|700px|'''Fundraising event posters'''. Source: [https://venngage.com/blog/poster-design/#Consider-your-target-audience Venngage]]]\n\n\nThe poster on the left for the Pride Color Run event looks vibrant, lively and is targeted to a younger demographic. The right poster is made for a charity dinner and has a more structured and professional layout, which normally attracts the older, established audience. Many differences in color scheme, typography, layout, styling and image use can be spotted between these two posters of the same cause, which emphasizes the importance of identifying who a poster should speak to and how to effectively draw their attention.\n\nAs for educational and scientific posters, keep in mind that the audience can be roughly divided into three categories: your field of specialization, the fields closely related to yours, and unrelated fields ([https://www.sciencedirect.com/science/article/abs/pii/0166223689900398 Woolsey, 1989]). It is best to adjust your content accordingly and be mindful when using jargons and other field-specific vocabularies, unless you are sure that everyone in the audience can understand them.\n\n===== '''3. Color has meanings''' =====\n* Each color creates a unique kind of energy that could reflect the poster\u2019s message, may it be bold, subtle, agony or ecstasy. Choosing the right color scheme will set a tone for your statement to shine, help drawing attention from the audience from afar, create a subliminal association between then and the topic even before they start reading the text.\n\n* The actual color shade is also very important. It may come to you as a surprise, but not only gray has (more than) fifty shade. The point is, it is important to take this into account when creating a color palette for your poster. Different shades combination will create different effects even though the rough colors stay them same. \n\n===== '''4. Typography \u2013 It\u2019s not just about the content''' =====\n* Typography is an [[Glossary|art]] itself. There is so much more you can convey with the use of fonts and arrangement of texts, sometimes incorporating illustrations and images. It is an integral part to the poster as a final product, and if done right, makes a unique lasting visual impact to the audience.\n\n===== '''5. Negative & White space \u2013 Things gotta breathe''' =====\n* A lot of times: less is more. With a poster you only have limited space, so deciding what\u2019s making it there and what not is crucial. Negative space, or white space, is the area of the layout that is purposefully left empty. It serves as a breathing room for all other elements on the poster. As mentioned at the beginning, your message has to stand out. But if everything on the poster is so crowded in technicolor, your message becomes a leaf in the forest. That\u2019s why it is important to selectively pick out some crucial elements and make them eye-catching, while creating a background with plenty of room for your message to pop.\n\n[[File:NegSpace.jpg|550px|center|thumb|Source: [https://www.behance.net/gallery/2175431/Sustainability-Poster-Series GO Sustainable (by Julian Nicole Salinas)]]]\n\n===== '''6. Contrast \u2013 Make it pop''' =====\n* High contrast between elements on a poster could help you catch the audience\u2019s eyes with just a glance. Experimental, bold use of colors and images are encouraged, especially when your poster is among a dozen other ones, each trying to scream for attention.\n\n===== '''7. Readable from a distance, but scalable''' =====\n* When designing a poster, you need to make sure that people are able to read the key information with a glimpse of an eye. The font, size and arrangement of text are therefore critical. The headline should be the main (and biggest) text element on the poster, and you can play around of different fonts or typography ideas to make it as attractive as possible. Then come the detailed information as discussed above. They don\u2019t need to be as dazzling, since the audience will come a bit closer to read them through if they decide that the poster is relevant. Still, the detailed information needs to be concisely and clearly displayed to make use of the short attention span an audience may have for your poster. \n* Oftentimes you will want to scale down and hand out your poster in the form of flyers. Thus, the size ratio of the headline and the detailed information should not be too drastic, as scaling down would make the details unreadable.\n\n== Links & Further reading ==\n* Have a look at the entry on [[Conceptual Figures]] - while these are not the same as Posters, there are certain tips that may also be of relevance for your poster.\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Ch\u00e2n L\u00ea."
                    },
                    "sha1": "ipn9iv068d44wkcyqvqje65uicn06cc"
                }
            },
            {
                "title": "Price Determinants of Airbnb Accommodations",
                "ns": "0",
                "id": "819",
                "revision": {
                    "id": "5979",
                    "parentid": "5827",
                    "timestamp": "2021-07-05T16:30:46Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Wordcloud */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "31006",
                        "#text": "'''In short:''' This entry represents an analysis of airbnb data, starting from data scraping and analysis, to training and creating a machine learning model for price prediction. In this entry you will learn how to prepare and explore data, perform data cleaning, build wordmaps, choropleth maps, [[Barplots,_Histograms_and_Boxplots|boxplots and scatterplots]], as well as extract features and train a machine learning model. \n</br>\n</br>\n'''Note:''' For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n__TOC__\n<br/>\n== Introduction ==\nKnowing the market value of an overnight stay in an Airbnb home is of high interest for both, renters (or hosts) and guests. As a guest one might be a bit overwhelmed going through all the offers Airbnb nowadays contains (approximately 51.000 for the city of New York as of November 2018). Knowing exactly what drives prices of Airbnb listings could make the whole search process more enjoyable. Setting the right price on the other hand is a crucial tasks for the hosts, as they want neither, their listing not being booked, nor to rent it out too cheap. \n\nTo tackle these problems, this report aims at identifying the determinants of prices for Airbnb listings. More precisely, the following questions will be answered throughout the analysis:\n\n# How do hosts promote their listing? How are expensive/cheap listings being promoted?\n# What areas are the most expensive ones? Does this correlate with the location score?\n# Are there any seasonal effects on the price?\n# What are the overall most important determinants for the price?\n\n== The data ==\n\n=== Data description ===\n\nAirbnb itself does only provide a very limited insight into their data, but a separate group named [http://insideairbnb.com/get-the-data.html Inside Airbnb] scrapes and compiles publicly available information about many cities' listings from the Airbnb website. For this report, data for the city of New York from the year 2018 was been analysed. The dataset can be accessed through [https://www.notion.so/Data-Airbnb-listings-and-calendar-2018-481e9fcdd55c4253adee68751f0268c3 this link]. Otherwise, more up-to-date datasets from the same website can be used for this analysis, making sure the features described and used for it appear in the tables.\n\nThe current dataset comprises of two tables:\n\n* <syntaxhighlight lang='R' inline>listings</syntaxhighlight>: This table contains the unique [http://insideairbnb.com/about.html listings] offered on Airbnb for New York at a particular time particular time. It has been scraped on the third and fourth of November, 2018. The data consist of 50968 entries (unique listings) and 96 features. The single identifier for a listing is the <syntaxhighlight lang='R' inline>id</syntaxhighlight>. Interesting features are, among others, the location (<syntaxhighlight lang='R' inline>zipcode</syntaxhighlight>) of the listing, it's overall score on reviews (<syntaxhighlight lang='R' inline>review_scores_rating</syntaxhighlight>) and the type of accommodation (<syntaxhighlight lang='R' inline>room_type</syntaxhighlight>). \n\nTo see how prices change over time and analyse whether there are seasonal effects on the price, the price of a listing on one particular day is not a sufficient information. Therefore also booking information has been added to the final dataset:\n\n* <syntaxhighlight lang='R' inline>calendar</syntaxhighlight>: This table contains booking information of a listing for the next year. So for a particular date, where this data has been scraped, the corresponding prices for a listing for the next 365 days are included in this table, together with the information whether the listing is available at a particular day. This data is available for the years 2017 and 2018, where each of these data has been scraped one year before the year they contain information about. The table also contains the <syntaxhighlight lang='R' inline>listing_id</syntaxhighlight> that is available in the <syntaxhighlight lang='R' inline>listings</syntaxhighlight> table and through which the tables can be joined together.\n\n=== Data preparation and exploration ===\nBefore working with the data let's make sure all necessary libraries are imported.\n<syntaxhighlight lang='R' line>\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(rpart)\nlibrary(cowplot)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(choroplethr)\nlibrary(choroplethrMaps)\nlibrary(choroplethrZip)\noptions(warn=-1) #sets the handling of warning messages. If 'warn' is negative all warnings are ignored.\n</syntaxhighlight>\n\nAt first, the two tables are loaded and processed individually. The data is in <syntaxhighlight lang='R' inline>.csv</syntaxhighlight>` format and hence can be loaded into R using the <syntaxhighlight lang='R' inline>read_csv()</syntaxhighlight> function.\n\n<syntaxhighlight lang='R' line>\nlistings = read.csv(\"listings.csv\") #or add your own pathway to the file\n</syntaxhighlight>\n\nThe price feature of both tables needs to be processed, as it contains commas as a thousands separator as well as a dollar sign at the beginning: \n\n<syntaxhighlight lang='R' line>\nlistings$price <- as.numeric(gsub(\",\", \"\", substring(listings$price, 2)))\n</syntaxhighlight>\n\nAs the table contains all types of information, for example the house rules or the name of the host, which probably do not contribute too much to the research questions, most of them will be dropped. Moreover, the table contains quite interesting information that could be useful but is hard to process as it is in free text format, like the property <syntaxhighlight lang='R' inline>description</syntaxhighlight> variable. The following variables will be considered during the further course of this report:\n\n\n* <syntaxhighlight lang='R' inline>id</syntaxhighlight>: The unique identifier of a listing\n* <syntaxhighlight lang='R' inline>neighbourhood_group_cleansed</syntaxhighlight>: The district of a listing reduced to five different factors\n* <syntaxhighlight lang='R' inline>bathrooms</syntaxhighlight>: Number of bathrooms in the Airbnb\n* <syntaxhighlight lang='R' inline>availability_90</syntaxhighlight>: Number of days the listing is available for the next 90 days\n* <syntaxhighlight lang='R' inline>bedrooms</syntaxhighlight>: Number of bedrooms \n* <syntaxhighlight lang='R' inline>beds</syntaxhighlight>: Number of beds \n* <syntaxhighlight lang='R' inline>review_scores_rating</syntaxhighlight>: The review score of a listing, scaled between 0 (poor) and 100 (excellent)\n* <syntaxhighlight lang='R' inline>number_of_reviews</syntaxhighlight>: Number of reviews\n* <syntaxhighlight lang='R' inline>host_total_listings_count</syntaxhighlight>: Total number of listings of the host of this particular listing\n* <syntaxhighlight lang='R' inline>property_type</syntaxhighlight>: Specifies the type of accommodation, e. g. loft or house\n* <syntaxhighlight lang='R' inline>room_type</syntaxhighlight>: Specifies, whether the accommodation is private or not\n* <syntaxhighlight lang='R' inline>accommodates</syntaxhighlight>: Number of possible guests for this listing\n* <syntaxhighlight lang='R' inline>host_since</syntaxhighlight>: Specifies the date the host joined Airbnb as a host\n* <syntaxhighlight lang='R' inline>maximum_nights</syntaxhighlight> / <syntaxhighlight lang='R' inline> minimum_nights</syntaxhighlight>: Maximum / minimum nights respectively that can be spent in this accommodation\n* <syntaxhighlight lang='R' inline>cancellation_policy</syntaxhighlight>: Specifies the degree of ease of cancelling a booking \n* <syntaxhighlight lang='R' inline>name</syntaxhighlight>: Is the title of the listing as shown on the Airbnb website. This is the only free text variable that will be used for analysis\n* <syntaxhighlight lang='R' inline>review_scores_location</syntaxhighlight>: The guests review score on location, scaled between 0 (bad location) and 10 (very good location)\n\n<syntaxhighlight lang='R' line>\npossible_features = c(\"id\", \"price\",\"neighbourhood_group_cleansed\", \"bathrooms\", \"availability_90\", \"review_scores_rating\", \"name\", \"number_of_reviews\", \"bedrooms\", \"beds\", \"host_is_superhost\", \"host_total_listings_count\", \"property_type\", \"room_type\", \"accommodates\", \"host_since\", \"maximum_nights\", \"minimum_nights\", \"cancellation_policy\", \"zipcode\", \"review_scores_location\")\n\nlistings = listings[, possible_features] #dropping the unnecessary variables\n</syntaxhighlight>\n\n====Wordcloud====\nThe <syntaxhighlight lang='R' inline>name</syntaxhighlight> variable will be used in order to analyse how hosts promote their listing, as it is one of the things potential guests see at first. More specifically, in the following it will be analysed, what words are used most frequently in the title and how these words differ between cheap and expensive listings. Therefore, the titles of the cheapest 15% and the most expensive 10% of the listings are used. The figure below shows the [[Wordcloud|wordcloud]] (containing the most frequently used words) for cheap and expensive listings. \n\n<syntaxhighlight lang='R' line>\n# remove unwanted characters from the titles\npattern = paste(\"/\", \"@\", \"/|\", \"+\", \"!\", \"w\", \"-\", \"1\", \"2\", \"3\", \"4\", \"a\", \"in\", \",\", sep=\" | \")\nlistings$name = gsub(pattern, \" \", listings$name)\n\n#creating a dataframe for the expensive listings (listings after the 90th quantile or the most expensive, 10% of the listings)\nquant_90 = as.numeric(quantile(listings$price, 0.9), na.rm = TRUE)\ntitle_exp = listings[listings$price > quant_90, \"name\"]\nword_counts_exp = data.frame(table(unlist(strsplit(tolower(title_exp), \" \"))))\n\n#creating a dataframe for the affordable listings (listings of the 15th quantile or the cheapest, 15% of the listings)\nquant_15 = as.numeric(quantile(listings$price, 0.15), na.rm = TRUE)\ntitle_cheap = listings[listings$price < quant_15, \"name\"]\nword_counts_cheap = data.frame(table(unlist(strsplit(tolower(title_cheap), \" \"))))\n\n# Fig. 1\n# creating two wordcloud visualisations\npar(mfrow=c(1,2))\n\nwordcloud(words = word_counts_exp$Var1, freq = word_counts_exp$Freq, min.freq = 5,\n          max.words = 82, random.order = F, colors = brewer.pal(8, \"Dark2\"))\n\nwordcloud(words = word_counts_cheap$Var1, freq = word_counts_cheap$Freq, min.freq = 5,\n          max.words = 60, random.order = F, colors = brewer.pal(8, \"Dark2\"))\n\n</syntaxhighlight>\n\n[[File:wordcloudsairbnbny.png|550px|thumb|center|Fig. 1: Wordcloud visualisations of the most frequently used words in titles of expensive (left) and the cheap (right) Airbnb listings.]]\n\nMost Airbnb hosts seem to promote the property type of their accommodation, like loft or apartment, as well as the location it is situated in, like Manhattan. \n\nMoreover, it stands out that the expensive listings contain words like luxury, beautiful and modern in their titles, while cheaper listings rather use euphemisms like cozy, spacious and comfy. \n\nBased on this knowledge, we can now track if a listing's title includes one of the most frequently used words for one or the other price category.\n\n<syntaxhighlight lang='R' line>\n#creating two lists with the most frequent words in the titles for expensive and cheap listings\nwords_cheap = c(\"spacious\", \"charming\", \"cozy\", \"columbia\", \"convenient\", \"affordable\")\n\nwords_exp = c(\"luxury\", \"amazing\", \"garden\", \"gorgeous\", \"best\", \n              \"stunning\", \"terrace\", \"luxurious\", \"times square\")\n\n#creating a function to count the number of words of interest (words_a) in the whole title (words_b)\ncount_word_matches = function(words_a, words_b){\n  if(sapply(strsplit(words_a, \" \"), length) == 0){\n    return(0)\n  }\n  else{\n    return(sum(sapply(strsplit(tolower(words_a), \" \"), function(x) x %in% words_b)))\n  }\n}\n\n#creating two variables with the function applied\nnum_words_cheap = sapply(listings$name, \n                         function(x) count_word_matches(as.character(x), words_cheap))\n\nnum_words_exp = sapply(listings$name, \n                       function(x) count_word_matches(as.character(x), words_exp))\n\n\n#adding the two new variables to the listings dataframe\nlistings = cbind(listings, num_words_cheap, num_words_exp)\n</syntaxhighlight>\n\n==== Choropleth map ====\n\nNext, the price differences between different areas of New York will be analysed. Therefore, the zipcodes will be used to group areas together and calculate their mean prices. In a second step, it will be analysed whether the price and the location rating score are correlated, as the latter is a continuous variable and could therefore be used easily as a predictor for the price, while the zipcode is a categorical variable, consisting of 206 levels, which would result in 206 (binary) dummy variables and make prediction difficult. \n\nThe code below calculates the average price per zipcode and plots the result on a map of New York. Analogous, the plot for the average location score per region is created.\n[[File:choroplethmap1.png|350px|thumb|right|Fig. 2: Visualisation of New York areas with a choropleth map, categorised by the average price]] \n<syntaxhighlight lang='R' line>\n\n# Fig. 2\nzipcode_prices <- listings %>% \n  group_by(zipcode = zipcode) %>% \n  summarise(avg_loc_review = mean(price, na.rm = TRUE))\n\ncolnames(zipcode_prices) <- c(\"region\",\"value\")\n\nzipcode_prices$region <- as.character(zipcode_prices$region)\n\nnyc_fips = c(36005, 36047, 36061, 36081, 36085)\n\ng_locations_price <- zip_choropleth(zipcode_prices, \n                                    county_zoom = nyc_fips, \n                                    title = \"Average Prices by Region\", \n                                    legend = \"Average Price\") + \n  ggtitle(\"Which area is the most expensive?\") +\n  theme(legend.text = element_text(size=7),\n        legend.key.size = unit(0.4, \"cm\"), \n        legend.title = element_text(size=8), \n        plot.title = element_text(size=10, hjust = 1.5))\n\n</syntaxhighlight>\n[[File:chloroplethmap2.png|350px|thumb|right|Fig. 2: Visualisation of New York areas with a choropleth map, categorised by the average rating score]] \n<syntaxhighlight lang='R' line>\n\n# Fig. 3\nzipcode_scores <- listings %>% \n  group_by(zipcode = zipcode) %>% \n  summarise(avg_loc_review = mean(review_scores_location, na.rm = TRUE))\n\ncolnames(zipcode_scores) <- c(\"region\",\"value\")\n\nzipcode_scores$region <- as.character(zipcode_scores$region)\n\nnyc_fips = c(36005, 36047, 36061, 36081, 36085)\n\ng_locations_score <- zip_choropleth(zipcode_scores,\n                              county_zoom = nyc_fips,\n                              title = \"Location Review Scores by Region\",\n                              legend = \"Average Score\") + \n  ggtitle(\"Which area is the best?\") + \n  theme(legend.text = element_text(size=7), \n        legend.title = element_text(size=8),\n        legend.key.size = unit(0.4, \"cm\"), \n        plot.title = element_text(size=10, hjust = 1.5))\n\n#grid function puts two plots in a row, if necessary\nplot_grid(g_locations_price, g_locations_score, nrow=1)\n\n</syntaxhighlight>\n\nFrom the plots, one can infer that a higher location score indicates a higher price. The correlation coefficient is calculated below:\n\n<syntaxhighlight lang='R' line>\nzipcode_scores_prices = zipcode_scores %>% \n  inner_join(zipcode_prices, by = \"region\")\n\nzipcode_scores_prices = zipcode_scores_prices[complete.cases(zipcode_scores_prices), ]\ncor(zipcode_scores_prices$value.x, log(zipcode_scores_prices$value.y))\n\n## Output:\n## [1] 0.3830552\n</syntaxhighlight>\n\nAnd with almost 0.4 one can conclude, that there is a positive correlation between the location score and the price of a listing. Note that the correlation coefficient is calculated based on the logarithm of the price, as the price is highly skewed towards very high prices.\n\n====Scatter plot and Boxplots====\nNext, the calendar data will be analysed. The focus here is wheather there are seasonal patterns in the price\nwhich should be included in the machine learning model for price prediction.\n\n<syntaxhighlight lang='R' line>\n\ncalendar2017 <- read.csv(\"calendar2017.csv\")\ncalendar2018 <- read.csv(\"calendar.csv\")\ncalendar <- rbind(calendar2017, calendar2018)\ncalendar = calendar[year(calendar$date) == 2018, ]\nrm(list = c(\"calendar2017\", \"calendar2018\"))\n\n</syntaxhighlight>\n\n<syntaxhighlight lang='R' line>\n\n### Fig. 4\n### Seasonal analysis #####\n\ncalendar$price <- as.numeric(gsub(\",\", \"\", substring(calendar$price, 2)))\n\nseasonal_df <- calendar %>% \n  group_by(date = date) %>% \n  summarise(avg_price = mean(price, na.rm = TRUE)) %>% \n  mutate(year = year(date))\n\nseasonal_df$date = as.Date(seasonal_df$date)\n\n# make a feature that indicates whether a day is a friday or a saturday\n# These are potentially the days where the prices for a night are the highest\n\nseasonal_df$dayofweek = wday(seasonal_df$date)\nis_weekend = seasonal_df$dayofweek %in% c(6, 7)\nseasonal_df$is_weekend = factor(is_weekend)\n\n# make a scatter plot of the daily average prices\ng_scatter = ggplot(seasonal_df, aes(date, avg_price)) + \n  geom_point(aes(color=is_weekend), alpha=0.5) +\n  ggtitle(\"Price Variations\") +\n  labs(x = \"Date\", y = \"Average price of Listings\") + \n  scale_x_date(date_labels = \"%m-%Y\")\n\n# make a boxplot of the price across the different months\n\ng_box = ggplot(seasonal_df, aes(as.factor(month(date)), avg_price)) + \n  geom_boxplot() + \n  stat_boxplot(geom = \"errorbar\", width = 0.5) + \n  ggtitle(\"Boxplot of price per month\") + \n  labs(x = \"Month\", y = \"Price\")\n\n# make a grid of both plots being in a row\n\nprow = plot_grid(g_scatter + theme(legend.position = \"none\"),  \n                 g_box + theme(legend.position = \"none\"),\n                 nrow=1)\n\n# create a legend for the scatter plot and put it in a new row of the grid\n\nlegend = get_legend(g_scatter + theme(legend.position = \"bottom\"))\nplot_grid(prow, legend, ncol=1, rel_heights = c(1,.2))\n\n</syntaxhighlight>\n[[File:DateMonthPriceVariation.png|550px|thumb|center|Fig. 4: Correlation between the price and the day of the week (left, scatter plot) as well as the month (right, boxplots)]]\n\nWhat immediately stands out is the affect of weekends on the price. Renting an appartment on Airbnb on a Friday or Saturday seems to be more expensive compared to other weekdays. However, this effect is a little bit exaggerated in this plot, as it is not scaled between zero and the upper bound. In fact, the relative difference in price between weekdays and non weekdays, is only about 3%.\n\n<syntaxhighlight lang='R' line>\na = mean(seasonal_df[(seasonal_df$is_weekend == T), ]$avg_price)\nb = mean(seasonal_df[(seasonal_df$is_weekend == F), ]$avg_price)\nabs(a-b)/b\n\n## Output:\n## [1] 0.03695569\n</syntaxhighlight>\n\nAnother thing that strikes attention is the monthly pattern in the price. Especially the difference between late December and the first months of a year seems to be big. Looking at the boxplot of the price per month there is a clear monthly seasonality visible.\n\n\n== Feature Engineering ==\n\nIn order to train a model that makes predictions for the price of a specific Airbnb accommodation, meaningful features have to be created which the supervised learning model can take as input to derieve patterns in the data that determine the prices of the very many accommodations in the data set.\n\nThe exploratory data analysis was very helpful for this, as it has revealed many determinants for the price already. For example, seasonality plays an important role for the price, hence need to be included in the model. \n\nFirst of all, the two tables need to be merged into one data frme, which contains all the information that is needed. The `calendar` data frame contains over 16mio. observations, hence is computationally very heavy. Therefore, the table will be reduced by sampling randomly `r sample_length` observations from it (without replacement). \n[[File:listings_dataframe.png|350px|thumb|right|Fig.5]]\n<syntaxhighlight lang = \"R\" line>\nlistings = listings[, -which(colnames(listings) %in% c(\"price\", \"zipcode\"))]\nlistings = listings[complete.cases(listings), ]\ncolnames(listings)[which(colnames(listings) == \"neighbourhood_group_cleansed\")] = \"neighbourhood\"\ncalendar = calendar[, -which(colnames(calendar) %in% c(\"available\"))]\ncalendar = calendar[complete.cases(calendar), ]\nsample_length = 100000\n\ndf_panel = calendar %>% inner_join(listings, by = c(\"listing_id\" = \"id\"))\ndf_panel = df_panel[complete.cases(df_panel), ]\nsamples = sample(c(1:dim(df_panel)[1]), sample_length)\ndf_panel = df_panel[samples, ]\n\n#visualising the dataframe\ntext_tbl <- data.frame(\n  c1 = colnames(df_panel)[1:11], \n  c2 = c(colnames(df_panel)[c(12:21)], \"\")\n)\n\nkable(text_tbl, col.names = NULL) %>%\n  kable_styling() %>%\n  column_spec(1) %>%\n  column_spec(2) \n</syntaxhighlight>\n\nThe resulting data frame can be seen on Figure 5. A lot of the variables in the data are categorical (nominal). Many machine learning algorithms cannot operate on nominal data directly. They require all input variables and output variables to be numeric. \n\nTo achieve that, the nominal variables have to be one-hot encoded. This means that each distinct value of the variable is transformed into a new binary variable ([https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ source]).\n\nFirst, all categorical variables will be determined.\n\n[[File:catvariables.png|500px|thumb|right|Fig. 6]]\n<syntaxhighlight lang = \"R\" line>\n#Fig. 6\ncat_vars = names(which(sapply(df_panel, class) == \"factor\"))\ndf_cat = df_panel[, which(colnames(df_panel) %in% cat_vars)]\nhead(df_cat[, c(1:4)]) %>% \n  kable() %>% \n  kable_styling()\n\ncat_vars = names(which(sapply(df_panel, class) == \"factor\"))\ndf_cat = df_panel[, which(colnames(df_panel) %in% cat_vars)]\nhead(df_cat[, c(4:dim(df_cat)[2])]) %>% \n  kable() %>% \n  kable_styling()\n</syntaxhighlight>\n\nNot for all these variables it makes sense to create a new binary variable for each of the distinct values, as they just contain to many different values or even contain only values that are unique, like the <syntaxhighlight lang = \"R\" inline>names</syntaxhighlight> variable. These variables need to be transformed in a more logical way. For example the <syntaxhighlight lang = \"R\" inline>host_since</syntaxhighlight> variable. This can be transformed into an integer variable by calculating the difference in days between the date the host joined Airbnb and the date of the listing being offered.\n\n<syntaxhighlight lang = \"R\" line>\ndf_panel$host_since = abs(as.integer(difftime(df_panel$date, df_panel$host_since)))\n\ncat_vars = cat_vars[cat_vars %in% \"host_since\" == FALSE] \n</syntaxhighlight>\n\nMoreover, the <syntaxhighlight lang = \"R\" inline>date</syntaxhighlight> variable would not provide a lot of information if each date would be transformed into a new binary variable. However, as has been shown in the exploratory data analysis, there are a lot of seasonal influences on the price that can be captured by the <syntaxhighlight lang = \"R\" inline>date</syntaxhighlight>. Therefore, the date variable will be transformed into two new columns. One column that specifies the month an accommodation is being offered and another column specifing the day of the week in which the accommodation can be rented. Monthly and daily seasonalities (especially weekends) seemed to be the most influencial. \n\n<syntaxhighlight lang = \"R\" line>\ndf_panel = df_panel %>% \n  mutate(\"month_of_year\" = month(date)) %>% \n  mutate(\"dayofweek\" = wday(date))\n\ncat_vars = cat_vars[cat_vars %in% \"date\" == FALSE] \ncat_vars = c(cat_vars, \"month_of_year\", \"dayofweek\")\n\n</syntaxhighlight>\n\nLastly, the <syntaxhighlight lang = \"R\" inline>name</syntaxhighlight> variable does not contain any information as well without further processing. As has been shown earlier, there are certain words, which are used to promote expensive as well as rather cheap accommodations. The words that have been identified there will be used now. For each title of an Airbnb listing, the number of words that are used from the set of words used to promote expensive flats is determined (as well as the number of words used to promote cheaper accommodations). These features will be saved as <syntaxhighlight lang = \"R\" inline>num_words_exp</syntaxhighlight> and <syntaxhighlight lang = \"R\" inline>num_words_cheap</syntaxhighlight> respectively.\n\n<syntaxhighlight lang = \"R\" line>\nwords_cheap = c(\"spacious\", \"charming\", \"cozy\", \"columbia\", \"convenient\", \"affordable\")\nwords_exp = c(\"luxury\", \"amazing\", \"garden\", \"gorgeous\", \"best\", \n              \"stunning\", \"terrace\", \"luxurious\", \"times square\")\n# function that returns the number of words_b that appear in words_a\ncount_word_matches = function(words_a, words_b){\n  if(sapply(strsplit(words_a, \" \"), length) == 0){\n    return(0)\n  }\n  else{\n    return(sum(sapply(strsplit(tolower(words_a), \" \"), function(x) x %in% words_b)))\n  }\n}\nnum_words_cheap = sapply(df_panel$name, \n                         function(x) count_word_matches(as.character(x), words_cheap))\nnum_words_exp = sapply(df_panel$name, \n                         function(x) count_word_matches(as.character(x), words_exp))\ndf_panel = cbind(df_panel, num_words_cheap, num_words_exp)\n\ncat_vars = cat_vars[cat_vars %in% \"name\" == FALSE] \n</syntaxhighlight> \n\nThe other categorical variables can now be transformed using one-hot encoding. After that, all categorical variables will be removed from the data frame.\n\n<syntaxhighlight lang = \"R\" line>\ndmy = dummyVars(\" ~ .\", data = df_panel[, cat_vars])\ndmy = predict(dmy, newdata = df_panel[,cat_vars])\ndf_panel = cbind(df_panel, dmy)\n\ndf_panel = df_panel[, -which(colnames(df_panel) %in% c(cat_vars, \"name\", \"date\", \"host_since\"))]\n\n# delete those entries where the price is zero\ndf_panel = df_panel[df_panel$price != 0, ]\n</syntaxhighlight> \n\n== Model training ==\nTo predict the price (a numeric variable) a regression model has to be used. There are, after all the feature engineering and one-hot encoding, 66 explanatory variables in the data set and probably a lot of interactions between these variables. For example, a big apartment in a suburb of New York, far away from the city center might be nothing special and therefore not extremely expensive. If the same sized flat, however, is located in the middle of Manhatten, the price would be huge. \n\nTherefore, a [https://en.wikipedia.org/wiki/Random_forest Random Forest] will be used in order to determine the price of an accommodation. A Random Forest is an ensemble learning model. It consists of many weak regression models, namely decision trees which are allowed only to make a small number of splits. In a decision tree, we can split branches by one variable first then split again by another variable. This is essentially creating interactions and thus is a good model class for the problem at hand. \n\nMoreover, having so many variables can cause a single model to overfit, hence leading to bad prediction results on observations that have not been used for training the model. A Random Forest however uses a lot of small decision trees, where each of them is not able to overfit as it cannot grow large enough to capture all the patterns of the target and additionally uses only a small (random) subset of the whole feature space. The plot on Figure 7 shows an example of a small decision tree for price prediction, where the large number in each of the nodes specifies the predicted price.\n[[File:airbnbdecisiontree.png|350px|thumb|right|Fig. 7]]\n<syntaxhighlight lang = \"R\" line>\n#Fig. 7\ntree = rpart(price~beds, data = df_panel)\nrpart.plot(tree, box.palette = \"RdBu\", shadow.col = \"gray\", nn=T)\n</syntaxhighlight> \n\nThe following code snipped first generates a matrix ''X'' that contains all explanatory variables and second a vector _y_ containing all the prices for the respective listings. After that, the matrix _X_ and the vector ''y'' are splitted into a train and a test set, where the train set is used to build the Random Forest regression model and the test set to validate its predictive power. \n\n<syntaxhighlight lang = \"R\" line>\nX = df_panel[, -which(colnames(df_panel) %in% c(\"price\", \"listing_id\"))]\ny = log(df_panel$price)\ntrain = sample(1:dim(X)[1], 0.7*dim(X)[1], replace = F)\nX_train = X[train, ]\ny_train = y[train]\nrfr = randomForest(X_train, y_train, maxnodes=15, ntree=1000, importance=TRUE)\ny_pred = exp(predict(rfr, X[-train,]))\ny_test = exp(y[-train])\nmape = mean(abs(y_test - y_pred) / y_test) #calculating the mean absolute percentage error\nprint(paste0(\"The MAPE of the Model is \", round(mape*100,2), \" percent\"))\n\n## Output:\n## [1] \"The MAPE of the Model is 31.72 percent\"\n</syntaxhighlight>  \n\n== Result and Conclusion ==\nThis short report attemted to predict the price of an overnight stay in an Airbnb accommodation based on various features. These features have to a large extend been generated based on insights gained through exploratory data analysis. The overarching aim was here to explain determinants of the price of an Airbnb accommodation. The prediction is based on a Random Forest, which results in a mean absolute percentage error of around 30% on the test set. The interpretation for this measure is, that on average the predicted price deviates by 30% from the actual one, which is quite high. Therefore, important influences on the variablity of the price have not been captured, i.e. '''important features have not been identified throughout this report'''. One of these neglected features could be for example information about holidays. For instance, the average price for an accomodation rose steadily a few days before Christmas and New Year, something that should be captured by the model.\n\nMoreover, due to computational limitations, the hyperparameters of the Random Forest have not been tuned and not all observations could be used to train and validate the model, leading to results worse than what could have actually been achieved.\n\nNonetheless, to a large extend the model developed during this report already captures the patterns of the variation in price across different listings on different days quite well. The following list contains the 15 most important variables to explain the price:\n[[File:featureimportance.png|500px|thumb|right|Fig. 8]]\n<syntaxhighlight lang = \"R\" line>\n#Fig. 8\n# make dataframe from importance() output\nfeat_imp_df <- importance(rfr) %>% \n  data.frame() %>% \n  mutate(feature = row.names(.))\nfeat_imp_df = feat_imp_df[c(1:20), ]\n# plot dataframe\nggplot(feat_imp_df, aes(x = reorder(feature, X.IncMSE),\n                        y = X.IncMSE, width=.5)) +\n  geom_bar(stat='identity') +\n  coord_flip() +\n  theme_classic() +\n  labs(\n    x     = \"Feature\",\n    y     = \"Importance\",\n    title = \"Feature Importance: Random Forest\"\n    )\n</syntaxhighlight>\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Laurin Luttmann."
                    },
                    "sha1": "ftbhsy9mxvaxw58e83e8dcyhrbyv3q6"
                }
            },
            {
                "title": "Principal Component Analysis",
                "ns": "0",
                "id": "543",
                "revision": {
                    "id": "5770",
                    "parentid": "5478",
                    "timestamp": "2021-06-13T14:11:50Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Motivation */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16415",
                        "#text": "[[File:ConceptOrdination.png|450px|frameless|left|[[Sustainability Methods:About|Method Categorization]] for [[Cohort Study]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method Categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n__NOTOC__\n<br/>\n<br/>\n<br/>\n'''In short:''' Principle Component Analysis is an unsupervised learning algorithm whose goals is to reduce the dimensionality of quantitative data down to a low number of dimensions.\n\n== Background ==\n[[File:PCA.png|400px|thumb|right|'''SCOPUS hits per year for Principle Component Analysis until 2020.''' Search terms: 'PCA', 'Principle Component Analysis' in Title, Abstract, Keywords. Source: own.]]\n=== Motivation ===\n[[File: PCAPizza.jpg|right|300px]]\n\nPrincipal Component Analyses are helpful when you have a lot of different [[Glossary|data]] samples with a variety of variables. For example, the following dataset which contains different nutrient measurements in various pizzas from different pizza brands.\n\n* brand -- Pizza brand (class label)\n* id -- Sample analysed\n* mois -- Amount of water per 100 grams in the sample\n* prot -- Amount of protein per 100 grams in the sample\n* fat -- Amount of fat per 100 grams in the sample\n* ash -- Amount of ash per 100 grams in the sample\n* sodium -- Amount of sodium per 100 grams in the sample\n* carb -- Amount of carbohydrates per 100 grams in the sample\n* cal -- Amount of calories per 100 grams in the sample\n\nHow can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.\n\n=== Curse of dimensionality ===\nThis term was coined by Richard R. Bellman, an American applied mathematician. As the number of features / dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. '''Principle Component Analysis helps to alleviate this problem'''.\n[[File: PCA_BiPlot.png|center|500px]]\n\n== What the method does ==\nPrinciple Component Analysis is one of the foundational methods to combat the curse of dimensionality. It is an unsupervised learning algorithm whose goals is to reduce the dimensionality of the data, condensing its entirety down to a low number of dimensions (also called principle components, usually two or three). \n\nAlthough it comes with a cost of losing some information, it makes data visualization much easier, improves the space and time complexity required for machine learning algorithms tremendously, and allows for more intuitive intepretation of these models. PCA can also be categorized a feature extraction techniques, since it creates these principle components - new and more relevant features - from the original ones.\n\nThe essence of PCA lies in finding all directions in which the data \"spreads\", determining the extent in which the data spreads in those directions, keeping only few direction in which the data spreads the most. And voila, these are your new dimensions / features of the data.\n\n=== Road to PCA ===\n==== Standardization ====\nOftentimes the features in the data are measured on different scales. This step makes sure that all features contribute equally to the analysis. Otherwise, variables with large range will trump thoses with smaller range (for example: a time variable that ranges between 0ms and 1000ms with dominate over a distance variable that ranges between 0m and 10m). Each variable can be scaled by subtracting its mean and dividing by the standard deviation (this is the same as calculating the z-score, and in the end, all variables with have the same mean 0 and standard deviation of 1).\n\n==== Covariance matrix ====\n\nThe covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties:\n* The size of the matrix is equal to the number of features in the data\n* The main diagonal on the matrix contains the variances of each initial variables.\n* The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2)\n\nThe covariance matrix gives you a summary of the relationship among the initial variables.\n* A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa)\n* A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa)\n\n==== Eigenvectors / Principle Components & Eigenvalues ====\nNow we have the covariance matrix. This matrix can be used to transform one vector into another. Normally when this transformation happens, two things happen: the original is  rotated and get streched/squished to form a new vector. When an abitrary vector is multipled by the covariance matrix, the result will be a new vector whose direction is nudged/rotated towards the greatest spread in the data. In the figure below, we start with the arbitrary vector (-1, 0.5) in red. Multiplying the red vector with covariance matrix gives us the blue vector, and repeating this gives us the black vector. As you can see, the result rotation tends to converge towards the widest spread direction of the data.\n\n[[File: PCAEigenvector01.png|center|500px]]\n\nThis prompts the questions: Can we find directly find the vector which already lies on this \"widest spread direction\". The answer is yes, with the help of eigenvectors. Simply put, an eigenvectors of a certain matrix is a vector that, when transformed by the matrix, does not rotate. It remains on its own span, and the only thing that changes is its magnitude. This (constant) change ratio in magnitude corresponding to each eigenvector is called eigenvalue. It indicates how much of the data variability can be explained by its eigenvector.\n\nFor this toy dataset, since there are two dimensions, we get (at most) two egenvectors and two corresponding eigenvalues. Even if we only plot the eigenvectors scaled by their eigenvalues, we will basically have a summary data (and its spreading). At this point, the eigenpairs are be viewed as the principle components of the data.\n\n[[File: PCAEigenvector02.png|center|500px]]\n\n==== Ranking the principle components ====\nAs you may have noticed, the eigenvectors are perpendicular to each other. This is no coincidence. You can think of it this way: because we want to maximize the variance explained by each of the principle components, these components need to be independent from one another, therefore their orthogonality.  Thus, to define a set of principle components, you find the direction which can explain the variability in the data the most: that is your first principle component (the eigenvector with the highest eigenvalue). The second principle compent will be percepdicular to the first, and explain most of what is left of the variability. This continues until the d-th principle component is found.\n\nBy doing so, you are also sorting the \"importance\" of the principle components in terms of the information amount it contains what is used to explain the data. To be clear, the sum of all eigenvalues is the total variability in the data. From here, you can choose to discard any PCs whose percentage of explained variances are low. In many cases, if around 80% of the variance can be explained by the first k PCs, we can discard the other (d - k) PCs. Of course, this is only one of the heuristics method to determine k.  You can also use thr elbow method (the scree plot) like in k-means.\n\n==== Summary ====\n* PCA is a feature extraction technique widely used to reduce dimensionality of datasets.\n* PCA works by calculating the eigenvectors and the corresponding eigenvalues of the initial variables in the data. These are the principle components. Number of PCs = number of eigenvectors = number of features.\n* The PCs are ranked by the eigenvalues, and iteratively show the directions in which the data spreads the most (after accounting for the previous PCs).\n* We can choose to keep a few of the first PCs that cummulatively explains the data well enough, and these are the new reduced dimension of the data.\n* Standardization is a crucial step in data pre-processing to ensure the validity of the PCA results.\n\n\n=== R Example ===\nGoing back to the example in the introduction, the dataset can be found here: https://www.kaggle.com/sdhilip/nutrient-analysis-of-pizzas\n\n<syntaxhighlight lang=\"R\" line>\n# Loading library\nlibrary(tidyverse) # For pre-processing data\nlibrary(factoextra) # For visualization\ntheme_set(theme_bw()) # Set theme for plots\n\n# Load data\ndata <- read_csv(\"Pizza.csv\")\nhead(data)\n</syntaxhighlight>\n\nAs shown here, there are seven measurements of nutrients for each pizza. Our goal is to reduce these seven dimensions of information down to only two, so that we can present the main patterns in our data on a flat piece of paper.\n\nTo conduct Principle Component Analysis in R, we use <code>prcomp</code>. From the original data, we only select the seven nutrient measurements as input for the function. In this function, we set <code>scale = TRUE</code> to perform scaling and centering (so that all variables will have a mean of 0 and standard deviation of 1). The result object is saved in <code>data.pca</code>\n\n<syntaxhighlight lang=\"R\" line>\ndata.pca <- prcomp(data[, mois:cal]), scale = TRUE)\n</syntaxhighlight>\n\nNow we can visualize and inspect the result of the analysis. We start off by looking at the contribution of each created principle component (PC) to the overall variance in the dataset. For this, we create a spree plot:\n\n<syntaxhighlight lang=\"R\" line>\nfviz_eig(data.pca)\n</syntaxhighlight>\n\n[[File: PCA_ScreePlot.png|center|500px]]\n\nLike I mentioned before, the number of PC created is equal to the number of input variables (in this cases, seven). Looking at the plot, the first two PCs combined can explain more than 90% of the dataset, an amazing number. This means this 7-dimensional dataset can be presented on a 2-dimensional space, and we still only lose less than 10% of the information. In other words, the first two PCs are the most important, and we can discard the other ones.\n\nNext, we look at the contribution of the original variables to the building of the two PCs, respectively.\n\n<syntaxhighlight lang=\"R\" line>\nfviz_contrib(data.pca, choice = \"var\", axes = 1)\nfviz_contrib(data.pca, choice = \"var\", axes = 2)\n</syntaxhighlight>\n\n[[File: PCA_ContribPlot.png|center|500px]]\n\nThe red, dashed line refers to the case where all of the variable contribute equally. In the left plot, ash, fat, sodium and carbohydrates contribute substantially to the forming of the first PC. On the other hand, moisture and calories influence the second PC heavily, as seen in the right plot.\n\nFinally, we look at the biplot of the analysis. This reveals the underlying patterns of the dataset:\n\n<syntaxhighlight lang=\"R\" line>\nfviz_pca_biplot(data.pca, label = \"var\", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlot.png|center|500px]]\n\nThe two axes of this plot are the newly created PCs. There are two main part of information presented on the plot:\n\n* The arrows show how the original variables correlate with the two PCs, and in turn, with each others. For example, from the way the moisture arrow presents itself we can infer a strong negative correlation of the variable and the second PC. Fat and sodium have a very strong positive correlation, and the more carbohydrates a pizza contains, the less protein it has. Adding argument <code>label = \"var\"</code> in the function allows for the variable names to be printed.\n* The points show how each individual pizza is plotted on this new coordinate system. Here, we go a step further and grouping those pizza under different brands (a categorical variable) using the arguement <code>habillage = data$brand</code>. By doing this, we unearth additional information about those brands. For example:\n** Brand A typically produce pizzas with a high level of fat and sodium.\n** Pizzas from brand B, C, and D are rich in proteins and ash, as opposed to pizza from brand E, F, G, H which are high in carbohydrates.\n** If you favorite pizza brand F goes out of business (for whatever reason), a pizza from brand E, G or H would be a good substitute in terms of nutritional value.\n\n=== R Example: Is standardization that important? ===\nTo answer this question, let us try an alternative scenario, where we conduct PCA without centering and scaling the variables.\n\n<syntaxhighlight lang=\"R\" line>\ndata.pca <- data %>% select(mois:cal) %>% prcomp(scale = FALSE)\nfviz_pca_biplot(data.pca, label = \"var\", habillage = data$brand, axes = c(1,2), addEllipse = TRUE)\n</syntaxhighlight>\n\n[[File: PCA_BiPlotNoScale.png|center|500px]]\n\n[[File: PCA_ContribPlotNoScale.png|center|500px]]\n\nSuddenly all that matters are only the carbohydrate, moisture and fat level of the pizzas. Why is that the case?\n\nBy plotting the distribution of the original data using boxplots (on the left), we can see that the value range of data for the variables are vastly different. For example, the variable carbohydrates has much higher mean and variance compared to calories. By nature, PCA tries to form PCs where there is a widest spread in the data, so it will always prefer those variables with high \"absolute\" variance. It's like comparing 1000 milliseconds and 5 kilometers and putting more weight on the 1000 milliseconds because 1000 is bigger than 5.\n\n[[File: PCA_BoxPlot.png|center|700px]]\n\nThis is why standardization, or sometimes called feature scaling (scale data to mean 0 and standard deviation 1) is a crucial pre-processing step in many data analysis procedures and machine learning algorithms, including PCA. This allows the analysis to pay attention to all features equally, so that no variable dominates the others (equal importance).\n== Strengths & Weaknesses ==\n'''Strengths'''\n* Reduce complexity of data\n* Allows for concise visualization of main patterns in data\n* Remove correlated features\n* Enhance performance of algorithms\n* Reduce overfitting\n'''Weaknesses'''\n* Principle components are created based on linear assumptions\n* The created principle components are hard to interpret\n* Information loss through reduction of dimensionality (oftentimes acceptable)\n== Key Publications ==\n* Wold, S., Esbensen, K., & Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3), 37-52.\n\n* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150202.\n== See Also ==\n\n* [https://www.youtube.com/watch?v=FgakZw6K1QQ&t A simple visual explanation of PCA] from StatQuest with Josh Starmer\n\n* [https://www.youtube.com/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&index=4 An in-depth walkthrough of PCA] and its mathematical root with Victor Lavrenko\n----\n\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| author]] of this entry is Ch\u00e2n L\u00ea."
                    },
                    "sha1": "7lwvbhrx170d5ow0xi865bxv6lki4ih"
                }
            },
            {
                "title": "Q-Methodology",
                "ns": "0",
                "id": "974",
                "revision": {
                    "id": "6871",
                    "parentid": "6741",
                    "timestamp": "2023-01-14T13:39:30Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19406",
                        "#text": "[[File:Q-Methodology.png|450px|frameless|left|**[[Sustainability Methods:About|Method categorization]] for [[Q-Methodology]]**]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Qualititative|Qualititative]]''' || colspan=\"2\" | [[:Category:Quantative|Quantative]]\n|-\n| [[:Category:Deductive|Deductive]] || colspan=\"2\"| '''[[:Category:Inductive|Inductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\n\n'''In short:''' The Q methodology is a qualitative, inductive and exploratory method to identify groups of people concerning their preferences or dissimilarities about specific information.\n\n\n== Background ==\n[[File:Q-Methodology scopus plot.png|400px|thumb|right|'''SCOPUS hits per year for Q-Methodology until 2022'''. Search term: \"q method\" OR \"q methodology\" in Title, Abstract, Keywords. Source: own.]]\nThe Q methodology was developed in 1935 at the University of London by William Stephenson in an attempt to challenge the logic of \u201ctesting\u201d that was and is predominant in psychology. However, in Stephenson`s opinion, psychology had not yet reached a \u201csophisticated theoretical status\u201d (such as e.g. physics had) which would have enabled a deductive mode of theory-testing. Therefore, he proclaimed a more inductive approach to derive new discoveries before moving on to testing theories (Watts and Stenner 2005). Therefore, he developed the Q methodology as a qualitative and inductive method to identify groups of people who share the same points of view on a certain topic (Stephenson 1935; Watts & Stenner 2005).\nIn the beginning, Q methodology was only used in psychology, but recently it is used in various research communities, e.g. nutrition research (see Dennis & Goldberg 1996), biodiversity management (see Hamadou et al. 2016) as well as research on the perceptions of ecosystem services (see Baral et al. 2017; Milcu et al. 2014).\n\n== What the method does ==\nThe Q methodology is applied to identify different perspectives on a topic that occur in a population. This is done by identifying groups of people who sort a pool of items in a comparable way (Watts and Stenner 2005). However, this method does not allow generalised statements about the whole population the participants are from. Instead, Q methodology only allows us to identify the main perspectives or perceptions that can be found in this population. \nThe process of the Q methodology follows the standard process of research. First, the data is gathered by letting participants sort a number of items called \u201cQ items\u201d (e.g. cards with statements or pictures) according to their preference from high to low. Second, data is analysed by correlating the resulting configurations, called \u201cQ sorts\u201d, with each other and performing a factor analysis. Third, the results of the factor analysis are interpreted. \nIn detail, the procedure works as follows:\n\n'''1. Generation of the Q set'''\n\nThe Q set is compiled of the Q items that the participants have to sort. The Q items can be statements that the participants agree or disagree with to a certain extent or pictures that they like more or less. The Q items can be everything that can be brought into an order of preference. In early studies, Q methodology was even conducted with bottled fragrances (Stephenson 1936). \nThe Q set is generated based on the research question. This means that the research question needs to be clear before generating the Q set. The research question should be straightforward and easily understandable for the participants. The Q set is supposed to enable the participants to answer the research question. Therefore, the Q items need to be as heterogeneous as possible to cover a broadly representative range of perspectives. An ideal Q set comprises between 40 and 80 items (Stainton Rogers 1995). The Q items need to be selected very carefully to find items that do not overlap, but at the same time no perspective should be missing. Therefore, the selection process takes the most time and effort of all the steps of the Q methodology. It is suggested to first create an overly large set of Q items and then reduce this number by doing pilot tests. \n\n'''2. Choosing the participants''' \n\nAs a rule of thumb, t 40 to 60 participants  in a Q methodological study are proposed to be an appropriate sample size (Stainton Rogers 1995). However, Q studies can also be carried out with fewer participants. Being a qualitative method, the number of participants is less important than the constitution of the participant group. To get a wide range of perspectives, it might make sense to strategically sample participants from different backgrounds. For instance, it makes sense to include experts who might have a pivotal opinion on a topic. The optimal relation between Q items and participants is described as 2:1 for classic Q studies (Watts & Stenner 2012, cited after Burkhardt 2017).\n\n'''3. Preparing the distribution matrix'''\n\nIn classic Q studies, the distribution matrix follows a normal distribution from -6 to +6. In this case, Q items that are ranked lowest by participants are placed at -6 whereas items that are highly preferred by participants are placed at +6. Items that are ranked at 0 would be the ones that the participants feel indifferent towards. Just as in a normal distribution, the extreme values -6 and +6 can be assigned the lowest number of items and the middle value 0 can be assigned  the most (Figure 1).\n\n\n[[File:Quasi-normal distribution.png|450px|frameless|center|]]\n\n\n\n'''Figure 1'''. Example of a Fixed Quasi-Normal Distribution for 60 Q items according to Stenner & Watts (2005). Q items that the participant agrees most with or values the most are placed at +6 and the ones that the participant disagrees with most or values the least are placed at -6. The items that are out in the middle around 0 are the ones that the participant feels indifferent about. Important to note is that only the ranking on the x axis matters. The order in which the items are placed on the y axis is of no importance.\n\n\n\nApart from this \u201cfixed\u201d or \u201cforced\u201d distribution also completely \u201cfree\u201d distributions are possible. In this case, the participants can assign any number of items to any of the ranking positions. Statistical comparisons have shown that the kind of distribution does not make a noticeable contribution to the results. However, the forced distribution is less work for the scientist and is more convenient for the participants (Watts & Stenner 2005).\n\n\n'''4. Conducting the sorting'''\n\nUsually, the sorting is conducted by only one participant at a time under the supervision of the researcher. In the beginning, the researcher is giving the instructions on how to sort the Q items according to the participant\u2019s preferences. It is important to state that there is no wrong or right kind of ranking, but the ranking is just based on personal preference. Since the time people have for sorting influences the outcome, the researcher should decide in advance if the participants should sort spontaneously or if they should have more time to think about the ranking. To influence the outcome as little as possible or at least to minimize the variance, the researcher should not give any input after explaining the way the ranking should be done, except for when urgent questions of understanding arise regarding the Q items. In the end, the Q sort is documented, e.g., by taking a photograph.\n\n'''5. Post sort interview (optional, but very helpful to understand the sorting)'''\n\nTo get a better interpretation of the Q sorts, it makes sense to do a post sort interview. The post sort interview can be used to ask the interviewee to explain their choices, especially regarding the top and bottom items. Moreover, basic demographic information should be collected from the interviewee such as age, occupation and personal history (Milcu et al. 2014). For later analysis, it is helpful to record the interview with the permission of the interviewee.\n\n'''6. Data analysis'''\n\nIf the Q sorting was done by manually sorting the items, the Q sorts of each participant have to be manually typed into an analysis program. For analysis, the freely available text-based statistics software R is proposed. The program R and the package \u201cqmethod\u201d (Zabala et al. 2021) implement diverse approaches of the method and can both be used for fixed and free distributions. The data can be imported as an Excel sheet. Additionally, with PCQ for Windows (commercial) and PQ Method (free), clickable software is available, too. \nQ-sample data are analysed through principal component analysis (PCA). Therefore, the Q sorts are intercorrelated and factor analysed. The Varimax rotation is often suggested for this purpose (Milcu 2014; Watts & Stenner 2005). For the factor analyses, the PCA function of the FactoMineR package in R is suggested. This analysis allows us to identify factors with eigenvalues above 1. This has the purpose to exclude factors that serve no data-reductive purpose. These are the factors that explain less of the variance than would any single Q sort (Watts & Stenner 2005). An interpretable factor must have at least two Q sorts that load significantly onto this factor alone. This means that at least two participants who share this factor must have produced very similar Q sorts. These participants make sense of the Q items in a similar way. \n\n'''7. Interpretation of results'''\n\nFor interpretation of the Q sorts, possible solutions are chosen based on statistical as well as content-related aspects.  Q sorts are selected that are characteristic of a factor and significantly load onto this factor alone. These are called \u201cfactor exemplars\u201d. They exemplify the configuration of Q items that is characteristic for this factor (Watts & Stenner 2005).  The factors exemplars are interpreted by using the information of the post-sort interview. By doing so, the researcher can figure out if interviewees whose Q sorts significantly load onto the same factor also share other properties such as age, occupation or interests.\n\n\n== Strengths & Challenges ==\nQ methodology  is often criticised since the outcome depends on the way the research question is stated and on the way the Q items are chosen and/ or formulated. Consequently, the researcher needs to pay a lot of attention towards these two aspects to get meaningful results (Watts and Stenner 2005). This can be done by doing pre-tests. Moreover, the interpretation of the results is rather subjective (Eden et al. 2005). Therefore, the validity and reproducibility of the Q methodology is often under debate. However, William Stephenson assures that the results of the repetitions of a Q method might not be identical, but still reveal relatively similar patterns (Stephenson 1972). Consequently the main conclusion of the repetitions of a Q methodology should be the same. \nDue to the qualitative nature of the Q methodology, it can only be used to identify points of view in a population, but not by how many people these points of view are shared. Therefore, it can only be used for exploratory purposes but not for hypothesis testing. \nAnother challenge regarding this method is that it does not include temporal dynamics, but portrays a temporally frozen image of viewpoints (Watts and Stenner 2005). Therefore, it is crucial to interpret the results of Q methodology in relation to the point in time when the data was collected. For instance, in the case of environmental questions and ecosystem services, it is known that perceptions of a population change with the seasons (Greenland-Smith & Sherren 2016; van Berkel & Verburg 2014). An ecosystem service such as outdoor recreation might be more important to people in summer than in winter. \nThe strength of the Q methodology lies in explorative studies in cases where there is not yet much data on a certain topic. In these cases, it creates a good overview of different perspectives. Moreover, it provides a large number of application opportunities as it can be used for various items such as statements, pictures and all kinds of objects. For the participants, it is more intuitive and fun to conduct a Q method-based interview than just answering questions or filling out a questionnaire. Since the implementation of the method is relatively straightforward compared to other methods, it is applicable to collect perceptions of all kinds of different stakeholders. \n\n\n== Normativity ==\nQ methodology is able to combine qualitative and quantitative aspects of knowledge, as it combines a statistical design with intuitive sorting of items such as pictures and statements. Therefore, this method has great potential for collecting data on stakeholders' perceptions on topics concerning the world's most disputed problems.\nHowever, the results are not representative for bigger populations as they are strongly dependent on the perspectives of the relatively few participants and their respective context. If the selection of participants is thus not representative, important perspectives could be unintentionally excluded and, as a result, overlooked (Danielson et al. 2009). The reason for the low number of participants is that in traditional Q studies, there used to be the rule that the number of participants should only be half the number of Q items. Otherwise, the authors feared to lose the qualitative aspects of this method (Watts & Stenner 2005). \nIn the future, the Q methodology might benefit from loosening this strict rule and allowing more participants to be included in the study. This would also allow more perspectives to be added and would therefore solve one of the main issues of this method. Additionally, more insightful statistical analyses would be possible with a larger number of participants, allowing not only a greater understanding of variance, but also a linkage of the outcome of the method with contextual data about the participants.\nThat a development of this method is possible and can be successful, is illustrated by the factor analysis which in traditional Q studies used the centroid method. This method provides a theoretically infinite number of rotated solutions. In the long run, this method proved to be too elaborate as the Q analyst has to consider a lot of different rotated solutions before they choose the most informative one. Instead, nowadays the PCA method is often used for factor analysis. This works much faster as the PCA automatically chooses the mathematically superior solution. The results of the PCA are described as \u201cequally satisfying\u201d as the results of the centroid method (Watts & Stenner 2005). \n\n== Outlook ==\nIn the future, Q methodology should be improved even further and adapted to the purposes it is nowadays used for. Since it is already used in interdisciplinary research, it makes sense to step away from the strict rules that it was associated with when it originated from qualitative psychology, where these rules were more of a conflict point instead of a solution\n\n\n== Key Publications ==\n'''Theoretical'''\n\nStephenson, W. (1935). Correlating Persons Instead of Tests. Journal of Personality 4(1), 17\u201324. https://doi.org/10.1111/j.1467-6494.1935.tb02022.x.\n\nStephenson, William (1936). Some recent contributions to the theory of psychometry. Journal of Personality 4 (4), 294-304. DOI: 10.1111/j.1467-6494.1936.tb02035.x.\n\nWatts, S., & Stenner, P. (2005). Doing Q methodology: Theory, method and interpretation. Qualitative Research in Psychology, 2(1), 67\u201391. https://doi.org/10.1191/1478088705qp022oa\n\n'''Empirical'''\n\nMilcu, A. I. , Sherren, K. , Hanspach, J. , Abson, D. , & Fischer, J. (2014). Navigating conflicting landscape aspirations: application of a photo-based Q-method in Transylvania (Central Romania). Land Use Policy, 408\u2013422.\n\n\n== References ==\nBaral, H., Jaung, W., Dutt, L., Sonam, B., Sunil, P., Kiran, S., Ardavan, P., Robin, Z., Sears, R., Sharma, R., Dorji, T., & Artati, Y. (2017). Approaches and tools for assessing mountain forest ecosystem services. Center for International Forestry Research and International  Center for Integrated Mountain Development.\n\nBurkhardt, V. (2017). Identification and Assessment of Ecosystem Services in the UNESCO Biosphere Reserve Schaalsee. Master Thesis at Leuphana University of Lueneburg.\n\nDanielson, S., Webler, T., & Tuler, S. P. (2010). Using Q method for the formative evaluation of public participation processes. Society and Natural Resources, 23(1), 92\u201396. https://doi.org/10.1080/08941920802438626\n\nDennis, K. E., & Goldberg, A. P. (1996). Weight Control Self-Efficacy Types and Transitions Affect Weight-Loss Outcomes in Obese Women. Addictive Behaviors, 21(I), 103\u2013116.\n\nEden, Sally; Donaldson, Andrew; Walker, Gordon (2005): Structuring subjectivities? Using\nQ methodology in human geography. Area 37 (4), 413-422. DOI: 10.1111/j.1475-4762.2005.00641.x.\n\nGreenland-Smith, Simon; Brazner, John; Sherren, Kate (2016). Farmer perceptions of wetl\nands and waterbodies: Using social metrics as an alternative to ecosystem service valuation. Ecological Economics 126, 58\u201369. DOI: 10.1016/j.ecolecon.2016.04.002.\n\nHamadou, I., Moula, N., Siddo, S., Issa, M., Marichatou, H., Leroy, P., & Antoine-Moussiaux, N. (2016). Mapping stakeholder viewpoints in biodiversity management: an application in Niger using Q methodology. Biodiversity and Conservation, 25(10), 1973\u20131986. https://doi.org/10.1007/s10531-016-1175-x.\n\nMilcu, A. I. , Sherren, K. , Hanspach, J. , Abson, D. , & Fischer, J. (2014). Navigating conflicting landscape aspirations: application of a photo-based Q-method in Transylvania (Central Romania). Land Use Policy, 408\u2013422.\n\nStainton  Rogers,  R.  (1995).  Q  methodology.  In Smith, J.A., Harre, R. and Van Langenhove, L., editors, Rethinking methods in psychology. London: Sage.\n\nStephenson, W. (1935). Correlating Persons Instead of Tests. Journal of Personality 4(1), 17\u201324. https://doi.org/10.1111/j.1467-6494.1935.tb02022.x.\n\nStephenson, William (1936). Some recent contributions to the theory of psychometry. Journal of Personality 4 (4), 294-304. DOI: 10.1111/j.1467-6494.1936.tb02035.x.\n\nStephenson, William (1972). Applications of Communication Theory: II - Interpretation of Keats\u2019 \u201cOde on a Grecian Urn\u201d. The Psychological Record 22 (2), 177-192. DOI: 10.1007/BF03394078.\n\nvan Berkel, Derek B.; Verburg, Peter H. (2014). Spatial quantification and valuation of cultural ecosystem services in an agricultural landscape. Ecological Indicators 37, 163\u2013174. DOI: 10.1016/j.ecolind.2012.06.025.\nWatts, S., & Stenner, P. (2005). Doing Q methodology: Theory, method and interpretation. Qualitative Research in Psychology, 2(1), 67\u201391. https://doi.org/10.1191/1478088705qp022oa\nZabala, A. (2014) qmethod: A Package to Explore Human Perspectives Using Q Methodology. The R Journal, 6(2):163-173.\n----\n[[Category:Qualititative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\n\nThe [[Table of Contributors|author]] of this entry is Anna-Lena Rau."
                    },
                    "sha1": "h8ul7uki0et3v344nfe60l8irzup31o"
                }
            },
            {
                "title": "Questioning the status quo in methods",
                "ns": "0",
                "id": "554",
                "revision": {
                    "id": "6364",
                    "parentid": "5927",
                    "timestamp": "2021-09-10T12:05:33Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14844",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Questioning the status quo in methods (German)]].\n\n'''In short:''' This entry discusses why the status quo of scientific methods is insufficient to solve the problems of our time.\n__NOTOC__\n== Questioning the ''status quo'' ==\nBuilding on the general recognition that current methods are not enough to approximate knowledge towards the solution needed for the wicked problems which we face, the questions arises how we can meaningfully question the ''status quo'' in methods, and do this in a way that moves us forward. Too many enthusiastic questions on the ''status quo'' in the realms of [[Glossary|scientific methods]] have been proposed without a clear understanding what is actually the problem, or better, which knowledge we are lacking. The current state of many lines of thinking urged us to move out of the dimensions of a normal science in the sense of Kuhn. However, calling out a revolution does not mean that all problems are automatically solved. On the contrary: concerning many aspects, we are still in a state of explicitly not knowing how to solve the wicked problems we face. Let us look at three examples.\n\n==== Climate Change ====\nClimate change is a complex phenomenon: at first, the realisation that human-induced climate change is happening was slowly emerging out of diverse scientific data sources and research approaches. While today, the majority of society has accepted that human-induced climate change is real, and that it is our responsibility to react and counteract, there is no general agreement on a best way to do so. Instead, there are calls for a globally orchestrated policy-response, which takes place are on a totally different scale compared to local adaptation. Most importantly, how can we convince citizens in countries which lead in terms of a negative contribution to climate change for decades to not only change their behaviour, but actually to contribute to reversing its catastrophic effects? This is the current frontier in research, and many promising suggestions and strategies are currently being investigated. However, since we may only know in retrospect how this problem was at least approachable if not completely solvable, after all, we need to further diversify our research approaches, and also consider the urgency of the problem. A 20-year longitudinal study won't do the trick - we need suggestions as soon as possible. The urgency and the wickedness of this problem showcase the need for novel methods to contribute to the approximation of solutions. \n\n==== Actor participation ====\nResearch about normative challenges - as well as research about joined problem framing between actors and researchers - has been on the rise. More and more studies engage in the new contract between science and society. However, the roles and [[Glossary|power]] relations of different actors within a system are deeply contextual, and so far the knowledge of such studies did not yet saturate into a more general understanding on how such study setting can be approached. While blueprints already exist and there is a growing understanding of relevant concepts, such as social learning, actor participation is still something that did not find its way into a broad diversity of textbooks, and available approaches are far from unified. Most of the normal sciences actually dispute whether the recognition of actor knowledge is actually contributing to scientific progress, and instead keep judging the difference between applied science and their own science. What is more, actor participation is approached from all sorts of disciplinary backgrounds. This increases diversity in terms of methods, but ultimately leads to more confusion because the different approaches rooted in diverse disciplines are often pitched as being either superior or inferior to each other. While it is clear that different methods have different values in a specific context, a comparison of different methods which allow for actor participation is widely lacking to date. However, only fair comparisons of diverse approaches may allow for a claim of which methodological approach has a higher validity in a respective context.\n\n==== Sustainable consumption ====\nThe question how we shift our consumption towards being more sustainable is another thriving debate within sustainability science and beyond. While there is research focusing on global trade and its inequalities, there is equally research on individual behaviour and the motivations of consumers. Understanding behavior - and even more so - driving behaviour change in terms of sustainable consumption is to date a diverse field, with methodological roots in psychology, social science and many other domains. On the other hand, global supply chains and trade arrangements are part of totally different fields in sciences and these two scales are hardly matched. There is a clear gap between research focusing on supply and research focussing on demand. From a methodological standpoint, integrating the global and the individual scale, and supply and demand, already poses a very complex challenge, showcasing how a link between these diverse line of thinking will preoccupy research for the foreseeable future. Atomising challenges into smaller chunks that represent parts of the picture follows a long tradition in science, yet integrating these diverse approaches will be vital in order to take the whole depth and width of the challenges into account.\n\n== Three pathways of methodological innovation ==\nWe can thus conclude that urgency, wickedness, normativity, context, scale integration and many other challenges are currently recognised in the community of sustainability science researchers, and deserve more focus in order to generate solutions. This non-exhaustive list of problems already showcases that this is more easily proposed than actually done. Within sustainability science and beyond, there is almost an obsession to question the ''status quo'', and to proclaim what methods should do. It sounds so appealing to \"dance with the system\" - a sentence borrowed from Donella Meadows (I always asked myself what that actually means). But how do we take the normative burden off of all the proposals for transformation and take a first step as individual researchers? Here, I propose three perspectives on how [[Glossary|innovation]] in terms of scientific methods could be more concretely approached. \n\n'''1) Invention of new methods'''\nInventing new methods surely sounds exciting. There is a whole world of knowledge that awaits us, and new methods might be potentially able to unlock this new knowledge. Despite the spirit that motivates many researchers to this end, I would like to highlight that in the past, the invention of new methods often came out of a clear recognition of a lack of methods in order to solve a specific problems. The proposals of Interviews as a scientific method, or [[Grounded Theory]], were rooted in the recognition of a lack of a suitable methodological approach for a specific problem. Take [[Open Interview|Open Interviews]] as an example, which allowed for an inductive recognition of the perceptions of individuals. This type of knowledge as such did not exist before, and the proposal of these new methods allowed for a gap to be closed. In order to close this gap, I believe that this gap needed to be recognized, which was rooted in the recognition of clear knowledge and experience with methods that were already there. \n\nTake Fischers Analysis of Variance as another example, which basically took the implementation of systematic [[Experiments and Hypothesis Testing|experimentation]] onto a completely new level. Fischer saw existing research at the agricultural research center he was working at as a statistician, and he saw a clear lack of a systematic method that allowed to generate knowledge in terms of pattern recognition based on the testing of a predominated hypothesis. Fischer knew the state of the art, and he recognised a clear gap in the canon of existing methods. What is more, he had experience in the already existing methods, which was a precondition for his formulation of the Analysis of Variance. The invention of new methods is thus quite often a gradual process in a continuous development, although historians of science often reduce it to one point in time. To this end, I propose to recognise more of a continuum in which such new methods are being proposed. Such innovative settings typically build on experience regarding existing methods, and through this a recognition of a gap within the already existing methods, or better even, of the knowledge which these methods can produce. \n\n'''2) Relocation of existing methods'''\nMethodological innovation often build on existing methods, if these are used in a different setting or context. For instance, [[Semi-structured Interview|interviews]] were applied in the world value survey at a global scale, allowing for a global comparison of people's values based on tens of thousands of interviews. The method of structured interviews existed long before that, and one could argue that such larger surveys even predated the 1960 proclamation of Strauss and Glaser regarding interviews. However, a global comparison was clearly unique. Another example is how the [[ANOVA|Analysis of Variance]] was once widely reserved for [[:Category:Deductive|deductive]] experiments, until the [[History of Methods|rise of data science]] and the availability of more data though the Internet and other sources led to the application of the Analysis of Variance in purely [[:Category:Inductive|inductive]] settings. Another example would be the diverse applications of text analysis, which were once postulated for very specific setting, but are today applied in all sort of branches of science. The relocation of a method basically utilises an existing method in a different setting of context than it was original intended to be used. Again, this does not only imply that experience with the existing method is a necessary precondition for its relocation, but the real innovation potential comes out of the recognition of a gap, and that the relocation of the existing method may close this gap by creating new knowledge. \n\n'''3) Recombination of existing methods\nAnother possibility of creating innovation in methodology is through the recombination of existing methods. For example, with the rise of data becoming available through surveys, statistics allowed for the utilisation of diverse methods to analyse data coming out of surveys, and later also from structured interviews. Such innovation is rather gradual, and less recognised by the scientific community. Another example would be the utilisation of statistical methods for coded items from a Grounded Theory approach. While originally this approach focused on qualitative research, there are now examples how coded information is compiled in a table which is then statistically analysed. Such innovation is often a violation of the original rules or norms that were proclaimed with a method, and thus have also attracted controversy and criticism in the past. Nevertheless, recombining diverse methods builds on a deep understanding and even experience of a respective method, showcasing yet again how researchers created new knowledge by building on already established methodological approaches.\n\n== Some remarks on innovation and experience ==\nThese three proposed possibilities of methodological innovation are clearly building on [[Design Thinking]], as the development of methodological applications can be understood as a design problem. Creating a methodological design is necessarily a commitment that may be criticised as static or even downright dogmatic by some researchers, yet the here proposed approaches for innovation should rather be seen as a starting point than as a new dogma. Instead, I propose a counterpoint to a hype for methodological innovation that may be appealing to many, yet makes it very hard for researchers to start with their specific research in my experience. There is a tendency in sustainability science to reject all methodological dogmas in order to be unbiased and more creative. While this sounds appealing in theory, I would counterargue in the spirit of [[Bias and Critical Thinking|critical realism]]: some observable realities can be accessed by existing methods, and some new activated knowledge that we try to unravel may demand methodological innovation. All this implies that - following Roy Baskhar - we should clearly reject any claim of any knowledge being perfect, independent of whether it is based on old or new methodologies. Knowledge can and most certainly will change. \n\nHowever, the urge for novel methodologies is so strong right now that experience in existing methodologies is rejected altogether. I consider this to be a dangerous new dogma, and kindly ask future researchers to consider experience in empirical methodologies as an important basis for innovation. As I have clearly shown above, there can be more than just theoretical considerations why methodological innovation can build on experience in existing methodologies. Such experience should always include a recognition of its limitations, and in the long run there might be a greater recognition of new design approaches that shall drive methodological innovation.\n\nThe interconnectedness of different forms of knowledge; reflexivity of researchers and other actors; the recognition of [[Agency, Complexity and Emergence|complexity]] in systems; and the necessity of power relations in knowledge production are prominent examples of current challenges that methodologies as of yet have not solved. We are probably still far away from the creation of a solidified canon of knowledge to solve these riddles. One could argue that we will never solve these methodological problems altogether, but that the joined action and interaction while working on these problems will be the actual solution, and that a real solution as with past methods of normal science shall never be reached. This is unclear to date, yet we need to become aware that when we question the ''status quo'' of knowledge production and methodologies, we may not be looking for goals, but more for a path. There are several examples in sustainability science that follow this path, such as [[Transdisciplinarity|transdisciplinarity]] and [[System Thinking & Causal Loop Diagrams|system thinking]]. However, according to critical realism, it may be good to solidify innovative and novel methodologies to a point that enables as to take meaningful actions based on the knowledge we create.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "gipt2m4ymn88do0m70ztbft9v8jtz2u"
                }
            },
            {
                "title": "Questioning the status quo in methods (German)",
                "ns": "0",
                "id": "556",
                "revision": {
                    "id": "6789",
                    "parentid": "6366",
                    "timestamp": "2022-10-10T12:30:55Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17176",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Questioning the status quo in methods]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag diskutiert, wieso der aktuelle Stand wissenschaftlicher Methodik angesichts der Probleme unserer Zeit unzureichend ist.\n__NOTOC__\n== Den ''status quo'' infrage stellen ==\nAusgehend von der allgemeinen Erkenntnis, dass die derzeitigen Methoden nicht ausreichen, um sich dem notwendigen Wissen f\u00fcr die L\u00f6sung der \"wicked problems\" anzun\u00e4hern, mit denen wir konfrontiert sind, stellt sich die Frage, wie wir den ''status quo'' der Methoden sinnvoll in Frage stellen k\u00f6nnen, und zwar auf eine Weise, die uns voranbringt. Allzu viele \u00dcberarbeitungen des ''Status quo'' im Bereich wissenschaftlicher Methoden wurden vorgeschlagen, ohne ein klares Verst\u00e4ndnis daf\u00fcr, was eigentlich das Problem ist, oder besser, welches Wissen uns fehlt. Der gegenw\u00e4rtige Zustand vieler wissenschaftlicher Denkschulen dr\u00e4ngt uns dazu, die Dimensionen einer normalen Wissenschaft im Sinne Kuhns zu verlassen. Der Aufruf zu einer Revolution bedeutet aber nicht, dass alle Probleme automatisch gel\u00f6st sind. Im Gegenteil: In vielerlei Hinsicht befinden wir uns immer noch in einem Zustand, in dem wir explizit nicht wissen, wie wir die \"wicked problems\" l\u00f6sen sollen, vor denen wir stehen. Schauen wir uns drei Beispiele an.\n\n==== Klimawandel ====\nDer Klimawandel ist ein komplexes Ph\u00e4nomen: Die Erkenntnis, dass es einen vom Menschen verursachten Klimawandel gibt, hat sich zun\u00e4chst nur langsam aus den verschiedenen wissenschaftlichen Datenquellen und Forschungsans\u00e4tzen herauskristallisiert. W\u00e4hrend heute die Mehrheit der Gesellschaft akzeptiert hat, dass der vom Menschen verursachte Klimawandel real ist und dass es in unserer Verantwortung liegt, darauf zu reagieren und gegenzusteuern, gibt es keine allgemeine Einigung \u00fcber den besten Weg, dies zu tun. Stattdessen gibt es Forderungen nach einer global orchestrierten politischen Reaktion, die auf einer ganz anderen Ebene stattfindet als lokale Anpassung an den Klimawandel. Am wichtigsten ist die Frage, wie wir die B\u00fcrger*innen in L\u00e4ndern, die seit Jahrzehnten den gr\u00f6\u00dften negativen Beitrag zum Klimawandel leisten, davon \u00fcberzeugen k\u00f6nnen, nicht nur ihr Verhalten zu \u00e4ndern, sondern tats\u00e4chlich dazu beizutragen, dessen katastrophalen Auswirkungen umzukehren. Dies ist die aktuelle Grenze der Forschung, und viele vielversprechende Vorschl\u00e4ge und Strategien werden derzeit untersucht. Da wir aber vielleicht erst im Nachhinein wissen, wie dieses Problem angegangen, wenn nicht sogar vollst\u00e4ndig gel\u00f6st werden konnte, m\u00fcssen wir unsere Forschungsans\u00e4tze weiter diversifizieren und auch die Dringlichkeit des Problems ber\u00fccksichtigen. Eine 20-j\u00e4hrige L\u00e4ngsschnittstudie wird nicht ausreichen - wir brauchen so schnell wie m\u00f6glich Vorschl\u00e4ge. Die Dringlichkeit und die Verquicktheit dieses Problems unterstreichen die Notwendigkeit neuartiger Methoden, die zur Ann\u00e4herung an L\u00f6sungen beitragen. \n\n==== Akteursbeteiligung ====\nForschung \u00fcber normative Herausforderungen - wie auch die Forschung \u00fcber gemeinsame Problemstellungen zwischen Akteur*innen und Forschenden - hat zugenommen. Mehr und mehr Studien besch\u00e4ftigen sich mit dem neuen Vertrag zwischen Wissenschaft und Gesellschaft. Allerdings sind die Rollen und Machtverh\u00e4ltnisse der verschiedenen Akteur*innen innerhalb eines Systems zutiefst kontextabh\u00e4ngig, und bisher ist das Wissen solcher Studien noch nicht in ein allgemeineres Verst\u00e4ndnis dar\u00fcber kumuliert, wie ein solches Forschungssetting angegangen werden kann. Zwar gibt es bereits Entw\u00fcrfe und ein wachsendes Verst\u00e4ndnis relevanter Konzepte, wie z.B. des sozialen Lernens, aber die Beteiligung von Akteur*innen hat immer noch nicht den Weg in eine breite Vielfalt von Lehrb\u00fcchern gefunden, und die verf\u00fcgbaren Ans\u00e4tze sind alles andere als einheitlich. Die meisten der normalen Wissenschaften streiten sogar dar\u00fcber, ob die Anerkennung von Akteur*innenwissen \u00fcberhaupt wirklich zum wissenschaftlichen Fortschritt beitr\u00e4gt, und beurteilen stattdessen weiterhin den Unterschied zwischen angewandter Wissenschaft und ihrer eigenen Wissenschaft. Hinzu kommt, dass die Akteur*innenbeteiligung von den unterschiedlichsten disziplin\u00e4ren Hintergr\u00fcnden her angegangen wird. Das erh\u00f6ht die Methodenvielfalt, f\u00fchrt aber letztlich zu mehr Verwirrung, weil die verschiedenen Ans\u00e4tze, die in unterschiedlichen Disziplinen wurzeln, oft als einander \u00fcberlegen oder unterlegen hingestellt werden. W\u00e4hrend es klar ist, dass verschiedene Methoden in einem bestimmten Kontext unterschiedlich wertvoll sind, fehlt ein Vergleich verschiedener Methoden, die Akteur*innenbeteiligung erm\u00f6glichen, bisher weitgehend. Nur durch einen fairen Vergleich verschiedener Ans\u00e4tze kann jedoch eine Aussage dar\u00fcber getroffen werden, welcher methodische Ansatz in einem jeweiligen Kontext eine h\u00f6here Aussagekraft hat.\n\n==== Nachhaltiger Konsum ====\nDie Frage, wie wir unseren Konsum in Richtung Nachhaltigkeit ver\u00e4ndern k\u00f6nnen, ist eine weitere lebhafte Debatte innerhalb der Nachhaltigkeitswissenschaft und dar\u00fcber hinaus. W\u00e4hrend es Forschung gibt, die sich auf den globalen Handel und seine Ungleichheiten konzentrier, gibt es ebenso solche zum individuellen Verhalten und zu den Motivationen von Verbrauchenden. Verhalten in Bezug auf nachhaltigen Konsum zu verstehen - und mehr noch, es zu ver\u00e4ndern - ist bis heute ein vielf\u00e4ltiges Feld, mit methodischen Wurzeln in der Psychologie, den Sozialwissenschaften und vielen anderen Bereichen. Auf der anderen Seite sind globale Lieferketten und Handelsvereinbarungen Teil ganz anderer Wissenschaftsbereiche und diese beiden Skalen sind kaum aufeinander abgestimmt. Es gibt eine klare L\u00fccke zwischen der Forschung, die sich auf das Angebot konzentriert, und der Forschung, die sich auf die Nachfrage konzentriert. Aus methodischer Sicht stellt die Integration der globalen und der individuellen Skala sowie von Angebot und Nachfrage bereits eine sehr komplexe Herausforderung dar, die zeigt, wie eine Verbindung zwischen diesen unterschiedlichen Denkrichtungen die Forschung in absehbarer Zeit besch\u00e4ftigen wird. Die Atomisierung von Herausforderungen in kleinere Teile, die Teile des Bildes darstellen, folgt einer langen Tradition in der Wissenschaft, doch die Integration dieser verschiedenen Ans\u00e4tze wird entscheidend sein, um die gesamte Tiefe und Breite der Herausforderungen zu ber\u00fccksichtigen. \n\n\n== Drei Pfade methodologischer Innovation ==\nWir k\u00f6nnen also schlussfolgern, dass Dringlichkeit, \"wickedness\", Normativit\u00e4t, Kontext, Skalenintegration und viele andere Herausforderungen derzeit in der Gemeinschaft der Nachhaltigkeitswissenschaftler*innen anerkannt sind und mehr Fokus verdienen, um L\u00f6sungen generieren zu k\u00f6nnen. Diese unvollst\u00e4ndige Liste von Problemen zeigt bereits, dass dies leichter gesagt als tats\u00e4chlich getan wird. Innerhalb der Nachhaltigkeitswissenschaft und dar\u00fcber hinaus gibt es fast eine Besessenheit, den ''status quo'' in Frage zu stellen und zu proklamieren, was Methoden tun sollten. Es klingt so verlockend, \"mit dem System zu tanzen\" - ein Satz, der Donella Meadows entlehnt ist (ich habe mich immer gefragt, was das eigentlich bedeutet). Aber wie nehmen wir all den Vorschl\u00e4gen zur Ver\u00e4nderung die normative Last ab und machen als einzelne Forscher einen ersten Schritt? Hier schlage ich drei Perspektiven vor, wie Innovation in Bezug auf wissenschaftliche Methoden konkreter angegangen werden k\u00f6nnte.\n\n'''1) Neue Methoden erfinden'''\nNeue Methoden zu erfinden klingt sicherlich aufregend. Es gibt eine ganze Welt des Wissens, die auf uns wartet, und neue Methoden k\u00f6nnten potenziell in der Lage sein, dieses neue Wissen zu erschlie\u00dfen. Trotz des Geistes, der viele Forschenden zu diesem Ziel motiviert, m\u00f6chte ich hervorheben, dass in der Vergangenheit die Erfindung neuer Methoden oft aus der klaren Erkenntnis eines Mangels an Methoden zur L\u00f6sung eines bestimmten Problems entstand. Die Vorschl\u00e4ge des Interviews als wissenschaftliche Methode oder der [[Grounded Theory]] wurzelten in der Erkenntnis eines Mangels an einem geeigneten methodischen Ansatz f\u00fcr ein spezifisches Problem. Als Beispiel seien hier [[Open Interview|offene Interviews]] genannt, die ein induktives Verst\u00e4ndnis der Wahrnehmungen von Individuen erm\u00f6glichten. Diese Art der Erkenntnis als solche gab es vorher nicht, und durch den Vorschlag dieser neuen Methoden konnte eine L\u00fccke geschlossen werden. Um diese L\u00fccke zu schlie\u00dfen, musste meiner Meinung nach diese L\u00fccke erkannt werden, was auf dem Erkennen von klarem Wissen und Erfahrungen mit Methoden, die bereits vorhanden waren, beruhte. \n\nNehmen wir zum Beispiel auch Fischers Varianzanalyse, die im Grunde genommen die Durchf\u00fchrung systematischer [[Experiments and Hypothesis Testing|Experimente]] auf eine v\u00f6llig neue Ebene brachte. Fischer sah die bestehende Forschung an dem landwirtschaftlichen Forschungszentrum, an dem er als Statistiker arbeitete, und er sah einen klaren Mangel an einer systematischen Methode, die es erlaubte, Wissen im Sinne von Mustererkennung basierend auf der Pr\u00fcfung einer vorherrschenden Hypothese zu generieren. Fischer kannte den Stand der Technik, und er erkannte eine deutliche L\u00fccke im Kanon der vorhandenen Methoden. Dar\u00fcber hinaus hatte er Erfahrung mit den bereits existierenden Methoden, was eine Voraussetzung f\u00fcr seine Formulierung der Varianzanalyse war. Die Erfindung neuer Methoden ist also durchaus ein allm\u00e4hlicher Prozess in einer kontinuierlichen Entwicklung, auch wenn Wissenschaftshistoriker*innen sie oft auf einen einzelnen Zeitpunkt reduzieren. Ich schlage daher vor, eher ein Kontinuum anzuerkennen, in dem solche neuen Methoden vorgeschlagen werden. Solche innovativen Einstellungen bauen typischerweise auf der Erfahrung mit bestehenden Methoden auf, und damit auf der Erkenntnis einer L\u00fccke innerhalb der bereits existierenden Methoden, oder besser noch, auf dem Wissen, das diese Methoden produzieren k\u00f6nnen. \n\n'''2) Relokation bestehender Methoden'''\nMethodische Innovationen bauen oft auf bestehenden Methoden auf, wenn diese in einem anderen Umfeld oder Kontext verwendet werden. So wurden z. B. [[Semi-structured Interview|Interviews]] in der 'World Value Survey' auf globaler Ebene angewandt und erm\u00f6glichten einen globalen Vergleich der Werte der Menschen auf der Grundlage von zehntausenden von Interviews. Die Methode der strukturierten Interviews gab es schon lange vorher, und man k\u00f6nnte argumentieren, dass solche gr\u00f6\u00dferen Umfragen sogar der 1960 von Strauss und Glaser verk\u00fcndeten Befragung vorausgingen. Ein globaler Vergleich war jedoch eindeutig einzigartig. Ein anderes Beispiel ist, wie die Varianzanalyse einst weitgehend f\u00fcr [[:Category:Deductive|deduktive]] Experimente reserviert war, bis der [[History of Methods|Aufstieg der Datenwissenschaft]] und die Verf\u00fcgbarkeit von mehr Daten durch das Internet und andere Quellen zur Anwendung der Varianzanalyse in rein [[:Category:Induktive|induktiven]] Settings f\u00fchrte. Ein weiteres Beispiel w\u00e4ren die vielf\u00e4ltigen Anwendungen der Textanalyse, die einst f\u00fcr ein sehr spezifisches Setting postuliert wurden, heute aber in allen m\u00f6glichen Wissenschaftszweigen Anwendung finden. Die Verlagerung einer Methode nutzt im Grunde eine bestehende Methode in einem anderen Setting oder Kontext, als sie urspr\u00fcnglich gedacht war. Auch hier bedeutet dies nicht nur, dass die Erfahrung mit der bestehenden Methode eine notwendige Voraussetzung f\u00fcr ihre Verlagerung ist, sondern das eigentliche Innovationspotenzial ergibt sich aus dem Erkennen einer L\u00fccke, und dass die Verlagerung der bestehenden Methode diese L\u00fccke durch die Schaffung neuen Wissens schlie\u00dfen kann. \n\n'''3) Rekombination bestehender Methoden\nEine weitere M\u00f6glichkeit, methodologische Innovation zu schaffen, besteht in der Neukombination bestehender Methoden. Mit der Zunahme von Daten, die durch Umfragen verf\u00fcgbar wurden, erm\u00f6glichte die Statistik beispielsweise die Verwendung verschiedener Methoden zur Analyse von Daten aus Umfragen und sp\u00e4ter auch aus strukturierten Interviews. Solche Innovationen sind eher allm\u00e4hlich und werden von der wissenschaftlichen Gemeinschaft weniger anerkannt. Ein weiteres Beispiel w\u00e4re die Nutzung statistischer Methoden f\u00fcr kodierte Items aus einem Grounded Theory-Ansatz. W\u00e4hrend sich dieser Ansatz urspr\u00fcnglich auf die qualitative Forschung konzentrierte, gibt es jetzt Beispiele, wie kodierte Informationen in einer Tabelle zusammengestellt werden, die dann statistisch ausgewertet wird. Solche Innovationen sind oft ein Versto\u00df gegen die urspr\u00fcnglichen Regeln oder Normen, die mit einer Methode proklamiert wurden, und haben daher in der Vergangenheit auch Kontroversen und Kritik hervorgerufen. Nichtsdestotrotz baut die Neukombination verschiedener Methoden auf einem tiefen Verst\u00e4ndnis und sogar Erfahrung mit der jeweiligen Methode auf und zeigt einmal mehr, wie Forscher neues Wissen schaffen, indem sie auf bereits etablierten methodischen Ans\u00e4tzen aufbauen. \n\n== Ein paar Anmerkungen zu methodologischer Innovation und Erfahrung ==\nDiese drei vorgeschlagenen M\u00f6glichkeiten der methodischen Innovation bauen eindeutig auf [[Design Thinking]] auf, da die Entwicklung von methodischen Anwendungen als ein Designproblem verstanden werden kann. Die Erstellung eines methodischen Designs ist notwendigerweise eine Verpflichtung, die von einigen Forschenden als statisch oder sogar geradezu dogmatisch kritisiert werden kann, dennoch sollten die hier vorgeschlagenen Ans\u00e4tze zur Innovation eher als Ausgangspunkt denn als neues Dogma gesehen werden. Stattdessen schlage ich einen Kontrapunkt zu einem Hype um methodische Innovation vor, der zwar f\u00fcr viele ansprechend sein mag, es Forschenden meiner Erfahrung nach aber sehr schwer macht, mit ihrer spezifischen Forschung zu beginnen. In der Nachhaltigkeitswissenschaft gibt es eine Tendenz, alle methodologischen Dogmen abzulehnen, um unvoreingenommen und kreativer zu sein. W\u00e4hrend dies in der Theorie verlockend klingt, w\u00fcrde ich im Sinne des [[Bias and Critical Thinking|Kritischen Realismus]] gegenargumentieren: Einige beobachtbare Realit\u00e4ten k\u00f6nnen mit bestehenden Methoden erschlossen werden, und manches neu aktivierte Wissen, das wir zu entr\u00e4tseln versuchen, kann methodologische Innovation erfordern. All dies impliziert, dass wir - in Anlehnung an Roy Baskhar - jeden Anspruch darauf, dass jegliches Wissen perfekt ist, klar zur\u00fcckweisen sollten, unabh\u00e4ngig davon, ob es auf alten oder neuen Methoden beruht. Wissen kann und wird sich ganz sicher ver\u00e4ndern. \n\nAllerdings ist der Drang nach neuen Methodologien im Moment so stark, dass die Erfahrung mit bestehenden Methodologien g\u00e4nzlich abgelehnt wird. Ich halte dies f\u00fcr ein gef\u00e4hrliches neues Dogma und bitte k\u00fcnftige Forschende, Erfahrungen in empirischen Methodologien als wichtige Grundlage f\u00fcr Innovationen zu ber\u00fccksichtigen. Wie ich oben deutlich gezeigt habe, kann es mehr als nur theoretische \u00dcberlegungen geben, warum methodologische Innovation auf Erfahrungen in bestehenden Methodologien aufbauen kann. Eine solche Erfahrung sollte immer auch eine Anerkennung ihrer Grenzen beinhalten, und auf lange Sicht k\u00f6nnte es zu einer gr\u00f6\u00dferen Anerkennung neuer Designans\u00e4tze kommen, die methodologische Innovation vorantreiben sollen.\n\nDie Verflechtung verschiedener Wissensformen, die Reflexivit\u00e4t von Forschenden und anderen Akteur*innen, die Anerkennung von [[Agency, Complexity and Emergence|Komplexit\u00e4t]] in Systemen und die Notwendigkeit von Machtverh\u00e4ltnissen in der Wissensproduktion sind prominente Beispiele f\u00fcr aktuelle Herausforderungen, die Methodologien bisher nicht gel\u00f6st haben. Von der Schaffung eines verfestigten Wissenskanons zur L\u00f6sung dieser R\u00e4tsel sind wir wahrscheinlich noch weit entfernt. Man k\u00f6nnte argumentieren, dass wir diese methodischen Probleme nie ganz l\u00f6sen werden, sondern dass die gemeinsame Aktion und Interaktion bei der Arbeit an diesen Problemen die eigentliche L\u00f6sung sein wird, und dass eine wirkliche L\u00f6sung wie bei den bisherigen Methoden der Normalwissenschaft nie erreicht werden wird. Das ist bis heute unklar, aber wir m\u00fcssen uns bewusst werden, dass wir, wenn wir den ''status quo'' der Wissensproduktion und der Methoden in Frage stellen, vielleicht nicht nach Zielen, sondern eher nach einem Weg suchen. Es gibt einige Beispiele in der Nachhaltigkeitswissenschaft, die diesem Weg folgen, wie z.B. [[Transdisciplinarity|Transdisziplinarit\u00e4t]] und [[System Thinking & Causal Loop Diagrams|Systemdenken]]. Dem kritischen Realismus zufolge kann es jedoch sinnvoll sein, innovative und neuartige Methoden bis zu einem Punkt zu verfestigen, der es uns erm\u00f6glicht, auf der Grundlage des von uns geschaffenen Wissens sinnvolle Ma\u00dfnahmen zu ergreifen.\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "m78kqpztsz1bizesjrip8a5uc6x39pg"
                }
            },
            {
                "title": "Reading and Writing Files in Python",
                "ns": "0",
                "id": "1033",
                "revision": {
                    "id": "7271",
                    "parentid": "7022",
                    "timestamp": "2023-07-04T15:36:02Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to reading and writing files in Python",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6335",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Reading Files==\nThere are different types of file systems in many operating systems. However, most of the time, you will get off the hook when knowing the two below:\n{| class=\"wikitable\"\n|-\n! File System !! Extension Code !! Use\n|-\n| Text || .txt || text file, for stories, documents\n|-\n| Comma Separated Value || .csv || exported excel file, structured data, data from sensors, and numbers\n|}\n\nFirst, you need a file to read, so let\u2019s create a file.\n \nOpen the folder in which you wish to store the file. When right-clicking on an empty space, you are able to create a new file. Name it test_text.txt and open it. After that write something inside the file, save, and close it. Alternatively, you can also open the program that creates text files (for Windows 10 it is \"Notepad\") and create a new file by left-clicking on the file tab in the left corner and choosing \"New\" or press CTRL+N (both for Windows 10). After the file has been created, we can try to read from that particular file.\n\nTo read files from your computer to Python, there is a built-in function called \u201copen\u201d.\n\n<syntaxhighlight lang=\"Python\" line>\nfile = open(\"test_text.txt\")\n</syntaxhighlight>\nDon't forget that you have to specify the working directory when first opening the file. Before the \"test_text.txt\" (in the same quotation marks) you need to provide the file path so that Python knows where to find your text file\nNow, you can check if the file has been successfully read:\nFirst, you can read the file name:\n<syntaxhighlight lang=\"Python\" line>\nfile.name\n</syntaxhighlight>\n\nAnd then, of course, you can also check out the content of the file:\n<syntaxhighlight lang=\"Python\" line>\nprint(file.read())\n</syntaxhighlight>\nOnce you have finished your work with the file, we need to \u201cclose()\u201d the connection manually because the \u201copen()\u201d method uses some computer resources to clean up.\n\n<syntaxhighlight lang=\"Python\" line>\nfile.close()\n</syntaxhighlight>\n\n===WITH Statement===\nThe WITH statement is a python specific statement that manages and simplifies the opening and closing of files. Let's compare 2 codes:\n\n<syntaxhighlight lang=\"Python\" line>\n# code without WITH statement\nfile = open (\"test_text.txt\", \"r\")\nprint(file.read())\nfile.close()\n</syntaxhighlight>\n\n\n<syntaxhighlight lang=\"Python\" line>\n#code with WITH statement\nwith open (\"test_text.txt\", \"r\") as file:\n\t\tprint(file.read())\n</syntaxhighlight>\n\nWith the WITH statement, the close() method is automatically included. It also makes the code easier to read because of indentation.\n\n==Writing to Files==\nBesides just reading from the file, we can also write to a file.\n\n<syntaxhighlight lang=\"Python\" line>\nwr_file_ex = 'new_file.txt' # write file example\nwith open(wr_file_ex , \"w\") as writefile:\n    writefile.write(\"this is the first line \\\\n\")\n\t\twritefile.write(\"second line\\\\n\")\n</syntaxhighlight>\n\nThe code above will write 2 lines. Note that inside the \u201copen()\u201d method, we have to specify \u201cw\u201d (write) as an argument. In the very beginning, we have not specified any mode, which is why Python automatically used the reading mode. In the table at the end of this tutorial, you can find different modes you can use in Pyhthon.\n\nWe can also try to check what we have created:\n\n<syntaxhighlight lang=\"Python\" line>\nlines = [\"First line!\\n\", \"Second line!\\n\", \"Third line!\"]\n\nwith open(wr_file_ex, 'w') as writefile: \n\t\tfor line in lines: \n\t\t\t\twritefile.write(line)\n\n# and you can read the file using the read method\nwith open (write_file_ex, 'r') as read_file:\n\t\tprint(read_file.read())\n</syntaxhighlight>\n\nWarning: setting a file to mode \u201cw\u201d will overwrite all data inside.\n\n<syntaxhighlight lang=\"Python\" line>\nold_file = \"test_text.txt\"\nwith open (old_file,\"r\") as read_file: \n\t\tprint(read_file.read())\n# Write line to file\nwith open(old_file, \"w\") as write_file:\n    write_file.write(\"overwritten\\n\")\n\n# read again afterwards\nwith open (old_file,\"r\") as read_file: \n\t\tprint(read_file.read())\n</syntaxhighlight>\n\n==Appending==\nAs we know from the exercise above, writing overwrites all the old lines or data. Many times, we don't want the old data to be overwritten, and we just want to add a new line or new data. That\u2019s where appending comes in.\n\n<syntaxhighlight lang=\"Python\" line>\n# appending new lines to the old file\nold_file = \"test_text.txt\"\nwith open (old_file, 'a') as append_file: \n\t\tappend_file.write(\"another line\\\\n\")\n\t\tappend_file.write(\"another another line\\\\n\")\n\n# reading the file after new lines have been appended\nwith open (old_file, 'r') as read_appended_file: \n\t\tprint(read_appended_file.read())\n</syntaxhighlight>\n\n==Different modes when opening a file==\nHere is a table with the most important different modes when opening a file (we have covered almost all of them). Note that the last three are more advanced modes. They allow you to combine two different modes and you don't have to use the WITH statement whenever you want to combine two modes.\n\n{| class=\"wikitable\"\n|-\n! Statement !! Mode name !! Use\n|-\n| w || read mode || Overwriting files\n|-\n| r || write mode || Reading files\n|-\n| a || append mode || Adding new content to file\n|-\n| x || exclusive creation mode || Creating a new file and opening it for writing\n|-\n| r+ || reading and writing mode || Reading and writing, does not truncate file\n|-\n| w+ || writing and reading mode || Writing and reading, truncates file\n|-\n| a+ || appending and reading mode|| Creates new file if the file does not exist\n|}\n\n==Quiz==\n\n#  Try out a+ mode by running this:\n\n<syntaxhighlight lang=\"Python\" line>\n# quiz a+ mode\nwith open('Quiz.txt', 'a+') as write_file:\n    write_file.write(\"A whole new line\\n\")\n    print(write_file.read())\n</syntaxhighlight>\n\nCan you see the printed line? If not, then you\u2019re correct!\nThere is no error, but the \u201cprint()\u201d also does not output anything.\n\nTry it again with the r+ mode:\n\n<syntaxhighlight lang=\"Python\" line>\n# quiz r+ mode\nwith open('Quiz.txt', 'r+') as write_file:\n    write_file.write(\"A whole new line\\n\")\n    print(write_file.read())\n</syntaxhighlight>\n\nCan you now see the printed line?\nHow could you find out if you have written anything in the a+ mode?\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "inwaxxanme7jrj9ttimzb9421frlet9"
                }
            },
            {
                "title": "Regression, Correlation, and Ordinary Least Squares Estimator in Python",
                "ns": "0",
                "id": "1120",
                "revision": {
                    "id": "7226",
                    "parentid": "7225",
                    "timestamp": "2023-06-21T05:35:26Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article provides an introduction to correlation, regression, and the ordinary least square estimator.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15500",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==Introduction==\nRegression analyses and assessment of correlation between different variables are common approaches when working with data in many different fields. Albeit it does not provide detailed insights into the mechanisms underlying the relationship of the variables investigated, it is a good first analysis to get an overview of what is going on. Correlation describes a linear relationship between at least two variables. It ranges between -1 and 1, which are called perfect positive and perfect negative correlation respectively. For example, a correlation between income (in Euros) and education (in years of schooling) of 0.5 indicates that there is a positive linear relationship between an increase in education and income. The most common approach to assessing corelation is via regression analysis. In regression analysis, a relationship between one dependent variable and at least on independent variable is assessed. With the help of the regression analysis, the effect of a change in the independent variable(s) by one unit on the dependent variable is modelled. This model therefore tries to find the linear curve that is best representing the actual relationship between the dependent and the independent variable(s). Returning to the example of income and education, we can set income to be the dependent variable and education as the independent variable. We can also add other variables affecting income, such as average wages in the country of the workplace (GDP), or relevant working experiences (in months of experience). With the regression analysis, we can then assess the change in income with a one unit increase of education and working experience, and an increase in GDP when having an international dataset. One of the most used estimators for assessing correlation is the ordinary least squares estimator (OLS). The OLS aims to produce a linear curve whose values minimize the sum of the squared values of the dependent variable and the predicted values from the regression equation. The OLS estimator relies on a number of assumptions that cannot be discussed in detail, but you can find them [https://towardsdatascience.com/assumptions-in-ols-regression-why-do-they-matter-9501c800787dXX here]. In the following, we will have a look at a record of computer science exercises results from students. We will prepare the data, do some analytic checks, and finally apply the OLS estimator.\n\n==Ordinary Least Squares Estimator==\nThe following parameters are used:\n\nid = final digit of the student matriculation number \ngroup = name of the learning group \nsex = gender in binary categories (m = male, f = female) \nquanti = number of solved exercises \npoints = total score achieved from the exercises exam = total score achieved from the final written exam (Students must have to participate in the final written exam. If not, they will be considered as fail) \npassed = dummy whether the person has passed the class or not\n\nFirstly, the aim for analyzing the dataset is to figure out the performance scored among the learning groups and gender for the solved questions, exercises and written exams.\n\nSecondly, we want to figure out the correlation between variables and most importantly to figure out heteroscedastic and homoscedastic dispersion, since the OLS is only apt when homoscedasticity is the case.\n\nTo analyse this data, some python libraries have to be imported:\n\n<syntaxhighlight lang=\"Python\" line>\nimport pandas as pd ## to organize data\nimport numpy as np ## to make mathematical computations\nimport statsmodels.api as sm ## needed for statistial anaysis\nimport statsmodels.formula.api as smf ## needed to write more complex statistical models\nimport matplotlib.pyplot as plt ## to make plots\nimport seaborn as sns ## for visualization\nfrom statsmodels.formula.api import ols ## to use OLS\nfrom sklearn import preprocessing ## needed for regression analysis\nfrom statsmodels.stats.diagnostic import het_breuschpagan ## for checking your multiple regression model\n</syntaxhighlight>\n\n===Data Inspection===\nData inspection is made to check if the data is clean for better graphical performance. The cleaning involves checking for missing values and the data format. The dataset is randomly generated based on a few cornerstone observations from an actual data science class. You can find the data [https://sync.academiccloud.de/index.php/s/1vHNoSbbGaoi2AO here].\n\n<syntaxhighlight lang=\"Python\" line>\ndata = pd.read_csv('final.csv') \n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.head(16)## looks at the first 16 rows\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.tail(16)## looks at the last 16 rows\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.isnull().sum()## checks if there is missing data \n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nbool_series = pd.isnull(data['exam'])## Assesses if the exam value takes 0 or not in Boolean terms (exam is 0 = TRUE, exam not 0 = FALSE).\n(data[bool_series])#Chooses only the rows where the exam is 0 (NaN)\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nlen(data[bool_series])#applying the bool series by checking the null value\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata = data.dropna()\ndata #now, only the rows without the Nas are part of the dataset\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndata.shape# you can see here the number of rows (72) and columns (7)\n</syntaxhighlight>\nResult: (72,7)\n\n<syntaxhighlight lang=\"Python\" line>\nduplicates= data.duplicated()## checks for duplicates which would distort the results\nprint(duplicates)\n</syntaxhighlight>\nThere is  no duplicated data so we can move on analyising the dataset.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.info()\n</syntaxhighlight>\nHere, you can get an overview over the different datatypes, if there are missing values (\"non-null\") and how many entries each column has. Our columns where we wish to make a quantitative analysis should be int64 (without decimal components) or float64 (with decimal components), but id can be ignored since it is only an individual signifier. The variables with the \"object\" data format can contain several data formats. However, looking at the three variables, we can see that they are in a categorical data format, having the categories for gender, whether one passed an exam or not, and the category of the different study groups. These categorical variables cannot be used for a regression analysis as the other variables since we cannot analyze an increase in one unit of this variables and its effect on the dependent variable. Instead, we can see how belonging to one of the categories affects the independent variable. For \"sex\" and \"passed\", these are dummy variables. Columns we will treat as dummy variables can take categories in binary format. We can then for example see if and how being female impacts the dependent variable.\n\n<syntaxhighlight lang=\"Python\" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=\"Python\" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.\n\nIf heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"sex\") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= \"sex\")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).\n\nLooking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"group\", diag_kind=\"hist\")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=\"Python\" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap='PuBu');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]\n\nThe results of the heatmap are quite surprising. There is no positive correlation between any activity prior to the exam and the exam points scored. In fact, a negative correlation between quanti and exam of -0.21 is considerably large. If this is confirmed in the OLS, one explanation could be that the students lacked time to study for the exam because of the number of exercises solved. The only positive, albeit not too strong correlation can be found between points and quanti. This positive relationship seems intuitive considering that with an increased number of exercises solved, the total of points that can be achieved increases and the students will generally have more total points.\n\n==OLS==\nNow, we will have a look at different OLS approaches. We will test for heteroscedasticity formally in each model with the Breusch-Pagan test.\n\n<syntaxhighlight lang=\"Python\" line>\nmodel_1 = smf.ols(formula='points ~ ID + quanti', data=data) ## defines the first model with points being the dependent and id and quanti being the independendet variable\nresult_bp1 = model_1.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp1_test = het_breuschpagan(result_bp1.resid, result_bp1.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp1_test[1])\n\n## If the p value is smaller then the set limit (e.g., 0.05, we need to reject the assumption of homoscedasticity and assume heteroscedasticity).\n\nresult = model_1.fit() ## estimates the regression\n\nprint(result.summary()) ## print the result\n</syntaxhighlight>\n\n[[File:new_attempt.png]]\n\nLooking at the Breusch-Pagan test, we can see that we cannot reject the assumption of homoscedasticity. \nConsidering the correlation coefficients, no statistically significant relationship can be identified. The positive relationship between quanti and points can be found again, but it is not statistically significant.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_2 = smf.ols(formula='points ~ ID + sex', data=data)\nresult_bp2 = model_2.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp2.resid, result_bp2.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp2_test[1])\n\n# Set the value for maxlags\nmaxlags = 25  # Update this value as needed\n\nresult = model_2.fit(cov_type='HAC', cov_kwds={'maxlags': maxlags})\nprint(result.summary())\n\n# Assuming 'sex' is a column in the DataFrame named 'data'\nsex_counts = data['sex'].value_counts()\n\n# Print the frequency table\nprint(sex_counts)\n</syntaxhighlight>\n\n[[File:OLS 2.png]]\n\nBased on the Breusch-Pagan test, the assumption of homoscedasticity needs to be rejected to the 0.1% significance level. Therefore, we correct for heteroscedasticity with HAC (for more details see here)\nLooking at the results, being female (\"sex\") has a large negative effect on points and is highly statistically significant. However, looking at the number of females in the dataset, we need to be very cautious to draw any conclusions. Since there are only four females in the dataset (and 73 males), the sample size is considered too small to make any statements about gendered effects on total points achieved. The correlation between ID and points can be ignored, since the last number of the matricle numbers follow no pattern.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nmodel_3 = smf.ols(formula='points ~ ID + exam', data=data)\nresult_bp2 = model_3.fit()\n\n# Checking for heteroscedasticity using the Breusch-Pagan test\nbp2_test = het_breuschpagan(result_bp3.resid, result_bp3.model.exog)\n\n# Print the Breusch-Pagan test p-value\nprint(\"Breusch-Pagan test p-value:\", bp3_test[1])\n\nresult = model_3.fit()\nprint(result.summary())\n</syntaxhighlight>\n\n[[File:OLS 3.png]]\n\nBased on the Breusch-Pagan test, the asssumption of homoscedasticity cannot be rejected to the 0.1% significance level. \nLooking at the results, no statistically significant result can be found, albeit the slight neative relationship between exam and points can also be found in the OLS. \n\n==Conclusion==\nThere are many steps needed, before a regression analysis can be conducted. First, the data needs to be prepared, for example the Nas need to be removed. Then, the data needs to be assessed visually, to gain an understanding of potential homo- or heteroscedasticity. Finally, after testing for heteroscedasticity, the regression can be run, albeit the results always need to be considered in light of the available data, such as with the case for the effect of gender on total points achieved.\n\n==References==\n1.Iqbal H.(2020),Linear Regression Analysis, Source:https://www.kaggle.com/datasets/iqbalrony/linear-regression-analysis , Accessed: 20.12.2022\n\n2.Dheeraja V.(2022),Heteroskedasticity,Source :https://www.wallstreetmojo.com/heteroskedasticity/ ,Accessed:20.12.2022\n\n3.Econometrics(2022),Introduction to Econometrics with R ,Source :https://www.econometrics-with-r.org/5-4-hah.html , Accessed view:24.12.2022\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart."
                    },
                    "sha1": "h1zodaec8atjocylm584ybt39hy7yi3"
                }
            },
            {
                "title": "Regression Analysis",
                "ns": "0",
                "id": "715",
                "revision": {
                    "id": "6601",
                    "parentid": "6231",
                    "timestamp": "2022-03-21T09:49:41Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "28979",
                        "#text": "[[File:ConceptRegression.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Regressions]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Regression analysis tests whether a relation between two continuous variables is positive or negative, how strong the relation is, and whether the relation is significantly different from chance.\n<br/>\n\n'''Note:''' This entry revolves around simple linear regressions and the fundamentals of regression analysis. For more info, please refer to the entries on [[Causality and correlation]] as well as [[Generalized Linear Models]].\n\n__TOC__\n\n== Background ==\n[[File:Regression Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Regression Analysis until 2020.''' Search terms: 'Regression' in Title, Abstract, Keywords. Source: own.]]\nThe question whether two continuous variables are linked initially emerged with the [[History of Methods|rise of data from astronomical observation]]. Initial theoretical foundations were laid by Gauss and Legendre, yet many relevant developments also happened much later. '''At their core, the basics of regressions revolved around the importance of the [[Data_distribution#The_normal_distributionnormal_distribution|normal distribution]].''' While Yule and Pearson were more rigid in recognising the foundational importance of the normal distribution of the data, Fisher argued that only the response variable needs to follow this distribution. This highlights yet another feud between the two early key innovators in statistics - Fisher and Pearson - who seem to be only able to agree to disagree on each other. The regression analysis was rooted famously in an observation by Galton called ''regression towards the mean'' - which proclaims that within most statistical samples, an outlier point is more likely than not followed by a data point that is closer to the mean. This proves to be a natural law for many dynamics that can be observed, underlining the foundational importance of the normal distribution, and how it translates into our understanding of [[Glossary|patterns]] in the world. \n\nRegressions rose to worldwide recognition through econometrics, which used the increasing wealth of data from nation states and other systems to find relations within market dynamics and others patterns associated to economics. Equally, the regression was increasingly applied in medicine, engineering and many other fields of science. The 20th century became a time ruled by numbers, and the regression was one of its most important methods. '''Today, it is commonplace in all branches of science that utilise quantitative data to analyze data through regressions''', including economics, social science, ecology, engineering, medicine, psychology, and many other branches of science. Almost all statistical software packages allow for the analysis of regressions, the most common software solutions are R, SPSS, Matlab and Python. Thanks to the computer revolutions, most regressions are easy and fast in their computation, and with the rising availability of more and more data, the regression became the most abundantly used simple statistical model that exists to date.\n\n\n== What the method does ==\n[[File:Linear Regression Example.png|450px|thumb|right|'''An exemplary linear regression. It shows the distribution of the data points alongside two axes, and the regression line.''' Source: [https://en.wikipedia.org/wiki/Regression_analysis#/media/File:Linear_regression.svg Sewaqu, Wikipedia]]]\nRegressions statistically test the dependence of one continuous variable with another continuous variable. Building on a calculation that resolves around ''least squares'', regression analysis can test whether a relation between to continuous variables is positive or negative, how strong the relation is, and whether the relation is significantly different from chance, i.e. following a non-random pattern. This is an important difference to [[Correlations]], which only revolve around the relation between variables without assuming - or testing for - a causal link. Thus, identifying regressions can help us infer predictions about future developments of a relation.\n\n'''Within a regression analysis, a dependent variable is explained by an independent variable, both of which are [[Data_formats#Continuous_data|continuous]].''' At the heart of any regression analysis is the optimisation of a regression line that minimises the distance of the line to all individual points. In other words, the ''least square'' calculation maximizes the way how the regression line can integrate the sum of the squares of all individual data points to the regression line. The line can thus indicate a negative or positive relation by a negative or positive estimate, which is the value that indicates how much the y value increases if the x value increases. \n\n'''The sum of the squares of the distance of all points to the regression line allows to calculate an r squared value.''' It indicates how strong the relation between the x and the y variable is. This value can range from 0 to 1, with 0 indicating no relation at all, and 1 indicating a perfect relation. There are many diverse suggestions of what constitutes a strong or a weak regression, and this depends strongly on the context. \n\nLastly, the non-randomness of the relation is indicated by the p-value, which shows whether the relation of the two continuous variables is random or not. If the p-value is below 0,05 (typically), we call the relation significant. If there is a significant relation between the dependent and the independent variable, then new additional data is supposed to follow the same relation (see 'Prediction' bevlow). There are diverse ideas whether the two variables should follow a normal distribution, but it is commonly assumed that if the residuals - which is the deviation of the data points from a perfect relation - should follow a normal distribution. In other words, the error that is revealed through your understanding of the observed pattern follows a statistical normal distribution. Any [[Data_distribution#Non-normal_distributions|non-normally distributed pattern]] might reveal flaws in sampling, a lack of additional variables, confounding factors, or other profound problems that limit the value of your analysis. \n\n\n==== Probability  ====\n'''[[A matter of probability|Probability]] is one of the most important concepts in modern statistics.''' The question whether a relation between two variables is purely by chance, or following a pattern with a certain probability, is the basis of all probability statistics (surprise!). In the case of linear relations, another quantification is of central relevance: the question how much variance is explained by the model. These two things - the amount of variance explained by a linear model, and the fact that two variables are not randomly related - are related at least to some amount. '''If a model is highly significant, it typically shows a high r squared value.''' If a model is marginally significant, then the r squared value is typically low. \n\nThis relation is however also influenced by the sample size. Linear models and the related p-value describing the model are highly sensitive to sample size. You need at least a handful of points to get a significant relation, even if the r squared value in this model with a small sample size may be already high. Therefore, the relation between sample size, the r squared value and the p-value is central to understand how meaningful a model is.\n\n\n==== Prediction ====\n'''Regression analysis may allow for predictions based on available data.''' Predictions are a common element of statistics. In general, prediction means that we are able to foresee data beyond the range of our available data, based on the statistical power of our model which we developed based on the available data. In other words, we have enough confidence in our initial data analysis so that we can try to predict what might happen under other circumstances. Most prominently, people predict data in the future, or in other places, which is called ''extrapolation''. By comparison, ''interpolation'' allows us to predict within the range of our data, spanning over gaps in the data. Hence while interpolation allows us to predict within our data, extrapolation allows prediction beyond our dataset.\n \n'''Care is necessary when considering interpolation or extrapolation, as validity decreases when we dwell beyond the data we have.''' It takes experience to know when prediction is possible, and when it is dangerous. \n# We need to consider if our theoretical [[Glossary|assumptions]] can be reasonable expanded beyond the data of our sample. \n# Our statistical model may be less applicable outside or our data range. \n# Mitigating or interacting factors may become more relevant in the space outside of our sample range.\n\n'''We always need to consider these potential flaws or sources of error that may overall reduce the validity of our model when we use it for interpolation or extrapolation.''' As soon as we gather outside of the data space we sampled, we take the risk to produce invalid predictions. The \"[https://en.wikipedia.org/wiki/The_Missing_Shade_of_Blue missing shade of blue]\" problem from Hume exemplifies the ambiguities that can be associated with interpolation already, and extrapolation would be seen as worse by many, as we go beyond our data sample space. \n\nA common example of extrapolation would be a mechanistic model of climate change, where based on the trend in CO2 rates on Mauna Loa over the last decades we predict future trends. A prominent example of interpolation is the Worldclim dataset, which generates a global climate dataset based on advanced interpolation. Based on ten thousands of climate stations and millions of records this dataset provides knowledge about the average temperature and precipitation of the whole terrestrial globe. The data has been used in thousands of scientific publications and is a good example of how open source data substantially enabled a new scientific arena, namely Macroecology.\n\nRegressions, in comparison, are rather simple models which may still allow us to predict data. If our regression shows a significant relation between two variables, and is able to explain a major part of the variance, we can use the regression for extra- or interpolation - while respecting the limitations mentioned above.\n\n\n== Strengths & Challenges ==\n==== Causality ====\nThe main strength of regressions is equally their main weakness: Regression analysis examines the dependence of one continuous variable on another continuous variable. '''Consequently, this may be either describing a causal pattern or not.''' The [[Causality and correlation|question of causality]] in regression can be extremely helpful if examined with care. More information on causality can be found [[Causality|here.]] The main points to consider are the criteria of Hume: If A causes B, then A has a characteristic that leads to B. If C does not lead to B, then C has a characteristic that differs from A. Also, causality is build on temporal sequence: only if A happens before B, then A can lead to B. All this is relevant for regressions, as the dependence between variables may be interpreted as causality. \n\nThe strongest point of regression models may be that we can test hypotheses, yet this poses also a great danger, because regressions can be applied both inductively and deductively. A safe rooting in theory seems necessary in order to test for meaningful relationships between to variables. \n\nRegression analysis builds on p-values, and many people utilising regression analysis are next to obsessed with high r squared values. However, the calculation of p-values can be problematic due to statistical fishing: '''the more models are calculated, the higher the chance to find something significant'''. Equally difficult is the orientation along r squared values. There is no universally agreed threshold that differentiates a good from a bad model, instead the context matters. As usual, different conventions and norms in the diverse branches of science create misunderstandings and tensions\n\n\n==== Data distribution ====\nRegressions strongly resolve around the normal distribution. While many phenomena that can be measured meet this criteria, the regression fails with much of the available datasets that consist of [[Data_formats#Discrete_data|count (or 'discrete') data]]. Other distributions are incorporated into more advanced methods of analysis, such as [[Generalized Linear Models|generalised linear models]], and we have to acknowledge that the regression works robustly for normally distributed data, but only for these datasets where this criteria is met. \n \nTaken together, it can be concluded that the regression is one of the most simple and robust model available in statistics, yet the world moved on and spawned more complex models that are better able to take more diverse assumptions into account. '''The regression is still suitable for the original case for which it was proposed''', testing the dependence between two continuous variables, and building on an understanding of such derived patterns that widely resolves around the normal distribution. \n\n\n== Normativity ==\nThe main problem for regression analysis is that these models are applied in cases that do not meet the necessary assumptions. Many problems that were derived from regressions models surfaced because users ignored the original assumptions resolving around normal distributions, or tried to press patterns into a linear line of thinking that does not meet the reality of the data. '''Regressions can be seen as a main tool that led to the failure of positivism, because regressions always analyse a snapshot in time, and dynamics of data may change.''' The financial crisis is a good example, where on one day all patterns deviated from the previous pattern that were analysed by most stock brokers, thereby triggering a cascade of effects and patterns that can be predicted by regression models, but only at a dramatically different time horizon. While before, market shifts operated on a scale of months, shifts in the market suddenly came to be in a matter of seconds. Both patterns can be explained by regressions, but the temporal horizon of the analysis makes them almost incomparable. Such effects can be described as phase shifts, and the standard regression is not able to meaningfully combine such changes into one model. \n\nAn equally challenging situation can be diagnosed for spatial scales. Econometrics can be able to generate predictions about market shifts at a global scale, but less can be said how an individual business operating at a much smaller scale will be affected. '''Regression model can be operationalised at one spatial scale, but these models cannot be easily upscaled to a different scale.''' If I measure the growth of biomass at a plant scale, it would be hard to upscale any assumptions based on these measurements and models of individual plants to a global scale. If I did, I would assume that the conditions at the specific spots where I measure are the same across the globe - which is rarely the case. Regressions are thus very robust at the spatial scale at which they are operationalised, but often can say very little beyond that scale.\n\nWhat is more, regressions are typically showing you a certain linear dependence that has an underlying variance. Especially in the public debate and its understanding of statistics, this is often overseen. Very often, people assume that a relation revealed by a regression is a very tamed relation. Instead, most regressions show a high variance, making predictions of future data vague. Many medical breakthroughs discuss matters that are based on models that explain less than half of the variance in the available data. In other words, many data points do not strongly follow this pattern, but instead show a high deviance - which we cannot explain. '''This creates a strong notion that statistics, or even statisticians, are lying'''. While this is certainly an overly bold conclusion, we can indeed pinpoint many examples where scientists discuss patterns as if they were very precise, although they are not. \n\nThis is one of the main fallacies of positivistic science. Not only are their fundamental assumptions about the objectivity of knowledge wrong, but positivists often fail to highlight the limitations of their knowledge. In the case of regressions, this would be the variance, and the less-than-perfect sum of squares that can highlight how much the model explains. This failure to highlight the limitations of a model is nowhere more drastic than in the complex statistics that are behind the word 'significance'. '''A significant relation is a non-random relation, indicating a pattern that cannot be associated to chance'''. More often than not, however, this says very little about the strength of the relation. On the contrary, for almost all but a small number cases, regressions that are significant not necessarily respresent strong relations. Instead, many significant relations can be quite weak, with an explained r squared of 0.13 already being significant in large case examples. In other words, the vast majority of the relation is unexplained. This would be perfectly alright if we understood it in this way, and indicate the abilities of the model. With the rise of soft science and its utilisation of statistics, however, this limitation of statistical analysis is often overseen when it comes to the public debate of statistical relations, and their relevance for policy decisions. \n\nYet there are also direct limitations or reasons for concern within science itself. Regressions were initially derived to test for dependence between two continuous variables. In other words, a regression is at its heart a mostly deductive approach. This has been slowly eroded over the last decades, and these days many people conduct regression analysis that do not test for statistical fishing, and end up being just that. If we test long enough for significant relations, there will eventually be one, somewhere in the data. It is however tricky to draw a line, and many of the problems of the reproducibility crisis and other shortcomings of modern sincere can be associated to the blur if not wrong usage of statistical models. \n\nThe last problem of regression analysis is the diversity of disciplinary norms and conventions when it comes to the reduction of complex models. '''Many regressions are multiple regressions, where the dependent variable is explained by many predictors (= independent variables).''' The interplay and single value of several predictors merits a model reduction approach, or alternatively a clear procedure in terms of model constructions. Different disciplines, but also smaller branches of sciences, differ vastly when it comes to these diversities, making the identification of the most parsimonious approach currently a challenge. \n\n== Simple linear regression in R ==\n\nAs mentioned before, R can be a powerful tool for visualising and analysing regressions. In this section we will look at a simple linear regression using \"forbes\" dataset.\n\nFirst of all we need to load all the packages. In this example, we will use the forbes dataset from the library MASS.\n\n<syntaxhighlight lang=\"R\" line>\ninstall.packages(\"MASS\", repos = \"ttp://cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(MASS)\n\n#Let's look closer at the dataset \n?forbes\n</syntaxhighlight>\n\nWe can see that it is a dataframe with 17 observations corresponding to observed boiling point and corrected barometric pressure in the Alps. Let's arrange the dataset and convert it into a tibble in order to make it easier to analyze. It allows us to manipulate the dataset quickly (because the variables type is directly displayed).\n\n<syntaxhighlight lang=\"R\" line>\nforbes_df <- forbes   # We just rename the data\nforbes_tib <- as_tibble(forbes_df) # Convert the dataframe into a tibble\nhead(forbes_tib)     # Shows first 6 rows of dataset\n\n#Output:\n## # A tibble: 6 x 2\n##      bp  pres\n##   <dbl> <dbl>\n## 1  194.  20.8\n## 2  194.  20.8\n## 3  198.  22.4\n## 4  198.  22.7\n## 5  199.  23.2\n## 6  200.  23.4\n\nstr(forbes_tib)     # Structure of Prestige dataset\n\n#Output:\n## Classes 'tbl_df', 'tbl' and 'data.frame':    17 obs. of  2 variables:\n##  $ bp  : num  194 194 198 198 199 ...\n##  $ pres: num  20.8 20.8 22.4 22.7 23.1 ...\n</syntaxhighlight>\n\nIt is important to be sure that there is no missing value (\"NA\") to apply the linear regression. In case of the forbes dataset, which is basically small, we can see that there is no NA.\n\n<syntaxhighlight lang=\"R\" line>\nsummary(forbes_tib)  # Summarize the data of forbes\n\n#Output:\n##        bp             pres      \n##  Min.   :194.3   Min.   :20.79  \n##  1st Qu.:199.4   1st Qu.:23.15  \n##  Median :201.3   Median :24.01  \n##  Mean   :203.0   Mean   :25.06  \n##  3rd Qu.:208.6   3rd Qu.:27.76  \n##  Max.   :212.2   Max.   :30.06\n</syntaxhighlight>\n\nIn order to make it easier to understand for us, we are going to convert the two variables:\n1. the boiling temperature of the water (From Farenheit to Celcius). We will use the formula to convert a temperature from Farenheit to Celcius: C = 5/9 x (F-32)\n\n<syntaxhighlight lang=\"R\" line>\nrequire(MASS)\nrequire(dplyr)\nFA <- forbes_tib %>%  # We define a table F that stands for the F in the above formula \n  dplyr::select(bp)  # and containing all the information concerning Temperatures\n\nTempCel <- ((5/9) * (FA-32))\nTempCel\n\n#Output:\n##           bp\n## 1   90.27778\n## 2   90.16667\n## 3   92.16667\n## 4   92.44444\n## 5   93.00000\n## 6   93.27778\n## 7   93.83333\n## 8   93.94444\n## 9   94.11111\n## 10  94.05556\n## 11  95.33333\n## 12  95.88889\n## 13  98.61111\n## 14  98.11111\n## 15  99.27778\n## 16  99.94444\n## 17 100.11111\n</syntaxhighlight>\n\n2. the barometric pressure (From inches of mercury to hPa). We will use the following formula to convert inches of mercury: hPa = Pressure (inHg) x 33.86389\n\n<syntaxhighlight lang=\"R\" line>\nrequire(MASS)\nrequire(dplyr)\nPress1 <- forbes_tib %>%\n  dplyr::select(pres)\n\n\nPressureHpa <- Press1 * 33.86389\nPressureHpa\n\n## Output:\n##         pres\n## 1   704.0303\n## 2   704.0303\n## 3   758.5511\n## 4   767.6944\n## 5   783.9491\n## 6   790.7218\n## 7   809.0083\n## 8   812.3947\n## 9   813.4106\n## 10  813.0720\n## 11  851.3382\n## 12  899.7636\n## 13  964.7822\n## 14  940.0616\n## 15  983.4074\n## 16 1011.8530\n## 17 1017.9485\n\n</syntaxhighlight>\n\nLet's save a new dataframe with the converted values. We will use a Scatter Plot to visualise if there is a relationship between the variables (so we can apply the linear regression). Scatter Plots can help visualise linear relationships between the response and predictor variables. The purpose here is to build an equation for pressure as a function of temperature: to predict pressure when only the temperature (boiling point) is known.\n[[File:BoilPointWaterP.png|350px|thumb|right|Fig.1]]\n\n<syntaxhighlight lang=\"R\" line>\n# Saving and viewing the new dataframe\nBoilingPoint <- data.frame(TempCe=TempCel,PressureHpa=PressureHpa)\nView(BoilingPoint)\n\n# Visualising\n# Fig.1\nggplot(BoilingPoint, aes(x = bp , y =  pres)) +  geom_point() + \nxlab (\"Temp in C\u00e6\u0088\u00bc\u00e3\u00b8\u00b0\") + ylab(\"Pressure (in hPa)\") +\n  ggtitle (\"Boiling Point of Water and Pressure\")\n</syntaxhighlight>\n\nThe scatter Plot suggest here a linear relationship between the temperature and the pressure.In this case, Pressure is the dependant variable ('target variable').\n\n<syntaxhighlight lang=\"R\" line>\ncor(BoilingPoint$bp,BoilingPoint$pres)\n\n## Output:\n## [1] 0.9972102\n</syntaxhighlight>\n\nWe compute the correlation coefficient in order to see the degree of linear dependence between temp and pres. The value of 0.9972102 is close to 1 so they have a strong positive correlation. To remember: Correlation can only take values between -1 and + 1.\n\n[[File:BoilPointWaterP2.png|350px|thumb|right|Fig.2]]\n<syntaxhighlight lang=\"R\" line>\n# Fig.2\nggplot(BoilingPoint, aes(x = bp , y =  pres)) +  geom_point() + \nxlab (\"Temp in C\u00b0\") + ylab(\"Pressure (in hPa)\") +\nggtitle (\"Boiling Point of Water and Pressure\") + geom_smooth(method=\"lm\")\n\n#Building linear model\nPressTempModel <- lm(pres ~ bp, data = BoilingPoint)\nprint(PressTempModel)\n\n## Output\n## \n## Call:\n## lm(formula = pres ~ bp, data = BoilingPoint)\n## \n## Coefficients:\n## (Intercept)           bp  \n##    -2178.50        31.87\n</syntaxhighlight>\n\nWe successfully established the linear regression model. It means that we built the relationship between the predictor (Temp) and response variables (Pressure) <- taking form of a formula: \n* Pressure = - 2178.5 + (31.87*temperature)\n* Pressure = Intercept + (Beta coefficient of temperature*temperature)\nIt will allow us to predict pressure values with temperature values.\n\n<syntaxhighlight lang=\"R\" line>\nsummary(PressTempModel)\n\n## Output:\n##\n## Call:\n## lm(formula = pres ~ bp, data = BoilingPoint)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.709 -3.808 -1.728  4.837 22.010 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -2178.504     58.536  -37.22 3.41e-16 ***\n## bp             31.873      0.616   51.74  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.885 on 15 degrees of freedom\n## Multiple R-squared:  0.9944, Adjusted R-squared:  0.9941 \n## F-statistic:  2677 on 1 and 15 DF,  p-value: < 2.2e-16\n</syntaxhighlight>\n\n* \"P Value: < 2.2e-16\" \u2013 effectively close to zero meaning we reject the null hypothesis (co-efficient of the predictor is zero), i.e. the model is statistically significant. Our p-value is < than the statistical significance level of 0.05. 0.05 can be compared as a threshold.\n* Residuals here can be considered as the distance from the data points to the line.\n* In our case, the t-value (in absolute value) is high, meaning our p-value will be small.\n* Multiple R-squared (0.9944) and adjusted R-squared (0.9941) show how well the model fits our data. We use R-Squared to measure how close each of our datapoints fits to the regression line. This measure is always between 0 and 1 (or 0% and 100%). We can say that the larger the R\u00b2, the better the model fits your observations. In our case, the R\u00b2 value is > 0.99, meaning our model fits our observations very well.\n\n== Outlook ==\nRegression models are rather limited in their assumptions, building on the normal distribution, and being unable to implement more complex design features such as random intercepts or random factors. While regressions surely serve as the main basis of frequentist statistics, they are mostly a basis for more advanced models these days, and seem almost outdated compared to the [[An initial path towards statistical analysis|wider available canon of statistics]]. To this end, regressions can be seen as a testimony that the reign of positivism needs to come to an end. '''Regressions can be powerful and robust, but regressions are equally static and simplistic.''' Without a critical perspective and a clear recognition of their limitations, we may exceed the value of regressions beyond their capabilities. \n\nThe question of model reduction will preoccupy statistics for the next decades, and this development will be interacting with a further rise of Bayes theorem and other questions related to information processing. Time will tell how regressions will emerge on the other side, yet it is undeniable that there is a use case for this specific type of statistical model. Whether science will become better in terms of the theoretical foundations of regressions, in recognising and communicating the restrictions and flaws of regressions, and not overplaying their hand when it comes to the creation of knowledge, is an altogether different story. \n\n\n== Key Publications ==\n\n== References ==\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|authors]] of this entry are Henrik von Wehrden and Quentin Lehrer."
                    },
                    "sha1": "qq3dgvumwysso0x99ujermh81lid28a"
                }
            },
            {
                "title": "Research Diary",
                "ns": "0",
                "id": "310",
                "revision": {
                    "id": "6737",
                    "parentid": "3269",
                    "timestamp": "2022-07-28T07:27:23Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* Research diary as undiscovered country */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7572",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]'''|| [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nWhen conducting research, being reflexive and documenting your own progress can be very helpful. If you procrastinate, then this is for you. So basically, it is for everybody.  As an active researcher, it is not only important to track your progress, you also have document it, and ideally reflect about it. This can be of specific importance for early career scientists who face their first large challenge, like writing a thesis or publishing a paper.\n\n== Goals ==\nDocument your research process thoroughly, not only how you conduct research, but also how it made you feel while you were conducting it. Different people get different facets out of keeping research diary, but people highlighted in the past, that it ca\n1) Provide a basis to find your own style of documentation\n2) Helps you to reflect on your timeline, goals and progress\n3) Generates an understanding about the personal mindest in which the research was set and conducted\n4) Continuous writing improves your writing per se- hence your academic writing may also improve over time.\n\n== Getting started ==\nGetting started is the hardest point of any research diary, or better, getting started and sticking to it. You need to design a time and ideally even a place where you want to document your research and reflect about it. Bullet journaling has introduced fancy litte A5 journals into the life of many people, and such a book could be a good start. What is most important is that no setting is ideal for everybody, you have to find your own setting. Some write in the morning before the day gets started, some write at a fixed time in the office, others use the calm in the evening, and even other write whenever ad wherever they feel like it. We have to be aware that writing a research diary should be a committed goal if we decide to do it, and it needs to be a habit. Naturally, hair changes need time, thus starting small may pay off if it is nevertheless continuously. While a diary may help us to write down how we perceive reality, a research diary helps us to write down how we perceive research, and how we perceive ourself in research. \n\n===What to write in a research diary===\nThe core goal of research diary is to document your own research and how you perceive it. While for instance the late Oliver Sacks started a new book every few weeks at time, other may just document the main steps. What is relevant is that you take a step back and and look at your research like an outsider, but with insider knowledge. It take trying to uncouple oneself from the own research process, yet it is well worth it. In the following are some reasons how a research diary has proven valuable in the past. \n\n===Research diary for documentation===\nDocumentation is key in science. While a certain systematic way of documentation has established itself over the last decades, the latest data security laws have put a burden on many scientists when it comes to the proper documentation of science. Although this was done with the best of intentions, a clear documentation considering all legal necessities is hard to achieve yet of course necessary. Here, a research diary can open another level, where beside the \"objective\" documentation of what was done in due course of our research, a personal perspective prevails. Why did we make certain choices in our research, and how did we feel about it. Having the possibility to come back to this can prove valuable, since our memories are deeply constructed and often plain wrong. Coming back to your research diary may thus offer some surprises.\n\n===Research diary for reflection===\nResearch is often a very emotional process, which is not surprising. Whenever we spend a lot of time on something, we are more often than not deeply emotionally invested. To this end, it can be quite helpful to see how already are quite relaxed about parts of your research -say failure- about things that some months ago made you an emotional wreck. Looking back at the process has something liberating for many, as it shows that we may after all gain some process, if only at a forward inching scale.For other, a diary may be a good source to learn to focus. What is the main dish of your research, and what are side dishes? Research is a continuous endevour, and documenting this continuously can be indeed very insightful. \n\n===Research diary as a data source===\nGaining insight out of the own research process is also a potential data source. The question how we conduct, perceive and ultimately facilitate research. While researchers by means of hermeneutics, content analysis etc. have investigated diaries and letters of famous or influential researchers since a long time, this is now also increasingly done while research is conducted. This allows for a clearer localisation within research, and provides an embeddedness in one's own research. Greta care needs to be taken to have a clear understanding of the respective roles, yet the potential outcome can be beneficial. Normal science may not always see the benefit in this, and would highlight the need to focus. This links to the next point. \n \n===Research diary as undiscovered country===\nResearch is ever evolving, this is what defines it. We may not know now how research will be in a few decades, but the current societal development showcases the need to slow down and be more reflexive -and critical- of our actions. Many opportunities can be on the horizon for reflexive researchers, and the still widely hierarchical scientific system that defends resources and fights for priority about specific aspects of knowledge. It is hard to imagine that this will always be like this. New modes of research may become more prevalent in the future, and the currently mostly utilitarian way to conduct research may be replaced by more reflexive setting. A small research diary may be one step of each and every single one of us towards this goal. \n\nGet a book, finde the time and place to write, and design this setting in a way that maximises your ability to stick to it. Once you have done this at least 20 times, it may become a habit, or you may at least realise that it does not work right now for you. In order to know this you have to try it.\n\n== Links & Further reading ==\n* [https://www.tandfonline.com/doi/pdf/10.1080/0305764810110106?casa_token=McRYpqwWLacAAAAA:xsm6p_zpZfoF5IwtmoBSMlpfYf4gGnNYCYdeiJTjXtbN5Y7d-KP8RLTSEqx0Jkn0ECtcJZ1eXE0E Robert Burgess - Keeping a Research Diary]\n* [https://www.emerald.com/insight/content/doi/10.1108/11766090610705407/full/html?casa_token=QuMXNjOVJUcAAAAA:saptFlzBZHjmkWkYYr13lh88vihlm10a5N54Y277B4NDA9bDBOFL3QAdTVZHLvpI9-lv8eySakzFHrdwh_IHhsYYtikcjUTBwQfRWaaSJ29MqLwZtHA Nadin & Cassell - The use of a research diary as a tool for reflexive practice: Some reflections from management research]\n* [https://www.publicationcoach.com/research-diary/ Why you should consider keeping a research diary]\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "tg90xpaz5scqqbm5h2e0gf0fkotv3or"
                }
            },
            {
                "title": "Sampling for Interviews",
                "ns": "0",
                "id": "906",
                "revision": {
                    "id": "6743",
                    "parentid": "6742",
                    "timestamp": "2022-08-01T14:45:34Z",
                    "contributor": {
                        "username": "Annrau",
                        "id": "128"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "30044",
                        "#text": "'''In short:''' This entry provides tips on how to sample participants for different forms of Interviewing Methodology. It includes tips and deliberations on defining the target population, choosing the right sample size and applying different forms of sampling strategies. For more on Interviews, please refer to the [[Interviews]] overview page.\n\n== The general rationale of sampling ==\nA sample is a selection of individuals from a larger population that researchers investigate instead of investigating the whole population, which is not possible for a range of reasons. Sampling is a crucial step in research: \"[o]ther than selecting a research topic and appropriate research design, no other research task is more fundamental to creating credible research than obtaining an adequate sample.\" (Marshall et al. 2013, p.11). This is also true for Interview methdology: the creation of a good sample directly affects the validity of the research results (7). How many people you ask your questions, hand out your questionnaire to, or invite to your Focus Groups directly influences the broadth and depth of the created insights. Inviting too few people might not deliver sufficient material to answer your research questions. An overly large sample, then again, might be superfluous, i.e. increase the costs and organisational effort of your project without providing further insights. A sample of the study population that is not representative of the full breadth of possible viewpoints and experiences within the whole population will definitely influence your results, and non-representative samples can happen no matter the sample size, but are less of a problem for bigger samples than for small ones. Thus, approaching the question of sampling can generally be guided by Ockham's Razor - you need a sample that is as big as necessary, but as small as possible. However, this is often easier to say than to apply.\n\nAs a rule of thumb, the more open an interviewing approach is, the smaller the sample is going to - and needs to - be. The more structured the methodology, the bigger the required sample. This is because of the depth of the created data in qualitative approaches, which needs time to be qualitatively assessed. Quantitative surveys, by comparison, rely on responses that can be analyzed by means of statistical approaches. These do not only deal better with large samples than, say, qualitative content analysis approaches, but even require larger numbers for valid insights. Still, this general dichotomy does not directly guide an individual interview approach. So first, let us have a look at the seletion of a study population, before examining what the literature suggests for the topic of sample sizes. Last, we will have a look at different forms of sampling (7).\n\n\n== Defining the study population ==\nIn a first step, the 'sample universe' - also known as 'study population' or 'target population' - needs to be defined. To do so, the researcher develops inclusion and / or exclusion criteria to guide the selection of all individuals that are theoretically of interest to answer the research question (see Figure). The inclusion and exclusion criteria that are applied will determine the characteristics of the sample universe. This universe can thus become very homogeneous as well as very heterogeneous (see Table). These criteria are mostly guided by theory, in accordance to the research purpose, questions, and methodological design, and potentially with regards to other work that has been done before. Further, organisational elements already play a role here, like geographical access, or the level of access that the researcher has to the population. Within this population, the sample will be drawn, which will help answer the research questions.\n<br>\n[[File:Sample Universe - Robinson 2014, p.27.png|500px|thumb|center|'''Defining the Sampling Universe'''. Source: Robinson 2014, p.27]]\n<br>\nThe sampling universe can be designed to be rather homogenous, or heterogenous. As you can see in the table below, a homogenous sample may be of interest to enable research on more specific phenomena. As an example, a researcher might be interested in how individuals with a specific position in an organisational context perceive a distinct topic. Therefore, they will conduct expert interviews with such individuals, and accordingly define the sampling universe more homogenously and strict (Meuser & Nagel 1991, p.442f). By comparison, \"[t]he rationale for gaining a heterogeneous sample is that any commonality found across a diverse group of cases is more likely to be a widely generalisable phenomenon than a commonality found in a homogenous group of cases. Therefore, heterogeneity of sample helps provide evidence that findings are not solely the preserve a particular group, time or place, which can help establish whether a theory developed within one particular context applies to other contexts\" (Robinson 2014, p.27).\n<br>\n[[File:Five Types of Sample Homogeneity - Robinson 2014, p.28.png|600px|thumb|center|'''Five types of sample homogeneity based on characteristics of the participants.''' source: Robinson 2014, p.28]]\n<br>\nThe definition of the sampling universe further differs for qualitative and quantitative approaches. For qualitative interviews, the general idea is to select interviewees in a way that promises \"information-rich\" interviews (1). The inclusion and exclusion criteria should be selected accordingly, and focus on individuals that will likely provide deeper and highly relevant insights into the topic of interest. For quantitative approaches, the inclusion and exclusion criteria may be less strict. While medical surveys may be interested in anyone with a specific disease, more general sociological research might simply include everyone working at a specific company, or every individual of a certain age in a selected city, or every potential voter in a country. So overall, the exclusion and inclusion criteria that define the sampling universe depend mostly on the intended methodological approach, as well as the research questions.\n\n\n== Choosing the right sample size ==\nBefore drawing the sample from the whole population within the sampling universe, the next step is to clarify the amount of individuals that are planned to be interviewed. This differs between Interview formats. \n\n==== Qualitative Interviews ====\nQualitative Interviews rely on questions and answers that help the researcher thoroughly understand the interviewee's perspective on the respective topic in order to conclusively investigate the research questions. It is not the researcher's primary interest to be able to count the number of times a specific thing was said, and to make a tally for the according code in the analysis. Indeed, quantitative analysis approaches do exist also for qualitative data (see Content Analysis). Generally however, qualitative interviews will be assessed qualitatively, and focus on investigating and understanding what interviewees have said in order to get a broad and deep overview of all relevant aspects - opinions, experiences - that the interviewees have on the topic of interest. Instead of a large number of cases, the complexity and diligence of the interview conduction are relevant to the quality of the results (Baker & Edwards 2012, p.39). To this end, semi-structured interviews allow for larger sample sizes than open interviews, since their analysis is more structured based on the pre-determined interview guide.\n\nIn general, the most important concept for the determination of the right sample size in research is 'saturation'. However, while saturation is an often-used concept, it is ill-defined (1; 2; 3). Commonly, researchers justify their sample size by stating they have reached saturation, but rarely explain what this means. Historically, and methodologically, ('theoretical') saturation emerged from [[https://sustainabilitymethods.org/index.php/Grounded_Theory|Grounded Theory]] research and refers to iterative, qualitative research which after some times reaches the point when further increasing the size of the sample will not provide any more insights that could further amend the theory that has been developed so far (Malterud et al. 2016, p. 1758). This means that the people you have asked so far have told you everything of relevance which the whole population that you could possible ask will be able to tell you. When you have reached saturation in your results, your sample - your ''N'' has been big enough (2). It is time to wrap up the Interview results; they likely cover all relevant facets. The expected point of saturation guides the planned sample size before starting the study, while conducting it, and when discussing the validity of the results (2). \n\nSaturation as a guiding principle also relates to the quality of the selected interviewees (1). You can find more information on sampling strategies in the paragraph below. For now, however, we will focus on the conflict that emerges from the fundamental idea of saturation: you can only know if you have reached a saturation in your data by analyzing the data as you get it, so that you could stop conducting interviews as soon as the results altogether lack new insights (Baker & Edwards 2012, p. 18). However, analyzing the data in-between interviewing more individuals is a logistical challenge, since analysis takes time and should not be done in passing. Also, researchers typically need to state their sample size before starting the research and not update it mid-gathering (1; 3;7). Interviewing costs money and requires resources, which need to be granted and organized beforehand and can rarely be flexibly adjusted as necessary. Further, this iterative approach can also influence your interviews. When you know how you will code the gathered qualitative interview data, and maybe even have some results already, this might consciously or unconsciously guide the way you conduct further interviews: you'd bias yourself. By comparison, a strict separation of data gathering and data analysis might reveal a lack of saturation only long after the interviewing phase, or show that you have conducted way more interviews than needed.\n\nSo, while saturation is often used and generally required, there is no easy guide to determining it. There is no general accepted N that would suffice for every qualitative interview study (3). Marshall et al. (2013) propose three approaches to determining adequate sample sizes: first, using recommendations from other researchers, which, as they show, range from 6-10 interviews for phenomenological studies, to 20-50 for [[Grounded_Theory|Grounded Theory]] studies. However, as the authors highlight, these recommended numbers are rarely explained by those other researchers. Second, adopting the sample size from comparable studies, which, again, may be quite arbitrary. Third, by statistically assessing one's own study in terms of its thematic saturation. However, very few to none of the studies they reviewed actually employed these three strategies, but instead \"(...) made superficial references to data saturation or altogether omitted justification for sample size.\" (Marshall et al. 2013, p. 20). Eventually, the authors conclude that as a rough guideline, Grounded Theory qualitative studies should use 20-30 interviews, single case studies 15-30 interviews. They also call for researchers to consider the expectations of the respective journal. \n\nHennink et al. (2017) follow the idea of statistically investigating saturation, doing so in their own line of interviews. Here, they differentiate between *code* saturation  - which are new topics emerging in the interview responses - and *meaning* saturation - which are new insights into the respective codes that provide a better understanding. They find code saturation to happen at around 9 qualitative interviews, which is similar to the findings in comparable studies, but also highlight that meaning saturation for all codes may take more interviews, or may even never be reached for all codes at all. Their findings thus '(...) underscore the need to collect more data beyond the point of identifying codes and to ask not whether you have \"heard it all\" but whether you \"understand it all\" - only then could saturation be claimed.' (Hennink et al. 2017, p.605).\n\nWhat is important to keep in mind is that the number of interviewees is just one part of the equation. You can interview as many individuals as you like, but if they are not at all representative of the diversity of the whole sampling universe, you will still not reach saturation with this approach. Therefore, the selection of individuals (see sampling strategies below) to be representative of the whole sampling universe will strongly influence how large your sample needs to be in order  to reach saturation.\n\nMalterud et al. (2016, p.1754) underscore that \"(...) sample size cannot be predicted by formulare or by perceived redundancy.\" Instead, they claim five quality criteria of the gathered data - which make up its \"information power\" - to be relevant to determine the adequate sample size. These are the aim of the study, the specificity of the sample, the extent to which theory guides the interviewing process, the quality of the dialogue data, as well as the focus of the analysis (see Figure). These criteria are presented as a frame in which researchers can iteratively assess their own research approach, and also assess other empirical studies in terms of their sample size. However, the authors underscore that their model \"(...) is not intended as a checklist to calculate N but is meant as a recommendation of what to consider systematically about recruitment at different steps of the research process.\"\n<br>\n[[File:Items and Dimensions of Information Power. Malterud et al. 2016 p. 1756..png|500px|thumb|center|'''Items and Dimensions of Information Power, and what they mean for sample sizes.''' Source: Malterud et al. 2016, p. 1756.]]\n<br>\nHennink et al .(2017), too, provide a set of criteria. In combination, these shall help reflect upon, although not necessarily entirely determine, the required sample size before data gathering. These criteria include the purpose of the study, the study population, the sampling strategy, the quality of the data, the type of codes and the quality of the codebook, as well as the intended level and type of saturation (see Figure). Overall, they advise researchers to more actively reflect upon, and more transparently communicate, their own process of reaching saturation when conducting qualitative interviews, and to spend time beforehand to consider the sample size needed for their specific study: \"Using these parameters of saturation to guide sample size estimates *a priori* for a specific study and to demonstrate within publications the grounds on which saturation was assessed or achieved will likely result in more appropriate sample sizes that reflect the purpose of a study and the goals of qualitative research.\" (Hennink et al. 2017, p.607) For more insights, please refer to Hennink et al. (2017 p. 606f).\n<br>\n[[File:Parameters of Saturation. Hennink et al. (2017 p. 606f)..png|500px|thumb|center|'''Parameters of saturation.''' Hennink et al. 2017, p. 606f]]\n<br>\nOverall, it becomes obvious that there is no simple answer to \"How many people do you need to ask?\" for qualitative Interviews. The aforementioned insights and criteria may guide the process, and orientating oneself on previous research certainly helps. However, the question of the 'right' sample size remains unsolved and is matter of ongoing discussions, which every researcher could contribute to by critically reflecting upon one's own sampling strategy.\n\n==== Focus Groups ====\nFocus Groups are not comparable to Interviews in every aspect, but also require a sample that is both informative and thematically exhaustive, without burdening the research team with too much organisational effort. Generally, many of the elements that can guide qualitative interview sampling may therefore also apply to Focus Groups.  More specifically, however, Focus Groups strongly focus on group-internal interactions and dynamics. Tang & Davis (1995, p.474) highlight that the ideal group size for Focus Groups \"(...) maximizes output of information and minimizes group dissatisfaction. It is believed that valid and rich information can be generated as a result of participants' willingess to express without inhibition.\" Because of this, the size of the group influences the data that the researcher can gather in the first place. Tang & Davis (1995) generally recommend between four and twelve members in a Focus Group, but express that bigger groups may inhibit maximized information output by every participant, and may lead to competitive or inconsiderate group dynamics. However, smaller groups might be too passive and constrained, and were found to generate fewer ideas than bigger groups. They also highlight that the nature of the study, the complexity of the topic, as well as the diversity, ability, expectations and needs of the group members play critical roles in determining the best group size. Also, organisational factors such as the format and length of the session, and the number or duration of questions play a role. Overall, they claim the biggest influencing factor to be the aim of the study: more explorational studies may need smaller groups, while bigger groups are more suited to structured approaches.\n\nIn addition to the size of each focus group, the overall number of focus groups is of relevance. Based on an literature review, Carlsen & Glenton (2011) highlight that the majority of studies held less than 10 focus groups in total. However, the authors indicate a severe lack of reflection on this number. Of their reviewed 220 studies, 183 did not justify the number of focus groups they held. Those who did referred to practical reasons or to recommendations by the available literature. Most (28 of 37) claimed to have reached saturation, half of which did not 'convincingly' describe the process which led to this assessment (Carlsen & Glenton 2011, p.5). Their results suggest that more methodological work on Focus Groups is needed to increase rigor in the determination of the adequate number of Focus Groups, which may well also be the case for the size of each group.\n\n==== Survey Research and quantitative interviews ====\nQuantitative Interview approaches, i.e. surveys based on questionnaires or structured interviews, intend to represent the target population adequately, and provide a sufficiently large sample for quantitative analyses. The sample chosen for the survey should make generalizations and statistical inferences about that population possible. Here, broader samples are more relevant than choosing individuals that promise deep insights, since surveys do not focus as much on understanding reasoning and perspectives in depth as much as qualitative interviews. \n\nWhen determining the sample for surveys,\u00a0**a bigger sample size is almost always better**. A bigger size will reduce skewness of the sample, and even out statistical errors that would influence the quantitative analysis. However, surveys also cannot cover the complete target population. You cannot sample endlessly, as organisational and financial restrictions limit the possible amount of survey respondents: you cannot hand out surveys to bypassers on the street for years, and you will never reach everyone in your target population via the Internet. Importantly, a sample that is very big may always lead to some kinds of analysis results if you search long enough. This is called statistical fishing, which is a common challenge in quantitative analyses. Therefore, the sample should not be drawn all over the place but should relate to the study objective. \n\nAlso, the adequate sample size for a survey depends on the study objective, the sampling universe and the sampling method. Importantly and contrary to popular belief, the mere size of the target population is of little or no relevance unless the target population is relatively small, i.e. only consists of a few thousand people. The composition of the sampling universe (target population) is of bigger importance. In case of simple random sampling, for example, the heterogeneity of the target population plays a role. Here, also the confidence interval you aim for, and how you want to analyze your data (e.g. analysis by subgroups, type of [[Statistics|statistical analysis]]) are highly relevant to determine the adequate sample size.\n<br>\n[[File:Sample Size Calculator - Surveymonkey.png|500px|thumb|right|'''Sample Size Calculator'''. Source: [https://www.surveymonkey.com/mp/sample-size-calculator/ SurveyMonkey]]]\nTaking this multitude of factors into account, sample size calculators on the Internet, such as\u00a0[this one](https://www.surveymonkey.com/mp/sample-size-calculator/), can help to get an idea of which sample size might be suitable for your study. Keep in mind that when determining the sample size, the estimated response rate needs to be accounted for as well, so you most likely need to invite more people for the survey than the calculated sample size indicates. For more on sampling sizes in Surveys, please refer to Gideon (2012) who dedicates a chapter on finding the right sample size.\n<br>\n\n== Sampling strategies ==\nAfter the sample universe is defined, you can approach the third element in accordance with the sample size: the sampling strategy. This refers to the way that the researcher draws the (previously) defined number of individuals from the whole sampling universe. As shown before, the sampling approach strongly influences the sample size, and is itself strongly influenced by the research intent, sampling universe, and research methdology. There are various sampling strategies, the most important of which will be presented below (based on Robinson 2014).\n\n==== Random & convenience sampling ====\nRandom sampling is a typical approach in surveys. In this approach, individuals are drawn randomly: for example, they are randomly approached with the survey or questionnaire on the street, by randomly calling telephone numbers from a list, or via e-mail newsletters online. This approach can still be systematic, e.g. by choosing individuals in a specific interval from a list. It can also take the form of cluster sampling, where specific groups within the overall population - e.g. a few schools out of all schools in a district - are randomly chosen to represent to the whole population of students (8). For surveys, this random approach is common: since the sample size can be much bigger here, the step of boiling down the study population to the actual sample is less sensitive to specific criteria. Validity in this approach emerges from numbers, so differences between sampled individuals do not matter as much (8). However, overly small samples, especially for broadly defined study populations, may make non-significant results hard to interpret. Were they not significant because there is no significant effect, or was the sample too small? Random sampling is also imaginable for qualitative research, but since the sample sizes are mostly small here, there is a big risk of omitting important perspectives. \n\nBy comparison, convenience sampling refers to asking people which are easy to access, for example people that you come across in a shopping mall. Be aware of the difference to random sampling: convenience sampling can potentially be as representative of the whole study population as any form of random sampling - or, stochastically speaking, even more so. Yet, most commonly, convenience sampling imposes a selection bias since it favors a specific group of people. In the shopping mall in the afternoon, you'll mostly meet people who are not currently working, who like to go to the Shopping Mall. Another example is the fact that psychology studies have famously often been conducted with psychology students, who are easy to access for researchers, but hardly representative of the whole country, and do not really represent a random sample, for that matter (7). Convenience sampling is also imaginable for qualitative research, but the sampling bias imposed by this strategy weighs even heavier here. For both qualitative and quantitative approaches, generalisations are hard to draw from convenience sampling-based research. If you still plan to go for convenience sampling, the gap between the sampling universe and the actual sample should not be too wide. For example, if the sample universe is defined as \"young university adults with interest in psychology\", then conveniently sampling psychology students will be less of an issue than if your sampling universe was \"every citizen below the age of 30\" (7). \n\n==== Snowball sampling ====\nBasically, snowball sampling is a form of random or convenience sampling. In snowball sampling, the researcher initially selects few individuals to interview. After the interview, the interviewee is asked to recommend further people that might be relevant to the research. This way, the researcher widens the sample with every step. Unfortunately, this approach may strengthen biases that existed with the initial interviewee selection, and another bias can be imposed by the recommendation rationale of each interviewee. Snowball sampling is possible for quantitative research, but mostly done for qualitative approaches where a specific group of individuals is of interest.\n\n==== Purposive sampling ====\nPurposive sampling is directed strongly by theoretical pre-assumptions. The researchers assume that a specific kind of individual will represent an interesting perspective on the research topic, and is therefore included in the sample. This is most common in qualitative research, and includes a range of variations:\n\n==== Stratified sampling ====\nIn stratified sampling, several groups are created, which are defined by shared characteristics of the individuals which are then assigned to these groups. For example, you create groups with people from different educational backgrounds, with different diseases, or different age groups. The categories may also overlap, like in a Venn Diagram (\"cell sampling\"). Then, a fixed number of individuals are randomly drawn from each of these groups. This approach intends to provide a representative composition of different perspectives, preconditions and experiences.\n\n==== Quota sampling ====\nQuota sampling works like stratified sampling, but instead of fixed numbers of individuals per category, there are minimum numbers of individuals, which can be exceeded. This way, the relevant groups are covered, but the researcher is more flexible in gathering individuals.\n\n==== Case Study sampling ====\nFor single case studies, various approaches are imaginable: the researcher might be interested in extreme or deviant cases, or individuals which promise to be very information-rich, or plainly very typical representations of a specific phenomenon. These are identified and sampled.\n\n\n== Concluding remarks ==\nTo summarize, there are four major parts to sampling: \n\n1) defining the sampling universe with exclusion and inclusion criteria in accordance with the intended methodology and the research question, \n\n2) defining the sample size in accordance with a demand for saturation (qualitative approaches), statistical significance (quantitative approaches), and representativeness regarding the research question,\n\n3) choosing the sampling strategy in accordance with the same criteria, and all the while\n\n4) documenting your sampling approach, so that your research is reproducible, and other researchers understand how you worked.\n\nSampling is a complex issue of Interview methodology which is not easily solved. As with many elements of scientific methodology, sampling requires experience, and there is room for learning from failed attempts. Further, we encourage the exchange on sampling approaches and critical reflections upon one's own - and others' - sampling strategies. This way, the validity of scientific claims will be improved, and the different demands of qualitative and quantitative research, as well as different forms of Interviews, will be more adequately fulfilled. If you want to read more about the normativity of Sampling, please refer to the entry on [[Bias in Interviews]].\n\n\n== References ==\n(1) Hennink, M.M. Kaiser, B.N. Marconi, V.C. 2017. ''Code Saturation Versus Meaning Saturation: How Many Interviews Are Enough?'' Qualitative Health Research 27(4). 591-608.\n\n(2) Malterud, K. Siersma, V.D. Guassora, A.D. 2016. ''Sample Size in Qualitative Interview Studies: Guided by Information Power.'' Qualitative Health Research 26(13). 1753-1760.\n\n(3) Marshall, B. Cardon, P. Poddar, A. Fontenot, R. 2013. ''DOES SAMPLE SIZE MATTER IN QUALITATIVE RESEARCH?: A REVIEW OF QUALITATIVE INTERVIEWS IN IS RESEARCH.'' Journal of Computer Information Systems\n\n(4) Tang, K.C. Davis, A. 1995. C''ritical factors in the determination of focus group size.'' Family Practice 12(4). 474-475.\n\n(5) Carlsen, B. Glenton, C. 2011. ''What about N? A methodological study of sample-size reporting in focus group studies.'' BMC Medical Research Methodology 11(26). \n\n(6) Baker, S.E., Edwards, R. 2012. ''How many qualitative interviews is enough? Expert voices and early career reflections on sampling and cases in qualitative research.'' National Centre for Research Methods Review Paper.\n\n(7) Robinson, O.C. 2014. ''Sampling in Interview-Based Qualitative Research: A Theoretical and Practical Guide.'' Qualitative Research in Psychology 11(1). 25-41.\n\n(8) Fife-Schaw, C. 2000. ''Surveys and Sampling Issues.'' Research methods in psychology 2. 88-104.\n\n(9) Gideon, L. 2012. ''Handbook of survey methodology for the social sciences.'' Springer.\n----\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.\n\n[[Category:Normativity of Methods]]"
                    },
                    "sha1": "rek0pgq4zrfm049u44su2uc98mfm8ny"
                }
            },
            {
                "title": "Sankey Diagrams",
                "ns": "0",
                "id": "935",
                "revision": {
                    "id": "6564",
                    "parentid": "6563",
                    "timestamp": "2022-03-20T20:22:36Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* R code */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4385",
                        "#text": "'''Note:''' This entry revolves specifically around Sankey diagrams. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nSankey diagrams show the flows, where a width of each flow is proportional to the quantity represented. The flows are called links. Links connect entities (called nodes) and may converge or diverge. Sankey diagrams help to understand a many-to-many mapping between two domains or multiple paths of data through a set of states. Since it is possible to see the considered entities / nodes and links between them, one can say, that there is an information about the structure of the defined system. The source node is the node where the flow originates. The target node is the node where the flow ends. The nodes are usually represented as rectangles with a label.\n\n==History==\nSankey diagram is named after Irish engineer Matthew H. Sankey, who created a diagram of steam engine efficiency, that used arrows having widths proportional to heat loss. The illustration is dated to 1898. In the 20th century the Austrian mechanical engineer Alois Riedler began to apply flow charts to analyze the power and the energy losses of passenger cars. Also some government departments used it for financial goals, focusing on material and energy efficiency in the beginning of the 20th century.\n\n==Why do we use it?==\nThere are many ways to use Sankey diagram. It can show data, energy, capacity, materials, costs, social and biological data (population, migration) and so on. Spheres, such as energy, facility, supply chain management, business, marketing analysis, apply these diagrams constantly. The use case examples can be found [https://www.sankey-diagrams.com/ here].\n\nSankey diagram can be perceived intuitively. There is no standard notation of how the diagram should look, therefore, diverse options exist. The viewer may pay attention on the largest flow width, linked entities or notice the losses of the definite process.\n\n==R code==\nA package <syntaxhighlight lang=\"R\" inline>networkD3</syntaxhighlight> has tools to build Sankey diagrams in R. To draw the plot, <syntaxhighlight lang=\"R\" inline>dplyr</syntaxhighlight>dplyr is also needed. Install packages, and then follow the code.\n\n[[File:sankey.png|450px|thumb|right|Fig.1: An example of a Sankey diagram.]]\n\n<syntaxhighlight lang=\"R\" line>\nlibrary(networkD3)\nlibrary(dplyr)\n\n# Step 1: Define each piece of the diagram. One piece consists of source node, target node and width (value). Thus, we connect \"A\" node with \"C\" node, where the link quantity is 5. (Source, target and value may be specified by columns from your own table.)\nlinks <- data.frame(\n  source=c(\"A\",\"A\", \"B\", \"C\", \"C\", \"E\", \"E\", \"E\"), \n  target=c(\"C\",\"D\", \"E\", \"F\", \"G\", \"F\", \"G\", \"H\"), \n  value=c(5, 2, 4, 3, 2, 1, 1, 1)\n)\n\n# Step 2: Name the nodes to show it on the plot. We take unique names from the dataframe, defined on the step 1.\nnodes <- data.frame(\n  name=c(as.character(links$source), as.character(links$target)) %>% unique()\n)\n\n# Step 3: Take ID from node names. networkD3 doesn`t work with categorical data. The easiest way to \"transform\" to integer is getting ID of node with necessary name.\nlinks$ID_source <- match(links$source, nodes$name)-1 \nlinks$ID_target <- match(links$target, nodes$name)-1\n\n# Step 4: Define settings of the plot, using created links and nodes.\nsankey_plot_example <- sankeyNetwork(Links = links, Nodes = nodes,\n                   Source = \"ID_source\", Target = \"ID_target\",\n                   Value = \"value\", NodeID = \"name\",fontSize = 12, height=300, sinksRight=FALSE #fontFamily = \"sans-serif\", nodePadding=40  \n)\n# Step 5: Show the plot.\nsankey_plot_example #Fig.1\n</syntaxhighlight>\n\nAdditional arguments for the plot settings can be found [https://www.rdocumentation.org/packages/networkD3/versions/0.4/topics/sankeyNetwork here].\n\nOn the plot it is possible to see detailed information about the link by holding the cursor on the flow. Furthermore, there is an opportunity to do a quick analysis of this data. In this case, \u201cE\u201d gets from \u201cB\u201d 4, but sends to \u201cF\u201d, \u201cG\u201d and \u201cH\u201d only 3 in total. 1 is missing. One can elaborate on it, depending on the data.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Evgeniya Zakharova."
                    },
                    "sha1": "pxtohhdqbhwopc0rl4qu6au7vef1rre"
                }
            },
            {
                "title": "Scenario Planning",
                "ns": "0",
                "id": "377",
                "revision": {
                    "id": "6136",
                    "parentid": "6135",
                    "timestamp": "2021-07-28T13:26:44Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19505",
                        "#text": "[[File:ConceptVisualisationScenarioPlanning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Scenario Planning]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Scenario Planning is a systematic designation of potential futures to enable long term strategic planning.\n\n==Background==\n[[File:Scenario planning.png|400px|thumb|right|'''SCOPUS hits per year for Scenario Planning until 2019.''' Search terms: 'scenario planning', 'scenario construction', 'scenario-based', 'scenario study' in Title, Abstract, Keywords. Source: own.]]\n\n'''The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s'''. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were \"(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]\" in the US. (Amer et al. 2013, p.24).\n\n'''Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.''' At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2). \n\nToday, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning.\n\n\n== What the method does ==\nScenario Planning is the systematic development of descriptions of (typically multiple) plausible futures, which are then called \"scenarios\". These descriptions of plausible futures may be illustrated with quantitative and precise details. However, the focus lies on presenting them \"(...) in coherent script-like or narrative fashion.\" (Schoemaker 1993, p.195). The scenarios developed in a Scenario Planning process are all \"fundamentally different\" (Schoemaker 1993, p.195) and may be contradictory and irreconcilable, but there is no inherent ranking between them (2). The core idea is not to present the most probable version of the future, but to get an idea about the range of possible developments of system variables and their interactions (2, 5). Scenarios \"(...) are not states of nature nor statistical predictions. The focus is not on single-line forecasting nor on fully estimating probability distributions, but rather on bounding and better understanding future uncertainties.\" (Schoemaker 1993, p.196).\n\n'''There is no ''one'' procedure for Scenario Planning''' (5). A commonly cited approach by Schoemaker (2, 3) includes the following steps:<br>\n1) Definition of time frame, scope, decision variables and major actors of the issue in question.\n\n2) Identification of current trends and predetermined elements and how they influence the defined decision variables, based on the knowledge of experts and the available data. It should be observed whether major trends are compatible with each other and which uncertainties exist. \n\n3) Construction of extreme future states along a specific continuum (positive vs negative, probable vs surprising, continuous vs divergent etc.) for all the elements or variables. These extremes are then assessed for their internal consistency and plausibility in terms of stakeholder decisions, trends and outcomes.\n\n4) Elimination of implausible or impossible futures and, based on the themes that emerged from these, the creation of new scenarios. This process is repeatedly done until internally consistent scenarios are found. The number of scenarios developed depends on the scope and purpose of the planning process (1, 5).\n\n[[File:Scenario Planning Process.png|800px|thumb|center|'''Evaluation of the number of scenarios developed.''' Source: Amer et al. 2013. p.33]]\n\n5) The preliminary scenarios are analysed in terms of which decisions would need to be taken by the key stakeholders to reach them, and potentially revised again when further investigation of these scenarios brings up additional or contradictory knowledge about them. The scenarios may also be transferred into quantitative models to learn about the development and inherent uncertainties of individual variables.\n\n6) Finally, these preliminary scenarios are re-iterated by going through the previous steps again, checking for their applicability to the issue at hand, until finally, scenarios emerge that can be used for planning processes.\n\n[[File:Scenario Planning Example.png|800px|thumb|center|'''Exemplary scenarios for life in Australian cities.''' Source: Gaziulusoy & Ryan 2017, p.1920]]\n\n\n== Strengths & Challenges ==\n* Scenario Planning allows for any organization that deploys it to be more innovative, flexible and thus better prepared for unforeseen disruptions and changes. In a corporate context, this can reduce costs, provide market benefits and improve internal communication (3, 5).\n* Scenario Planning broadens the structural perspective of an actor to think about the future (5). For example, an oil company may well be able to assess risks in their technical processes of oil exploration and extraction, but only through a more detailed scenario analysis they may be enabled to include economic, political and societal trends into their planning (2).\n* Scenarios are psychologically attractive. They are a way of transforming seemingly disparate data into relatable, coherent narratives. They present uncertainty across scenarios instead of providing probabilistic information for all elements within each individual one. In addition, they reduce the complexity and uncertainty of the future into graspable states (2).\n* Scenario Planning differs from adjacent methodological approaches. While a ''scenario'' illustrates a possible state of the future, a ''vision'' (see [[Visioning & Backcasting]] revolves around a desirable state of the future without taking its likelihood into consideration. Compared to Visioning, Scenario Planning might therefore be more useful for actual [[Glossary|decision-making]] but might as well be too narrow to envision holistic systemic changes (6). Additionally, a ''prediction'' as the classical method of economic ''forecasting'' describes likely states of the future as an extension of current developments, without the openness for [[Glossary|change]] that is inherent to Scenario Planning. \n\n'''Good scenarios should fulfill a range of characteristics''' (3, 5):\n* they need to be plausible and internally consistent, i.e. capable of happening.\n* they should be relevant, i.e. of help for decision making and connected to the issue that is to be solved.\n* they should be archetypal, i.e. not represent variations on the same theme but describe distinct futures.\n* they should describe a future that is in a state in which \"the system might exist for some length of time, as opposed to being highly transient.\" (Schoemaker 1995, p.30)\n* they should challenge the existent way of thinking about the future.\n* while Scenario Planning generally permits actors to broaden their perspective, this only works if the construction of scenarios is not biased, which easily happens (2, 3). One may unconsciously look for confirming evidence for personal presuppositions when identifying trends and uncertainties. Also, overconfidence that certain trends will (not) prevail may distort one's assessment. This should be paid attention to during the process (3). As Schoemaker (1995, p.38) puts it: \"When contemplating the future, it is useful to consider three classes of knowledge: 1. Things we know we know. 2. Things we know we don't know. 3. Things we don't know we don't know. Various biases (...) plague all three, but the greatest havoc is caused by the third.\" Ignorance in terms of the future should be acknowledged and addressed in order to challenge biases. \"And this is where scenario planning excels, since it is essentially a study of our collective ignorance.\" (ibid).\n\n[[File:Scenario Planning They Believed It.png|450px|thumb|right|'''Throughout history, smart minds have underestimated technological, economic and political developments.''' Scenario Planning can be a mean of addressing the overconfidence in thinking that things will not change. Source: Schoemaker 1995, p.26]]\n\n\n== Normativity ==\n==== Connectedness ====\n* Scenario Planning is connected to various other methodological approaches. First, it is based on a [[System Thinking and Causal Loop Diagrams|System Thinking]] approach, recognizing the interconnectedness and causal interference of elements within a system.\n* To gather a good understanding of the relevant stakeholders and trends, useful approaches are [[Social Network Analysis]] or [[Stakeholder Analysis]]. To support the analysis of how systemic variables interact with each other, [[Agent Based Modelling]] may be applied.\n* Scenario Planning may be done after a [[Visioning & Backcasting|Visioning]] process: after envisioning what is desirable, stakeholders may think about what is actually realistic (4).\n* Scenario Planning is to some extent comparable to the [[Delphi]], where experts share their opinions on the future of an issue and come to a joint prediction. However, Scenario Planning differs in that it attempts to develop several complex scenarios instead of reaching one single (often quantitative) result to a question.\n\n==== Everything normative about this method ====\n* The aforementioned distinct ways of planning for the future differ from each other not only in the process, but also on a normative dimension. The ''world how it should be''-way of thinking in a Visioning approach is highly normative. In forecasting, it is assumed that the world does not change much, which is often the case, but also often not. Scenario Planning, by comparison, assumes that the world could be changing and takes the possible changes and their consequences into consideration. It is a matter of how a company, a person or a scientist views the world that decides the approach that is taken to think about the future (see (1)). In a highly complex and dynamic world such as ours today, however, it may be advisable to consider the possibility of changing circumstances. As Wack (1985, p.73) already stated in the 1980s: \"The future is no longer stable; it has become a moving target. No single \"right\" projection can be deduced from past behavior.\"\n* As Wack (1985) highlights, developing good scenarios is not sufficient to foster action in decision-makers. Scenarios should also be able to \"(...) change the decision-makers' assumptions about how the world works and compel them to reorganize their [[Glossary|mental model]] of reality.\" (Wack 1985, p.74). Scenario Planning must be accompanied by a transformational mindset shift in the respective organization (i.e. state, company, community). This may be hard, because it challenges fond concepts of how the world works. Therefore, \"[s]cenario planning requires intellectual courage to reveal evidence that does not fit our current conceptual maps, especially when it threatens our very existence. (...) But (...) opportunities can be perceived only when you actively look for them. (...) In addition to perceiving richer options, however, we must also have the courage and vision to act on them.\" (Schoemaker 1995, p.39)\n\n\n== Outlook ==\nIt is fair to say that maybe more than ever, a constant of our time is that nothing remains as it is. New developments, such as AI, genetic engineering, environmental decline, space travel and many more, partly unforeseeable (technological) changes make it reasonable to apply more systematic approaches on thinking about the future that challenge our presupposition. Several experts people saw the COVID-19 pandemic coming, others did not, and many countries were not sufficiently prepared for it. Scenario Planning may be one approach that helps decision-makers to plan for states of the future that may arise, and especially to steer developments into the directions of desirable ones (see 1, 5).\n\n\n== An exemplary study ==\n[[File:System Thinking - Exemplary Study Hanspach et al. 2014 - Title.png|600px|frameless|center|Title from Hanspach et al. 2014]]\n[[File:Scenario Planning - Exemplary study - Hanspach et al. 2014 - Scenario axes.png|300px|thumb|right|'''The scenario matrix with four plausible alternative futures alongside two axes for Southern Transylvania.''' Source: Hanspach et al. 2014, p.37]]\nIn their 2014 publication, Hanspach et al. (see References) present - among other methodological approaches - the results of a Scenario Planning process they conducted with stakeholders in Southern Transylvania. Here, they conducted '''workshops''' with \"(...) all relevant ethnic groups, political parties, churches, and schools, as well as local police officers and organizations concerned with nature conservation, regional development, forestry, agriculture, and tourism.\" (p.34) First, they held individual workshops with each stakeholder group, in which these were asked to present their \"(...) understandings of changes in the regions\" for the past and the future \"as well as of social-ecological system dynamics and key uncertainties\" (p.35). The researchers then used the insights from these workshops to create '''\"four plausible storylines''' describing sequences of social, ecological, and economic changes\" for a 30-year time horizon (p.36). These scenarios were developed alongside two axes, which each represented endogenous uncertainties (''the ability of the locals to caplitalize on opportunities'') as well as exogenous uncertainties (''national and supranational policy emphasis'').\n\nThey received feedback to these scenario drafts in a second round of workshops. Then, they used existent data on social-ecological trends in the region and rated how these trends might develop under each scenario, resulting in '''scenario maps,''' which provide a better idea of how the region would change under each scenario. \n<br>\n[[File:Scenario Planning - Exemplary study - Hanspach et al. 2014 - Scenario Maps.png|750px|thumb|center|'''Scenario maps created for Southern Transylvania.''' Each map shows how a specific social-ecological trend (three out of eight are shown) develop under one of the four scenarios. Red = likely, blue = unlikely. Source: Hanspach et al. 2014, p.38]]\n<br>\nFurther, '''visual representations (drawings) of the four scenarios''' were created to support imagination of what they would mean for the region.\n<br>\n[[File:Scenario Planning - Exemplary study - Hanspach et al. 2014 - Scenario Drawings.png|750px|thumb|center|'''These drawing visualize Southern Transylvania under each of the four developed scenarios.''' Source: Hanspach et al. 2014, p.37]]\n<br>\nOverall, the results of the scenario planning process and the analysis of the scenarios shows how this method can help identify the most important impact factors on future developments of a system and how distinct potential developments influence specific aspects of the system. It also becomes clear how this method can facilitate policy responses and how scenarios are a helpful form of communicating scientific data.\n\n\n== Key Publications ==\nWack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n* A first-hand report from inside of Shell's planning process in the 1960s.\n\nSchoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n* A detailed description of how to conduct Scenario Planning, explained through case studies in the advertisement industry.\n\nAmer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n* A (rather complex) overview on different types of Scenario Planning across the literature.\n\nSwart, R.J., Raskin, P., Robinson, J. 2004. ''The problem of the future: sustainability science and scenario analysis.'' Global Environmental Change 14(2). 137-146.\n* A conceptual paper that elaborates on the potential of scenarios in and for sustainability science.\n\n\n== References ==\n(1) Wack, P. 1985. ''Scenarios: uncharted waters ahead.'' Harvard Business Review 63(5). 72-89.\n\n(2) Schoemaker, P.J.H. 1993. ''Multiple Scenario Development: Its Conceptual and Behavioral Foundation.'' Strategic Management Journal 14(3). 193-213.\n\n(3) Schoemaker, P.J.H. 1995. ''Scenario Planning: A Tool for Strategic Thinking.'' Sloan Management Review 36(2). 25-50.\n\n(4) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design Journal 20(1). 1916-1926.\n\n(5) Amer, M. Daim, T.U. Jetter, A. 2013. ''A review of scenario planning.'' Futures 46. 23-40.\n\n(6) Wiek et al. 2006. ''Functions of scenarios in transition processes.'' Futures 38(7). 740-766.\n\n(7)  Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n\n== Further Information ==\n* Shell works with Scenarios still today. [[On this websitehttps://www.shell.com/energy-and-innovation/the-energy-future/scenarios.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL3NjZW5hcmlvcy5odG1s|On this website]], you find a lot of information about their scenario approach.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "onqqi0yf5pj61rgxalgfjskj68pxhit"
                }
            },
            {
                "title": "Scientific methods and societal paradigms",
                "ns": "0",
                "id": "555",
                "revision": {
                    "id": "6508",
                    "parentid": "6334",
                    "timestamp": "2022-01-21T11:23:16Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Science looking at parts of reality */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18560",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Scientific methods and societal paradigms (German)]].\n\n'''In short:''' This entry discusses how [[Glossary|scientific methods]] have influenced society - and vice versa.\n__NOTOC__\n== The role of scientific paradigms for society ==\nFrom early on, scientific [[Glossary|paradigm]]s were drivers of societal development. While much else may have happened that is not conveyed by the archaeological record and other accounts of history, many high cultures of the antiques are remembered for their early development of science. Early science was often either having a pronounced practical focus, such as in metallurgy, or was more connected to the metaphysical, such as astronomy. Yet even back then, the ontological (how we make sense of our knowledge about the world) and the epistemological (how we create our knowledge about the world) was mixed up, as astronomy also allowed for navigation, and much of the belief systems was sometimes rooted, and sometimes reinforced by astronomical science. Prominent examples are the star of Bethlehem, the Mesoamerican Long Count calendar, and the Mayan calendar. However, science was for the most part of the last two millennia in a critical relation to the metaphysical, as there was often a quest for ontological truths between religions and science. While the East was more open to allow science to thrive and made active use of its merits; in Europe, many developments were seen as critical, with Galileo Galileo being a prominent example. Since this changed with the [[History of Methods|Enlightenment]], science paved the way for the rise of the European empires, and with it the associated paradigms.\n\n== Three examples for an active interaction ==\nWhile the [[History of Methods|history of methods]] was already in the focus before, here we want to focus on how the development of scientific methods interacted with societal paradigms. It is often claimed that science is in the Ivory Tower, and is widely unconnected from society. While this cannot be generalised for all branches of science, it is clear that some branches of science are more connected to society than others. Let us have a look at three examples. \n\n==== Medicine ====\nA prominent example of a strong interaction is medicine, which has at its heart the care for the patient. However, this naive assumption cannot hold the diverse paradigms that influenced and build medicine over time. Today, ananmesis - the information gained by a physician by asking specific questions of a patient - gained in importance, and the interdisciplinary conferences of modern treatments combine different expertise with the goal of a more holistic recognition of the diseases or challenges of the individual patient. \n\n==== Engineering ====\nEngineering is another branch of science which builds on a long tradition, and has at its early stages quite literally paved the road for many developments of modernity. While factories and production processes are today also seen more critically, it has become clear already since Marx that the working condition of modern production are not independent of questions of inequality. In addition, production processes are shifting in order to enable more sustainable production processes, indicating another paradigm shift in engineering. \n\n==== Agricultural science ====\nThe last example, agricultural science, is also widely built around positivistic methodology of modern science, allowing of an optimisation of agricultural production in order to maximise agricultural yield, often with dire consequences. The so-called [https://www.thoughtco.com/green-revolution-overview-1434948 'Green Revolution'] wreaked havoc on the environment, destroyed local livelihoods across the globe, and untangled traditional social-ecological systems into abusive forms that led ultimately to their demise in many parts of the world. \n\nThese three examples showcase how the development of modern science led to abusive, unbalanced, and often unsustainable developments that would in the long run trigger new paradigms such as the post-modernity, degrowths and other often controversially discussed alternatives to existing paradigms. Science was clearly an accomplice in driving many negative developments, and willingly developed the basis for many methodological foundations and paradigms that were seen in a different light after they were utilised over a longer time.\n\nEqually did society drive a demand onto scientific inquiry, demanding solutions from science, and thereby often funding science as a means to an end. Consequently, science often acted morally wrong, or failed to offer the deep [[Glossary|leverage points]] that could drive transformational [[Glossary|change]]. Such a critical view on science emerged partly out of society, and specifically did a view on empirical approaches emerge out of philosophy.\n\n\n==Science looking at parts of reality==\nSince the Enlightenment can be seen as an age of solidification of many scientific disciplines, prominent examples of an interaction between scientific developments and societal paradigms can be found here, and later. Since scientific disciplines explicitly look at parts of reality, these parts are often tamed in scientific theories, and these theories are often translated into societal paradigms. Science repeadtedly contributed to what we can interpret as category mistakes, since scientific theories that attempt to explain one part of the world were and still are often translated into other parts of the world. The second mistake is that scientific progress can be seen as continuous (see Laudan: Progress and its Problems), while societal paradigms are often utilising snapshots of scientific theories and tend to ignore further development in the respective branch of science. This makes science in turn vulnerable, as it has to claim responsibility for mistakes society made in interpreting scientific theories, and translating them into societal paradigms. In the following message I will illustrate these capital mistakes of science based on several examples. \n\n==== Social Darwinism ====\nThe evolutionary theory of Charles Darwin can be seen as a first example that illustrates how a scientific theory had catastrophic consequences when it was adapted as a societal paradigm. Ideas that the poor in late Victorian England were unworthy of state intervention, and that social welfare was hence a mistake were build on a misunderstanding of Darwins theory, and Darwin opposed the application of his theory for societal debates. Furthermore, he was horrified that his ideas was also taken as a basis to claim superiority of some races over other races, a crude and scientifically wrong claim that paved the road for some of the worst atrocities of the 20th century. \n\n==== The Friedman doctrine ====\nAnother example is Milton Friedman's theory of shareholders, which claims that corporations have first and foremost responsibility against their shareholders. While this seems like a reasonable thought, the consequences for the global economy were considered to be catastrophic by many. Friedman's theory demanded privatisation at a country-wide scale, also for many countries outside of the USA this was attempted and destroyed entire economies. Finally, the stakeholder theory offered a sound development that could counter Friedman's doctrine, and allowed for a recognition of resources, market as well as other important factors such as corporate social responsibility. While the word 'stakeholder' is deeply ambiguous, stakeholder theory and the relation to Friedman's doctrine showcases direct and drastic interactions between science and society. \n\n==== Mode 2 research ====\nA last example to showcase how a new research paradigm can even become deeply entangled with social paradigms is the concept of 'Mode 2' research. This new paradigm proposed the systematic recognition of the context that is investigated by multidisciplinary teams in a mode of knowledge production aimed at the real world. By focusing on problems of civil society, this new mode of research departs from basic research, and attempts to develop an active interaction between science and society. While academia hence opened up, gatekeepers criticised this new paradigm, or claimed that it had existed long before it was officially proclaimed. What is clear is that this line of thinking grew in importance, and that it showcases that interactions between academia and civil society gained a wider recognition, and were increasingly institutionalised, although most of academia is to this day organised in disciplines. Deep understanding in one branch of science is undeniably helpful, but there is an ongoing debate whether academia should be problem- or even solution-orientated, and whether many of the wicked problems society faces have to be addressed not only in collaboration between different disciplines, but also together with society. From a methodological standpoint, this posed new challenges, since this type of new knowledge production demanded not only a new array of methodological approaches, but also questioned the still widely dominating paradigm of positivism.\n\n== The grand abduction of science and society ==\nSince the Antique, science and society have been in a continuous spiralling movement around each other. While scientific methods are shaped by their time, the times are also shaped by scientific methodology. What is however necessary is the close and mutual interaction between empirical inquiry and the question how we ought to act based on our growing knowledge. Science is only starting to unlock the complicated relation between epistemology and ontology, and the confusion between the two created many problems in scientific tradition to date. In order to develop a robust and critical theory of science, learning from the past development of science offers a great chance. However, between the recognition of positivism, [[Bias and Critical Thinking|critical theory]] and Kuhn's recognition of scientific revolution, there is also a more continuous understanding of the history of science, which Larry Laudan coined 'a History of Ideas'. Laudan spoke of a 'History of Ideas' instead of a history of disciplines or scientists and criticized the focus on elites in the history of science. He highlighted that many people had similar ideas and that ideas can break through realities. Larry Laudan was deeply sceptical of disciplines whose alleged boundaries he did not see as strict. He preferred ideas as a unit of thought to understand concepts and the development process of solutions. A central element of his 'History of Ideas' was the question what is real and what was real in the past, which is strongly connected to [[Bias and Critical Thinking|Critical Realism]].\n\nWhen looking at past developments in science, there is a clash between rational views on past developments, and the role of social, cultural and societal factors that influenced the past developments of science. Laudan differentiated between the modes of organisation and institutional structures that science and scientists of the past had, and differentiates these from their beliefs. Laudan made a distinction between 'cognitive and non-cognitive aspects', and thus claims that science is not always made in a rational conduct. Instead - Laudan argues - scientists often made bold but irrational decisions, choose less convincing theories, alienated traditions, or even opted for non-progressive theories in their research. All this should be well understood as scientific dispute, yet sociology of science has as of yet to come up with one or probably several explanations on why scientists have chosen to deviate from rational paths in the past. The myths that scientific progress, and with it the formulation of knowledge that may be summarised as scientific revolutions, is produced by rational agents should be seen critical at best.\n\n\n== Methodological paradigms over time ==\nConcerning the tensions between new paradigms and the existing methodology, an adaptation of existing methods or a development of new methods altogether may be needed. While for instance in the 1720, following Locke and Bacon, the approach to Newtonian theory was widely inductive, the following decades saw a proclamation of heat, electricity and phenomena of chemistry that could hardly be claimed to be inductively derived, as they are not observable as such. In consequence, a methodology was derived which enabled the hypothesis-forming deduction that should prevail and even dominate for quite some time to come. Lakatos offered a modification to Kuhn's \"revolutions\", as he proclaimed that several alternative \"research programs\" can exists in parallel, and these are interacting, maybe even several theories that are built around a heuristic core. However, Lakatos - equally like Kuhn - still considers empirical evidence or methodology as pivotal, allowing only the measurable reality to be the measure of success of a new paradigm. Again, Laudan introduced a new argument to add to the measure of the success of a theory: instead of relying on how many significant problems a new theory can solve, he raises concern about the truth of theories, and instead suggests to compare how one theory is more effective or progressive than another theory. Kuhn and Lakatos both claimed that a paradigm, and with it the associated branch of science, reaches maturity if it gains enough standing to ignore anomalies, and becomes independent of outside criticism. Both Kuhn and Lakatos consider this to be positive, as it makes this part of science more progressive.\n\nLaudan criticised this notion deeply, and considered their view on history of science widely flawed and constructed. In addition, history of science looks at parts of reality, beyond the illusion that it is rational agents acting as scientists. This notion of parts of reality can be linked to Roy Bhaskars view that all science can only unlock parts of reality that are not necessarily connected or can be meaningfully connected, since some parts of reality cannot be observed. This is an important connection towards Laudan, who claims that we have not yet understood rational scientific choice, yet this understanding is a precondition to investigate the social background that the respective science is embedded in. What I call the ''grand abduction'' here is hence the seamless interaction between science and society, where we have to recognise that these two realms are not two different entities, but instead are often embedded, integrated, and at times cannot be differentiated at all. While much of positivism often claimed a deductive position, societal development surely operates at longer time scales. Society has questions that science may answer, and demands that science needs to fulfill. Science has willingly fulfilled many demands of society, and has also contributed to many developments in society, many of which are rightly criticised, while other developments also lead to positive merits. However, following Laudan we should not only question that scientists are objective, but following Bhaskar we also have to question their claim of trying to explain objective reality. Neither is science rational, nor can scientists be framed as rational actors, nor can society claim a complete disconnect from the proposed ivory towers of science.\n\n\n== A way out of the current dilemma of societies doubt towards science, and the arrogance of science towards society ==\nWhy is this relevant today? Following Bhaskar we could argue that many parts of reality will never be unlocked by us, despite being part of reality. These phenomena will never be part of our reality, and shall not concern us further. This is the first obstacle societal paradigms and society as such face today, since the overarching acceptance of the limitations of science does not exist. Instead, the pendulum swings between criticism of science - often as a means to an end as for instance climate change deniers attempt - or a naive surprise whenever scientific results change, calling the recognition of so-called 'facts' into question. The recognition of critical realism that permanent realities may not exist is especially quite alien from the current societal debate in the Western world, where the distance between a acceptance or rejection of scientific paradigm grew larger of the last years. Regarding the changeability of paradigms, we have to follow Bhaskar in recognising that it can be wise to act based on certain scientific results, yet we also need to be critical. Blindly following everything that scientists claim does not work, because as Laudan highlighted, we cannot assume that scientific actors are always rational. Post truths and fake news are thus facets of knowledge that can be equally driven by irrational scientists or by an ignorant thrive for knowledge by societal actors. Surely, many more complex phenomena are part of the reality of post truths, fake news and associated challenges we currently face. Yet by blaming either scientists or society we will fail to overcome these challenges. Instead of claiming what science ought to be, we should shift to the responsibility of the individual scientist, and building on a critical 'history of ideas' will allow for a better understanding of how scientific [[Glossary|innovation]] happened in the past. \n\nTo summarise, science and society were never as disconnected as it was framed in the past. Instead, such constructed realities were built with misleading purposes, and during the last decades, philosophy of science has increasingly tried to solve these problems. Considering the current state and validity of knowledge debates in science and society, we can clearly claim that the responsibility of the individual researcher is a good starting point in order to further overcome these challenges. Current scientific methodologies still widely resolve around dogmas and rigid traditions that participated and built the challenges we currently face. Recognising these flaws is a first step in order to overcome these problems. Seriously engaging with this will be one of the leading challenges of our generation.\n\n== Additional Information ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html More information] on Kuhn's theory of scientific paradigm shifts.\n----\n[[Category:Normativity_of_Methods]]\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "iwtbpci16r66shopagxlw32makgraw5"
                }
            },
            {
                "title": "Scientific methods and societal paradigms (German)",
                "ns": "0",
                "id": "557",
                "revision": {
                    "id": "6788",
                    "parentid": "6509",
                    "timestamp": "2022-10-10T12:17:14Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "22093",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Scientific methods and societal paradigms]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag diskutiert den Einfluss der Wissenschaft auf die Gesellschaft, und anders herum.\n__NOTOC__\n== Die Rolle wissenschaftlicher Paradigmen f\u00fcr die Gesellschaft ==\nVon fr\u00fch an waren wissenschaftliche Paradigmen Treiber gesellschaftlicher Entwicklungen. W\u00e4hrend viel passiert sein mag, was nicht durch die arch\u00e4ologischen Aufzeichnungen und andere Darstellungen der Geschichte vermittelt wird, sind viele Hochkulturen der Antike f\u00fcr ihre fr\u00fche Entwicklung der Wissenschaft in Erinnerung geblieben. Die fr\u00fche Wissenschaft hatte oft entweder einen ausgepr\u00e4gten praktischen Fokus, wie in der Metallurgie, oder war mehr mit dem Metaphysischen verbunden, wie in der Astronomie. Doch schon damals wurden das Ontologische (wie wir unser Wissen \u00fcber die Welt deuten) und das Epistemologische (wie wir Wissen \u00fcber die Welt erzeugen) vermischt, da die Astronomie auch die Navigation erm\u00f6glichte, und viele der Glaubenssysteme wurzelten manchmal in der astronomischen Wissenschaft oder wurden durch sie verst\u00e4rkt. Prominente Beispiele sind der Stern von Bethlehem, der mesoamerikanische Long Count Kalender und der Maya-Kalender. Allerdings stand die Wissenschaft den gr\u00f6\u00dften Teil der letzten zwei Jahrtausende in einem kritischen Verh\u00e4ltnis zum Metaphysischen, da es oft eine Suche nach ontologischen Wahrheiten zwischen Religionen und Wissenschaft gab. W\u00e4hrend der Osten der Wissenschaft offener gegen\u00fcberstand und ihre Vorz\u00fcge aktiv nutzte, wurden in Europa viele Entwicklungen als kritisch gesehen, ein prominentes Beispiel ist hier Galileo Galilei. Da sich dies mit der [[History of Methods|Aufkl\u00e4rung]] \u00e4nderte, ebnete die Wissenschaft den Weg f\u00fcr den Aufstieg der europ\u00e4ischen Reiche und die damit verbundenen Paradigmen. \n\n\n== Drei Beispiele aktiver Wechselwirkung ==\nW\u00e4hrend zuvor bereits die [[History of Methods|Methodengeschichte]] im Fokus stand, wollen wir uns hier darauf konzentrieren, wie die Entwicklung wissenschaftlicher Methoden mit gesellschaftlichen Paradigmen interagierte. Oft wird behauptet, die Wissenschaft befinde sich im Elfenbeinturm und sei von der Gesellschaft weitgehend abgekoppelt. W\u00e4hrend dies nicht f\u00fcr alle Wissenschaftszweige verallgemeinert werden kann, ist es klar, dass einige Wissenschaftszweige st\u00e4rker mit der Gesellschaft verbunden sind als andere. Werfen wir einen Blick auf drei Beispiele. \n\n==== Medizin ====\nEin prominentes Beispiel f\u00fcr eine starke Interaktion beider Sph\u00e4ren ist die Medizin, in deren Zentrum die Sorge um die Patient*innen steht. Diese naive Annahme kann jedoch die verschiedenen Paradigmen nicht zusammenfassen, die die Medizin im Laufe der Zeit beeinflusst und aufgebaut haben. Heute hat die Anamnese - also die Informationen, die ein*e \u00c4rzt*in durch gezielte Fragen an den*die Patient*in gewinnt - an Bedeutung gewonnen, und die interdisziplin\u00e4ren Konferenzen moderner Behandlungen verbinden unterschiedliche Expertisen mit dem Ziel, die Krankheiten oder Herausforderungen des*der einzelne*n Patient*in ganzheitlicher zu erkennen. \n\n==== Ingenieurswissenschaften ====\nIngenieurswissenschaften sind ein weiterer Wissenschaftszweig, der auf einer langen Tradition aufbaut und in seinen Anf\u00e4ngen vielen Entwicklungen der Moderne buchst\u00e4blich den Weg geebnet hat. W\u00e4hrend Fabriken und Produktionsprozesse heute auch kritischer gesehen werden, ist schon seit Marx klar geworden, dass die Arbeitsbedingungen der modernen Produktion nicht unabh\u00e4ngig von Fragen der Ungleichheit sind. Hinzu kommt, dass sich die Produktionsprozesse heute verschieben, um nachhaltigere Produktionsprozesse zu erm\u00f6glichen, was auf einen weiteren Paradigmenwechsel in der Technik hinweist. \n\n==== Landwirtschaft ====\nDas letzte Beispiel, die Agrarwissenschaft, ist ebenfalls weitgehend auf der positivistischen Methodik der modernen Wissenschaft aufgebaut, die eine Optimierung der landwirtschaftlichen Produktion zur Maximierung des landwirtschaftlichen Ertrags erm\u00f6glicht, oft mit schlimmen Folgen. Die sogenannte [https://www.thoughtco.com/green-revolution-overview-1434948 \"Gr\u00fcne Revolution\"] hat die Umwelt verw\u00fcstet, lokale Lebensgrundlagen auf der ganzen Welt zerst\u00f6rt und traditionelle sozial-\u00f6kologische Systeme in missbr\u00e4uchliche Formen verwickelt, die schlie\u00dflich in vielen Teilen der Welt zu ihrem Untergang f\u00fchrten. \n\nDiese drei Beispiele zeigen, wie die Entwicklung der modernen Wissenschaft zu missbr\u00e4uchlichen, unausgewogenen und oft nicht nachhaltigen Entwicklungen f\u00fchrte, die auf lange Sicht neue Paradigmen wie die Postmoderne, Degrowth, und andere oft kontrovers diskutierte Alternativen zu bestehenden Paradigmen ausl\u00f6sten. Die Wissenschaft war eindeutig eine Komplizin beim Vorantreiben vieler negativer Entwicklungen und entwickelte bereitwillig die Basis f\u00fcr viele methodische Grundlagen und Paradigmen, die nach l\u00e4ngerer Nutzung in einem anderen Licht gesehen wurden.\n\nEbenso trieb die Gesellschaft die Nachfrage nach wissenschaftlichen Untersuchungen an, verlangte L\u00f6sungen von der Wissenschaft und finanzierte dadurch oft die Wissenschaft als Mittel zum Zweck. Folglich handelte die Wissenschaft oft moralisch falsch oder bot nicht die tiefgreifenden Hebelpunkte, die einen transformativen Wandel vorantreiben k\u00f6nnten. Eine solche kritische Sicht auf die Wissenschaft entstand teilweise aus der Gesellschaft heraus, und insbesondere entstand eine Sicht auf empirische Ans\u00e4tze aus der Philosophie heraus.\n\n\n== Isolierte Wissenschaft ==\nDa die Aufkl\u00e4rung als ein Zeitalter der Festigung vieler wissenschaftlicher Disziplinen gesehen werden kann, finden sich hier und sp\u00e4ter prominente Beispiele f\u00fcr eine Wechselwirkung zwischen wissenschaftlichen Entwicklungen und gesellschaftlichen Paradigmen. Da wissenschaftliche Disziplinen explizit Teile der Realit\u00e4t betrachten, werden diese Teile oft in wissenschaftlichen Theorien geb\u00e4ndigt, und diese Theorien werden oft in gesellschaftliche Paradigmen \u00fcbersetzt. Die Wissenschaft hat wiederholt zu dem beigetragen, was wir als Kategorienfehler interpretieren k\u00f6nnen, denn wissenschaftliche Theorien, die einen Teil der Welt zu erkl\u00e4ren versuchen, wurden und werden oft auf andere Teile der Welt \u00fcbertragen. Der zweite Fehler besteht darin, dass wissenschaftlicher Fortschritt als kontinuierlich angesehen werden kann (siehe Laudan: Progress and its Problems), w\u00e4hrend gesellschaftliche Paradigmen oft Momentaufnahmen wissenschaftlicher Theorien verwenden und dazu neigen, die weitere Entwicklung im jeweiligen Wissenschaftszweig zu ignorieren. Dies macht die Wissenschaft wiederum angreifbar, da sie die Verantwortung f\u00fcr Fehler der Gesellschaft bei der Interpretation wissenschaftlicher Theorien und deren Umsetzung in gesellschaftliche Paradigmen \u00fcbernehmen muss. In der folgenden Botschaft werde ich diese kapitalen Fehler der Wissenschaft anhand einiger Beispiele illustrieren. \n\n==== Sozialdarwinismus ====\nDie Evolutionstheorie von Charles Darwin kann als ein erstes Beispiel daf\u00fcr angesehen werden, wie eine wissenschaftliche Theorie katastrophale Folgen hatte, wenn sie als gesellschaftliches Paradigma adaptiert wurde. Die Vorstellung, dass die Armen im sp\u00e4tviktorianischen England einer staatlichen Intervention nicht w\u00fcrdig waren und dass die Sozialf\u00fcrsorge daher ein Fehler war, baute auf einem Missverst\u00e4ndnis von Darwins Theorie auf, und Darwin wandte sich gegen die Anwendung seiner Theorie f\u00fcr gesellschaftliche Debatten. Dar\u00fcber hinaus war er entsetzt dar\u00fcber, dass seine Ideen auch als Grundlage f\u00fcr die Behauptung der \u00dcberlegenheit einiger Rassen \u00fcber andere Rassen genommen wurden, eine krude und wissenschaftlich falsche Behauptung, die den Weg f\u00fcr einige der schlimmsten Gr\u00e4ueltaten des 20. Jahrhunderts ebnete. \n\n==== Die Friedman-Doktrin ====\nEin weiteres Beispiel ist Milton Friedmans Theorie der Aktion\u00e4r*innen, die behauptet, dass Unternehmen in erster Linie Verantwortung gegen\u00fcber ihren Aktion\u00e4r*innen haben. W\u00e4hrend dies wie ein vern\u00fcnftiger Gedanke erscheint, wurden die Folgen f\u00fcr die Weltwirtschaft von vielen als katastrophal angesehen. Friedmans Theorie forderte eine fl\u00e4chendeckende Privatisierung, auch f\u00fcr viele L\u00e4nder au\u00dferhalb der USA wurde dies versucht und zerst\u00f6rte ganze Volkswirtschaften. Die Stakeholder-Theorie schlie\u00dflich bot eine fundierte Entwicklung, die der Friedman'schen Doktrin etwas entgegensetzen konnte und eine Anerkennung von Ressourcen, Markt sowie anderen wichtigen Faktoren wie der sozialen Verantwortung von Unternehmen erm\u00f6glichte. W\u00e4hrend das Wort \"Stakeholder\" zutiefst zweideutig ist, zeigt die Stakeholder-Theorie und die Beziehung zu Friedmans Doktrin direkte und drastische Wechselwirkungen zwischen Wissenschaft und Gesellschaft auf. \n\n==== Mode-2 Forschung ====\nEin letztes Beispiel, um zu zeigen, wie ein neues Forschungsparadigma tief mit sozialen Paradigmen verstrickt werden kann, ist das Konzept der \"Mode 2\"-Forschung. Dieses neue Paradigma schlug die systematische Anerkennung des Kontexts vor, der von multidisziplin\u00e4ren Teams in einem auf die reale Welt gerichteten Modus der Wissensproduktion untersucht wird. Indem er sich auf Probleme der Zivilgesellschaft konzentriert, entfernt sich dieser neue Forschungsmodus von der Grundlagenforschung und versucht, eine aktive Interaktion zwischen Wissenschaft und Gesellschaft zu entwickeln. W\u00e4hrend sich die Wissenschaft damit \u00f6ffnete, kritisierten Gatekeeper dieses neue Paradigma oder behaupteten, dass es schon lange existierte, bevor es offiziell verk\u00fcndet wurde. Klar ist, dass diese Denkrichtung an Bedeutung gewann und zeigt, dass die Interaktionen zwischen Wissenschaft und Zivilgesellschaft eine breitere Anerkennung fanden und zunehmend institutionalisiert wurden, obwohl die Wissenschaft bis heute gr\u00f6\u00dftenteils in Disziplinen organisiert ist. Tiefes Verst\u00e4ndnis in einem Wissenschaftszweig ist unbestreitbar hilfreich, aber es gibt eine anhaltende Debatte dar\u00fcber, ob die Wissenschaft problem- oder sogar l\u00f6sungsorientiert sein sollte und ob viele der \"wicked problems\", mit denen die Gesellschaft konfrontiert ist, nicht nur in Zusammenarbeit zwischen verschiedenen Disziplinen, sondern auch zusammen mit der Gesellschaft angegangen werden m\u00fcssen. Aus methodischer Sicht stellte dies neue Herausforderungen dar, da diese Art der neuen Wissensproduktion nicht nur ein neues Spektrum an methodischen Ans\u00e4tzen erforderte, sondern auch das immer noch weithin dominierende Paradigma des Positivismus in Frage stellte. \n\n\n== Der Tanz von Wissenschaft und Gesellschaft ==\nSeit der Antike befinden sich Wissenschaft und Gesellschaft in einer st\u00e4ndigen spiralf\u00f6rmigen Bewegung umeinander. W\u00e4hrend die wissenschaftlichen Methoden von ihrer Zeit gepr\u00e4gt sind, sind die Zeiten auch von der wissenschaftlichen Methodik gepr\u00e4gt. Notwendig ist jedoch die enge und wechselseitige Interaktion zwischen empirischer Untersuchung und der Frage, wie wir auf der Grundlage unseres wachsenden Wissens handeln sollen. Die Wissenschaft f\u00e4ngt gerade erst an, die komplizierte Beziehung zwischen Epistemologie und Ontologie zu entschl\u00fcsseln, und die Verwechslung zwischen beiden hat in der wissenschaftlichen Tradition bis heute viele Probleme geschaffen. Um eine robuste und kritische Wissenschaftstheorie zu entwickeln, bietet das Lernen aus der vergangenen Entwicklung der Wissenschaft eine gro\u00dfe Chance. Zwischen der Anerkennung des Positivismus, der [[Bias and Critical Thinking|Kritischen Theorie]] und Kuhns Anerkennung der wissenschaftlichen Revolution gibt es jedoch auch ein kontinuierlicheres Verst\u00e4ndnis der Wissenschaftsgeschichte, das Larry Laudan als \"eine Geschichte der Ideen\" (History of Ideas) bezeichnete. Laudan sprach von einer \"Geschichte der Ideen\" anstelle einer Geschichte von Disziplinen oder Wissenschaftler*innen und kritisierte den Fokus auf Eliten, den Wissenschaftsgeschichte oft einnimmt. Er stellte heraus, dass viele Menschen \u00e4hnliche Ideen hatten, und dass Ideen bestehende Realit\u00e4ten durchbrechen k\u00f6nnen. Laudan war Disziplinen gegen\u00fcber sehr skeptisch, deren vermeintliche Grenzen er nicht so streng wahrnahm. Er bevorzugte Ideen als Denkeinheit, um Konzepte und die Entwicklung von L\u00f6sungen zu verstehen. Ein zentrales Element seiner \"History of Ideas\" war die Frage, was real ist und war, was wiederum eng mit [[Bias and Critical Thinking (German)|Kritischem Realismus]] zusammenh\u00e4ngt.\n\nBei der Betrachtung vergangener Entwicklungen in der Wissenschaft gibt es einen Zusammensto\u00df zwischen rationalen Ansichten \u00fcber vergangene Entwicklungen und der Rolle sozialer, kultureller und gesellschaftlicher Faktoren, die die vergangenen Entwicklungen der Wissenschaft beeinflusst haben. Laudan unterscheidet zwischen den Organisationsformen und institutionellen Strukturen, die die Wissenschaft und die Wissenschaftler*innen der Vergangenheit hatten, und differenziert diese von ihren \u00dcberzeugungen. Durch die Unterscheidung zwischen dem, was er als \"kognitive und nicht-kognitive Aspekte\" bezeichnete, ebnet Laudan den Weg f\u00fcr eine Entmystifizierung der Wissenschaft als rationales Verhalten, da er behauptet, dass Wissenschaftler*innen k\u00fchne Entscheidungen trafen, weniger \u00fcberzeugende Theorien w\u00e4hlten, Traditionen entfremdeten oder sogar nicht fortschrittliche Theorien w\u00e4hlten. All dies sollte als laufender wissenschaftlicher Disput verstanden werden, doch die Wissenschaftssoziologie hat bisher noch keine oder wahrscheinlich mehrere Erkl\u00e4rungen daf\u00fcr gefunden, warum Wissenschaftler*innen in der Vergangenheit von rationalen Wegen abgewichen sind. Die Mythen, dass wissenschaftlicher Fortschritt und damit die Formulierung von Wissen, die man als wissenschaftliche Revolutionen zusammenfassen kann, von rationalen Akteur*innen hervorgebracht wird, sind bestenfalls kritisch zu sehen. \n\n\n== Methodologische Paradigmen im Lauf der Zeit ==\nIm Spannungsfeld zwischen neuen Paradigmen und der bestehenden Methodik kann eine Anpassung bestehender Methoden oder eine Entwicklung g\u00e4nzlich neuer Methoden erforderlich sein. W\u00e4hrend z.B. in den 1720er Jahren im Anschluss an Locke und Bacon die Herangehensweise an die Newtonsche Theorie weitgehend induktiv war, wurden in den folgenden Jahrzehnten W\u00e4rme, Elektrizit\u00e4t und Ph\u00e4nomene der Chemie proklamiert, von denen kaum behauptet werden kann, dass sie induktiv abgeleitet sind, da sie als solche nicht beobachtbar sind. In der Folge wurde eine Methodik abgeleitet, die die hypothesenbildende Deduktion erm\u00f6glichte, die noch eine ganze Weile vorherrschen und sogar dominieren sollte. Lakatos bot eine Modifikation der Kuhn'schen \"Revolutionen\" an, indem er proklamierte, dass mehrere alternative \"Forschungsprogramme\" parallel existieren k\u00f6nnen, und zwar interagierende, vielleicht sogar mehrere Theorien, die um einen heuristischen Kern herum aufgebaut sind. Allerdings h\u00e4lt Lakatos - ebenso wie Kuhn - die empirische Evidenz bzw. die Methodologie immer noch f\u00fcr zentral und l\u00e4sst nur die messbare Realit\u00e4t als Ma\u00dfstab f\u00fcr den Erfolg eines neuen Paradigmas gelten. Wiederum f\u00fchrte Laudan ein neues Argument ein, um das Ma\u00df f\u00fcr den Erfolg einer Theorie zu erg\u00e4nzen: Statt sich darauf zu verlassen, wie viele signifikante Probleme eine neue Theorie l\u00f6sen kann, macht er sich Gedanken \u00fcber den Wahrheitsgehalt von Theorien und schl\u00e4gt stattdessen vor, zu vergleichen, inwiefern eine Theorie effektiver oder fortschrittlicher ist als eine andere Theorie. Sowohl Kuhn als auch Lakatos behaupteten, dass ein Paradigma und damit der zugeh\u00f6rige Wissenschaftszweig dann Reife erlangt, wenn es gen\u00fcgend Ansehen erlangt, um Anomalien zu ignorieren, und unabh\u00e4ngig von \u00e4u\u00dferer Kritik wird. Sowohl Kuhn als auch Lakatos betrachten dies als positiv, da es diesen Teil der Wissenschaft fortschrittlicher macht.\n\nLaudan kritisierte diese Vorstellung zutiefst und hielt ihre Sicht auf die Wissenschaftsgeschichte f\u00fcr weitgehend fehlerhaft und konstruiert. Dar\u00fcber hinaus betrachtet die Wissenschaftsgeschichte Teile der Realit\u00e4t, jenseits der Illusion, dass es sich um rationale Agenten handelt, die als Wissenschaftler*innen handeln. Diese Vorstellung von Teilen der Realit\u00e4t kann mit Roy Baskhars Ansicht verbunden werden, dass alle Wissenschaft nur Teile der Realit\u00e4t erschlie\u00dfen kann, die nicht notwendigerweise miteinander verbunden sind oder sinnvoll verbunden werden k\u00f6nnen, da einige Teile der Realit\u00e4t nicht beobachtet werden k\u00f6nnen. Dies ist eine wichtige Verbindung zu Laudan, der behauptet, dass wir die rationale wissenschaftliche Entscheidung noch nicht verstanden haben, dieses Verst\u00e4ndnis aber eine Voraussetzung daf\u00fcr ist, den sozialen Hintergrund zu untersuchen, in den die jeweilige Wissenschaft eingebettet ist. Was ich hier als \"gro\u00dfe Entf\u00fchrung\" bezeichne, ist also die nahtlose Interaktion zwischen Wissenschaft und Gesellschaft, bei der wir erkennen m\u00fcssen, dass diese beiden Bereiche nicht zwei verschiedene Entit\u00e4ten sind, sondern oft eingebettet und integriert sind und manchmal \u00fcberhaupt nicht unterschieden werden k\u00f6nnen. W\u00e4hrend ein Gro\u00dfteil des Positivismus oft eine deduktive Position beanspruchte, operiert die gesellschaftliche Entwicklung sicherlich auf l\u00e4ngeren Zeitskalen. Die Gesellschaft hat Fragen, die die Wissenschaft beantworten kann, und Anforderungen, die die Wissenschaft erf\u00fcllen muss. Die Wissenschaft hat viele Anforderungen der Gesellschaft bereitwillig erf\u00fcllt und hat auch zu vielen Entwicklungen in der Gesellschaft beigetragen, von denen viele zu Recht kritisiert werden, w\u00e4hrend andere Entwicklungen auch zu positiven Verdiensten f\u00fchren. Nach Laudan sollten wir jedoch nicht nur infrage stellen, dass Wissenschaftler*innen objektiv sind, sondern nach Bhaskar m\u00fcssen wir auch ihren Anspruch infrage stellen, die objektive Realit\u00e4t erkl\u00e4ren zu wollen. Weder ist die Wissenschaft rational, noch k\u00f6nnen Wissenschaftler*innen als rationale Akteur*innen dargestellt werden, noch kann die Gesellschaft eine v\u00f6llige Abkopplung von den vorgeschlagenen Elfenbeint\u00fcrmen der Wissenschaft beanspruchen. \n\n\n== Weg vom derzeitigen gesellschaftllichen Zweifel an der Wissenschaft, und der wissenschaftlichen Arroganz gegen\u00fcber der Gesellschaft ==\nWarum ist das heute relevant? In Anlehnung an Baskhar k\u00f6nnten wir argumentieren, dass viele Teile der Realit\u00e4t nie von uns erschlossen werden, obwohl sie existieren. Diese Ph\u00e4nomene werden nie Teil unserer Realit\u00e4t sein und sollen uns nicht weiter besch\u00e4ftigen. Dies ist das erste Hindernis, vor dem gesellschaftliche Paradigmen und die Gesellschaft als solche heute stehen, da die \u00fcbergreifende Akzeptanz der Grenzen der Wissenschaft nicht existiert. Stattdessen schwingt das Pendel zwischen Kritik an der Wissenschaft - oft als Mittel zum Zweck, wie es etwa die Klimawandelleugner*innen versuchen - und naiver \u00dcberraschung, wenn sich wissenschaftliche Ergebnisse \u00e4ndern und die Anerkennung sogenannter \"Fakten\" in Frage gestellt wird. Die Erkenntnis des Kritischen Realismus, dass es m\u00f6glicherweise keine dauerhaften Realit\u00e4ten gibt, ist vor allem der aktuellen gesellschaftlichen Debatte in der westlichen Welt recht fremd, wo die Distanz zwischen einer Akzeptanz oder Ablehnung wissenschaftlicher Paradigmen in den letzten Jahren gr\u00f6\u00dfer geworden ist. Was die Ver\u00e4nderbarkeit von Paradigmen betrifft, so m\u00fcssen wir Bhaskar darin folgen, dass es klug sein kann, auf der Grundlage bestimmter wissenschaftlicher Ergebnisse zu handeln, aber wir m\u00fcssen auch kritisch sein. Blind allem zu folgen, was Wissenschaftler*innen behaupten, funktioniert nicht, denn wie Laudan hervorhob, k\u00f6nnen wir nicht davon ausgehen, dass wissenschaftliche Akteur*innen immer rational sind. Postwahrheiten und Fake News sind also Facetten des Wissens, die gleicherma\u00dfen von irrationalen WissenschaftlerInnen oder von einem ignoranten Wissensdrang gesellschaftlicher Akteur*innen getrieben sein k\u00f6nnen. Sicherlich sind noch viel komplexere Ph\u00e4nomene Teil der Realit\u00e4t von Postwahrheiten, Fake News und den damit verbundenen Herausforderungen, denen wir derzeit gegen\u00fcberstehen. Doch indem wir entweder den Wissenschaftler*innen oder der Gesellschaft die Schuld geben, werden wir es nicht schaffen, diese Herausforderungen zu bew\u00e4ltigen. Anstatt zu behaupten, was Wissenschaft sein sollte, sollten wir uns auf die Verantwortung des*der einzelne*n Wissenschaftler*in fokussieren, und der Aufbau auf einer kritischen \"Ideengeschichte\" wird ein besseres Verst\u00e4ndnis daf\u00fcr erm\u00f6glichen, wie wissenschaftliche Innovation in der Vergangenheit stattfand. \n\nZusammenfassend kann man sagen, dass Wissenschaft und Gesellschaft nie so unverbunden waren, wie es in der Vergangenheit dargestellt wurde. Stattdessen wurden solche konstruierten Realit\u00e4ten mit irref\u00fchrenden Zielen aufgebaut, und in den letzten Jahrzehnten hat die Wissenschaftsphilosophie zunehmend versucht, diese Probleme zu l\u00f6sen. In Anbetracht des aktuellen Zustands und der G\u00fcltigkeit der Wissensdebatten in Wissenschaft und Gesellschaft k\u00f6nnen wir eindeutig behaupten, dass die Verantwortung des*der einzelne*n Forscher*in ein guter Ausgangspunkt ist, um diese Herausforderungen weiter zu \u00fcberwinden. Die aktuellen wissenschaftlichen Methoden l\u00f6sen sich immer noch weitgehend um Dogmen und starre Traditionen herum auf, die an den Herausforderungen, mit denen wir derzeit konfrontiert sind, mitgewirkt und diese aufgebaut haben. Das Erkennen dieser Fehler ist ein erster Schritt, um diese Probleme zu \u00fcberwinden. Sich ernsthaft damit auseinanderzusetzen, wird eine der f\u00fchrenden Herausforderungen unserer Generation sein.\n\n== Weitere Infos ==\n* [https://www.simplypsychology.org/Kuhn-Paradigm.html Mehr Infos] \u00fcber Kuhns Theorie der Paradigmenwechsel.\n----\n[[Category:Normativity_of_Methods]]\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "62wb2q663le475dx6szzb45lx8lsmqg"
                }
            },
            {
                "title": "Scrum",
                "ns": "0",
                "id": "383",
                "revision": {
                    "id": "5799",
                    "parentid": "5060",
                    "timestamp": "2021-06-13T23:34:39Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "3174",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\n'''Scrum is a process [[Glossary|framework]] for [https://www.apm.org.uk/resources/find-a-resource/agile-project-management/ agile] project management.''' You'll want to use Scrum in complex and uncertain project environments when developing a 'product'. Note that ''product'' can be defined very widely and is not confined to a physical object but can also be a piece of software, a paper or a presentation. If you are faced with a lot of uncertainty about the end-result of your project and need good, constant [[Glossary|communication]] within your team, Scrum might be just right for you.\n\nScrum is designed for a team of no more than 5 - 7 members. While its application needs some practice, teams can easily start incorporating basics step-by-step and learn along the way. A person with Scrum experience that can advise is nonetheless very helpful.\n\n== Goals ==\n* Manage uncertainty\n* Achieve discussable & presentable results early\n* Foster good communication\n\n== Getting Started ==\n[[File:Scrum.png|550px|center|thumb|'''The Scrum framework.''' Source: [https://medium.com/i-want-to-be-a-product-manager-when-i-grow-up/the-scrum-framework-95b16bc216c4 Scrum]]]\n\nScrum is based around the agile principles of embracing constant [[Glossary|change]] and planning as little as possible and as much as necessary. Its origins are in software development. Since Scrum is a really extensive framework, please refer to ''Links & Further reading'' in order to get to know it.\n\n== Links & Further reading ==\nPlease note that most Scrum material is typically written in the context of software development as this is where Scrum originated. It can nevertheless be used for other types of project management.\n\n'''Videos'''\n* [https://www.youtube.com/watch?v=WxiuE-1ujCM Scrum in 120 seconds]: Use for a very brief overview\n* [https://www.youtube.com/watch?v=2Vt7Ik8Ublw Scrum in under 5 minutes]: Use for a tad more detailed overview\n\n'''Text'''\n* [https://resources.collab.net/agile-101/what-is-scrum What is Scrum]: A very brief overview\n* [https://www.codeproject.com/Articles/704720/Scrum-explained Scrum explained]: A brief overview\n* [https://agilemanifesto.org/principles.html The Agile Manifesto]: The principles upon which Scrum is built\n\n'''Books'''\n\n* Essential Scrum - [https://www.buecher.de/shop/sonstige-themen/essential-scrum-ebook-pdf/rubin-kenneth-s-/products_products/detail/prod_id/40889891/ English] / [https://www.amazon.com/-/de/dp/0137043295/ German]\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n\nThe [[Table_of_Contributors| author]] of this entry is Matteo Ramin."
                    },
                    "sha1": "81ruuv60b05h6mkyfaorgpo1z4on7pl"
                }
            },
            {
                "title": "Semi-structured Interview",
                "ns": "0",
                "id": "247",
                "revision": {
                    "id": "6864",
                    "parentid": "6861",
                    "timestamp": "2022-11-28T22:59:12Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "17984",
                        "#text": "[[File:ConceptSemi-structuredInterview.png|450px|frameless|left|Method categorization for Semi-structured Interview]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''In short:''' Semi-structured Interviews are a form of qualitative data gathering through loosely pre-structured conversations with Interviewees. For more Interview forms, and more on Interview methodology in general, please refer to the [[Interviews|Interview overview page]].\n\n== Background ==\n[[File:Semi-structured interviews.png|400px|thumb|right|'''SCOPUS hits per year for Semi-structured Interview until 2019.''' Search terms: 'semi-structured interview', 'half-open interview', 'half-structured interview' in Title, Abstract, Keywords. Source: own.]]\n\n\"Interview methodology is perhaps the oldest of all the social science methodologies. Asking Interview participants a series of informal questions to obtain knowledge has been a common practice among anthropologists and sociologists since the inception of their disciplines. Within sociology, the early-20th-century urban ethnographers of the Chicago School did much to prompt interest in the method.\u201d (1) '''Barney Glaser and Anselm Strauss''' are crucial figures in this regard, developing the [[Grounded Theory]] approach to qualitative data analysis during the 1960s. With their 1967 landmark book (see Key Publications) they paved the road towards an integration of methodologically building on Interviews, thereby creating a whole new area of research in sociology and beyond. Building on their proposal, yet also before, several publications contributed to a proper understanding of Interview methodology and how to best conduct Interviews (see Key Publications). Today, qualitative Interviews are used mostly in gender studies, social and political studies as well as ethnographics (2, 8).\n\n\n== What the method does ==\n* Interviews are a form of data gathering. They can be conducted in direct conversation with individuals or groups. This is also possible online or via the phone (1, 2). It is possible for more than one researcher to conduct the Interview together (2). The necessary equipment includes an Interview guide that helps ask relevant questions, a recording device and paper & pencil or a computer for note taking during the Interview.\n* Semi-structured Interviews can be used for many research purposes, including specialized forms. Among the latter, more common versions include the expert Interview where the Interviewee is a person of special knowledge on the topic, the biographical Interview that serves to investigate a life-history, the clinical Interview that helps diagnose illnesses, and the dilemma-Interview that revolves around moral judgements (8).\n* \"Interview methodology is particularly useful for researchers who take a phenomenological approach. That is, they are concerned with the way in which individuals interpret and assign meaning to their social world. It is also commonly used in more open-ended inductive research whereby the researcher observes specific patterns within the Interview data, formulates hypotheses to be explored with additional data, and finally develops theory.\" (1) '''Interviews allow the researcher to gain insight into understandings, opinions, attitudes and beliefs of the Interviewee''' (2).\n\n[[File:ResultVisualisationOpenInterview.png|400px|thumb|right|'''An exemplary Interview transcript from a Semi-structured Interview.''' Source: own.]]\n\n==== How to do a semi-structured Interview ====\n'''Creating the Interview guideline'''<br/>\nIn a semi-structured Interview, the researcher asks one or more individuals open-ended questions. These questions are based on and guided by an Interview guide that is developed prior to the conduction of the Interview. The Interview guide is based on the research intent and questions, and can be informed by existing literature: the researchers think of the information they need to gather from the Interviews to answer the research questions, and develop the Interview guide accordingly. The Interview guide thus contains keywords or issues that need to be covered during the Interview in order to answer the research questions. However, in this form of Interviews, there is still room for new topics, or focal points to existing questions, emerging during the Interview itself. The guide, therefore, ought to be as \"open as possible, as structuring as necessary\" (Helfferich 2019, p.670). The contents are structured in broad overarching categories (dimensions) and subordinate, more precise elements (3, 1). To create an Interview guide, first, questions relevant to the research interest are collected in an unstructured manner. After this, they are sorted according to their cohesiveness (Helfferich C. 2019, p.675f). The Interview guide should be tested before conducting the first Interviews that ought to be analyzed. In these pre-tests with peers or test subjects, the length of the Interview guide, the cohesiveness and viability of the questions, and the overall structure of the guide can be checked and possibly refined.\n\n'''Conducting the Interview'''<br/>\nBefore the start of the Interview, the Interviewer lets the Interviewee sign a Consent Form, guaranteering voluntary participation in the research. The Interviewer should provide Full Disclosure (information about the main goal of the research and further use of the information), confidentiality of the Data as well as the right of the Interviewees to review the results before publication and to withdraw from the Interview if they feel uncomfortable.\n\nThe Interviewer then starts posing general questions based on the overarching dimensions of the Interview guide. They are answered openly by the Interviewee and can be supported by asking for more details. Subsequently, more detailed questions based on the subordinate elements are posed to gather knowledge on previously un-addressed issues (5). Commonly, although not always, the Interviewer is also the researcher, which provides him/her with a role that can be potentially conflicting: He/she/they can decide ''ad hoc'' to change the order of questions, pre-empt elements that were designated for later questioning or add questions based on insights that arise during the Interview (1). The Interviewer may encourage the Interviewee to extend on his/her/their remarks but also guide the Interview back to the pre-determined structure in case the Interviewee digresses (3). At the end of the Interview, all subordinate aspects of the Interview guide should be covered (3). Additional questions may be asked if they emerge during the Interview, but should not replace previously planned questions from the guide.\n\n'''Transcribing the Interview'''<br/>\nThe Interview should be video- or audio-recorded for later transcription. This transcription is preferably done by writing down the recorded speech either word by word (denaturalism) or including stutters, pauses and other idiosyncratic elements of speech (naturalism). The chosen approach depends on the research design and purpose. In any case, the form of transcription should not impose too much interpretation upon the Interview data, and allow for intersubjective data readibility (7). The text may be structured and punctuations may be added to improve legibility. For ethical reasons, the identity of the Interviewee should be secured in the transcribed text (7). To allow for quotations, the lines of the transcript should be numbered or time codes should be added at the end of each paragraph (6). The subsequent analysis of the gathered data relies on this transcription. For more on this step, please refer to the entry on [[Transcribing Interviews]].\n\n\n== Strengths & Challenges ==\n* Due to the structure of the Interview procedure, the Semi-structured Interview allows for comparability of the results while ensuring openness. Thereby, it allows for the testing of existing hypotheses as well as the creation of new ones (3).\n* Compared to the more restricted standardized format of [[Survey|surveys]], the qualitative Interview allows for an open collection and investigation of self-interpretations and situational meanings on the part of the Interviewees. This way, theories from psychology and sociology can more easily be empirically tested (3).\n* At the same time, due to this focus on the subjective position of the Interviewee with regards to his/her feelings, attitudes and interpretations, the results gathered in qualitative Interviews are not as easily generalizable and more open to criticism than data gathered in quantitative [[Survey|surveys]] (2).\n* Language barriers may challenge the understanding of the Interviewees' statements during the Interview and in the transcription process.\n* '''Pitfalls during the Interview:''' It is crucial that the Interviewer is well-acquainted with the theoretical approach and design of the research. Only then can he/she assess when to depart from the previously developed questions, ask follow-up or intentionally open questions (8). This process imposes high demands on the Interviewer. The Interviewer must remain attentive and flexible throughout the Interview in order to make sure all relevant aspects of the Interview guide are profoundly answered. The quality and richness of the data depend on the proficiency in this process (1). In addition, the Interviewer must make sure not to impose [[Bias and Critical Thinking|bias]] on the Interviewee. Therefore, he/she should not ask closed yes/no-questions or offer answers for the Interviewee to choose from. The Interviewer should not be impatient while listening to the Interviewee's narration. Answers should not be commented or confirmed. The questions should not be judgemental, unexpected or incomprehensible to the Interviewee (3, 5). It is thus recommendable that the Interview be rehearsed and the Interview guide be tested before the first real Interview so as to ensure the quality of the Interview conduction. Lastly, the issue of Interviewee fatigue - i.e. the Interviewee becoming tired of overly long Interviews, which may lead to a decline of the quality of the responses - should be counteracted by limiting the amount of questions in the Interview guide and thus the length of the overall Interview.\n* The amount and depth of data that is gathered in long Interviews prohibits a big sample size. Also, the more extensive the Interview is, the longertakes the transcription process. Especially longer Interviews cannot be replicated endlessly, as opposed to the huge number results possible with standardized surveys. Semi-structured Interviews therefore tend to medium-sized samples (1, 2).\n\n\n== Normativity ==\n==== Connected methods ====\n* Qualitative Interviews can be used as a preparation for standardized quantitative Interviews ([[Survey]]), [[Focus Groups]] or the development of other types of data gathering (2, 8). To this end, they can provide an initial understanding of the situation or field of interest, upon which more concrete research elements may follow.\n* A [[Systematic Literature Review|literature review]] may be conducted in advance to support the creation of the Interview guide.\n* A Stakeholder Analysis may be of help to identify relevant interviewees.\n* The transcripts ought to be analysed using a qualitative / quantitative form of content analysis (e.g. with MAXQDA) (1).\n\n==== Everything normative related to this method ====\n* '''Quality criteria:''' The knowledge gathered in qualitative Interviews, as in all qualitative research, is dependent on the context it was gathered in. Thus, as opposed to standardized data gathering, objectivity and reliability cannot be valid criteria of quality for the conduction of qualitative Interviews. Instead, a good qualitative Interview properly reflects upon the subjectivity involved in the data gathering process and how it influences the results. It is therefore crucial to transparently incorporate the circumstances and setting of the Interview situation into the data analysis. The quality criteria of validity, i.e. the extent to which the gathered data constitutes the intended knowledge, must be approached by the principles of openness and unfamiliarity. An open approach to the Interview ensures a valid understanding of the Interviewee's subjective reality (4).\n* '''Ethics''': As indicated throughout the text, a range of ethical principles should guide the Interviewing process. These include that Interviewees should be informed about the goal of the research and the (confidential) use of their data and statements. The creation of recordings requires consent. Interviewees should be allowed to review and withdraw statements, and if desired, they should be credited for their participation in the Interviews.\n* The [[sampling]] strategy may heavily influence the gathered data, with snowballing- or opportunistic sampling leading to potential bias in the Interviewee sample. The sample size depends on the intended knowledge in the first place. While more structured, systematic approaches (see [[Survey]]) need bigger sample sizes from which a numerical generalization may be done, more qualitative approaches typically involve smaller samples and generalization of the gathered data is not the researcher's main goal (2).\n\n\n== Key publications ==\nFielding, Nigel G., ed. 2009. ''Interviewing II.'' London: SAGE.\n* A four-volume collection of essays of which the wide-ranging contributions comprehensively cover all the theoretical and practical aspects of Interviewing methodology.\n\nGlaser, Barney G., and Anselm L. Strauss. 1967. ''The discovery of grounded theory: Strategies for qualitative research.'' Chicago: Aldine.\n* The principles of [[Grounded Theory]] were first articulated in this book. The authors contrast grounded theories derived directly from the data with theories derived from a deductive approach.\n\nPatton, Michael Q. 2002. ''Qualitative research and evaluation methods.'' 3d ed. Thousand Oaks, CA, and London: SAGE.\n* In chapter 7, Patton provides a comprehensive guide to qualitative interviewing. This chapter highlights the variations in qualitative interviews and the interview guides or schedules that can be used. It provides a very useful guide as to how to formulate and ask questions and offers practical tips about recording and transcribing interviews. The chapter also covers focus groups, group interviews, ethics, and the relationship between researcher and interview participants.\n\nHelfferich C. ''Leitfaden- und Experteninterviews.'' In: Baur N., Blasius J. (Hrsg.) 2019. ''Handbuch Methoden der empirischen Sozialforschung.'' Springer VS Wiesbaden. 669-685.\n* A brief, but precise description of the basics of interview conduction. The chapter includes definitions, thoughts on the interview situation, a guide to interview guide creation and remarks on expert interviews in German language.\n\nWidodo, H.P. 2014. ''Methodological Considerations in Interview Data Transcription.'' International Journal of Innovation in English Language 3(1). 101-107.\n* Includes considerations on data management and transcription of interview data.\n\nWhyte, William Foote. 1993. ''Street corner society: The social structure of an Italian slum.'' 4th ed. Chicago and London: Univ. of Chicago Press.\n* The appendix describes in great detail how Whyte carried out his ethnographic research. He writes about how he had to learn not only when it was appropriate to ask questions, but also how to ask those questions\u2014and that, once he was established in the neighborhood, much of his data was gathered during casual conversations.\n\nBryman, A. 2012. ''Social Research Methods.'' 4th Edition. Oxford University Press.\n* An all-encompassing guide to the basics of social science research, including insights into interview methodology.\n\n\n== References ==\n(1) Hamill, H. 2014. ''Interview Methodology.'' in: Oxford Bibliographies. Sociology.\n\n(2) Arksey, H. Knight, P. 1999. ''Interviewing for Social Scientists. An Introductory Resource with Examples.'' SAGE Publications, London.\n\n(3) Rager, G., Oestmann, I., Werner, P. ''Leitfadeninterview und Inhaltsanalyse. 1. Das Leitfadeninterview.'' 35-43. In: Viehoff, R., Rusch, G., Segers, R.T. 1999. Siegener Periodicum zur Internationalen Empirischen Literaturwissenschaft (SPIEL). Heft 1. Europ\u00e4ischer Verlag der Wissenschaften.\n\n(4) Helfferich C. ''Leitfaden- und Experteninterviews.'' In: Baur N., Blasius J. (Hrsg.) 2019. ''Handbuch Methoden der empirischen Sozialforschung.'' Springer VS Wiesbaden. 669-685.\n\n(5) Helfferich, C. 2011. ''Die Qualit\u00e4t qualitativer Daten. Manual f\u00fcr die Durchf\u00fchrung qualitativer Interviews.'' 4th edition. Springer VS Wiesbaden.\n\n(6) Dresing, T., Pehl, T. 2015. ''Praxisbuch Interview, Transkription & Analyse. Anleitungen und Regelsysteme f\u00fcr qualitativ Forschende.'' 6th edition.\n\n(7) Widodo, H.P. 2014. ''Methodological Considerations in Interview Data Transcription.'' International Journal of Innovation in English Language 3(1). 101-107.\n\n(8) Hopf, C. ''Qualitative Interviews: An Overview'': In: Flick, U. von Kardorff, E. Steinke, I. (eds). 2004. ''A Companion to Qualitative Research.'' SAGE Publications, London. 203-208.\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "dnyu2e0fswuc001im6t0iraz4lhgkxg"
                }
            },
            {
                "title": "Serious Gaming",
                "ns": "0",
                "id": "265",
                "revision": {
                    "id": "6174",
                    "parentid": "6173",
                    "timestamp": "2021-07-30T09:07:49Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "comment": "/* Further Information */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18700",
                        "#text": "[[File:ConceptSeriousGaming.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Serious_Gaming]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Serious Gaming involves games, i.e. interactive analoguous or digital formats, that harvest or create knowledge within a game setting with stakeholders.\n\n== Background ==\n[[File:Serious gaming.png|400px|thumb|right|'''SCOPUS hits per year for Serious Gaming until 2019.''' Search term: 'serious gaming' in Title, Abstract, Keywords. Source: own.]]\n\n\"Gaming has roots in systems analysis, operations research and decision sciences. The earliest use of gaming in support of decision making are in war games (...). They have a long history and originated as devices for planning military operations (...). The use of gaming in a political-military-security context was subsequently transferred to a non-military context, hence the interest in gaming simulation by not only computer scientists and game designers, but also decision makers, public policy makers, engineers and scientists.\" (Savic et al. 2016, p.457).\n\nSerious Games are a form of gaming and thus located within the field of simulation and gaming research, alongside other aspects such as computer simulations, agent-based modeling, role-plays and virtual reality (2). '''\"The notion of serious gaming was introduced by Abt (1970), who established how simulation games could be used for education, decision making and for public policy making.\"''' (Savic et al. 2016, p. 457, emphasis added). Since Abt's primary work, the method has found its way into several fields, including business, corporate, healthcare, government, military (among these the RAND corporation who invented the [[Delphi]] Method) and science (1, 2, 8). However, its primary usage is for education, training and conflict resolution (1, 5).\n\n\n== What the method does ==\n* \"[A game] is almost impossible to define, but we recognize one when we see it.\" writes Crookall (2), citing the philosopher Wittgenstein. This statement highlights that there is a wide, unanimous range of definitions for the term 'game', and subsequently for the term 'Serious Gaming'. However, most of the existing definitions agree upon the aspect that \"(...) they are games used for purposes other than mere entertainment.\" (Savic et al. 2016, p.457). Different from most popular video games, entertainment is not the primary goal of a (serious) game, but rather a means to an end (4, 5, 8). '''A Serious Game pursues a specific purpose that lies predominantly in training, educating and achieving behavioral change in the players''' (4, 5). Specific goals can be, among others, raising attention, challenging prejudgments, communicating political statements or merely offering information, all of which shall impact the players' lives beyond the gaming experience itself (4, 5). For decision makers and planners as well as scientists, Serious Games can be helpful tools since they \"(...) provide a means of identifying and evaluating the consequences of alternative plans and policies.\" (Mitgutsch & Alvarado 2012, p.11).\n<br/>\n[[File:Serious Gaming Types.png|800px|thumb|center|'''Different types of Serious Games, differentiated based on their purpose and field of application.''' One of the columns indicates the types of Serious Games used for Science and Research purposes. Source: The 'Games Taxonomy' by Sawyer and Smith (2008), available [http://edutechwiki.unige.ch/en/Serious_game here].]]\n<br/>\n* Games differ from simulations. While a simulation is a \"simplified, dynamic and accurate model of reality\" (Coovert et al. 2017, p.5 citing Adroitly), a game is a \"ficticious or artificial situation in which the player is expected to perform\" (ibid). The Serious Game may be seen as a combination of both forms: the simulation model no longer focuses on system analysis, but instead serves a learning purpose, adds the player as an actor amongst the other stakeholders, allows for interactive engagement with the model and is used in several different groups (3).\n* While games meant something different when Abt first defined the term, today, a 'game' mostly refers to a computer technology or video graphics based experience (2, 5). '''A Serious Game is a \"playful environment\"''' (Mitgutsch & Alvarado 2012, p.122) '''and more than the hardware or software it is based on, but rather the overall experience the player has while interacting with it''' (5). Relevant elements of a Serious Game include its purpose, its mechanics and rule-sets, fiction and narrative, its aesthetics and framing and its content (4). Also, debriefing at the end of the gaming experience (i.e. summarizing and reflecting upon it) is a crucial yet often neglected aspect of (Serious) Gaming (2).\n* '''The term 'serious' is debated, but reflects the usage of a video game for purposes other than pure entertainment''' (2). According to Abt, the term relates to \"matters of great interest and importance, raising questions not easily solved, and having important possible consequences\" (Abt 1970, p.10).\n* To be a proper tool for research, a Serious Game must have a) a valid representation of the real-world phenomenon of interest, b) a pedagogical component, and c) a gamification aspect that incentivizes and motivates the players (5). It can then be used as a form of qualitative data gathering where several stakeholders are brought together in a workshop format. In this workshop, the game helps them state problems (e.g. land-use conflicts), but also learn about the underlying issue and develop visions as well as shared solutions.\n\n\n== Strengths & Challenges ==\n* '''In general, positive impacts of games include analytical and spatial skills, strategic planning skills, better orientation in three-dimensional spaces, psychomotor skills, learning capabilities and more''' (8). When applied as a tool for planning and education, further advantages include:\n** Serious Gaming, compared to purely technical simulations, better acknowledges socio-technical challenges of complex systems (1).\n** \"Serious games combine the analytic and questioning concentration of the scientific viewpoint with the intuitive freedom and rewards of imaginative, artistic acts\" (4, p.12)\n** Games are cheap and motivating forms of education and training and allow the players to receive feedback on their actions. They enable the evaluators to easily assess the players' performances and provide more [[Glossary|creativity]] for the game designers without having the associated risks of implementing the simulated decisions in the real world (4).\n\n* '''More specifically, Serious Games may be an effective tool for research purposes''' (especially for research teams):\n** they offer flexible and consistent data gathering,\n** they enable teams to easily distribute new information within the team and provide feedback to the team members' reactions to this information, and\n** they facilitate debriefing (all from (5)).\n\n* However, there is currently still a lack of research on whether these alleged advantages really translate into practice (see Outlook).\n* On the negative side, the simplified representation of the analyzed issue of the Serious Game - while serving an educational purpose - can counteract a practical usage for solving real problems (1).\n* As Khaled & Vasalou (2014) highlight, Serious Games may impose a barrier for participatory design. They state that participants do not only need to be familiar with the topic at hand, but also possess gaming literacy, which may differ between user groups. Especially with more complex digital gaming applications, this may be an issue.\n\n\n== Normativity ==\n* Serious Gaming may be built upon other methodological approaches, such as [[Geographical Information Systems]] or forms of [[System Analysis]], including [[Social Network Analysis]] or [[System Thinking & Causal Loop Diagrams|Causal Loop Diagrams]].\n* Serious Gaming may be used as a [[Transdisciplinarity|transdisciplinary]] research method in stakeholder workshops. Thus, all notions of normativity in transdisciplinary research may apply, such as conflicting roles and challenges in problem framing.\n* It should be noted that the educational purpose of Serious Games attributes an important role of the method in the normative field of Sustainability Science. The information and framing that are part of the game serve a specific purpose, which may represent unanimous positions within the sustainability community, but may as well inflict opposing and harmful moral positions. The knowledge transported within a Serious Game should be fact-based, whereas the scope and direction of framing depends on the underlying purpose of the game which is contributed by the originator of the game (a company, an NGO, a government, an academic actor...). No matter the specific framing, this should be reflected upon when creating the game and assessing its effects (see (3)).\n\n\n== Outlook ==\n*  As a scientific method, Serious Gaming has existed for 50 years now, but there is still a lack of a homogeneous usage of the term, with common alternatives being 'digital learning games', 'game-based learning' or 'edutainment games', among others (2, 8). However, as Crookall (2) hopes, this ambiguity should not halt the further development of the method. He claims that the field of simulation and gaming may be moving towards being a full scientific discipline rather than only a set of methodological approaches (2). While the usage of Serious Games is currently widespread across different disciplines, a suggested name for the discipline-to-be is \"Game Science\" (5).\n* A relevant issue is the additional lack of research on the actual effectiveness of simulations and games in the respective fields of application (2, 4, 8). More research should be done in this field to help further the methodological development of Serious Gaming.\n* The decreased cost of game development, higher flexibility of the needed tools and the increased technological fidelity of the games have solved previous issues of games as a research tool (5). In 2010, it was mentioned that research contributions on Serious Gaming had been increasing lately (2). One may further assume that the ongoing monetary growth of the video game industry, the current diversification of game development through easier access to development tools, improvements in societal gaming literacy and acceptance for video games suggest a promising future for both the medium itself and its application in the contexts described in this article.\n\n\n== An exemplary study ==\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Title.png|450px|frameless|center|Serious Gaming - exemplary study - Wascher et al. 2010 - Title]]<br>\n'''The SUSMETRO project was a Serious Gaming project in the field of land-use planning''' that is described as follows in Wascher et al. 2010, p.22 (see References):\n\"By offering decision support tools such as maps, figures on sustainability and impact scenarios for stakeholders of metropolitan regions, SUSMETRO facilitates and enables evidence-based decision making by means of a \u2018serious game\u2019.\" More specifically, stakeholders were asked to decide for agricultural innovations and land-use types for specific areas in their respective metropolitan regions, and the game provided feedback on the outcomes of these land-use changes also on aspects such as recreational and nature conservation functions. In the end, the stakeholders were supposed to find a mix of agricultural land-use types that has the best outcome on the triple bottom-line of people, planet, and profit.\n\nTechnically, '''the SUSMETRO game was based on Excel and an adapted ArcGIS software which ran on a large touch-sensitive computer screen''' (Maptable). It was assisted by a facilitator and could be played by maximum 15 persons at once. On the Maptable, maps were displayed, based on which the stakeholders selected a region that represented their respective urban region. Then, first, they had to decide on what the region looks like now by selecting and defining different types of 'Landscape Character Areas', which were drawn onto the map. \n<br>\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Landscape Character Areas.png|600px|thumb|center|'''The defined 'Landscape Character Areas' that defined the Rotterdam region in the SUSMETRO workshop.''' Source: Wascher et al. 2010, p.40]]\n<br>\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Landscape Character Areas Map.png|600px|thumb|center|'''The identified 'Landscape Character Areas' on the map.''' Source: Wascher et al. 2010, p.41]]\n<br>\nIn Excel, then, the stakeholders could learn about four general types of 'Land Use Functions' with sub-types, which were pre-defined definitions of different kinds of land use, such as 'conventional agriculture (greenhouse production)', or 'recreation (low density recreation)'. Each definition entailed values and assumptions about these kinds of land-use, which could be discussed and adapted by the stakeholders. \n<br>\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Land Use Functions Map 1.png|600px|thumb|center|'''The Land Use Functions Map of the current state of the Rotterdam region.''' Source: Wascher et al. 2010, p.42]]\n<br>\nThe Land Use Functions for the current landscape could not be changed, but the '''stakeholders could subsequently draw their own distribution of land-use functions onto the map''' based on discussions about the potential, required, or desirable development of the area. These Land Use Functions were meant to provide more detailed information than the initial Landscape Character Areas, and more data to work with for the subsequent evaluation.\n<br>\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Land Use Functions Map 2.png|600px|thumb|center|'''The Land Use Functions map for the future as projected based on stakeholder input.''' Source: Wascher et al. 2010, p.43]]\n<br>\n'''Based on these drawings, ArcGIS and Excel could be used to calculate the consequences of these changed land-use types for the respective areas.'''\n<br>\n[[File:Serious Gaming - exemplary study - Wascher et al. 2010 - Assessment Results.png|600px|thumb|right|'''The results of the assessment of the current (T0) and projected (T1) land use in the  Rotterdam region, with regards to the the three areas of food and recreational supply, profits, and nature area.''' Source: Wascher et al. 2010, p.44]]\n<br>\nThe results from these calculations were displayed and provided information on\n* which % of the population in the region could be fed with the available or planned agricultural areas,\n* whether the amount of recreational space is sufficient for the local residents,\n* the profit gained in the region, based on the revenues from agriculture and recreation, substracted by the costs of nature management, and\n* what % of the intended remaining nature area could be reached with the planned land-use.\n\nAlthough the researchers discussed some shortcomings of their gaming approach, both technologically and methodologically, they highlighted good interaction by the stakeholders and helpful results for policy planning in the region.\n\n== Key publications ==\nAbt, C. 1970 (reprinted in 1987). ''Serious games''. University Press of America.\n* The first work that established the term Serious Game.\n\nSusi, T. Johannesson, M. Backlund, P. 2007. ''Serious Games - An Overview''. Technical Report HS-IKI-TR-07-001.\n* A (by now outdated, but still worthwhile) summary of the concept, advantages and application of Serious Games.\n\nWascher et al. 2010. ''SUSMETRO. Impact Assessment Tools for Food Planning in Metropolitan Regions: IA tools and serious gaming in support of sustainability targets for food planning, nature conservation and recreation.'' Alterra.\n* A report on the development and implementation of a Serious Game with the purpose of allowing \"a wide range of stakeholders to co-develop images for sustainable Metropolitan Agriculture.\"\n\n\n== References ==\n(1) Savic, D.A. Morley, M.S. Khoury, M. 2016. ''Serious Gaming for Water Systems Planning and Management.'' Water 8. 456-467.\n\n(2) Crookall, D. 2010. ''Serious Games, Debriefing, and Simulation/Gaming as a Discipline.'' Simulation & Gaming 41(6). 898-920.\n\n(3) van der Zee, D-J. Holkenborg, B. 2010. ''Conceptual Modelling For Simulation-Based Serious Gaming''. Proceedings of the 2010 Winter Simulation Conference.\n\n(4) Mitgutsch, K. Alvarado, N. 2012. ''Purposeful by design? A Serious game design assessment framework.'' Proceedings of the International Conference on the foundations of digital games. 121-128.\n\n(5) Coovert, M.D. et al. 2017. ''Serious games are a serious tool for team research.'' International Journal of Serious Games 4(1).\n\n(6) Abt, C. 1970 (reprinted in 1987). ''Serious games.'' University Press of America.\n\n(7) Wascher et al. 2010. ''SUSMETRO. Impact Assessment Tools for Food Planning in Metropolitan Regions: IA tools and serious gaming in support of sustainability targets for food planning, nature conservation and recreation.'' Alterra.\n\n(8) Susi, T. Johannesson, M. Backlund, P. 2007. ''Serious Games - An Overview.'' Technical Report HS-IKI-TR-07-001.\n\n(9) Khaleda, R. Vasalou. 2014. ''Bridging Serious Games and Participatory Design''. Preprint submitted to Child Computer Interaction.\n\n\n== Further Information ==\n* The ESPREssO project, which included a board game on disaster risk reduction and climate change adapation, is described [https://www.dkkv.org/de/serious-gaming/ here] (German language).\n* [https://www.rwth-aachen.de/cms/root/Studium/Lehre/Digitalisierungsstrategie-der-Lehre/Blended-Learning-Formate/~oiav/Serious-Games/ Some information] (also in German) on two serious gaming projects by RWTH Aachen on the topics of managing a car manufacturer and for medicine training.\n----\n[[Category:Qualitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "la41oqkg3d6ymh1ltke4s4c8a5vvggj"
                }
            },
            {
                "title": "Simple Statistical Tests",
                "ns": "0",
                "id": "551",
                "revision": {
                    "id": "6370",
                    "parentid": "6369",
                    "timestamp": "2021-09-14T06:55:36Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23858",
                        "#text": "[[File:SimpleTestsCategorisation 1.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization for Simple Statistical Tests]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>\n<br/>\n<br/>\n'''In short:''' Simple statistical tests encapsule an array of simple statistical tests that are all built on probability, and no other validation criteria.\n__NOTOC__\n\n== Background ==\nSimple statistical tests statistics provide the baseline for advanced statistical thinking. While they are not so much used today within empirical analysis, simple tests are the foundation of modern statistics. The student t-test which originated around 100 years ago provided the crucial link from the more inductive thinking of Sir Francis Bacon towards the testing of hypotheses and the actual statistical testing of [[Glossary|hypotheses]]. The formulation of the so-called null hypothesis is the first step within simple tests. Informed from theory this test calculates the probability whether the sample confirms the hypothesis or not. Null hypotheses are hence the [[Glossary|assumptions]] we have about the world, and these assumptions can be confirmed or rejected.\n\nThe following information on simple statistical tests assumes some knowledge about data formats and data distribution. If you want to learn more about these, please refer to the entries on [[Data formats]] and [[Data distribution]].\n\n\n== Most relevant simple tests ==\n====One sample t-test====\n'''The easiest example is the [https://www.youtube.com/watch?v=VPd8DOL13Iw one sample t-test]''': it allows us to test a dataset (more specifically, its mean value) versus a specified value. For this purpose, the t-test gives you a p-value at the end. If the p-value is below 0.05, the sample differs significantly from the reference value. Important: The data of the sample(s) has to be normally distributed.\n\n'''Example''': Do the packages of your favourite cookie brand always contain as many cookies as stated on the outside of the box? Collect some of the packages, weigh the cookies contained therein and calculate the mean weight. Now, you can compare this value to the weight that is stated on the box using a one sample t-test. \n\nFor more details and R examples on t-tests, please refer to the [[T-Test]] entry.\n\n\n====Two sample t-test====\n[[File:Bildschirmfoto 2020-04-26 um 17.57.43.png|thumb|With a two sample test we can compare the mean gross of both our groups - with or without fertilizer - and state whether they are significantly different.]]\n'''[https://www.youtube.com/watch?v=NkGvw18zlGQ Two sample tests] are the next step'''. These allow a [https://explorable.com/independent-two-sample-t-test comparison of two different datasets] within an experiment. They tell you if the means of the two datasets differ significantly. If the p-value is below 0,05, the two datasets differ significantly. It is clear that the usefulness of this test widely depends on the number of samples - the more samples we have for each dataset, the more we can understand about the difference between the datasets.\n\nImportant: The data of the sample(s) has to be normally distributed. Also, the kind of t-test you should apply depends on the variance in the parent populations of the samples. For a '''Student\u2019s t-test''', equal variances in the two groups are required. A '''Welch t-test''', by contrast, can deal with samples that display differing variances (1). To know whether the datasets have equal or varying variances, have a look at the F-Test.\n\n'''Example:''' The classic example would be to grow several plants and to add fertiliser to half of them. We can now compare the gross of the plants between the control samples without fertiliser and the samples that had fertiliser added. \n\n''Plants with fertiliser (cm):''  7.44 6.35 8.52 11.40 10.48 11.23 8.30 9.33 9.55 10.40 8.36 9.69 7.66 8.87 12.89 10.54 6.72 8.83 8.57 7.75\n\n''Plants without fertiliser (cm):'' 6.07 9.55 5.72 6.84 7.63 5.59 6.21 3.05 4.32 8.27 6.13 7.92 4.08 7.33 9.91 8.35 7.26 6.08 5.81 8.46\n\nThe result of the two-sample t-test is a p-value of 7.468e-05, which is close to zero and definitely below 0,05. Hence, the samples differ significantly and the fertilizer is likely to have an effect.\n\nFor more details on t-tests, please refer to the [[T-Test]] entry.\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for two-sample t-Tests.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for a two-sample t-test.\n#The two-sample t-test can be used to test if two samples of a population differ significantly regarding their means.\n#H0 hypothesis: The two samples' means are not different.\n#H1 hypothesis: The two samples' means are different.\n\n#We want to analyse the iris Dataset provided by R\niris <- iris\n\n#Lets have a first look on the Data\nhead(iris)\nstr(iris)\nsummary(iris)\n#The dataset gives the measurements in centimeters of the variables 'sepal length' and 'sepal width' \n#and 'petal length' and 'petal width', \n#respectively, for 50 flowers from each of 3 species of Iris (Iris setosa, versicolor, and virginica).\n\n#Lets split the dataset into 3 datasets for each species:\nversicolor <- subset(iris, Species == \"versicolor\")\nvirginica <- subset(iris, Species == \"virginica\")\nsetosa <- subset(iris, Species == \"setosa\")\n\n#Let us test if the difference in petal length between versicolor and virginica is significant:\n#H0 hypothesis: versicolor and virginica do not have different petal length\n#H1 hypothesis: versicolor and virginica have different petal length\n\n#What does the t-test say?\nt.test(versicolor$Petal.Length,virginica$Petal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it's almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis. \n\n#--------------additional remarks on Welch's t-test----------------------\n#The widely known Student's t-test requires equal variances of the samples.\n#Let's check whether this is the case for our example from above by means of an f-test.\n#If variances are unequal, we could use a Welch t-test instead, which is able to deal with unequal variances of samples.\nvar.test(versicolor$Petal.Length, virginica$Petal.Length)\n\n#the resulting p-value is above 0.05 which means the variances of the samples are not different.\n#Hence, this would justify the use of a Student's t-test. \n#Interestingly, R uses the Welch t-test by default, because it can deal with differing variances. Accordingly, what we performed above is a Welch t-test.\n#As we know that the variances of our samples do not differ significantly, we could use a Student's t-test.\n#Therefore, we change the argument \"var.equal\" of the t-test command to \"TRUE\". R will use the Student's t-test now.\n\nt.test(versicolor$Petal.Length,virginica$Petal.Length, var.equal = TRUE)\n\n#In comparison to the above performed Welch t-test, the df changed a little bit, but the p-value is exactly the same.\n#Our conclusion therefore stays the same. What a relief. \n</syntaxhighlight>\n|}\n\n\n====Paired t-test====\n'''[https://www.statisticssolutions.com/manova-analysis-paired-sample-t-test/ Paired t-tests] are the third type of simple statistics.''' These allow for a comparison of a sample before and after an intervention. Within such an experimental setup, specific individuals are compared before and after an event. This way, the influence of the event on the dataset can be evaluated. If the sample changes significantly, comparing start and end state, you will receive again a p-value below 0,05.\n\nImportant: for the paired t-test, a few assumptions need to be met.\n* Differences between paired values follow a [[Data distribution|normal distribution]].\n* The data is continuous.\n* The samples are paired or dependent.\n* Each unit has an equal probability of being selected\n\n'''Example:''' An easy example would be the behaviour of nesting birds. The range of birds outside of the breedig season dramatcally differs from the range when they are nesting. \n\nFor more details on t-tests, please refer to the [[T-Test]] entry.\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example on paired t-tests.\n|-\n|<syntaxhighlight lang=\"R\" line>\n\n# R Example for a Paired Samples T-Test\n# The paired t-test can be used when we want to see whether a treatment had a significant effect on a sample, compared to when the treatment was not applied. The samples with and without treatment are dependent, or paired, with each other.\n\n# Setting up our Hypotheses (Two-Tailed):\n# H0: The pairwise difference between means is 0 / Paired Population Means are equal\n# H1: The pairwise difference between means is not 0 / Paired Population Means are not equal\n\n# Example:\n# Consider the following supplement dataset to show the effect of 2 supplements on the time \n#it takes for an athlete (in minutes) to finish a practice race\n\ns1 <- c(10.9, 12.5, 11.4, 13.6, 12.2, 13.2, 10.6, 14.3, 10.4, 10.3)\ns2 <- c(15.7, 16.6, 18.2, 15.2, 17.8, 20.0, 14.0, 18.7, 16.0, 17.3)\nid <- c(1,2,3,4,5,6,7,8,9,10)\nsupplement <- data.frame(id,s1,s2)\nsupplement$difference <- supplement$s1 - supplement$s2\nView(supplement)\n\n# Exploring the data\nsummary(supplement)\nstr(supplement)\n\n# Now, we can go for the Paired Samples T-Test\n\n# We can define our Hypotheses as (Two-Tailed):\n# H0: There is no significant difference in the mean time taken to finish the race\n#     for the 2 supplements\n# H1: There is a significant difference in the mean time taken to finish the race\n#     for the 2 supplements\n\nt.test(supplement$s1, supplement$s2, paired = TRUE)\n\n# As the p-value is less than the level of significance (0.05), we have\n# sufficient evidence to reject H0 and conclude that there is a significant \n# difference in the mean time taken to finish the race with the 2 supplements\n\n# Alternatively, if we wanted to check which supplement gives a better result, \n# we could use a one tailed test by changing the alternate argument to less or greater\n</syntaxhighlight>\n|}\n\n\n====Chi-square Test of Stochastic Independence====\n[[File:Chi square example.png|frameless|500px|right|Source: John Oliver Engler]]\nThe Chi-Square Test can be used to check if one variable influences another one, or if they are independent of each other. The Chi Square test works for data that is only categorical.\n\n'''Example''': Do the children of parents with an academic degree visit a university more often, for example because they have higher chances to achieve good results in school? The table on the right shows the data that we can use for the Chi-Square test.\n\nFor this example, the chi-quare test yields a p-value of 2.439e-07, which is close to zero. We can reject the null hypothesis that there is no dependency, but instead assume that, based on our sample, the education of parents has an influence on the education of their children.\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the Chi-Square Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n\n#R example for Chi-Square Test\n#We create a table in which two forests are differentiated according to their distribution of male, female and juvenile trees.\n\nchi_sq<-as.table(rbind(c(16,20,19), c(23,12,10)))\ndimnames(chi_sq)<- list(forest= c(\"ForestA\",\"ForestB\"), gender= c(\"male\", \"female\", \"juvenile\"))\n\nView(chi_sq)\n\n#The question is, whether the abundance of trees in these forests is random (i.e. stochastically independent) or do the two variables (forest, gender) influence each other?\n#For example, is it just coincidence that there are 23 male trees in forst B but only 16 in forest A?\n\n#H0:there is no dependence between the variables\n#H1:there is a dependency\n\nchisq.test(chi_sq)\n\n#Results of the Pearson's Chi-squared test\ndata: chi_sq\nX-squared = 5.1005, df = 2, p-value = 0.07806\n\n#p-value > 0.05 -> We cannot reject hypothesis H0.\n#Thus, there is probably no dependency between the variables.\n\n</syntaxhighlight>\n|}\n\n\n====Wilcoxon Test====\n'''The next important test is the [https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon rank sum test]'''. This test is also a paired test. What is most relevant here is that not the real numbers are introduced into the calculation, but instead these numbers are transformed into ranks. In other words, you get rid of the question about normal distribution and instead reduce your real numbers to an order of numbers. This can come in handy when you have very skewed distribution - so a exceptionally non-normal distribution - or large gaps in your data. The test will tell you if the means of two samples differ significantly (i.e. p-value below 0,05) by using ranks.\n\n'''Example:''' An example would be the comparison in growths of young people compared to their size as adults. Imagine you have a sample where half of your people are professional basketball players then the real size of people would not make sense. Therefore, as a robust measure in modern statistics, rank tests were introduced.\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the Wilcoxon Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an Wilcoxon rank-sum test\n#We want to analyse the iris Dataset provided by R\niris <- iris\n\n#Lets have a first look on the Data\nhead(iris)\nstr(iris)\nsummary(iris)\n#The dataset gives the measurements in centimeters of the variables sepal length and width \n#and petal length and width, \n#respectively, for 50 flowers from each of 3 species of iris. \n#The species are Iris setosa, versicolor, and virginica.\n\n#lets split the dataset into 3 datasets for each species\nversicolor <- subset(iris, Species == \"versicolor\")\nvirginica <- subset(iris, Species == \"virginica\")\nsetosa <- subset(iris, Species == \"setosa\")\n\n#Now we can for example test, if the difference in sepal length between setosa and virginica is significant:\n#H0 hypothesis: The medians (distributions) of setosa and virginica are equal\n#H1 hypothesis: The medians (distributions) of setosa and virginica differ\n\n#test for normality\nshapiro.test(setosa$Sepal.Length)\nshapiro.test(virginica$Sepal.Length)\n#both are normally distributed\n\n#wilcoxon sum of ranks test\nwilcox.test(setosa$Sepal.Length,virginica$Sepal.Length)\n\n#the p-value is 2.2e-16, which is below 0.05 (it's almost 0). \n#Therefore we neglect the H0 Hypothesis and adopt the H1 Hypothesis.\n#Based on this result we may conclude the medians of these two distributions differ. \n#The alternative hypothesis is stated as the \u201ctrue location shift is not equal to 0\u201d. \n#That\u2019s another way of saying \u201cthe distribution of one population is shifted to the left or \n#right of the other,\u201d which implies different medians. \n\n</syntaxhighlight>\n|}\n\n\n====f-test====\n'''The f-test allows you to compare the ''variance'' of two samples.''' Variance is calculated by taking the average of squared deviations from the mean and tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean. If the p-value of the f-test is lower than 0,05, the variances differ significantly. Important: for the f-Test, the data of the samples has to be normally distributed. \n\n'''Example''': If you examine players in a basketball and a hockey team, you would expect their heights to be different on average. But maybe the variance is not. Consider Figure 1 where the mean is different, but the variance the same - this could be the case for your hockey and basketball team. In contrast, the height could be distributed as shown in Figure 2. The f-test then would probably yield a p-value below 0,05.\n\n[[File:Normal distribution.jpg|400px|thumb|left|Figure 1 shows '''two datasets which are normally distributed, but shifted.''' Source: [https://snappygoat.com/s/?q=bestof%3ALda-gauss-variance-small.svg+en+Plot+of+two+normal+distributed+variables+with+small+variance+de+Plot+zweier+Normalverteilter+Variablen+mit+kleiner+Varianz#7c28e0e4295882f103325762899f736091eab855,0,3 snappy goat]]]\n[[File:NormalDistribution2.png|400px|thumb|right|Figure 2 '''shows two datasets that are normally distributed, but have different variances'''. Source: [https://www.notion.so/sustainabilitymethods/Simple-Statistical-Tests-bcc0055304d44564bc41661453423134#7d57d8c251f94da8974eeb8a658aaa29 Wikimedia]]]\n\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"width: 100%; background-color: white\"\n|-\n! Expand here for an R example for the f-Test.\n|-\n|<syntaxhighlight lang=\"R\" line>\n#R example for an f-Test.\n#We will compare the variances of height of two fictive populations. First, we create two vectors with the command 'rnorm'. Using rnorm, you can decide how many values your vector should contain, besides the mean and the standard deviation of the vector. To learn, what else you can do with rnorm, type:\n?rnorm\n\n#Creating two vectors\nPopA=rnorm(40, mean=175, sd=1)\nPopB=rnorm(40, mean=182, sd=2)\n\n#Comparing them visually by creating histograms\nhist(PopA)\nhist(PopB)\n\n#Conducting a f-test to compare the variances\nvar.test(PopA, PopB) \n\n#And this is the result, telliing you that the two variances differ significantly\nF test to compare two variances\n\ndata:  PopA and PopB\nF = 0.38584, num df = 39, denom df = 39, p-value = 0.00371\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2040711 0.7295171\nsample estimates:\nratio of variances \n         0.3858411 \n</syntaxhighlight>\n|}\n\n\n== Normativity & Future of Simple Tests ==\n'''Simple tests are not abundantly applied these days in scientific research, and often seem outdated.''' Much of the scientific designs and available datasets are more complicated than what we can do with simple tests, and many branches of sciences established more complex designs and a more nuanced view of the world. Consequently, simple tests grew kind of out of fashion.\n\nHowever, simple tests are not only robust, but sometimes still the most parsimonious approach. In addition, many simple tests are a basis for more complicated approaches, and initiated a deeper and more applied starting point for frequentist statistics. \n\nSimple tests are often the endpoint of many introductionary teachings on statistics, which is unfortunate. Overall, their lack in most of recent publications as well as wooden design frames of these approaches make these tests an undesirable starting point for many students, yet they are a vital stepping stone to more advanced models.\n\nHopefully, one day school children will learn simple test, because they could, and the world would be all the better for it. If more people early on would learn about probability, and simple tests are a stepping stone on this long road, there would be an education deeper rooted in data and analysis, allowing for better choices and understanding of citizens.\n\n\n== Key Publications ==\n* Student\" William Sealy Gosset. 1908. ''The probable error of a mean.'' Biometrika 6 (1). 1\u201325.\n* Cochran, William G. 1952. ''The Chi-square Test of Goodness of Fit''. The Annals of Mathematical Statistics 23 (3). 315\u2013345. \n* Box, G. E. P. 1953. ''Non-Normality and Tests on Variances.'' Biometrika 40 (3/4). 318\u2013335.\n\n\n== References ==\n(1) [https://en.wikipedia.org/wiki/Student's_t-test Article on the \"Student's t-test\" on Wikipedia]\n\n\n== Further Links ==\n* Videos\n[https://www.youtube.com/watch?v=HZ9xZHWY0mw The Hypothesis Song]: A little musical introduction to the topic\n\n[https://www.youtube.com/watch?v=ZzeXCKd5a18 Hypothesis Testing]: An introduction of the Null and Alternative Hypothesis\n\n[https://www.youtube.com/watch?v=ptADSmJCVwQ The Scientific Method]: The musical way to remember it\n\n[https://www.youtube.com/watch?v=wf-sGqBsWv4 Popper's Falsification]: The explanation why not all swans are white\n\n[https://www.youtube.com/watch?v=a_l991xUAOU Type I & Type II Error]: A quick explanation\n\n[https://www.youtube.com/watch?v=F6LGa8jsdjo Validity]: An introduction to the concept\n\n[https://www.youtube.com/watch?v=m0W8nnupcUk Reliability]: A quick introduction\n\n[https://www.youtube.com/watch?v=tFWsuO9f74o The Confidence Interval]: An explanation with vivid examples\n\n[https://www.youtube.com/watch?v=rulIUAN0U3w Choosing which statistical test to use]: A very detailed videos with lots of examples\n\n[https://www.youtube.com/watch?v=VPd8DOL13Iw One sample t-test]: An example calculation\n\n[https://www.youtube.com/watch?v=NkGvw18zlGQ Two sample t-test]: An example calculation\n\n[https://www.youtube.com/watch?v=QZ7kgmhdIwA Introduction into z-test & t-test]: A detailed video\n\n[https://www.youtube.com/watch?v=7_cs1YlZoug Chi-Square Test]: Example calculations from the world of e-sports\n\n[https://www.youtube.com/watch?v=FlIiYdHHpwU F test]: An example calculation\n\n* Articles\n[https://www.sciencedirect.com/science/article/pii/S0092867408009537 History of the Hypothesis]: A brief article through the history of science\n\n[https://journals.sagepub.com/doi/pdf/10.1177/014107680309601201 The James Lind Initiative]: One of the earliest examples for building hypotheses\n\n[https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology The Scientific Method]: A detailed and vivid article\n\n[https://www.brgdomath.com/philosophie/erkenntnistheorie-tk10/falsifikation-popper/ Falsification]: An introduction to Critical Rationalism (German)\n\n[https://explorable.com/statistical-validity Statistical Validity]: An overview of all the different types of validity\n\n[https://www.scribbr.com/methodology/reliability-vs-validity/ Reliability & Validity]: An article on their relationship\n\n[https://explorable.com/statistical-reliability Statistical Reliability]: A brief article\n\n[https://www.statisticssolutions.com/directory-of-statistical-analyses-reliability-analysis/ Reliability Analysis]: An overview about different approaches\n\n[https://opinionator.blogs.nytimes.com/2012/05/17/how-reliable-are-the-social-sciences/ How reliable are the Social Sciences?]: A short article by The New York Times\n\n[https://www.visionlearning.com/en/library/Process-of-Science/49/Uncertainty-Error-and-Confidence/157 Uncertainty, Error & Confidence]: A very long & detailed article\n\n[https://www.britannica.com/science/Students-t-test Student t-test]: A detailed summary\n\n[https://www.statisticssolutions.com/manova-analysis-one-sample-t-test/ One sample t-test]: A brief explanation\n\n[https://explorable.com/independent-two-sample-t-test Two sample t-test]: A short introduction\n\n[https://www.statisticssolutions.com/manova-analysis-paired-sample-t-test/ Paired test]: A detailed summary\n\n[https://www.mathsisfun.com/data/chi-square-test.html Chi-Square Test]: A vivid article\n\n[https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon Rank Sum Test]: A detailed example calculation in R\n\n[http://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r F test]: An example in R\n\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Henrik von Wehrden and Carlo Kr\u00fcgermeier."
                    },
                    "sha1": "13rch0m8z8sz7sln7dhrn6w6z9g9uch"
                }
            },
            {
                "title": "Skills & Tools",
                "ns": "0",
                "id": "353",
                "revision": {
                    "id": "5914",
                    "parentid": "5751",
                    "timestamp": "2021-06-28T15:15:22Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2139",
                        "#text": "'''Skills & Tools are approaches, ideas, tips or gimmicks that can help improve your work.''' They can support individuals as well as small to large groups to become more efficient, creative and supportive co-workers, researchers, students, collaborators, leaders, teachers and moderators.\n__NOTOC__\n\n'''The Skills & Tools on this Wiki belong to one or more of the following groups.''' You can click on each category to be redirected to an overview of all respective entries. Each entry includes information on the group size that it can be used for.\n'''{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = Collaborative Tools | class = center |\n|content =\n<nowiki></nowiki>\n<imagemap>Image:Teamwork - Image for Skills & Tools.jpg|300px\nrect 32 8 1880 1172 [[:Category:Collaborative Tools|Collaborative Tools support communication, cooperation and shared idea development.]]\n</imagemap>\n}}\n\n{{InfoCard\n|heading = Software | class = center |\n|content =\n<nowiki></nowiki>\n<imagemap>Image:Software - Image for Skills & Tools.jpg|300px\nrect 32 8 1880 1172 [[:Category:Software|Software solutions make work-life easier.]]\n</imagemap>\n}}\n\n\n}}\n\n{{ContentGrid\n|content = \n<!--\nnowiki markers are added so the first bullet in the list are rendered correctly!\nplease don't remove those markers, thank you\n-->\n\n{{InfoCard\n|heading = Personal Skills | class = center |\n|content =\n<nowiki></nowiki>\n<imagemap>Image:Personal Skills - Image for Skills & Tools.jpg|300px\nrect 32 8 1880 1172 [[:Category:Personal Skills|Personal Skills help you become better in communicating, writing, organizing and learning.]]\n</imagemap>\n}}\n\n{{InfoCard\n|heading = Productivity Tools | class = center |\n|content =\n<nowiki></nowiki>\n<imagemap>Image:Productivity Tools - Image for Skills & Tools.jpg|300px\nrect 32 8 1880 1172 [[:Category:Productivity Tools|Productivity Tools boost your productivity and make work more fun.]]\n</imagemap>\n}}\n\n}}\n\nFor all Skills & Tools entries, please visit [[:Category:Skills and Tools|this overview page]]."
                    },
                    "sha1": "0cu3nkm951ug7ellxvg35t9jvjkmpku"
                }
            },
            {
                "title": "Social Network Analysis",
                "ns": "0",
                "id": "558",
                "revision": {
                    "id": "6859",
                    "parentid": "6307",
                    "timestamp": "2022-11-20T12:53:46Z",
                    "contributor": {
                        "username": "MeliF",
                        "id": "162"
                    },
                    "minor": null,
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "22350",
                        "#text": "[[File:ConceptSocialNetworkAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Social Network Analysis]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n|'''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]\n\n'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions.\" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|'''Moreno's original work on Social Networks.''' Source: Borgatti et al. 2009, p.892]]\n\nRomanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).\n\nMoreno's and Jennings' work was subsequently taken up and furthered as the field of ''''group dynamics', which was highly relevant in the US in the 1950s and 1960s.''' Simultaneously, sociologists and anthropologists further developed the approach in Britain. \"The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties.\" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).\n\n\n== What the method does ==\n\"Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm.\" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.\n\nSocial Network Analysis is based on the idea that \"(...) social life is created primarily and most importantly by relations and the patterns formed by these relations. '''Social networks are formally defined as a set of nodes (or network members) that are tied by one or more types of relations.\"''' (Marin & Wellman 2010, p.1; Scott 1988). These network members are also commonly referred to as \"entitites\", \"actors\", \"vertices\" or \"agents\" and are most commonly persons or organizations, but can in theory be anything (Marin & Wellman 2010). The nodes are \"(...) tied to one another through socially meaningful relations\" (Prell et al. 2009, p.503), which can be \"(...) collaborations, friendships, trade ties, web links, citations, resource flows, information flows (...) or any other possible connection\" (Marin & Wellman 2010, p.2). It is important to acknowledge that each node can have different relations to all other nodes, spheres and levels of the network. Borgatti et al. (2009) refer to four types of relations in general: similarities, social relations, interactions, and flows.\n\n[[File:Social Network Analysis Type of Ties.png|800px|thumb|center|'''Types of Ties in a Social Network.''' Source: Borgatti et al. 2009, p.894]]\n\nThe Social Network Analyst then analyzes these relations \"(...) for structural patterns that emerge among these actors. Thus, an analyst of social networks looks beyond attributes of individuals to also examine the relations among actors, how actors are positioned within a network, and how relations are structured into overall network patterns.\" (Prell et al. 2009, p.503). '''Social Network Analysis is thus not the study of relations between individual pairs of nodes, which are referred to as \"dyads\", but rather the study of patterns within a network.''' The broader context of each connection is of relevance, and interactions are not seen independently but as influenced by the adjacent network surrounding the interaction. This is an important underlying assumption of Social Network Theory: the behavior of similar actors is based not primarily on independently shared characteristics between different actors within a network, but rather merely correlates with these attributes. Instead, it is assumed that the actors' behavior emerges from the interaction between them: \"Their similar outcomes are caused by the constraints, opportunities, and perceptions created by these similar network positions.\" (Marin & Wellman 2010, p.3). Surrounding actors may provide leverage or influence that affect the agent's actions (Borgatti et al. 2009)\n\n==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa.\n* '''Network boundaries:''' In a next step, the approach to defining nodes needs to be chosen. Three ways of defining networks can be named according to Marin & Wellman (2010, p.2, referring to Laumann et al. (1983)). These three are approaches not mutually exclusive and may be combined:\n** ''position-based approach'': considers those actors who are members of an organization or hold particular formally-defined positions to be network members, and all others would be excluded\n** ''event-based'' approach: those who had participated in key events are believed to define the population\n** ''relation-based approach'': begins with a small set of nodes deemed to be within the population of interest and then expands to include others sharing particular types of relations with those seed nodes as well as with any nodes previously added.\n** Butts (2008) adds the ''exogenously defined boundaries'', which are pre-determined based on the research intent or theory which provide clearly specified entities of interest.\n* '''Type of ties:''' Then, the researcher needs to decide on which kinds of ties to focus. There can be two forms of ties between network nodes: ''directed'' ties, which go from one node to another, and ''undirected ties'', that connect two nodes without any distinct direction. Both types can either be [[Data formats|binary]] (they exist, or do not exist), or valued (they can be stronger or weaker than other ties): As an example, \"(..) a friendship network can be represented using binary ties that indicate if two people are friends, or using valued ties that assign higher or lower scores based on how close people feel to one another, or how often they interact.\" (Marin & Wellman 2010, p.14; Borgatti et al. 2009)\n* '''Data Collection''': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a ''prompted recall'' approach, they are asked which people they would think of with regards to a specific topic (e.g. \"To whom would you go for advice at work?\") while they are shown a pre-determined list of potentially relevant individuals. In the ''free list'' approach, they are asked to recall individuals without seeing a list (Butts 2008, p.20f).\n* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition.\n* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the '''visual representation''' of the network. For this, the researcher \"(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections.\" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the \"number of lines which it is necessary to traverse in order to get from one point to another.\" (Scott 1988, p.114)\n[[File:Social Network Analysis - Network Types.png|400px|thumb|right|'''Different network structures.''' Source: Borgatti et al. 2009, p.893]]\n[[File:Social Network Analysis - Example.png|300px|thumb|center|'''An exemplary network structure.''' The dyads BE and BF - i.e. the connections between B and E, and B and F, respectively - are equally long in this network although BF appears to be shorter. This is due to the visual representation of the network, where B and F are closer to each other. Additionally, the central role of A becomes clear. Source: Scott 1988, p.114]]\n\n== Strengths & Challenges ==\n* There is a range of challenges in the gathering of network data through [[Semi-structured Interview|Interviews]] and [[Survey|Surveys]], which can become long and cumbersome, and in which the interviewees may differently understand and recall their relations with other actors, or misinterpret the connections between other actors. (Marin & Wellman 2010, p.15)\n* The definition of network boundaries is crucial, since \"(...) the inappropriate inclusion or exclusion of a small number of entities can have ramifications which extend well beyond those entities themselves\". Apart from the excluded entities and their relations, all relations between these entities and the rest of the network, and thus the network's structural properties, are affected. (Butts 2008). For more insights on the topic of System Boundaries, please refer to [[System Boundaries|the respective article]].\n\n\n== Normativity ==\n* The structure of any network and thus the conclusions that can be drawn in the analysis very much depend on the relation that is observed. A corporation may be differently structured in terms of their informal compared to their official communication structures, and an individual may not be part of one network but central in another one that focuses on a different relational quality (Butts 2008)\n* Further, the choice of network boundaries as well as the underlying research intent can have normative implications. Also, actors within the network may be characterized using specific attributes, which may be a normative decision (such as for attributes of ethnicity, violence, or others).\n* The way in which a Social Network is visualized plays an important role. Researchers may choose from a variety of visualization forms in terms of symmetry, distribution and color of the represented network structures. It is important to highlight that these choices heavily influence how a Social Network is perceived, and that properly visualizing the available data is a matter of experience.\n* The method of social network analysis is connected to the methods Stakeholder Analysis as well as [[Clustering Methods|Clustering]]. Further, as mentioned above, the data necessary for Social Network Analysis can be gathered in [[Survey|Surveys]] or [[Semi-structured Interview|Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] or similar methods of data gathering. Last, the whole idea of analyzing systemic interactions between actors is the foundational idea of [[System Thinking & Causal Loop Diagrams|Systems Thinking.]]\n\n\n== An exemplary study ==\n[[File:Social Network Analysis - Lam et al. 2021 - Exemplary study title.png|600px|frameless|center|Title of Lam et al. 2021]]\n'''In their 2021 publication, researchers from Leuphana (Lam et al., see References) investigated the network ties between 32 NGOs driving sustainability initiatives in Southern Transylvania.''' Based on the [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226895/ [[Glossary|leverage points]] concept], they attempted to identify how these NGOs contribute to systemic change. For this, they focused on the positions of the NGOs in different networks, with each network representing relations that target different system characteristics (parameters, feedbacks, design, intent). As a basis for this, the authors identified types of ties between the NGOs that contribute to each of these four system characteristics.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Leverage Points.png|900px|frameless|center|Leverage Points and Social Network Analysis. From Lam et al. 2021]]<br>\n\nFurther, they investigated the amplification processes the NGOs applied to their initiatives to foster transformative change, grouped into four kinds of amplification processes: ''within'', ''out (dependent)'', ''out (independent)'' and ''beyond''.\n\n[[File:Social Network Analysis - Lam et al. 2021 - Social Networks and Amplification.png|750px|frameless|center|Survey questions on amplification processes by NGOs in Social Network Analysis on Leverage Points. Source: Lam et al. 2021]]<br>\n\nBased on these conceptual structures, '''the authors conducted an online survey''' in which 30 NGOs participated. In this survey, the NGOs were asked about their relations to other NGOs in terms of the kinds of questions shown above relating to the four leverage points. The NGOs were asked to rate the strength of the respective relations to another NGOs over the past five years with either \"not at all\" (0), low extent (1), moderate extent (2), high extent (3), and \"I don't know\". Further, the survey asked the NGOs four questions about the four kinds of amplification as shown above. \n\nWith the survey data, the authors created four networks using [https://gephi.org/ Gephi] and [https://nodexl.com/ NodeXL] software, one for each system characteristic. '''For each NGO (= node), they calculated three measures''' (Lam et al. 2021, p.816):\n* the ''weighted degree'', which measures the relations of the node to other nodes in the network, taking into consideration the weight of the relations. This measure provides insights on each node's individual interconnectedness.\n* the ''betweenness'', which highlights how often a node links other nodes that would otherwise be unconnected. The higher the betweenness of a node is, the more power it exerts on the network. (By the way, the node with the highest betweenness is often called a 'broker').\n* the ''eigenvector centrality'', which measures the influence of a node in the network, weighted by the influence of its neighboring nodes. This highlights the future influence of a node.\n\nThey additionally tested for the influence of each NGO's amplification actions on these network measures by comparing those NGOs that applied a specific amplification action to those that did not, using a Mann-Whitney U test (also known as [[Simple_Statistical_Tests#Wilcoxon_Test|Wilcoxon rank-sum test]]).\n\nIn their results, they \"(...) found that while some NGOs had high centrality metrics across all four networks (...), other NGOs had high weighted degree, betweenness, or eigenvector in one particular network.\" (p.817)\n[[File:Social Network Analysis - Lam et al. 2021 - Result visualisations.png|750px|frameless|center|Results from Lam et al. 2021, p.817]]\n\nWithout going into too many details at this point, we can see that the created networks tell us a lot about different aspects of the relation between actors. Based on the shown and further results, the authors concluded that\n# actors (NGOs) may have central roles either concerning all kinds of networks, or just in specific networks,\n# actors that amplify their own impact actively are potentially more central in networks, and\n# that Social Network Analysis with a leverage points perspective can help identify how and which actors play an important role for different kinds of sustainability transformations.\n\nFor more on this study, please refer to the References.\n\n\n== Key publications ==\n* Scott, J. 1988. ''Trend Report Social Network Analysis''. Sociology 22(1). 109-127.\n* Borgatti, S.P. et al. 2009. ''Network Analysis in the Social Sciences.'' Science 323. 892-895.\n* Rowley, TJ. 1997. ''Moving beyond dyadic ties: A network theory of stakeholder influences.'' Academy of Management Review 2284). 887-910.\n* Bodin, \u00d6., Crona, B., Ernstson, H. 2006. ''Social networks in natural resource management: What is there to learn from a structural perspective?'' Ecology and Society 11(2).\n* Wasserman, S., Faust, K. 1994. ''Social network analysis: Methods and applications'' (Vol. 8). Cambridge university press.\n* Prell, C. (2012): ''Social network analysis: History, theory and methodology'', London.\n* Reed, M.S., Graves, A., Dandy, N., Posthumus, H., Hubacek, K., Morris, J., Prell, C., Quinn, C.H., Stringer, L.C. 2009. ''Who\u2019s in and why? A typology of stakeholder analysis methods for natural resource management.'' Journal of Environmental Management 90, 1933-1949.\n\n\n== References ==\n(1) Marin, A. Wellman, B. 2009. ''Social Network Analysis: An Introduction.'' In: SAGE Handbook of Social Network Analysis. 2010.\n\n(2) Scott, J. 1988. ''Trend Report Social Network Analysis.'' Sociology 22(1). 109-127.\n\n(3) Butts, C.T. 2008. ''Social network analysis: A methodological introduction.'' Asian Journal of Social Psychology 11. 13-41.\n\n(4) Borgatti, S.P. et al. 2009. ''Network Analysis in the Social Sciences.'' Science 323. 892-895.\n\n(5) Prell, C. Hubacek, K. Reed, M. 2009. ''Stakeholder Analysis and Social Network Analysis in Natural Resource Management.'' Society and Natural Resources 22. 501-518.\n\n(6) Stattner, E. Vidot, N. 2011. ''Social network analysis in epidemiology Current trends and perspectives''. Research Challenges in Information Science\n\n(7) Lam, D.P.M. Mart\u00edn-L\u00f3pez, B. Horcea-Milcu, A.I. Lang, D.J. 2021. ''A leverage points perspective on social networks to understand sustainability transformations: evidence from Southern Transylvania. Sustainability Science 16. 809-826.\n\n\n== Further Information ==\n* [https://www.youtube.com/watch?v=Qj2uWpYsdcM&list=PL1M5TsfDV6Vs7tnHGNgowEUwJW-O8QVp5 An introduction to Social Network Analysis incl. R coding guidance] by Mod-U: Powerful Concepts in Social Science on YouTube.\n* [Martin Grandjean http://www.martingrandjean.ch/] provides a range of good examples and deeper insights into Social Network Analysis on his website. His introductory series on Social Network Analysis can be found [http://www.martingrandjean.ch/introduction-to-social-network-analysis/ here].\n* A few free tools / software to create Social Networks are the following:\n** [Commetrix http://commetrix.de/]\n** [Cuttlefish https://github.com/dev-cuttlefish/cuttlefish]\n** [Social Network Visualizer https://socnetv.org/]\n** [Pajek https://pajek.software.informer.com/]\n\n----\n[[Category:Methods]]\n[[Category:Quantitative]]\n[[Category:Qualitative]]\n[[Category:Deductive]]\n[[Category:Inductive]]\n[[Category:Global]]\n[[Category:System]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Statistics]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "i8t5by1mx8lj9pp2ovkqb4ygzh8oggw"
                }
            },
            {
                "title": "Speed Typing",
                "ns": "0",
                "id": "506",
                "revision": {
                    "id": "6251",
                    "parentid": "5066",
                    "timestamp": "2021-08-16T12:43:53Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5760",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nFor anyone who works in front of a computer, being able to type quickly, accurately and withouth looking at the keyboard is a key asset. Learning to do so is far from rocket science, but let's have a quick look at it, anyway.\n\n== Goals ==\nLearn to touch-type quickly without making too many mistakes.\n\n== Getting Started ==\nWhile there is definitely value in writing manually, and it certainly has the nostalgic charme of writing a letter to your grandmother, we must admit that almost all [[Glossary|communication]] is digital today. Throughout your (non-)scientific career in the 21st Century, you will write on a computer. A lot. And while voice recognition and text-to-speech are getting better by the day, we will have to deal with pushing letters on a keyboard for quite some time. Might as well learn to do it quickly.\n\n==== The typing ====\n''Touch Typing'' <br/>\nThere is an endless range of websites that allow you to learn ten-finger-typing. Often, these offer writing challenges, providing you with texts you need to type and ranking your results (speed, accuracy) compared to others. While this can help getting better, you should start off with the basics to really get to know the keyboard. Fortunately, many websites also offer tutorials to learn the first steps. '''You should be able to recreate the keyboard from your mind and know the position of every letter before really attempting to increase your speed.''' Otherwise, you will make so many mistakes that correcting them slows you down, equalizing your speed gains. Only later will your muscle memory take over, and you may as well what a keyboard looks like, while still being able to type blindly. Below are some of the many websites you might consider visiting for the start. oarea\n* [https://www.typingclub.com/ Typingclub] and [https://www.typing.academy/ TypingAcademy] teach you the basics of touch typing and allow you to challenge yourself.\n* [https://play.typeracer.com/ Typeracer] lets you race against others.\n* [https://www.typinglounge.com/how-to-type-faster Typing Lounge] provides some tips on typing faster.\n\n[[File:Keyboard.jpg|500px|thumb|center|'''A keyboard.''' Ideally, each finger is responsible for a set of letters. Colour-coding can help when learning the touch-typing process. Source: [https://cms-api.galileo.tv/app/uploads/2016/10/farblichetastatur.jpg Galileo]]]\n\nOnce you've mastered the basics, you may want to extend your skills. Maybe, it's helpful to know some keyboard shortcuts for your work? Or you need a lot of those special icons, like $ and %, \u00b0 and ~? Learn where to find them with your hands instead of looking for them each time anew.\n\n''Two finger typing'' <br/>\nLegend has it that some brave individuals manage to type quickly and accurately while only using two fingers and even - behold! - looking at the keyboard while doing so, scanning it for the required letters. While this may certainly work for some people, you might want to consider learning touch typing instead. Not only might your neck and hands suffer from the posture you attain when typing letters individually, it also has this weird typewriter nostalgia that we've left behind a while ago. However, if this works best for you and you are happy with your results... who are we to judge.\n\n''Expert Level''<br>\nFor this, I'll hand over to Ali Abdall on YouTube, who provides some great tips in this short video:\n{{#evu: https://www.youtube.com/watch?v=1ArVtCQqQRE\n|alignment=center\n}}\n\n==== The keyboard ====\n'''There are different kinds of keyboards, and different people have different preferences.''' The most common type is the membrane keyboard, which you probably have in front of you right now. It's cheap and thus the standard, and it mostly works well. There are differences here, too - some keyboards are rather flat, some are rather bulky, and it's mostly a matter of taste which you prefer. However, there are also mechanical keyboards. Often referred to as \"gaming keyboards\", they provide a better feedback and feel when pressing them, and often have a better built quality. However, they also produce more noise, which is why they are rather seldomly used in typical work environments. You might still want to consider acquiring one if you type a lot.\n\nIn addition, you  might want to test some differently sized keyboards. Some keyboards have differently sized Space bars, Enter keys and the like, and while changing keyboards may change weird at first, it might pay off in the long run due to a better handling. Lastly, be aware of the differences between keyboards from different languages. Some letters may be missing and only accessible via shortcuts. Also, the general layout may differ, like the QWERTZ (German), QWERTY (English) or AZERTY (French) layout of the top row.\n\n==== The rest ====\n* Practice makes perfect. Don't be too harsh with yourself. \n* Take breaks while typing, and even more so, while practicing and learning to type. \n* Look out of the window now and then, your eyes will be thankful. \n* Maintain a healthy posture at the desk.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n[[Category:Productivity Tools]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "ryq9m5zoem72sbwaajyc619fio1rwcm"
                }
            },
            {
                "title": "Stacked Area Plot",
                "ns": "0",
                "id": "938",
                "revision": {
                    "id": "6577",
                    "parentid": "6576",
                    "timestamp": "2022-03-21T07:29:39Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Plotting in R */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4728",
                        "#text": "'''Note:''' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nThis entry aims to introduce Stacked Area Plot and its visualization using R\u2019s <syntaxhighlight lang=\"R\" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.\n\nConsider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=\"R\" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=\"R\" inline>geom_area()</syntaxhighlight> has the following syntax:\n\n'''Syntax''': <syntaxhighlight lang=\"R\" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.\n\nNow, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=\"R\" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n</syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=\"R\" inline>\"uspopage\"</syntaxhighlight> using the function <syntaxhighlight lang=\"R\" inline>geom_area()</syntaxhighlight> from the <syntaxhighlight lang=\"R\" inline>ggplot2 package</syntaxhighlight>:\n\n<syntaxhighlight lang=\"R\" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n</syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of \u201cBlues\u201d, and add labels.\n\n<syntaxhighlight lang=\"R\" line>\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")+\n  labs(title = \"US Population by Age\", \n       subtitle = \"Between 1900 and 2000\",\n       x = \"Year\",\n       y = \"Population (Thousands)\")\n</syntaxhighlight>\n\n==References==\n\n* R Graphics Cookbook, 2nd edition by Winston Chang\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Maria Jose Machuca."
                    },
                    "sha1": "05pk8jlziqbf9kz4u24xejow5ow8p4r"
                }
            },
            {
                "title": "Stacked Barplots",
                "ns": "0",
                "id": "617",
                "revision": {
                    "id": "5588",
                    "parentid": "5587",
                    "timestamp": "2021-05-27T12:18:25Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "4526",
                        "#text": "'''In short:''' Stacked bar plots show the quantitative relationship that exists between a main category and its subcategories. This entry helps visualise two different types of stacked bar plots, ''Simple Stacked Plots'' and ''Proportions Stacked Plots'', and explains the difference between them. For more on the basics of barplots, please refer to the [[Barplots, Histograms and Boxplots]] entry.\n\n\n==Stacked Barplots: Proportions vs. Absolute Values==\nStacked bar plots show the quantitative relationship that exists between a main category and its subcategories. Each bar represents a principal category and it is divided into segments representing subcategories of a second categorical variable. The chart shows not only the quantitative relationship between the different subcategories with each other but also with the main category as a whole. They are also used to show how the composition of the subcategories changes over time.\n\nStacked bar plots should be used for Comparisons and Proportions but with emphasis on Composition. This composition analysis can be static for a certain moment in time, or dynamic for a determined period of time.\n\nStacked bar Plots are two-dimensional with two axes: one axis shows categories, the other axis shows numerical values. The axis where the categories are indicated does not have a scale (*) to highlight that it refers to discrete (mutually exclusive) groups. The axis with numerical values must have a scale with its corresponding measurements units.\n\n\n===When you should use a stacked bar plot===\nThe main objective of a standard bar chart is to compare numeric values between levels of a categorical variable. One bar is plotted for each level of the categorical variable, each bar\u2019s length indicating numeric value. A stacked bar chart also achieves this objective, but also targets a second goal.\n\nWe want to move to a stacked bar chart when we care about the relative decomposition of each primary bar based on the levels of a second categorical variable. Each bar is now comprised of a number of sub-bars, each one corresponding with a level of a secondary categorical variable. The total length of each stacked bar is the same as before, but now we can see how the secondary groups contributed to that total.\n\n\n==Two types of Stacked Barplots==\n1. '''Simple Stacked Plots'''\u00a0place the\u00a0'''absolute value'''\u00a0of each subcategory after or over the previous one. The numerical axis has a scale of numerical values. The graph shows the absolute value of each subcategory and the sum of these values indicates the total for the category. Usually, the principal bars have different final heights or lengths.\n   \nWe use simple stacked plots when relative and absolute differences matter. Ideal for comparing the total amounts across each group/segmented bar.\n\n[[File:Simple_stacked_barplot.png|400px|frameless|right]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecie <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(specie,condition,value)\n \n# Stacked\nggplot(data, aes(fill=condition, y=value, x=specie)) + \n    geom_bar(position=\"stack\", stat=\"identity\")\n</syntaxhighlight>\n\n2. '''Proportions Stacked Plots''' place the '''percentage''' of each subcategory after or over the previous one. The numerical axis has a scale of percentage figures. The graph shows the percentage of each segment referred to the total of the category. All the principal bars have the same height.\n\nIn proportions stacked plots the emphasis is on the percentage composition of each subcategory since the totals by category are not shown; in other words, they are used when the key message is the percentage of composition and not the total within the categories. We use proportions stacked plots only when relative differences matter.\n\n[[File:Proportions_stacked_barplot.png|400px|frameless|left]]\n<syntaxhighlight lang=\"R\" line>\n# library\nlibrary(ggplot2)\n \n# create a dataset\nspecies <- c(rep(\"sorgho\" , 3) , rep(\"poacee\" , 3) , rep(\"banana\" , 3) , rep(\"triticum\" , 3) )\ncondition <- rep(c(\"normal\" , \"stress\" , \"Nitrogen\") , 4)\nvalue <- abs(rnorm(12 , 0 , 15))\ndata <- data.frame(species,condition,value)\n \n# Stacked + percent\nggplot(data, aes(fill=condition, y=value, x=species)) + \n    geom_bar(position=\"fill\", stat=\"identity\")\n</syntaxhighlight>\n\n----\n[[Category:Statistics]] [[Category:R examples]]"
                    },
                    "sha1": "d1mam1l2peahlgnuo5e3e579y1ifsv5"
                }
            },
            {
                "title": "Stakeholder Mapping",
                "ns": "0",
                "id": "296",
                "revision": {
                    "id": "6500",
                    "parentid": "6499",
                    "timestamp": "2021-12-20T15:57:49Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9297",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== Why & When ==\nSustainability Research is a research field that aims at tackling societal problems by developing systemic understandings of these problems and oftentimes collaboratively solving them through [[Glossary|transdisciplinary]] research designs. In this context, the [[Glossary|concept]] of stakeholders is a crucial component. Stakeholders are people that are directly affected by sustainability problems and can contribute to their solution. '''Mapping-out these stakeholders is a common entry-point''' to gaining an understanding of the system and problem at hand, and is further important to gather an overview of the relevant stakeholders in this context.\n\n== Goal(s) ==\n* Gain an overview of relevant stakeholders and their positionality towards a real-world problem\n* Get a methodological starting point for subsequent transdisciplinary research\n* Enable the development of strategies for jointly developing solutions\n\n\n== Getting started ==\nStakeholder Mapping is a part of stakeholder analysis, which are often used as interchangeable terms. While stakeholder analysis is also relevant in business contexts, for our purpose, we will refer to its usage in transdisciplinary research.\n\n'Transdisciplinary research' refers to a research mode that is common in sustainability science,  focusing on the collaboration of academic and non-academic actors with a solution-oriented perspective towards real-world problems. Please refer to the entry on transdisciplinarity for more details.\n\n'Stakeholders' are individuals, groups of individuals or institutions that have an interest in, or are affected by, decision-making related to a specific issue or problem. The stakeholders' interest in the respective decision-making may stem from being affected by the problem itself and thus the consequences of potential solutions to it, or from the intention of influencing the decision for any other reason. '''The relevant stakeholders in transdisciplinary projects are typically non-academic actors, such as NGOs, citizens, politicians, economic actors.''' However, academic actors may also be identified as stakeholders. Stakeholders can be distinguished more broadly, e.g. when everyone in a region is a potential stakeholder, or more narrowly, depending on the issue (Lelea et al. 2014). An example is the installation of wind turbines in a village. Researchers may be interested in investigating success criteria for social acceptance of renewable energies and therefore accompany this process. In this example, relevant stakeholders would likely be the local administration, citizens, farmers, the energy company and environmentalists.  \n\nThe initial collection of relevant stakeholders can be based on a mere brainstorming by the research team. These individuals may be further investigated, e.g. through questionnaires or interviews, or based on available external information. In this '''snowball-sampling''' approach, more and more actors are identified that are related to the initial actors. However, '[g]oing beyond the \u2018usual suspects\u2019 to find stakeholders that you had not already thought about, or stakeholders who are further removed from your research can seem like an impossible task.' (Research to Action 2015). A systematic approach to an initial identification of actors based on a questionnaire is provided by Leventon et al. (2016, see References).\n\n'Mapping' refers to the '''visual representation and structured arrangement of the identified actors''' in preparation to a transdisciplinary research project. Such a 'map' can take various forms. One approach is to categorize all relevant stakeholders according to their position towards the researched problem, or towards solutions to this problem. For example, in the Conviva project in Brazil, stakeholders were mapped in a 2x2 matrix according to their support or resistance towards nature conservation efforts.\n\n[[File:Stakeholder Mapping Template CONVIVA.jpg|600px|thumb|right|'''A template for stakeholder mapping.''' Source: [https://conviva-research.com/stakeholder-mapping-as-a-transdisciplinary-exercise-lessons-learned-from-the-conviva-brazilian-team/ Conviva]]]\n\nFurther types of matrices are possible. It can be helpful to compile a mere list of stakeholders, with information on their influence, needs and interests, potential contributions and possible strategies for engagement (tools4dev 2021). Another form of visually representing stakeholder positionality is a [[Venn Diagram]], which shows overlapping interests between stakeholders. [https://netmap.wordpress.com/about/ Net-Map] is a form of positioning, defining and linking stakeholders in a network structure. Ultimately, the visual form depends on the research project at hand. Maybe the connection between actors is of utmost interest, maybe their influence and power, maybe their interests and needs.\n\nThrough the process of stakeholder mapping, the research team is enabled to identify all relevant actors for the given issue. This helps '''plan next steps''', i.e. the conduction of interviews to gather further insights, or provides an overview on which individuals to invite to workshop formats or further methods of [[Transdisciplinarity|transdisciplinary research]]. This way, all important actors are involved in the process, which is important for the success of the envisioned solutions. A Leventon et al. (2016) highlight: \"Researchers in natural resource management consistently find that stakeholders should be included in solution-finding in order to facilitate negotiation and mutual learning; reduce conflict; and increase support and actor buy-in for decisions made.\" Apart from these advantages, stakeholder mapping also '''ensures that all relevant knowledge and perspectives are included''' and no key aspects of the topic at hand are neglected. Especially the perspectives of marginalised groups are important to this end (Leventon et al. 2016).\n\nFurther, the mapping already includes an initial analysis of the stakeholders position. Therefore, it '''helps assess their existent roles, interests and needs'''. Based on this, the research design and the process of jointly developing solutions can be adapted to the specific context (Learning for Sustainability 2021). Not all stakeholders will share the same aims and target the same outcomes from the getgo. Through mapping, strategies may be identified to bring actors together towards a common goal (Research to Action 2012) and \"create an environment for good decision-making\" [https://conviva-research.com/stakeholder-mapping-as-a-transdisciplinary-exercise-lessons-learned-from-the-conviva-brazilian-team/ (Conviva 2021)]. Stakeholder Mapping / Analysis should be done at the start of the research project, but can also be done continuously to track developments of the relevant stakeholders and their positions.\n\n\n== Links & Further reading ==\n'''Sources:'''\n* Lelea et al. 2014. ''Methodologies for stakehoklder analysis''. RELOAD project. Deutsches institut f\u00fcr Tropische und Subtropische Landwirtschaft (DITSL). Online available [https://www.researchgate.net/publication/280234554_Methodologies_for_Stakeholder_Analysis_-_for_Application_in_Transdisciplinary_Research_Projects_Focusing_on_Actors_in_Food_Supply_Chains here].\n* Learning for Sustainability. ''Stakeholder mapping and analysis''. Last accessed 20.12.2021. Available [https://learningforsustainability.net/stakeholder-analysis/ here.]\n* Research To Action. 01.05.2012. ''Stakeholder Analysis: a basic introduction''. Available [https://www.researchtoaction.org/2012/05/stakeholder-analysis-a-basic-introduction/ here].\n* Leventon et al. 2016. ''An applied methodology for stakeholder identification in transdisciplinary research.'' Sustainability Science 11. 763-775.\n* Research to Action. 07.09.2015. ''Problems encountered Mapping Stakeholders... and some suggested solutions''. [https://www.researchtoaction.org/2015/09/problems-encountered-mapping-stakeholders-and-some-suggested-solutions/ Available here].\n* tools4dev. ''Stakeholder Analysis Matrix Template.'' Last accessed 20.12.2021. Available [https://tools4dev.org/resources/stakeholder-analysis-matrix-template/ here.]\n\n* Research To Action provides an overview on detailed resources for Stakeholder Mapping on [https://www.researchtoaction.org/2015/09/stakeholder-mapping-resource-list/ this website.]\n* Net-Map, a low-tech approach for linking actors visually in a network structure, can be found [https://netmap.wordpress.com/about/ here.]\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "9diyn35449rpxgy4jsxziof6hievb98"
                }
            },
            {
                "title": "Statistics",
                "ns": "0",
                "id": "572",
                "revision": {
                    "id": "6200",
                    "parentid": "6199",
                    "timestamp": "2021-08-09T06:51:11Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2132",
                        "#text": "'''This page provides an overview on all entries that relate explicitly to statistical analyses.'''\n__NOTOC__\n=== New to statistics? ===\n'''These are some basic entries on statistics that you should read when you start your statistics journey:'''\n* '''[[ANOVA]]''' -  a statistical method that allows to test differences of the mean values of groups within a sample.\n* '''[[Introduction to statistical figures]]''' - an overview page for all kinds of data visualisation.\n* '''[[Data formats]]''' - explains all different kinds of data types.\n* '''[[Data distribution]]''' - what is a normal distribution, and how can you identify it?\n* '''[[Correlations]]''' - Correlations are a basic method in statistics that you should know about.\n* '''[[Regression Analysis]]''' - Regressions are Correlations + Causality and let you predict data.\n* '''[[Experiments]]''' - an overview on the history and different kinds of experiments.\n* '''[[Why statistics matters]]''' - why should you engage with statistics?\n* '''[[Bias in statistics]]''' - statistics can be flawed, and we tell you why.\n\n\n=== How to find the right statistics method ===\nOften, you know which kind of data you want to analyze, but there are so many options how to do it, and you don't know how to start.<br>\nDo not worry. The following interactive page asks you simple questions about your [[Glossary|data]] and guides you to the best statistical analysis method for your research.<br> \n'''[[An initial path towards statistical analysis]]'''\n<br/>\n\n\n=== How to code in R ===\nOn this Wiki, we believe in the importance of experience, and therefore provide guidance on how to approach statistical analyses in the Software [https://www.r-project.org/ R]. <br>\nThe following list includes all Wiki entries that include R code examples: '''[[:Category:R examples|All R example pages]]'''\n\n\n=== All statistics entries ===\nThere is a lot more than the aforementioned entries. Some revolve around statistical methods, others around the normativity of statistics.<br>\nThe following list includes all Wiki entries on Statistics: '''[[:Category:Statistics|All statistics entries]]'''"
                    },
                    "sha1": "3p9rzpbqbxeys3iglocijirnazr23zu"
                }
            },
            {
                "title": "Statistics and mixed methods",
                "ns": "0",
                "id": "210",
                "revision": {
                    "id": "6376",
                    "parentid": "5951",
                    "timestamp": "2021-09-20T07:25:35Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Spatial and temporal scales */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26083",
                        "#text": "'''Note:''' This entry revolves around mixed methods in statistics. For more on mixed methods in general, please refer to the entry on [[Mixed Methods]].\n\n==Mixed methods== \n'''Much in modern science is framed around statistics, for better or worse.''' Due to the arrogance of \"the scientific method\" being labeled based on deductive approaches, and the fact that much of the early methodological approaches were biased to and dominated by quantitative approaches. This changed partly with the rise or better increase of qualitative methods during the last decades. We should realise to this end, that the development of the methodological canon is not independent but interconnected with the societal paradigm. Hence the abundance, development and diversity of the methodological canon is in a continuous [[Glossary|feedback loop]] with changes in society, but also driven from changes in society. Take the rise of experimental designs, and the growth it triggered through fostering developments in agriculture and medicine, for better or worse. Another example are the severe developments triggered by [https://www.thoughtco.com/critical-theory-3026623 critical theory], which had clearly ramifications towards the methodological canon, and the societal developments during this time. Despite all these ivory towers is science not independent from the Zeitgeist and the changes within society, and science is often both rooted and informed from the past and influence the presence, while also building futures. This is the great privilege society gained from science, that we can now create, embed and interact with a new knowledge production that is ever evolving.<br/>\n\n[[File:Bildschirmfoto 2020-06-14 um 17.59.31.png|1000px|center|thumb|'''This picture shows a brief history of statistics and its most important methods.''' For a more detailed look look at their [http://www.statslife.org.uk/images/pdf/timeline-of-statistics.pdf website]]]<br/>\n\n'''Mixed methods are one step in this evolution.''' [https://www.theguardian.com/science/2012/aug/19/thomas-kuhn-structure-scientific-revolutions Kuhn] spoke of scientific revolutions, which sounds appealing to many. As much as I like the underlying principle, I think that mixed methods are more of a scientific evolution that is slowly creeping in. The scientific canon that formed during the enlightenment and that was forged by the industrialisation and a cornerstone of modernity. The problems that arose out of this in modern science were slowly inching in between the two world wars, while methodology and especially statistics not only bloomed in full blossom, but [https://blog.udemy.com/importance-of-statistics/ contributed their part to the catastrophe]. Science opened up to new forms of knowledge, and while statistics would often contribute to such emerging arenas as psychology and clinical trials, other methodological approaches teamed up with statistics. [https://www.simplypsychology.org/interviews.html#structured Interviews] and surveys utilised statistics to unleash new sampling combined with statistics. Hence statistics teamed up with new developments, yet other approaches that were completely independent of statistics were also underway. Hence, new knowledge was unlocked, and science thrived into uncharted territory. \n\nFrom a systematic standpoint we can now determine at least three developments: 1) Methods that were genuinely new, 2) methods that were used in a novel context, and 3) methods that were combined with other methods. Let us embed statistics into this line of thinking. Much of the general line of thinking in statistics in the last phase of modernity, i.e. before the two world wars. While in terms of more advanced statistics of course much was developed later and is still being developed, but some of the large breakthroughs in terms of the general line of thinking were rather early. In other words, in terms of what is most abundantly being applied up until the computer age, much was already developed early in the 20th century. Methods in statistics were still developed after that, but were often so complicated that they only increased in application once computers became widely available. \n\nWhat was however quite relevant for statistics was the [[Glossary|emergence]] into diverse disciplines. Many scientific fields implemented statistics into their development to a point that it was dominating much of the discourse (ecology, psychology, economics), and often this also led to specific applications of statistics and even genuinely new approaches down the road. \nWhere statistics also firmly extended their map on the landscape of methods was in the combination with other methods. Structured Interviews and surveys are a standard example where many approaches served for the gathering of data and the actual analysis is conducted by statistics. Hence new revolutionary methods often directly implemented statistics into their utilisation, making the footing of statistics even more firm.\n\n\nFor more on the development of new methods, please refer to the entries on [[History of Methods]] as well as [[Questioning the status quo in methods]].\n\n==Triangulation of statistics within methodological ontologies==\n[[File:Multiple-perspectives.gif|thumb|right|This wonderful picture from [https://proswrite.com/2013/02/26/what-is-plain-language-part-four-putting-it-all-together-in-a-process/ prowrite.com] shows how different methods can help to illuminate different aspects of a phenomenon. Triangulation is about bringing the different perspectives together, use them in harmony.]]\n[https://ebn.bmj.com/content/22/3/67 Triangulation] is often interpreted in a purely quantitative sense, which is not true. I would use it here in a sense of combination of different methods to triangulate knowledge from diverse sources. '''Ideally, triangulation allows to create more knowledge than with single method approaches.''' More importantly, [http://www.qualres.org/HomeTria-3692.html triangulation] should allow to use methods in harmony, meaning the sum of the methods that are triangulated is more than the sum of its parts. To this end, triangulation can be applied in many contexts, and may bring more precision to situations where mixed methods are not enough to describe the methodological design to enable for a clear communication of the research design or approach.\n\nNot least because of the dominating position that statistics held in modern science, and also because of the lack of other forms of knowledge becoming more and more apparent, qualitative methods were increasingly developed and utilised after the second world war. I keep this dating so vague, not because of my firm belief that much was a rather continuous evolvement, but mostly because of the end of modernity. With the enlightenment divinely ending, we entered a new age where science became more open, and methods became more interchangeable within different branches of science.\n\n== Mixed Methods and Design Criteria ==\n'''Note:''' For more details on the categories below, please refer to the entry on [[Design Criteria of Methods]].\n\n====Quantitative vs qualitative====\n[[File:Bildschirmfoto 2020-06-14 um 18.03.39.png|thumb|This picture emphasizes the main differences between quantitative and qualitative research. But do we really need to either decide for the one or the another?]]\n\nThe opening of a world of quantitative knowledge started a discourse between [https://www.simplypsychology.org/qualitative-quantitative.html quantitative and qualitative research] that has effectively never ended. Pride, self esteem and ignorance are words that come to my mind if I try to characterise what I observe still today in the exchange between these two lines of thought. Only when we establish [[Glossary|trust]] and appreciation, we can ultimately bring these two complementary types of knowledge together. From the standpoint of someone educated in statistics I can only say that it is my hope that words such as [https://sustainabilitymethods.org/index.php/Misunderstood_concepts_in_statistics \"significant\", \"correlated\", \"clustered\"] are not used unthoughtfully, but as the analysis approaches and associated concepts that these words stand for. What is even more difficult to me is when these concepts are rejected altogether as outdated, dogmatic, or plain wrong. Only if we can join forces between these two schools of thought, we may solve the challenges we face. \nLikewise, it is not sound that many people educated in statistics plainly reject qualitative knowledge, and are dogmatic in their slumber. '''I consider the gap between these two worlds as one of the biggest obstacles for development in science.''' People educated in statistics should be patient in both explaining their approaches and results, and being receptive and open minded about other forms of knowledge. There is still a lot of work in order to bridge this gap. If we cannot bridge it, we need to walk around the gap. Connect through concrete action, and create joined knowledge, which may evolve into a joined learning. Explain statistics to others, but in a modest, and open approach.\n\n====Inductive vs deductive====\n[[File:Bildschirmfoto 2020-06-14 um 18.07.47.png|thumb|The Leuphana and also many other research institutes conduct a board of ethics before a study starts.]]\n\nA fairly similar situation can be diagnosed for the difference between [https://conjointly.com/kb/deduction-and-induction/ inductive and deductive approaches]. Many people building on theory are on a high horse, and equally claim many inductive researchers to be the only ones to approach science in the most correct manner. '''I think both sides are right and wrong at the same time.''' What can be clearly said for everybody versatile in statistics is, that both sides are lying. The age of big data crashed with many disciplines that were once theory driven. While limping behind the modern era, these disciplines are often pretending to build hypotheses, merely because their community and scientific journals demand a line of thinking that is build on hypotheses testing. Since these disciplines have long had access to large datasets that are analysed in an inductive fashion, people pretend to write [[Glossary|hypothesis]], when they formulated them in fact after the whole analysis was finalised. While this may sound horrific to many, scientists were not more than frogs in the boiling pot, slowly getting warmer. This system slowly changed or adapted, until no one really realised that we were on the wrong track. There are antidotes, such as [https://www.cos.io/our-services/prereg preregistration] of studies in medicine and psychology, yet we are far way from solving this problem. \n\nEqually, many would argue that researchers claim to be inductive in their approach, when they are in fact not only biased all over the place, but also widely informed by previous study and theory. Many would claim, that this is more a weak point of qualitative methods, but I would disagree. With the wealth of data that became available over the last decades though the internet, we also have in statistics much to our disposal, and many claim to be completely open minded about their analysis, when in fact the suffer also from many types of biases, and are equally stuck in a dogmatic slumber when they claim to be free and unbiased. \n\nTo this end, statistics may rise to a level where a clearer documentation and transparency enables a higher level of science, that is also aware of the fact that knowledge changes. This is a normal process in science, and does not automatically make previous results wrong. Instead, these results, even if they are changing, are part of the picture. \nThis is why it is so important to write an outline about your research, preregister studies if possible, and have an ethical check being conducted if necessary. We compiled the [https://www.leuphana.de/forschung/transparenz-in-der-forschung/gute-wissenschaftliche-praxis.html application form for the board of ethics from Leuphana University] at the end of this Wiki.\n\n====Spatial and temporal scales====\n[[File:Bildschirmfoto 2020-06-14 um 18.12.31.png|thumb|Network analysis can be applied if you want to know how all your actors are tied together which relationships occur between themen and who has how much influence.]]\nStatistics can be utilised basically across all spatial and temporal scales. Global economic dynamics are correlated, experiments are conducted on plant individuals, and surveys are conducted within systems. This showcases why statistics got established across all spatial scales, but instantly also highlights once more that statistics can only offer a part of the picture. More complex statistical analysis such as [https://www.statisticssolutions.com/structural-equation-modeling/ structural equation models] and [https://www.jessesadler.com/post/network-analysis-with-r/ network analysis] are currently emerging, allowing for a more holistic system perspective as part of a statistical analysis. The rise of big data allows us to make connections between different data sources, and hence bridge different forms of knowledge, but also different spatial scales. While much of these statistical tools were established decades ago, we only slowly start to compile datasets that allow for such analysis.\n\nLikewise, with the increasing availability of more and more data, an increasing diversity of temporal dimensions are emerging, and statistics such as panel statistics and [https://ourcodingclub.github.io/tutorials/mixed-models/#what mixed effect models] allow for an evolving understanding of change like never before. Past data is equally being explored in order to predict the future. We need to be aware that statistics offers only a predictive picture here, and is unable to implement future changes that are not implemented into the analysis. We predict the future to the best of our knowledge, but we will only know the future, once it becomes our presence, which is trivial in general, but makes predictions notoriously difficult. \n\nTo this end, statistics needs to be aware of its own limitations, and needs to be critical of the knowledge it produces in order to contribute valid knowledge. '''Statistical results have a certain confidence, results can be significant, even parsimonious, yet may only offer a part of the picture.''' [https://sustainabilitymethods.org/index.php/Non-equilibrium_dynamics#The_world_can_.28not.29_be_predicted  Unexplained variance] may be an important piece of the puzzle in a mixed method approach, as it helps us to understand how much we do not understand. Statistics may be taken more serious if it clearly highlights the limitations it has or reveals. We need to be careful to not only reveal the patterns we may understand through statistics, but also how these patterns are limited in terms of a holistic understanding. However, there is also reason for optimism when it comes to statistics. Some decades ago, statistics were exclusive to a small fraction of scientists that could afford a very expensive computer, or calculations were made by hand. Today, more and more people have access to computers, and as a consequence to powerful software that allows for statistical analysis. The rise of computers led to an incredible amount of statistical knowledge being produced, and the internet enabled the spread of this knowledge across the globe. We may be living in a world of exponential growths, and while many dangers are connected to this recognition, the rise in knowledge cannot be a bad, at least not all of it. More statistical knowledge bears hope for less ignorance, but this demands as well the responsibility of [[Glossary|communicating]] results clearly and precisely, also highlighting gaps in and limitations of our knowledge.\n\n==Possibilities of interlinkages==\n[[File:Bildschirmfoto 2020-06-14 um 18.16.39.png|thumb|Scenario planning is a popular method in interdiscplinary and transdisciplinary research. It combines systemic knowledge about the current state and normative target knowledge of the desired future state.]]\nWe only start to understand how we can combine statistical methods with other forms of knowledge that are used in parralel. Sequential combination of other methods with statistics has been long known, as the example of interviews and their statistical analysis has already shown. '''However, the parallel integration of knowledge gained through statistics and other methods is still in its infancy.''' [https://www.forbes.com/sites/stratfor/2015/01/08/scenario-planning-and-strategic-forecasting/#766d057e411a Scenario planing] is a prominent example that can integrate diverse forms of knowledge; other approaches are slowly investigated. However, the integrational capability needed by a team to combine such different forms of knowledge is still rare, and research or better scientific publications are only slowly starting to combine diverse methods in parralel. Hence many papers that claim a mixed method approach often actually mean either a sequential approach or different methods that are unconnected. While this is perfectly valid and a step forward, science will have to go a long way to combine parralel methods and their knowledge more deeply. [[Glossary|Funding]] schemes in science are still widely disciplinary, and this dogma often dictates a canon of methods that may have proven its value, some may argue. However these approaches did not substantially create the normative knowledge that is needed to contribute towards a sustainable future.\n\n==Bokeh==\n[[File:Bildschirmfoto 2020-06-14 um 19.24.14.png|thumb|The picture allows us very clearly to depict a person. We can see a lot of details about this person, such as the clothes, a not too cheap umbrella, and much more. We can slo see that it is obviously in a part of the world where is rains, where you have public transportation, both tram and buses. We have high buildings, but also low building, traffic, street lights, hence a lot of information that can be depicted despite the blurriness.]]\nOne of the nicest metaphors of interlinking methods is the japanese word Bokeh. Bokeh means basically depth of field, which is the effect when you have a really beautiful camera lens, and keep the aperture - the opening of the lens - very wide. Photographs made in this setting typically have a laser sharp foreground, and a beautifully blurry background. You have one distance level very crisp, and the other like a washed watercolour matrix. A pro photographer or tec geek will appreciate such a photo with a \"Whoa, nice Bokeh\". Mixed method designs can be set up in a similar way. While you have one method focussing on something in the foreground, other methods can give you a blurry understanding of the background. Negotiating and designing this role in a mixed method setting is central in order to clarify which method demands which depth and focus, to follow with the allegory of photography. Way too often we demand each method having the same importance in a mixed method design. More often than not I am not sure if this is actually possible, or even desirable. Instead, I would suggest to stick to the Bokeh design, and harmonise what is in the foreground, and what is in the background. Statistics may give you some general information about a case study setting and its background, but deep open interviews may allow for the focus and depth needed to really understand the dynamics in a case study.\n\nAn example of a paper where a lot of information was analysed that is interconnected is [https://www.ecologyandsociety.org/vol19/iss4/art32/ Hanspach et al 2014], which contains scenarios, [[System Thinking & Causal Loop Diagrams|causal-loop analysis]], [[Geographical Information Systems|GIS]], and much more methodological approaches that are interlinked.\n\n\n==Statistics and disciplines==\n[[File:Bildschirmfoto 2020-06-14 um 19.34.30.png|thumb|Disciplines in science differentate themselves in many many subcategories. Thinking in disciplines can be seen as double-edged sword. They create specific knowledge but on the other hand not looking behind your own horizon is problematic.]]\nStatistics are deeply embedded in the DNA of many scientific disciplines, and strangely enough is the rejection of statistics as a valid method by many disciplines that associate themselves with critical thinking.\n\n'''I think statistics should be utilised when they provide a promising outcome.''' Due to the formation and [https://www.youtube.com/watch?v=YvtCLceNf30&list=PL8dPuuaLjXtNppY8ZHMPDH5TKK2UpU8Ng&t=0s development of scientific disciplines], this is however hardly the case. Instead, we have many disciplines that evolved around and based on statistics, such as psychology, ecology, (empirical) social science or parts of economics. All these scientific disciplines sooner or later encountered struggles with statistics. Examples are models when better statistics becoming more and more complicated, hence violating parsimony which led to the crisis questioning the value of probability driven statistics. Many disciplines evolved from deductive schools of thought, but with more and more data becoming available, this closed into a mishmash of inductive and deductive thinking, which I have mentioned before. Yet another problem is embedded into [https://sustainabilitymethods.org/index.php/Causality causality], as many studies from economics can not rely on causal links, yet inform policy and politics as if they are investigating nothing but causal links.\n \nMany scientific disciplines question the value of statistics, which is important. However, while statistics only offer a piece of the puzzle, rejecting statistics altogether is equally harsh. Instead, we ought to add precision in communicating the limitations of statistical results and evolve the utilisation of approaches to build trust between disciplines into our agenda and learning. A more fundamental question is most deeply embedded in philosophy, if I were to name a discipline. Philosophy still deals with the most fundamental questions such as \"Is there an objective way to act?\" or \"Are there normative facts?\". These questions illustrate why statistics will ultimately need to team up more deeply with philosophy to solve these questions. [https://lux.leuphana.de/vufind/Record/1696677149 \"Making comparisons count\"] is an example for a work from philosophy that clearly informs statistics, and more importantly, the results that are reported from statistical analysis. \n\n'''Methods are at the heart of the disciplinary dogma.''' I postulate that the method that started them all - philosophy - will be able to solve this problem, but it is not clear when this will happen. Sustainability science is an example of a solution oriented and diverse arena that tries to approach solutions to this end, yet this is why many scientists that define themselves through their discipline reject it. [[Glossary|Dialogue]] will be necessary, and we need to understand that deep focus is possible even for diverse approaches. Learning a method truly takes about as much time as learning a musical instrument, hence thousands of hours of hours at least. Yet while some researchers will be pivotal in integrating diverse approaches to research, the majority will probably commit to a deeper focus. However, appreciation and respect for other forms of knowledge should become part of the academic curriculum, or should be on the agenda even earlier. \n\nTo cut a very long story short, scientific disciplines arose out of a need for a specific labour force, the need for specific types of knowledge, and the surplus in resources that came out of the colonies, among many other reasons. I hope that we may overcome our differences and the problems that arise out of disciplinary identities, and focus on knowledge and solutions instead. Time will tell how we explore this possibility.\n\n==External Links==\n\n====Articles====\n\n[https://www.thoughtco.com/critical-theory-3026623 Critical Theory]: an example of mixed methods\n\n[https://www.theguardian.com/science/2012/aug/19/thomas-kuhn-structure-scientific-revolutions Thomas Kuhn]: The way he changed science\n\n[https://blog.udemy.com/importance-of-statistics/ The Importance of Statistics]: A look into different disciplines\n\n[https://www.simplypsychology.org/interviews.html#structured Interviews]: The way statistics influenced interviews\n\n[https://ebn.bmj.com/content/22/3/67 Triangulation]: A detailed article with lots of examples\n\n[http://www.qualres.org/HomeTria-3692.html Triangulation II]: An overview\n\n[https://www.simplypsychology.org/qualitative-quantitative.html Qualitative and Quantitative Research]: A comparison\n\n[https://conjointly.com/kb/deduction-and-induction/ Deduction and Induction]: The differences\n\n[https://www.cos.io/our-services/prereg Short explanation and the possibility for further reading]: Preregistration\n\n[https://www.leuphana.de/forschung/transparenz-in-der-forschung/gute-wissenschaftliche-praxis.html What they do and application forms]: The Leuphana Board of Ethics\n\n[https://www.leuphana.de/forschung/transparenz-in-der-forschung/gute-wissenschaftliche-praxis.html Guidelines for good scientific practive]: Leuphana University\n\n[https://www.statisticssolutions.com/structural-equation-modeling/ Structural Equation Modeling]: An introduction\n\n[https://www.jessesadler.com/post/network-analysis-with-r/ Network Analysis]: An inroduction with an analysis in R\n\n[https://ourcodingclub.github.io/tutorials/mixed-models/#what Linear Mixed Models]: An introduction with R code\n\n[https://www.forbes.com/sites/stratfor/2015/01/08/scenario-planning-and-strategic-forecasting/#766d057e411a Scenario Planning]: The most important steps\n\n====Videos====\n\n[https://www.youtube.com/watch?v=YvtCLceNf30&list=PL8dPuuaLjXtNppY8ZHMPDH5TKK2UpU8Ng&t=0s The History of Science]: A very interesting YouTube series\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "9lv94ytvdl8ptkcm056744xpew1049f"
                }
            },
            {
                "title": "Staying on top of research",
                "ns": "0",
                "id": "320",
                "revision": {
                    "id": "5924",
                    "parentid": "5850",
                    "timestamp": "2021-06-30T15:11:01Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* What, Why & When */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19292",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n== What, Why & When ==\nAs an aspiring [[Glossary|scientist]], it is important to follow the current discussions in science. Staying on top of research is an essential skill that will enhance your science game significantly. Because that's the gist: There is more to science than searching for terms in Google Scholar. Henrik did an [https://www.youtube.com/watch?v=yDuS-ugfa_0 entire video] on how he does research and stays up to date - saying I wasn't inspired by his take on the topic would be a lie.\n\nSo let's get started, fellow scientist. In the game of science, ultimately, you want to:\n* identify '''key papers''' that are relevant for your research and read the papers that cite these key papers\n* identify '''key authors''' in your field and follow whatever they publish next\n* identify '''key [[Glossary|journals]]''' in your field that have a high rating (so are being cited frequently) and therefore publish the most important news in your field\n\n== ==\n[[File:ChooseYourLevelLong.png|400px|frameless|center]]\n\n=== The Beginner Level ===\nYou have just started out with your study program and want to understand research as such. You may have recently found out that journals exist and that journal ratings are a thing. This stage is all about you gaining knowledge about research and also discovering what interests you most. On this level, you could follow:\n* The journals [https://www.nature.com \"Nature\"] and [https://www.sciencemag.org \"Science\"] are very basic newspapers of science. They tend to have rather sensational titles, feature mostly short papers but also report the main news. Follow them to get the latest of the scientific world.\n* If you are new to reading research and papers, there's a bunch of help out there to get you started. What's important is that papers are the main way of communicating and this ''primary literature'' is the ultimate go-to for science. Secondary sources can only scarcely account for the complexity of a paper. Your professors and instructors pinpoint you to resources in their lectures, and your text books reference papers as well. Knowing and having scanned these basic resources is already helpful as such and at this stage, you don't to follow all authors you read about.  You approach papers always in the same way: scan the abstract, understand the basic method the authors are using and dig into the parts that are relevant for you.\n\n=== The Advanced Level ===\nYou've understood the game of science and have written your first few papers for your degree program. This means you levelled up! Now it's time to follow some general resources in your field. Ask your professors for these key resources or - since we did that already for you - take some from the lists below:\n\n{| class=\"wikitable\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n| '''Digital Media''' || [https://journals.sagepub.com/home/nms New Media and Society] || [https://journals.sagepub.com/home/sts Science, Technology and Society] || [https://journals.sagepub.com/home/bds Big Data & Society] || [https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=85 Annals of the History of Computing] || [http://computationalculture.net/ Computational Culture]\n|-\n| rowspan = \"2\" | '''Business Administration / Entrepreneurship''' || [https://www.springer.com/journal/10551 Journal of Business Ethics] || [https://journals.sagepub.com/home/bas Business & Society] || [https://onlinelibrary.wiley.com/journal/10990836 Business Strategy and the Environment] || [https://aom.org/research/journals/journal Academy of Management Journal] || [https://us.sagepub.com/en-us/nam/journal/journal-management Journal of Management]\n|-\n| [https://aom.org/research/journals/review Academy of Management Review] || [https://journals.sagepub.com/home/asq Administrative Science Quarterly] || colspan = \"2\" | [https://journals.sagepub.com/home/etp Entrepreneurship Theory and Practice] || [https://pubsonline.informs.org/journal/orsc Organization Science]\n|-\n| '''Educational Sciences''' || [https://journals.sagepub.com/home/rer Review of Educational Research] || [https://www.apa.org/pubs/journals/edu/ Journal of Educational Psychology] || [https://www.journals.elsevier.com/learning-and-instruction/ Learning and Instruction] || [https://www.journals.elsevier.com/teaching-and-teacher-education Teaching and Teacher Education] || [https://www.springer.com/journal/11618 Zeitschrift f\u00fcr Erziehungswissenschaft] \n|-\n| '''Environmental & Sustainability Science''' || [https://www.springer.com/journal/11625 Sustainability Science] ||  colspan = \"2\" | [https://www.journals.elsevier.com/journal-of-cleaner-production/ Journal of Cleaner Production] || [https://www.nature.com/natsustain/ Nature Sustainability] || [https://www.journals.elsevier.com/ecological-economics/ Ecological Economics]\n|-\n| '''Engineering''' || [https://www.tandfonline.com/toc/tprs20/current International Journal of Production Research] || [https://www.tandfonline.com/toc/tppc20/current Production Planning & Control] || [https://www.ieee.org/publications/index.html#ieee-publications IEEE Publications] || [https://www.journals.elsevier.com/journal-of-materials-processing-technology Journal of Materials Processing Technology] || [https://www.journals.elsevier.com/cirp-annals CIRP Annuals Manufacturing Technology]\n|-\n| rowspan = \"2\" | '''Cultural Studies''' || [https://zeitschrift-kulturwissenschaften.de/ Zeitschrift f\u00fcr Kulturwissenschaften] || [https://www.zfmedienwissenschaft.de/ Zeitschrift f\u00fcr Medienwissenschaft] || colspan = \"2\" | [https://meiner.de/zeitschriften-ejournals/zeitschrift-fur-kulturphilosophie-zkph.html Zeitschrift f\u00fcr Kulturphilosophie] as at Leuphana, Cultural Studies focuses on the philosophy of media || [https://zuklampen.de/buecher/sachbuch/zeitschrift-fuer-kritische-theorie.html Zeitschrift f\u00fcr Kritische Theorie]\n|-\n| colspan = \"2\" | [https://journals.sagepub.com/home/tcs Theory, Culture and Society] ||colspan = \"2\" | [https://read.dukeupress.edu/new-german-critique New German Critique] || [https://www.mitpressjournals.org/grey Grey Room] \n|-\n| '''Political Science''' || colspan = \"2\" | [https://www.cambridge.org/core/journals/american-political-science-review American Political Science Review] || [https://www.cambridge.org/core/journals/world-politics World Politics] || [https://journals.sagepub.com/home/cps Comparative Political Studies] || [https://www.tandfonline.com/toc/fwep20/current West European Politcs]\n|-\n| '''Psychology''' || [https://www.apa.org/pubs/journals/bul/ Psychological Bulletin] || [https://www.apa.org/pubs/journals/apl/ Journal of Applied Psychology] || [https://www.journals.elsevier.com/journal-of-experimental-social-psychology/ Journal of Experimental Social Psychology] || [https://www.apa.org/pubs/journals/xge/ Journal of Experimental Psychology: General] || [https://www.apa.org/pubs/journals/psp/ Journal of Personality and Social Psychology]\n|-\n| '''Law''' || colspan = \"2\" |[https://www.beck-shop.de/NJW-Neue-Juristische-Wochenschrift/product/1318 Neue Juristische Wochenzeitschrift] || [https://www.beck-shop.de/juristenzeitung-jz/product/22095 Juristenzeitung]|| colspan = \"2\" | [https://beck-online.beck.de/?vpath=bibdata%2Fzeits%2FJUS%2Fcont%2FJUS.htm Juristische Schulung] \n|-\n| '''Economics''' || [https://www.aeaweb.org/journals/aer American Economic Review] || [https://onlinelibrary.wiley.com/journal/14680262 Econometrica] || [https://www.journals.uchicago.edu/toc/jpe/current Journal of Political Economy] || [https://academic.oup.com/qje Quarterly Journal of Economics] || [https://www.restud.com/ Review of Economic Studies]\n|-\n| '''Business Informatics''' || [http://www.bise-journal.com/ Business & Information Systems Engineering] || [https://goitsystems.de/index.php/de/nuetzliches/abschlussarbeiten/69-ais-basket-of-eight.html AIS Basket of Eight] || [https://aisel.aisnet.org/ecis/ Proceedings ECIS] || [https://aisel.aisnet.org/wi2019/ Proceedings WI] || [https://aisel.aisnet.org/icis/ Proceedings ICIS]\n|-\n| '''Machine Learning''' || [https://icml.cc International Conference of Machine Learning (ICML)] || [https://nips.cc NeurIPS Conference] || [http://www.jmlr.org Journal of Machine Learning Research (JMLR)] || [https://www.springer.com/journal/10618 Data Mining and Knowledge Discovery (DAMI)] || [https://www.springer.com/journal/10994 Machine Learning Journal (MLJ)]\n|}\n\n=== The Expert Level ===\nAt this stage, you know where you are going in science and have identified your research niche. You want to become an expert in that niche? Well, you need to stay uptodate on a granular level:\n* identify 2-3 '''key papers''' for your research niche and follow whoever cites the paper\n* on top of the '''journals''' you are following for your field, find specific journals relevant to you. Be selective and check the rating to not follow unnecessarily small journals\n* identify the '''authors''' that do the same thing as you and use the same methods - the more specific, the better. ''Note:'' At earlier levels, following people is not necessarily useful. We believe following \"gurus\" in your field mostly manifests hierarchies. This is why until this level, following the important journals is enough to get a broad overview to the field.\n\n== ==\n[[File:ChooseYourWeaponLong.png|400px|frameless|center]]\n\nDifferent tools allow you to follow the authors, papers and journals you just identified. The following services and apps are your friend:\n* Google Scholar Alerts\n* Visualping\n* Feedly\n* Twitter\n\nThe first three apps are aggregators, they merely ''fetch'' existing literature from other webpages. You know this from '''Google''', '''visualping''' does it for a website you define and '''Feedly''' does it for RSS style pages. On this list, only '''Twitter''' is where you follow the primary source directly.\nThere is a number of other pages out there that let you follow content directly, websites like [https://www.researchgate.net researchgate] or journal websites. We believe that most use cases should be covered by the 4 services mentioned above, but of course, this list is not exhaustive.\n\n=== Where to follow journals ===\nCongratulations on having written that brave mail to your professor about the journals to follow or on having found your discipline in the list above. What you want do now is stay on top of these journals. Add \"actually reading my Google Scholar emails\" and \"scrolling through Feedly\" to your morning routine or your social media addiction and you are all set.\n\n[[File:GoogleScholarJournal.png|thumb|Create a new email alert for a journal.]]\n1. '''Google Scholar''': creating a new Google Scholar alert is simple. Do it for each journal you are following. Click on the three bars in Google Scholar and navigate to \"Alerts\". Create a new alert for a journal by entering \"source:NAME_OF_JOURNAL\" into the alert query. \n\n2. ['''Visualping''': Yeah, I know, it's on the list. Scroll down for better examples when to use email alerts with visualping.]\n\n[[File:FeedlyJournal.png|thumb|left|The journal [https://feedly.com/i/subscription/feed%2Fhttp%3A%2F%2Flink.springer.com%2Fsearch.rss%3Ffacet-content-type%3DArticle%26facet-journal-id%3D10618%26channel-name%3DData%2BMining%2Band%2BKnowledge%2BDiscovery \"Data Mining and Knowledge Discovery\"] in Feedly.]] [[File:FeedlyFeeds.jpg|thumb|Nature and Science are in different Feedly feeds than the other journals.]]\n3. '''Feedly''': Feedly is an aggregator for sources and RSS style webpages. Once you have downloaded the app, you can search for the journal you are interested in and follow it - the big journals should be already on Feedly. For those that are not, you can simply add their webpage: Navigate to the journal's webpage and find the page where they list their recent volumes. Add the URL to Feedly and it will automatically grab the most recent articles. Whenever you now open Feedly, the latest published papers await you! Exciting! ''Pro-Tip'': organize [https://www.nature.com \"Nature\"] and [https://www.sciencemag.org \"Science\"] in a different so-called \"feed\" - this way, the very regular updates by Nature and Science don't mask the less regular updates by the other journals. \n\n4. '''Twitter''': Most popular journals have twitter accounts. Depending on the disciplines, these accounts are better or worse maintained (you could almost try to derive from which discipline they are coming based on the amount of tweets, but evil to him who evil thinks). While Google Scholar and visualping send you email alerts, Twitter requires you to become active. Make it a habit to scroll through science Twitter, maybe attached to whenever you scroll through other social medias. Maybe even create an extra account for this so you can safely follow scientists without them reading potential private tweets :-) Science Twitter awaits you, and it doesn't judge if on top, you also follow your favourite non-related NASA scientist.\n\n\n=== Where to follow authors ===\nLet's get on with research. We will be following [https://psyarxiv.com/km37w a paper that did a computational model for '''Panic Disorder''']. I am very interested in computational models and the network approach they chose seems to be exactly what I was looking for.\n\n[[File:PanicPaper.png|frameless|700px|center]]\n\n\nAfter having decided I want to follow this work, I have different options.\n\n\n[[File:GoogleScholarHaslbeck.png|thumb|left|Jonas Haslbeck's Google Scholar Profile.]]\n[[File:VisualpingEnterWebiste.png|thumb|Enter the website in [https://visualping.io visualping]...]] [[File:VisualpingFollow.png|thumb|... and keep track whenever anything changes.]]\n1. '''Google Scholar''': Jonas Haslbeck, the second author, seems to publish more on this topic. I therefore choose to follow his work. I found him on [https://scholar.google.com/citations?hl=en&user=jmFh_isAAAAJ '''Google Scholar''']. I can then click on the follow button to receive email alerts whenever he publishes an exciting new paper. \n\n2. [https://visualping.io '''Visualping''']: Sometimes, authors don't have a Google Scholar profile you can follow. This is the case for the first author, Donald Robinaugh. I therefore decide to monitor changes on his website using services like [https://visualping.io '''visualping''']. I can search for the author's institutional webpage or their private webpage where their newest publications are listed. Whenever Mr. Robinaugh publishes a new paper, visualping will detect a change and send me an email. In our case, the panic disorder paper is uploaded to arxiv, a webpage for pre-prints. I'm especially eager to find out when the paper will be published in a journal, maybe even with a different title, so I eagerly wait for Mr. Robinaugh's website to be updated.\n\n3. ['''Feedly''': This time, I'd opt out of Feedly. The other methods are definitely better suited for authors.]\n\n4. '''Twitter''': There's a whole community of scientists on Twitter. Authors share their recent papers and discuss other people's research below their tweets. The authors that you'd follow on Google Scholar and visualping you can also follow there. Science Twitter once again!\n\n=== Where to follow papers ===\nNext on, you want to follow the [https://psyarxiv.com/km37w/ panic disorder paper] itself. I see it as a key to my work so every related work aka citation is also potentially interesting for me. \n\n[[File:GoogleScholarPaper.png|thumb|Click on who cited the paper...]]\n[[File:GoogleScholarPaperEmail.png|thumb|... and create an email alert.]]\n1. '''Google Scholar''': I found the paper on Google Scholar, clicked on who cited it and created an email alert.\n\n2. [https://visualping.io '''Visualping''']: The panic disorder paper is published [https://psyarxiv.com/km37w/ on psyarxiv] currently. Additionally to the Google Scholar Alert, I can enter the paper URL to [https://visualping.io visualping] and receive email updates whenever the website changes - so whenever other papers cite this paper.\n\n3. ['''Feedly''': If I think about it, Feedly is most useful for journals. Of course you could follow the same website you enter into visualping here, but visualping definitely is better at tracking these changes.]\n\n4. ['''Twitter''': Well, hello friend. Twitter really isn't the right platform to follow papers. Maybe try a different platform ;)]\n\n\n== A whole new world: Keywords ==\nFinally, keywords. You've come to a dark place. Following keywords is an art for itself and be warned, it is dark and full of traps. Google Scholar but also sites like [https://www.scopus.com/ Scopus] (use with VPN and click on \"Check Access\" to access the site) allow you to search for different parameters:\n* Papers with certain words in their ''title''\n* Papers by certain ''authors''\n* Papers in certain ''journals''\n* Papers with certain author-specified ''keywords''\n\nScopus and also Google Scholar allow you to follow your search queries. Both offer extensive options, you could for example specify certain words to ''not'' be part of the paper. This could be:\n[[File:ScopusSearch.png|thumb|Searching for keywords and specifically excluding other keywords.]]\n* If you are interested in sustainability papers published in Nature but definitely not in bacteria: \"''Sustainability''\" in \"Title, Abstract, Keywords\" AND \"''Nature''\" in \"Source Title\" AND NOT \"''Bacteria''\" in \"Title, Abstract, Keywords\"\n* If you like Machine Learning but are not a fan of neural networks: \"''Machine Learning''\" in \"Title, Abstract, Keywords\" AND \"''ICML''\" in \"Source Title\" AND NOT \"''Neural Networks''\" in \"Title, Abstract, Keywords\"\n\nNow I see where you are going. It is ''very'' tempting to follow \"Sustainability\" or \"Fairtrade\" if you are interested in these. Then, however you've moved on to a new full-time job: reading emails about these updates. Be very selective which search queries you follow and which you don't. Even a very niche-like sounding search query could potentially result in a mailbox overflow - and believe me, I've been there.\n\nThis being said, stick to authors, papers, journals, depending on your level. Have fun with the game of science and make sure to add \"reading papers\" to your morning routine. You can do it! Happy reading!\n\n== Links & Further reading ==\n'''References''':\n* Icons for \"Choose Your Level\" are designed by lagotdesign and taken from: https://thenounproject.com/lagotdesign/collection/lagotline-science/\n* The visualping icon: Salager / CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0)\n\n'''Further Reading:'''\n* [https://www.youtube.com/watch?v=yDuS-ugfa_0 Henrik's take] on how to follow scientific literature including how literature reviews work\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|author]] of this entry is Lisa Gotzian."
                    },
                    "sha1": "48tla6c03hvia198kq97wy2a5u5i43h"
                }
            },
            {
                "title": "Structured Query Language in Python",
                "ns": "0",
                "id": "1057",
                "revision": {
                    "id": "7268",
                    "parentid": "7119",
                    "timestamp": "2023-07-04T15:18:42Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to structure query language (SQL) and compares database management systems which can interact with SQL",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "18319",
                        "#text": "'''THIS ARTICLE IS STILL IN EDITING MODE'''\n\nThis article will provide an overview of the functioning of Structured Query Language (SQL), and discuss which package in Python should be used under which circumstances. First, we will have a look at how SQL actually works!\n\n==Introduction to SQL==\nSQL is the standard database language for relational databases. It is used for retrieving and manipulating data. When analyzing data, SQL is great to retrieve and organize data. It is used by many different institutions, such as hospitals and large companies. Basically anywhere, where data needs to be analyzed! To use SQL in Python, you need to choose a package. We will have a look at the different options later. For now, we will focus on how SQL works.\n\n==Relational Databases==\n\nA relational database is a collection of data elements having specified relationships following the relational model. This model for data organization was developed by E. F. Codd in 1970 and pursues the goal of making data from large databases accessible without knowledge about its structure and location in the database. \n\nThe relational model allows data to be organized into tables. Each table contains one type of record which is described by a set of attributes. In the tables, each attribute is represented by a column whereas each record is represented by a row. Relationships between the records of different tables are established using primary and foreign keys. Primary keys are unique identifiers for each row of a table. Foreign keys contain the primary key of another table and thus link the rows of the tables (Codd 1970).\n\nFigure 1 shows an exemplary relational database that could be applied in a hospital. Its first table contains the patients of the hospital (rows) which are described by patient_id, name, address, and insurance_number (columns). In the second table, the treatments (rows) are stored, described by treatment_id, drug, intakes_per_day, and patient_id as attributes (columns). The first column of both tables contains a unique identifier for each record which is the primary key. The patient ID in the treatments table is a foreign key since it contains a primary key from the patients' table and thus allows to link each treatment to a patient.\n\n===Figure 1: Exemplary Relational Database===\n[[File:Figure_1_database.png|center]]\n\n==Query Relational Databases with SQL==\nThe following sections provide an overview of the main SQL elements used for querying relational databases. Code examples based on the exemplary database shown above are used to illustrate the use of the language elements in typical query structures.\n\nRemark: SQL is standardized by an ISO norm (ISO/IEC 2016). Nonetheless, the use of the language slightly varies between database systems.\n\n===SELECT (DISTINCT) Statement===\nThe SELECT statement is used to retrieve data from a relational database. You can either specify a list of columns that should be selected from a table or select all columns using a \u201c*\u201d. Furthermore, the source table has to be defined using the FROM clause. Instead of the SELECT statement the SELECT DISTINCT statement can be used in order to show only unique rows in the output table.\n\n===Code example 1: Simple SELECT statement===\n<syntaxhighlight lang=\"Python\" line>\nSELECT\n    name,\n    address,\n    insurance_number\nFROM patients\n</syntaxhighlight>\n\nThe query would return a table containing the name, address, and insurance number of all patients.\n\n===Aliases===\nAliases can be assigned to tables and columns using the AS keyword. Renaming columns can help to make output tables more readable (code example 5); aliases for tables are used to make queries shorter and clearer (code example 4).\n\n===Filtering Data===\nData can be filtered by using the WHERE clause. It is followed by a boolean function that outputs \u201cTrue\u201d for all records which should be included and \u201cFalse\u201d for those which should be excluded.\n\nSuch Boolean functions usually contain comparisons of attributes with fixed values or other attributes. Comparisons are expressed using comparison operators (table 1).\n\n===Table 1: Common comparison operators===\n{| class=\"wikitable\"\n|-\n! Operator  !! Description\n|-\n| = || Equal to\n|-\n| > || Greater than\n|-\n| < || Less than\n|-\n| >= || Greater than or equal to\n|-\n| <= || Less than or equal to\n|-\n| <> || Not equal to\n|}\n(Source: adapted from w3schools 2022b)\n\nMissing values can be identified by the IS NULL operator returning \u201cTrue\u201d for all records having a null value in the data field of interest. IS NOT NULL is the negation of this operator.\n\nExpressions can be linked with the logical operators AND and OR. AND returns \"True\" if both linked expressions are true; OR returns \"True\" if one of those is.\n\n===Code example 2: WHERE clause and filter condition===\n<syntaxhighlight lang=\"Python\" line>\nSELECT\n    *\nFROM treatments\nWHERE\n    intakes_per_day > 1\n    AND drug = \u201cibuprofen\u201d\n    AND patient_id IS NOT NULL\n</syntaxhighlight>\nThe query would return a table with all treatments where ibuprofen is taken more than one time per day and the patient_id is not missing.\n\n==Sorting Data==\nThe rows of the output table can be sorted using the ORDER BY keyword. The keyword is followed by the columns by which the output should be sorted. Thereby, the order of the columns determines the priority with which they are considered. The keywords ASC and DESC after a column determine whether the values are put in ascending or descending order.\n\n===Code example 3: ORDER BY keyword===\n<syntaxhighlight lang=\"Python\" line>\nSELECT\n    *\nFROM treatments\nORDER BY intakes_per_day DESC\n</syntaxhighlight>\nThe query would return all treatments and order them by descending intakes_per_day.\n\n===Joins===\nJoins are used to connect rows from different tables based on matching attributes. Different types of joins are differentiated by the rows contained in the output table. Common join clauses are the (INNER) JOIN, LEFT (OUTER) JOIN, RIGHT (OUTER) JOIN, and FULL (OUTER) JOIN. Inner joins only return rows that match a row from the other table. Full joins return all rows of both tables. Left joins return all rows from the left table and the matching rows from the right table. Right joins work vice versa. Thereby, the table mentioned before the join clause is considered to be the left table. Join clauses are used in conjunction with the ON keyword which is followed by the join condition defining one or more attributes that will be checked for matching.\n\nRemark: In case of similar column names in two or more source tables, column names must be supplemented by table names. Aliases can help to make those references shorter (code example 4).\n\n===Figure 2: Common joins===\n[[File:Visualisation_joins.png|900 px|center]]\n(Source: adapted from w3schools 2022a)\n\n===Code example 4: LEFT JOIN===\n<syntaxhighlight lang=\"Python\" line>\nSELECT\n    p.name,\n    p.insurance_number,\n    t.drug,\n    t.intakes_per_day\nFROM treatments AS t\nLEFT JOIN patients AS p\nON t.patient_id = p.patient_id\n</syntaxhighlight>\nThe query would return all treatments, described by drug and intakes_per_day as well as the patient's name and insurance_number if there is an entry with a matching patient_id in the patients' table.\n\n==Aggregation functions==\nAggregation functions aggregate the values of a specified column over several rows. Common aggregation functions are listed in table 2.\n\n===Common aggregation functions===\n\n{| class=\"wikitable\"\n|-\n! Function !! Description\n|-\n| MIN() || returns the minimal value\n|-\n| MAX() || returns the maximal value\n|-\n| SUM() || returns the sum of the values\n|-\n| AVG() || returns the average of the values\n|-\n| COUNT() || counts the values\n|-\n| COUNT(DISTINCT) || counts the unique values\n|}\nTable 2\nThe GROUP BY clause is used to define attributes that form the groups whose values will be aggregated (see table 2). If such attributes are not defined, the whole table gets aggregated into one row.\n\n===Code example 5: Aggregation functions and GROUP BY clause===\n<syntaxhighlight lang=\"Python\" line>\nSELECT\n    patient_id,\n    COUNT(DISTINCT drugs) AS number_of_drugs,\n    SUM(intakes_per_day) AS total_intakes_per_day\nFROM treatments\nGROUP BY patient_id\n</syntaxhighlight>\nThe query would count the different drugs administered and sum up the intakes per day over all drugs per patient_id.\n\n==SQL in Python==\nThere are several packages you can choose from to use SQL in Python. The two most important ones are ''SQLite'' and oracle. We will have a look at these two options and discuss their advantages and disadvantages.\n===SQLite===\nSQLite is a relational database management system (RDBMS). It can also be used as a software library, which is a topic for another article. The \"Lite\" refers to the lightness of setup, administration, and required resources, SQLite stores the data in a file that can be used on multiple operating systems, which is not the case for many systems making use of SQL. Deploying it is as simple as linking a library and creating a regular file, this fact causes a high capacity of accessibility.\n\n<syntaxhighlight lang=\"Python\" line>\n# import the package for SQLite\nimport sqlite3\n\n# Connect to an SQLite database. If no such file exists, one will be created\nconn = sqlite3.connect('example.db')\n\n# Create a table\nconn.execute('''CREATE TABLE IF NOT EXISTS stocks\n             (date text, trans text, symbol text, qty real, price real)''')\n\n#The triple quotes allow spanning the statement over multiple lines. \"CREATE TABLE IF NOT #EXISTS are case-insensitive SQL keywords that have been written in capital letters to make #the code more readable. A table with five columns is created. The \"text\" or \"real\" behind the #name of the columns determines whether the values should be text or floating-point numbers.\n\n# Insert some data into the table\nconn.execute(\"INSERT INTO stocks VALUES ('2023-04-19', 'BUY', 'AAPL', 100, 136.71)\")\n\n# Commit the changes\nconn.commit()\n\n# Query the table\ncursor = conn.execute(\"SELECT * FROM stocks\")\nfor row in cursor:\n    print(row)\n\n# Close the database connection\nconn.close()\n</syntaxhighlight>\n\nHere are some benefits and limitations of SQLite:\n===Benefits===\n* SQLite is an in-memory open-source library\n* Does not need any configuration or installation and license\n* The size is less than 500kb\n* It is serverless and does not need a server process\n* Enables You to work on multiple databases in the same session\n* It is a cross-platform system and does not need a specific platform to run, such as Linux\n* Easy to backup\n* It's atomic and transactional property which means if part of the operation is failed, all the successful operations will be reverted to the original state.\n* Database can be recovered from system crashes and power outage\n* Resilient to out-of-memory and storage errors\n\n===Limitations===\n* Loose approach towards the datatype handling, allows the insertion of invalid values, you can insert a string in an integer column\n* It does not support all the datatypes for example there is no Boolean or Datetime\n* The previous drawback can cause your data to be invalid when moving it from SQLite to another platform\n* On the same file, simultaneous write is impossible\n* The previous limit causes lockage when writing on file and it reduces performance in heavy scenarios\n* Database-level user management is less applicable in SQLite\n* There are a lot of limitation in the maximum usage of features for instance the maximum number of attached databases are 125, the maximum number of rows in a table is 264, the number of tables in a schema is 2147483646, the maximum number of columns is 32767 in a table, the maximum number of tables in a join is 64 tables, maximum number of arguments on a function is 127, the SQLite dataset is limited to 281 terabytes, which is why it is not a good choice for large scale projects\n* SQLite stores the entire data in a single file which makes it hard to handle under filesystem limitations\n* Due to the database accessibility, it may require more work to provide the security of\nprivate data\n\nWhen to Use SQLite?\n* SQLite is a good choice when you want to combine the SQL querying and storage ability with the ease of access\n* Can be a good selection in environments where end users should not be aware of data existence\n* As SQLite requires no installation and configuration, beginners can use SQLite for the learning purpose\n\n==What is Oracle?== \nOracle database is another (RDBMS). It stocks the largest part of the current market among relational database management systems.\nThe company provides multiple editions including:\nEnterprise Edition, Standard Edition, Express Edition (XE), Oracle Lite, Personal Edition.\nAs Oracle is an RDBMS software, it is built on SQL and has some additional extensions. Oracle is a fully scalable database architecture. It is therefore being used by big enterprises which require management and process within their local network that is spread across the world. It also has its own network components to facilitate connections across different networks.\n\n===Benefits===\n* Cost reduction: It Enables you to consolidate several databases into one which can cause shrinking operations and costs. You have the option of cloud bursting when peaks occur, which means that they can temporally increase their cloud resources. This increases the resistance to crashes\n* One single database is easier to manage and maintain\n* High availability and performance, including performance optimization features\n* Secure hybrid cloud environment offers more options by providing more tools than the other platforms\n* User control and identity management that is hard to reproduce which means high safety standards\n* Quick backup and recovery options to protect and reconstruct data\n* Flashback technology: this option allows to recover of missed or lost data, decreases human errors, and makes the administration much easier\n* Reliability and portability: It can run on almost 20 network protocols and 100 hardware platforms easily\n* Market presence: Oracle is the most famous and largest RDBMS vendor with more experience and researchers, tons of supported third parties, and professional staff\n* Support for enterprise applications\n* Oracle DB is compatible with the major platforms, including Windows, UNIX, Linux, and macOS. The Oracle database is supported on multiple operating systems, including HP-UX, Linux, Microsoft Windows Server, Solaris, SunOS, and macOS\n\n===Limitations===\n* Complexity: It requires specialized skills to install and maintain due to its complex engine\n* Cost: While it can save costs due to the option to consolidate databases into one, it might also be costly if the potential of the offers is not fully exhausted\n* Difficult to manage: It is generally more difficult in managing certain operations\n* Not suitable for small or medium size databases.\n* High hardware requirements\n===When to Use Oracle?===\nOracle Database supports SQL language to interact with the database. It's considered one of\nthe best databases because it supports all data types like relational, graph, structured, and\nunstructured information. It is apparently a wise choice for large-scale projects and can\nsupport large quantities of data. From the announcement of 256 big companies, such as Netflix, Linkedin, ebay, it can be concluded that it is appropriate for a large amount\nof data.\n\n==References==\n\u201c10 Benefits of Oracle\u2019s Data Management Platform.\u201d n.d. https://www.oracle.com/a/ocom/docs/10-benefits-of-oracle-data-management-platform.pd\n\nCodd, E.F. 1970. \"A Relational Model of Data for Large Shared Data Banks.\" *Communications of the ACM* 13, no. 6: 377-387. [https://dl.acm.org/doi/pdf/10.1145/362384.362685](https://dl.acm.org/doi/pdf/10.1145/362384.362685) \n\n\u201cIntroduction to Oracle Database,\u201d n.d. https://docs.oracle.com/database/121/CNCPT/intro.htm.\n\nISO/IEC. 2016. \"ISO/IEC 9075-1:2016(en): Information technology \u2014 Database languages \u2014 SQL \u2014 Part 1: Framework (SQL/Framework).\" [https://www.iso.org/obp/ui/#iso:std:iso-iec:9075:-1:ed-5:v1:en](https://www.iso.org/obp/ui/#iso:std:iso-iec:9075:-1:ed-5:v1:en)\n\nKoulianos, Petros. 2020. \u201c5 Reasons to Use SQLite the Tiny GIANT for Your next Project.\u201d\nThe Startup. August 9, 2020. https://medium.com/swlh/5-reasons-to-use-sqlite-the-tiny-giant-for-your-next-project-a6bc384b2df4.\n\nNguyen, Spencer. 2022. \u201cThe Benefits of Oracle DBMS for Your Organization.\u201d DreamFactory Software- Blog. May 25, 2022. https://blog.dreamfactory.com/the-benefits-of-oracle-dbms-for-\nyour-organization/.\n\nSingh, Hemendra. 2019. \u201cMobile App Development Company | Web Development Company USA India.\u201d The NineHertz: August 14, 2019. https://theninehertz.com/blog/advantages-of-using-oracle-database.\n\n\u201cSQLite Pros and Cons: A.\u201d 2022. ThinkAutomation. February 17, 2022. https://www.thinkautomation.com/our-two-cents/sqlite-pros-and-cons-a/.\n\n\u201cSQLite Python.\u201d 2018. SQLite Tutorial. 2018. https://www.sqlitetutorial.net/sqlite-python/.\n\nTeam, CherryRoad Consultancy. 2021. \u201cWhat Are the Top 8 Reasons to Use an Oracle Database?\u201d CherryRoad Technologies. October 22, 2021. https://www.cherryroad.com/2021/10/22/oracle-database-cloud/.\n\nw3schools. \"SQL Joins.\" Accessed December 23, 2022a. [https://www.w3schools.com/sql/sql_join.asp](https://www.w3schools.com/sql/sql_join.asp)\n\nw3schools. \"SQL Operators.\" Accessed December 23, 2022b. [https://www.w3schools.com/sql/sql_operators.asp](https://www.w3schools.co /sql/sql_operators.asp)\n\n\u201cWhat Are the Limitations of SQLite.\u201d n.d. Www.dbtalks.com. https://www.dbtalks.com/tutorials/learn-sqlite/what-are-the-limitations-of-sqlite.\n\n\u201cWhat Is Oracle? - Definition from WhatIs.com.\u201d n.d. SearchOracle. https://www.techtarget.com/searchoracle/definition/Oracle.\n\n\u201cWhat Is an Oracle Database? | NetApp.\u201d https://www.netapp.com/oracle/what-is-oracle-database/.\n\n\u201cWhat Is SQLite?\u201d n.d. Codecademy. https://www.codecademy.com/article/what-is-sqlite.\n\n\u201cWhat Is SQLite and When to Use SQLite.\u201d 2021. Simplilearn.com. July 22, 2021. https://www.simplilearn.com/tutorials/sql-tutorial/what-is-sqlite.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|authors]] of this entry are XX Reinhardt and Hajar Zare. Edited by Milan Maushart"
                    },
                    "sha1": "10cvaro3grtdi78772zkw1o3xgj377j"
                }
            },
            {
                "title": "Studying",
                "ns": "0",
                "id": "923",
                "revision": {
                    "id": "6880",
                    "parentid": "6879",
                    "timestamp": "2023-01-16T12:20:53Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "33666",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || '''[[:Category:Productivity Tools|Productivity Tools]]''' || '''[[:Category:Team Size 1|1]]''' || [[:Category:Team Size 2-10|2-10]] || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\n'''In short:''' This entry is a rather personal account on how it is to study at academia, provided by the Wiki team. It shall help interested students sort their own experiences being a new student.\n\n== Prof. Henrik von Wehrden ==\n'''What to expect when you study at Leuphana, or basically any given University'''<br>\nStudying is about learning. Learning will be the main focus of your whole day, and sometimes even beyond the day. Learning to learn is a key goal that schools still miss out on big time, even if the situation has already greatly improved. I only learned to learn properly at university in a structured and self-motivated way. Balancing your learning efforts against your motivational levels is something that you develop and learn, and different people learn differently, which is a trivial detail but important to consider given the pressure that many people perceive. Being in academia means tinkering with the skills you need as an academic, often your whole life, even if you do not remain in academia.\n\n'''About reading'''<br>\nAcademia is first and foremost about reading. Reading a lot is inevitable, and in addition you need to learn to cross-read large chunks of text and fast. Quite often it is not about getting every bit of the text, but instead the bigger picture. This is not some ability that you are being born with, but it will take years to evolve. Still, reading you must, and a lot, and all the time. The reading assignments of an average study program should amount to several ten thousands of pages, and since you have other interests that are also transported in written texts, it is important to develop strategies how to read. Find a spot that is comfortable, but not too comfortable. Also, work on your posture. Some people benefit from alternating locations. A location change for a power session can be a beneficial strategy. Also, try to find audiobooks, which may be available for some of the more mainstream stuff you have or want to read. My audiobook account is an important staple in my life, and contains hundreds of books. I also love books made of paper, but there is only so much I can read on an average day, and audiobooks are great during transit, gardening and sports. I also get a lot of inspiration from podcasts. Still, the staple in my branch of science are scientific articles and books.\n\n'''Doing Group work'''<br>\nScience is about collaboration. Many people have a misconception about this. Collaboration does not mean that you work together non-stop, are all exited standing in front of whiteboards and sitting at round tables, cheering each other on. Instead much of the time spent in group work is about planning together, working solitarily, and then bringing the different bits and pieces together. You still will have to do most of the work alone, otherwise it is going to be a huge time drag. It is an important skill to do brainstorming in a group, and a whiteboard can indeed go a long way. Group work is however often about dragging people along, and can be even about working against people. Compromising is a great skill, but how do you deal with unequal knowledge and experience in a group is often altogether different; an unbalanced work load can be the most destructive force in any group work. Therefore, it is important to find strategies how to cope with all of these challenges, which again takes practice. To be able to foster more effective groupwork in which you feel comfortable and you know how to deal with your different group members try out the Belbin test for group roles. Looking back at my own experience, I think it was important to learn how to adapt myself to diverse settings. Many problems we face in groups are because the group is a reflection of our own flaws, so that we can overcome them. This is why I like working in groups. \n\n'''Writing daily'''<br>\nIntegrating your thoughts quickly into a written text is at the heart of academia. If we want to learn to communicate our thoughts to others, we need to learn to write. Many people claim they are not good writers, yet I would counter argue that they are not good at developing their writing skills. The first way to become a good writer is to become a good reader. Being inspired by others can help you to consciously grasp why some sentences are better than others, and how you may borrow from the writers able to produce better sentences. It is surprising how well a professional educator on writing can empower you to this end, and at Leuphana we are fortunate enough to have the writing center that is excellent in teaching these insights. There are also some really good books on learning to write, and I would argue that among the more known books on writing, it is almost impossible to make any mistakes. Just get one that speaks to you or was recommended by somebody, and put time into it.\n\nAnother approach that was beneficial to me is to write about things that keep preoccupying your mind. If you keep coming back to a certain thought, yet cannot really verbalise why you cannot let it be, why not write about it? Writing can be a surprisingly catalytic and clarifying approach to structure and analyse your thoughts. Writing 500-1000 words per day should be a no-brainer for any aspiring academic. Bear with me, this will grow on you if your are lucky. Start with a research diary, reflecting and verbalising what you learned on this specific day. This only works if you make it a habit. Remember that mails and other communication counts into the word count. I sometimes receive mails that are pieces of art. New years resolutions are worthless to start a new habit like writing. You need to minimise the friction instead, finding the right time, place and mood that makes you automatically start writing no matter what. Me, I sit in the chair where I write most texts, listening to the \"Tales from the Loop\"-soundtrack that propelled about 90% of all texts I wrote in the last year. If I put on this soundtrack, my fingers start twitching almost by instinct. Writing should be a reward, as I think it is a privilege. Writing cannot be pressed between two other time slots, it needs to be free and unbound, allowing your mind to do nothing else. From then on it is to me how Jazz is in music. Much of Jazz music is hard work and practice, almost to the point where your subconscious takes over and you are in autopilot mode. You need to practice enough so that your lack of skill does not stop you from writing. To me, this learning curve is surprisingly rewarding, it is almost like learning to be a rock climber. The first day is the horror. All muscles ache, you are basically destroyed. This will get worse for a few days. Suddenly, after two weeks of daily practice you will surprise yourself. After three months of daily practice you will lift yourself easily up the wall on previously impossible routes, and to others your path looks smooth and easy going. Writing is just like this.\n\n'''Studying teaches you to try things out'''<br>\nBeside the three staples of academics - reading, group work and writing - learning at a University is also about many other opportunities to learn and grow. This list is very specific and context depended for each and every single person. Still, the general consensus is that studying is about trying things out, how you can learn best, and find out what you are good at, and how you can contribute best. Here are some points that I consider to be relevant.\n\n'''Soft skills'''<br>\nAmong the diverse term of soft skills are personal traits and approaches that basically help us to interact. While this could be associated to group work (see above), I think it is good to make a mind map that you keep updating and exchange about with others. This is nothing that you need to obsess about, but more like a conscious and reflexive diary of your own personal development. Actually, a research diary can be a good first step. Also, if you witness others that excel at a certain soft skill, approach them and ask them how they learned their respective skills. It is also quite helpful -surprise- to practice. Presentations are something that are often not right the first time, and constructive feedback from critical people that you trust goes a long way. Much of the literature and other resources on soft skills are often over-enthusiastic, and promise the one and only best approach. Do not let yourself be fooled by such simple fixes, some of the soft skill literature is rather fringe. Still, new approaches to knowledge and interaction await, much can be gained, and only a bit of time may be lost. Why not giving another soft skill a go? The most important step is then to try it really out. Doing meditation once will tell you nothing about it, yet after some weeks you may perceive some changes. Your first World Caf\u00e9 was a failure? Well, try it again, several times, in different settings. For soft skills you need to stay open minded.\n\n'''Digital natives?'''<br>\nWe are awash with information to the brim, and continuously on the edge of drowning in it. Mastering all things digital may be one of the most important skills in this age and place. I think the most important rule to this end is: Less is more. Evidence how bad too much exposure to the digital world seems to be is mounting. Social media made many promises, yet I am not sure how many were kept. I can acknowledge that it can create meaningful linkages, build capacity, and even be a lifeline to your distant friend. Nevertheless, I would propose to be very reflexive which emotions are triggered by social media within you. This may lead to the conclusion to detox. The same holds true for all things extreme, namely binge watching, youtube or Spiegel Online. Instead you need to become versatile in a word processor, Powerpoint, maybe a graphical software, and get a hold of your direct digital communication. E-mail is still a thing, and writing a good e-mail is a skill that is equally admirable and often missed out on by students. I have been there. Again, practice goes a long way. Also, be conscious about data structure, backups, and online plans. You should be able to impress others with you digital skills. This will open many doors, and tilt many opinions towards your direction. Get at it!\n\n'''Work-life-balance and motivation'''<br>\nCurb your enthusiasm. There are no simple answers here. Work-life-balance became a pretty big thing lately, and we all hope that the prices we paid as long as it was ignored will now not become the flip-side of the coin, since we basically talk non-stop about work-life-balance these days. Personally, I never really quite understood this hype. It is undeniable that having a balanced work-life dynamic is central and often overseen. However, having a filled curriculum after hours that is supposed to relax you by adding to your already busy schedule further to the brink may not be doing the trick. Having a nine to five schedule is no guarantee for a happy life, just like long working hours can be ok if you are ok with this. It is currently 21:36 when I write this text, and I do that because I want to do this. The danger is -I believe- if we let the system dictate us what we should do, and when. It does not matter wether it is about work or about relaxation. After all, it is really hard to relax on command, especially when you are hyped, and have still energy. All in all, I still give a note of caution. I overworked myself in the past, not only because of societal expectations, but also because I basically had no radar about my own balance, and how easy it can be thrown off. It took me a long time to figure this one out for myself, and I think in the spirit of being better safe than sorry, go easy on yourself. We are currently in an epidemic of psychological challenges, especially among the younger generation. We cannot go on like this. Being motivated is like the worst pressure point we ever discovered. If I can only put one piece of advise here, then I would suggest that you should always try to establish a path, and not goals. Being on a way and establishing the associated mindset is the most fundamental change we need. If we keep rushing towards goals, and keep changing these goals like all the time, and never acknowledge when we reach these goals, then all is lost. I much prefer to just be on a path, even if I am not clear in all points where it will lead me. You may write this one down on a teabag.\n\nStudying is a privilege, and a challenge. Practice reading, talk to other students, and start writing a learning diary. Being an academic to me means committing to lifelong learning, because neither the world nor knowledge stands still. It took me a while to figure learning against my other commitments in life, and I am still learning to this end as well. I am very glad to keep learning.\n\n== Christopher (B.Sc. Biogeowissenschaften, M.Sc. Sustainability Science) ==\n'''Being overwhelmed'''<br>\nWelcome to university! If you've come straight from school, you might be overwhelmed by your first semester. I certainly was - after all, each semester requires at least as much studying as your school finals. Every semester! It is natural to be challenged by this initially. I struggled with my first and third semester, then my bachelor thesis, then the first master semester. However, as cheesy as it may sound: you'll grow with these challenges. It is alright to skip a class if you cannot spend the proper time on it, or aren't quite feeling yourself. Not a question. Still, you won't be doing yourself a favor by postponing everything to eternity just because you can. Take on the challenge: You'll become more efficient and pragmatic by going through those three months, and next time, you'll better know how to deal with what awaits you. Cherish this process. I guess it's part of getting older.\n\n'''The right path'''<br>\nPicking a study program is hard. There is an incredible diversity nowadays, so how can you choose the right path for your future? You'll likely feel at some point like you didn't. Someone said to me in my first semester, \"if you've never doubted your study program, you haven't really studied\". I think there's truth in this. Few people will enjoy 100% of their study program. Maybe it's the professor of that one class, maybe the topic, maybe your assignment. However, this shouldn't let you doubt the general decision. There is a reason why you chose this exact study program. Don't get me wrong, there is no shame in starting new if things really don't feel right after some time still. And for some people, university just isn't right, and it's good to realize that and pursue one's real interests. At the same time however, you don't decide your life's fate with your bachelor's. Life isn't like an RPG game where you choose the wrong path and now you're a pickpocket in a close-combat situation. Oops. At university, you can choose a class from time to time that you are interested in, even if it doesn't seem to fit into your 'profile'. No one will care about your first semester grades once you graduated. Other things will be more important, like practical experience, and proving that you are passionate about things may outweigh the name of a study program.\n\n'''Studying early'''<br>\nI made good experiences being that guy who studies early-ish. I mean, every professor will tell you to start writing that essay, or start learning for exam, early, like TODAY, and not just the night before. Some eager students will accept this advice and start studying right away, and I guess that's good for them. You don't need to do that. I think it's already sufficient to just start a month or two early. Like, be prepared for the February exam by Christmas. Refresh and update your knowledge once more close to the exam, and enjoy the relaxation of not having to binge-study three days up to the exam. Essays, too, age like fine wine: draft something early after receiving the instructions, and give yourself some to think about it before continuing again and again. There are students who will do totally fine grade-wise with last minute all-nighters, and will claim that this was way more efficient. What they don't tell you about is the agony of these nights. Spare yourself that.\n\n'''Self-care'''<br>\nMy final advice may sound contradictory to what I wrote before, but hear me out. Take off time now and then. Yes, I recommend preparation, and I appreciate challenges, but only now that I finished studying, I have also come to cherish weekends. During my studies, I barely had any weekends, at least mentally. By this I mean that as a student, you are never really 'off' work. There's always something you could be writing, studying, or preparing. Especially during my later semesters this took a bit of a toll on me, so I'd recommend not doing that. There is a limit to everything, so let weekends be weekends. Or at least take a day off per week, where you actively divert your thoughts. You'll be fine if that assignment doesn't receive another five hours of care. Maybe it's rather you who needs these hours for yourself this time. This might come off cynical to anyone who's working next to studying, but if you can make it possible in any way, remember: Studying is not a sprint but a marathon. You'll need that breath. And now enough with the metaphors.\n\n\n== Ollie (Bachelor in Linguistics, B.Sc. Environmental Sciences, 5th semester) ==\nMoving to study in a foreign country can be crazy intimidating. Everything around is strange and you are on your own. At least that is what I thought, but Germany surprised me. You don\u2019t have to be alone, if you need help, the most important thing is just to reach out and ask for it. University staff and your peers can make your life and first months in the new country so much easier. Go around the campus, explore what it has to offer, check out initiatives and different events. I know it can all be quite overwhelming at first, but you do not have to have it all figured out right away. No one really has after all. Get a couple of people on your radar who you could ask for some assistance in case needed, make some friends if lucky. First buffer-months are necessary for acclimatisation and it is natural to question your choice of study. Pleasant surprise, professors and teachers are also very happy to advise you on your studying path! Check out when their consultation hours are and go speak to them: they might approach your question from a new perspective. With more experience and knowledge about the way things at the University are they could thus guide you towards your answer.\n    \nSo, as we say in Russia, two heads are better than one: you do learn much better in groups, because you get an opportunity to reflect. I first experienced it here in Germany and I was amazed. This does not only make studying easier, it also gives more diversity to the way you learn a subject, as well as sometimes puts you in a position\u00a0of learning by teaching. If you have a subject to learn on your own, do the same thing: explain it to yourself step by step, ask yourself questions. Do not make learning too much of a routine, the aspect of novelty can be crucial for our brains to stay focused and excited. Go to a new place from time to time, study in a cafe, at a different table in the library, at home on the sofa, or outside if the weather is good. And never forget to take breaks. Slow and steady is your key, be aware of the mighty burn out \u2013 it is the saddest thing not to be able to want to do things that you know you genuinely love. So pace yourself, start learning early and do active recall!\n    \nThe world worships the original, so stay true to yourself. However, mind the difference between anxious competition with the others and gaining inspiration from the people whose success you find admirable. Learn from them, talk to them, and believe in yourself: stay open-minded and keep trying, and don\u2019t take your mistakes too seriously. There is no way you can grow a muscle without slightly tearing it up now and then.\n    \nLast but not least, this is your journey, at the end. Ask yourself if it is what you want to do, there is no right or wrong way, there is only you and your life. Coming from a country where thinking outside the box was not encouraged, your skill of critical thinking might feel quite rusty in a European university, but eventually you catch up on that. Nothing is more refreshing than breaking the paradigms and generating new ideas.\n    \nSo, you go, pal! And good luck!\n\n\n== Linda (B.Sc. Environmental Sciences, 3rd semester)==\nBefore I say anything about studying, I should begin with a short disclaimer: I started my studies in COVID 19-times and gained my first experiences mainly in two full online semesters. Nonetheless, I already learned a lot about myself, universities and studying during this time.\n\nYou'll often hear: \"My time at university was the best time of my life.\" But what exactly do parents, other relatives, acquaintances or graduated students even mean by this, especially when sometimes studying just feels hard, overwhelming and never-ending? I suppose that life at university just offers so much that you can dive into, try out and learn. And this broad field of possibility, like-minded people, motivation and freedom is what makes studying great. What I want to say with this is, that studying has more to it than sitting through lectures. Try out engaging in an initiative and try to take advantage of the university's infrastructure (groups, clubs, organizations etc.). \n    \nMoving on to more practical stuff, studying to me, up until now, has been a lot about learning to learn and also learning to criticize. \n    \nFirstly, learning to learn: By being constantly overwhelmed by what you could do, you ideally start to see what you have to do and which things can wait or don't even help you learn at all. Sometimes, this may seem hard or there just isn't anything that isn't important. In this case, try asking peers to work together or spilt some work: When there are for example 6 important papers, then it could be helpful to assign 2 papers each (in a group of 3). Then everyone shares their notes and summarizes key messages - et voil\u00e0, less work load for each of you individually. When you don't see advantages in working together with peers, this might not be it for you, but maybe just give it a try. \n    \nSecondly, learning to criticize: Many of the concepts, approaches, ideas and facts taught in the first semesters were completely new to me. Don't let yourself be too caught up in this amazement and try to always think a step further. What could be strengths or weaknesses? Can this be combined with something else that I learned? This approach may help understanding some of the seemingly indisputable, abstract concepts. The knowledge you gain through studying shouldn't just stay in your notebook or laptop. It should wander through your brains, be discussed and differentiated. You'll see that many things make more sense along the way and even more so, when you develop an opinion on them.\n\n\n== Elli (B.Sc. Environmental Sciences, 7th semester) ==\n'''Relax!'''<br>\nYou may be overwhelmed in your first semester especially during your first weeks. Certainly I was. There is a new city to explore (and L\u00fcneburg offers you just very much!), new people that you live with and not to mention a university where you have to arrive and to figure out how to study. But I can tell you, you will get used to it. Do not stress yourselfs in the beginning. You don't have to be everywhere, meet everyone and read and study everything. Your bachelor will take at least three years. You have plenty of time to explore the beautiful city of L\u00fcneburg, engage yourself in clubs, meet your friends, go to parties and of course study. So my advice for you would be: Take it easy and don't forget to rest in between all your busy and full packed days!\n\n'''You'll change, and that's ok'''<br>\nI chose L\u00fcneburg and especially Environmental Sciences because it interested me the most back in 2017. First I thought that I would apply for the Envi-Program and go to Sweden for one year but in the end I didn't. Today I would say that I am a truly different person than I was when I started my studies here in L\u00fcneburg. I came here and thought that I would end in the natural sciences, doing something with chemistry or even climate physics and generate new data that would be needed to understand climate change. Today that sounds really far away for me. Today my interests, wishes and goals have completely changed. That does not mean that everyone of you will have experience the same. But I want to encourage you to stay open minded and to find your way. To find a field that truly interests and inspires you. I think university can be the best place for that.\n\n'''Self-organization'''<br>\nSometimes people ask me how I manage to do so many different things at the same time and never forget something. I would say the big mystery behind that is: being organized. The only solution to get university, work, engagement and social time under one roof for me is having a planner in which I write down all my appointments, all my to-dos and all the small important things that I need to remember. Again find the solution that suits you best. I know many that use notion as their key tool to organize their lifes. Not for me. I am old-school. I have to write things down to remember them.\n\n'''How to study: an easy recipe'''<br>\nI know many of you wish that there would be an easy-peasy recipe on how to study that I hand you right now and then you are done. I don't think this is possible. That is actually why we wrote this article. To show you the diversity of formats and options that you have. Try them out and find the best way for yourself. My usual day for example is structures into sessions of 1.5-2 hours. In this time I am doin something. Reading a text and making notes, doing something for work, watch a lecture etc. Then I will take a digital break. Means I will do some house work (laundry, dishes, whatever you like) or run some errands (fresh air is the key!). The most important thing is for me that I will allow my mind to rest and not overload it with new content in my break.\n\nThere is nothing more for me to say than this is your study time here at Leuphana. Find your own way, make the best of it and do not pressure you!\n\n== Oskar (B.Sc. Environmental Sciences, 5th semester) ==\n\n'''Feeling overwhelmed'''<br>\nStarting your studies at a university can be overwhelming. New city, new living situation, new people, new structures, new phase in life. It is common to feel like this and it is quite reasonable.\nWhat has helped me, is to get acquainted with university structures (for example the AStA) and connect with fellow students. These people will often feel similarly and I have found that talking with each other about struggles helps a lot - especially when neverending to do lists and readings start to kick in after the first very exciting two weeks.\nHowever, I am here to tell you also that readings will get easier over time as you learn the meaning behind certain words and methods. Moreover, you don\u2019t have to understand everything from the beginning whether that be in texts or lectures. Oftentimes the more further input you get, the better you can contextualize previous topics and see connections between lectures. For me, this still happens in parts until the last third of the semester.\nSo relax and focus on the steps in front of you, not the whole staircase! You can do this and you are not alone!\n\n'''The power of teamwork''' <br>\nTeam up with other people and embrace the strength of teamwork so you don\u2019t have to read every paper to the t. Building study groups has really helped me as well. It comes in very handy to have a few people to ask questions if you didn\u2019t understand something or in case you are sick. In terms of exam preparation, it offers the chance for active recall, where one person questions others.\nIf you need to work on something alone, I recommend having people to co-work with to hold each other accountable. I have learnt that the sole act of having other people in the same room studying helps a lot and to makes working more enjoyable.\nDon\u2019t shy away from seeking help from your fellow students or even teachers. Most are happy to help you out or even give some advise.\n\n'''Doing group work'''<br>\nDuring your studies, you will need to do quite some group work. These are some insights I have gained along the way:\nGet to know each other a bit. Especially in your first semester, it is probable that you will need to work with people you know nothing about (yet). Taking the time to chat a bit before diving into groupwork-related topics will make communicating easier in later stages of your work.\nSpeaking of communication: depending on your group work\u2019s extent, you might want to clarify how to keep in contact (e.g. Telegram chat, shared online board), how fast replies are expected, how frequently meetings should take place, if anyone has preferred tasks they enjoy doing (and then assign roles to people accordingly), whether to write protocols to keep track of your process and decisions, how to share notes, how to manage in case someone is sick etc. It might seem a bit unnecessary and much management-related, yet this, too, pays off in the long-run from my experience.\nIn order to move forward with group processes and content, it is crucial to come prepared and on time. This is just a mere sign of respect for the people you are working with. Also, if you are not able to accomplish a set to do, goal or meet an internal deadlines like you were supposed to, that\u2019s okay - it happens to all of us! However, be sure to let your group members know as soon as you can and communicate transparently.\nIn general, I have found that short check-ins on people, for example at the beginning of meetings, are a game-changer. It allows for more empathy and kindness towards each other and can reduce tensions.\n\n'''Personal development''' <br>\nBesides learning about topics related to your study programme and scientific methods, university is about learning to structure yourself as well. Try out what works for you, whether that be getting up early or working late at night. Personally, I like to get the to do\u2019s requiring high concentration out of my way first and then proceed with more fun tasks. Especially during stressful periods I find it very important to take breaks in between where I go for a short walk or enjoy a cup of coffee. \nMoreover, structuring yourself also means learning to prioritize. It is okay to push some things if they are not of high relevance at the moment to make more time for things that are. However, be careful and don\u2019t let it get a habit to completely avoid tasks. Hold yourself accountable!\nFind a system of keeping track of your lecture and seminar notes. Organise and back-up your data regularly (!) or use a programme that uses cloud storage (like notion, oneNote,\u2026). Nothing is more painful than a laptop crushing on page 12 of a term paper without a copy. Furthermore, some notes might be useful in later semesters and finding them quickly when needed is super helpful.\n\n'''Looking left and right, and taking time''' <br>\nAlthough the official study plan is designed with 30 CP each semester in order to finish your bachelors in 3 years, there is no need to stick to this plan! Doing the workload of 30 CP is challenging and if you experience that it comes at costs of your physical well-being or mental health, there is no shame in dropping a course to make it a little easier. For example, I have realized after my first semester that I would rather only do 20 or 25 CP per semester but feel like I can really get into these topics and have time to do other activities outside my studies that bring me joy and recharge.\nThe same motto applies for courses outside your regular studies: if you would like to explore a new topic or method, go for it - that\u2019s the beauty of the interdisciplinarity model at Leuphana ;)\n\n'''The future might be unknown''' <br>\nYou might change during your time at university and so can the topics you are passionate about. This and the thought of where you will end up after your bachelor can be intimidating. Stay open-minded, curious, and be patient with yourself. You don\u2019t need to have it all figured out! \nWhat has helped me navigating this struggle is making use of different ressources at Leuphana, for example the Student Counselling Service and Career Service, or speaking to lecturers about their career paths. Most of them are happy to offer words of advise and provide some guidance.\nRemember that this is about you and your future. Do what '''you''' consider right and purposeful. \nGood luck!!\n\n----\n[[Category:Skills_and_Tools]]\n[[Category:Personal Skills]]\n[[Category:Productivity]]\n[[Category:Team Size 1]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Henrik von Wehrden, Christopher Franz, Olga Kuznetsova, Linda von Heydebreck, Elisabeth Frank and Oskar Lemke"
                    },
                    "sha1": "otyx73206ay5gdnjlie9rhfvym8x1l9"
                }
            },
            {
                "title": "Supply Chain Analysis",
                "ns": "0",
                "id": "911",
                "revision": {
                    "id": "6873",
                    "parentid": "6472",
                    "timestamp": "2023-01-14T13:42:42Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "28392",
                        "#text": "[[File:ConceptSupplyChainAnalysis.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Supply Chain Analysis]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br>\n'''In short:''' Supply chain analysis is the systematic analysis of different stages in a production cycle.\n\n== Background ==\n[[File:Supply Chain Analysis scopus plot.png|400px|thumb|right|'''SCOPUS hits per year for Supply Chain Analysis until 2022.''' Search term: \"value chain analysis\" OR \"supply chain analysis\" in Title, Abstract, Keywords. Source: own.]]\nIn the 1990s, the corporate view on logistical processes shifted due to changes in the manufacturing environment, including increasing costs, globalized markets, shrinking resources and more (1). '''Instead of a rather vertical, hierarchical understanding, logistical processes were seen as functional, integrated and important for managing a company.''' Thus, the framework of supply chains (and their analysis) emerged and initiated a new management discipline: supply chain management; including new corporate positions such as a firm's Supply Chain President. Different approaches of analyzing supply chains were created, investigating either physical flows of material, underlying organizational segments or supply chain processes, i.e. purchasing, sales etc. (9).\n\nThis last approach began to evolve in 1996 with the development of the SCOR (Supply Chain Operations Reference)-model by the Supply Chain Council (SCC), which aimed at standardizing the analysis and terminology of supply chain analyses (14) [S\u00fcrie & Wagner, p. 41]. This introduction for the first time provided a systematic and standardized framework for supply chain assessments.\n\nDuring the time after the introduction of the SCOR model, each process within a supply chain was typically analyzed and studied individually. In modern SCAs, however, the supply chain is seen as a complex process and as an entity consisting of multiple parts. Recently, there have been more holistic, transformative approaches to supply chains and their performance, called 'closed-loop supply chain management'. Supply chains are called 'closed-loop' when they combine a traditional forward chain with reverse processes to recover products that then eventually re-enter the forward supply chain (11, p.247). This can be explained by the increasing wish to optimize a supply chain's performance and to include \"reverse logistics\" (meaning recycling and reusing of products) [1].\n\nMore recently, also, transdisciplinary research of supply chains, i.e. closed-loop SCM, has been emerging, creating new knowledge by enabling multiple forms of collaboration (inter-academic, management - academic, academic - non-academic (11). Disciplines that play a crucial role in this approach are: natural & engineering sciences, management sciences and the industry itself (11). Through collaboration and information exchange, innovation and collaborative optimization can be achieved (11). Especially for Corporate Sustainability (i.e. an implementation of sustainable strategic management into the core business to create value for stakeholders), the approach of Closed-loop SCM plays an important role (11).\n\n\n== What the method does ==\nA supply chain can be defined as an \"integrated manufacturing process wherein raw materials are converted into final products, then delivered to customers\" [1, p. 282]. Supply chains combine production planning and inventory control processes as well as product distribution and logistical processes.\n\n[[File:Supply Chain Analysis - Process (Beamon p.282).png|600px|thumb|center|Supply chain process visualized as a whole. source: Beamon, p. 282]]\n<br>\n\n'''The analysis of an existing supply chain typically serves as the basis for an improvement process in an industrial or economic sector.''' The operations and processes along and within the supply chain are studied in detail, keeping both a strategic and operational view. The findings are put into an initial analytical model. These can be:\n\n* deterministic (variables are known and specified),\n* stochastic (at least one variable unknown; assumptions on its distribution are made),\n* economic (relationship matrix of product and process specificity; capturing risks for respective buyer and supplier), or \n* simulating (evaluating effects and effectiveness of strategies on supply chain and demand patterns; simulating improvement strategies),\n\ndepending on the input and objective of the analysis (1). Adaptations to the modeled supply chain can be made when the real-world supply chain changes and a development in its performance is observed over a long timespan [S\u00fcrie & Wagner, p. 37].\n\n'''A supply chain analysis consists of two main tasks:''' the ''process modeling,'' meaning the abstraction of real-world processes into a model, and the ''performance measurement.'' The latter revolves around identifying metrics to measure a supply chain's success and economic performance (14)]. These tasks can be divided into more detailed steps as follows:\n\n==== Process modelling ==== \n1. Identifying the most important processes in the supply chain, which typically include: customer relationship and service management, demand management, order fulfillment, manufacturing flow management, supplier relationship management, product development (commercialization) and lastly, returns management. These processes \"can be traced best by the flow of materials and information flows\" (14, p.39. Notably, not all processes are equally important in a given company or for a given product or service. Further, there might be interferences between different processes, so it is helpful to focus on these eight core processes and always keeping a strategic view on the supply chain (14, p.39).\n    \n'''SCOR-models''' comprise three relevant hierarchical levels of standard processes in supply chains. With each level, the amount of detail in the analysis of the process (or interaction between supplier and customer) increases. Lower level processes can be arranged in higher levels. From the highest to lowest level, these levels are: \n    \n* Process types: on this level, the supply chain is arranged into ''plan, source, make, deliver, return'' (16, see visualisation below). \n\n* Process categories: on this level, there are 26 categories in total as part of the process types. They can be split into the meta-categories ''planning, executing'' or ''enabling.'' (see visualisation below).\n** The ''planning'' category describes the initial planning step to each process which entails the \"allocation of resources to the expected demand\" (14, p.43).\n** The ''executing'' category can be understood as the moment of realizing and executing the planned processes, \"triggered by planned or current demand\" (14, p.43). Therefore, this category covers the supply chain from the source processes to the return processes.\n** Processes in the ''enabling'' category support the other processes by providing the flow of information and process interrelations.\n\n[[File:SCOR Model Level 1 & 2.png|600px|thumb|center|'''Visualization of process types and categories below according to the SCOR Model.''' Source: [S\u00fcrie & Wagner, p. 44]]]\n\n* Process elements: on the third level, the processes are decomposed by looking at specific input and output streams for each process and individual element. There exist detailed metrics that evaluate the \"reliability, flexibility, responsiveness, cost and assets\" (14, p.46) to help the analysis.\n\n2. Tracing the flow of material (e.g. goods), information (e.g. purchase orders) and finances (e.g. payments) along the chronological sequence from the supplier to the customer, or reverse in case of returns (14).\n3. Translating the observations into a standardized language, for example the process chain notation, which was developed by Kuhn in 1995 and allows for a hierarchical structuring of sub-processes. This prevents misconceptions, helps to structure the processes hierarchically and to focus on the most important processes (14).\n4. Visualizing and modeling the supply chain processes by using standardized tools, e.g. [http://iieom.org/ieom2011/pdfs/IEOM153.pdf ARIS]. To this end, the aforementioned SCOR-model is still the most popular and widespread approach also for this step (14). \n    \nIn short, the application of the SCOR model for the visualisation and modelling of supply chain processes entails:\n    \n4.1) defining the business unit that is to be analyzed. Such areas of business or a given company can be organization, sales, controlling or even a combination of key aspects in a successful supply.\n    \n4.2) geographic placement and denomination of entities in supply chains. In global supply chains, suppliers and production steps can be highly dispersed, which is why it is important to map and indicate all relevant locations in the supply chain (14).\n    \n4.3) + 4.4) defining major material flows and visually connecting locations on the map with lines with regards to the specific process type ('source', 'make', 'deliver', 'return').\n    \n[[File:SCOR Model Visualisation 1.png|600px|thumb|center|'''Visualization of step 4.4 in the application of the SCOR Model for Supply Chain modelling.''' Source: [S\u00fcrie & Wagner, p. 48]]]\n\n4.5) defining partial process chains. Here, a subchain/a chain is defined that follows only one specific product group. This serves the purpose of further breaking down the complex, interwoven material flows defined above.\n\n4.6) Finally, drawing dashed lines for the process type 'plan', therefore illustrating execution and planning processes. Here, P2-P5 are the different planning process categories mentioned above (Figure 2.1) [S\u00fcrie & Wagner, p. 48].\n\n4.7) Lastly, defining a 'top-level' planning process, i.e. a process that coordinates and connects two or more partial process chains. This may not be possible for every supply chain due to a lack of overarching processes. \n\n[[File:SCOR Model Visualisation 2.png|600px|thumb|center|'''A possible visualization of steps 4.5 and 4.6.''' This shows the interconnectedness and interdependence of multiple subchains and processes within the whole supply chain. The abbreviations are listed in table 2.1 further above. Source: S\u00fcrie & Wagner, p. 48]]\n\n\n==== Performance measurement ====\nAfter the supply chain is modeled (based on the described processes), an SCA includes the measurement of the supply chain's unique performance. \"A performance measure, or a set of performance measures, is used to determine the efficiency and/or effectiveness of an existing system, or to compare competing alternative systems\" (1, p. 287). To do so, the most important metrics have to be defined and connected with the individual supply chain strategy and goal. Measures can be qualitative or quantitative and can be divided into more specific indicators (1, p. 287). These indicators are either ''informing'' (i.e. helpful for management and decision-making, comparing indicators with specific values), ''steering'' (i.e. guiding towards a set target/outcome) or ''controlling'' (i.e. helpful for supervising) (14). \n\n[[File:SCOR Performance Attributes Ayyildiz & Gumus.png|600px|thumb|center|'''One possible approach to sort SCOR Performance Attributes.''' Source: Ayyildiz & Gumus 2021, p.562]]\n\n'''There are different approaches to grouping and assessing key indicators for SCAs.''' One possible way to sort them is according to the following aspects:\n* delivery performance: Indicators that are oriented on customers (expectations, reactions etc.) and surround the quality of customer service and ordering, as well as the timing of delivery (14).\n* supply chain responsiveness: Indicators surrounding the quantified ability of adjustment to changes in the market. This is important because \"[s]upply chains have to react to significant changes within an appropriate time frame to ensure their competitiveness\" (14, p.55). For example, the number of days a supply chain needs to respond to an increase of x% in demand could be such an indicator.\n* assets: Indicators dealing with asset (capital), revenue (sales/turnover), and/or transactions. In general, these indicators measure a company's asset efficiency but may vary across industries.\n* inventories: Indicators analyzing the inventory. These can either focus on the age of the inventory (= the amount of time the product was stored/in stock), or on \"the ratio of total material consumption per time period over the average inventory level of the same time period\" (14, p.55). Increases and decreases in inventories are observed and connected to components such as costs or possible changes in value and demand. For the result to be as complete and objective as possible, a holistic view on the supply chain is of utmost importance (14). In supply chains with inflow and outflow processes, inventories (storage) are necessary - a fact often overlooked. Inventories have to be dealt with as an aspect that at the same time causes costs but also benefits (1, 14).\n* costs: Financial indicators related to cost accounting. Other aspects of interest in this indicator category are productivity (employment value), product quality and problems regarding warranty (guarantee) costs (14).\n* another possible indicator that is often considered important is productivity which can be measured by the ratio of revenue (i.e. in $) per labour time (i.e. in hours).\n\nFor further reading and a detailed list of performance measurement indicators, see Beamon (1998, see References).\n\nWith the process modelling and performance measurement done, the SCA is completed. Based on the resulting insights, the supply chain can then be managed (SCM = supply chain management), optimized and its performance can be improved. The SCM often pursues a specific aim, i.e. to generate more profit, to become more efficient, to adapt to new regulations or react to societal demands and become more sustainable or fair. \n\n==== Role of SCA for Science ====\nSCAs build the basis for any further management, modification and innovation of supply chains. They result in a visualized analysis of the current state of a supply chain. This has value for academic research in multiple disciplines: the results of an SCA can be used as a basis for [[Survey Research|surveys]] that aim at improving stakeholder relationships and economic success. They can be used for transformative [[Scenario Planning|scenario planning]] and the modeling of alternative supply chains. The results can further be compared to other existing SCAs, e.g. in a [[Meta-Analysis]]. This can be done to either optimize research frameworks and methodological approaches, to enable a more transparent insight into a company's supply chain and management decisions, or to [[Experiments and Hypothesis Testing|test other hypotheses]] surrounding supply chains (14). Regarding the SCOR model, there are multiple new versions and applications, such as the GreenSCOR model, which tackles sustainable issues along supply chains and the shift towards corporate sustainability (7). This, too, opens new possibilities for research on supply chains.\n\n\n== Strengths & Challenges ==\n* SCAs enable an overview on the supply chain as a whole. In a second step, they allow for adjustments that can make supply chains more profitable and efficient.\n* In a more transformative thinking, supply chains can become fairer and more sustainable through SCAs. Considering the increasing legal demands for more transparent supply chains and corporate responsibility, this can be seen as a very important aspect for action-oriented science focussing on aspects of transparency, fair labor, resource efficiency and ethical business management etc.\n* Most indicators for a supply chain and its performance are [[Category:Quantitative|quantitative]], since they need to be measurable and comparable (14). Non-quantitative ([[Category:Qualitative|qualitative]]) aspects are harder to quantify and are thus often neglected in the process. There are however some emerging approaches to a more qualitative analysis, i.e. the so-called model of sustainable supply chain management integration (SSCMI). With this, researchers try to embed qualitative and quantitative factors leading to an integration of sustainable standards (see 17, p. 221-223).\n* Following Thompson (1967), there are three types of interdependencies, i.e. relational dependencies between entities in e.g. inter-organizational relations. SCAs cover only the ''sequential'' interdependencies, i.e. dependencies with serial structures where the decisions and activities of one supplier (or firm) influences those of the next firm. Thinking of a supply chain as a linear, chronological sequence, starting with a supplier and ending with a customer, the sequential interdependence seems fitting. This however leads to the other two types not being integrated. These are ''pooled'' interdependencies (\"each individual in a group makes a discrete, well-defined contribution to a given task\" (6, p. 11) and ''reciprocal'' interdependencies, which \"involves simultaneous, ongoing relationships between parties in which each agent\u2019s input is dependent on the others\u2019 output and vice-versa\" (6, p. 11). The implementation of the other interdependencies could however create new knowledge and offer new insights into the supply (value) chain.\n    \n[[File:Representation of the interdependency-types in SCAs. Lazzarini, p. 11.png|600px|thumb|center|'''Representation of the interdependency-types in SCAs.''' Source: Lazzarini, p. 11]]\n    \n* SCAs are additionally confronted with the challenge of combining measures of productivity, eco-friendly and ethical production, and economic efficiency into one analysis model and standardized indicators. This may lead to prioritizing and a narrow focus that contradicts the \"cross-functional process-oriented nature of the supply chain\" (14, p.50).\n* Industries differ. Some differ in the asset-revenue relationship, whereas some might differ in the global-local scale of their supply chain (14, p.55). The difference in industries goes hand in hand with issues in a comparison of supply chains and a normative viewpoint on them. For example, for a long time, a main focus of SCAs was on supply chains of industrial production, while SCAs have only recently started to be conducted in agricultural production.\n* An important distinction can be found between ''innovative'' and ''functional'' product supply chains. The former can be characterized by short product life-cycles and unstable demands but at the same time high profit margins and flexibility to react to changes in supply and demand. When analyzing innovative supply chains, one has to keep the market-orientation and -responsiveness in mind. The latter functional product supply chains comprise the exact opposite, and an SCA would have to consider their tendency to prioritize efficient \"cost reductions of physical material flows and [...] value creating processes\" (14, p.38). Consequently, there is a difference in suitable performance measures (4).\n\n\n== Normativity ==\n* The interpretation of performance indicators, such as productivity, has to be linked to causal models of underlying processes, i.e. \"when calculating productivity[,] a causal link between revenue and labour is assumed implicitly\" (14, p.50). [[Causality]] itself however is always prone to normativity.\n* Keeping a holistic view on the supply chain and its management is crucial because \"overall supply chain costs are not necessarily minimized, if each partner operates at his optimum given the constraints imposed by supply chain partners\" (14, p.39). When a SCA shows some potential areas for improvement for a product's supply chain, a firm might want to act on the findings. One has to keep in mind however, that each firm, each subchain and each process is interwoven and interdependent. Actions and decisions at one point of the supply chain therefore have huge impacts on following processes. There exist \"attempts to match the performance metric of individual supply chain managers with those of the entire supply chain, in an attempt to minimize the total loss associated with conflicting goals\" (1, p.291). This view is deeply normative, since it is not possible to consider every partner's interests. In case of global supply chains, where the majority of power and profit is often centered in Europe, and the majority of production is centered in other parts of the world, a euro-centristic view may be clouding the analysis. Issues of a colonialist history and even trends of neo-colonialism can be very problematic and the source of several [[Bias and Critical Thinking|biases]].\n* SCAs can be combined with other methodological approaches. These include a [[Meta-Analysis]] of multiple supply chains of a specific product, potentially over time, to gain insights into aspects such as the carbon footprint [2, 5, 8]. [[Life Cycle Analysis|Life cycle assessment]] can be named as another important method in this field that attempts to analyze the (socio-)environmental impact of a product through its lifespan.\n* A SCA can be a useful tool to investigate supply chains for ethical and environmental concerns.\n** In terms of the environment, there is an increasing trend to improve supply chains, e.g. in terms of the length of product life cycles and recycling issues (1). SCAs can be used to \"address problems in the food system\" (5, p. 336) or to asses issues concerning aspects of sustainability, for example the carbon footprint or chemical pollution of the environment along the production and consumption of goods and services.\n** Regarding human ethical issues, issues of transparency of unethical behavior can emerge along the supply chain. At each step, from the farmers to the workers in production, the retailers, suppliers and the customers, people interact with each other and ethical decisions are taking place. Customers can especially influence fair trade, transparency, and ethical development or a lack thereof among the supply chain with their consumption behavior (12). However, some aspects are focussed on more frequently, e.g. child labor, wages and ecological sustainability of production processes. Agreeing on what is \"fair\" often is a first challenge when tackling ethical issues along supply chains because the definition of fairness may differ among stakeholders (12). As an example for new developments on this aspect, the German government [https://www.bmz.de/de/entwicklungspolitik/lieferkettengesetz recently agreed] on a new law that requires companies to assure the compliance of basic human rights along the supply chain. This can be seen as a first attempt to govern and influence global supply chains regarding the working conditions.\n\n\n== Outlook ==\nThere are several new approaches regarding SCAs. A uniting criterion is that all approaches combine SCAs with another method or disciplinary focus. \n* SCAs can, for example, be interpreted into \"netchain analysis\" (6, p.7). Here, researchers go a step further than classical SCAs, taking into account all types of interdependencies (pooled, sequential & reciprocal). This is possible by combining a SCA with a network analysis.\n\n[[File:Netchain Analysis 1 Lazzarini p.8.png|500px|thumb|center|'''An example of a Netchain''' Source: Lazzarini p.8]]\n<br>\n[[File:Netchain Analysis 2 Lazzarini p.14.png|500px|thumb|center|'''An Overview of Netchain Analysis.''' Source: Lazzarini p.14]]\n\n* The Internet enables new forms of exchange, transaction, standardization, collaboration and also inter-organizational relationships.\n* A field of sustainable supply chain management (SSCM) is also emerging and staring to not only optimize a supply chain in economic terms but in social and ecological terms, too (13). It has to be kept in mind, that global supply chains hereby differ greatly from local supply chains that function on a smaller scale.\n\n\n== Key Publications ==\nBeamon, B. M. 1998. ''Supply chain design and analysis: Models and methods.'' International journal of production economics 55(3), 281-294.\n\nS\u00fcrie, C. & Wagner, M. 2005. ''Supply chain analysis''. In Supply chain management and advanced planning, 37-63. Springer, Berlin, Heidelberg.\n\n\n== References ==\n(1) Beamon, B. M. 1998. ''Supply chain design and analysis: Models and methods.'' International journal of production economics 55(3), 281-294.\n\n(2) Darmawan, M. A., Putra, M. P. I. F.,  Wiguna, B. 2014. ''Value chain analysis for green productivity improvement in the natural rubber supply chain: a case study.'' Journal of Cleaner Production 85, 201-21\n\n(3) Dass, M. & Fox, G. L. 2011. ''A holistic network model for supply chain analysis.'' International Journal of Production Economics 131(2), 587-594.\n\n(4) Fisher, M. L. 1997. ''What is the right supply chain for your product?''\u00a0Harvard business review 75, 105-117.\n\n(5) Hawkes, C. 2009. ''Identifying innovative interventions to promote healthy eating using consumption-oriented food supply chain analysis''. Journal of Hunger & Environmental Nutrition 4(3-4), 336-356.\n\n(6) Lazzarini, S. Chaddad, F. & Cook, M. 2001. ''Integrating supply chain and network analyses: the study of netchains.'' Journal on chain and network science 1(1), 7-22.\n\n(7) Ntabe, E. N. LeBel, L. Munson, A. D. & Santa-Eulalia, L. A. 2015. ''A systematic literature review of the supply chain operations reference (SCOR) model application with special attention to environmental issues.''\u00a0International Journal of Production Economics 169, 310-332.\n\n(8) Onat, N. C., & Kucukvar, M. 2020. ''Carbon footprint of construction industry: A global review and supply chain analysis.'' Renewable and Sustainable Energy Reviews 124, 109783.\n\n(9) Poluha, R. G. 2007.''\u00a0Application of the SCOR model in supply chain management''. Cambria Press.\n\n(10) Radwan, A. & Aarabi, M. 2011. ''Modeling Supply Chain Management Information System Using ARIS Framework.'' In\u00a0Proceedings of the 2011 International Conference on Industrial Engineering and Operations Management Kuala Lumpur, Malaysia, January, 22-24.\n\n(11) Sahamie, R. Stindt, D. & Nuss, C. 2013. ''Transdisciplinary research in sustainable operations\u2013an application to closed\u2010loop supply chains.'' Business Strategy and the Environment 22(4), 245-268.\n\n(12) Schlegelmilch, B. B. & \u00d6berseder, M. 2007. ''Ethical issues in global supply chains.'' Symphonya. Emerging Issues in Management (2), 12-23.\n\n(13) Song, W. Ming, X. & Liu, H. C. 2017. ''Identifying critical risk factors of sustainable supply chain management: A rough strength-relation analysis method.'' Journal of Cleaner Production 143, 100-115.\n\n(14) S\u00fcrie, C. & Wagner, M. 2005. ''Supply chain analysis. In Supply chain management and advanced planning'', 37-63. Springer, Berlin, Heidelberg.\n\n(15) Taylor, D. H. 2005. ''Value chain analysis: an approach to supply chain improvement in agri\u2010food chains.'' International Journal of Physical Distribution & Logistics Management.\n\n(16) Webb, G. S. Thomas, S. P. & Liao\u2010Troth, S. 2014. ''Teaching supply chain management complexities: A SCOR model based classroom simulation.''\u00a0Decision Sciences Journal of Innovative Education\u00a012(3), 181-198.\n\n(17) Wolf, J. 2011. ''Sustainable supply chain management integration: a qualitative analysis of the German manufacturing industry.'' Journal of Business Ethics 102(2), 221-235.\n\n(18) Ayyildiz, E. & Gumus, A. T. 2021. ''Interval-valued Pythagorean fuzzy AHP method-based supply chain performance evaluation by a new extension of SCOR model: SCOR 4.0.''\u00a0Complex & Intelligent Systems\u00a07(1), 559-576.\n\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\nThe [[Table of Contributors|author]] of this entry is Linda von Heydebreck."
                    },
                    "sha1": "5x8ymtbzkm8wpfjufls5iyjrw8qjc6k"
                }
            },
            {
                "title": "Survey Research",
                "ns": "0",
                "id": "897",
                "revision": {
                    "id": "6422",
                    "parentid": "6394",
                    "timestamp": "2021-10-21T06:35:48Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "33897",
                        "#text": "[[File:ConceptSurveyResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Survey Research]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | [[:Category:Qualitative|Qualitative]]\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br>\n'''In short:''' Surveys are highly systematized and structured forms of data gathering through written or oral questioning of humans. This entry includes Structured Interviews and Questionnaires as forms of Survey Research. For more Interview forms, and more on Interview methodology in general, please refer to the [[Interviews|Interview overview page]].\n\n\n== Background ==\nSurveys were first applied in research in the field of sociology in the 1930s and 1940s (1, 2). Back then, the sociologist Paul Lazarsfeld used surveys to study the effects that the radio had on the public's political opinions in the US (1). Today, surveys are considered a '''widespread quantitative method in the social sciences''' (1).\n\n\n== What the method does ==\nA survey, i.e. survey research, is a set of research methods which uses \u201cstandardized questionnaires or interviews to collect data about people and their preferences, thoughts, and behaviors in a systematic manner\u201d (Bhattacherjee 2012, p.\u00a073; 2). Surveys are commonly applied in \u201cdescriptive, exploratory, or explanatory research\u201d (Bhattacherjee 2012, p.\u00a073). They are used to \u201cinvestigate social phenomena and understand society\u201d (Brenner 2020, p.\u00a02) as well as \u201cto inform knowledge, challenge existing assumptions, and shape policies\u201d (Gideon 2012, p.\u00a03).\n\n==== Types of Surveys ====\nSurvey research can be divided into an 'exploratory' and an 'explanatory' type (8). Explanatory survey research, which is typically seen as more important, \u201cis devoted to finding causal relationships among variables. It does so from theory-based expectations on how and why variables should be related. [\u2026] Results then are interpreted and in turn contribute to theory development.\u201d (Malhotra & Grover 1998, p. 409). By comparison, exploratory survey research is an inductive approach and is not based on theories or models. It rather aims at understanding a topic or measuring a concept better. This type can thus be especially helpful in early research stages and can prepare the ground for explanatory survey research and hence for studying the variables in-depth. (8). Even though more and more researchers follow an inductive approach by e.g. applying exploratory survey research to contribute to theory development, the deductive explanatory approach still predominates survey research (88).\n\nSurvey types can further be distinguished according to their mode, their target population, the data they collect, and the number or duration of data collection activities, among other variables (3). The survey mode, i.e. how the information is collected, exists in two types: '''questionnaire surveys and structured interview surveys'''. Questionnaires, also called self-completion surveys, are filled out independently by the participants either online or on a piece of paper e.g., after having received them by mail or handed out on the street. By comparison, an interview survey has an interviewer present the questions of the questionnaire to the participant, either in person (face-to-face), via the telephone or in the context of a focus group (1; 3). Interview surveys might be the mode of choice \u201cwhen the survey is long [...] or when additional information has to be collected\u201d (Gideon 2012, p.\u00a014).\n\nSurveys can also be distinguished based on the times or duration of data collection (3). Surveys which involve a single data collection activity are called 'cross-sections'. Surveys that are repeated regularly by addressing a different sample of the target population every time are called 'repeated cross-section' or 'longitudinal' surveys. Here, the latter term emphasizes the survey\u2019s comparative nature over time. A survey that targets the same sample with each regular repetition is called a 'longitudinal panel' and allows for examining \u201cchange at an individual level\u201d (Gideon 2012, p.\u00a016).\n\n==== Sampling for Survey Research ====\nWhen planning a survey, '''a population of interest \u2013 the target population \u2013 needs to be identified, of which only a proportion \u2013 the sample \u2013 is asked to participate''' (3). A survey is used \u201cto obtain a composite profile of the population\u201d (Gideon, 2012, p.\u00a08), but neither intends \u201cto describe the particular individuals who, by chance, are part of the sample\u201d (Gideon 2012, p.\u00a08), nor covers the entire target population as is done in a census (3). The latter is often not possible due to a lack of feasibility and funding (1).\n\nThe sample chosen for the survey should represent the target population so that generalizations and \u201cstatistical inferences about that population\u201d (Bhattacherjee 2012, p.\u00a065) can be made based on the results of the survey (1). To ensure the representativeness of a sample, different strategies can be applied, including simple random sampling, stratified sampling, and cluster sampling, among others (3).\n\nWhen determining the sample, '''a bigger sample size is almost always better'''. A bigger size will reduce skewness of the sample, and it takes a sufficiently large sample to reach a normal distribution within the sample. Of course, you cannot sample endlessly, as organisational and financial restrictions limit the possible amount of survey respondents: you cannot hand out surveys to bypassers on the street for years, and you will never reach everyone in your target population via the Internet. Also, the adequate sample size for a survey depends on many different factors, such as the study objective and the sampling method. In case of simple random sampling, for example, the heterogeneity of the target population plays a role, as does the confidence interval you aim for, and how you want to analyze your data (e.g. analysis by subgroups, type of statistical analysis). Contrary to popular belief, the size of the target population is of little or no relevance unless the target population is relatively small, i.e. only consists of a few thousand people (3). Taking this multitude of factors into account, sample size calculators on the Internet, such as [https://www.surveymonkey.com/mp/sample-size-calculator/ this one], can help to get an idea of which sample size might be suitable for your study. Keep in mind that when determining the sample size, the estimated response rate needs to be accounted for as well, so you most likely need to invite more people for the survey than the calculated sample size indicates (see below; 3).\n\nMoreover, the unit of analysis which the information is collected on needs to be defined to adequately sample, and then be consistently addressed by each question in the survey (8). Here, the decisive factor is not the fact that the person who responds is an individual, but the unit that the responding person represents (8). This unit can be the individual itself as well as its \u201cwork group, project, function, organization or even industry\u201d (Malhotra & Grover, 1998, p. 410). It is crucial that the participants of a survey suit the unit of analysis, i.e. have sufficient insight into the topic and thus unit under study (8). To this end, while survey research works with individuals in the first place, it is a system-level investigation of societal groups, organizations etc.\n\nFor more on sampling in Survey Research, please refer to Bhattacherjee (2012) who dedicates an entire chapter on this topic, as well as to the entry on [[Sampling for Interviews]].\n\n==== Survey Design ====\nSurvey research is commonly used as a quantitative method (8) for studies \u201cthat primarily aim at describing numerical distributions of variables (e.g. prevalence rates) in the population\u201d (Jansen 2010, The Qualitative Survey section, para. 4). This can be done for survey responses that are qualitative in nature (nominal, ordinal) as well as for quantitative responses (see Data formats below). However, survey responses can also be analyzed qualitatively, making it a qualitative method which \u201cdoes not aim at establishing frequencies, means or other parameters but at determining the diversity of some topic of interest within a given population. This type of survey does not count the number of people with the same characteristic (value of variable) but it establishes the meaningful variation (relevant dimensions and values) within that population.\u201d (Jansen 2010, The Qualitative Survey section, paragraph 5). The differentiation of both approaches is most relevant when it comes to the subsequent analysis. Check out the wiki entry on quantitative and qualitative [content analysis](https://sustainabilitymethods.org/index.php/Content_Analysis) to learn more about the analysis of survey data.\n\nIn any case, an important aspect of survey research is the '''highly standardized procedure''' when collecting information, which means \u201cthat every individual is asked the same questions in more or less the same way\u201d (Gideon 2012, p.\u00a08). The design of a survey is moreover highly dependent on its purpose e.g., how fast the data need to be collected, how precise the results should be or if responses must be comparable between different target populations. \n\nWhen designing a survey, different types of questions can be included (3). Note that the questions asked in the survey (questionnaire items) are not the same as the ones you aim to answer with your research (internal questions). The internal questions usually have to be broken down into several questions to encompass the entirety of the research question or reformulated to be comprehensible for the survey participants and to produce answers suitable for analysis (65).\n\n'''Generally, one distinguishes between 'objective' and 'subjective' questions as well as unstructured (also called 'open-ended') and structured (also called 'closed') questions.''' Objective questions refer to topics such as one\u2019s employment, living situation, education level, or health condition. Subjective questions correspond to personal opinions and values (3). \n\nOpen-ended questions require the respondents to formulate their answer freely and \u201cin their own words\u201d (Krosnick 1999, p. 543). This is especially \u201csuitable for exploratory, interviewer-led surveys\u201d (Kasunic 2005, p. 43), as its response format is less restrictive, but comes with its own challenges (see \u201cStrengths & Challenges\u201d).\n<br>\n[[File:Kasunic 2005 p. 67 Open question.png|500px|thumb|center|'''Example of an open-ended, subjective question.''' Source: Kasunic, 2005, p. 67]]\n<br>\nThe more common closed questions provide participants with a set of prescribed answer options. These questions can have ordered, [[Data_formats#Ordinal_data|ordinal]] answer options in form of a \u201cgraduated scale along some single dimension of opinion or behavior\u201d (Kasunic 2005, p. 42). Typically, this is done by using a 5-point [[Likert Scale]]. However, these answer options can also have 3-point answer options (e.g. Yes, Maybe, No) or be dichotomous ([[Data_formats#Binary_data|binary]]), e.g. yes/no, agree/disagree (Bhattacherjee 2012, p. 75; Kasunic 2005, p. 44). When using ordinal scales, an appropriate range, i.e. number of answer options, needs to be chosen so that participants can find an option that suits their perspective (6). \n<br>\n[[File:Kasunic 2005 p. 61 Closed question-ordered responses .png|600px|thumb|center|'''Example of a closed-ended, subjective question with ordinal answer options'''. Source: Kasunic, 2005, p. 86]]\n<br>\nClosed questions can also offer unordered answer options in form of [[Data_formats#Nominal_data|nominal]] data, like distinct categories in a list (6). In this case, the researcher can choose how many answers can be checked by the respondents for each question (6). Further, closed questions can also have a continuous response format, \u201cwhere respondents enter a [[Data_formats#Quantitative_data|continuous (ratio-scaled)]] value with a meaningful zero point, such as their age or tenure in a firm. These responses generally tend to be of the fill-in-the blanks type.\u201d (Bhattacherjee 2012, p. 75). \n<br>\n[[File:Kasunic 2005 p. 66 Closed question-check boxes.png|600px|thumb|center|'''Example of a closed, objective question with unordered response options (nominal)''' Source: Kasunic, 2005, p. 46]]\n<br>\nExamples for different question types and response formats (including Likert-type scales) are offered by Bhattacherjee (2012) on page 75 and by Kasunic (2005) in section 4.2.1 and appendix D. Additionally, the wiki entry on [[Data formats]] can help you understand the different response formats in surveys better.\n\nBesides the response format, further aspects regarding the question design include the number and order of questions as well as corresponding answers, the wording and \u201cthe context in which the question is asked\u201d (Gideon 2012, p.\u00a030). Moreover, the \u201cintroduction, framing, and questionnaire designs\u201d (Gideon 2012, p.\u00a030) need to be thoroughly devised to appeal to the target group and avoid biases. All these decisions can influence the survey results in terms of the \u201cresponse rate, response accuracy, and response quality\u201d (see below; Gideon 2012 p.\u00a030).\n\n==== Pretesting a Survey ====\n'''Before the actual conduction of a survey, it is usually pretested.''' The pretest is designated to identify items in the survey which are biasing, difficult to understand or misconceived by the participants e.g., because of their ambiguity (1; 3; 7). Further, the validity of the survey items can be evaluated through a pretest (3). The people participating in the pretest are ideally drawn from the target population of the study, while time constraints or other limitations sometimes only allow for resorting to a convenience sample (1; 3). After the pretest, necessary adaptations in the survey are made (1).\n\nDifferent pretesting methods exist which \u201cfocus on different aspects of the survey data collection process and differ in terms of the kinds of problems they detect, as well as in the reliability with which they detect these problems\u201d (Krosnick 1999, p.\u00a0542). Cognitive pretesting \u2013 also called \u201c\u2019thinkaloud\u2019 interviews\u201d (Gideon 2012, p.\u00a030) \u2013 is one such method. It \u201cinvolves asking respondents to \u2018think aloud\u2019 while answering questions, verbalizing whatever comes to mind as they formulate responses\u201d (Krosnick 1999, p.\u00a0542). By getting to know the cognitive processes behind making sense of and answering the survey questions, \u201cconfusion and misunderstandings can readily be identified\u201d (Krosnick 1999, p.\u00a0542).\n\n==== Response Rate and Data Quality of a Survey ====\n'''A high response rate is always desirable when conducting a survey.''' A good sampling strategy won't do much if no one answers to the survey. Also, a stratified sampling strategy won't work if there are skewed responses. There are multiple strategies that can be applied to increase the response rate. First, the chosen topic of the survey should be of interest or relevance for the target population under study. Then, \u201cpre-notification letters, multiple contact attempts, multi-mode approaches, the use of incentives, and customized survey introductions\u201d (Gideon 2012, p.\u00a031) are further strategies which can help increase the response rate. In case of interview surveys, a thorough training of the interviewers can be effective in motivating the interviewees to share their answers openly (3).\n\nThe response quality is influenced by the personal interest and intrinsic motivation of the participants, too (3). Unwilling respondents who want to go through the survey as quickly as possible might \u201cengage in satisficing, i.e. choosing the answer that requires least thought\u201d (Gideon, 2012, p.\u00a0140). Further, the attention span and motivation level of the participants might decrease over the course of the survey, which is referred to as respondent fatigue and negatively affects the data quality (108). Indicators for respondent fatigue and a limited response quality in general are an increased number of \u201cdon\u2019t know\u201d-answers, short and superficial responses to open-ended questions, \u201c\u2019straight-line\u2019 responding (i.e. choosing answers down the same column on a page)\u201d and ending the survey prematurely (3; 10).\n\nOf course, a good survey is also measured by its validity (the degree to which it has measures what it purports to measure) and reliability (the degree to which comparable results could be reproduced under similar conditions) (3).\n\n\n== Strengths & Challenges ==\n==== Strengths ====\nSurveys \u2013 especially self-completion questionnaires \u2013 are often associated with economical advantages such as savings in time, effort, and cost when compared to more open [[Interviews|interviews]] (1; 3). Further, (digital) surveys allow to collect information which would otherwise be difficult to observe, e.g. because of the size or remoteness of the target population or because of the type of data, such as subjective beliefs or personal information (1). To this end, survey research allows for larger samples compared to more qualitative open or semi-structured interviews. Furthermore, \u201clarge sample surveys may allow detection of small effects even while analyzing multiple variables, and depending on the survey design, may also allow comparative analysis of population subgroups\u201d (Bhattacherjee 2012, p.\u00a073).\n\nA strength of questionnaire surveys and the reason why they are sometimes preferred by survey participants compared to interview surveys is their \u201cunobtrusive nature and the ability to respond at one\u2019s convenience\u201d (Bhattacherjee, 2012, p.\u00a073).\u00a0Interview surveys, by contrast, make it possible to reach out to difficult-to-reach target populations e.g., homeless people or illegal immigrants (1).\n==== Challenges ====\nAmong the biggest challenges related to survey methods are the multitude of biases that can occur, as those biases can \u201cinvalidate some of the inferences derived\u201d (Bhattacherjee, 2012, p.\u00a080) from the collected data. Those biases include, among others, non-response bias, sampling bias, social desirability bias, recall bias, and common method bias. While the different biases are discussed briefly in the Wiki article [[Bias in Interviews]], you can find a more detailed explanation of those biases as well as strategies on how to overcome them in Bhattacherjee (2012).\n\nFurther challenges in survey research include the avoidance of respondent fatigue and the assurance of the validity and reliability of the gathered data and analyzed results (3; 10).\n\nA number of challenges also come with the increased reliance on the Internet for the implementation of surveys (3), such as safeguarding data protection (4). Participants may also engage in Internet surveys only to receive a promised incentive at the end, which can \u201clead to [\u2026] false answers, answering too fast, giving the same answer repeatedly [\u2026], and getting multiple surveys completed by the same respondent\u201d (Hays et al. 2015, p. 688). Ways to avoid such incidents are e.g., the provision of feedback to the respondents while they are filling out the survey by pointing out that they seem to be rushing or giving the same answer for many questions in a row, or the verification of their e-mail and IP address to \u201cto ensure the identities of individuals that join and to minimize duplicate representation on the panel\u201d (Hays et al. 2015, p. 688). Moreover, before analyzing the survey results, those survey submissions can be excluded which exceed a certain number of missing answers, which were submitted faster than a minimum time span defined, or which contain identical answers for a defined number of consecutive questions (4).\n\nFurthermore, the use of each question type comes with challenges. For example, \u201ca closed-ended question can only be used effectively if its answer choices are comprehensive, and this is difficult to assure\u201d (Krosnick 1999, p. 544). Also, \u201crespondents tend to confine their answers to the choices offered\u201d (Krosnick 1999, p. 544) instead of adding their own response if no fitting answer option is listed. Including open-ended questions in a survey can lead to a time-consuming analysis, as the individual answers need to be sorted and interpreted. Further, the interpretation of the open responses \u2013 especially in self-completed surveys \u2013 and thus the assurance of validity is considered challenging as no follow-up questions can be asked (6). Further, \u201cmeaningful variables for statistical analysis\u201d (Kasunic 2005, p. 40) can hardly be extracted from open-ended answers.\n\n\n== Normativity ==\nDesigning and conducting a survey entails several ethical challenges (3). One main concern is the avoidance of biases. First, the answers of the survey participants must not be influenced to fit the researchers\u2019 hypothesis or a research client\u2019s desired outcome e.g., by formulating or presenting the questions in a certain way. Second, the results need to be generated and presented to \u201caccurately reflect the information provided by respondents\u201d (Gideon 2012, p. 23). In addition, the data and response quality should be critically reflected on during and after data collection (3). Furthermore, data privacy as well as the protection of the survey participants from any other type of abuse needs to be ensured (3). \n\n\n== Outlook ==\nGideon (2012) points out that with regard to quality control of survey research, many sources of error and bias have not received enough attention in the past. The author highlights that (early career) researchers have placed much focus on sampling errors, which are comparatively easy to detect and resolved by adapting the sample size. However, non-sampling errors, such as response and non-response errors and their underlying challenges, have often been insufficiently dealt with, even though their negative impact on the survey quality and results is substantial. Such errors can occur at several stages in the survey research process and are usually characterized by a higher complexity than sampling errors; they cannot be resolved by simply increasing the sample size or by ensuring the representativeness of the sample (3). Therefore, Gideon (2012) calls for the total survey error \u2013 which takes sampling as well as non-sampling errors into account \u2013 to be \u201cthe dominant paradigm for developing, analyzing, and understanding surveys and their results\u201d (Gideon 2012, p. 4) in the future.\n\nAccording to Miller (2017), one main challenge for future survey research consists in sustaining sufficient response rates. As more and more people have declined their participation when invited to a survey during the past years, both the risk of nonresponse bias and the costs allocated for participant recruitment increase (9). How well survey research can adapt to further changes in communication technology (e.g. through the Internet) will play an important role in this regard (9). However, more research on the causes of and possible solutions for the decline in survey participation needs to be conducted (9).\n\nAnother development which is likely to affect the use of survey research and its results in the future is the blending of data, i.e. the combination of different data sources (9). According to Miller (2017), \u201cblending survey data with other forms of information offers promising outcomes and technical challenges\u201d (p. 210) at the same time.\n\nDespite the challenges survey research might face during the decades to come, Miller (2017) concludes that survey research will continue to be relevant by providing important (statistical) information for academia as well as policymaking.\n\n\n== An exemplary study ==\n[[File:Survey Research - Exemplary Study - Cotton et al. 2007 - Title.png|400px|frameless|center|Survey Research - Exemplary Study - Cotton et al. 2007 - Title]]\n'''In their 2007 publication, Cotton et al. (see References) used a digital survey to explore lecturers\u2019 understanding of and attitudes towards sustainable development and their beliefs about incorporating sustainable development into the higher education curriculum.''' The study was conducted at the University of Plymouth, where lecturers from different disciplines were invited to participate.\n\nThe methodical approach consisted of two stages, the first one being an online questionnaire survey and the second one being in-depth semi-structured interviews. '''The questionnaire survey had two main objectives.''' First, it was used \u201cto establish baseline data on support for sustainable development across the university\u2019s faculties and provide lines of enquiry for further research\u201d (p. 584). Second, it should allow for a systematic selection of survey participants who could be interviewed in the second stage of the research by calling for volunteers.\n\nThe questionnaire contained both '''closed and open-ended questions''', where the latter question type was used \u201cto complement quantitative data where it was felt useful to elicit more detailed information\u201d (p. 584). The software SPSS was used for the analysis of the quantitative data, while Pearson\u2019s [[Simple_Statistical_Tests#Chi-square_Test_of_Stochastic_Independence|Chi-Square test]] functioned as the \u201ctest for association between certain variables, with significance accepted where p < 0.05\u201d (p. 584). The analysis of the qualitative data was conducted \u201cusing thematic analysis employing the [https://delvetool.com/blog/ccm constant comparative method] (Silverman, 2005)\u201d (p. 584).\n\nThe questionnaire was repeatedly '''pre-tested and revised''' before it was piloted with researchers and lecturers from different fields. For the pilot test, two versions of the questionnaire were chosen which differed with regard to their terminology. While one version referred to \u201csustainability\u201d, the other version used the term \u201csustainable development\u201d. This was done to find out which of the two terms was understood better by the potential respondents and resulted in the use of \u201csustainable development\u201d for the final questionnaire version. However, as lecturers from different disciplines and with a different background knowledge on the topic were targeted with the survey, unfamiliarity with different terminologies was still difficult to avoid.\n\nThe '''recruitment of participants''' for the online questionnaire was done via e-mail to the academic staff of the university, which included a link to the online survey that was accessible for a month. With 328 responses received, the response rate consisted of 29 percent which the authors consider comparable with similar online surveys. Moreover, the respondents were found to be \u201cbroadly representative of university lecturers as a whole in terms of gender, age and contract status (full/part time)\u201d (p. 585). However, the authors do not discount \u201cthe probability that those who responded had a better understanding of, or perhaps stronger opinions on, sustainable development than non-respondents\u201d (p. 585), which is of particular relevance with regard to the interpretation of results.\n\nThe authors present their '''findings''' in three main sections: (1) Understandings of sustainable development, (2) Attitudes towards sustainable development, and (3) Beliefs about incorporating sustainable development into the higher education curriculum.\n\nThe following table (p. 586) shows the results for the '''lecturers\u2019 interpretation of the concept of sustainable development''', i.e. the percentage of respondents who \u201cagree\u201d and \u201cstrongly agree\u201d to the listed propositions. For this closed-ended question, a five-point [[Likert Scale|Likert scale]] was used. The participants further had the possibility to give an open-ended response to this question. While many viewed the somewhat oversimplified tickbox definitions as problematic and elaborated on their opinions further, \u201cothers admitted that they struggled to make sense of the options offered\u201d (p. 587). This shows the varying levels of knowledge in the group and moreover indicates \u00a0\u201cthe immense difficulties of designing a questionnaire to examine views on a multifaceted and contested concept like sustainable development\u201d (p. 587).\n<br>\n[[File:Survey Research Exemplary Study Cotton et al. 2007 Results-Understanding SD p.586.png|600px|frameless|center|Survey Research Exemplary Study Cotton et al. 2007 Results-Understanding SD p.586]]\n<br>\nThe next diagram (p. 588) presents the responses to the question on the '''lecturers\u2019 attitudes towards sustainable development'''. While the quantitative data shows a \u201cfairly strong agreement amongst respondents in support of sustainable development\u201d (p. 588), the authors acknowledge the challenges when it comes to the interpretation of \u201csuch a general statement\u201d (p. 588): \u201c[W]ere respondents commenting favourably on the idea of sustainable development per se, or was this simply a convenient proxy for a range of environmental, social and economic concerns?\u201d (p. 588).\n<br>\n[[File:Survey Research Exemplary Study Cotton et al. 2007 Results-Attitudes SD p.588.png|500px|frameless|center|Survey Research Exemplary Study Cotton et al. 2007 Results-Attitudes SD p.588]]\n<br>\nRegarding the lecturers\u2019 beliefs about incorporating sustainable development into the higher education curriculum, \u201c[o]ver 50% of respondents predicted including elements of sustainable development in their teaching in the coming year\u201d (p. 589). The authors point out that this number can be the result of overreporting, as the questionnaire is self-reported and does not observe actual behaviors and actions. In addition, the authors clarify that \u201cwhat constitutes ESD remains a matter of personal interpretation, and may include changes to either content or process (approaches to teaching the subject)\u201d (p. 589), which further complicates the interpretation of the quantitative data.\n\nFor more on this study and the study\u2019s findings, please refer to the References.\n\n\n== Key Publications ==\nBhattacherjee, A. 2012. ''Social Science Research: Principles, Methods, and Practices''. CreateSpace Independent Publishing Platform.\n* The chapter on survey research includes an own section on questionnaire surveys and interview surveys each and gives detailed insights into different types of biases in survey research.\n\nGideon, L. 2012. ''Handbook of survey methodology for the social sciences''. Springer.\n* This handbook provides detailed insights on how to do survey research and gives advice along the entire research process.\n\nKasunic, M. 2005. ''Designing an Effective Survey''. Available at https://apps.dtic.mil/sti/pdfs/ADA441817.pdf.\n* This publication offers hands-on advice and a step-by-step instructions on how to do survey research with questionnaires.\n\n\n== References ==\n(1) Bhattacherjee, A. 2012. ''Social Science Research: Principles, Methods, and Practices''. [https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4a705b29-6e7a-4380-b86e-43cfcf6d6195/Social_Science_Research__Principles_Methods_and_Practices.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210621%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210621T065623Z&X-Amz-Expires=86400&X-Amz-Signature=0eb2adf091c549d3d5dcb9e216bbbe7b79a1f84f985e5c690250381c8a11d44b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Social%2520Science%2520Research_%2520Principles%2520Methods%2520and%2520Practices.pdf%22 CreateSpace Independent Publishing Platform.] \n\n(2) Brenner, P. S. (Ed.). 2020. ''Frontiers in sociology and social research: Volume 4. Understanding survey methodology: Sociological theory and applications''. Springer.\n\n(3) Gideon, L. 2012. ''Handbook of survey methodology for the social sciences.'' Springer.\n\n(4) Hays, R. D., Liu, H., & Kapteyn, A. 2015. ''Use of Internet panels to conduct surveys.'' Behavior Research Methods 47(3), 685\u2013690.\n\n(5) Jansen, H. 2010. ''The Logic of Qualitative Survey Research and its Position in the Field of Social Research Methods.'' Forum Qualitative Sozialforschung / Forum: Qualitative Social Research 11 (2). Visualising Migration and Social Division: Insights From Social Sciences and the Visual Arts.\n\n(6) Kasunic, M. 2005. ''Designing an Effective Survey.'' Available [https://apps.dtic.mil/sti/pdfs/ADA441817.pdf here.]\n\n(7) Krosnick, J. A. 1999. ''Survey Research.'' Annual Review of Psychology 50, 537\u2013568. \n\n(8) Malhotra, M. K., & Grover, V. 1998. ''An assessment of survey research in POM: from constructs to theory.'' Journal of Operations Management 16(4), 407\u2013425. \n\n(9) Miller, P. V. 2017. ''Is There a Future for Surveys?'' Public Opinion Quarterly 81(S1), 205\u2013212.\n\n(10) P. J. Lavrakas (Ed.) 2008. ''Respondent fatigue''. Encyclopedia of survey research methods. SAGE.\n\n(11) Cotton, D.R.E. Warren, M.F. Maiboroda, O. Bailey, I. 2007. ''Sustainable development, higher education and pedagogy: a study of lecturers' beliefs and attitudes.'' Environmental Education Research 13(5), 579-597.\n----\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Fine B\u00f6ttner."
                    },
                    "sha1": "n3b6n319yhpd2hb627dr8e5z30djlcq"
                }
            },
            {
                "title": "System Boundaries",
                "ns": "0",
                "id": "515",
                "revision": {
                    "id": "6367",
                    "parentid": "5923",
                    "timestamp": "2021-09-10T12:07:15Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16152",
                        "#text": "'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries. \n\n== The problem with System Boundaries ==\nWhile boundaries of some isolated systems as well as simple theoretical models can be rather clearly defined, much attention of the recent literature has actually focussed on the effects that systemic interactions create across distances, which have historically been considered separate (see for example [['https://www.ncdc.noaa.gov/teleconnections/'|'teleconnections']] in atmospheric research). It is now evident that inland watersheds are linked to oceangraphic processes through freshwater and nutrient inputs, although these have historically and disciplinarily been examined separately. Furthermore, the nestedness of systems is another challenge - the idea that smaller systems are contained within, and are the constituent components of, larger systems such as organs within the larger integrated functionality of the human body. Some organs can be removed or replaced, but only in relation to the overall functions they provide and how they fit or match the context of the overall body (i.e, blood type or immune response). Government jurisdictions give other clear examples, where countries may contain several provinces as part of a federal system, which in turn can contain smaller administrative units, creating a multi-level administrative whole. \n\nThe debate often resonates around a void between the global and the local levels of social and ecological [[Glossary|scales]] such as space, jurisdiction, administration and even time (e.g., slow to fast). While the global level of most scales is trivial to define, the local level of a scale is less clear. What becomes clear in the definition process between global and local, is the ambiguity and often arbitrary nature of defining what the reality of the system being examined is. Normative choices have to be made, especially for defining research questions and hypotheses, i.e., what is a part of the specific system, what is not considered to be a part of it, and what is an important element that you want to know about? This has implications for the relationships and outcomes you want to investigate. '''Standardization of system definitions, or system defining [[Glossary|processes]], would also be a useful consensus activity to increase comparability and exchange within system science.''' Entire fields of research have thus emerged around normative understandings of what the optimal system level unit to examine might be, such as rural sociology, landscape ecology, coastal governance or micro-economics. What is evident - and becomes unclear - is the spectrum of definitions and choices between the two ends of a scale, and the degree to which the categorical differences in system boundary definitions are meaningful for how we analyse and interpret their functionality.\n\n[[File:System Boundaries - Farm.png|600px|thumb|center|'''Defining the system boundaries influences the scope and content of analysis'''. Source: [https://www.researchgate.net/publication/323959122_D11_Report_on_resilience_framework_for_EU_agriculture Meuwissen et al. 2018.]]]\n\n== Defining System Boundaries ==\nOut of these challenges to define system boundaries we recognize a clear gap on how to approach a globally coherent definition, which can recognize the wide array of contextual differences in how systems are defined and measured. We need to consider that some systems are either divided from larger or parallel systems, or that smaller systems are nested into other systems. \n\n'''System boundaries can be defined based on a variety of parameters.''' For the simplicity of the argument, we focus on one parameter out of several, yet prefer not to discuss in depth the interaction of different parameters. Many people would define such interactions as [[Agency, Complexity and Emergence|complexity]] within systems thinking, but the examination of system complexity is premised on a coherent understanding of what is in and out of the system being examined. This is an inherent precondition for analysis, which is often discussed or taken as an assumption that is often not clearly defined. For example, spatially explicit parameters are an intuitive aspect of many defining processes, and therefore shape our perceptions of what an appropriate system boundary might be. To use a spatial example: '''a larger city can be divided into smaller neighbourhoods, while neighbourhoods can be defined based on different parameters, such as function, ethnic groups or cultural settings.''' While the definition of 'neighbourhoods' can be also informal, it is also well established within urban planning. There is an evident awareness that system boundaries exist, yet many would not be able to define why two systems differ. It is overall easier to define system boundaries based on spatial parameters, such as structural elements, ecosystems, or the design of a city, while informal parameters such as those based on normative dimensions are less clearly definable. Being initially clear within a research project about the boundaries of a system may help to clarify a ranking of what relevant parameters of the system are in terms of its boundaries, but also considering the variance of these parameters within a system. Basically, some parameters can be dramatically different within a system than outside, have no relevance within a system, or can have only relevance within the system. All [[Glossary|assumption]]s are plausible parameters to serve as system boundaries.\n\n'''Independent of the given parameter, we propose that both the state and the variance can be clear delimiters in defining boundaries between systems.''' When looking at a larger system, it can have an overall average value as well as an overall variance regarding a system parameter. If we would now divide the larger system into two smaller parts, it might be that the two systems have a different average value regarding this parameter. However, two smaller systems could also be different regarding their variance, where one smaller system has an overall large heterogeneity, and the other one a small heterogeneity.\n\nA good example of this could be two neighbourhoods within one city. One could be very heterogeneous in terms of green infrastructure, having many smaller parks, trees, and green rooftops. The other one could be highly homogenous, being widely dominated by houses without any green infrastructure. Clearly, both systems vary both in terms of the average value, as well as the variance. Thus, where boundaries are drawn to define those neighbourhoods in that city will dictate the analysis and conclusions about its variance and values.\n\nAnother example would be two smaller systems, one again being highly heterogeneous, and the other consisting of one half that is a park, and another half that are large apartment houses. Many would opt to divide the second smaller system into two sub-systems. This example illustrates how different parameters allow for a different division of systems according to the habitat recognition. However, the park could be functionally related directly to the apartments, constituting its own unit in an urban planning context. This illustrates how system boundaries depend on the parameters we observe, but also the reasons why we want to measure them and what we want to know and value about the system.\n\nMore importantly, system boundaries can also differ regarding the average value of a parameter but can show different [[Glossary|patterns]] when looking at the variance. This difference is often associated with the grain size of parameters. Grain size can be defined as the resolution at which a parameter is observed. A parameter can be for instance very homogenous at a large scale, yet very heterogenous at a small scale. The question of grain size is often associated with nestedness, yet grain size and nestedness are only indirectly connected, but not always necessarily linked. \n\nFor example, the functions of neighbourhood can be very homogenous at a coarse grain, yet heterogeneity is revealed at a more localized scale. This can be used to define the smallest local scale in a system, that is the scale at which the average as well as the variance of a parameter do not change any more. The smallest definable unit could hence be defined as the smallest local unit of a system. This is especially relevant since different disciplinary perspectives and approaches are often determined by method, and therefore often also by grain. While for instance a method might not find any relevant patterns based on one differentiation of sub-systems, another division might lead to meaningful patterns. This highlights why a clear determination of systems and their boundaries is so relevant.\n\n== System Boundaries and Normativity ==\nMuch of the currently published analysis in scientific publications is determined by system descriptions yet enabling transformation of systems demands a clear definition of the unit of analysis, i.e. a system, where common boundaries of those systems are not collectively agreed upon by all who are measuring and drawing conclusions about them. If a system being investigated is too large, relevant patterns of a parameter might be lost against its constituent internally contained parts. If a system is bound to small, again relevant patterns might be undetectable due to over-magnification.\n \nThis opens the door to criticism that could be raised regarding this line of thinking: '''Should the system boundaries be determined by the overall results that can be gained?''' From an [[:Category:Inductive|inductive]] perspective, the answer could be different compared to a [[:Category:Deductive|deductive]] perspective. Hence this question might be unsolvable for now due to the different epistemic preferences for trying to understand what is real through the scientific process. However, what can be said here is that Occam\u2019s razor could apply. '''Every system should be as simple as possible, and as complex as necessary.''' This can be directly applied to the question of system boundaries, since these would clearly enable an overly simplistic or an overly complex analysis. While it is clear that system analysis cannot opt for trivial outcomes that are hard to understand, this highlights that there might be an optimal level and scale of analysis, which is probably closely tied to the relevance or ranking of the different parameters.\n  \nThis is an important basis in order to enable the transformation of systems. The normative dimensions or parameters in systems are challenging to analyse and even more difficult to transform, yet building on two examples we show how the average as well as the variance of systems can allow for a clear analysis of how cultural difference can define system boundaries over time. For example, inheritance laws during the middle ages in Germany differed by religion. In Catholic regions, the eldest son inherited farms in rural areas; while in protestant areas, farms were divided into subsequent smaller units among the family. This had a dramatic impact on land use patterns as well as social dimensions within the given systems over time. Reflected in architecture and the landscape, these differences are often visible today, and can testify to how cultural variation can define systems functionality and boundaries.\n\n[[File:Gerrymandering.png|400px|thumb|left|'''Gerrymandering'''. Source: [https://en.wikipedia.org/wiki/Gerrymandering#/media/File:DifferingApportionment.svg M.boli - Wikipedia] (under CC BY-SA 4.0)]]\n\n'''Another example is the process of political gerrymandering.''' This process depicts the manipulation of administrative boundaries for voting in the United States, with the goal to favour one political party over the other by drawing the lines around those neighbourhoods who will vote for your political party to be in the district, and those who won\u2019t as out of it. This is a prominent example of how through the altering of system boundaries the normative characteristics of a system are altered. This is done with the rationale to diminish the variance in terms of some factors, for instance with the goal to represent a minority. De facto, is this rational often misused to influence the average, or in other words to construct majorities. This highlights how system boundaries are already actively recognized, even on such a deep normative dimension such as the political value system. \n\nHence system borders can extend beyond biophysical spatial system entities and can also include the difference in mindsets and cultural settings. The most important question when defining system boundaries then becomes, which variables determine or influence the system dynamics the most. When trying to define system boundaries, we are hence looking at a multiverse of multidimensional parameters. We of course acknowledge the interactions between these parameters - which is often coined as complexity - but let us [[Big problems for later|not discuss this further]] at this point. \n\nInstead we conclude by shortly discussing the importance of a ranking or the development of hierarchies of different system parameters to guide systems analysis. '''Some parameters might be more important than others regarding the dynamics within one system. On the other hand, some parameters may be more important in one system than in others.''' Based on these two assumptions, any definition of system boundaries, which can be defined as an early step in systems analysis and transformation, can be approaches as an iterative approach. Ideally, when defining system boundaries, we would propose a parsimonious approach, which we define as being as simple as possible and as complex as necessary.\n\nIn an example where three systems neighbour each other, two systems (S1 and S2) could share a larger number of parameters that are important for the dynamics of these two systems. If then one of these two (S1) shared a strong overlap with a third system (S3), and these were strongly explained by only one parameter, we should prioritize the declaration of S1 and S3 as more similar. Consequently, S1 and S3 would be declared as sharing a less strong boundary than systems S1 and S2. However, if system S2 and S3 share no parameters that explain their system dynamics, then the boundary between S2 and S3 is the strongest. These types of boundaries, can be spatially, but quite often they can also manifest in forms of social values, economic barriers and in other phenomena.\n\nThis example underlines the importance of defining system boundaries by similarities and dissimilarities, which can be defined both by the average value of parameters, as well as by the variance of parameters. The additional layer of the social value we assign to those parameters then provides a means by which we can make analytical choices based on how we view order such parameters into value hierarchies.\nWe propose that instead of relying on informal or non-reproducible definitions of system boundaries, '''it is vital to pre-analyse system dynamics to define system boundaries, which are a pre-condition for a proper analysis of system dynamics.''' Otherwise a system analysis will lack the preconditions necessary that enable its meaning and implications to be understood ad compared with other systems.\n\n----\n[[Category: Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "0n1tm6ds6x8wttpk5qpjrxzpt8fi50n"
                }
            },
            {
                "title": "System Thinking & Causal Loop Diagrams",
                "ns": "0",
                "id": "410",
                "revision": {
                    "id": "6865",
                    "parentid": "6320",
                    "timestamp": "2022-12-08T11:05:08Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* What the method does */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "33777",
                        "#text": "[[File:Causal Loop Diagram Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[System Thinking & Causal Loop Diagrams]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/>\n\n'''Annotation:''' The method of Causal Loop Diagrams is strongly connected to Systems Thinking. The latter describes a mode of thinking - a way of research - and will be described in this entry, too.\n\n'''In short:''' Causal Loop Diagrams allow to visualize system dynamics, i.e. interactions between multiple variables, and to qualify or quantify these interactions.\n\n== Background ==\n[[File:Systems thinking.png|thumb|right|400px|'''SCOPUS hits per year for System Thinking & Causal Loop Diagram until 2019.''' Search terms: 'system thinking', 'systems thinking', 'causal loop diagram' in Title, Abstract, Keywords. Source: own.]]\n\nIn the 1920s, '''Russian researcher Alexander Bogdanov''' formed the first comprehensive theoretical [[Glossary|framework]] revolving around a system, describing the organisation of living and non-living systems (1, p.5). Then, Austrian biologist \"'''Ludvick von Bertalanaffy''' independently followed up the work of Bogdanov and initiated the general system theories in the 1940s from which the modern cybernetic movement emerged.\" (Haraldsson 2004, p.5, emphasis added). The 'cybernetic movement', \"(...) formed after World War II was a group of mathematicians, neuroscientists, social scientists and engineers, led by '''Norbert Wiener and John von Neumann'''. They developed important concepts of feedback and self-regulation within engineering and expanded the concept of studying patterns, which eventually led to theories of self-organisation (...). It enabled a contextual analysis of seeing different system levels interacting in a larger whole, e.g. the interaction of species in an ecosystem or the urban society and people.\" (Haraldsson 2004, p.6)\n\n'''Jay Forrester''', a MIT computer scientist, subsequently coined the term 'System Dynamics' in the 1960s and was the first to discuss the Causal Loop Diagrams concept (1). The expression 'Systems Thinking' was introduced by Forrester's student '''Barry Richmond''' in 1987 (6). Considerable contributions to the field were voiced by management researcher '''Peter Checkland''' from the 1970s on, who built upon the General System Theories (GST) but then introduced the distinction between 'hard' und 'soft' system thinking (see below) (9). \"In the sixties and seventies the environmental movement identified many complex problems that integrated the economy and resource management, as a more holistic perspective emerged. The concept \"sustainable development\" is perhaps the most recognised term to address the need of an interdisciplinary approach. In recent years, the importance of creating an educational basis that incorporates a system approach is increasingly realised, and for the last decade or so System Thinking has been finding its way into universities and corporations.\" (Haraldsson 2004, p.6; 6)\n\nSystem Thinking can be applied in any discipline to understand underlying structures and develop models of the system that is analysed. However, sustainability science relies heavily on System Thinking since it acknowledges the complex and lagged interrelationships within and in between ecological, economic, social and further systems. A Key Publication in this field was 'Thinking in Systems' by '''Donella Meadows''', who had been co-authoring the landmark 'Limits to Growth' before. Sustainability Science, which attempts to bridge the world how it is with the world how it ought to be, relies on System Thinking to understand how the world is and in which '[[Glossary|leverage points]]' one needs to intervene to bring about change. System Thinking can also be applied outside of scientific research, e.g. to analyze company-internal processes, for marketing purposes etc. (see e.g. 3)\n\n\n== What the method does ==\nBefore explaining System Thinking, it should first be explained what is a 'system'.\n\n==== Systems, System Thinking, System Analysis & System Dynamics ====\nA system is a \"(...) network of multiple variables that are connected to each other through causal relationships\", based on which the network \"(...) expresses some sort of behaviour, which can only be characterized through observation as a whole\" (Haraldsson 2004, p.11). This behavior remains persistent in a variety of circumstances. More simply put, a system is a \"(...) collection of connected things (...) that influence one another.\" (Toole 2005, p.2) Both the collection of elements and their interactions are important elements of the system which is emphasized by saying that a system is 'more than a collection of its parts' (Arnold & Wade 2015, p.2).\n\nEvery system is \"(...) defined by its boundaries\" (Haraldsson 2004, p.13). The borders we draw for our system analysis influence which level of detail we apply to our view on the system, and which elements we investigate. System elements can be animate (animals, humans) or inanimate (rocks, rain), conceptual (motivation) or real (harvest), quantifiable (money) or rather qualitative (well-being) (2). For example, a system could be a tree, with the leaves, the stem and such elements interacting with each other, but also the forest in which our tree interacts with the soil, the weather, other plants, animals and inanimate objects. The system could also be the globe, where this forest interacts with other ecosystems, or the system in which Planet Earth interacts with the rest of the universe - our solar system. For more background on the definition of System Boundaries, please refer to [[System Boundaries|this entry.]]\n\n'''The system is at the basis of System Thinking.''' System Thinking is a form of scientific approach to organizing and understanding 'systems' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of 'holistic' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that \"(...) all system dynamics are in principle non-linear\" and that \"(...) only non-linear equations are capable of describing systems that follow non-equililbrium conditions\" (Haraldsson 2004, p.6).\n\n'''Peter Checkland introduced the notion that there are two main types of System Thinking:''' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a 'system' in this sense.\n\nSoft System Thinking (SST), by comparison, considers a 'system' an \"(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world\" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a 'system'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|'''Hard System Thinking and Soft System Thinking''' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: ''System Analysis'' and ''System Dynamics'' (1).\n\n''System Analysis'' \"(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships.\" (Haraldsson 2004, p.5). ''System Analysis'', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].\n\n''System Dynamics'', then, focuses on the interaction part of the system. It \"(...) refers to the re-creation of the understanding of a system and its feedbacks. It aims at exploring dynamic responses to changes within or from outside the system. (...) System Dynamics deals with mathematical representation of our mental models and is a secondary step after we have developed our mental model.\" (Haraldsson 2004, p.5). System Dynamics, as the name suggests, enables the researcher to observe and measure the behavior of the system. The interactions between the individual elements are not just recreated, but the consequences of these interactions are quantified and assessed.\n\nSystem Thinking allows for a shift in the perception of [[Causality|causality]]. Instead of assuming linear causality (A causes B, B causes C) it allows for the integration of further influencing factors, as well as a more neutral depiction of the system at hand. C may now be seen as a property that [[Agency, Complexity and Emergence|emerges]] from the relation between A and B, instead of perceiving it as a direct consequence of B. Haraldsson (2004, p.21) provides an illustrative example here: \"We start by asking the initial question: \"I want to understand how water flows into the glass and what I do to fill it up.\" Instead of looking at the action from an individual point of view, where the \"I am\" is the active part and at the centre of focus, we shift our perception to the structure of the action. The \"I am\" simply becomes a part of the feedback process, not standing apart from it. Suddenly we have shifted out attention to the structure of the behavior and we can observe that the structure is causing the behavior. (...) We have now transformed the traditional linear thinking into a circular argument.\"\n\n==== Causal Loop Diagrams ====\nCausal Loop Diagrams (CLDs) are a crucial method for System Analysis since they allow for the modelling of systems. They make it possible \"(...) to 'map' the complexity of a problem of interest\" (McGlashan et al. 2016, p.2). A CLD allows us to not suppose a linear relationship between system elements, but rather understand cause and effect (1). It makes it possible to \"(...) understand how a behavior has been manifesting itself in a system so we can develop strategies to work with, or counteract the behavior. We also want to know to what extent and how the problem is connected with other 'systems'\" (Haraldsson 2004, p.20). By developing a CLD, one can visualize and thus better understand the feedback mechanisms that happen in a system (1). Therefore, they enable both a look at the structure as well as the processes in a system. The CLD also helps transfer system understanding (1). CLDs \"(...) provide a language for articulating our understanding of the dynamic, interconnected nature of our world. (...) By stringing together several loops, we can create a coherent story about a particular problem or issue.\" (Team TIP 2011, p.1)\n\n'''A Causal Loop Diagram consists of rather few elements:''' Variables, Causal Relationships and Polarity.\n\n'''Variables''' represent the individual elements of a system, but are adapted to the research intent underlying the creation of the CLD. They are \"(...) dynamic causes or effects of the problem under study\" (McGlashan et al. 2016, p.2). For example, in a CLD that recreates a forest ecosystem, the system element may be the soil, whereas the variable that we use in the CLD would be, for example, the change in the amount of carbon in the soil - if this is of interest for the research.\n\nBetween these variables, the CLD depicts '''causal relationships'''. Causal relationships \"(...) are represented by arrows that represent a directed cause from one variable to another\" (McGlashan et al. 2016, p.2). When a number of causal relationships (i.e. arrows) connects two or more variables to a circular structure, a loop constitutes.\n\nEach of these causal relationships has one of two polarities (or 'feedbacks'): positive or negative. Note that these are not equal to growth or decline! If the polarity is positive (exhibited by a '+'), the causally related variables change in the same direction: if the first variable grows in size, the other does, too; and if the first variable decreases, so does the second one. Negative polarity ('-') works into opposite directions: if the first variable decreases, the second one increases - and if the first one increases, the second one decreases (1, 3). The polarities influence the outcome of the systemic processes. \n\nThis is especially true for ''loops'', which arise when an output of a system is circled back and used as one or more inputs, through direct or indirect causal links. '''When there is an even number of negative polarities in a loop, the loop is called 'amplifying' or 'reinforcing', indicated by an (R).''' This includes the case where there are only positive polarities, since this is an even amount of negative polarities, too (zero, to be precise). In a reinforcing loop, some variables may be ever-increasing and others ever-decreasing (this is the case when there are some negative polarities), or all variables are ever-increasing or decreasing (only positive polarities). \n\n'''When there is an odd number of negative polarities in a loop, it is a 'balancing' loop, indicated by a (B).''' You can check whether a loop is reinforcing or balancing by choosing one variable, assuming that it is increasing (or decreasing) and going through the loop. If the behavior of this element leads to the same behavior, again (e.g., an increase in A leads to an increase in A), the loop is reinforcing. If the behavior switches (e.g. an increase in A leads to a decrase in A), it is balancing.\n\n[[File:Causal Loop Example 1.png|200px|frameless|left|This is a '''reinforcing loop''' where each variable decreases (or increases) the other one. The (R) in the middle highlights this reinforcing process. Source: Made with Loopy.]]\n<br/><br/>\n'''Let's look at an example of a reinforcing loop''' with positive polarities only: A student's motivation to learn ahead of an exam is causally  - and positively ('+') - linked to the amount of learning they get done. If they are not motivated, they will not get much done. And the less they get done, the less motivated they are... and so on. On the other hand, if they are very motivated, they will get a lot done, which motivates them to do even more. In an ideal world, that is. We all know that this is not always the case, don't worry.\n<br/><br/>\n\n[[File:Causal Loop Example 2.png|200px|frameless|left|This is a '''reinforcing loop''' where all variables influence each other into '''opposite directions.''' The overall result for each variable is still escalation. Source: Made with Loopy.]]\n<br/><br/><br/><br/><br/><br/><br/>\nIf we substitute the purely positive polarities with purely negative, we have a reinforcing loop (with an even number of negative links!) where one variable skyrockets, and the other one diminishes. Say, we substitute 'Motivation' with 'Stress Level': an increase in the stress level decreases the amount of studying, which further increases the stress level, which further decreases the studying. While the stress level escalates into therapeutically dangerous heights, the amount of studying approximates zero. On the other hand, a low stress level allows for a lot of studying, which provides a good feeling and further decreases the stress level.\n<br/><br/><br/>\n\n[[File:Causal Loop Example 3.png|250px|frameless|left|This is a '''balancing loop''' where each variable decreases (or increases) the other one. The (B) in the middle highlights this balancing process. Source: Made with Loopy]]\n<br/><br/><br/>\n'''Then, there is the self-regulating or 'balancing' (B) loop where the odd number of negative polarities hinders the loop from escalating.''' For example, our exemplary student takes coffee breaks now and then, because - let's admit it - they are a coffee junkie. The less they are motivated, the more coffee they drink (positive link), which motivates them to study more (positive link). Studying more increases their motivation (positive link). We have now gone full circle, and the initial decrease in motivation ultimately led to an increase in motivation through the cycle. Now, if we do another round, their newly increased motivation decreases their coffee consumption; without coffee, they won't get anything done - and her motivation decreases again. That's a bummer, but it shows how the balancing loop works both ways.\n\n<br/><br/><br/>\n[[File:CLD 4.png|250px|frameless|left|'''A reinforcing causal loop.''' Source: Made with Loopy]]\nLastly, let us have a look at another re-inforcing loop (R). Here, one more variable in the loop led to an even number of negative links - there are now two negative and two positive polarities. If we follow the loop, we see that a decrease in the motivation leads to an increase of coffee consumption, which leads to an increase of coffee shopping, which leads to a decrease in studying, which leads to a decrease in motivation. Less motivation led to less motivation. The longer we pursue this loop, the more the motivation will decrease, as will the amount of studying. Meanwhile, the student hoards and drinks coffee like a maniac. Of course, this could go the other way around if the motivation increased in the beginning.\n\n<br/><br/><br/><br/>\n=== Causal Loop Diagrams: Step by Step ===\n'''So how does one create a Causal Loop Diagram?''' The process consists of the following steps (from 1, 2, 3):\n\n1) Define the [[System Boundaries|system boundaries]], the time horizon and the question to be answered with the Causal Loop Diagram. The time horizon and system boundaries influence the design of the CLD because some elements may not play a role in short or long time horizons, or in a more narrow or wider perspective on the system.\n\n2) Place the variables that are relevant to the system, starting with just a few very important ones (based on knowledge about the system, e.g. through an initial system analysis). The level of detail (e.g. if your variable is the forest, the individual tree, or its leaves) depends on the research interest and the defined system boundaries. An overly detailed CLD will unnecessarily complicate the latter analysis of the system.\n\n3) Determine the causalities between these variables - which of them are linked with each other? - and draw the respective arrows.\n\n4) Attach the polarities (feedbacks) - either positive or negative - to each linkage.\n\n5) Identify loops in your system and evaluate how each develops over time - does it escalate or regulate itself? Mark them as either balancing (B) or reinforcing (R), accordingly. Note that a loop might include a larger number of variables. Remember: if it is not circular, it is not a loop!\n\n6) Re-iterate. The CLD will not be perfect from the start, so you may change and add variables, decrease or increase the system boundaries and time horizon. However, you should not change the polarities if their original placement was based on sound and data-based knowledge of the interactions.\n\n7) A few additions may be made to this process. \n* There may be further linkages between variables of the loop and external variables that are not immediately part of the loop but still influence the loop's behavior. A generally reinforcing loop can still have balancing behavior if the external feedback prevents escalation (1).\n* There may be delays in the feedback. These delays can be signified by drawing to lines across the respective arrow (1)\n\n[[File:Causal Loop Delays.png|600px|thumb|center|'''An example of a delayed feedback loop.''' Source: Haraldsson 2004, p.30]]\n\n* Sometimes, one of the loops in a CLD may be more important for the dynamic of the whole system. This loop (or also a single feedback) may be drawn bolder to highlight its weight (1). This highlights how a subsequent quantitative analysis of the loop's behavior, e.g. through appropriate software, can shed light on more details of the system's behavior.\n* The loops may also follow a sequence, i.e. one or more of the loops is predominant until a specific event happens (e.g. when a certain threshold is passed). Afterwards, another loop is more relevant. This can get complicated - if you want to learn more about these cases, see Haraldsson 2004 in the Key Publications.\n* Causal-Loop Diagrams may depict systemic interactions over [[Time|time]], as well as across [[:Category:Global|spatial scales]]. For example, a local system may be influenced by global developments, and this interaction can be included and visually highlighted in a CLD.\n\n== Strengths & Challenges ==\nSystem Thinking provides advantages over linear analyses that do not do complex problems justice. \n* System Thinking helps find the real roots of problems instead of applying 'end of pipe' solutions that only fix symptoms, not causes. It also helps not to disimprove things by fixing one element but worsening another one while doing so (1).\n* Human thinking often tends to fall into simple cause-and-effect patterns that make it easier to understand the world. Applying System Thinking and the creation of Causal Loop Diagrams may be seen as solutions to this tendency of the human mind to simplify complex relationships where several variables influence each other. System Thinking makes it easier to get a feeling for how things really interact, as opposed to a very simplified model that neglects too many interactions. However, one should keep in mind that any model is just a (more or less detailed) approximation of the real world, and sometimes, problems are not as complex as they seem.\n\nHowever, there are some potential pitfalls when developing Causal Loop Diagrams.\n* The variables in the CLD must be quantitatively increasable or decreasable states of elements, not the elements themselves. For example, a variable should not be \"attitude towards something\", which cannot be increased or decreased but only described. Instead, the CLD should work with \"tolerance towards something\". The variables should also be self-explanatory and neutral, because \"the action is in the arrows\" (Haroldsson 2004, p.42) (1, 2).\n* The communicative purpose of a CLD can only be seen as accomplished when the reader / viewer understands the thinking behind the model and the principles that guide the system (1). This is especially relevant since Causal Loop Diagrams can become huge, which makes it increasingly difficult to find root causes for developments and not lose the overview (see Figure above).\n\n\n== Normativity ==\n==== Everything normative about this method ====\nChu (2011) raises concerns about the notion of 'systems'. In her view, common definitions of systems (especially those which Checkland would consider 'hard' systems) imply that the 'real' world existed of systems and non-systems, and that the complexity of each system model (which CLDs are a form of) was dependent on how the system 'is'. Instead, she suggests to realise that the complexity of any system model depends on how complex the researcher decides to develop the model: \"How much detail one takes into account in models depends entirely on the perspective one takes that is on what needs to be taken into account.\" (Chu 2011, p.184). Mitchell (2004) claims equally: \"The idealized and partial character of our representations suggests that there will never be a single account that can do all the work of describing and explaining complex phenomena. Different degrees of abstraction, attention to different components of a system, are appropriate to our varying pragmatic goals and conceptual and computational abilities.\" (p.1) \n\nDeveloping a CLD does include the steps of [[System Boundaries|setting system boundaries]] and defining the research interest and thus the necessary elements to be included. In this regard, these concerns highlight that a CLD (and any form of system modelling) is always dependent on the previously made decisions and simplifications and any results of the system analysis must acknowledge this. Chu therefore dismisses the idea of quantifying the complexity of systems based on the properties of system models; and Mitchell suggests not to attempt to break down the world into singular reductionist models that do not represent real complexity, but rather to endorse 'integrative pluralism' that combines different models of the same phenomenon and thus allows for more realistic representations. This critique is in line with Checkland's 'soft' system understanding that offers an alternative to the assumption that systems are objectively real and independent from the observer's relation to it (9).\n\n==== Connectedness ====\n* While Causal Loop Diagrams are a simplified form of [[System Analysis]], the latter involves more diverse approaches.\n* To support the analysis of how systemic variables interact with each other, [[Agent Based Modelling]] may be applied.\n* CLDs may be analyzed using [[Social Network Analysis]] (see 4).\n* System Thinking is an important foundation for any method that views interactions between different agents. This is of relevance to methods that support thinking about and planning for the future, most notably [[Scenario Planning]] as well as [[Visioning & Backcasting]].\n\n\n== Outlook ==\nArnold and Wade (2015) describe the increasing growth of complex systems in our daily lives due to increasingly tied international trade relations, technological advancements and international policy decisions that influence other nations. They claim that '[s]ystems, if ever they were separated, are indomitably moving towards interconnectedness as we hurtle into a globalized future. (...) Now, more than ever, systems thinkers are needed to prepare for an increasingly complex, globalized, system of systems future in which everything (...) will produce ripple effects throughout the globe.\" (p.2)\n\n\n== An exemplary study ==\n[[File:System Thinking - Exemplary Study Hanspach et al. 2014 - Title.png|600px|frameless|center]]\nIn their 2014 publication, Hanspach et al. (see References) present - among other methodological approaches - a Causal Loop Diagram of the social-ecological system in Southern Transylvania. To create it, they conducted '''stakeholder workshops''' with \"(...) all relevant ethnic groups, political parties, churches, and schools, as well as local police officers and organizations concerned with nature conservation, regional development, forestry, agriculture, and tourism.\" (p.34) First, they held individual workshops with each stakeholder group, in which these were asked to present their \"(...) understandings of changes in the regions\" for the past and the future \"as well as of social-ecological system dynamics and key uncertainties\" (p.35). Based on the insights from these workshops, the researchers drafted causal effect chains and first causal loop diagrams, before '''they combined all individual results into one single CLD''', which they refined after receiving feedback in a second set of workshops. They further used the workshop insights to develop scenarios for the region - for more on this, please refer to [[Scenario Planning]].\n\n[[File:Result Visualisation Causal Loop Diagrams.png|700px|thumb|center|'''Causal loop diagram summarizing the dynamics of the\nregional social-ecological system in Southern Transylvania.''' R1 refers to the reinforcing feedback loop around local\neconomy, poverty, conflicts, and social capital. Source: Hanspach et al. 2014. p.36]]\n\n'''As the researchers highlight, this Causal Loop Diagram shows that local stakeholders think about their region that:'''\n* there is a strong link between the economy and the social capital of a given village\n* the low profitability of small-scale farming stimulates youth emigration and land abandonment\n* there is a negative influence of poor infrastructure on economic conditions\n* the collapse of the communist regime negatively influenced the social capital in the region\n* economic development could lead to short-term financial benefits, but could harm natural resources\n* there is a reinforcing feedback loop around poverty, conflict, low social capital, and poor education (R2 in the diagram), which caused rural emigration\n* the dual processes of farmland intensification in some areas and abandonment in others lead to a decrease in traditional small-scale farming and consequently negatively affect farmland biodiversity, as well as cultural, regulating, and supporting ecosystem services, and\n* forest exploitation for timber and firewood is a threat to forest biodiversity and the ecosystem services provided by forests.\n\nAs this study shows, systematically assessing causalities in a system and taking into account the interconnectedness between elements provides profound insights into system dynamics. Research can thus find further aspects to investigate, and policy and local actors may act according to these insights.\n\n== Key Publications ==\nForrester, J. W. 1961. ''Industrial dynamics.'' Pegasus Communications, Waltham, MA.\n* The publication that widely introduced system thinking to the world.\n\nHaraldsson, H.V. 2004. ''Introduction to System Thinking and Causal Loop Diagrams.'' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n* A brief explanation of System Thinking and a dailed description of how to develop a Causal Loop Diagram.\n\nMeadows, D. 2008. ''Thinking in Systems. A Primer.'' Chelsea Green Publishing, Vermont.\n* An good introduction to the topic in the field of sustainability science.\n\nCheckland, P. Systems Thinking. In: Curry, W.L. Galliers, B. 1999. ''Rethinking Managagement Information Systems.'' Oxford University Press. 45-56.\n* Representative for the various contributions Checkland made since the 1970s.\n\n\n== References ==\n\n(1) Haraldsson, H.V. 2004. ''Introduction to System Thinking and Causal Loop Diagrams.'' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n\n(2) Team TIP. 2011. ''Guidelines for drawing causal loop diagrams.'' Systems Thinker 22(1). 5-7.\n\n(3) Toole, M.T. 2005. ''A Project Management Causal Loop Diagram.'' ARCOM Conference, London, UK.\n\n(4) McGlashan et al. 2016. ''Quantifying a Systems Map: Network Analysis of a Childhood Obesity Causal Loop Diagram.'' PLoS ONE 11(10):\n\n(5) Hanspach et al. 2014. ''A holistic approach to studying social-ecological systems and its application to Southern Transylvania.'' Ecology and Society 19(4). 32-45.\n\n(6) Arnold, R.D. Wade, J.P. 2015. ''A Definition of Systems Thinking: A Systems Approach.'' 2015 Conference on Systems Engineering Research.\n\n(7) Chu, D. 2011. ''Complexity: Against Systems.'' Theory in Biosciences. 182-196.\n\n(8) Mitchell, S. 2004. ''Why integrative pluralism?'' Emergence: Complexity and Organization 1. 1-14.\n\n(9) Zexian, Y. Xuhui, Y. 2010. ''A Revolution in the Field of Systems Thinking - A Review of Checkland's System Thinking.'' Systems Research and Behavioral Science 27. 140-155.\n\n(10) Checkland, P. 2000. ''Soft Systems Methodology: A Thirty Year Retrospective''. Systems Research and Behavioral Science 17.11\u201358.\n\n\n== Further Information ==\n* [https://ncase.me/loopy/ LOOPY] is a simple web-based tool for creating your own feedback loops. It's easy to apply and a good start to get to know the process.\n* [https://www.youtube.com/watch?v=UgZTXf5PDis This video by Climate Interactive] on YouTube showcases how Causal Loop Diagrams can be used to model actions against Climate Change.\n* [https://ocw.mit.edu/courses/sloan-school-of-management/15-871-introduction-to-system-dynamics-fall-2013/index.htm An open MIT class] for those who want to dive deeper into System Dynamics. There is a short introduction lecture by MIT professor John Sterman available [https://www.youtube.com/watch?t=163&v=AnTwZVviXyY&feature=youtu.be here.]\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."
                    },
                    "sha1": "7ybrmiexvnvfwl105gffcmger0iowa5"
                }
            },
            {
                "title": "Systematic Literature Review",
                "ns": "0",
                "id": "282",
                "revision": {
                    "id": "6308",
                    "parentid": "6287",
                    "timestamp": "2021-09-01T06:58:30Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "28914",
                        "#text": "[[File:ConceptSystematicLiteratureReview.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Systematic_Literature_Review]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/>\n\n'''In short:''' In a Systematic Literature Review, existing publications are systematically analysed to derive an overview of a specific slice of the literature, typically summarizing the state of the art regarding a specific topic.\n\n== Background ==\n[[File:SLR.png|thumb|400px|right|'''SCOPUS hits per year for Systematic Literature Review until 2019.''' Search term: 'systematic literature review' in Title, Abstract, Keywords. Source: own.]]\n\nWith the rise of empirical knowledge in the [[History of Methods|Enlightenment]] arose the possibility to synthesize knowledge from different studies into an overview work. '''''A Treatise on the Scurvy - A Critical and Chronological View of What has been Published on the Subject'' by James Lind is seen as the first systematic review''' (3, 5, 6), highlighting the importance of knowledge integration. Another important origin of research synthesis can be traced to the work of 17th Century astronomers who combined data sets from different studies to ammend their own observations (3). '''Systematic literature reviews gained a vital tool through the work of Karl Pearson, whose work on statistics allowed to compile the results from several datasets into an overview.''' His 1904 publication - in which he combined 11 studies on typhoid vaccines and highlighted irregularities in the results - can be considered the first Meta-Analysis (3, 5). Meta-Analyses were subsequently applied more commonly during the 20th Century, for example in agriculture (5, 6). \n\nAfter the Second World War, US social scientists began to recognize the need to review the rising amount of research data while considering how to reduce [[Glossary|bias]] and enhance reproducibility of systematic reviews (6). This also led to the increasing recognition of qualitative elements. In the 1970s, statistician Gene Glass and colleagues proclaimed Meta-Analyses as a valid procedure for synthesising studies which helped to consolidate the approach (3). However, Systematic Literature Reviews were long viewed as second-class studies within Academia, since they did not yield primary data. This changed during the last decades, partly due to increasing interest in scientific evidence on diverse topics on the part of public policy makers, practitioners and the general public (6).\n\nMore recently, due to the emergence of digitalisation and improvements in information storage and retrieval, it became significantly easier to identify, gather and analyze the available research on a specific topic (3). Today, Systematic Literature Reviews are most commonly used in Medicine, in the Social Sciences, Business and Economics, but have found their way into several other disciplines (5).\n\n\n== What the method does ==\nSystematic Literature Reviews exist in a broad variety of types. While the Literature Review may be seen as the overarching term, sub-types include the Meta-Synthesis, the Systematic Review, the Case Survey or the strict Meta-Analysis (7). Their differentiation may be done in terms of the data they summarize (quantitative and/or qualitative) as well as the way this data is analyzed (qualitatively or quantitatively) (7). Reviews may also be differentiated according to their focus, goal, perspective, coverage, organization and audience. In this entry, we will focus on the Systematic Literature Review. More information on different sub-types of the method can be found in (3). Further, the approach of Meta-Analyses as the statistical tool of summarizing a variety of studies on a specific topic into one is an important method in this regard. [[Meta-Analysis|For more on Meta-Analyses, please read the respective Wiki entry.]]\n\n==== Definition ====\nA Systematic Literature Review is, in short, a reproducible process in which scientific publications that \"contain information, ideas, data and evidence\" (Hart 2018, p.13) on a specific topic are gathered; studies that fulfill a previously defined level of quality are selected; and their results and insights are summarized and evaluated. For the researcher, the results from this process provide insights into the current state of research and highlight relevant new directions for (their) further research (1, 2, 3). The term 'systematic' refers to the fact that the process is structured to minimize [[Glossary|bias]] (6) and maximimize reproducibility. Being 'systematic' means being reproducible and goes along with an a priori specified, dedicated research design and an explicit documentation of the steps taken during the research (3, see Normativity).\n\n[[File:Systematic Literature Review - Use of the method.png|500px|thumb|center|'''Some of the questions that a Literature Review can answer.''' Source: Hart 2018, p.14]]\n\nA Systematic Literature Review can be applied as the primary method of a scientific study, but is often also used as a first step in a larger research projector endeavor. In both cases, a review can help:\n* recognize what has already been done already regarding a specific research field or topics\n* find and resolve conflicts in (seemingly) contradictory studies, and\n* identify evolving or even unexplored research topics, questions or new hypotheses for further research.\n\nWhen used as a preparation to one's own study, a Systematic Literature Review additionally helps the researcher\n* identify relevant literature and researchers to consult,\n* design appropriate methodological approaches,\n* understand important concepts, theories and topics and summarise this knowledge to the reader\n* contextualize this research and show why it would answer an open question, often on integration level. (1, 3, 4)\n\n==== Step by Step ====\nA Systematic Literature Review follows a set of steps that is similar to any scientific research process (1, 3, 4):\n\n'''1) Planning:''' \nThe research is designed by formulating the question and scope of the review (here, the different forms of the review - see above - are of relevance). This planning can be both [[:Category:Inductive|inductive]] as well as [[:Category:Deductive|deductive]], depending on the focus of the review. For example, the researcher may be interested in how the literature defines \"Sustainable Agriculture\" (see example below), and thus choose to search for literature that apply and preferably define this concept.\n\n'''2) Data gathering:'''\nData (= literature) is searched and acquired. There may exist a wide range of documents of relevance to the research endeavour, which is why the researcher should attempt to become familiar with all related topical fields. The core of reviews are often articles in scientific [[Glossary|journals]] due to their comparable structure and assured quality. However, also books and practitioner articles may be of help (1). The data collection can be done using library catalogues or online search engines and databases, such as Google Scholar, SCOPUS, or Researchgate (1). Relevant literature can either be found by applying single search terms, but combining more specific search terms through AND, NOT or OR. This helps specify the results, whereas searching for single words can lead to very broad results. For example, \"sustainable\" AND \"agriculture\" may be of better help than just either of the terms. In addition, useful documents should be identified asking colleagues and by scanning the bibliographies of already selected papers until saturation is reached, i.e. no more relevant documents come light (4). When all data is gathered from the relevant different sources, duplicates are removed.\n\n'''3) Data selection:'''\nThe researcher now has a wide selection of literature available that fulfills the search terms. Now, they apply pre-defined quality and selection (= exclusion and inclusion) criteria to decrease the number of documents. One of four different approaches may be applied (3):<br>\n* exhaustive coverage (citing all relevant literature)\n* exhaustive coverage with selective citation\n* representative coverage (discussion of works which typify particular groupings in the literature) or \n* coverage of pivotal works. \nWhile the existing amount of research makes a truly 'exhaustive' collection rather 'exhausting' for many topics, any selection of articles comes with the danger of a selection [[Bias and Critical Thinking|bias]]. The relevance of the documents may be assessed based on reading the whole texts, just the abstracts, just the titles or some combination of these elements, which should be documented (4). The process description may include:\n** the literature search terms, the date of search, the number of search results and exclusion criteria,\n** a list of the included studies, as well as a description of the synthesis process,\n** a sample description of the data extraction process as well as\n** a description of the applied quality standards (3).\nFinally, the researcher attempts to obtain all full-texts for the selected publications. These may be available on the aforementioned platforms, sometimes via paid subscriptions, or by contacting the researchers directly, which are often happy to provide their publications. Some more publications may be excluded at this point, when it becomes obvious that a previously eligible-thought publications does not really fit the inclusion criteria after all.\n\n'''4) Data synthesis & analysis:'''\nAfter the documents were selected, the researcher reads them and extracts information that helps answer the research questions. In the process of synthesizing, a [[Mindmap]] or [[Concept Maps]] may be useful tools which can help organize and understand the concepts and theories used in the documents. The documents may further be arranged in such a map to support the structuring process of the review (1). \nThe focus and thus the methodological process of the data extraction depend on the goal of the review. For example, a researcher might want to focus on theories or concepts, definitions, methodological approaches or different scales and actors that played a role in the specific paper that is being reviewed. Typically, a coding book or procedure should be developed (and documented) in which the extraction process of information is defined, before all information of interest is sorted into the respective coding categories (4). While some variables can be extracted based on foundational work form previous studies, many parameters are often also extracted in an inductive way. \n\nThere are many diverse approaches to summarise and analysed the respective data, and to present it to the reader. Depending on the intended outcome of the review as well as the type of data gathered, these can be quantitative or qualitative:\n\nQualitative analyses are appropriate for the review of purely qualitative or mixed studies and can be often offer perspectives that go beyond quantitative analyses, for instance concerning deeply normative aspects. The qualitative review revolves around identifying essential themes of the documents and their relationships. Also, contrary findings and contradicting interpretations are of interest. \"The goal here, unlike meta-analysis, is to increase the understanding of the phenomena being investigated, not to integrate outcomes and identify factors that covary with outcomes.\" (4, p.10). \n\nBeside systematic literature reviews there is also another form of reviews, which is often referred to as 'narrative reviews', 'expert reviews', or simply as 'literature reviews'. These are often conducted in a non-systematic sense, but instead consist of the purely deliberate selection of the literature, typically by an author deeply experienced in the literature, and consist a balanced overview of the available literature. While such reviews are often seen to be more subjective, they can contain a lot of experience, and were more abundant in the past. These days such narrative reviews are often frowned upon, which is a pity, because there is a difference between knowledge and experience, and such narrative reviews can often offer a lifetime of experience.\n\nOften, a systematic literature review also additionally applies a Meta-Analysis in the analysis step. A Meta-Analysis is the statistical process of assessing quantitative, numerical data from separate studies that investigated a specific phenomenon. You might for instance calculate means or variations, and evaluate how specific factors influence specific processes. Combining both types of review has the benefit of first focusing on conceptual and qualitative insights from the body of literature, and then going deeper into quantitative measures. For more information on this, please refer to the [[Meta-Analysis]] entry.\n\n'''5. Writing & presentation of the review:'''\nFinally, the results of the Systematic Literature Review are compiled into a structured paper. A sample structure for the review as a preface to an original study may look like this (Rowley & Slack 2004, p.38): \n# Basic Definitions of key terms\n# Why is the subject of interest?\n# What research has already been undertaken on the topic, and is there any research on aspects of the topic that this research might investigate? \n# A clear summary of the results and new research opportunities that emerge from the literature review. Quotations may be used to underline specific findings from the review. \n# In addition, the literature gathered during the review should be listed. Many reviews consists a combination of information on the specific topic, the conceptual foundation, methodological approaches, and relevant scales that are associated to the available literature.\n\nIn summary, Systematic Literature Reviews are methods of data gathering - building on primary data from other empirical papers - and analysis. They are inductive because they conclude based on existing literature, but also deductive since they typically start with theoretical assumptions and a pre-defined thematic scope. They can be qualitative and quantitative, cover very local to global phenomena, and investigate past and present states of the literature, often offering a state of the art of the literature, and thereby suggestions and agenda for future research.\n\n\n== Strengths & Challenges ==\n* A literature review is a helpful starting point for any research: it provides a structured overview of the current knowledge and open questions in the respective field and may lead to new research questions that had not been obvious to the researcher before. The strength here is the systematic nature of the literature review: by not only reading randomly through literature, but instead following a systematic and reproducible approach, the researcher generates a more objective overview of the literature. Of course, the selected literature may still be biased and heavily depends on the research and selection criteria, but if this process is properly reasoned and documented, the bias may be reduced compared to a non-systematic review.\n* When used in preparation to an original study, \"[r]eviewing the literature in a systematic way helps the author to be clear, to build confidence in their work and demonstrate the rigour of their methods.\" (3, p.9). This is why Systematic Reviews are often done in a first step of a larger study, or as a first paper of a PhD student.\n* The systematic approach enables the researcher to state conclusions about the strength of available evidence to a specific assumption. This does not only support their own subsequent work, but also provide insight into the state of science in a given field to policy makers and other public actors. This way, the Systematic Literature Review may for example also shine light on the effectiveness of programs and policies (3).\n\nPotential mistakes and thus challenges in the review process include (4):\n* not clearly relating the findings of the literature review to the researcher's own study\n* not taking sufficient time to identify the best (primary!) sources\n* not critically reflecting upon the studies' research designs and analysis\n* not reporting the search procedures\n* reporting isolated statistical results instead of meta-analytic or [[Simple Statistical Tests|chi-square]] methods\n* not considering contrary findings or alternative interpretations when synthesizing quantitative literature\n\n\n== Normativity ==\n==== Connectedness ====\n* The Systematic Literature Review is strongly connected to further methods. As explained before, this form of secondary research is often included as a groundwork in original primary studies to help justify the research topic, design and methodology (2). \"Indeed, the concluding paragraphs of the literature review should lead seamlessly to research propositions and methodologies.\" (1, p.32)\n* The analysis of the gathered studies, as mentioned before, can take place in a quantitative way based on a variety of quantitative approaches. Most notably, the [[Meta-Analysis]] is of relevance here.\n* Systematic Literature Reviews represent an interesting and important form of methodological approach: they allow research on research. If scientific results only cumulated and no one looked at the bigger picture, science would not work. Methods like the literature review are thus crucial for academic processes.\n* Again, we would like to highlight that there is a difference between Systematic Literature Reviews and Literature Reviews in general. Often, you will do a literature review for a university report, or your thesis. For this, you gain an understanding of the available theoretical and empirical literature on a topic by browsing through literature, possibly through snowball sampling, until you feel like you have read enough. This may be necessary, enables you, to structure your own data collection, or to write an essay about the topic. This is absolutely valid, but is different from a systematic review with the presented systematic approach and documentation, which may enable much deeper insights into available research in the field.\n\n\n==== Quality criteria ====\n\"Quality [of the literature review] means appropriate breadth and depth, rigour and consistency, clarity and brevity, and effective analysis and synthesis; in other words, the use of the ideas in the literature to justify the particular approach to the topic, the selection of methods, and demonstration that this research contributes something new.\" (Hart 2018, p.1f). The quality criteria of objectivity, validity and reliability apply to the Systematic Literature Review as follows:\n* Objectivity & Validity: Literature reviews are to some level subjective because the synthesis of the screened literature (as well as the identification and selection of the literature in the first place) is, although to some extent reproducible, still a matter of the reviewer's normative decisions (3). The approach to gathering as well as excluding data may change as the researcher gains more experience in the process. Also, \"(...) [a]ll reviews, irrespective of the topic, are written from a particular perspective or standpoint of the reviewer. This perspective often originates from the school of thought, vocation or ideological standpoint in which the reviewer is located. As a consequence, the particularity of the reviewer implies a particular reader. Reviewers usually write with a particular kind of reader in mind: a reader that they might want to influence.\" (Hart 2018, p.25). It can be said that the more systematically the method is applied, the more potential [[Bias and Critical Thinking|bias]] is reduced (3). Special attention may be paid to the 'publication bias', highlighting the fact that often, those studies are preferred for publication that offer new or 'interesting' results, which may influence the review results (3). Also, researchers should not make the mistake of full-text-on-net-bias, but make sure to include all relevant literature, even though it may take some more time to access them.\n* Reliability or auditability is safeguarded by a detailed description of the system of the methodological process. This way, other reviewers following the same procedures under the same conditions should - in theory - find an identical set of articles and come to the same results and conclusions. Being systematic in these steps and reporting on the process improves clarity regarding what has been done and what has not been done, i.e. why certain documents have been included and why others have not (3).\n\n\n== Outlook ==\nIn the face of the ever-increasing amount of data gathered on diverse topics, as well as due to the changing role of science in society, there may be a increased interest and opportunity of the public in scientific results that may be summarized through Systematic Literature Reviews. With more and more data becoming available, scientists need to integrate existing knowledge to allow for a measured and responsible planning of future research. In addition, such reviews offer a necessary critical perspective, which may not only generate a future research agenda, but in addition can highlight flaws and biases in past research. As it is already the case in medicine, systematic literature reviews should be continuously or at least regularly be updated to offer the latest finding in an integrated way. This would ideally generate a structured but critical research agenda, and a better integration of knowledge that can be increasingly communicated to the public.\n\n\n== An exemplary study ==\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Paper title.png|600px|frameless|center|Exemplary study for Systematic Literature Review (Velten et al. 2015)]]\nIn their 2015 publication, Velten et al. investigate the definition and conceptualization of 'sustainable agriculture' in the available literature, with a focus on how social processes shaped the concept. They referred to '''both academic and practitioner-oriented literature'''. \n\nAcademic publications were searched on SCOPUS by use of the following '''search criteria:'''\n* search terms were \"sustainable agriculture\" OR \"agricultural sustainability\" in the title, abstract or keywords\n* English, German, French, Spanish and Portuguese publications\n* up to the year 2012 \n* in the subject areas of social sciences and humanities.\n\nNon-academic literature  - 'grey literature' - that is directed more towards practictioners and decision-makers was searched on websites, reports and brochures, using the same search terms and language restrictions on Google. The search continued until saturation was reached and \"no new usable publications were found\" (p.7836). Further, the pages of international organizations were searched for relevant documents.\n\nThe initial set of publications was then limited to those that \"gave at least a minimal definition or explanation of what was meant by sustainable agriculture\" (p.7836), resulting in 129 academic and 26 non-academic documents. These documents were analyzed using an '''inductive qualitative [[Content Analysis|content analysis]]''', during which key elements ('categories') and overarching topics ('themes') were extracted in three general groups that play a role in the definition of the concept of 'sustainable agriculture':<br>\n... Goals\n<br>\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Analysis scheme 2.png|500px|frameless|center|Analysis scheme 2 from Velten et al. 2015, p.7838]]\n<br>\n... Strategies\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Analysis scheme 3.png|500px|frameless|center|Analysis scheme 3 from Velten et al. 2015, p.7839]]\n<br>\n... and Fields of Action\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Analysis scheme 4.png|500px|frameless|center|Analysis scheme 4 from Velten et al. 2015, p.7839]]\n<br>\n\nFurther, the researchers developed an analysis scheme to analyze the extracted categories and themes quantitatively:\n<br>\n[[File:Systematic Literature Review - Exemplary Study - Velten et al. 2015 - Analysis scheme.png|750px|thumb|center|'''Analysis scheme for the systematic review on sustainable agriculture.''' Source: Velten et al. 2015, p.7837]]\n<br>\nIn a third step, the authors conducted a [[Clustering Methods|Cluster Analysis]] on the academic literature based on the quantitative data in order to identify groups of literature that bring up specific aspects of the concept.\n\nBased on their qualitative results, the researchers highlight that 'sustainable agriculture' can be separated into three distinct groups (Goals, Strategies, Fields of Action) with specific themes and categories, each. Based on their quantitative analysis, they show how the most frequent topics revolve around anthropocentric rather than ecocentric values when it comes to defining what is 'sustainable agriculture', and that the realization of sustainable agriculture mostly focuses on technological solutions on the farm level. However, they also found a strong alternative discourse that focuses on ecocentric and alternative approaches. They could further identify categories that changed in importance over time, and how the distribution of themes differed between academic and non-academic literature and between scientific disciplines. Lastly, their subsequent cluster analysis led to five groups of conceptual understandings of 'sustainable agriculture'.\n\nThis study exemplifies how a Systematic Literature Review with both qualitative and quantitative elements provides a rich conceptual overview of a topic which can be used to further structure work in this field, and raises questions for academia as well as practitioners. In their conclusion, the authors \"(...) recommend embracing the complexity of sustainable agriculture with its varied and seemingly contradictory aspects\" since they found \"(...) the different conceptions of sustainable agriculture to be not as contradicting and mutually exclusive as they\nhave often been portrayed\". (p.7857). '''This shows how a systematic review can help grasp the diversity of aspects on a given topic, and draw the respective conclusions.'''\n\n== Key publications ==\nHart, C. 2018. ''Doing a Literature Review''. SAGE Publications.\n* An extensive overview on all relevant steps of the literature review process targeted at young scholars at the Master's and Doctorate level.\n\nBooth, A. Sutton, A. Papaioannou, D. 2016. ''Systematic approaches to a successful literature review.'' Second Edition. SAGE Publications.\n* A step-by-step illustration of the literature review process and what makes it 'systematic'.\n\n\n== References ==\n(1) Rowley, J. Slack, F. 2004. ''Conducting a Literature Review.'' Management Research News 27(6). 31-39.\n\n(2) Hart, C. 2018. ''Doing a Literature Review.'' SAGE Publications.\n\n(3) Booth, A. Sutton, A. Papaioannou, D. 2016. ''Systematic approaches to a successful literature review.'' Second Edition. SAGE Publications.\n\n(4) Randolph, J. 2009. ''A Guide to Writing the Dissertation Literature Review.'' Practical Assessment, Research, and Evaluation 14(14).\n\n(5) Wikipedia. ''Systematic Review.'' Available at [https://en.wikipedia.org/wiki/Systematic_review#Research_fields](https://en.wikipedia.org/wiki/Systematic_review#Research_fields) (last accessed 09.07.2020)\n\n(6) Chalmers, I. Hedges, L.V. Cooper, H. 2002. ''A Brief History of Research Synthesis.'' Evaluation & The Health Professions 25(1). 12-37.\n\n(7) Newig, J. Fritsch, O. 2009. ''THE CASE SURVEY METHOD AND APPLICATIONS IN POLITICAL SCIENCE.'' APSA 2009 Conference Paper.\n\n(8) Velten, S. Leventon, J. Jager, N. Newig, J. 2015. ''What is Sustainable Agriculture? A Systematic Review''. Sustainability 7. 7833-7865.\n\n\n== Further Information ==\n* [https://www.youtube.com/watch?v=WUErib-fXV0 A short YouTube Video on Systematic Literature Reviews] by 'Research Shorts' that summarizes the method in 3 minutes.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "4sluu9unoi3ed4oygsemnry8lcwm7t6"
                }
            },
            {
                "title": "T-Test",
                "ns": "0",
                "id": "631",
                "revision": {
                    "id": "6272",
                    "parentid": "5269",
                    "timestamp": "2021-08-17T08:12:23Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "10388",
                        "#text": "'''In short:''' T-tests can be used to investigate whether there is a significant difference between two groups regarding a mean value (two-sample t-test) or the mean in one group compared to a fixed value (one-sample t-test). \n\nThis entry focuses on two-sample T-tests and covers the concept and purpose of the t-test, underlying assumptions, its implementation in R, as well as multiple variants for different conditions. For more information on the t-test and other comparable approaches, please refer to the entry on [[Simple Statistical Tests]].\n\n==General Information==\nLet us start by looking at the basic idea behind a two-tailed two-sample t-test. Conceptually speaking we have two hypotheses. \n\nH<sub>0</sub>: Means between the two samples do not differ significantly.<br/>\nH<sub>1</sub>: Means between the two samples do differ significantly. <br/>\n\nIn mathematical terms (\u03bc<sub>1</sub> and \u03bc<sub>2</sub> denote the mean values of the two samples): \n\nH<sub>0</sub>: \u03bc<sub>1</sub> = \u03bc<sub>2</sub> <br>\nH<sub>1</sub>: \u03bc<sub>1</sub> \u2260 \u03bc<sub>2</sub>. \n\n[[File:Kurt_olaf.jpg|500px|frameless|right]]\n'''Here is an example to illustrate this.''' The farmers Kurt and Olaf grow sunflowers and wonder who has the bigger ones. So they each measure a total of 100 flowers and put the values into a data frame.\n\n<syntaxhighlight lang=\"R\" line>\n# Create a dataset\nset.seed(320)\n\nkurt <- rnorm(100, 60.5, 22)\nolaf <- rnorm(100, 63, 23)\n</syntaxhighlight>\n\nOur task is now to find out whether means values differ significantly between two groups.\n<syntaxhighlight lang=\"R\" line>\n# perform t-test\nt.test(kurt, olaf)\n\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.1275\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -11.97670   1.50973\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n</syntaxhighlight>\n\nSo now, we performed a t-test and got a result. '''But how can we interpret the output?''' <br/>\nThe criterion to consult is the p-value. This value represents the probability of the data given that H0 is actually true. Hence, a low p-value indicates that the data is very unlikely if H0 applies. Therefore, one might reject this hypothesis (in favor of the alternative hypothesis H1) if the p-value turns out to be below a certain threshold (\u03b1, usually set prior testing), which is often set to 0.05 and usually not larger than 0.1. <br/>\n\nIn this case, the p-value is greater than 0.1. Therefore, the probability of H0 is considered to be \u201ctoo large\u201d to reject this hypothesis. Hence, we conclude that means do not differ significantly, even though we can say that descriptively the sample mean of Olaf\u2019s flowers is higher.\n<br/>\nThere are multiple options to fine-tune the t-test if one already has a concrete hypothesis in mind concerning the direction and/or magnitude of the difference. In the first case, one might apply a one-tailed t-test. The hypotheses pairs would change accordingly to either of these: <br>\n\nH<sub>0</sub>: \u03bc<sub>1</sub> \u2265 \u03bc<sub>2</sub><br>\nH<sub>1</sub>: \u03bc<sub>1</sub> < \u03bc<sub>2</sub>\n<br>\nor\n<br>\nH<sub>0</sub>: \u03bc<sub>1</sub> \u2264 \u03bc<sub>2</sub>\n<br>H<sub>1</sub>: \u03bc<sub>1</sub> > \u03bc<sub>2</sub>\n\nNote that the hypotheses need to be mutually exclusive and H0 always contains some form of equality sign. In R, one-tailed testing is possible by setting '''alternative = \"greater\"''' or '''alternative = \"less\"'''. Maybe Olaf is the more experienced farmer so we have already have \u00e0 priori the hypothesis that his flowers are on average larger. This would refer to our alternative hypothesis. The code would change only slightly:\n\n<syntaxhighlight lang=\"R\" line>\nt.test(kurt, olaf, alternative = \"less\")\n\n##  Output:\n## \n##  Welch Two Sample t-test\n## \n## data:  kurt and olaf\n## t = -1.5308, df = 192.27, p-value = 0.06373\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##       -Inf 0.4172054\n## sample estimates:\n## mean of x mean of y \n##  57.77072  63.00421\n</syntaxhighlight>\n\nAs one can see, the p-value gets smaller, but it is still not below the \u201cmagic threshold\u201d of 0.05. The question of how to interpret this result might be answered differently depending on whom you ask. Some people would consider the result \u201cmarginally significant\u201d or would say that there is \u201ca trend towards significance\u201d while others would just label it as being non-significant. In our case, let us set \u03b1\n = 0.05 for the following examples and call a result \u201csignificant\u201d only if the p-value is below that threshold.\n\nIt is also possible to set a \u03b4 indicating how much the groups are assumed to differ.\n<br>\n<br>\nH<sub>0</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> = \u03b4\n<br>\nH<sub>1</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> \u2260 \u03b4\n<br>\n<br>\nH<sub>0</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> \u2265 \u03b4\n<br>\nH<sub>1</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> < \u03b4\n<br>\n<br>\nH<sub>0</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> \u2264 \u03b4\n<br>\nH<sub>1</sub>: \u03bc<sub>1</sub> - \u03bc<sub>2</sub> > \u03b4\n\nTo specify a \u03b4, set '''mu = *delta of your choice*'''. If one sets a mu and only specifies one group, a one-sample t-test will be performed in which the group mean will be compared to the mu.\n\n\n==Assumptions==\nLike many statistical tests, the t-test builds on a number of assumptions that should be considered before applying it.\n\n#The data [[Data formats|should be continuous or on an ordinal scale.]]\n#We assume normally distributed data. While this is often not the case with smaller samples, according to the central limit theorem, the empirical distribution function of the standardised means converges towards the standard normal distribution with growing sample size. Hence, this prerequisite can be assumed to be met with a sufficiently large sample size (\u2265 30). \n#The data must be drawn randomly from a representative sample.\n#For a '''Student\u2019s t-test,''' equal variances in the two groups are required. However, by default, the built in function '''t.test()''' in R assumes that variances differ ('''Welch t-test'''). If it is known that variances are equal, one can set '''var.equal = TRUE''', which will lead to a Student\u2019s t-test being performed instead.\n\n==Paired t-test==\n\nIn many cases one has to do with groups that are not independent of each other. This is the case for example in within-subject designs where one compares data between the time points but within one participant. In this case, a paired t-test should be the means of choice. In R, this option can be chosen by simply setting '''paired = TRUE'''. It has the advantage of increasing statistical power, that is to say the probability of finding an effect (getting a significant result) if there actually is one. Let\u2019s modify our example: One hundred farmers try out a new fertiliser. Now we want to investigate whether the height of flowers treated with this fertiliser differs significantly from naturally grown ones. In this case, in each row of the dataset there is the mean value of flower size per farmer for each treatment:\n[[File:head3rows.png|500px|frameless|right]]\n<syntaxhighlight lang=\"R\" line>\n# Create dataset\nset.seed(320)\n\nfarmer <- c(1:100)\nwithFertilizer <- rnorm(100, 60, 22)\nwithoutFertilizer <- rnorm(100, 64, 17)\n\nds <- data.frame(farmer, \n                 withoutFertilizer, \n                 withFertilizer)\n\n# inspect the data\nhead(ds, 3)\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"R\" line>\n# perform t-test\nt.test(ds$withoutFertilizer, ds$withFertilizer, paired = T)\n\n\n##  Output:\n## \n##  Paired t-test\n## \n## data:  ds$withoutFertilizer and ds$withFertilizer\n## t = 2.2801, df = 99, p-value = 0.02474\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   0.8737431 12.5910348\n## sample estimates:\n## mean of the differences \n##                6.732389\n</syntaxhighlight>\n\nIn this case, the p-value is below our chosen \u03b1 = 0.05. Therefore, we can reject H<sub>0</sub> and conclude that there is probably a significant difference between means. But be careful! One might suspect that the fertiliser leads to an increased growth, but by looking at the mean values we realize that here, the contrary seems to be the case:\n[[File:withwithoutfertiliser.png|900px|frameless|center]]\n\n\n==Multiple testing==\nWhen performing a large number of t-tests (e.g. when comparing multiple groups in post-hoc t-tests after performing an [[ANOVA]]), one should keep in mind the issue of alpha-error-cumulation resulting in the \u201cproblem of multiple comparison\u201d. It states that with increasing number of tests performed the probability of making a Type-I error (falsely rejecting the null-hypothesis in favor of the alternative hypothesis) increases. To account for this risk, p-values should be adjusted. For instance, one might use the function '''pairwise.t.test(x = *response vector*, g = *grouping vector or factor*)'''. It performs t-tests between all combinations of groups and by default returns bonferroni corrected p-values. The adjustment method can be chosen by setting '''p.adjust.method''' equal to the name of a method of choice. For options see '''?p.adjust'''. An alternative would be to adjust p-values \u201cmanually\u201d:\n\n<syntaxhighlight lang=\"R\" line>\np.adjust(p = result_t.test[3], # p = object in which result of t.test is stored\n         method = \"bonferroni\", # method = p-adjustment method\n         n = 3) # n = number of tests\n</syntaxhighlight>\n\n==An additional tip for R Markdown users==\n\nIf you are using R Markdown you may use inline code to automatically print nicely formatted results (according to apa) of a t-test including degrees of freedom, test-statistic, p-value and Cohen\u2019s d as measure for effect size. You would need to use the funcion t_test (similar to t.test) and store the result in an object\u2026\n\n<syntaxhighlight lang=\"R\" line>\nresult <- t_test(ds$withoutFertilizer, \n                 ds$withFertilizer, \n                 paired = T)\n\n# the inline-code would look like this:\n# `r apa(result)`\n</syntaxhighlight>\n\n\u2026that can then be called inline (see above). The result will look like this: t(99) = 2.28, p = .025, d = 0.23. Note that the functions '''t_test()''' and apa() require the package \u201capa\u201d.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]"
                    },
                    "sha1": "q2ocfj846iqidpat6w4myaq58u2u52m"
                }
            },
            {
                "title": "Table of Contributors",
                "ns": "0",
                "id": "148",
                "revision": {
                    "id": "7183",
                    "parentid": "7157",
                    "timestamp": "2023-05-22T11:59:08Z",
                    "contributor": {
                        "username": "Annrau",
                        "id": "128"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "16907",
                        "#text": "This page lists all those who contributed to entries on this Wiki - either by writing articles, feedbacking them or adding further information and links. Each entry is only listed once, so if an entry is tagged with several categories, it will only show up in one of the tables below.\n\nThe author of each entry is also indicated in the line \"The [[Table_of_Contributors|author]] of this entry is AUTHORNAME\" on the bottom of every Wiki entry.\n\n== The Entries ==\n__NOTOC__\nClick on \"Expand\" on the right to see the complete table.\n<br/>\n==== [[Statistics]] ====\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! style=\"width: 30%\"| Entry !! style=\"width: 30%\"|Main author !! style=\"width: 40%\"| Contributor(s)\n|-\n| [[A matter of probability]] || Henrik von Wehrden || \n|-\n| [[An initial path towards statistical analysis]] || Henrik von Wehrden, Christopher Franz ||\n|-\n| [[Ancova]] || ||\n|-\n| [[Apply, Lapply and Tapply]] || ||\n|-\n| [[Back of the envelope statistics]] || Henrik von Wehrden || Christopher Franz\n|-\n| [[Bias in statistics]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Max Kretschmer, Prabesh Dhakal, Elisabeth Frank\n|-\n| [[Bootstrap Method]] || Andrei Perov ||\n|-\n| [[Case studies and Natural experiments]] || Henrik von Wehrden || Elisabeth Frank, Carlo Kr\u00fcgermeier\n|-\n| [[Causality]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Elisabeth Frank\n|-\n| [[Causality and correlation]] || Henrik von Wehrden ||\n|-\n| [[Chord Diagram]] || ||\n|-\n| [[Correlation Plots]] || ||\n|-\n| [[Cronbach's Alpha]] || ||\n|-\n| [[Data distribution]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Elisabeth Frank, Prabesh Dhakal, Max Kretschmer\n|-\n| [[Data formats]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Milan Maushart, Brad Dobberfuhl, Prabesh Dhakal, Max Kretschmer, Elisabeth Frank\n|-\n| [[Descriptive statistics]] || Henrik von Wehrden || \n|-\n| [[Designing studies]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Elisabeth Frank\n|-\n| [[Ethics and Statistics]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Elisabeth Frank\n|-\n| [[Experiments]] || Henrik von Wehrden || Prabesh Dhakal, Max Kretschmer, Elisabeth Frank, Carlo Kr\u00fcgermeier\n|-\n| [[Field experiments]] || Henrik von Wehrden || Carlo Kr\u00fcgermeier, Elisabeth Frank\n|-\n| [[Histograms and Boxplots]] || Ilkin Bakhtiarov ||\n|-\n| [[How long do you store data?]] || Prabesh Dhakal ||\n|-\n| [[Introduction to statistical figures]] || Henrik von Wehrden || Christopher Franz\n|-\n| [[Kernel density plot]] || Archana Maurya ||\n|-\n| [[Likert Scale]] || Nora Pauelsen ||\n|-\n| [[Sampling for Interviews]] || Christopher Franz ||\n|-\n| [[Stacked Barplots]] || ||\n|-\n| [[Statistics and mixed methods]] || Henrik von Wehrden || Elisabeth Frank, Carlo K\u00fcrgermeier\n|-\n| [[T-Test]] || ||\n|-\n| [[The future of statistics?]] || Henrik von Wehrden ||\n|-\n| [[Venn Diagram]] || Olga Kuznetsova ||\n|-\n| [[Why statistics matters]] || Henrik von Wehrden || Milan Maushart, Carlo Kr\u00fcgermeier, Elisabeth Frank, Prabesh Dhakal, Max Kretschmer\n|-\n| R example responsibility || Olga Kuznetsova ||\n|}\n\n==== [[Methods]] ====\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! style=\"width: 30%\"| Entry !! style=\"width: 30%\"|Main author !! style=\"width: 40%\"| Contributor(s)\n|-\n| [[ANOVA]] || Henrik von Wehrden ||\n|-\n| [[Bayesian Inference]] || Prabesh Dhakal, Henrik von Wehrden ||\n|-\n| [[Citizen Science]] || Christopher Franz || Esther Kohlhase, Henrik von Wehrden\n|-\n| [[Clustering Methods]] || Prabesh Dhakal || Henrik von Wehrden\n|-\n| [[Cohort Study]] || Christopher Franz || Henrik von Wehrden\n|-\n| [[Correlations]] || Henrik von Wehrden, Christopher Franz || Stefan Kruse, Elisabeth Frank, Carlo Kr\u00fcgermeier\n|-\n| [[Content Analysis]] || Christopher Franz || Fine B\u00f6ttner\n|-\n| [[Counting Birds]] || Anna-Lena Rau || \n|-\n| [[Delphi]] || Christopher Franz || Esther Kohlhase\n|-\n| [[Ethnography]] || Christopher Franz || Katharina Kirn\n|-\n| [[Focus Groups]] || Fine B\u00f6ttner || Christopher Franz \n|-\n| [[Generalized Linear Models]] || Henrik von Wehrden ||\n|-\n| [[Geographical Information Systems]] || Henrik von Wehrden || \n|-\n| [[Grounded Theory]] || Henrik von Wehrden ||\n|-\n| [[Group Concept Mapping]] || Christopher Franz || Henrik von Wehrden\n|-\n| [[Hermeneutics]] || Katharina Kirn || Christopher Franz\n|-\n| [[Iconology]] || Mona H\u00fcbner || Christopher Franz\n|-\n| [[Legal Research]] || Christopher Franz || Jelena B\u00e4umler\n|-\n| [[Life Cycle Analysis]] || Christopher Franz ||\n|-\n| [[Living Labs & Real World Laboratories]] || Christopher Franz || Philip Bernert\n|-\n| [[Machine Learning]] || Prabesh Dhakal || Henrik von Wehrden\n|-\n| [[Macroinvertebrates]] || Anna-Lena Rau || \n|-\n| [[Meta-Analysis]] || Henrik von Wehrden ||\n|-\n| [[Mixed Effect Models]] || Henrik von Wehrden\n|-\n| [[Narrative Research]] || Christopher Franz || Fine B\u00f6ttner\n|-\n| [[Open Interview]] || Christopher Franz || Henrik von Wehrden, Esther Kohlhase\n|-\n| [[Principal Component Analysis]] || Ch\u00e2n L\u00ea || Christopher Franz, Henrik von Wehrden\n|-\n| [[Regression Analysis]] || Henrik von Wehrden, Quentin Lehrer ||\n|-\n| [[Scenario Planning]] || Christopher Franz || Ricarda Hille, Henrik von Wehrden\n|-\n| [[Serious Gaming]] || Christopher Franz || Henrik von Wehrden, Prabesh Dhakal, Fine B\u00f6ttner\n|-\n| [[Semi-structured Interview]] || Christopher Franz || Henrik von Wehrden, Esther Kohlhase\n|-\n| [[Simple Statistical Tests]] || Carlo Kr\u00fcgermeier, Henrik von Wehrden || Elisabeth Frank, Prabesh Dhakal, Max Kretschmer, Lisa Sch\u00f6nrock, R. Shaurya, Aleksandra Viktorova\n|-\n| [[Social Network Analysis]] || Christopher Franz || Jacqueline Loos\n|-\n| [[Survey Research]] || Fine B\u00f6ttner || Christopher Franz\n|-\n| [[Systematic Literature Review]] || Christopher Franz || Henrik von Wehrden\n|-\n| [[System Thinking & Causal Loop Diagrams]] || Christopher Franz || Henrik von Wehrden, Dave Abson\n|-\n| [[Thought Experiments]] || Henrik von Wehrden ||\n|-\n| [[Video Research]] || Christopher Franz || Fine B\u00f6ttner\n|-\n| [[Visioning & Backcasting]] || Christopher Franz || Ricarda Hille, Henrik von Wehrden\n|-\n| [[Walking Exercise]] || Oskar Lemke || \n|}\n\n==== [[Skills & Tools]] ====\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! style=\"width: 30%\"| Entry !! style=\"width: 30%\"|Main author !! style=\"width: 40%\"| Contributor(s)\n|-\n| [[Anki]] || Matteo Ramin || Dagmar M\u00f6lleken\n|-\n| [[Belbin Team Roles]] || Esther Kohlhase || Christopher Franz\n|-\n| [[Check In]] || Alexa B\u00f6ckel || Christopher Franz\n|-\n| [[Citations]] || Katharina Kirn || Christopher Franz\n|-\n| [[Code of Conduct]] || Julius Rathgens || Max Kretschmer\n|-\n| [[Conceptual Figures]] || Christopher Franz, Matteo Ramin  ||\n|-\n| [[Concept Maps]] || Christopher Franz || \n|-\n| [[Coping with psychological problems]] || Matteo Ramin || \n|-\n| [[Design Thinking]] || Alexa B\u00f6ckel || Esther Kohlhase\n|-\n| [[Digital Energizers]] || Matteo Ramin ||\n|-\n| [[Disney Method]] || Christopher Franz || Henrik von Wehrden, Matteo Ramin\n|-\n| [[Elevator Pitch]] || Christopher Franz ||\n|-\n| [[Empathetic Listening]] || Mona H\u00fcbner ||\n|-\n| [[Feynman Method]] || Christopher Franz ||\n|-\n| [[Fishbowl Discussion]] || Christopher Franz || Alexa B\u00f6ckel\n|-\n| [[Flashlight]] || Dagmar M\u00f6lleken ||\n|-\n| [[Flipped Classroom]] || Matteo Ramin, Christopher Franz ||\n|-\n| [[Gender First Aid Kit]] || Elisabeth Frank, Oskar Lemke || Henrik von Wehrden\n|-\n| [[Giving Feedback]] || Max Kretschmer || Julius Rathgens\n|-\n| [[Graphic Recording]] || Dagmar M\u00f6lleken || Matteo Ramin\n|-\n| [[How to read an empirical paper]] || Henrik von Wehrden ||\n|-\n| [[How to write a thesis]] || Henrik von Wehrden || \n|-\n| [[How to PhD]] || Henrik von Wehrden ||\n|-\n| [[Kanban]] || Matteo Ramin || Christopher Franz\n|-\n| [[Learning for exams]] || Matteo Ramin || Henrik von Wehrden, Christopher Franz\n|-\n| [[Lego Serious Play]] ||  Henrik von Wehrden || Christopher Franz, Matteo Ramin\n|-\n| [[Loom]] || Christopher Franz\n|-\n| [[Microsoft Word For Academic Writing]] || Matteo Ramin || Henrik von Wehrden\n|-\n| [[Mindfulness]] || Henrik von Wehrden, Katharina Kirn ||\n|-\n| [[Mindmap]] || Christopher Franz ||\n|-\n| [[Miro]] || Matteo Ramin ||\n|-\n| [[Notion]] || Matteo Ramin || Christopher Franz\n|-\n| [[Online Conduct]] || Matteo Ramin ||\n|-\n| [[Overcoming Exam Anxiety]] || Katharina Kirn ||\n|-\n| [[PechaKucha\u2122]] || Cristina Apetrei || Dagmar M\u00f6lleken\n|-\n| [[Persona Building]] || Christopher Franz\n|-\n| [[Pomodoro]] || Matteo Ramin ||\n|-\n| [[Poster Design]] || Ch\u00e2n L\u00ea || Lisa Gotzian\n|-\n| [[Research Diary]] || Henrik von Wehrden|| Lisa Gotzian\n|-\n| [[Scrum]] || Matteo Ramin || Christopher Franz\n|-\n| [[Speed Typing]] || Christopher Franz ||\n|-\n| [[Stakeholder Mapping]] || Christopher Franz ||\n|-\n| [[Staying on top of research]] || Lisa Gotzian || Christopher Franz, Henrik von Wehrden\n|-\n| [[Studying]] || Henrik von Wehrden, Christopher Franz, Olga Kuznetsova, Elisabeth Frank, Linda von Heydebreck, Oskar Lemke ||\n|-\n| [[The Do's and Don'ts of Meetings]] || Henrik von Wehrden ||\n|-\n| [[Tips for digital lectures]] || Christopher Franz, Matteo Ramin, Elisabeth Frank, Carlo Kr\u00fcgermeier, Iman Aoulkadi ||\n|-\n| [[World Caf\u00e9]] || Dagmar M\u00f6lleken || Cristina Apetrei\n|-\n| [[Yes, and]] || Christopher Franz || Esther Kohlhase\n|}\n\n==== [[Normativity of Methods]] ====\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! style=\"width: 30%\"| Entry !! style=\"width: 30%\"|Main author !! style=\"width: 40%\"| Contributor(s)\n|-\n| [[Agency, Complexity and Emergence]] || Henrik von Wehrden || Christopher Franz\n|-\n| [[Bias and Critical Thinking]] || Henrik von Wehrden || Christopher Franz\n|-\n| [[Bias in Interviews]] || Christopher Franz ||\n|-\n| [[Design Criteria of Methods]] || Henrik von Wehrden || Christopher Franz, Katharina Kirn\n|-\n| [[Design Criteria of Methods in Sustainability Science]] || Henrik von Wehrden, Christopher Franz ||\n|-\n| [[Experiments and Hypothesis Testing]] || Henrik von Wehrden ||\n|-\n| [[History of Methods]] || Henrik von Wehrden || Christopher Franz, Brad Dobberfuhl\n|-\n| [[Levels of Theory]] || Henrik von Wehrden, Christopher Franz ||\n|-\n| [[Mixed Methods]] || Henrik von Wehrden ||\n|-\n| [[Non-equilibrium dynamics]] || Henrik von Wehrden ||\n|-\n| [[Questioning the status quo in methods]] || Henrik von Wehrden ||\n|-\n| [[Transcribing Interviews]] || Christopher Franz || Dagmar Moelleken\n|-\n| [[Scientific methods and societal paradigms]] || Henrik von Wehrden ||\n|-\n| [[System Boundaries]] || Henrik von Wehrden ||\n|-\n| [[The tao of R-coding]] || Henrik von Wehrden ||\n|-\n| [[Time]] || Henrik von Wehrden ||\n|-\n| [[To Rule And To Measure]] || Christopher Franz, Henrik von Wehrden ||\n|-\n| [[Transdisciplinarity]] || Christopher Franz || Henrik von Wehrden\n|}\n\n==== Other pages ====\n{| class=\"wikitable mw-collapsible mw-collapsed\" style=\"text-align: center; width: 100%; background-color: white\"\n|-\n! style=\"width: 30%\"| Entry !! style=\"width: 70%\"| Contributor(s)\n|-\n| [[Sustainability_Methods:About|About]] page || Henrik von Wehrden, Christopher Franz\n|-\n| [[Main Page]] || Henrik von Wehrden, Christopher Franz, Prabesh Dhakal\n|-\n| Technical Supervision || Henrik von Wehrden, Prabesh Dhakal, Christopher Franz\n|}\n<br/>\n\n== The Contributors ==\nThe following individuals have contributed to the Wiki. Further contributions were written by Leuphana Data Science and Sustainability Science Master students, who we are very thankful for!\n\n==== Henrik von Wehrden ====\nHenrik von Wehrden is a professor of methods at Leuphana since 2010. Henrik conceived the original idea of the Wiki as a contribution to a post-disciplinary agenda, and his research focuses on the gap between results based on empirical research and the question how we ought to act based on ethics.\n\n==== Prabesh Dhakal ====\nPrabesh Dhakal finished his M.Sc. in Data Science at Leuphana in 2021. He is interested in machine learning and data visualization, and was, among other projects, responsible for technical supervision of the Wiki.\n\n==== Christopher Franz ====\nChristopher Franz worked as a scientific assistant in the team from 06/2020 to 01/2022, working on methodology teaching and the development of the Sustainability Methods Wiki. He holds a B.Sc. in Biogeosciences and a M.Sc. in Sustainability Science and is interested in Climate Change and Sustainability Communication.\n\n==== Matteo Ramin ====\nMatteo Ramin studies Studium Individuale with a specific emphasis on how to transform educational systems. His personal experience spans a bachelor\u2019s degree in Computer Sciences and work experience in IT management consulting and software development.\n\n==== Elisabeth Frank ====\nElisabeth Frank studies Environmental and Political Science and joined the team in 09/2019. She has mostly worked on Wiki entrys regarding the bachelors statistics lecture. She is especially interested topics regarding gender and intersectionality in sustainability and political science.\n\n==== Carlo Kr\u00fcgermeier ====\nCarlo Kr\u00fcgermeier studies Environmental Sciences and Political Science and has been part the team since 09/2019. He is interested in quantitative methods and the functionality of political systems.\n\n==== Fine B\u00f6ttner ====\nFine B\u00f6ttner is pursuing her Master degree in Sustainability Science with a focus on transdisciplinary research and education for sustainable development. She is mostly interested in qualitative methods.\n\n==== Ch\u00e2n L\u00ea ====\nCh\u00e2n L\u00ea studied Business Administration and is currently working on his M.Sc. in Data Science at Leuphana in 2021. He joined the team at the beginning of 2020 and mainly focused on statistics teaching.\n\n==== Ricarda Hille ====\nRicarda Hille finished her M.Sc. in Sustainability Science in 2021. Her background is in fostering student participation for sustainable development at higher education institutions. In her master thesis she raised the question how research in Sustainability Science is currently designed.\n\n==== Lisa Gotzian ====\nLisa Gotzian holds a bachelor\u2019s degree in Business Psychology and finished her M.Sc. in Data Science at Leuphana in 2021. In the team, she focused on Statistics teaching.\n\n==== Dagmar Moelleken ====\nDagmar Moelleken has a background in agricultural practice, and studied Agroecology, Philosophy and Sustainability Science. In her PhD she explores ethics, normativity and paradigms in relation to sustainable agriculture using a mixed methods approach.\n\n==== Anna-Lena Rau ====\nAnna-Lena Rau finished her PhD on temporal dynamics of ecosystem services under the supervision of Henrik von Wehrden in May 2021. Since May 2022, she works on improving the Wiki for the Project \"Digital Transformation Lab for Teaching and Learning\" (DigiTaL).\n\n==== Cristina Apetrei ====\nCristina Apetrei is a Ph.D. candidate investigating the role of knowledge in sustainability transformations. Her theoretical interests are in complex system science, behavioural economics, agent-based modeling, transdisciplinarity and worldview based approaches.\n\n==== Esther Kohlhase ====\nEsther Kohlhase holds a B.Sc. in Business Psychology and M.Sc. in Sustainability Science from Leuphana. She worked in the team as a PhD student in the field of land use from 2020 to 2021.\n\n==== Julius Rathgens ====\nJulius Rathgens worked as a PhD student in the team until 2021. His PhD research was concerned with the production and consumption of food in global supply chains. His passion is about mixed methods, with a focus on structural equation modeling, network analyses and participatory research practices.\n\n==== Max Kretschmer ====\nMax Kretschmer did a bachelor in Economics and Sustainability Science with a focus on the sustainable transformation of monetary systems. He joined the team in 05/2018, where he was mostly engaged in projects in the field of ecological economics.\n\n==== Alexa B\u00f6ckel ==== \nAlexa B\u00f6ckel holds a Bachelors in Business Administration and Environmental Sciences from Leuphana University and finished her M.Sc. Sustainability Science in 2021. She is interested in sustainable entrepreneurship and crowdfunding and worked on digitizing the processes and communication of the working group.\n\n==== Olga (Ollie) Kuznetsova ====\nOlga Kuznetsova holds a B.A. in Linguistics and at the moment studies B.Sc. Environmental Sciences at Leuphana University. She has a special interest in Data Science, Machine Learning and programming with R and Python. Olga joined the team in 11/2020.\n\n==== Linda von Heydebreck ====\nLinda von Heydebreck is a B.Sc. student in Environmental Science. She joined the team in 2021.\n\n==== Oskar Lemke ====\nOskar pursues a Bachelor degree in Environmental Science. He is interested in social (political) movements, intersectionality and sustainability. He joined the team in 2021."
                    },
                    "sha1": "nc7guz8kuaemwks2lc12xrmg1z0why3"
                }
            },
            {
                "title": "The \"I\" in this wiki",
                "ns": "0",
                "id": "566",
                "revision": {
                    "id": "6717",
                    "parentid": "5903",
                    "timestamp": "2022-06-15T20:15:25Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "5674",
                        "#text": "Using the first person is usually discouraged in scientific writing. In a nutshell, I disagree with this notion, and explain in this text, why we use the first person - both ''I'' and ''we'' - in this Wiki, and how you should read and understand this.\n\n'''I perceive the norm of writing in a third person as a scientist as a clear indicator of the arrogance of science that is deeply rooted in the fallacy of positivism.''' By writing in the third person, many scientists [https://www.editage.com/insights/is-it-acceptable-to-use-first-person-pronouns-in-scientific-writing claim they convey objective information] that can be reproduced by anyone, while [https://www.sciencemag.org/careers/2012/03/how-write-scientist the first person humanises our work]. \nI think that by clearly indicating something as our personal opinion or to be based on our personal experience we give the information more value, and not less. This is rooted in me defining myself as a [[Bias and Critical Thinking|critical realist]]. I think there is higher value information in ethics that help us make sense of the world and find meaning, and these are ontologically objective [[Glossary|assumptions]]. However, when it comes to epistemological knowledge - and this is what sciences is mostly preoccupied with - we only have subjective information at our disposal. \n\n'''Within this epistemological knowledge, it is possible to clearly differentiate between activated observations that I made, and observations that are shared by the scientific community.''' For the latter, I would clearly use the third person. By doing this, I clearly acknowledge that our knowledge may change in the future, but there is a scientific canon that currently agrees on specific knowledge. An example of such knowledge would be the continuous development of life through evolution. Our knowledge about the mechanisms which we understand that are behind these [[Glossary|processes]] still continuously change (consider for instance the impact of epigenetic on our lives). Still, we can widely agree that there are processes such as natural selection and mutations that drive the adaptation and further development of living organisms. It is thus justified to say that science generally accepts evolution as the mechanisms behind the diversity of life. This is different from information that is conveyed by a sub-community within the wider arena of science. Altruism is a good example where you have basically two principal sub-communities: the scientists that view altruism as a trait of evolutions, and scientists who look for something beyond a mere kinship advantage perceived through altruism. I count myself to the latter group. I do no want to act altruistically because evolution tells me to, but look instead for higher reasons that are independent of my genes. Another example would be in statistics. I am more interested in parsimonious models, and many scientists would agree that these are preferable to full models. However, some researchers believe that full models - without any model reduction towards a parsimonious model - deserve our attention, and hence publish full models. An increasing number of researchers within frequentist statistics consider that this approach is wrong. Consequently, when I say \"we propose that model reduction is preferable to reporting full models\", I mean myself and likeminded scientists. This is a deeply normative statement rooted in my experience and the experience of other scientist, and therefore ideally deserves further explanation. Consequently, I might write something to the end of \"by avoiding p-values and building my model reduction on AIC values, the reported statistic model are more robust under the criteria of parsimony\".\n\n'''The use of third person is to me a great destroyer of scientific discourse since it makes the way we conduct discourse less precise.''' I think the scientific community made a grave mistake when it shifted to the third voice almost exclusively, since this took away any possibility to clearly indicate whether you speak of widely accepted knowledge, knowledge accepted by a specific community, or your own experience. The last sentences are all written in the first person, because I wrote down my opinion. It is \"I\" who has this opinion, and while I am sure and even vaguely know that I am not the only person that has this opinion, the norms of the scientific community widely reject my opinion. I have no precise knowledge about other scientists who share my opinion. Hence I verbalised a new thought, and this deserves the first person. It is I who has this thought, and I offer this thought to the scientific community as a basis for discourse.\n\nA word on opinions. An opinion can be defined as a statement rooted in a personal believe or perceptions that is neither supported by evidence, nor cannot be tested. All I write on this Wiki is clearly supported by evidence, and can be indeed tested and thus be confirmed or rejected. It is up to the readers to do this, but all written here is not about my opinions. People have confused this in this past, hence it seems reasonable to clarify this. \n\n'''To summarise, this wiki will use three different perspectives in terms of statements.''' The widest most general agreement includes knowledge that is accepted by the majority of scientists - written in the third person. The second category is knowledge by a scientific sub community - \"we\". The third - innermost - category is knowledge that is proclaimed by me as a curator of this Wiki, on this Wiki, marked by \"I\". All three categories are normative, since the knowledge within all three categories may change."
                    },
                    "sha1": "t6d3ad3yprrvvhpt2p7e58mehh0kh29"
                }
            },
            {
                "title": "The Academic System",
                "ns": "0",
                "id": "1008",
                "revision": {
                    "id": "6911",
                    "timestamp": "2023-02-11T21:30:39Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "comment": "Created page with \"Academia is equally an opportunity and a challenge, and many times we cannot even say which is which. Some of its elements date back into the ages, and on the other end is aca...\"",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "20657",
                        "#text": "Academia is equally an opportunity and a challenge, and many times we cannot even say which is which. Some of its elements date back into the ages, and on the other end is academia defined by creating knowledge that evolves, pushing our insights and the way we act based upon them forward, ideally radically changing the world for the better. Academia originated in a resource surplus of early societies, which could suddenly afford to have citizens that concentrated on nothing but knowledge with all its facets. Over time, academia became subsequently more focussed, and scientific disciplines emerged out of schools of thinking. Today, there is a dendritic network of disciplines that divides into ever smaller units with strong identities, which are often defined by their focal topics, underlying concepts and theories, and the associated canon of specific methods. Finding your roles and more importantly your vector of strategies and reflection points in the modern academia is a key challenge that serves as an initial building block of your career. Whole books are available on how to make it in academia, yet such approaches are somewhat beyond the point. If there was a recipe that everybody can follow, then we would reproduce the same academics all over. Academia is defined by evolvement and diversity, or at least it should be. Scientific disciplines often counteract this baseline. Out of the enlightenment and the industrial age, scientific disciplines were initially designed to produce workers of the mind that would administer economic development and colonial abuse. In the 20th century, knowledge became more and more compartmentalised, and ever finer layers of disciplines developed. To this date, the system is strongly competitive and resembles mostly a social Darwinist funnel. While all countries to this day build on a hierarchical system where professors are the highest caste, this system has lately been drastically changed in many countries. Germany is at the forefront of a movement where you have an overall higher number of full professors, but in comparison less assistant professors and lecturers. While in the past much of the teaching was long-term shouldered by these mid-level academia workers, most of these increasingly vanished today, and much of the remaining positions are limited to a maximum of six years. This limited time frame is often one of the main breaking points in modern academia. You have 6 years for your PhD, and 6 years as a postdoc. If you do not have a permanent contract after 12 years, you are legally out. Other countries have comparable mechanisms, and after such a long time it is anyway clear for most people if you can make it, or not. There is however still a larger convolute of lecturer positions in Scandinavian and English-speaking countries. However, even these positions are now characterised by an expectation that often goes beyond excellence in teaching and includes usually a research impact and the deep commitment towards management tasks and development of the institution. Yet way before the question how to gain a permanent position, other puzzles cross your way: different hierarchies, institutional constructs, tacit barriers and simply too many unknowns to find your way into the system. There are mainly two ways that can help prospering young academics to find their way into a successful career: A solid peer-network and one or few mentors. Without a mentor, life in academia and more importantly life into academia is almost impossible. May it be the medieval apprenticeship in the style of \"The Sword in the Stone\", shall it be the Kung Fu disciple just as in the \"36th chamber of the Shaolin\", or may it be \"Mulan\". Having someone see the raw germ and unleashing its full potential is a collaborative act of learning, and current professors can thus become one of the best bets that future professors will be better. One of my first mentors did actually always say that he only accepts disciples that are better than he is. A mentor ideally helps a learner to deal with all the challenges you face in the system, and does not only explain all cliffs, but also the path how to let them pass. Finding a mentor is like finding any other social relationship, just maximise your serendipity space, and then it may play out in your favour. Beside the knowledge may the serendipity space of your mentor also translate into a larger space of opportunities for yourself. What is most important for any mentorship is that it actually works both ways at the same time, since good students question aspects of the professors, and push the system to evolve, and also ideally turn the teacher into a learner, which is vital. Professors may have more experience, but the innovative drive often happens on the lower ranks, and certainly much of the wider workload. What is almost more important is a peer network, because this is your emotional safety net and lifeline. Joined suffering creates strong bonds, and many of the strongest bonds were formed over a solid network of early career peers suffering together and helping each other out. Such a network can also grow over time, and link into existing networks and create beneficial connections that help you get your career going. More often than not it is not so much about knowing people, it is more about knowing information. Thus, beside the emotional coping is a network a living body of knowledge that helps you to overcome obstacles. The most important information is often that your struggle is not unique, but indeed many people face similar issues. A peer network is also relevant for one main movement: If a lot of lower ranks talk about the same stuff, it actually translates up, and professors will become more interested in what gets the young folks around.\n\n=== Discrimination and Bias ===\n\nAcademic institutions are deeply committed to integrate diversity into their DNA, and they should, because diversity clearly thrives innovation. However, there is ample evidence that institutions still fail to this end, not on all fronts, but there are too many prominent cases and hard information which more than underline that there is a lot to be done to recognise, appreciate and integrate diversity. While institutions are deeply committed to have gender and diversity offices, responsible colleagues to support people with disabilities, to aid international students and so on, it is clear that academic institutions are strongly biased in many directions. Emotions run high to this end, and many non-minorities currently create backlashes against the necessary pushes towards more diversity. However, I have a perspective spanning over decades now, and what most people are not aware of is that changes are happening. They are not fast enough, which is more than clear. Recent generations are not ok with that kind of a slow adaptational evolutionary process, and the demand for more revolutionary changes is currently creating tensions yet also triggering the necessary changes. Future academics finding their way under the assumption of diversity and bias has an unmountable level of layers, and it is hard to evaluate what to concretely suggest on how to deal with the system.\nHaving a dedicated and committed group of people who have experiences on a specific issue or topic is again pivotal, because members of such an in-group can learn tricks on how to deal with the systemic failures. If you know the pressure points and highlight the demands within the respective situation, every institution will increasingly try to implement change for the better. While there is still a long way to go, it is surely worthwhile to team up, share the burden, and drive change. However, there is one point about diversity that is also highly relevant for academic institutions. Having a minority perspective can be a strong motivation to drive change. Deep emotional experiences can be \u2013 as sad as they are in a personal hemisphere of experience \u2013 be a driver to change the system away from underlying wrongs. If for instance someone has strong experiences about being bullied, then there is a chance that this person will not tolerate bullying in the future. If then again someone has a strong experience how majorities dominate minorities, then this person may draw conclusions that cut deeply into the status quo of science. Experiences about identity constructs may translate into more inclusive or deconstructive assumptions about constructs in science. Diversity breeds diversity, and being tolerant and inclusive will be ultimately the better way beyond any dimensions we all single-handedly could ever imagine. \n\n=== Work-Life-balance ===\n\nAcademia faces just like all work environments and increasing demand to balance the work life and the life after hours. Being part of the great acceleration, academia was not spared from becoming increasingly competitive over time, and thus pushing people into higher and more dense working hours. Psychological challenges are increasingly recognised in academia as well, both as a diagnosis and as an epidemic. Creating a balance is often proclaimed, but hardly achieved. In a system that is resource limited, the pressure that is being put on individuals is mounting, and people willing to make sacrifices are still supported by the clandestine and tacit design that is deeply engrained in academia. This creates many injustices that were increased by the pandemic, and we are at a point in time where we need to take stock and look not only at the bigger picture, but first and foremost at the demand and capabilities of the individual. While more work creates more output it should become critical how much output and change people can drive within the work time. The rest should be reserved for friends, family, or other elements to spend the time. The agency to thrive in your personality and to use the time to relax and reflect is increasingly recognised as pivotal, and a paradigm shift concerning work-life balance is slowly emerging. Yet there are very diverse approaches and models how people deal with time within academia. Most academics these days are completely cluttered and slaves of their calendar. There are no mincing words here, many people do not have free time for weeks. We became drowned in tasks, and often need to relearn how to prioritise. The system throws tasks at us scientists at a rate that has become hard to bear. There is more and more administration, safeguards and tasks that are means yet not ends. An academic today has a much more diverse portfolio of tasks as compared to decades ago. This creates a social Darwinist funnel that we need to escape from. It is a clear challenge not only of academia but also other spheres of society that strangely evolved into more control and less action. There are more parts of academic institutions that control us compared to the amount or capacity that can create action. This is rooted in good intentions, because the control tries to prevent injustices. Out of this urge, we created an even bigger injustice that punishes the commons in trying to safeguard the system. It is clear that \"safety first\" is a mantra to first and foremost protect minorities that face injustices, yet many of these cannot cope with the rising and mounting workload. For instance is it difficult to square the care work as a parent with the schedule and pressure of an academic career, to name just one challenge. To this day the border between public and private life in academia is deeply blurred, and there remain many open questions. While continuous education should be a mantra of academics, many do this in their private time. Our social circle is often an equivalent of the work folks that surround us all day, making is even more difficult to differentiate work from private life if we want to. Shifting work places due to the precarious hiring situation lead to a constant turnover of our social circles. Relationships have to catch up or compromise with a constant mobility. If you look to stay at the same place throughout your career, academia might not be your ideal turf. Continuity is then often designed to take place in the digital work. Social media and other digital skills are these days part of the standard portfolio of academics, yet these new media forms also led to an increasing crumbling of the of the border between work and private life. Being online 24/7 is technically not expected, yet while other branches of our work system try to carve out non-work time more clearly, academia is on the edge of always dragging you into the Maelstrom of more and more work all the time. Stringing boundaries may be a survival skill for many. I have a good friend who works strictly 9-5, and indeed very successfully. He just makes the time he does work really count, and uses the rest of his time to reboot and re-energise. Mapping out the border between academic work time and the rest of your life is a challenge, though. There are too many who love their work so much, that they literally never stop. I count myself among them. Others need time to rest and balance their day more strongly. This creates again injustices, but it is of course difficult to stop people from working who just love their work, and who would find it troubling to stop. After all it is really hard to not think about something if you are excited about it, and many academics are hyped by their work all the time. This creates a penalty for people who need explicit time to recover. This is a problem that is slowly changing, yet will create inequalities and injustices for decades to come. We need to be openly honest, legally clear, and aware of the normative dimensions of academic evaluation, otherwise we will not be able to change academia towards a better state.\n\n=== Institutions ===\n\nAcademia is a funnel which starts extremely broad and becomes more and more restrictive. There are for instance almost three million students in Germany right now, a number which was rising sharply over the last decades. While there are about 250000 Bachelor degrees per year in Germany, there are way less Master degrees (ca 150000). A fraction of those go on for a PhD, and also external students join German academia. There are about 29000 PhD degrees per year. Overall, there are about a total of 50000 professors in Germany, and considering age and retirement structure, there are professor positions for less than 20 % of the PhD students, and in some disciplines certainly way less. This pyramid structure of the different hierarchical levels of academia brings several challenges with it. First, students feel next to invisible for most professors. Given that there is one professor for about 60 students at an average, it is clear that this notion is more often than not a reality. While this can be argued in a resource limited system, there is an increasing demand to more deeply recognise the demands of the students. 3 hour non-stop lectures in the form of full frontal lobotomies \u2013 which equal the professor talking non-stop without any interaction \u2013 are increasingly questioned. On the other end, students are increasingly driven by the hunt for credits and other optimisations of their CVs. It becomes a rising challenge to have a convincing CV that does not equal a CV of everybody else, which again links to a higher workload and associated inequalities. The grading system is clearly in dire need of reform, yet without any grades is it still a challenge to evaluate students. Creative collaborative formats such as group work are not fully explored yet, and many lectures still build on frontal learning with few reflexive or interactive elements. Worse still, all the learning is hardly contextualised concerning concrete goals and the associated challenges. For instance does academia need to seize a superior role concerning responsibility for and with society, yet this brings several challenges with it that are often ignored. Trying to drive change in a system that is often changing or even characterised by failures is difficult within the increasingly normative agenda of many scientific arenas. For instance is environmental science often a field that links science with activism, and this mutual link creates challenges for individuals that are widely ignored by academia to date. Yet also professors are framed in a form that can be seen critical, because most professors are still to this day a reflexive surface that is supposed to equal endless competence, ubiquitous knowledge and a capacity to be always failsafe. No person can live up to these standards, and the urge for perfectionism serves as breeding ground to reconsider the role of authority and authoritative knowledge in academia. Deep changes are needed to properly designate and relocate the role of leaders and educators in academia, and this process is slow and will need even more time.\n\nWhile much more can be reflected about academia, for better or worse, the above text is a mere starting point to reflect about the academic system and structures. It is clear that all constructed institutions are forever changing. Academia is defined by change and supposed to drive change. Knowledge evolves, and so do knowledge structures. The rise of Western science did however lead to a diminishing of other forms of knowledge. Thus, while disciplines evolved and become ever more branching into finer domains, whole other forms of knowledge were ignored or neglected. Academic structures need to decolonize and take all inequalities into account, better today than tomorrow. Future academics need to reflect and position themselves in order to become drivers and amplifiers of such change. On the other hand do future academics also need to recognise their limitations concerning which changes can be implemented in which time frame. If we try to drive change concerning aspects that are static, frustration may arise. For instance are the resource limitations in academia a fact of our current reality. I do not expect that we will get way more positions any day soon, although I would more than welcome this. Wanting to change this notion is if at all only possible in the long run. On the other end can several changes be in a shorter time window. For instance did we move the welcome lecture of new professors into the afternoon, allowing people with family to attend, who before could not make such gatherings if these were in the evening. While such changes seem minute yet obvious, they represent important shifts in the wider landscape that designs and locates academia. Learning the difference between short-term changes and long-term changes is a key goal in the reflection of how you locate yourself in academia. Which aspects do you want to change? Where do you believe your energy is worthwhile to drive fast change? Where you can focus your limited energy and resources? There are often people who try to throw all their might against injustice that we have to change in the long run. It is up for everybody on their own to evaluate this, yet many negative emotions yield whenever unchangeable or at least only long-term changeable challenges are attacked head on, with an expectation to change theses now. All perceived injustices should be always called out and if identified attacked head on. Yet we need to be gentle with our head, and make sure we do not endanger ourselves in this process. I invite you all al sit down and reflect:\n\n1.\tWhat do you want to change in the wider academia in the short term?<br>\n2.\tWhat do you want change in academia in the long run?<br>\n3.\tWhich challenges in academia \u2013 but also for yourself \u2013 do you consider to ignore for the time being?<br>\n4.\tWhich challenges do you ignore for now, but plan to engage later on?<br>\n\nAcademia is both place-based and non-physical at the same time. We always speak of norms, tacit knowledge and practices, mechanisms and processes that are part of the wider hemisphere that makes all of academia a non-physical entity. On the other end is academia being practiced and changed in places that have a clear location, and it is here that we need to understand and evolve our impact to begin with. We may be able to drive change in academic places, and if these prove their worth, the change will spill over to other places. In the long run, this may change academia as such. Take your time and consider what you want to change and what you chose to ignore or change later. Be mindful of your energy and capacity. No one needs more frustration or yet another overworked academic. Be the academic you want to see in the world."
                    },
                    "sha1": "omh6pcogw9x04ox9a15yp3s9wvr2mt3"
                }
            },
            {
                "title": "The Art of Hosting",
                "ns": "0",
                "id": "295",
                "revision": {
                    "id": "2268",
                    "parentid": "2209",
                    "timestamp": "2020-08-03T12:39:47Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "699",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== Why & When ==\n\n\n== Goal(s) ==\n\n\n== Getting started ==\n\n\n== Links & Further reading ==\n\n\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]"
                    },
                    "sha1": "oblotubjdk9361vixp3cfzwl228nk4i"
                }
            },
            {
                "title": "The Dos and Don'ts of meetings",
                "ns": "0",
                "id": "880",
                "revision": {
                    "id": "6181",
                    "parentid": "6180",
                    "timestamp": "2021-08-04T14:01:32Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15850",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || '''[[:Category:Productivity Tools|Productivity Tools]]''' || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || [[:Category:Team Size 30+|30+]]\n|}\n\n'''We drown in meetings.''' When the end of humanity comes, it will probably happen in a meeting. We humans are creatures that thrive through communication and collaboration, and meeting can - at least in theory - be prime places to identify goals, clear obstacles, and energise a team towards a joint goal. Instead, however, many of us cement ourselves into an endless track of useless meetings that are dull, aimless and numbing. Here, I will outline my perception why it came to this, and propose my attempts how to make it out of this conundrum.\n\n== 10 reasons why our meetings fail ==\n'''1) Meetings without an agenda or established norms.''' Meetings should either have a clear agenda or follow an established norm. Ideally, the agenda is either sent around before, or there is a central repository that contains the items to be discussed. Ideally, an agenda needs to be shared early enough to allow for a proper preparation. Within my role as a dean we established as a team a central document where everybody can add agenda items. This is to me really helpful as it is both empowering and also helps participants to prepare themselves for the meeting. More often than not, items can already be cleared beforehand, and an agenda helps structure a meeting. Fist, the group focuses on items that are relevant for everybody, and then make a smaller circle focusing on a subset of items. In an ideal world, each item should be solved in the end, and it should be clear from the beginning to everybody how this will be achieved.\n\n'''2) Unprepared Meetings.''' Unprepared meetings are not only due to a lack of an agenda, but also because participants did not clarify their role and manage their own expectations. If I decide to join a meeting I would ideally want to either trust the person leading the meeting, or alternatively prepare my own agenda. The latter is a strategy with which I actually made some good experience in the past. Writing your own agenda if an actual agenda is missing is a good move to ultimately turn a dull meeting into something that at least contributes to your own goals or to the parts of the institution you represent. I have great admiration for some specific colleagues who prepare diligently for meetings, which can make even the most minor or junior role a raving success.\n\n'''3) Wrong composition of people.''' Any given meeting will gain its dynamics out of the composition of participants. To me, the composition of team members in a meeting is so relevant since group identity is built over time. Resilience is built out of diversity, and it is quite important to enable people to have their say in a given meeting. Though some people are missing at some times - or may have colliding obligations - I came to the conclusion that in the long run, those people participating in meetings are the ones who build and propel the team spirit further.\n\n'''4) Lack of facilitation.''' Facilitation can be defined as a designed support structure by one or several people to foster constructive integration within a specific setting. Facilitation is currently somewhat of a holy grail in academia and beyond, and rightly so, as we were widely missing out on this dimension beforehand. Facilitation done right can be a form of art, and facilitation can be prove that a path can surely be more important than the actual goals. One could write a whole book on all aspects of facilitation, yet since it is experience based, the most important advice is to encourage people to try it out for themselves. While some are clearly naturals, other may struggle. Still, gathering this initial experience can help everybody understand the struggles and challenges of the role of a facilitator.\n\n'''6) Lack of balance.''' Lack of balance can manifest in many ways within meetings. Often, one or few people totally dominate a meeting. This can be ok if the goals are still achieved, and other participants do not feel disempowered. In my experience, it can however be quite a negative experience if a substantial amount of talking time is monopolised by one person. While this can be an indicator of some underlying problems in the team, it makes the meeting often a frustrating experience. Like commitment and facilitation, this is something where you need to build an identity as a group and openly reflect upon such a disbalance with the respective person afterwards. Another disbalance can be created if one agenda item dominates the meeting, which is often in coherence with one person dominating the scene. To this end, it is most important to highlight when you are not gaining some ground on the respective item, or you start going in circles. While this is a matter of experience and diplomacy, in my perception it is often these situations where you end to step in and suggest to have another - maybe smaller - meeting later to focus on the respective issue. Time in a meeting is shared time, and this should take the interests of all people present into account.\n\n'''7) Meetings are not work sessions.''' Lack of balance links to a common mistake we all face: when a meeting becomes a brainstorming session. This can be ok if a meeting is designated as such, yet more often than not it defeats the purpose of a meeting, and throws the time off balance, and creates an setting that is focused on few people. Brainstorming sessions are already hardly good in a large group and with a generous time budget, but many meetings are even larger, and have limited time available. This makes leadership and facilitation a core challenge. It should be already clarified before the meeting how the meeting will go down, or this can be manifested with a good [[Code of Conduct|code of conduct]] within a team. If it is part of the norms of a group what such meetings are made for and what should be avoided, then you can really gain some ground.\n\n'''8) Lack of a follow-up agenda.''' \nHaving clear action items at the end is another often forgotten cornerstone of any successful meeting. While this demands commitment and team spirit, I made very positive experience in a less hierarchical setting, as people do not only feel empowered, but also want to show their worth, especially but not exclusively if you include early career students. Who will do what, and until when? While this seems trivial, it is often moving to the background, and people tend to leave meetings and never think about what was being said or discussed. The best meetings I was in were always the ones that made me think about the next steps, and others as well. A meeting is an exchange of a current status of a a group's thought process, yet time goes on, and so should our thinking. Importantly, complex challenges will evolve over time, and it may take some time to come to conclusions that then translate into the next step. Often, a single meeting is insufficient to make progress, and it takes several meetings to become something good. By showing investment inbetween meetings you may show the participants how valuable the meeting time is for you, and thus value the other people's time best.\n\n'''9) Group identity voids.''' Building team spirit takes time. Put that in a fortune cookie! I remember how when I was young, I felt that these team meetings sometimes waste my time, yet people were patient with me, and in the midrun I learned that building a joint identity does not happen overnight. Especially the informal time spots before and after a meeting are the times that are most relevant. Being part of a larger institution with many diverse meetings can equally help you to establish your profile, and proves to other your value by contributing to the greater good. Not all meetings are sources of instant gratification, and we should always remember how the others feel. Within an ideal meeting, you become a mirror for the other people, and through your reflection they move the agenda forward together. In my experience, people understand your agenda and solutions best when they actually think they came up with them themselves.\n\n'''10) Dare to differ.''' There are quite some established norms and procedures when it comes to meetings, yet it is not always the case that all these things were well planned and designed, but often such settings just kind of happened. While I would always advise you to basically say nothing in a setting where you are new - unless you are leading the meeting - after some sufficient amount of time, maybe some ten meetings, you may want to discuss ideas to improve the setting if need be. Typically you should not raise this in the big group, but instead to the person that has the right ratio of open-mindedness and experience to consider your suggestion. If you are in the right group for you, things may change over time, yet never forget to be patient.\n\nPersonally, I believe that writing a [[Code of Conduct|code of conduct]] is the single most important step you may consider as a team to build a joint meeting culture. It is good to discuss these things within the group, agree on certain rules, certainly not too many, and then write these down. Nothing will bring you faster and more reliably to a good meeting culture. Always start with why you have the meeting, who is present, and what potential outcomes or goals are. I have one colleague who I greatly admire who starts every meeting like this, guides you gently but diligently through the meeting, and ultimately summarises all conclusions and next steps at the end, and sends you an e-mail about it as a last step. What a difference it would make if we would all find the strengths and stamina to do this!\n\n\n== Not all meetings are equal ==\nAnother core point worthwhile considering is the different types of meetings we have at our disposal. Most meetings are a handful or more of people, and these should have clear guidance structures, an unbiased facilitation, and a good time management. However, there are other types of exchanges that we should check out in order to differentiate from such ordinary meetings\n\n'''1) Meetings with two people'''\nThis setting is the most intense, personal, and focused type of meeting. It also has a great flexibility, which makes it tempting to get distracted, but can also be great to create a joint identity. Being a mentor, it can be great to structure your thoughts beforehand, and to identify tangible goals that you want to reach. Two-person meetings without a goal can often be a pain, and I am often very insistent towards myself to derive goals in such personal settings, if only for myself. Yet we also have to acknowledge that just like any other meeting, such exchanges may build over a longer timeline. Having regular meetings i.e, with PhD students can be a beacon of hope, and are often the most fun. Since such settings are deeply personal, as a mentor I feel you need to adapt to each and every single person. Still, honesty goes a long way. Pushing people out of their comfort zone can be hard, but necessary if you are thriving towards a continuous development. I am often taking notes afterwards, yet try not to come to obviously back to prevoious points in a follow up meeting. Another key point is that within such meeting you may want to focus your attention without any compromises, yet be also clear when it comes to expectations and the time frame. If someone goes into such a meeting with a challenged mindset, you may not shift that within one hour, but it can be good to make that visible, and reflect about it. Things take time.\n\n2) '''Brain storming sessions''' are an altogether different beast. Here, the timeframe should be more or less long, and you will not necessarily reach tangible goals, but only start to develop ideas and a follow-up structure. Different people act in different ways in such settings, thus adapting speed and making sure that the individual gains are shared is essential. More often than not I get so carried away in such a process that I need to remind myself to regularly check if the other people are still with me. Similarly is it important to clearly indicate if your are lost, or feel disconnected. Brainstorming\u00a0sessions are about trust and joint identity. A whiteboard is often the key tool, or maybe working in a shared document. I spent a lot of trial and error until I got my brainstorming session to work best for me, and this seems to be a reoccurring challenge among many people. Building experience to this end demands patience.\n\n3) '''Emergency calls''' can be a lifeline when people are at their wits' end. Within a team, establishing such safety nets that help people that are stuck in their mind can most rewarding, as it catches people when they are most desperate. Asking for help is - sadly - still difficult in our society, yet all the while so important. Often, it already helps to just get it all out to clarify your thought process. Within such situations, a healthy peer-network can be a lifesaver, and often builds on informal structures. Within a team, great care should be thus taken to leave no one behind, and to consciously seek out whether people are stuck or shifting into crisis. Remember that any team is only as strong as its weakest link.\n\n4) The last meeting type are '''informal meetings'''. The corona crisis exemplified how much we miss the casual exchange in the hall or at a coffee place, often without any goal whatsoever, where suddenly ideas begin to develop. Our team has informal games nights that are often gold for the team spirit at least for some people, and a recent barbecue after a long corona hiatus showed how much a joint evening can do for the team. If there is one part where we still have to learn the most, it is tacit knowledge. There are all sorts of informal meetings, and these are at the heart of institutional knowledge. I consider informal meetings to be most relevant of all, often the most fun, yet hardest to tame and just get right. We all know how it is to be in an informal meeting where do not want to be. Honesty and clear communication goes a long way to this end.\n\nI recognise within our team a great effort to make meetings worth our while. We often differentiate in our meetings how we worked regarding research, teaching and management. These are the three elements that allow us to have a more clear structure in our meetings. Also, I recognise that we have a strong set of norms where new people that join the team are introduced to. Over the last years, a lot of informal knowledge and experience accumulated, which made the meetings continuously better. We have weekly team meetings, project meetings, smaller meetings, informal exchanges, meetings with individual PhDs or Postdocs, mentoring sessions, writing day meetings, and so much more. \n\nI believe that what defines us humans the most is our interconnectedness, and how we want to explore this. The 21st century will be most focused on making a difference to this end, and I look forward to a future where the connections between us will be way more important than the differences that define our individual identities. I invite you to reconsider the way you meet people, and to try out new ways to facilitate meetings. I hope in a not too far future we will all be more versatile to this end, at least this is what I am thriving for.\n----\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "3tp8a7he0660tmklk3i9654m99als3c"
                }
            },
            {
                "title": "The future of statistics?",
                "ns": "0",
                "id": "820",
                "revision": {
                    "id": "6289",
                    "parentid": "5967",
                    "timestamp": "2021-08-22T14:16:33Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "comment": "/* Plausible validation criteria */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "31042",
                        "#text": "'''In short:''' This entry revolves around the future of statistics. Which issues need to be solved, and which role can and should statistical analyses play in science and society in the next years and decades? For more on statistics, please refer to this [[Statistics|overview page]].\n\n==What will be the future of statistics?==\nQuite some people apply statistics these days, and it has become a staple of many diverse branches within science. However, [[Limitations of Statistics|critical perspectives]] increased as well over the last decades, and the computer revolution and the vast increase of peer-reviewed publications triggered a diversification in statistics. Sadly, the critical perspective is often a more of an add-on or afterthought, or settles into universal rejection, but has hardly triggered a deeper critical reflection or even revolution within statistics and the people who apply it regularly. '''Because of this, the benefits of these critical perspectives are often restricted to some few branches of science, or to a level of theory of science, yet do not translate into the empirical knowledge production where statistics is still so pivotal.''' The increasing diversity of statistics as such has also increased the mess, as it led to a increasing loss of linguistic coherence in the diverse disciplines, and led to developments where knowledge production or the testing of theories is relying on a small canon of statistical methods. \n\nStatistics hence added to the ever-spiraling specialisation of the scientific disciplines, and contributed to the demise of the old kingdoms. This is reflected in the role that statistics have played [[History of Methods|in the 20th century and even before]], where the analytical view on numbers was a willing accomplice to the injustices and inequalities of human thriving. The current culture wars are a reflection of these same unsolved problems where statistics contributed to, yet there is hope for the future. Statistics may be able to contribute to a better world for all people, but we need to overcome several struggles that are within the branch of statistics itself, within science, and in the contribution that statistics through science can have [[Scientific methods and societal paradigms|towards society.]] \n\nIn this text, I will propose my current view on how statistics may evolve in the future, how statistics may grow up into something better, and how it shall overcome the haunting  problems it faces today. This text will thus first look at the future developments within statistics, trying to anticipate what could happen to evolve the arena of statistics. The second section will critically examine the role of statistics across disciplines, and which problems may be solved in the future. The last section will paint a picture of the future contributions of an evolved arena of statistics and its role in society. It is in the nature of this endeavour that this text is not only offering a highly subjective [[Glossary|vision]], but also a bold one. However, since next to no one dares to dream about the future of statistics, I shall give it a try.\n\n== The future of statistics ==\n===Plausible validation criteria===\nThe question I get asked the most as a statistician is: '''How much does the model explain, and is that good or bad?''' Of course this is a loaded question that cannot be answered. Imagine the following example. You are terminally ill, and the doctors told you to settle your affairs. Suddenly a cure for your condition is discovered, but it only cures 10% of all patients. Do you take the shot? Sure you will. Now imagine you are in a different setting. You are perfectly healthy while planning your travels to the town of Samarra. Suddenly, Death knocks on your door, and tells you that you have a 10% chance of dying if you travel to Samarra. Would you follow through with your travel plans? Most certainly not. Yet one time you have a slim chance of survival, and the other time you have a slim chance of dying, which is also at 10 %. This highlights that the validation criterion of [[A matter of probability|probability]] is context-dependent. Thus, instead of focussing on absolute criteria, relative context-dependent criteria seem more helpful. While [[Bayesian Inference|Bayesian statistics]] would in theory bring us much closer to such a relative measure, it still relies on validation criteria familiar from frequentist statistics. Still, Bayes theorem may pose an answer for a relative validation criteria. Unfortunately, this would demand everybody to understand Bayesian statistics, which is where we may talk about the rather distant future.\n\n===Parsimonious models===\nIn statistics, today, there is a tendency to use more [[Agency, Complexity and Emergence|complex]] models as a sells pitch, as well as too simple models due to lack of experience. In the long run however, knowledge and experience may spread, and people may use the most appropriate models. '''I think a starting point for this will be pursuing different models at the same time, not in an ensemble way where the model results are put together and averaged, but in a reflexive way.''' Imagine I see a weather development that may have ramifications for the nomads in an area, being a foreshadow of a drought that may affect their livestock. I could not use [[Regression Analysis|simple linear models]], which would not help me to grapple such an extreme event, but would allow me to see some long-term trend statistics and get some explanation of what might happen in terms of the big picture. Then I might focus with a finer grain on the actual weather data right now, comparing climate and weather in a nested way, using non-linear statistics to focus on the actual prediction. Next, I could ask myself what happens if I take a fresh look, using a Bayesian approach to allow myself to only grapple the current data with real time updates. All these models unravel different aspects of the same phenomena, yet today -more often than not - statistics decide for one way of analysis, or combine all ways (e.g. ensemble models) as an average. Yet I propose that much knowledge is like an onion, and [[Statistics and mixed methods|different statistical models]] can help us to peel of the different layers.\n\n===Examine plausible questions===\nMuch of the knowledge that science produces is driven by scientific demand, and this can be a good thing. However, our growth-driven economy shows the catastrophic shortcoming of a line of thinking that focuses on one aspect of knowledge, all the while actively rejecting or denying other parts of knowledge or reality. '''Empirical knowledge depends on the questions we ask.''' A manager of an organisation that attempts to maximise their profits may or may not ask the right questions for their goal, as focussing on growth may be the main reason for the organisation's demise or prosperity in the future. To the manager, focusing on growth seems plausible, so they pursue it. If it is reasonable, they might misjudge. Plausibility is closely linked by definition with people being reasonable, and plausible questions have some probability of being worthwhile our investigation. Following Larry Laudan, however, it is clear that scientists of the past were not always reasonable. Larry Laudan thought that many events and discoveries in the history of science were not as rational or reasonable as we think these scientific discoveries were. Future changes such as a more tightly-knit exchange culture in science or more pronounced ethical checks of research proposals may foster more plausible research questions, but this may not the right way to more plausible research questions. Instead, society may shift in the future, and people may just not come to the conclusion to investigate questions that are implausible. This may sound like a very distant future, but if we compare us today to people or even researchers 100 years ago or even 500 year ago, it become clear that we already came a long way, and shall go on.\n\n===Integrate more & diverse data===\nStatistics was born out of the urge to understand and control system dynamics, partly with a purely non-materialistic focus such as in Astronomy - a motor of early statistics - or with a total focus on materialistic gain, as was the case in early econometrics. With the rise of the Internet, our inflict of data increased at an exponential pace - following Moore's law - triggering both positive but also devastating effect on the global dynamics we witness today. '''Access to information has become on of the most important privilege people have today, and much has been gained, but much was also lost in translation.''' Examples such as information spreading from activists showcase what can be gained, yet terrorists and insurgents are coordinating themselves equally in the digital age. Data security is stronger shifting into the focus of societal debate, but we are far away from real data security, and in many aspects seemingly only a step away from a Black Mirror episode. All the while is is clear that we have no idea of which [[To Rule And To Measure|types of data]] may be available in the future. Movement patterns of elderly risk patients can predict a risk of a heart attack weeks before it would actually happen. Research can trigger a tilt towards fairer income distribution, or support the notion of a universal basic income. Detection systems of weather, oceans and the upper crust of Earth may prevent countless losses of life in in case of disasters. However, all these examples are still connected to our reality today, and we have to assume that future people will utilise data in ways that will be hard to imagine for us today.\n\n===Tame complexity===\n[[Agency, Complexity and Emergence|Complexity]] is almost like a holy grail in terms of current debates about system dynamics, because many use complexity either as an argument why change might happen, or to underline our limitations in understanding system dynamics. I believe that both lines of thinking are capital mistakes, as they may lead to false argumentations or to inaction in investigating system dynamics. What is complex for us may be simple to future people. Also, while there will most likely always be dynamics or patterns that we do not understand, in the future we may have mapped out which dynamics can be understood, and which dynamics follow other principles such as chaos theory, and can thus not be predicted. '''Complexity is not an ontological truth, but a buzzword that frames our current limitations.''' Once we have mapped out what we cannot understand, we will be much better able to act within the remaining rest of reality. Many people frame complexity and knowledge about it right now as a privilege, which they have access to, but other people do not. I can declare something to be complex, and thus mark this pattern to be not understandable. A good example are weather dynamics, which beyond a certain time window become unpredictable. While our current thinking dictates us that this will always be the case, we are different people today than we were once more 100 years ago. We have weather forecasts, ensemble models, satellites in orbit, and many means to at least predict the weather for the next 1-2 weeks, which would have been unthinkable at a planetary scale some centuries ago.\n\n===Solve causality===\nThe gap between [[Causality and correlation|causal links]] and predictive pattern detection is one of the largest missed opportunity of statistics to establish a proper link to philosophy. While many philosophers such as Hume explored matters of causality deeply, this is still widely ignored in large parts of the everyday life of research. The unlocking of [[Machine Learning|machine learning]] and associated analytics even led to an outright rejection of any causal link, focussing on prediction instead. This can be a good thing, as a life saved through a good prediction is surely better than a life being lost, yet a post mortem helps us to understand why the person died. Anyone would surely prefer a life based on a blur prediction, compared to an explainable death. Mapping out this difference is of pivotal importance, because prediction can inform us so that we may decide how to act, yet causal explanation can help us understand why we should act. '''Causality is thus key to decisions and intentional acts.''' Without causality, we may be doomed to act based on our external senses alone, or on a prediction of some machine, yet it is our reflection based on causal information that may give our actions meaning and reason. It is part of human nature that some of our actions may always be not explainable, and it is part of statistics that some patterns and mechanisms will never be explainable. Still, a proper embedding of causal information and how we define may give us reasons to act, and not only our actions matter, as well as reason alone does not matter, but the combination of acting reasonable based on causal information may matter deeply.\n\n\n==The future of statistics within science==\n===Solve theory of science===\nUntangling the crisis of Western science for good is like the moonshot on this list. The perceived [[Bias in statistics|ghost of positivism]] that is still haunting us to this day, and the counter-revolutionary movements that emerged out of it triggered a division that left much of current research still being stuck in an illusion of objectivism, while some are lost in their maze of universal rejection or critical reflection. '''Critical realism with its subjective view of scientific knowledge, and its possibility for ontological truths still being out there, may have solved the current dilemmas of theory of science.''' Unfortunately, most researcher are not aware of this, or reject or ignore it. The errors of the past, and the [[Bias in statistics|biases]] these errors create inside of us as individuals as well as within the scientific community deserve a critical perspective on science. Equally, we need to create knowledge to continue the path of this human civilization, since we unleashed many wicked problems that need to be solved. Otherwise all may be in vain, and science needs to acknowledge that. Statistics is probably one of the branches of science that is furthest away from critical realism, yet if we change our education systems to enable a reflexive humanism as a baseline for our education, I cannot see why critical realism should not spread, and ultimately prevail. From a current viewpoint, it looks like our best ticket to the moon, and beyond.\n\n===Establish postdisciplinary freedom===\nScientific disciplines are a testimony of the oppressive evolving of science out of the [[History of Methods|Enlightenment]], leading to silos of knowledge that we call scientific discipline. '''While it is clear that this minimises the chances of a more holistic knowledge production, scientific disciplines are still necessary from a perspective of depth of knowledge.''' Medicine is a good example where most researchers are highly specialised, because there is hardly any other way to contribute to the continuous evolution of knowledge. We may thus conclude that focus in itself is necessary, and often helpful. There are however also other factors about the existence of scientific disciplines that are important to raise. First of all, scientific disciplines are in a fight about priorities of knowledge and interpretation. Many disciplines claim that their knowledge is indeed of a higher value than the knowledge of the other discipline. It is clear that this notion needs to be rejected once we take a step back and look at the whole picture, since such claims of superiority do not make any sense. Yet from a perspective of [[Bias and Critical Thinking|critical realism]], one could claim that ethics and maybe even philosophy are on a different level, because the can transcend epistemological perspective, and may even create ontological truths. While other disciplines thus vanish in the future, philosophy, and more importantly [[Ethics and Statistics|ethics]], are about our responsibility as researchers, and may thus play a pivotal role. I would propose that statistics could contribute to this end, because statistics is at its heart not disciplinary. Instead, statistics could provide a reflexive link between different domains of knowledge, despite it being almost in an opposite position today, since statistics is often the methodological dogma of many scientific disciplines.\n\n===Clarify the role of theory===\nStatistics today is stuck between a rock and a hard place. Statistics can help to test hypotheses, leading to a accepting or rejection of our questions that are rooted in our theories, making [[:Category:Deductive|deductive]] research often rigid and incremental. At the extreme opposite end, there is the [[:Category:Inductive|inductive]] line of thinking, which claims an open mind independent of theory, yet often still looks at the world through the lens of a theoretical foundation. '''Science builds on theory, yet the same theories can also lock us into a partial view of the world.''' This is not necessarily bad, yet the divide between inductive and deductive approaches has been haunting statistics just as many other branches of science. Some approaches in statistics almost entirely depend on deductive thinking, such as the [[ANOVA]]. Other approaches such as [[Clustering Methods|cluster analysis]] are widely inductive. However, all these different analyses can be used both in inductive and deductive fashion, and indeed they are. No wonder that statistics created great confusion. The ANOVA for example was a breakthrough in psychological research, yet the failure to reproduce many experiments highlights the limitations of the theories that are being pursued. Equal challenges can be raised for ecology, economy, and many other empirical branches of science. Only when we understand that our diverse theories offer mere partial explanations, shall these theories be settled in their proper places.\n\n===Reduce and reflect bias===\n'''[[Bias and Critical Thinking|Bias]] has been haunting research from the very beginning, because all humans are biased.''' Statistics has learned to quantify or even overcome some biases, for instance the one related to sampling or analysis are increasingly tamed. However, there are many more biases out there, and to this day most branches of science only had a rather singular focus on biases. In the future we may pool our knowledge and build on wider experience, and may learn to better reflect our biases, and through transparency and open communication, we may thus reduce them. It seems more than unclear how we will do this, but much is to be gained.\n\n===Allow for comparability===\n'''How can we compare different dimensions of knowledge?''' To give an example, how much worth in coin is courage? Or my future happiness? Can such things be compared, and evaluated? Derek Parfit wrote that we are irrational in the way how we value the distant future less as compared to the presence, even if we take the likelihood of this distant future becoming a reality into account. This phenomenon is called temporal discounting. Humans are strangely incapable of such comparisons, yet statistics have opened a door into a comparability that allows to unravel a new understanding of the comparisons in our head with other comparisons, or in other words, to contextualise our perspectives. Temporal discounting is already today playing less of a role because of teleconnections and global market chains. What would however be more important, is if people gained - through statistics - a deeper insight into their existence compared to everybody else. Such a radical contextualisation of ourselves would surely change our perspective on our role in the world.\n\n===Evolve information theory===\nWhile frequentist statistics evolve around [[A matter of probability|probability]], there are other ways to calculate the value of models. Information theory is - in a nutshell - already focusing on diverse approaches to evaluate information gained through statistical analysis. The shortcoming of [[Simple Statistical Tests|p-values]] have been increasingly moved in the focus during the last one or two decades, yet we are far away from alternative approaches (e.g. AIC) being either established or accepted. Instead, statistics are scattered when it comes to analysis pathways, and model reduction is currently at least in the everyday application of statistics still closer to philosophy than our way of conducting statistics. The 20th century was somewhat reigned by numbers, and probability was what more often than not helped to evaluate the numbers. New approaches are emerging, and probability and other measures may be part of the curriculum of high school students in the future, leaving the more advanced stuff that we have no idea about today to higher education.\n\n\n==The future contribution of statistics to society==\n===Generate societal education===\nIt is highly likely to assume that even advanced statistics may become part of the education of young schoolchildren. After all, today's curriculum is vastly different from what was taught 100 years ago. Statistics could be one stepping stone towards a world with a higher level of reflection, where more and more people can make up their own mind based on the data, and can take reflected decisions based on the information available. '''Inequalities can only be diminished if they are visible, and statistics are one viewpoint that can help to this end, not as a universal answer, but as part of the picture.''' The COVID-19 pandemic has shown how the demand for data, patterns and mechanism went through the roof, and more people got into the data and analysis, and acted accordingly - given that they had the capability. The greatest opportunity of a more dispersed statistical education is surely within other cultures. While Europe and North America are widely governed by knowledge gained from statistics, much could be learned ensuring these approaches with different knowledge from other cultures. We only started to unravel the diversities of knowledge that is out there. Statistics may also be a staple in the future, yet knowledge become more exciting if it is combined with other knowledge.\n\n===Tell stories===\nIn order to become able to meaningfully contribute to a societal as well as cross-cultural dialogue and joined learning, statistics would need to learn better ways to tell stories. '''Ultimately, it is joint stories that create identity and unite people.''' We should not be foolish and consider a so-called \"objective\" worldview propelled by statistics to be a meaningful goal, because this would surely deprive us of many dimensions of cultural diversity. Instead, statistics needs to emerge into an arena that can tell stories through data, and engage people, and thus help to create identities that are reflexive, immersive and ultimately aimed at understanding different perspectives. If statistics became less about privileged knowledge and instead take the understanding of all people as a baseline, we would have an ambitious yet tangible long-term goal.\n\n===Qualify statistical results===\nRight now, the general understanding of statistics is vastly different. '''Few people actually understand statistics, and often this is the small privileged group being able to get a higher education.''' If statistics is presented, then most people do not understand it, or understand only parts. Gaining a broader understanding about the different valuations of statistical results is hardly ever done in a societal debate, hence citizens rely on the translation and condensation by the media. In the future, qualifying statements that allow for a contextual understanding of statistics shall be commonplace, and more efforts need to go into building a broader understanding what this actually means. Selective presentation of statistics will be a mistake of the past.\n\n===End information wars===\nSadly, the selective presentation of statistics is part of our current reality. The media, politicians and other institutions present statistics often in a partial form to literally sell their view of the world. More often than not, a look at the whole data or other sources reveals a different picture - not just in nuances, but altogether different and sometimes even reversed. '''Much of the information that is ultimately presented publicly is designed to be palatable and to divide people.''' While there is surely a lot of good journalism already out there, much information is wrong, and political wars are waged about selling information, and how to utilise the partial realities for a political agenda. The Internet, which was originally set to realise the vision of global knowledge exchange, became a breeding ground for the great divide we currently face. We need to return to the vision of a free information flow, and educate people, and allow them to form their own picture, maybe even without the media as mediators.\n\n===Overcome resource fights===\nMuch of the current conflicts are about resources, albeit often indirectly. Statistics enables or fuels these conflicts, as it allows for comparisons that ultimately have a utilitarianism purpose, often for an in-group, thereby excluding other people that are designated as out-groups. Hence statistics are - besides cultural identities - one of the origin points of current conflicts. This may change in the future. Resources may become less sparse, i.e. through a different harnessing of energy. If we were not limited by energy constraints, many conflicts might be overcome. Equally, future societies may avoid singular aims of materialistic values, consumption and growth-driven economies. This would tilt the way statistics is embedded into society, yet would certainly make it not less important. '''Instead, statistics would shift to brighten our knowledge, instead of fueling inequalities.'''\n\n===Link people to analysis===\nFundamental for overcoming inequalities between different people would be a different mode of transparency when it comes to data security, as well as the way people demand data, depend on it, and ultimately utilise it. '''''Power to the people'' is from this viewpoint not enough: we also need to give data and means of analysis to the people.''' Consequently, people also need to raise the question which data they need to reflect and take decisions. Which data is being created is right now decided by a small elite of people, thereby excluding the vast majority of people. This could be changed already now, since more data is being created by the people thanks to smartphones, the Internet and other technology. This data should however not be used to manipulate people, but to empower them. Incredible amounts of data can be expected in the future, and science needs to work with citizens to focus on the data that is most needed to overcome the problems we may still face, and the solutions we need.\n\n===Clarify limitations===\nMuch of the bad reputation within statistics is because it is hardly explained what statistics cannot do. Today, much emphasis in the media is put on what statistics can do, and remarkably often, the media does not get the point. To this end, limitations are not about individual statistics and results, but are about [[Limitations of Statistics|statistics in general]]. We know for instance that some statistical tests and models reveal different results than other models. All models are wrong, some models are useful. '''Society needs to understand and learn the value of statistics, but also the limitations.''' Otherwise, populistic leaders will continue to devalue statistics with clever selective criticism rooted in the fear of some voters. It is in the nature of science that scientific results change, and nothing else can be expected from statistics.\n\n===Integrate data continuity===\nChanging statistical results may be one of the greatest changes we shall see in the future. Up until the recent past, much of statistical results that are presented are temporal slices. However, there is an increasing temporal continuity being merged by dashboards in the Internet, leading to ever-changing results, which often gain in precision. '''Constant updates in statistical data and analysis will in the future enable us better and better to react to changes and anticipate challenges.''' Through an integrated data continuity, we will finally come to a view that does justice to the world, with little more fix points beside our cultural identity and our moral principles. Anything beyond this may be a constant fluctuation of data, which is nothing but a testimony of the interconnectedness of all people. This will not only enable new information age, but ultimately lead to a different dominion of insights, where we leave much of the static worldview that still closes in on us, and even traps us, today. Instead, we will become relative dots of information in space and time, where the interconnections between us are what ultimately matters, and we are able to see and feel this more than ever before.\n\n\n==Epilogue==\nMuch has been gained, yet some say that much is lost. Derek Parfit claimed that the future could be wonderful, and I would agree. We may - to follow Dereks line of thinking- not be able to imagine how future humans might be, act and think. '''However, it is hard to imagine a future of humanity where numbers do not matter at all.''' For all I care, they may matter less - proportionally - but will still be important. \n\nTo me, there is another aspect to it. '''Statistics could even contribute to make us more free.''' If you make hundreds of models that all show you patterns and mechanisms of diverse data sets, yet also show you that there are always limitations, and there will always be unexplained variance, this may even be able tilt your worldview. Seeing order in the chaos, finding patterns, unlocking mechanisms, and ultimately also mapping the limitations of our knowledge may be a step from knowledge to experience that can change us as a people, at least this is my naive hope. Many aspects I raised here seem to be unrealistic, or at least far-fetched. To this end, I would always consider how we were 100 years ago. What did we know about statistics? What about reason? What was causal knowledge back then? And how was the situation some 500 years ago? [[History of Methods|Or 2000 years ago?]] We have changed dramatically over the past decades and centuries, and out trajectory of change is on an exponential path right now. Who knows how it will go on, but I will always settle for hope. I am all with Martha Nussbaum on this one: \u201cHope really is a choice, and a practical habit.\u201d\n\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "2iydj9uo46x7fxeegnf7bew9dnbifa1"
                }
            },
            {
                "title": "The great statistics recap",
                "ns": "0",
                "id": "827",
                "revision": {
                    "id": "6527",
                    "parentid": "5966",
                    "timestamp": "2022-02-19T11:47:58Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "comment": "/* How to go on */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14581",
                        "#text": "'''Annotation:''' This entry concludes the Introductory class on statistics for Bachelor students at Leuphana. It is not an autonomous entry on its own.\n\n==The great recap==\nWithin this module, we focused on learning simple statistics. '''Understanding basic [[Descriptive statistics|descriptive statistics]] allows us to calculate averages, sums and many other measures that help us grasp the essentials about certain data.''' [[Data formats]] are essential in order to understand the diverse forms that data can take. We learned that all data is constructed, which becomes most apparent when looking at [[To Rule And To Measure|indicators]] which can tell some story, yet without deeper knowledge about the construction - i.e., the context of an indicator - it is hard to grasp. \n\nOnce you get a hold of the diverse data formats, you can see then how data can represent different [[Data distribution|distributions]]. While much quantitative data is normally distributed, there are also exceptions, such as discrete data of phenomena that can be counted that is often showing a skewed distribution. Within frequentists statistics, statistical distributions are key, because these allow form a statistical standpoint to [[Experiments and Hypothesis Testing|test hypotheses]]. '''You assume that data follows a certain distribution, and that is often one important preconditions for the test or model you want to conduct.''' Whether your data then shows non-random patterns, but whether a hypothesis can actually be accepted or rejected, depends actually more often than not on the p-value. This value is the calculation whether your results are random and follow mere chance, of whether there is a significant pattern that you tested for. This is at its core what frequentist statistics are all about. \n\nThe most [[Simple Statistical Tests|simple tests]] can test for counts of groups within two variables (chi-square test), comparisons of two distributions (f-test) and the comparison of the mean values of two samples of a variables (t-test). Other tests avoid the question of statistical distribution by breaking the data into ranks, which are however less often applied (Wilcoxon test). '''A breakthrough in statistics was the development of the [[Correlations|correlation]], which allows to test whether two continuous datasets are meaningfully related.''' If one of the variables increases, the other variable increases as well, which would be a positive correlation. If one variable increases, and the other one decreases, we speak of a negative correlation. The strength of this relation is summarised by the correlation coefficient, which ranges from -1 to 1, and a values furthest from 0 indicates a strong relation, while 0 basically indicates complete randomness in the relation. This is tested again by p-values, where once more a values smaller than 0.05 indicates a non-random relation, which in statistics is called a significant relation. \n\nWhile correlation opened Pandoras box of statistics, it also raised a great confusion concerning the question whether a relation is [[Causality and correlation|causal]] or not. There are clear criteria that indicate causality, such as similarity in features of phenomena that have the same effect onto a variable. '''In order to statistically test for causal relations, [[Regression Analysis|regressions]] were developed.''' Regressions check for relations between variables, but revolve around a logical connection between these variables to allow for causal inferences. In addition, they allow to test not only the relation of one continuous variable in relation to another dependent variable. Instead, several independent variables can be tested, thus allowing to build more complex models and test more advanced hypotheses. Again, the relation is indicated to be significant by the p-value. However, the strength of the model is not measured in a coefficient, but instead in the r-square value, which is the sum of squares of the individual data points distance from the regression line. A regression line is hence the line that represents the regression model, which best explains the relation between the dependent and independent variable(s). \n\nWhile regressions were originally designed to test for clear hypotheses, these models are today utilised under diverse contexts, even in [[:Category:Inductive|inductive]] research, thereby creating tensions when it comes to the interpretation of the model results. A significant regression does not necessarily indicate a causal relation. This is a matter of the [[Normativity of Methods|normativity]] of the respective branch within science, and ultimately, also a question of philosophy of science. This is comparable to the [[ANOVA|analysis of variance]] (ANOVA), which unleashed the potential to conduct [[experiments]], starting in agricultural research, yet quickly finding its way into psychology, biology, medicine and many other areas in science. '''The ANOVA allows to compare several groups in terms of their mean values, and even to test for interaction between different independent variables.''' The strength of the model can be approximated by the amount of explained variance, and the p-value indicates whether the different groups within the independent variables differ overall. One can however also test whether one groups differs from another groups, thus comparing all groups individually by means of a posthoc test (e.g. Tukey).\n\nWhen designing an ANOVA study, great care needs to be taken to have sufficient samples to allow for a critical interpretation of the results. Subsequently, ANOVA experiments became more complex, combining several independent variables and also allowing to correct for so called random factors, which are elements for which the variance is calculated out of the ANOVA model. This allows for instance to increase the sample size to minimise the effects of the variance in an agricultural experiment which is being conducted on several agricultural fields. In this example, agricultural fields are then included as block factor, which allows to minimise the variance inferred by these replications. Hence, the variance of the agricultural fields is tamed by a higher number of replicates. This led to the ANOVA becoming one of the most relevant methods in statistics, yet recent developments such as the reproducibility crisis in psychology highlight that care needs to be taken to not overplay ones hand. Preregistering hypotheses and more recognition of the [[Limitations of Statistics|limitations]] of such designs currently pave a path towards a more critical future of statistical designs. \n\nAnother development that emerged during the last decades is the conducting of so called real-world experiments, which are often singular case studies with interventions, yet typically less or no control of variables. These approaches are slowly being developed in diverse branches of research, and allow to open a [[Meta-Analysis|meta-analytical]] dimension, where a high number of case studies is averaged in terms of the research results. The combination of different studies enables a different perspective, yet currently such approaches are either restricted to rigid clinical trials or to meta-analyses with more variables than cases. \n\nReal-world experiments are thus slowly emerging to bridge experimental rigour with the often perceived messiness of the problems we face and how we engage with them as researchers, knowing that one key answer involving these is the joint learning together with stakeholders. This development may allow us to move one step further in current [[System Thinking & Causal Loop Diagrams|systems thinking]], where still many phenomena we cannot explained are simply labeled as complex. We will have to acknowledge in the future which phenomena we may begin to understand in the future, and which phenomena we may never be able to fully understand. [[Non-equilibrium dynamics|Non-equilibrium theory]] is an example where unpredictable dynamics can still be approaches by a scientific theory. Chaos theory is another example, where it is clear that we may not be able to grasp the dynamics we investigate in a statistical sense, yet we may be able to label dynamics as chaotic and allow a better understanding of our own limitations. Complexity is somewhat inbetween, leaning partly towards the explainable, yet also having stakes in the unexplainable dynamics we face. '''Statistics is thus at a crossroad, since we face the limitations of our approaches, and have to become better in taking these into account.''' \n\nWithin statistics, new approches are rapidly emerging, yet to date the dominion of scientific disciplines still haunts our ability to apply the most parsimonious model. Instead, the norms of our respective discipline still override our ability to acknowledge not only our limitations, but also the diverse biases we face as statisticians, scientists and as a people. Civil society is often still puzzled how to make sense of our contributions that originate in statistics, and we have to become better in contextualising statistical results, and translate the consequences of these to other people. '''To date, there is a huge gap between [[Ethics and Statistics|statistics and ethics,]] and the 20th century has proven that a perspective restricted to numbers will not suffice, but instead may contribute to our demise.''' We need to find ways to not only create statistical results, but also face the responsibility of the consequences of such analyses and interpretations. In the future, more people may be able to approximate knowlegde though statistics, and to be equally able to act based on this knowledge in a reasonable sense, bridging societal demands with our capacity for change. \n\n\n==What was missing==\nEverybody who actively participated in this module now has a glimpse of what statistics is all about. I like to joke that if statistics is like the iceberg that sank the Titanic, then you now have enough ice for a Gin-Tonic, and you should enjoy that. The colleagues I admire for their skills in terms of statistics spent several thousand hours of their life on statistics, some even tens of thousands of hours. By comparison, this module encapsulates about 150 hours, at least according to the overall plan. Therefore, this module focuses on knowledge. It does not include the advanced statistics that demand experience. Questions of models reductions, [[Mixed Effect Models|mixed effect models]], [[An_initial_path_towards_statistical_analysis#Multivariate_statistics|multivariate statistics]] and many other approaches were never touched upon, because this would have been simply too much. \n\n'''In itself, this whole module is already daring endeavour, and you are very brave that you made it through.''' We never had such a course when we were students. We learned how to calculate a mean value, or how to make a t-test. That was basically it. Hence this course is designed to be a challenge, but it is also supposed to give you enough of an overview to go on. Deep insights and realisation happen in your head. We gave you a head start, and gave you the tools to go on. '''Now it is up to you to apply the knowledge you have, to deepen it, transfer it into other contexts and applications, and thus move from knowledge to experience.''' Repetition and reflection forge true masters. Today, there are still too few people willing to spend enough time on statistics to become truly versatile in this arena of science. If you want to go elsewhere now, fine. You now learned enough to talk to experts in statistics, given that they are willing to talk to you. You gained data literacy. You can build bridges, the problems we face demand that we work in teams, and who knows what the future has in stock for you. \n\nNevertheless, maybe some of you want to go on, moving from apprenticeship to master level. Statistics is still an exciting, emerging arena, and there is much to be learned. One colleague of mine once said about me that I could basically \"smell what a dataset is all about\". I dare you to do better. I am sure that the level of expertise, skill and experience I gained is nothing but a stepping stone to deeper knowledge and more understanding, especially between all of us, regarding the interconnectedness of us all. '''I hope that all of you find a way how you can contribute best, and maybe some of you want to give statistics a try.''' If so, then the next section is for you.\n\n\n==How to go on==\n'''Practise, practise, practise.''' One path towards gaining experience is by analysing any given dataset I could get my hands on. Granted, in the past this was still possible because the Internet was not overflowing with data, yet still there is surely enough out there to spend time with, and learn from it. The Internet is full of a lot of information on statistics. Not all of it is good, necessarily, yet all is surely worth checking out. After a few hundred hours of doing statistics you will realise that you develop certain instincts. However, in order to get there, I suggest you find some like-minded peers to move and develop together. I had some very patient colleagues/friends who were always there for a fruitful exchange, and we found our way into statistics together. It certainly helped to have a supervisor who was patient and experienced. Yet one of the biggest benefits I had was that I could play with other peoples data. If you become engaged in statistics, people will seek your advise, and will ask you for help when it comes to the analysis of their data. The diversity of datasets was one of the biggest opportunities for learning I had, and this goes on up until today. Having a large diversity of data rooted in the actual experience of other people can be ideal to build experience yourself. \n\nIf I can give you one last advice: There is certainly still a shortage of people having experience in statistics, hence it may be this skill that may allow you to contribute to the bigger picture later in life. What I am however most certain about that there is next to no person that has practical experience in statistics and a deeper understanding about ethics. While this may be the road less taken, I personally think it may be one of the most important ones we ever faced. I might be biased, though. In the end, I started this Wiki not only to overcome this bias, but to invite others on the road less taken.\n\n----\n[[Category:Statistics]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "12nkoojbc03745jgo5kmmvjlralvx4w"
                }
            },
            {
                "title": "The tao of R-coding",
                "ns": "0",
                "id": "500",
                "revision": {
                    "id": "5889",
                    "parentid": "5888",
                    "timestamp": "2021-06-27T14:56:38Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Non-duality in R-coding */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "28351",
                        "#text": "'''In short:''' This entry provides guidance on R coding in general.\n\n== Pretext ==\nI learned from many, and many learned from me. Much of my alumni will question details that I write here, as they have different views on coding. I think this is a good thing. After all, you have to find your own meaning for the tao of R-coding. \n\n'''Learning to write code in R takes a long time'''. This text will not take this time away from you, but it may help you to find some shortcuts here and there, through which hopefully you may learn faster afterwards. I made many mistakes in my time, and I hope that this will go on, since otherwise there is nothing left to learn. Still, I am glad to say that I can recognise a difference between knowledge and experience, and this key difference is the main insight one can get into mastery. I believe, that once you felt bored out by an R-coding problem, this is the closest one can get to wisdom. Let us begin.\n\n== You in your surrounding ==\nFirst, the setting. Perfecting patience in R-coding demands the right surrounding. Some people cannot code everywhere, others can code everywhere. I think this is actually inaccurate; the latter are simply better in adapting, and would do their prime work in a well defined setting as well, but this is just my own experience. I thought I am very adaptive, but learned that I benefit from a defined setting as well.  \n\n==== Place ====\nYou need to code regularly, and a lot. Hence it is good to design a space where you can sit easily for hours to an end, and focus on your coding. You do not need the best laptop, but ideally one that is a tool that you feel comfortably with. There is the old Mac vs. a PC rant, and I guess this will never be solved. However, it is good if you can afford a computer that works well for you. Some people use a mouse and a keyboard, which can be beneficial. However, I advice you to use as few mouse clicks as possible. Try to rely on keyboard shortcuts instead, which are the easiest bits of code that you should learn first. Switching between windows, copy & paste, tab setting and shortcuts are the rudimentaries you need to master. In my experience, a second screen can get you a long way. For students, a Padlet can be a welcome extension of the screen, as it allows for more mobility. Many people can code everywhere, and this is an advantage. Therefore, it is so important to get a Laptop, and to have a a good backpack to allow for mobility and relocation. \n\n==== Time ====\n''\u201cThere is something to be learned from a rainstorm. When meeting with a sudden shower, you try not to get wet and run quickly along the road. But doing such things as passing under the eaves of houses, you still get wet. When you are resolved from the beginning, you will not be perplexed, though you will still get the same soaking. This understanding extends to everything.\u201d'' - Hagakure\n\n'''You need to code as much as you can.''' Still, some time spots seem to work better for some than other time spots. Find your sweet spot, and more importantly, practice! Coding should be a reward you look forward you, which interludes the other duties and activities of the day. Still, it needs to be a pronounced focus if you are really planning to get good at it. I have the experience that sometimes a short break can get you really further. In addition, planning your code beforehand in a rough form is really helpful. I feel that to this end, coding is just like any other form of writing. To conclude, find the best mode how to excercise, since only daily practice can allow you to evolve. Lean back, and start coding.\n\n==== The hardware ====\nWhen you learn programming, there is a high likelihood that you stay in one place for at least some years. If your budget permits, how about making a sedentary commitment: A second screen can make a real difference when programming, and using a screen as a main device and the laptop screen as a secondary screen can also be more comfortable on your eves and neck, as the position of the screen can be more upward. Some people are also faster on a keyboard, and it can be adjusted to different typing poses. Lastly, while you should minimise the use of a mouse or touchpad by all means, I prefer a touchpad, mostly because it is more agile in Finalcut. Overall, I think the specs of the computer matter less than people think they do. In my experience, you either do not wait at all for a calculation, or you wait very very long - if not too long - anyway. In this case, you anyway need to find an alternative server to calculate your stuff. In R, this hardly ever happens, at least while learning to code. '''If something calculates really long, in most cases you made a mistake.''' Only large data or endless loops can stall the average calculations. Bayesian stuff may pose a problem, and larger data. Hence try to avoid the latter, and brace for the Bayesian coffee break. \n\nWhere the hardware is related to the software is in the language settings. Macs and PCs differ in their language settings, and it is good to consider this difference when it comes to comma delimiters, number formats etc. These settings often have a severe influence onto Excel- and .csv-files, which is why it is wise to use number formats without any dots at 000 number groups, and to definitely use points and not commas. Also, switch your computer to English settings, which is a source of many errors. Please forget that any other letters outside of the english language exist, since this is a constant source of error. In addition, some Internet resources are a bit picky when it comes to the browser, hence using at least two browsers seems preferable, and Firefox has proven to be rather robust. \n\n\n== The software and settings ==\n\n''I believe that R base is the weapon of a Jedi. Not as clumsy or random as R Studio; an elegant weapon for a more civilised age.''\n\nThe bare minimum is to install R, though most people these days also install R Studio. It is also good to have a code file that automatically installs the most important packages that you may need. Slack or other chat software is essential to exchange code with others. However, remember that Social media can be a time sink. Many R programmers rely on [[Git_and_GitHub|Github]], Reddit and other such Internet forums. \n\nConsidering the organisation of your hard drive, make a long term structure. You will not remember in which semester you did this course in 5 years, yet time goes on. Try to use a structure that is long term, but focuses on your current work. I propose that folders that equal a canvas structure can be quite helpful, allowing you to systematise the project you are currently working on. I work in a Canvas structure about the publications I currently work on, and move finished projects into folders that are sorted by countries. Also, you may want to link this to a [https://www.notion.so/ Notion]-fuelled data base, which can help you to keep track of your projects and deliverables. Some people use Notion also as a platform for a second brain, however I use '[https://roamresearch.com/ roam research]' to this end, because is resembles my personal structure of thinking more.\n\nMake backups. Nothing is more horrible to a programmer than a data loss. Have at least three physical backups, and ideally also at least two online backups. Make a clear backup plan, where you have short term, mid term and long term backups that allow you a nested structure in case you have a fatal system flaw. I once almost went through a data loss, and I would like to have this feeling never again. Hence I have a dozen hard drive backups (ok, a bit too much), two time machines, a raid and dropbox as well as iCloud. In addition, I always have a SSD with me and use MicroSD cards for the core business. \nI divide my data into four sections: \n* 1) All R stuff, which is as of today about 22 GB. This is the most important data plus Code. The Code files are a few MB, but I prefer to keep some data in case I want to get back at it. For some more intense calculations I also saved the workspace into a file, however these are rare exceptions. \n* 2) The larger datasets are on hard drives, where I keep a backup structure two have the data associated to published papers on two hard drives, one at University, and one at home. \n* 3) All writing and presentations (about 60 GB, mostly because of teaching presentations, the word files are a few hundred MB), so basically all \"office\" related files are in my documents folder. This one is stout across all my technical Units, so if I work on the laptop it automatically syncs to my desktop computer. The latter one is also connected to \n* 4) All movie files. Filming eats up hard drive space in terabytes, and I went to only keeping the raw originals, the final cut projects and the finally rendered movies. Everything in between I delete once a project is finished. \n\nI think the desktop is generally a bad place to save your data, so I use my desktop folder only as a big turnaround folder to download stuff, work at it right away and send it back. Every few months I move all this stuff into a folder and delete the originals, and backup this folder. When working with data from the Internet, remember that the Internet is changing, and sometimes data will be gone. Hence keeping a copy of the more important projects can be a lifesaver. Also, always add a small text file with the meta-information of the downloaded file in the folder, containing the link where and the data when you downloaded the file.  \n\nFirefox, Word, Excel, Keynote, Notion, Drafts, RocketChat, and FinalCut. Lastly, of course, R. I can highly endorse Slack for all people that are coding, and consider it to be very convenient for collaboration that is focused on coding. Also, I use Notion to structure larger projects, and develop nested structures for planning of larger projects. An example would be the data science class, which is one project, the different days are sub-projects, and slides, sessions etc are smaller tasks. Within such smaller tasks, I discovered that nested toggle lists work best for me, and I also use this to plan the analysis of a larger dataset. Nested toggle lists match my brain structure when planning. However, I also use 'roam research' in order to build a second brain. In my case, this is almost exclusively focused on my work on [[Normativity of Methods]]. Ever since I started using a second brain, no precious thought or bit of information went lost, but instead it is integrated into a growing structure in 'roam research'. I wish I would have started earlier with this. So many thoughts are lost forever. \n\n\n== Coding ==\nThe hagakure says:\n''\u201cWhen one is writing an R-Code for someone, one should think that the recipient will make it into a hanging scroll.\u201d'' - Hagakure\n\nEverything up until now was only pretext, and now we come to the real deal: Coding. Writing code in R is like writing a language. You have dialects, different capabilities of the tongue, and even these little minute details that make us all different when we write and talk. '''You need to find your own way of writing code in R,''' yet there are many suggestions and norms how people write code in R. Generally, when thinking in code, I thinking in a nested structure of at least four different levels: 1) Scripts, 2) Sections, 3) Lines and 4) Commands. Let's break these down individually.\n\n==== Scripts ====\nYou should develop a clear procedure how you name your files. If they have the name of the folder they are in, it does not make any sense. If they all have the same name within each folder, it also does not make any sense. Searching your hard drive for the \"main_analysis\" file will not bring you anywhere, if you call the central file like this in every folder. I tend to name it after the different steps of methods, which could be as an example data preparation and the project name \"data_prep_gobi_plant_diversity\". It can be good to add a timestamp, which should be something other than the date you last saved within the file, because this information is anyway available. I tend to just label files into starting, pending and finished. \n\nWithin the scripts, I always have the same structure. I start with all libraries I need to load, and then continue with the data I am loading. Then I proceed with data preparations and reformatting. The typical next steps are initial data inspection commands, often intermingled with some simple plots that serve more as a data inspection. Then I shift to deeper analysis, and also to some final plots. It is important to label every section with a hashtag-label-structure, and to divide the sections by some empty lines. I often tend to copy code towards the end of the script if it has proven to be important in the beginning but then maybe shifted out of the main focus. Sometimes you wish you could go back, which why it can be helpful to keep different versions of your base code. In this case, I recommend that you save the files with the date, since names just as \"test\", \"test_again\", \"once_more\", \"nonsense\", \"shit\", \"shitoncemore\", \"nowreally\", \"andagain\" have proven to be emotionally fulfilling at first, but confusing down the road. \n\nWhat is most relevant to me within the script is to clearly indicate open steps if you need to take a break, or want to continue to code later. Writing a little line to your future self can be really helpful. Also, clearly describe all steps that were part of your final report or publication. You may think you remember all this, but if you need to redo an analysis after you get a revision for a manuscript, you might not remember what you did six months ago. Some label which code creates which figure - this can be essential to avoid this challenge. Lastly, write into the Code which version you used, and when you last updated R. At rare occasions packages change, and sometimes even the analysis may reveal different [[Glossary|patterns]]. While this may sound alarming, this is a normal process of science evolving. So far, this has happened three times to me, but I can clearly state that these three occasions were all very special in their own wonderful way. \n\nA last word on the length of one script. I think every script should be like a poem, and thus should be longer than a Haiku, but shorter than - say - a thousand lines. If the code is too long, you will have trouble finding things. Even the two longest scripts I ever complied did not exceed a thousand lines. However, if this may be the case, build a nested structure, and try to divide the code into data crunching, analysis and plotting, or other steps. Make your code like a poem, hence it should not be too long, and it should be elegant. Take this advice from someone who could not be further away from an elegant code. More on this later.\n\n==== Code sections ====\nI already mentioned to label all different code sections. However, we should not forget about the poetry of the code itself. \n'''Different sections in code are like different verses in a poem.''' They should be consistent, and contain some inner logic and flow. As I already mentioned, each section should have a label line that describes what the section is about. I try to balance sections, and when they get too long, I divide them into different sections. This is however less guided by their lengths, but more by the density of information they contain. If a section is long, but repetitive, it is ok as the information density is comparable to shorter sections that are more complicated. I guess everybody needs to find their own rhyme scheme. Nothing would be worse than if we all coded the same way, as [[Glossary|innovation]] might become less and less. Therefore, I propose you experiment with your section composition, and try to find your own style. This is also the part where one can learn much from other people. Their solutions might give you a better diversity of approaches that you can integrate for yourself. If you have repeated structures in your code, it might be good to find a way to keep repeating sections in a way that makes it easy to adapt whenever necessary. Using tabs to make nested structures can be very helpful to define an inner logic on how sections are nested within each other. For instance, you can have the larger steps as main tabs, smaller subsection with one tab, and then within that individual steps with 2 tabs. Sections are the unit of code that can be quickly exchanged between people, as it is often enough to understand what this specific step is about, yet not too long so that it takes too long to understand everything. Hence exchange code snippets with others to get feedback, and to learn from each other. A really elegant section of code can be like a Haiku.\n\n==== Lines of code ====\nWhile it is easy to label sections with hashtags, and you may add an explanation to a line of code, this is only advisable if it is short and not self-explanatory. If you want to add a hashtag explanation to every line, it would probably look a bit clumsy. What is however often the case is that lines become too long. Personally, I think this should be avoided. Some graphic command may go over several lines, but then it is about specifications, and these should become part of your intuitive understanding in the long run. However, many learners want to show off in the beginning and make super long code lines, and these are hard to grasp for others, and also for you after a long time. Try to contain your lines, so they do not become dragons. Code lines are like poetry - they should make sense instantly. '''I think a good line solves exactly one problem.''' Also, a good line should look effortless. Try not to impress anyone, least yourself. You need to find your own rhyme scheme and rhythm, but lines should ideally flow in the long run. You might need to reconsider and think every now and then, but the majority of your lines should be part of your repertoire. \n\n==== R Language ====\nJust likely other language, R can be learned and has different dialects and accents. Just as there are wonderful words such as Serendipity and bamboozlement in the English language, there are fantastic words in R. You need to become a connoisseur of R commands and specifications. Many of the lesser important arguments can be the most relevant ones for shortcuts, and a loop or a reduction can bring you a long way when doing repetitive tasks. There are differences in terms of grammar, which often also depend on packages. However, especially data crunching is something where a small loophole can bring you a long way. Luckily, today there are many many resources in the Internet, and almost all questions can be asked in one of the larger search engines, and will reveal at least vital steps towards a solution. \n\n'''The first initial steps towards starting to build your own stock of commands:'''\n* https://rmarkdown.rstudio.com/lesson-1.html <br>\n* https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf <br>\n* https://cran.r-project.org/doc/contrib/Short-refcard.pdf\n\nThese norms and support structure are a great way to start. However, I think the most important way to practice is to work together with peers, help each other, review each other's code, also trying to dissect problems within a larger circle of learners, and briefing each other on the progress. This way you can gain the fastest progress. \n\n\n== The Tao of data analysis ==\n\n''\u201cThere is surely nothing other than the single purpose of the present moment. A persons's whole life is a succession of moment after moment. There will be nothing else to do, and nothing else to pursue. Live being true to the single purpose of the moment.\u201d'' - Hagakure\n\n==== Pretext ====\nThe next best step should be to go to the data() examples and just look at as many as possible. More datasets come with the individual packages. You may want to check out at least a few hundred examples to understand the rudimentaries. More data is in the web galore, hence it is vital to look at many examples, and diverse examples. Data is constructed, and these days there are all sorts of constructs available, and all sorts of samples, often through the Internet. The availability and diversity of datasets is every increasing, and it is vital to become versatile in the different viewpoints that data can offer. You learn to look at parts of the picture, and this does not only imply questions about data quality and bias, but also about the nature of data itself. Hence your data arise a part of reality but your analysis can also change reality. '''Data analysis is a question of responsibility.''' \n\n==== The scripts of the ancients ====\nThe more you know about the knowledge of the old, the more you will be able to derive from your data. While it is true that no methods can solve everything, all methods can solve more than nothing. Being experienced in scientific methods is of utter importance to get the best initial understanding of any given dataset. Statistics stand out to this end, as they are the basis of most approaches related to numbers. However, one has to recognise that there is an almost uncountable amount of statistical approaches. Start with the most important ones, as they will help you to cover most of the way. '''Take correlations'''. I think it can be done to understand the simple correlation within a few hours, maybe days. You will get the general mechanics, the formula, the deeper mathematics behind it. You can even square this with the textbook preconditions, the interpretation, maybe even the limitations. You are all set. However this will help you as much as practicing a punch alone and without an experienced teacher when you suddenly find yourself in a real fight. You may be able to throw the punch the way you practised it, but your opponent will not stand in the spot you practised to hit. Reality is messy, you need to be agile and adaptive. R coding is like kung fu, you need a lot of practice, but you also need to get into peer-to-peer practice, and you need an experienced teacher, and learn from the Ancient Ones. Just as every attack of an opponent is different, every dataset is different. However when you are versatile, you can find you goal no matter what. As the Hagakure says: ''\"Taking an enemy on the battlefield is like a hawk taking a bird. Even though one enters into the midst of thousands of them, it gives no attention to any bird other than the one it has first marked.\"'' Finding patterns in data is exactly like this. Once you become experienced and versatile, this is how you will find patterns in data. \n\nThere is also much new knowledge that is evolving, as data science is a thriving arena. Being versatile in the basics is one thing, but a true master in data science needs to equally rely on the ancients as well as the revolutionary renegades. They all offer knowledge, and we should perceive their knowledge and experience as pure gold that is potentially true. Some methods are established, and it is a good thing to know these. Other methods are evolving, and it is equally good to know the latest tricks and developments. Much [[Glossary|creativity]] however comes from also combining the old and the new schools of thinking. Innovation within scientific methods is often rooted in the combination of different scientific methods. These unlock different types of knowledge, and this can be seen as often appropriate to acknowledge the complexity within many datasets. Combination of approaches is the path to new knowledge, and new knowledge is what we need these days quite often, since the old knowledge has not solved the problems we face, and new problems are emerging. When you want to approximate solutions to these problems, you have to be like water.\n\n==== Non-duality in R-coding ====\nNon-duality in data science relates to the difference between predictive power and explanatory power. Any given dataset can be seen from these two perspectives, and equally from none of these perspectives. This is what non-duality of data science is all about. You need to learn to see both in data, and also none. Predictive power and explanatory power are one and the same, and they are not. As the ancients said, it is bad if one thing becomes two. The same is true for data analysis. Many guide their analysis through predictive power, and they become obsessed by the desire to have what they call \"the best model\". Thoughts on the goodness of fit circulate in their heads like wild dragons, and they never manage to see anything but the best model they can possibly achieve, and hence they fail. Many compromises have been made by people to find the best model, and sadly, the best model may never be found. As the Ancient Ones said: '''all models are wrong, some models are useful'''. One should never forget this. \n\nEqually, there are people who obsess about the explanatory power of models. They see the promise of causality in the smallest [[Glossary|patterns]] they find, and never stop iterating about how ''it all makes sense now''. Much has been explained in the past, but much may remain a mystery. Much that once was causal is lost, for no papers are available to remember it. The knowledge of people evolves, and with it the theories that our causality is often built upon. The predictive and the explanatory people even go to war against one another claiming victory after victory and their fight about superiority. However, many souls were lost in these endless quests, and to me it remains unclear if any real victory was ever won in this eternal iteration on whether patterns are causal or predictive. Both approaches can make sense, and the clever ones never went down the paths of priority, and avoided claiming what is better. They simply claimed their worldview, and were fine. They lived in their world of the two realms, and ignored the other realm completely. Many thus lived a happy life, half ignorant, but happy. These people are like the Ancient Ones living in their kingdom of old, with scientific disciplines and textbook knowledge, where kingdoms fought other kingdoms at times, but there was also peace and prosperity. \n\nWhat can be called a 'modern data scientists' will know nothing of these worlds of Old, but will become unattached to their data. One should be aware that all data is normative, but the patterns detection and analysis is ideally done with a beginner's mind, knowing everything and nothing at the same time. As the Ancients said, ''matters of great concern should be treated lightly'', to which someone replied, that matters of small concern should be treated seriously. The same is true for data analysis. '''We need to remember that the world is constructed, and that our data is only looking at parts of the picture.''' Despite all that, one should become detached from the analysis itself and versatile at it at the same time. Then coding becomes a form of art, and one can exceed one's own expectations. Your code becomes a statement, and even if people admire it, this does not matter to you. And thus you become the coder that you should be, and never will be. Then coding becomes a way.\n\n'''There are small mindfulness exercises in R-coding that have proven beneficial in the past:'''\n1) Chant the mantra \"library\". ''l-i-b-r-a-r-y''. Only by perfecting this mantra will you master the art to spell the word correctly, and load a library.\n2) Close the bracket. Through endless training, you can master the ancient art of closing every bracket you ever opened, understanding the inner circularity of life itself, and R-coding. Brackets in R are like breathing. Just as you breathe in and breathe out, you open and close brackets.\n3) Cleaning the workspace is like cleaning your inner self. After a long work session, the master will ritually close R as if it is second nature, and the knowledge of the ancients know how the very question \"do you want to save your workspace\" test the worthiness of the decibel always anew. \n4) Let go of your fear when coding. Simply breathe in, and breathe out. More is not expected of you to master the art of coding.\n\n----\n[[Category:Normativity of Methods]]\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "tf82ej87ldm4i1p0csowdrddp2ybepk"
                }
            },
            {
                "title": "Thought Experiments",
                "ns": "0",
                "id": "503",
                "revision": {
                    "id": "4514",
                    "parentid": "4032",
                    "timestamp": "2021-03-21T18:46:53Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Outlook */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "15163",
                        "#text": "[[File:Thought Experiment Concept Visualisation.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Thought Experiments]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| [[:Category:Inductive|Inductive]] || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.\n\n== Background ==\n[[File:Scopus hits Thought Experiment.png|450px|thumb|right|'''SCOPUS hits per year for Thought Experiments until 2019.''' Search term: 'thought experiment' in Title, Abstract, Keywords. Source: own.]]\n\n'''The Thought Experiment may well be the oldest scientific method.''' The consideration of potential futures was a vital step when our distant ancestors emancipated themselves and became humans. Asking themselves the ''What if'' questions was a vital step in the dawn of humankind, and both in the West and the East many of the first thinkers are known to have engaged in Thought Experiments. Methodologically, Thought Experiments provide a link between the metaphysical - or philosophy - and the natural world - i.e. natural sciences. Ancient Greece as well as Buddhism and Hinduism are full of early examples of Thought Experiments. Aristoteles remains a first vital step in the West, and after a long but slow growth of the importance of the methods, he represents a bridge into the early [[History of Methods|enlightenment]]. Galileo and Newton are early examples of famous Thought Experiments that connect theoretical considerations with a systematic reflection of the natural world. This generation of more generalisable laws was more transgressed with the ''Origin of Species'' by Charles Darwin. This theory was initially one epic Thought Experiment, and although DNA initially confirmed it, the role of epigenetics was another step that proved rather problematic for Darwinism. Mach introduced the term \"Thought Experiment\", and Lichtenberg created a more systematic exploration of thought experiments. Many ethical Thought Experiments gained province in the 20st century, and Derek Parfit is a prominent example how these experiments are used to illustrate and argument cases and examples within ethics.\n\n== What the method does ==\nThought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.  \n\nThe easiest way to set up a Thought Experiment is to ask yourself a \"What if\" question. Many Thought Experiments resolve around decisions, choices or potential states of the future. A famous example is the Trolley experiment, where a train rolls towards five train track workers, who would all be killed be the oncoming train, unaware of their potential demise. You can now change the direction of the train, and lead it to another track. There, one worker would be killed. Uncountable numbers of variations of this experiment exist (see Further Information), as it can teach much about choice, ethics, and responsibility. For instance, many people would change the train direction, but hardly anyone would push a person onto the track to derail the train. This would save the other five, and the outcome would be the same. However the difference between pushing a lever or pushing a person has deep psychological ramifications that resolve around guilt. This exemplifies that the Thought Experiment does not necessarily have a ''best'' outcome, as the outcome depends - in this example - on your moral choices. Some might argue that you should hurl yourself onto the track to stop the train, thereby not changing the countable outcome, but performing a deeply altruistic act that saves everybody else. Most people would probably be unable to do this. \n\n'''Such deeply normative Thought Experiments differ from Thought Experiments that resolve around the natural world.''' Dropping a feather and a stone from a high building is such an example, as this experiment is clearly not normative. We are all aware that the air would prevent the feather to fall as fast as the stone. What if we take the air now out of the experiment, and let both fall in a vacuum. Such a Thought Experiment is prominent in physics and exemplifies the great flexibility of this method. Schr\u00f6dinger's Cat was another example of the Thought Experiment, where quantum states and uncertainty are illustrated by a cat that is either dead or not, which depends on the decay of some radioactive element. Since radioactive decay rates are not continuous, but represent a sudden change, the cat could be dead or not. The cat is dead and not dead at the same time. Many argue that this is a paradox, and I would follow this assumption with the supporting argument that we basically look at two realities at the same time. This exemplifies again that our interpretation of this Thought Experiment can also be normative, since a definitive proof of my interpretation is very difficult. This is insofar relevant, as seemingly all Thought Experiments are still based on subjective realisations and inferences. \n\n[[File:Schr\u00f6dingers Cat.png|400px|thumb|left|'''Schr\u00f6dinger's Cat.''' Source: [https://ad4group.com/schrodingers-cat-and-the-marketing-strategy/ AD4Group].]]\n\nSo far, we see that there are Thought Experiments that resolve exclusively about a - subjective - human decision, and other types of Thought Experiments that are designed around setting in the physical world. The difference between these two is in the design of the experiment itself, as the first are always focused on the normative decisions or people, while the second focuses on our normative interpretation of anticipation of a design that is without a human influence. This distinction is already helpful, yet another dimension is about time. Many Thought Experiments are independent of time, while others try to reinterpret the past to make assumptions about a future about which we have no experience. Thought Experiments that focus on re-interpretation of the past (\"What if the assassination of Franz Ferdinands failed? Would the first World War still have happened?\") look for alternative pathways of history, often to understand the historical context and - more impotantly - the consequences of this context better. Most Thought Experiments are independent of a longer time scale. These experiments - such as the Trolley experiment - look at a very close future, and are often either very constructed or lack a connection to a specific context. Thought Experiments that focus on the future are often build around utopian or at least currently unthinkable examples that question the status quo, either form an ethical, societal, cultural or any other perspective. Such desirable futures are deeply normative yet can build an important bridge to our current reality through backcasting (\"What if there is no more poverty, and how can we achieve this?\"). \n\nAll this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https://en.wikipedia.org/wiki/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. '''Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.'''\n\n== Strengths and Challenges ==\nThe core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people's actions or decision, and the ''What if'' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. '''Schr\u00f6dinger's Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.''' This latest strength is equally also the greatest weakness of Thought Experiments. We might consider all different option about alternative states of the world, yet we have to acknowledge that humankind has a long history of being wrong in terms of our assumptions about the world. In other words, while Thought Experiments are so fantastic because they can be unburdened by reality, this automatically means that they are also potentially different from reality. Another potential flaw of Thought Experiments is that they are only as good as our assumptions and reflections about the world. A four-year-old making up theThought Experiment \"What if I have an unlimited amount of ice cream?\" would consequently drown or freeze in the unlimited amount of ice cream. Four-year-olds are not aware of the danger of the \"unlimited\", and may not be the best Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.\n\n== Normativity ==\nThought experiments are as we have already learned as normative as are our assumptions about the world. Hume argued that Thought Experiments are based on the laws of nature, yet here I would disagree. While many famous Thought Experiments are about the physical world, others are only made up in our minds. Many Thought Experiments are downright impossible to be matched with our reality, and are even explicitly designed to do this, such as Thomas Nagels Encyclopaedia of the Universe. It is therefore important to realise that basically all Thought Experiments are in essence normative, and one could say even downright subjective. Building on Derek Parfit, I would however propose a different interpretation, and propose that we should not measure the normativity of Thought Experiments through the design and setting, but instead by their outcome. Many people might come to the same conclusions within a given Thought Experiment, and some conclusion drawn from Thought Experiments may matter more than others. Consequently the penultimate question - also for Derek Parfit - is whether there are some Thought Experiments that are not normative. \n\n== Outlook ==\nMuch of [[Glossary|art]] and the media can be seen as a Thought Experiment, and there are ample examples that Thought Experiments in the media and the arts triggered or supported severe societal transformations. Thought Experiments are of equal importance in ethics and physics, and the bridge-building of the methodological approach should not be overestimated. Examples from the past prove that Thought Experiments can enable a great epistemological flexibility and diversity. This flexibility is even so large that Thought Experiments serve as a bridge between the epistemological and the ontological, or in other words between everything we know - and how we know it - and everything we believe. By enabling the transformation of our own most individual thoughts, Thought Experiments may provide a boat or a bridge to link the metaphysical with the world of knowledge.\n\n== Key Publications ==\nParfit, D. (2011). ''On what matters.'' Oxford University Press.\n\nParfit, D. (1984). ''Reasons and persons''. OUP Oxford.\n\nKamm, F. M. (2015). The trolley problem mysteries. Oxford University Press.\n\nhttps://www.thoughtexperiments.net/people-who-divide-like-an-amoeba/\n\n== Further Information ==\nWhen you start thinking about '''variations of the famous Trolley experiment''', you can dive into normative questions of all sorts. Here are some versions that our students came up with (remember: in the classical version, a trolley threatens to kill five people on the tracks, and you have the choice to divert the train towards one other person instead).\n* The trolley can also be diverted towards five cows who are standing on the tracks. How do you value animals' lives compared to human lives?\n* The trolley threatens to kill one child, but can also be diverted towards three healthy, old people. Which role does age play for your decision?\n* The trolley might stop automatically. There is a 25% chance that it stops before it reaches three people that it is on track to kill, and a 75% chance of stopping when you divert it towards a single individual. How do probabilities influence the decision?\n* Instead of the option to divert the trolley to one individual instead of five, you have the option to push another person onto the tracks, which would stop the trolley. How does letting someone die differ from actively pushing them onto the tracks?\n* You are tied to the tracks, but can also divert the train down a cliff, killing the (unspecified number of) passengers. Would you?\n----\n[[Category:Qualitative]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table_of_Contributors| author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "ahdul6xkekzohhvir565mhb2956p6v9"
                }
            },
            {
                "title": "Time",
                "ns": "0",
                "id": "895",
                "revision": {
                    "id": "6301",
                    "parentid": "6300",
                    "timestamp": "2021-08-30T10:08:35Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "36092",
                        "#text": "'''In short:''' This entry revolves around the history of Time as a concept, and its implications for scientific inquiry. Please refer to the [[Design Criteria of Methods]] entry for a general overview on all design criteria.\n\nTime is one of the greatest misconceptions in terms of methods (and humans in general), and together with space (or spatial scale, or grain) poses one of the key challenges in methodological aspects of science. Here, we give a short introduction to the different aspects of time from a methodological standpoint. Starting with the most relevant general aspects of time, the text will then focus on concrete methodological aspects of time. An outlook on the necessary next steps concerning time in scientific methods concludes the text.\n\n== A historical view on time ==\n'''We humans are known to be a mere glitch in the long cosmic developments that lay already behind us.''' Much time has already passed, probably billions of years, and we as humankind have only been here for a few thousand or a few hundred thousand years, depending on how you define us. Yet within this comparably short or even negligible period, we have become an entity that - according to Derek Parfit - starts to understand the universe. In the long run, this may be our most remarkable achievement, and we already understood as much as that we will probably not be there forever. This has led to many debates about the reasons for our being, and our role in the universe, and these debates will likely not end anytime soon. There exist diverse flavours to make sense of our presence, and while I invite everyone to participate in this great abyss of a debate, I am sure that it will be difficult to come up with a conclusion that we all share. Following Derek Parfit, we may agree that our future could be much better than our past, and it would be worthwhile in this spirit to go on, and to contribute to a future of humankind that could be so much better than pur past. Nihilism, existentialism and many other -isms are telling us to not buy into this optimistic rhetoric, yet personally, I would disagree. \n\nThus, let us focus on the main obstacle to this bright future that we have faced ever since we began: ''Temporal autocorrelation.'' This principle defines that we humans value everything that occurs in the close past or future to be more relevant than occurrences in the distant future or past. This is even independent of the likelihood whether future events will actually happen. As an example, imagine that you want to have a new computer every few years, and you can pay 5\u20ac to have a 50% chance to get a new computer tomorrow. If you actually needed one, you would surely do that. However, even adjusted for inflation, many people would not pay the same 5\u20ac to have a 50 % chance to win the latest computer in 10 years. What difference does it make? It is the latest tech in any case. '''Temporal discounting is one of the main ways how people act unreasonably'''. This even extends well beyond the point where we are dead already, although this also plays a role. Our unreasonable inabilities to transcend temporal discounting extend likewise into the past. The strongest argument to this end is how many people insist that they are worse off today when compared to the past - in the 'good old days'. The mistake that is typically made is that some aspects in the past were better, while many other aspects were worse. People hence have a tendency to value the time period closer to them different from the time periods more distant. For the past, only the good things are remembered, and for the present, the negative sides are also acknowledged. It widely depends on other circumstances whether this unreasonable evaluation is better or worse than a more objective view on each time period. In any case, the sheer difference between how people evaluate distant time periods or events compared to closer ones is one indirect motivation for science to put this knowledge into perspective.\n\n\n== Understanding the past ==\nHistorical research as well as other branches of science that investigate data or information from the past allow us to put our current knowledge, context and situation into the context of past periods or events. This is widely driven by the historical sources that are at our disposal. Palaenonthology, which is the science that deals with the oldest records, is a good example how single artefacts and findings can be embedded into a [[:Category:Quantitative|quantitative]] timeline via carbon dating. Be measuring the decay of radionucleids in the objects, it is today possible to get a rather good tracking of the age of fossils. However, before the establishment of this hard scientific method, palaeontologists and geologists relied widely on the context of the sediments in which the fragments were found. The sedimental history hence become an early dating method, and many other indicators such as thin layers of global catastrophes through meteors allow to pinpoint the temporal origins with an often high precision. One prominent example is the layer at the Cratecous-Tertiary line, when an Earth-shattering meteor not only extinguished the dinosaurs, but also created a thin sedimental layer that allows to date this event in time with often remarkable precision. Other important methods in dating distant developments can be hidden in the molecules and systematics of many organisms, since sophisticated DNA analysis often allow to generate a holistic developmental history of organisms if suitable material for DNA analysis is found. To this end, preserved record in moorlands is often a source of material dating back 10 thousands of years, creating a database of the development of life itself.\n\nHuman development can be preserved as part of the archeological record, which is often a deeply qualitative analysis in describing and contextualising relevant artefacts. Written accounts and [[Iconology|paintings]] are for instance one of the vital sources we had before the invention of photography. Before the written word, but also during its existence, there is the archeological record that investigates surviving pieces - quite literally - of the past. This leads to a [[Bias and Critical Thinking|bias]] that we all - unfortunately - suffer from: for instance, much of the archaeological record is preserved from the temperate or subtropical zones of planet Earth, where erosion, wind and weather did not destroy not as many remnants of the distant past, while in the tropics much of what once was has been destroyed by the elements. Hence, little information is for example found in sub-saharan Africa, despite indirect hints pinpointing to long-dating cultures, and the rare fossil founds testify even the origin of humans as a people (Homo sapiens) in Eastern Africa. Importantly, the precious testimonies of the past are often part of a bigger emerging picture of humankind, hence expert knowledge of a specific period or culture is pivotal in contextualising each relevant findings in a systematic way. These archaeological records can be locally clustered, opening windows into regional developments, yet sadly omitting other developments, especially in environments that are unfavourable in preserving precious artefacts, such as the tropics. This exemplifies that our understanding of the past is often like looking at a puzzle of which we have only some few pieces left, making it difficult to make out a whole picture. One methodological approach can be to actively try and reproduce the living conditions of the past, and maybe find out what certain artefacts were being used for. \n\nOnce we reach the time of the first modern civilisation, the fossil record becomes more abundant, and whole buildings or even settlements are preserved. Early painting and pieces of art give testimony of the ethical understanding of our ancestors, and with the preservation of language the road is paved to a direct documentation of past time. The Stone of Rosetta is a famous example of a key to unraveling the ancient languages of the Egypts, allowing to translate much of the record preserved in tombs. Early texts in the East were often preserved distant from their origin, with the Pali canon being a famous example of texts that were literally all destroyed in the region of origin in India due to the Muslim invasion. On the other hand, much of what was written by the ancient Greeks has been preserved in Persia and the Muslim world, and the Bible is a famous example of a text that found its way to Europe through the Greek translations. '''Hence the analysis of texts is among the first systematic scientific methods of analysis, and dates back to the origins of texts itself.''' All world religions build on a -often critical - analysis and annotation of the texts closest to their origin, often triggering an early diversification in the different branches of world religions. Different interpretations of the Koran or the Pali canon hence lead to the main streams of these religions, and in the case of the Bible was it the translation of the original texts into lay people's language that triggered not only a new branch of Christianity, but also centuries of war and destruction. This shows how normative text analysis can be, and what the direct consequences are in the world. \n\nEqual considerations can be made for the early methodological approaches that rose to prominence in the antique, the most notable among them being astrological observations, mathematics, physics, or early medicine. It was often within urban cultures that such early examples of scientific methods flourished, and are still known today. A noteworthy example are the precise astronomical calculations of the Maya, which were often closely linked with their daily conduct, since the movement of the planets played a vital role in their religion. Their writing system shows similarity to the Japanese writing and many other approaches that provide a link to a documentation through paintings and thus deserve a deep interpretation well beyond the content of the symbols itselves. This documentation hence now only shows us how scientific methods emerged, but also allow us to investigate the past with our current methods. To this end, paintings are a form of art that deserves a closer analytical look, and many diverse approaches exist that allow for a close examination of the preserved record. It was the financial surplus in urban settlements that led to the developments of much what is now know as different epochs of painters, and the digital age has made many paintings accessible through the Internet. With the representation through paintings, there is also a rising record of maps, and other forms of more systematic representations that give testimony of a clear geographical picture of the past. Equally, the texts of Herodot can be seen as one of the earliest geographical accounts, albeit in writing. The scientific analysis of these records is vital to understand the past, and the density of the historical record has often increased the closer we move to the present. \n\nWith the rise of photography an altogether different kind of record was created, and the development of modern cartography had an equally dramatic influence on the density of knowledge that became available. The detailed scientific examination of the diverse sources since the development of the printing press by Johannes Gutenberg was already a a severe development leading to an exponential growth of the printed word. The development of printed images and multiplication of pictures that came later was an altogether different media unleashed, leading to a totally different world. Once these image started moving and even talking, human civilisation created an exponentially growing record of the world, of which the Internet is the latest development. '''From a methodological standpoint, this diversity of media triggered two relevant developments:''' An ever-increasing differentiation of empirical analyiss, and severe philosophical consequences of this brave new world. The role of art and its critique changed fundamentally, with consequences that translate directly into modern society. It is impossible to do justice to these developments here, yet Walter Benjamin should be noted as an early stepping stone towards a clearer role of the critique within society. This triggered direct and long overdue consequences for the scientific method as well, and led to a redevelopment of the role of the critical observer, commentator and ultimately unleashed a more nuanced view on science and its consequences. Methodologically, [[deconstruction]] as well as the critical perspective emerged over the last decades, all the while the possibilities of modern sciences [[History of Methods|diversified]] as well. The exponential track of the 20th century triggered a diversity in scientific approaches that dwarves by far the scale of everything that existed before. Along the way, our understanding of the distant past has become a question of the preservation of the immense emerging record. The amount of information that is being created and stored on the Internet is increasing at a pace that would have been unthinkable before, leaving us to at least try to spare some thoughts about the role this information may play in the future. Will everything be preserved for longer times? What is the turnover of our data? And what will be the meaning of the information we create for future people?\n\n\n== Understanding the future ==\nDerek Parfit concluded that our future may be wonderful, and that we cannot make the ethical decision whether future people should exist at all. In other words, we have no moral grounds to end human history. The fact that humans can think about the future is one of the most defining elements about our species. Compare us to the chipmunk. The chipmunk may store nuts, and forget about most of these. Birds migrate in anticipation of the seasons changing. Whales may follow their food. '''It is probably only us who have a abstract understanding about different futures, and can adapt our actions based on this knowledge.''' To do so, the scientific examination of our futures became more and more systematic over the last centuries and especially decades. We know that the earliest cultures cared about their future - the artefacts found in graves showcase the complex world our ancestors anticipated in the afterlife. Some of the earliest texts offer a testimony of what might happen in the future, often as a motivation or a basis for moral authority for the living. Moses' interpretation of the dreams of the Pharao showcases how the anticipation of a possible future and its shortcoming was central to the ancient culture. \n\nWhile the oracles and mysticisms of the ancients were often complicated yet not systematical in their methodological approaches, this changed with modern agriculture. Human civilisation got domesticated by its crops, and depended on their harvest. The demise of the Maya may be an early testimony of crop failures, and especially in Asia and Europe, records of the central role of the harvest within the seasonal calendar have been preserved for centuries. The ripening of the wine harvest is a record often known since several centuries, and deviances from the known averages often led to catastrophic famine and migrations. To prevent such catastrophes, societies began to index and plan their agricultural practice into the future, and the rise of numbers - with examples from basically all early cultures - testify how this allowed for the thriving of many diverse cultures.\n\nHowever, also more [[:Category:Qualitative|qualitative]] and vivid approaches to the future emerged in the literature and the other arts, among them More's Utopia as an early testimony on how he imagined a radically different society already in 1516. Sadly, Christopher Columbus in his anticipation of a new path to India via the East triggered a reality that was - and still is - so different from More's anticipation of the future. The human urge for discovery is what drove many people into the New World, often looking for a more hopeful future. The people that already lived in these regions of the world had no means to anticipate the grim futures most of them would face under these developments. Through the rising inequalities of colonialism and other deeply regrettable developments of the rise of Europe, a surplus in resources was extracted from the colonies, enabling an economic system that was built around a more systematic exploitation of the regions that would become the Global South. The urge to make a business became the backbone of the rise of utilitarianism and its associated economic paradigms, each willing to cash in on an anticipation of their future, typically on the costs of the future of other people. This showcases how the capability of an anticipation of the future, and being privileged with the means to act based on that, created an inequality that is basically still on the rise today. \n\nScientific methods such as [[Scenario Planning]] were initially tools of a mercantile elite willing to improve their future chances of profit. Many of the dramatic environmental catastrophes of the 20th and 21st century were rooted in the systematic ignoring of small or even imperceptible risks that still became a reality. Chernobyl, Minamata, Bhopal, the Exxon Valdez and the Aral Sea are testimonies on how risks were perceived incorrectly, with dire consequences for the environment and the people in it. '''Anticipating the potential futures has branched into diverse possibilities rooted in the emerging methodological canon.''' Examples are several approaches known under the term [[Machine Learning]] which allow for a data-driven anticipation of future developments, often with a focus on [[Correlations|correlative]] patterns. Another development in science is the inclusion and joint learning of science and society together. Methodological approaches such as [[Visioning & Backcasting|Visioning]] allow for a more integrational perspective of potential futures based on the diverse knowledge of involved actors. Often applied in sustainability and futures research, they also hold normative implications for the kind of future we want, and the ones we don't.\n\n\n== Understanding change ==\nRooted in the agricultural experiments about 100 years ago, so-called longitudinal studies investigate in a designed methodological setting how changes can be quantified over time. Rooted in the [[Experiments|experimental]] approaches of the [[ANOVA|Analysis of Variance]], these [[Statistics|statistical]] methods became the gold standard in clinical trials, and enabled such different disciplines as ecology and psychology to systematically [[Designing studies|design studies]] to quantify changes over time. However, such approaches have increasingly been considered to be too rigid to adequately consider the diversity of parameters that characterise real world dynamics. The Corona pandemic is an example where many dynamics can be anticipated, such as the increase in numbers, and other dynamics are investigated in longitudinal studies, such as the effectiveness of vaccines. However, some changes can also not be instantly anticipated or investigated, which highlights that in order to explore future dynamics, longitudinal studies may enable some focal points to investigate, yet more [[Agency, Complexity and Emergence|complex]] dynamics may demand other approaches.\n\nLongitudinal studies may deserve an own Wiki entry in the long run, yet what is most relevant here is that designing such studies demands a clear recognition of the time intervals and anticipated hypotheses that are being investigated. While more and more temporal data becomes available, most prominently through the Internet, and this data can be analysed by diverse methodological approaches, it is a general rule of thumb that temporal analyses of quantitative data demand a high level of expertise. This can be related to four main challenges within research that investigates more than one current snapshot in time:\n\n\n== Types of temporal changes ==\nA general awareness of the different ways how changes can manifest themselves over time is a precondition for a selection of a specific method. '''To this end, we can distinguish between three general patterns within changes over time: Linear changes, non-linear changes, and repeated patterns.''' Many patterns that can be derived from changes are following linear dynamics. A lack of such linear dynamics or changes may be rooted in a low data density, yet human perception is also often following non-linear dynamics. We are for instance able to perceive a light to be on or off, yet the fine change between these two states is too rapid to be visually perceived by humans. This is different from fire, where a smouldering fire is not really burning, but it is also not cold. A pile of wood will start to burn, the fire will eventually spread, and then slowly fade away. Thus, an electric light being switched on is a non-linear pattern, while a fire burning is more linear. Examples of sudden changes are prominent in physics, yet also found their way into theories in economy or ecology. Within psychology or social science this is even more complex because of the peculiarity of human perception, much of which is not linear, but instead we think in groups or levels. This may be the reason why it is so difficult for us to actually establish a gradual line of perception. Take the example of being happy, or being sad. We can often say when we are either happy or sad, but what about some in-between state? Can you define when you are half happy and half sad? Hence the 'glas-half-empty-and-half-full'-allegory. Such normative groups tend to force us to think in categories, often in dualities, which is deeply embedded in Western culture, yet also beyond. This is the point when we should acknowledge the value of a critical perspective at least as much as the value of qualitative methods when it comes to perceptions. This testifies one of the direct peers of [[Bias and Critical Thinking|critical realism]], and the associated shrinking importance of personal identity but not of personal perception. Researchers should want to understand more about subjective perspectives, and try to integrate them, and realise especially how these may change over time. Exciting research emerges to this end, and new methods allow to unravel human perception from a qualitative perspective, thereby allowing for a more critical viewpoint on many of the categories through which we tend to see the world. However this does not only depend on the constructs in which we tend to group the world from a quantitative or qualitative categorical viewpoint, but also the temporal grain that we establish in our perspective.\n\n\n== Temporal grain and measures of time ==\nTemporal grain can be defined as the temporal resolution at which we observe a certain phenomenon. Take again the example of the electrical lightbulb. If we switch it on, we see that it is instantly on. This can be - sometimes - different in a Neon tube, which in some occasions needs a few seconds until it is filled with plasma and becomes bright. Here, there is a delay between the off state and the on state - an in-between state. If I now close my eyes for five seconds, and only open them at a regular interval every five seconds, and then close them again, then this in-between state would be missed by me. '''Such temporal grain effects are crucial in our view of the world, since we can only observe what is within the frequence of our observations'''. Within quantitive analysis, this is also closely related to the question of statistical power, since each time step needs to be sufficiently supported by data. If you conduct repeated [[Interviews]] with the same people, one missing interview may make the whole data point useless, depending on the analysis method that is being used. Equally important is temporal grain in qualitative approaches, where repeated observations or gathering of information may hold a key to human perceptions and their change. If you want to inquire the mood of a participant in a study, then '''it is clear that you do not want to ask the person every minute how they feel, yet you equally do not want to ask the person every month, because more often than not, moods shift quickly'''. Another good example of the relevance of temporal grain in qualitative approaches is the analysis of the paintings of an artist. When we look at Picasso there are clear periods that are widely accepted today, and Georgia O'Keeffe is yet another example where periods are well known. While typically there is a close process in the analysis of the work of artists with the biography, a special role falls to the critique of art in this context, which - in a nutshell - demands not only to recognise but also to transcend the context, including the social and societal circumstances surrounding the origin and influences of the art. Since this make the critique an methodological approach well beyond the mere reflection on the art itself, it creates new knowledge, and is often a highly relevant analysis of societal developments. This impact of temporal grain is often overseen or not contextualised in the recognition of scientific methods, and will surely increase even more with a more diverse and less disciplinary science.\n\n\n== Temporal relativity ==\nThe aspect of temporal perception is closely linked to the fact that time is normative. '''Since we perceive time differently, it is not objective.''' Consequently, everything that can be associated with time is equally not objective. We should not confuse this with the mere fact that some aspects of time are more precise than others. The invention of the chronograph as a means to locating a ship in the vastness of the oceans allowed not only for a more precise navigation, but also for a more efficient abuse of the regions known as colonies back then. Hence, even the mere consequences of time or its more precise utilisation are relative and normative. Yet what about time itself, as a physical entity? Since Einstein it became clear that under a clever experimental settings, twins may have a different age, depending on their relative travels through the Universe being different. That does not mean that everything is relative, but it can be. The discovery of gravitational waves is another example of a physical postulate that has been proven by a precise calculation of time. Within such experimental settings time is of the essence. If the 20st century was the period of Utilitarianism and its failures, then this regime ran more often than not on a strict time regime. Just as the results of science rooted in positivism were increasingly criticised and questioned, there is an equally strong urge to question scientific results once time has passed. While it is unclear how much time leads to sufficiently different circumstances to make results questionable, the same process can even be - counterintuitively - reverse. Many of the models and results predicting the effects of climate change have been more severe in reality than predicted years of decades before. Climate change became, sadly, a classic example where many models were actually too conservative compared to reality. It is a weird plot twist that this fact has become an argument of climate change deniers, which showcases that the means justify all ends, even our own. This links to the ultimate end, or at least how it is potentially proclaimed. Since Nostradamus and probably long before, the end of human kind as predicted by prophets and later scientists has been used as threat to make people submit, or at least leave them in awe, if not fear. The end of time (or at least humankind) has been a continuous motivation for all sorts of actions, and uncountable apocalypses came to pass once the due date went by. Once more, time proves its normative capacity, at least until some point. How strongly people feared these ends showcases the normative strengths time can encapsule, and more methodological dimensions of time may unlock in the future.\n\n\n== Methodological viewpoints of time ==\nInstead of glancing to the horizon, let us take a step back and look at the main points we should all agree upon when it comes to time concerning the nexus of scientific methods.\n\n[[Methods|Within scientific methods, documentation is key.]] Scientific studies should either produce knowledge that is reproducible, or the documentation of the knowledge production process should enable a seamless understanding of the overall process that led to the scientific result. The methodological design is often deeply rooted within the norms of scientific disciplines, making it a highly normative process. Concerning time, this is insofar relevant that researchers need to clearly document when, for instance, data gathering and analysis were conducted. The Internet is a good example of an often unstable data source, hence the date when you accessed specific information may be the key to the results you got often much later based on your analysis. On the other hand, the date of your analysis also reveals whether you took the latest information into account. While next to no one does that - myself included - the Corona Pandemic is a prominent example how rapid knowledge emerges and even changes. To this end, [[Citations|citations]] are like indirect time stamps since they allow the reader to understand which knowledge went into the conclusions you had. This helps however very little if the knowledge you used is primary data, for example a database with numbers of infected people. Yet just as you would clearly document the date on which you conducted specific Interviews, it is advisable to have a meta-file that contains information about the time stamps of your research. Natural science often have lab books for this, and ethnographic research has equally clear examples how such emerging data and knowledge can be documented. '''Research diaries should thus emerge as more important in the coming years, because they allow for a critical examination of the temporal dimensions of our own research processes and conducts in retrospect.''' These existing accounts will allow us deeper insights into research processes and mechanisms that many people assume to be rational, yet as [[Scientific methods and societal paradigms|Larry Laudan]] rightly concluded, research is often not exactly rational. Instead we talk about perceptions, interactions and often also serendipity, making the documentation of our research processes a key endeavour where more possibilities, norms and standards will continue to emerge.\n\nAnother key aspect where time patterns within research conduct is workload. It is almost a cliche how much people underestimate the time that needs to be put into research to arrive the respective results, and again this is often a deeply personal process. How many people finished their [[How to write a thesis|thesis]] in time? Me, certainly not. Making a time plan and overview of milestones for your research is surely worth your while, yet you should not feel bad if this then changes later on  - it almost always does. Planning your research is the ultimate joke that time plays on us folks, and learning to become realistic to this end is something one may thrive for a whole life. So why should it be different when it comes to the perception of time?\n\n\n== Temporal aspects of the combination of methods ==\nA crucial aspect that deserves more reflection when it comes to scientific methods and time is the sequence in which research is conducted. While this was already mentioned before, a more detailed account is necessary in order to present general principles in the temporal sequence in which methods are conducted. The concept of triangulation is one example, where for instance quantitative and qualitative methods are combined and the respective results are being compared. Ideally, this could also be process where the sequence of methods being used is enabling a higher specificity in order to answer the research question. An example would be to make a broad survey within a community, become more focussed with semi-structured interviews, and then gain really deep knowledge in [[Ethnography|ethnographic observations]] of some key actors in the community. Reversing this order would make hardly any sense. Still, in medicine it is often single cases from clinical practice that may inspire larger complex investigations through clinical trials, thus while in the first example the specificity increases, in the second example it is actually reversed. A key challenge when increasing specificity is to maintain validity, which is often scattered in the beginning, but becomes more focused in a later stage.\n\nWithin larger research projects, different methodological approaches are often used in parallel instead of a sequential order. While one branch of a project may make workshops, another branch may focus on ethnographic observation, and yet another part revolves around an experiment with an intervention. More often than not, there is little anticipation how these different forms of knowledge are integrated, as the united goal is not always straightforward. To this, end, integration is a frontier that is not always in focus, and it will be a challenge how to find a consensus within most interdisciplinary fields of research beyond medicine. Interestingly, in medicine such temporal sequences or parallel conducts are more established and follow clearer guidelines and also practicalities within clinical practice. The united goal to help the patient may serve a strong integration purpose to this end, with the overall goal being less clear in other research let alone practice.\n\nAnother often overseen aspects regarding time in the combination of methods are the aspects of long term observations. Due to the funding in research hardly exceeding some few years, long-term research is rare. Especially maintaining longer experiments or investigating a larger dataset e.g. within the arts, it is often difficult to generate knowledge that can deviate from knowledge synthesised in a rather short period. More focus on long-term research is needed in order gain the knowledge necessary when facing the problems we need to overcome. Climate change is a rare example where institutions such as the IPCC make sure that enough momentum and coherence was generated to create a body of knowledge that integrates millions of working hours of researchers, looks at incredible large and diverse dataset, and creates an integration and synthesis of these often long-term datasets that allows for clear policy recommendations.\n\n\n== The future of temporal aspects in methods ==\nThe problems we still face regarding temporal aspects of research may all come to pass in future research. The last decades have already shown how dense the available data become in many aspects of science, and how digital communication and means of documentation led to a new era of research when it comes to temporal aspects of scientific methods. Analytical methods allow for longitudinal analysis unheard of some decades ago, and [[Machine Learning|machine learning]] unravels altogether new ways of research. Online libraries of art allow a broader access to data, yet all these exciting technologies cannot lead us to ignore the key challenge: '''Long-term research and funding are to date rare, and the role of academia within society will need to change in order to allow for research that is more continuously embedded.''' This would allow us to learn from past mistakes, and utilise more diverse approaches to gather knowledge about the different futures, and how we may reach these.\n\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "23zb8cdgc94a6m5tla1uq6yalsfbvyl"
                }
            },
            {
                "title": "Time Series Data in Python",
                "ns": "0",
                "id": "1111",
                "revision": {
                    "id": "7264",
                    "parentid": "7207",
                    "timestamp": "2023-06-30T06:01:18Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This articles provides an introduction to handling time series data in Python",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "19874",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022). \n\n<syntaxhighlight lang=\"Python\" line>\nimport numpy as np ## to prepare your data\nimport pandas as pd ## to prepare your data\nimport plotly.express as px ## to visualize your data\nimport os ## to set your working directory\n</syntaxhighlight>\n\nIt is important to check which folder Python believes to be working in. If you have saved the dataset in another folder, you can either change the working directory or move the dataset. Make sure your dataset is in a location that is easy to find and does not have a long path since this can produce errors in setting the working directory. \n<syntaxhighlight lang=\"Python\" line>\n##Check current working directory\ncurrent_dir = os.getcwd()\nprint(current_dir)\n## Change working directory if needed\nos.chdir('/path/to/your/directory')\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\ndf = pd.read_csv('D202.csv')\ndf.head()\n</syntaxhighlight>\nBy looking at the first few rows we can see that the electric usage is documented every 15 minutes. This means that one day has 4*24 data points.\nWe can also see the different columns that provide further information about electricity consumption.\nNext, let's choose the most relevant columns for our research:\n\n<syntaxhighlight lang=\"Python\" line>\n## Let's choose the most relevant columns for our research:\ndf['start_date'] = pd.to_datetime(df['DATE'] + ' ' + df['START TIME'])\ndf['cost_dollars'] = df['COST'].apply(lambda x: float(x[1:]))\ndf.rename(columns={'USAGE': 'usage_kwh'}, inplace=True)\ndf = df.drop(columns=['TYPE', 'UNITS', 'DATE', 'START TIME', 'END TIME', 'NOTES', 'COST']).set_index('start_date')\n</syntaxhighlight>\nWe select DATE and START time to create a dataframe called start_date. These two columns are transformed into a date and time format. \nWe then create the dataframe \u201ccost_dollars\u201d by creating the dataframe based on the COST column and transform it to float data. \nThe USAGE column is then renamed and we drop a number of columns that are not needed.\n\nThe dataset contains about 2 years of data, we will only have a look at the first 2 weeks. For this we use iloc. iloc is an indexing method (by Pandas) with which you can choose a slice of your dataset based on its numerical position. Note that it follows the logic of exclusive indexing, meaning that the end index provided is not included.\nTo select the slice we want we first specify the rows. In our case, we chose the rows from 0 (indicated by a blank space before the colon) to the 4*14*24th row. This is because we want the first fourteen days and one day is 4*24 data points. We want all columns which is why we don't specify anything after that. If we wanted to, we would have to separate the row indexes with a comma and provide indexes for the columns.\n<syntaxhighlight lang=\"Python\" line>\ndf = df.iloc[:24*4*14]\ndf.head()\n</syntaxhighlight>\n\n==Challenges with Time Series Data==\nOften, time series data contains long-term trends, seasonality in the form of periodic variations, and a residual component. When dealing with time series data, it is important to take these factors into account. Depending on the domain and goal, trends, and seasonality might be of interest to yield important value, but sometimes, you want to get rid of the two, when most of the information is contained in the residual component.\nThe latter is the case in an analysis of a group project of mine from 2020. In that project, we try to classify the type of surface while cycling with a smartphone worn in the front pocket and need to remove the periodicity and long-term trend to analyze the finer details of the signal. The analysis can be found at [https://lg4ml.org/grounddetection/ here]. Unfortunately, it is only available in German.\n\n==Dealing with Time Series Data==\n===Visualizing Data===\nThe first step when dealing with time series data is to plot the data using line plots, scatter plots, or histograms. Line plots can visualize the time domain of the data, while scatter plots can be used to inspect the frequency domain obtained by a fast Fourier transformation. It would exceed the scope to explain the fast Fourier transformation, but it suffices to say that it can transform the data into different frequencies of electricity usage (x-axis) and how many times this frequency occurred (y-axis).\n\n<syntaxhighlight lang=\"Python\" line>\n###Line Plot to visualize electricity usage over time\npx.line(df, y='usage_kwh',\n        title='Usage of Electricity over 2 Weeks',\n        labels={'start_date': 'Date', 'usage_kwh': 'Usage (KWh)'}) ## uses the data from \"start_date\" called \"Date\", and the data of \"usage_kwh\" called \"usage (KwH)\"\n</syntaxhighlight>\n\n[[File:Figure 1.png|700px|center|]]\n<small>Figure 1: Line Plot visualizing electricity usage over time</small>\n\n<syntaxhighlight lang=\"Python\" line>\n###Scatter plot to visualize the number of times certain frequencies occurred\nfrom numpy.fft import rfft, rfftfreq ## imports the needed fast Fourier functions from the numpy package\n\ntransform = np.abs(rfft(df['usage_kwh'])) ## transforms into frequencies\nfrequencies = rfftfreq(df['usage_kwh'].size, d=15 * 60) ## fits the result into an array, d=15*60 determines that the time intervall is 15 minutes (15 * 60 seconds)\n\nn = 100 ##plots the first 100 frequencies\npx.line(x=frequencies[:n], y=transform[:n], markers=True,\n           title='Magnitude of Frequencies',\n           labels={'x': 'Frequency', 'y': 'Magnitude'}) ## creates plot with n=100 frequencies\n</syntaxhighlight>\n\n[[File:scatter plot.png|700px|center|]]\n<small>Figure 2: Scatter plot visualizing the number of times a certain frequency of electricity usage occurred</small>\n\n==Statistical Analysis==\nThere are a multitude of statistical analysis methods that can be used to analyze time series data. While simple statistical tests like the t-test, ANOVA, and regression analysis can be used with time series data to identify relationships and dependencies in the data, there are also more specific methods to analyze time series data.\n\n===Decomposition===\nTime series data and be decomposed into three components: trend, seasonality, and residuals. The trend represents the long-term direction of the data, the seasonality represents the repeating patterns in the data, and the residual represents the noise in the data. Decomposing the data in this way can help to better understand the underlying structure of the data and identify any patterns or trends.\n\n<syntaxhighlight lang=\"Python\" line>\nfrom statsmodels.tsa.seasonal import seasonal_decompose ## imports the necessary functions from statsmodel\n\ndecomp = seasonal_decompose(df['usage_kwh'], model='additive', period=15)\n## creates decomp which is the result of decomposing the electricity ##usage data to analyze seasonality, trend, and residuals. \"Period= ##15\" indicates that data was collected every 15 minutes and ##\"model=addtiive\" makes the assumption that the three components add ##up linearly.\n\ndf1 = pd.DataFrame({'value': decomp.trend, 'name': 'trend'}) ## analyzes trend, so long-term direction\ndf2 = pd.DataFrame({'value': decomp.seasonal, 'name': 'seasonal'}) ##analyzes seasonal changes, so repeating patterns\ndf3 = pd.DataFrame({'value': decomp.resid, 'name': 'resid'}) ## analyzes residuals, so noise\n\nfull_df = pd.concat([df1, df2, df3]) ## combines all three data frames\nfig = px.line(full_df, y='value', facet_row='name', title='Decomposition of electricity usage') ## creates plot\nfig.update_layout(height=1000) ## adapts height so that all three plots can be seen\n</syntaxhighlight>\n\n\n[[File:decomposition 1.png|700px|center|]]\n[[File:decomposition 2.png|700px|center|]]\n[[File:decomposition 3.png|700px|center|]]\n<small>Figure 3: The graphs for the decomposition of trend, seasonality, and the residuals respectively.</small>\n\nNo long-term direction or seasonal patterns can be detected. But we can see that there is a large variance in the residuals, so quite a lot of noise.\n\n===Moving Average===\nWhen there is a lot of noise in the data, it can be useful to calculate the moving (or rolling) average. The moving average is a statistical measure that calculates the average of a window around a data point. It smooths out the data and helps identify patterns or trends that may not be immediately apparent.\n\n<syntaxhighlight lang=\"Python\" line>\ndf_full = pd.concat([\n    pd.DataFrame({'value': df['usage_kwh'], 'name': 'raw'}), ## creates a dataframe the actual residuals\n    pd.DataFrame({'value': df['usage_kwh'].rolling(window=24*4).mean(), 'name': 'day'}) ## creates a dataframe with the daily means (1-day time window, 24 hours times 4 measurements per hour (every 15 min)\n]) ##the concat command combines these two dataframes\npx.line(df_full, y='value', color='name',\n        title='Usage vs. a 1-day moving average of usage',\n        labels={'start_date': 'Date', 'value': 'Usage (KWh)'})\n</syntaxhighlight>\n\n[[File:MA plot.png|700px|center|]]\n<small>Figure 4: Electricity Usage with and without rolling average calculations</small>\n\nWe can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.api as sm ## needed to use the autocorrelation function\nautocorr = sm.tsa.acf(df['usage_kwh'], nlags=24*4*2)## determines #autocorrelation with a lag of 15 minutes over 2 days (24 hours * 4 (every 15 min) * 2 for two days) \nsteps = np. arange (0, len(autocorr) * 15, 15) / 60 ## produces an ##array of the length of the autocorrelation data times 15 (so per #minute) and indicates that each lag is 15 minutes apart. By dividing it by 60, the values are converted from minutes to hours.\npx.line(x=steps, y=autocorr, markers=True,\n        title='Autocorrelation of electricity usage',\n        labels={'x': 'Lag (hours)', 'y': 'Correlation'}) ## creates #plot of the autocorrelation function\n</syntaxhighlight>\n[[File:ACF plot.png|700px|center|]]\n<small>Figure 5: Autocorrelation of electricity usage over two days</small>\n\nThe autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)\n\nfig = plot_acf(df['usage_kwh'], alpha=.05, lags=24*4*2) ## creates a plot for the autocorrelation function of the electricity usage for two days (24*4 measurements per day (4 per hour) times 2)\nlabel_range = np.arange(0, len(steps), 24*2) ## sets the range for the  days of the label\nplt.xticks(ticks=label_range, labels=[x*15/60 for x in label_range]) ## determines the number of ticks on x axis\nplt.xlabel('Lag (hours)') ## title x axis\nplt.ylabel('Autocorrelation of electricity usage with confidence interval') ## title y axis\nplt.title('') ## no plot title\nplt.ylim((-.25, 1.)) ## sets the limit of the y axis\nfig.show()\n</syntaxhighlight>\n[[File:ACF conf.png|700px|center|]]\n<small>Figure 6: Autocorrelation of electricity usage over two days</small>\n\nValues in the shaded region are statistically not significant, so there is a chance higher than 5% that there is no autocorrelation.\n\n===Detecting and Replacing Outliers===\nIn time series data, there are often outliers skewing the distributions, trends, or periodicity. There are multiple approaches to detecting and dealing with outliers in time series data. We will have a look at detecting outliers first: 1. Distribution-based outlier detection: This detection method relies on the assumption that the data follows a certain known distribution. Then all points outside this distribution are considered outliers. (Hoogendoorn & Funk 2018) 2. Distance-based outlier detection: This method computes the distance from one point to all other points. A point is considered \u201cclose\u201d to another point when the distance between them is below a set threshold. Then, a point is considered an outlier if the fraction of points deemed close to that point is below a threshold. (Hoogendoorn & Funk 2018) 3. Local outlier factor (LOF): The local outlier factor (LOF) is a measure of how anomalous an object is within a dataset, based on the density of its local neighborhood. The LOF is computed by comparing the density of an object's neighborhood to the densities of the neighborhoods of its nearest neighbors. If the density of an object's neighborhood is significantly lower than the densities of its neighbors' neighborhoods, then it is considered an outlier (Hoogendoorn & Funk 2018).\n\n1. is best to use when you have an idea of the distribution of your data, ideally if the data is normally distributed. This is especially the case for very large datasets.\n\n2. Is best when you expect outliers spread across the distribution, but you don't know the distribution.\n\n3. Is best when you want to identify outliers in clusters or in varying densities because you compare the data points to their neighbors. To decide, you can assess the distribution visually. For a better overview of the different approaches to handling outliers in general, see [[Outlier_Detection_in_Python|here]].\nIn general, many outlier detection methods are available ready-to-use in numerous python packages. This is an example of using the local outlier factor from the package scikit-learn:\n\n<syntaxhighlight lang=\"Python\" line>\nimport matplotlib.pyplot as plt ## imports the necessary packages for visualization.\nfrom sklearn.neighbors import LocalOutlierFactor ## imports the function for the local outlier function \n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=.03)## considers 20 neighboured data points and expects 3% of the data to be outliers (see contamination)\nprediction = lof.fit_predict(df['usage_kwh'].to_numpy().reshape(-1, 1))## transforms the electricity usage columns to a NumPy array (\"to_numpy()\") and reshapes it to a single column (\"reshape (-1, 1)\"), fit.predict then predicts the outliers (outlier if the prediction = -1)\ndf['outlier'] = prediction == -1 ## creates a new column in the initial dataframe called an outlier, with true or false, depending on whether it is an outlier or not\n\noutliers = df[df['outlier']] ## creates  a column where only outliers are selected\n\nplt.plot(df['usage_kwh']) ## plots the dataframe\nplt.scatter(x=outliers.index, y=outliers['usage_kwh'], color='tab:red', marker='x') ## creates a scatter plot with the outliers, marked with a red x\nplt.title('Outliers in the dataset (marked in red)') ## title of the plot\nplt.xlabel('Date') ## titlex axis\nplt.ylabel('Usage (KWh)')## title y axis\nplt.show()\n</syntaxhighlight>\n[[File:outlier plot.png|700px|center|]]\n<small>Figure 7: Outlier in the chosen part of the dataset (marked with red cross)</small>\n\nIn this case, these are probably false-positive outliers. The dataset is already pretty clean and does likely not contain many outliers.\n\nThe imputation of missing values is the next challenge after detecting outliers. The naive solution to this problem is to replace a missing point with the mean of all points. While this approach will not skew the data distribution significantly, the imputed values might be far off from the actual value and make messy graphics. An alternative is applying regression models that use predictive models to estimate missing points.\n\nA combined approach to both detects and replaces outliers is the Kalman Filter, which predicts new points based on the previous points and estimates a noise measure for new points. Thus, when an anomaly is detected, it is replaced by the prediction from the model using historical data (Hoogendoorn & Funk 2018).\n\n===Forecasting Time Series Data===\n\nForecasting time series data can be of tremendous value in information gain. Since this is a large topic, this article will only touch on one method for forecasting: AutoRegressive Integrated Moving Average models, or ARIMA for short. It consists of three parts:\n\n* The autoregressive (AR) component, which models the autocorrelation of the time series\n* The integrated (I) component, which models the non-stationary bits of the time series\n* The moving average (MA) component, which models the noise or error in the time series.\n\nEach component requires an appropriate selection of parameters. Choosing the right parameters can yield a powerful model. However, finding good values of the parameters can be difficult and requires further complicated techniques.\n\n<syntaxhighlight lang=\"Python\" line>\nimport statsmodels.api as sm ## needed to employ ARIMA\n\n# Fit the model on all data except for the last day (last 96 measurements). Order refers to the order of the different processes. The three numbers refer to the order of autoregression, the order of integration, and the order of the moving average process, respectively. With this information, Python can correct these issues and therefore optimize the forecasting results. freq='15T' indicates that the data has been collected every 15 minutes. \nmodel = sm.tsa.ARIMA(df['usage_kwh'].iloc[:-96], order=(1, 0, 0), freq='15T')\nmodel_fit = model.fit() ## estimates the best model\n\n# Predict how the last day is going to be\nforecast = model_fit.forecast(96)\n\nfull_df = pd.concat([\n    pd.DataFrame({'value': forecast, 'name': 'forecast'}),\n    pd.DataFrame({'value': df['usage_kwh'].iloc[-192:], 'name': 'existing'})## -192 to get the data for the last two days\n])\n\npx.line(full_df, y='value', color='name',\n        title='Last day of electricity usage with prediction of the next 24 hours',\n        labels={'index': 'Date', 'value': 'Usage (KWh)'}) ##produce plot\n</syntaxhighlight>\n\n[[File:forecast plot.png|900px|center|]]\n<small>Figure 8: Forecast of the last day of the dataset compared to the actual values</small>\n\nARIMA models struggle with seasonality and often predict the mean of a highly seasonal time series a few steps ahead, just like in this example. Seasonal ARIMA (SARIMA) models take into account seasonality and are thus more suited for datasets that exhibit a strong seasonality (Hoogendoorn & Funk 2018). This would then be the next step to use another model, but this more simple example should suffice to show how forecasting with time series data generally works.\n\n==References==\nHoogendoorn, Mark and Funk, Burkhardt (2018): Machine Learning for the Quantified Self: On the art of learning from sensory data. Cognitive Systems Monographs. Springer Cham.\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "2alil9uj1nwcfh3dpp6o775e55jjfq2"
                }
            },
            {
                "title": "Tips for digital lectures",
                "ns": "0",
                "id": "731",
                "revision": {
                    "id": "5965",
                    "parentid": "5952",
                    "timestamp": "2021-07-01T11:18:49Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11024",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || '''[[:Category:Software|Software]]''' || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nDigital lectures have been common for years, but especially during the COVID-19 crisis, we saw the challenges - but also benefits - that came with presenting content and having seminars in online formats. This entry provides a list of tips for both teachers and students in online sessions.\n\n== Goals ==\n* Improve engagement in online teaching.\n* Make more out of online sessions as a student.\n\n== Getting Started ==\n=== Tips for lecturers ===\n==== Tips for better (video) lectures and Zoom sessions ====\n* It is tempting to record yourself in front of the camera, talking over slides. While this may be useful for many lectures, sometimes, it might be easier for your students to understand and engage with your content when you '''record your lecture as a conversation''' with a relative, a colleague, or someone from your household to which you explain the lecture's content. Don't stage this - if you listener does not understand your lecture, you students probably won't, either. Let your listener ask questions, and explain things in more detail, if necessary.\n* Include '''interactive elements''' in your (pre-recorded) lectures, such as small tasks or questions for the students, which you come back to during or after the lecture, or in a Zoom session.\n* In Zoom calls, '''use [http://mentimeter.com Mentimeter]'' to facilitate participation for your students. You can create quizzes or ask for thoughts and let students rank items, but also show them slides or videos. You can also host Q&A sessions via Mentimeter, where students can upvote questions, which brings order into Q&A. However, a Q& A may best be supported by letting students raise their hands in the Zoom call so they can also speak and listen to fellow students, and not just read, type and click.\n* In Zoom sessions, '''use breakout rooms''' for the students to discuss questions that you posted, or questions that came from the students themselves. After some minutes, you can let the students share their thoughts, for example via Mentimeter. The breakout rooms also make some students keep their camera on, which may improve the atmosphere in the session.\n* In Zoom sessions, '''use gimmicks''' to maintain a positive atmosphere. Hold a contest about the best virtual background, or let students share their favorite snacks during a short break. It always helps when as many students as possible have their cameras turned on, so think of ways to achieve this.\n\n==== Some ways to jumpstart and foster an open and collaborative atmosphere when teaching a class or leading a team ====\n'''Quick personal meetings'''\nFrom time to time, give participants the opportunity to briefly (e.g. 5-10 mins.) talk with you one-on-one. You can offer an open Discord channel for this, or a Zoom room, in which you are approachable during pre-determined timeslots. Here, let students and team members join you individually. This fosters a more personal relationship and gives everyone the possibility for the type of interaction that is more spontaneous and less guarded (and therefore more personal). Give it the atmosphere similar to an exchange during a coffee break, while walking down a hallway, or similar situations to break the behaviour patterns that are typically prevalent during a class or a formal meeting.\n\n'''How is everyone feeling?''\nStart and end sessions with giving the participants the possibility to say a one-sentence statement about their current (subjective) personal status/mood/thougths/feelings without anyone commenting on them or referring to them (in the sense of a non-judgmental environment). The participants should be free to say whatever they feel like (it is not a round of feedback on the subject matter or the class, but more a statement of \"where each person is\" at the moment).\nThis requires a certain amount of trust and openness in the group and needs a bit of getting used to (risk-taking).\n\n'''Personal object bring-along'''\nEncourage each participant - in a smaller seminar, or workshop - to bring a personal object into the class, or meeting, and ask them to place it so that everyone in the digital room can see it. Encourage to choose a different object sponanteously  each time based on instincts (e.g. while they are preparing for the meeting or while they are leaving their place to attend the class). These objects can foster dialogue and enjoyment as well as express the person's attitude and mood which are vital clues to socially connect - especially in online meetings (but also in in-person gatherings). It can be a good starting point when getting to know new people. Optionally, you can introduce a game variant (that should be non-competitive) by asking participants to take into consideration previously chosen objects introduced by other participants - in that way reacting to them.\n\n\n==== Wiki entries on Skills & Tools that can support learning and teaching ====\n* [[Flipped Classroom]], a teaching format that is especially interesting for digital times.\n* [[Digital Energizers]], to gain energy in digital sessions.\n* [[Check In]], to provide a good atmosphere for smaller groups.\n* [[Flashlight]], to make sure everyone's on the same page in smaller groups.\n* [[Digital Workshop Facilitation]], an overview on general learnings about digital workshops and sessions. \n* [[Online Conduct]], a few ideas on how to improve conduct in shared online spaces.\n\n\n=== Tips for Students ===\nBeing a student in home office times is rough, and even when the COVID pandemic is over, you will still encounter online lectures, seminars, conferences and MOOCs. To some extent, luckily, being at home instead of in a lecture hall also has its merits. That being said, the following are some (not so) obvious tips for you to improve your online learning experience.\n\n* '''Always have something to drink''' (water/tee/coffee/mate/...) or to a snack with you. You could also have proper meals while listening to someone talk, or even prepare food, but be aware that this might distract you and result in you not really taking away much from the online session.\n* Wear comfortable clothes and '''create a nice atmosphere in your room''' or wherever you study at the moment. In Home Office situations, you do not necessarily have to sit at your desk, although it can create a more 'official' atmosphere and stimulate learning and engagement. For some, however, other spatial arrangements work, too, so find the spot in your apartment - or even outside - that does not distract you, but supports a comfortable learning experience.\n* '''Put your phone away from you''' so you do not get easily distracted from messages or tempted to do something else. We are all used to having second or third screens, and it is already tempting to browse on the computer while in a Zoom session - so don't burden yourself with the temptation of another device. Check your phone regularly, if you want to, but do so in breaks between sessions.\n* Make sure you '''have a system to organize yourself''' that works for you and helps you to structure your everyday life during online classes. For this purpose, we can highly recommend [[Notion]], which allows you to organize pretty much everything, presumed that you prefer digital documentation. For analoguous notes, you might want to consider a DIN A3 sheet to summarize your courses, or a specific course's content.\n* Try to use the breaks in between classes to do things that do not require much brain power, such as walking outside, doing the laundry or dishes, talking to your room mate etc. Especially some '''physical activity''' - a walk, a jog, yoga, or some workout at home - can do wonders between online sessions.\n* Make sure you \"zoom\" away from your screen every now and then. Your eyes aren't too happy to be kept in the same position for too long, and sometimes this might even result in a headache. So make sure to '''sometimes look out of the window''', or - as mentioned above - go into nature without any screens.\n* If you have to read a lot for your studies, '''consider printing texts on paper'''. Reading on paper can be a nice alternative to reading on the screen. Or - fancy - take a walk to the library and lend the book.\n* Though we all try to avoid too many distractions, using the chat function in Zoom may help to refocus your attention from time to time. If you write a short message to your friend taking the same seminar, exchanging on how you are, and how you perceive the class, you emulate the classroom experience to some extent, which can help an follow the content afterwards more easily. We cannot stay focused in 4 Zoom sessions \u00e0 90 min. a day, so it is ok to allow us '''some pleasant distractions every now and then'''.\n\n==== Wiki Entries that can help students to learn in digital times ====\n* [[Learning for exams]], which includes tips that are still - or even more - valid for digital exams.\n* [[Mindfulness]], some guidance on how to clear your mind.\n* [[Overcoming Exam Anxiety]], since the last thing you need in digital teaching is to be afraid of the exam.\n* [[Notion]], which is an online workspace that can be of great help to manage your own work.\n\n\n== Further Information and guidance for digital teaching ==\n* Of course, the Sustainability Methods Wiki, both as a resource for content and for its Skills & Tools section, is relevant beyond the aforementioned entries.\n* A selection of available textbooks from Cambridge University: [https://www.cambridge.org/core/what-we-publish/textbooks?fbclid=IwAR0c8XK_gSH9i5xc2u0O-RJZNfGN16OtR-qvzqkOhfFmJx6Eup2UWVWyDd0 1], [http://cambridge.org/core/what-we-publish/textbooks?fbclid=IwAR0c8XK_gSH9i5xc2u0O-RJZNfGN16OtR-qvzqkOhfFmJx6Eup2UWVWyDd0 2]\n* [https://teach.kiron.ngo/ Tips on digital teaching by Kiron University]\n* [https://hochschuldidaktik-online.de/checkliste-virtuelle-prasenzlehre/ A Checklist for virtual teaching] from ''hochschul-didaktik''\n* [https://www.go.twigeducation.com/covid19-global Twig Education] - Bespoke curriculum-aligned videos, supporting lesson materials and hands-on activities that you can do with limited resources. Ages 4-7, 7-11 and 11-16.\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Productivity Tools]]\n[[Category:Software]]\n[[Category:Team Size 1]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors| authors]] of this entry are Christopher Franz, Matteo Ramin, Elisabeth Frank, Carlo Kr\u00fcgermeier and Iman Aoulkadi."
                    },
                    "sha1": "583luoayjfriut7ktjue2tnutiay272"
                }
            },
            {
                "title": "To Rule And To Measure",
                "ns": "0",
                "id": "527",
                "revision": {
                    "id": "6512",
                    "parentid": "6342",
                    "timestamp": "2022-01-24T08:23:08Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26682",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[To Rule And To Measure (German)]].\n\n'''In short:''' This entry illustrates different types of data units and approaches to measuring our world.\n\n== Introduction ==\nThroughout history, humans have developed myriads of tools and units that help measure, categorize, compare, analyze and understand things, processes and people - in short, the world. '''We have measured space and time, sound and light, the movement of atoms and molecules, and there is no end in sight'''. You use measurements every day - stepping onto the weighing scale in the morning (don't do that!), calculating the time you need to get to university, the amount of sugar you need for the cake, or the money you spent last weekend. You may even (unconsciously) reflect on the amount of fun different potential activities might be, or think about your level of satisfaction with your last meal. \n\nSome units of measurement are very strongly connected to the physical world, such as measuring weights or lengths, despite the existence different units to do so. Others are very much socially constructed, such as the Human Development Index (HDI), which puts a number to the whole quality of life. And then, there is a wide range in between, like the Beaufort scale which does measure something physical (wind), but is quite an arbitrary form of doing so. \n\nThe same can be said for tools that enable the translation of physical or social phenomena into such units: some, like a ruler, are very much related to physical properties, while others, such as IQ tests, are based on so many assumptions about how intelligence should or could be measured, that there is no denying strong normative implications. There is no judgement to be made here, but one should be aware of the fact that units of measurement may imply specific assumptions about the world and how much we can know about it. There are things we can measure, things we can measure but are not necessarily like that, and things we cannot measure at all.\n\n'''The following entry provides examples on units of measurement and tools to measure.''' There will be more additions over time. All of these represent major breakthroughs and have consolidated over time. All these diverse data formats and tools should strengthen our understanding that we look at parts of the picture. The emergence of new measures does not only unlock new knowledge, it also highlights the limitations of our total knowledge. As [[Bias and Critical Thinking|critical realism]] claims, we may never be able to fully understand all aspects of all mechanisms of reality that are real. For instance, many institutions are riddled by [[Glossary|tacit]] knowledge that we only now start to slowly unlock. We thus have to acknowledge that the examples given here have limitations, and that there are many forms of knowledge that cannot be tamed or constructed into numbers. \n\nTo this end, this entry is not only an example of diverse possibilities, but also highlights how several different data formats may even contradict other forms of knowledge, or may even hamper a different but necessary understanding of certain mechanisms. The 20th century was in many aspects dominated by numbers, which informed policy and management, and served as a baseline for diverse aspects of emerging [[Scientific methods and societal paradigms|societal paradigms]]. Especially economic thinking was widely based on such understanding, and equally was positivism mostly built around numbers and measures. '''What is often overseen to this end is the fact that these numbers are not objective, but also normative.''' For instance, measures of IQ are not only deeply constructed, but also deeply contested. While much of empirical research still claims to create objective knowledge, it becomes increasingly clear that such claims are often falsifying each other. If we however clearly recognise the limitations of all these diverse measure, we may also be better enabled to understand the value of these measures.\n\nPlease refer to the [https://sustainabilitymethods.org/index.php/Data_formats entry on Data formats] for more general information on how data can be formatted.\n\n\n== Data units ==\n==== Celsius vs Fahrenheit vs Kelvin ====\nCelsius, Fahrenheit and Kelvin represent the most common temperature scales used today. While Celsius is used in most places in the world, the USA and associated territories still use the Fahrenheit scale. Kelvin is based directly on Celsius and is most commonly used by scientists to communicate their results (e.g. when speaking about a temperature increase of 2K), but not really used in everyday practice in any country.\n\n[[File:To Rule and To Measure - Celcius.png|300px|thumb|right|'''Fahrenheit, Celsius and Kelvin.''' Source: [https://cryo.gsfc.nasa.gov/introduction/temp_scales.html NASA]]]\n\nIn Celsius, water freezes at (~) 0\u00b0C and boils at (~) 100\u00b0C. -273,15\u00b0C represents the lowest possible temperature a gas can (theoretically) reach, and can be translated into 0 Kelvin. Kelvin is thus always 273,15 higher than Celsius. The unit of Fahrenheit is based on a thermometer using a mixture of water, ice and ammonium chloride - the minimum temperature that could be reached with this mixture was set as 0\u00b0F. Fahrenheit can be translated into Celsius as follows: \u00b0F = (\u00b0C)x(9/5) + 32\n<br/>\n(Source: https://cryo.gsfc.nasa.gov/introduction/temp_scales.html)\n\n\n==== Richter Scale ====\nThe Richter Scale is used to measure the strength of Earthquakes. It was developed in 1935 by US American seismologists Richter and Gutenberg. It is based on the logarithm of the amplitude of the largest seismic wave of an earthquake event. Each increase of one unit on the scale signifies a tenfold increase in the magnitude of an earthquake, and a 31 times higher amount of energy released. While today's technologies can measure seismic waves below what was possible with the Richter scale, and are better to measure very strong earthquakes, the Richter scale is still often used, especially in the media. (Source: britannica.com/science/Richter-scale)\n[[File:To Rule and To Measure - Richter Scale.png|400px|thumb|center|'''The Richter Scale.''' Source: [https://www.britannica.com/science/Richter-scale Britannica]]]\n\n\n==== Beaufort scale ====\nThe Beaufort Wind Scale was created by British Admiral Sir Francis Beaufort in 1805. It allows to assess the speed of winds based on their effects on both land and oceans. It goes from 0 to 12, with 12 being the strongest winds (Source: weather.gov./mfl/beaufort)\n\n[[File:To Rule and To Measure - Beaufort.png|600px|thumb|center|'''The Beaufort Scale.''' Source: [https://en.wikipedia.org/wiki/Beaufort_scale Wikipedia]]]\n\n\n==== Decibel ====\nDecibel (dB), as well as Bel (1 Bel = 10dB) are dimensionless physical units that refer to the relation between two measurements (measured and reference) on a logarithmic scale. They were first publicized in 1931 and still used in various fields of application; however, dB still most commonly referred to when speaking about the loudness of sounds. However, it is important to note that dB does not indicate the absolute loudness of something, but only the logarithmic (not linear!) relation to a reference point, which, for sound, is the point where the human ear can no longer perceive any sound. (Source: https://www.elektronik-kompendium.de/sites/grd/0304011.htm, https://de.wikipedia.org/wiki/Bel_(Einheit) ) <br/>\nFor this usage, some examples include:\n* 20dB - soft whispering\n* 50dB - normal conversation\n* 110dB - rock concert\n* 140dB - rifle shot\n* 190dB - deadly loudness\n\n\n==== Happiness ====\nMeasuring the well-being of people and progress thereof has been subject of debates for a long time. While GDP is the most widespread approach to measure human (presumable) well-being on a nation-wide or global level, there is an increasing demand for alternative indicators. One of those under discussion is happiness, with Bhutan being the first and so far only country to prioritize 'Gross National Happiness' in their decisions. \n<br/>\nHappiness - being a very subjective unit of measurement - is difficult to evaluate. The 'World Happiness Report' attempts to do so over 200 pages. It has been published regularly since 2013 and uses representative, self-reported data from (mostly) the [[https://www.gallup.de/182606/gallup-world-poll.aspx Gallup World Poll]]. Here, participants answer questions about over 100 aspects of life, including their average life evaluation on a scale from 1-10. The World Happiness Report, then, tries to understand how different aspects of life explain variations in this self-reported life evaluation. In the 2020 report, for example, they focus on how the elements GDP, Social Support, Life Expectancy, Freedom to make life choices, Generosity and Perception of orruption influence it. Other elements that are analyzed are inequality, worry and anger, discrimination, income, trust in society, systems and the policy, intimacy and many others. Overall, the report states Finland, Denmark and Switzerland to be the countries with the happiest people (on average), while Zimbabwe, South Sudan and Afghanistan mark the bottom three. You can find out more about this report on the [https://worldhappiness.report/ respective website.]\n\n[[File:To Rule and To Measure - Happiness.png|500px|thumb|center|'''Results of the World Happiness Report 2020'''. Source: World Happiness Report 2020, p.30.]]\n\nThe image shows the average global happiness between 2017 and 2019. Life evaluation is measured by the Cantril ladder question that asks respondents to evaluate the quality of their lives on an 11-point ladder scale, with the bottom step of the ladder (0) being the worst possible life they can imagine and the highest step (10) being the best possible life. Positive affect is measured by a two-item index asking respondents whether or not they frequently experienced (1) enjoyment and (2) laughter on the day before the survey. Negative affect is measured by a three-item index asking respondents whether they frequently experienced (1) worry, (2) sadness, and (3) anger on the day before the survey. <br/>(Source: World Happiness Report 2020, p.30.)\n\n\n==== IQ ====\nIQ, short for intelligence quotient, is a common metric to indicate and compare human intelligence. There is no standard definition of intelligence and a variety of normalized IQ tests exist. Their choice needs to be considered when speaking about the IQ value itself. Generally, an IQ test covers questions from different fields, including general logical understanding, understanding of language, general knowledge, mathematical skills and others. After conducting an IQ test, each participant receives a numeric value to indicate his or her intelligence.\n\nThe average result is defined to be 100, with a standard deviation of 15. Because of this, IQ test norms have to be updated regularly. This was not done in the 20th century, leading to ever-increasing average IQ values, which has been called the Flynn effect and cannot be explained. Commonly, anyone with an IQ <70 is considered witless, while anyone with >130 is considered highly talented. Importantly however, there is severe scientific criticism concerning the IQ, since a person's results to an IQ test are very dependent on his or her daily form, and the test does not cover all relevant aspects of intelligence. Also, the societal discourse around the IQ is problematic since it might lead to wrong assumptions and inferences about people's abilities and value. (Source: https://flexikon.doccheck.com/de/Intelligenzquotient, https://de.wikipedia.org/wiki/Intelligenzquotient)\n\n\n==== HDI ====\nThe Human Development Index (HDI) is an index published by the United Nations since 1990. It calculates a value between 0 and 1 based on three aspects of life: life expectancy at birth, quality of education as measured in the number of years a person visits school, and income per capita. Each country can reach a value between 0 and 1, with 1 being the best possible result. In the 2019 report, the countries with the highest HDI were Norway, Switzerland and Ireland which all reached values above 0,94, whereas the bottom was marked by Chad, the Central African Republic and Niger.\n\nThe HDI is a fairly simple measure, focusing only on those three elements. There is also the IHDI, which acknowledges and and respects inequalities within these criteria (with the same top and bottom three). The 2019 report revolved around inequalilties within and across nations, indicating an increasing recognition of more integrated evaluation of the HDI. For equality, the GINI coefficient is also of relevance, indicating financial (in)equality within a nation  - for more on this, see [https://www.investopedia.com/terms/g/gini-index.asp here].\n(Source: https://en.wikipedia.org/wiki/Human_Development_Index, http://hdr.undp.org/en/2019-report)\n\n[[File:To Rule and To Measure - HDI.png|500px|thumb|center|'''The global distribution of the Human Development Index from the 2019 report.''' The darkest areas represent values above 0.8, then >0.7, >0.55 and >0.35. Source: [https://en.wikipedia.org/wiki/Human_Development_Index Wikipedia]]]\n\n\n==== United States customary units vs the Metric System ====\nTo this day, the United States (as well as Liberia and Myanmar) use a system of measurements (the USC) that differentiates from the Metric System that the majority of countries use. The USC is based on English units and was formalized in 1832, whereas Britain went on to use the Imperial and later the Metric system. The US rejected any metric system, stating that these were not 'acceptable to the Lord'. In 1975, the Metric Conversion Act made the common Metric System the preferred system for trade and commerce, and it is the standard in, among other sectors, science and military, but the USC is still commonly used in everyday practice in the US, especially in consumer products and industrial manufacturing.\nSome examples include:\n* Lenght and area:\n** 1 inch = 2,54cm\n** 1 foot = 30,48cm\n** 1 yard = 91,44cm\n** 1 mile = 1,609344 km\n** 1 acre = 4046,873 m\u00b2\n\n* Mass and weight:\n** 1 ounce = 28,35g\n** 1 pound = 453,59g\n(Source: https://en.wikipedia.org/wiki/United_States_customary_units)\n\n\n==== The weight of the soul (21 grams) ====\nIn 1907, physician Duncan MacDougall attempted to prove the existence of the human soul - scientifically. He chose six individuals which were about to die and placed them on a precise scale. The moment they died, he claimed in the publication of his results, they lost 21 grams in body weight, which were not explainable by any other processes other than the test subjects losing their soul as they died. Naturally, there was criticism to this study, and other explanations were raised. However, the idea that the human soul weighs 21 grams - and therefore exists - has been popularized and even influenced movie director Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu, who named a 2003 movie just that: \"21 Grams\". To this day, the experiment raises the question what science can, and cannot, know. (Source: https://www.thevintagenews.com/2018/11/30/21-grams-experiment/)\n\n\n== Tools and Units ==\n==== Knots for depth of water ====\nDepth sounding is a method to measure the depth of water. It was already applied in Ancient Greece and Rome, and still finds use for hobby fishing and as a backup for today's standard echo sounding in case of malfunction.\nIn Depth Sounding (which has nothing to do with 'sound' as in 'noise'), a thin rope is combined with a plummet, i.e. a weight, that hangs at the end of it. This rope is then dropped into the water until it reaches the ground surface. By looking at marks made at the rope, one can quickly measure the depth of the water.\nInterestingly, the author Mark Twain got his artist's name from this technique. In Depth Sounding, the depth of the water can be indicated by saying \"By the mark\" and adding the number that is met by the water surface. In Mississippi in the 1850s, the number 'two' would be called 'twain', so when the leadsmen measured the depth of the river and it was two fathoms (one fathom is 1,8288m), they would say \"by the mark twain\". (Source: https://en.wikipedia.org/wiki/Depth_sounding)\n\n\n==== Thermometer ====\nA Thermometer is based on the fact that liquids (and gases) expand and contract depending on the temperature. While the first steps towards modern thermometers were made in Ancient Greece and later again during the 16th and 17th century, major breakthroughs were the thermometers invented by Fahrenheit in 1714 and Celsius in 1742 (see Celsius vs Fahrenheit vs Kelvin).\nThere have been different scales, units and liquids (or gases) used in thermometers throughout time; today, most classic thermometers rely on mercury, and despite the differences in measuring units, there is an internationally agreed range of temperatures to be displayed by a thermometer. (Source: https://en.wikipedia.org/wiki/Thermometer)\n\n\n==== Sextant ====\n[[File:To Rule and To Measure - Sextant.png|250px|thumb|left|'''A Sextant'''. Source: [https://www.brighthubengineering.com/seafaring/31615-marine-navigational-equipment-the-sextant Bright Hub Engineering]]]\nA Sextant is a nautical tool to identify a ship's position. It is based on measuring the angular distance between astronomical objects and was invented by Newton and Hadley around 1700. A Sextant consists of a circular sector with a mirror, a half-mirror and a telescope. One looks at one specific point (at a star, the sun or the horizon) through the telescope and moves the mirror until a second reference point is reflected onto the half-mirror, which makes the objects ultimately overlap. Based on the movement of the mirror, one can read off the angle on the scale of the sextant. The angle between a star above the horizon, in combination with the time of day, helps identify the geographical latitude with the help of astronomical tables. The angle between the moon and a star leads to the geographical altitude.\n(Sources: https://www.brighthubengineering.com/seafaring/31615-marine-navigational-equipment-the-sextant/, https://www.spektrum.de/lexikon/physik/sextant/13240)\n\n\n==== Geodetic measurement - Trigonometry ====\n[[File:To Rule and To Measure - Trigonometry.png|400px|thumb|right|'''The Six Trigonometric Functions.''' Source: [http://www.gradeamathhelp.com/image-files/trigonometry-functions.gif Grade A Math Help]]]\nTrigonometry is one of the oldest forms of calculating and measuring, with its origins spanning across Sumer, Babylon, Greece, Egypt, India and other early civilizations up until 4000 years ago. The word means \"triangle measuring\" and refers to the usage of side lenghts and angles of triangles for calculations. By knowing the sizes of enough sides and angles of a triangle, it is possible to calculate the size or length of the remaining sides and angles. Trigonometry has been used historically in geodetic measurements, astronomy, navigation and many other fields of application, allowing for the development of many relevant technologies and inventions throughout history.\n\nWhile trigonometry can become quite elaborate, the most basic planar trigonometry as we know it today is based on the work of Rheticus and his student Otho in the 16th Century, and includes six fundamental trigonometric functions: sinus, cosinus, tangens, as well as cosecant, secant and cotangent. By knowing two variables of the triangle, the third can be calculated with the use of a calculator, or historically with a table book or slipstick. (Source: https://math.wikia.org/wiki/History_of_trigonometry)\n\n\n==== LiDAR ====\n[[File:To Rule and To Measure - LiDAR.png|300px|thumb|right|'''The principle of LiDAR technology.''' Source: [https://archaeologicalphotorelief.files.wordpress.com/2014/07/how_lidar_works.jpg Archeological Photo Relief]]]\nLiDAR is short for \"light detection and ranging\". It is a technology that measures the distance between LiDAR sensor and any object in its visible range. LiDAR can be applied in plane or satellite based measuring, as well as near the surface. The sensor sends out a light impulse with a laser and detects the light that is reflected from each object. Based on the time the light needed, it calculates the distance to the object. The objects may be solid, but can also be liquid or even gaseous. This way, the LiDAR sensor is able to construct a 3D image of, for example, the Earth's atmosphere or surface. LiDAR technology can help create maps and has recently be used to identify aerosols in the air, temperature profiles or trace gas concentrations. (Source: https://www.dlr.de/rd/desktopdefault.aspx/tabid-5626/9178_read-17527/) \n\n\n==== Chinese counting with fingers ====\n[[File:To Rule and To Measure - Chinese Finger Counting.png|400px|thumb|center|'''Chinese Finger Counting'''. Source: [https://en.wikipedia.org/wiki/Chinese_number_gestures Wikipedia]]]\nIn China, there is a system to indicate the numbers from 1-10 with only one hand (as opposed to the Western custom of indicating one unit per finger, thus only reaching five per hand). This system probably emerged because of the Chinese language variety, or also because of a wish to convey information privately. (Source: https://en.wikipedia.org/wiki/Chinese_number_gestures)\n\n\n==== The Ruler ====\n[[File:To Rule and To Measure - Ruler.png|300px|thumb|left|'''A Steel Ruler'''. Source: [https://en.wikipedia.org/wiki/Ruler#/media/File:Steel_ruler_closeup.jpg Wikipedia]]]\nRulers, also referred to as 'line gauges', are the archetype of measuring tools. They help identify the distance between points or the length of lines (which is the same, if you think about it), or can be used a an geometrical aid to determine whether a line is straight or bend, and to draw straight lines when necessary. Rulers are commonly made of wood, plastic, steel, flexible tape or such, depending on the field of application, user preference and necessary length. The latter can differ: ancient rulers had varying lengths, rulers that can be encountered in school typically measure 30 cm in length, and more flexible (sometimes foldable) rulers used in architecture, construction or other such fields often measure several meters. Historically, rulers were more commonly applied once the metric system (see above) was popularized from the late 18th Century on. However, the oldest preserved ruler (made of copper-alloy) dates back to 2650 BC at Nippur, Sumer, and rulers from Ivory were used in the Indus Valley before 1500 BC. (Source: https://en.wikipedia.org/wiki/Ruler)\n\n\n==== Space Telescope ====\n[[File:To Rule and To Measure - Hubble Space Telescope.png|250px|thumb|right|'''The Hubble Space Telescope'''. Source: [https://en.wikipedia.org/wiki/Space_telescope Wikipedia]]]\nA Space Telescope is a telescope in outer space. Different from from satellites that image Earth, it points into outer space in order to map or survey distant stars, planets and other astronomical objects. Space observations have the benefit not to be affected by filtering and distortions through Earth's atmosphere as well as light pollution on Earth. Therefore, they enable scientists to better observe space despite the difficulties to maintain and high costs to build space telescopes. (Source: https://en.wikipedia.org/wiki/Space_telescope)\n\n\n==== Gas and Liquid Chromatography ====\nGas Chromatography (GC) and Liquid Chromatography (LC) are the two main forms of Chromatography, which describes an analysis technique used to quantify compounds in a mixture, for example in quality control in industrial contexts, in forensics, or in environmental sciences. The term 'Chromatography' was coined by Russian botanist Tswett in 1906, whereas the method itself was first used in 1952. \n\nA Chromatography has two main components: a stationary phase, and a mobile phase which moves along the medium within the Chromatograph. For Gas Chromatography, the mobile phase consists of an inert gas, like helium or nitrogen, while the stationary phase consists of a liquid. For Liquid Chromatography, the mobile phase is a liquid and the stationary phase solid. The sample that is to be analyzed is inserted into these phases. Those compounds that interact more strongly with the stationary phase (\"like-dissolves-like\" rule, i.e. chemically similar compounds stay together) remain in this phase for a longer time, while those compounds that interact less with the stationary phase move with the mobile phase. The distribution of compounds inbetween both phases can be analyzed using different forms of detectors, leading to insights about the composition of the initial sample. (Source: http://www.chem.ucla.edu/~bacher/General/30BL/gc/theory.html, https://microbenotes.com/chromatography-principle-types-and-applications/)\n\n[[File:To Rule and To Measure - Chromatography.png|400px|thumb|center|'''The principle of Chromatography'''. Source: [https://microbenotes.com/chromatography-principle-types-and-applications/ Microbenotes]]]\n\n\n== Concluding remarks ==\nMany of these measures are often defined by researchers as scientific methods. We would disagree, because these measures do not create knowledge in itself, yet are often an important basis for data gathering and the structuring of the gathered knowledge. We have to acknowledge that there is a wealth of data available, and the here presented examples showcase the great diversity that highlight the challenge to integrate different data. This will be one of the main methodological challenges of the 21 century, and underlines both the importance of the recognition of qualitative methods as well as the deep normativity that is part of the diverse datasets that we have.\n\n\n== Further Information ==\n* [https://www.iqmindware.com/wiki/what-does-my-iq-score-mean IQ]: An explanation\n* [https://www.youtube.com/watch?v=7p2a9B35Xn0 IQ]: Answering the question if the IQ really measures how smart you are\n* [https://www.youtube.com/watch?v=7p2a9B35Xn0 IQ]: A critical reflection\n* [https://www.investopedia.com/terms/g/gdp.asp GDP]: A detailed article\n* [https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Measurements]: Reflecting upon different measurement systems across the globe\n* [http://hdr.undp.org/en/content/human-development-index-hdi The Human Development Index]: An alternative to the GDP\n* [https://www.investopedia.com/terms/g/gini-index.asp The GINI index]: A measure of inequality\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Christopher Franz and Henrik von Wehrden."
                    },
                    "sha1": "bk2uuq64pyocno6lw610viacuzquir5"
                }
            },
            {
                "title": "To Rule And To Measure (German)",
                "ns": "0",
                "id": "565",
                "revision": {
                    "id": "6790",
                    "parentid": "6513",
                    "timestamp": "2022-10-10T12:54:05Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "29808",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[To Rule And To Measure]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Dieser Eintrag illustriert verschiedene Ma\u00dfeinheiten und Messger\u00e4te vor.\n\n__TOC__\n==Einf\u00fchrung==\nIm Laufe der Geschichte haben die Menschen Myriaden von Werkzeugen und Einheiten entwickelt, die helfen, Dinge, Prozesse und Menschen - kurz gesagt, die Welt - zu messen, zu kategorisieren, zu vergleichen, zu analysieren und zu verstehen. '''Wir haben Raum und Zeit, Schall und Licht, die Bewegung von Atomen und Molek\u00fclen gemessen, und es ist kein Ende in Sicht'''. Wir alle verwenden diese Messwerkzeuge jeden Tag - wenn wir morgens auf die Waage treten (oder das besser bleiben lassen), wenn wir die Zeit berechnen, die wir brauchen, um zur Universit\u00e4t zu kommen, die Menge an Zucker, die wir f\u00fcr den Kuchen ben\u00f6tigen, oder das Geld, das wir dieses Wochenende ausgegeben haben. Vielleicht denken wir sogar (unbewusst) dar\u00fcber nach, wie viel Spa\u00df verschiedene m\u00f6gliche Aktivit\u00e4ten machen k\u00f6nnten, oder \u00fcberlegen, wie zufrieden wir mit unserer letzten Mahlzeit waren. \n\nEinige Ma\u00dfeinheiten sind sehr stark mit der physischen Welt verbunden, wie z.B. das Messen von Gewichten oder L\u00e4ngen, obwohl es verschiedene Einheiten daf\u00fcr gibt. Andere sind sehr stark sozial konstruiert, wie zum Beispiel der Human Development Index (HDI), der die gesamte Lebensqualit\u00e4t mit einer Zahl belegt. Und dann gibt es eine gro\u00dfe Bandbreite dazwischen, wie die Beaufort-Skala, die zwar etwas Physikalisches (Wind) misst, aber eine ziemlich willk\u00fcrliche Form ist, dies zu tun. \n\nDas Gleiche gilt f\u00fcr Werkzeuge, die es erm\u00f6glichen, physikalische oder soziale Ph\u00e4nomene in solche Einheiten zu \u00fcbersetzen: Einige, wie ein Lineal, sind sehr stark auf physikalische Eigenschaften bezogen, w\u00e4hrend andere, wie IQ-Tests, auf so vielen Annahmen dar\u00fcber beruhen, wie Intelligenz gemessen werden sollte oder k\u00f6nnte, dass starke normative Implikationen nicht zu leugnen sind. Das soll keine Bewertung darstellen, aber man sollte sich der Tatsache bewusst sein, dass Ma\u00dfeinheiten bestimmte Annahmen \u00fcber die Welt und dar\u00fcber, wie viel wir \u00fcber sie wissen k\u00f6nnen, implizieren k\u00f6nnen. Es gibt Dinge, die wir messen k\u00f6nnen, Dinge, die wir messen k\u00f6nnen, aber nicht unbedingt so sind, und Dinge, die wir \u00fcberhaupt nicht messen k\u00f6nnen.\n\n'''Der folgende Eintrag enth\u00e4lt Beispiele zu Ma\u00dfeinheiten und Messwerkzeugen.''' Im Laufe der Zeit wird es weitere Erg\u00e4nzungen geben. Sie alle stellen wichtige Durchbr\u00fcche dar und haben sich im Laufe der Zeit konsolidiert. All diese verschiedenen Datenformate und -werkzeuge sollten unser Verst\u00e4ndnis daf\u00fcr st\u00e4rken, dass wir Teile des Ganzen betrachten. Das Auftauchen neuer Ma\u00dfeinheiten und -werkzeuge erschlie\u00dft nicht nur neues Wissen, sondern zeigt auch die Grenzen unseres gesamten Wissens auf. Wie der [[Bias and Critical Thinking (German)|Kritische Realismus]] behauptet, werden wir vielleicht nie in der Lage sein, alle Aspekte aller Mechanismen der Realit\u00e4t vollst\u00e4ndig zu verstehen, die real sind. Zum Beispiel sind viele Institutionen von '[[Glossary|tacit knowledge]]' durchdrungen, das wir erst jetzt langsam erschlie\u00dfen. Wir m\u00fcssen also anerkennen, dass die hier angef\u00fchrten Beispiele Grenzen haben und dass es viele Formen von Wissen gibt, die sich nicht z\u00e4hmen oder in Zahlen fassen lassen. \n\nIn diesem Sinne ist dieser Beitrag nicht nur ein Beispiel f\u00fcr die vielf\u00e4ltigen M\u00f6glichkeiten, sondern zeigt auch auf, wie verschiedene Datenformate anderen Wissensformen sogar widersprechen k\u00f6nnen oder ein anderes, aber notwendiges Verst\u00e4ndnis bestimmter Mechanismen behindern k\u00f6nnen. Das 20. Jahrhundert war in vielerlei Hinsicht von Zahlen dominiert, die Politik und Management informierten und als Grundlage f\u00fcr verschiedene Aspekte sich entwickelnder [[Scientific methods and societal paradigms|Gesellschaftsparadigmen]] dienten. Insbesondere das \u00f6konomische Denken basierte weitgehend auf einem solchen Verst\u00e4ndnis, und ebenso war der Positivismus meist auf Zahlen und Ma\u00dfst\u00e4ben aufgebaut. '''Was dabei oft \u00fcbersehen wird, ist die Tatsache, dass diese Zahlen nicht objektiv, sondern auch normativ sind.''' So sind zum Beispiel Ma\u00dfe des IQ nicht nur zutiefst konstruiert, sondern auch zutiefst umstritten. W\u00e4hrend ein Gro\u00dfteil der empirischen Forschung immer noch den Anspruch erhebt, objektives Wissen zu schaffen, wird immer deutlicher, dass sich solche Anspr\u00fcche oft gegenseitig falsifizieren. Wenn wir jedoch die Grenzen all dieser verschiedenen Ma\u00dfe klar erkennen, sind wir vielleicht auch besser in der Lage, den Wert dieser Ma\u00dfe zu verstehen.\n\nF\u00fcr mehr Informationen dar\u00fcber, wie Daten formatiert werden k\u00f6nnen, ist der [https://sustainabilitymethods.org/index.php/Data_formats Wiki-Eintrag \u00fcber Datenformate] zu empfehlen.\n\n== Ma\u00dfeinheiten ==\n==== Celsius vs Fahrenheit vs Kelvin ====\nCelsius, Fahrenheit und Kelvin stellen die heute am h\u00e4ufigsten verwendeten Temperaturskalen dar. W\u00e4hrend Celsius an den meisten Orten der Welt verwendet wird, wird in den USA und den dazugeh\u00f6rigen Territorien noch die Fahrenheit-Skala verwendet. Kelvin basiert direkt auf Celsius und wird am h\u00e4ufigsten von Wissenschaftlern verwendet, um ihre Ergebnisse zu kommunizieren (z. B. wenn von einem Temperaturanstieg von 2 K die Rede ist), aber in der allt\u00e4glichen Praxis in keinem Land wirklich verwendet.\n\n[[File:To Rule and To Measure - Celcius.png|300px|thumb|right|'''Fahrenheit, Celsius und Kelvin.''' Quelle: [https://cryo.gsfc.nasa.gov/introduction/temp_scales.html NASA]]]\n\nIn Celsius gefriert Wasser bei (~) 0\u00b0C und siedet bei (~) 100\u00b0C. -273,15\u00b0C stellt die niedrigste Temperatur dar, die ein Gas (theoretisch) erreichen kann, und kann in 0 Kelvin \u00fcbersetzt werden. Kelvin ist also immer 273,15 h\u00f6her als Celsius. Die Einheit Fahrenheit basiert auf einem Thermometer mit einem Gemisch aus Wasser, Eis und Ammoniumchlorid - die niedrigste Temperatur, die mit diesem Gemisch erreicht werden kann, wurde mit 0\u00b0F festgelegt. Fahrenheit kann wie folgt in Celsius umgerechnet werden: \u00b0F = (\u00b0C)x(9/5) + 32\n<br/>\n(Quelle: https://cryo.gsfc.nasa.gov/introduction/temp_scales.html)\n\n\n==== Richterskala ====\nDie Richterskala wird zur Messung der St\u00e4rke von Erdbeben verwendet. Sie wurde im Jahr 1935 von den US-amerikanischen Seismologen Richter und Gutenberg entwickelt. Sie basiert auf dem Logarithmus der Amplitude der gr\u00f6\u00dften seismischen Welle eines Erdbebenereignisses. Jeder Anstieg um eine Einheit auf der Skala bedeutet eine Verzehnfachung der St\u00e4rke eines Erdbebens und eine 31-fach h\u00f6here freigesetzte Energiemenge. Obwohl die heutigen Technologien seismische Wellen unterhalb dessen messen k\u00f6nnen, was mit der Richterskala m\u00f6glich war, und besser geeignet sind, sehr starke Erdbeben zu messen, wird die Richterskala immer noch h\u00e4ufig verwendet, insbesondere in den Medien. (Quelle: britannica.com/science/Richter-scale)\n[[File:To Rule and To Measure - Richter Scale.png|400px|thumb|center|'''Die Richterskala.''' Quelle: [https://www.britannica.com/science/Richter-scale Britannica]]]\n\n\n==== Beaufort-Skala ====\nDie Beaufort-Windskala wurde vom britischen Admiral Sir Francis Beaufort im Jahr 1805 geschaffen. Sie erm\u00f6glicht es, die Geschwindigkeit von Winden anhand ihrer Auswirkungen auf Land und Ozeane zu bewerten. Sie geht von 0 bis 12, wobei 12 die st\u00e4rksten Winde sind. \n(Quelle: weather.gov./mfl/beaufort)\n\n[[File:To Rule and To Measure - Beaufort.png|600px|thumb|center|'''Die Beaufort-Skala.''' Quelle: [https://en.wikipedia.org/wiki/Beaufort_scale Wikipedia]]]\n\n\n==== Dezibel ====\nDezibel (dB) oder auch Bel (1 Bel = 10dB) sind dimensionslose physikalische Einheiten, die sich auf das Verh\u00e4ltnis zwischen zwei Messwerten (Mess- und Referenzwert) auf einer logarithmischen Skala beziehen. Sie wurden erstmals 1931 publiziert und werden bis heute in verschiedenen Anwendungsbereichen verwendet; am h\u00e4ufigsten wird jedoch immer noch von dB gesprochen, wenn es um die Lautst\u00e4rke von Ger\u00e4uschen geht. Es ist jedoch wichtig zu beachten, dass dB nicht die absolute Lautheit von etwas angibt, sondern nur das logarithmische (nicht lineare!) Verh\u00e4ltnis zu einem Bezugspunkt, der f\u00fcr Schall der Punkt ist, an dem das menschliche Ohr keinen Ton mehr wahrnehmen kann. (Quelle: https://www.elektronik-kompendium.de/sites/grd/0304011.htm, https://de.wikipedia.org/wiki/Bel_(Einheit) )<br/>\n\nEinige Beispiele f\u00fcr diese Verwendung sind:\n* 20dB - leises Fl\u00fcstern\n* 50dB - normale Unterhaltung\n* 110dB - Rockkonzert\n* 140dB - Gewehrschuss\n* 190dB - t\u00f6dliche Lautst\u00e4rke\n\n\n==== Gl\u00fcck ====\nDie Messung des Wohlbefindens von Menschen und deren Fortschritt ist seit langem Gegenstand von Debatten. W\u00e4hrend das BIP der am weitesten verbreitete Ansatz ist, um menschliches (vermeintliches) Wohlbefinden auf nationaler oder globaler Ebene zu messen, gibt es eine zunehmende Nachfrage nach alternativen Indikatoren. Einer davon, der in der Diskussion ist, ist das Gl\u00fcck, wobei Bhutan das erste und bisher einzige Land ist, das das \"Bruttonationalgl\u00fcck\" in den Mittelpunkt seiner Entscheidungen stellt. \n<br/>\nGl\u00fcck - als eine sehr subjektive Ma\u00dfeinheit - ist schwer zu bewerten. Der 'World Happiness Report' versucht dies auf \u00fcber 200 Seiten zu tun. Er wird seit 2013 regelm\u00e4\u00dfig ver\u00f6ffentlicht und verwendet repr\u00e4sentative, selbstberichtete Daten, zumeist aus dem [[https://www.gallup.de/182606/gallup-world-poll.aspx Gallup World Poll]]. Hier beantworten die Teilnehmenden Fragen zu \u00fcber 100 Aspekten des Lebens, darunter ihre durchschnittliche Lebensbewertung auf einer Skala von 1-10. Der World Happiness Report versucht dann zu verstehen, wie verschiedene Lebensaspekte Variationen in dieser selbstberichteten Lebensbewertung erkl\u00e4ren. Im Bericht f\u00fcr das Jahr 2020 wird zum Beispiel untersucht, wie die Elemente BIP, soziale Unterst\u00fctzung, Lebenserwartung, die Freiheit, Lebensentscheidungen zu treffen, Gro\u00dfz\u00fcgigkeit und die Wahrnehmung von Korruption diese beeinflussen. Andere Elemente, die analysiert werden, sind Ungleichheit, Sorgen und \u00c4rger, Diskriminierung, Einkommen, Vertrauen in die Gesellschaft, das politische System, Intimit\u00e4t und viele andere. Insgesamt stellt der Bericht fest, dass Finnland, D\u00e4nemark und die Schweiz die L\u00e4nder sind, in denen die Menschen (im Durchschnitt) am gl\u00fccklichsten sind, w\u00e4hrend Simbabwe, der S\u00fcdsudan und Afghanistan die drei Schlusslichter bilden. Mehr \u00fcber diesen Bericht erfahren Sie auf der [https://worldhappiness.report/ entsprechenden Website].\n\n[[File:To Rule and To Measure - Happiness.png|500px|thumb|center|'''Die Ergebnisse des World Happiness Report 2020'''. Quelle: World Happiness Report 2020, p.30.]]\n\nDas Bild zeigt das durchschnittliche globale Gl\u00fcck zwischen 2017 und 2019. Die Lebensbewertung wird mit der Cantril-Leiter-Frage gemessen, bei der die Befragten gebeten werden, die Qualit\u00e4t ihres Lebens auf einer 11-stufigen Leiter-Skala zu bewerten, wobei die unterste Stufe der Leiter (0) das schlechtestm\u00f6gliche Leben darstellt, das sie sich vorstellen k\u00f6nnen, und die h\u00f6chste Stufe (10) das bestm\u00f6gliche Leben ist. 'Positiver Affekt' wird durch einen Zwei-Punkte-Index gemessen, der die Befragten fragt, ob sie am Tag vor der Befragung h\u00e4ufig (1) Freude und (2) Lachen erlebt haben oder nicht. 'Negativer Affekt' wird durch einen Index mit drei Items gemessen, in dem die Befragten gefragt werden, ob sie am Tag vor der Befragung h\u00e4ufig (1) Sorgen, (2) Traurigkeit und (3) Wut erlebt haben. <br/>(Quelle: World Happiness Report 2020, S.30.)\n\n\n==== IQ ====\nIQ, kurz f\u00fcr Intelligenzquotient, ist eine g\u00e4ngige Messgr\u00f6\u00dfe zur Angabe und zum Vergleich der menschlichen Intelligenz. Es gibt keine Standarddefinition von Intelligenz und es existiert eine Vielzahl von normierten IQ-Tests. Ihre Auswahl muss ber\u00fccksichtigt werden, wenn man \u00fcber den IQ-Wert selbst spricht. Im Allgemeinen umfasst ein IQ-Test Fragen aus verschiedenen Bereichen, darunter allgemeines logisches Verst\u00e4ndnis, Sprachverst\u00e4ndnis, Allgemeinwissen, mathematische F\u00e4higkeiten und andere. Nach der Durchf\u00fchrung eines IQ-Tests erh\u00e4lt jede*r Teilnehmer*in einen numerischen Wert, der seine*ihre Intelligenz angibt.\n\nDas durchschnittliche Ergebnis ist mit 100 definiert, mit einer Standardabweichung von 15. Aus diesem Grund m\u00fcssen die IQ-Test-Normen regelm\u00e4\u00dfig aktualisiert werden. Dies wurde im 20. Jahrhundert nicht getan, was zu immer h\u00f6heren durchschnittlichen IQ-Werten f\u00fchrte, was als Flynn-Effekt bezeichnet wurde und nicht erkl\u00e4rt werden kann. Gemeinhin gilt jede*r, der*die einen IQ <70 hat, als 'geistlos', w\u00e4hrend jemand mit >130 als 'hochbegabt' gilt. Dabei gibt es jedoch heftige wissenschaftliche Kritik am IQ, da die Ergebnisse eines Menschen bei einem IQ-Test stark von seiner*ihrer Tagesform abh\u00e4ngen und der Test nicht alle relevanten Aspekte der Intelligenz abdeckt. Auch der gesellschaftliche Diskurs um den IQ ist problematisch, da er zu falschen Annahmen und R\u00fcckschl\u00fcssen auf die F\u00e4higkeiten und den Wert eines Menschen f\u00fchren kann. (Quelle: https://flexikon.doccheck.com/de/Intelligenzquotient, https://de.wikipedia.org/wiki/Intelligenzquotient)\n\n\n==== HDI ====\nDer Human Development Index (HDI) ist ein Index, der seit 1990 von den Vereinten Nationen ver\u00f6ffentlicht wird. Er berechnet einen Wert zwischen 0 und 1 basierend auf drei Aspekten des Lebens: Lebenserwartung bei der Geburt, Qualit\u00e4t der Bildung, gemessen an der Zahl von Jahren, die eine Person die Schule besucht, und Pro-Kopf-Einkommen. Jedes Land kann einen Wert zwischen 0 und 1 erreichen, wobei 1 das bestm\u00f6gliche Ergebnis ist. Im Bericht 2019 waren die L\u00e4nder mit dem h\u00f6chsten HDI Norwegen, die Schweiz und Irland, die alle Werte \u00fcber 0,94 erreichten, w\u00e4hrend das Schlusslicht der Tschad, die Zentralafrikanische Republik und Niger bildeten.\n\nDer HDI ist ein recht einfaches Ma\u00df, das sich nur auf diese drei Elemente konzentriert. Es gibt auch den IHDI, der Ungleichheiten innerhalb dieser Kriterien anerkennt und ber\u00fccksichtigt (mit den gleichen oberen und unteren drei L\u00e4ndern). Der Bericht 2019 drehte sich um Ungleichheiten innerhalb und zwischen den Nationen, was auf eine zunehmende Anerkennung einer st\u00e4rker integrierten Bewertung des HDI hinweist. F\u00fcr die Gleichheit ist auch der GINI-Koeffizient von Bedeutung, der die finanzielle (Un-)Gleichheit innerhalb einer Nation anzeigt - mehr dazu [https://www.investopedia.com/terms/g/gini-index.asp hier].\n(Quelle: https://en.wikipedia.org/wiki/Human_Development_Index, http://hdr.undp.org/en/2019-report)\n\n[[File:To Rule and To Measure - HDI.png|500px|thumb|center|'''Die globale Verteilung des HDI gem\u00e4\u00df des 2019er Berichts.''' Die dunkelsten Bereiche stellen Werte \u00fcber 0.8 dar, gefolgt von >0.7, >0.55 und >0.35. Quelle: [https://en.wikipedia.org/wiki/Human_Development_Index Wikipedia]]]\n\n\n==== US-Amerikanisches Ma\u00dfsystem vs Metrisches Ma\u00dfsystem ====\nBis heute verwenden die Vereinigten Staaten von Amerika (sowie Liberia und Myanmar) ein Ma\u00dfsystem (genannt 'USC'), das sich von dem metrischen System unterscheidet, das die meisten L\u00e4nder verwenden. Das USC basiert auf englischen Einheiten und wurde 1832 formalisiert, w\u00e4hrend Gro\u00dfbritannien zun\u00e4chst das Imperiale, und sp\u00e4ter das Metrische System nutzte. Die USA lehnten jedes metrische System mit der Begr\u00fcndung ab, dass es nicht \"f\u00fcr den Herrn akzeptabel\" sei. Im Jahr 1975 machte der 'Metric Conversion Act' das verbreitete metrische System zum bevorzugten System f\u00fcr Handel und Gewerbe, und es ist noch immer der Standard unter anderem in der Wissenschaft und im Milit\u00e4r, aber in der allt\u00e4glichen Praxis wird in den USA immer noch h\u00e4ufig das USC-System verwendet, insbesondere bei Konsumg\u00fctern und in der industriellen Fertigung.\n\nEinige Beispiele sind:\n* L\u00e4nge und Fl\u00e4che:\n** 1 Zoll = 2,54cm\n** 1 Fu\u00df = 30,48cm\n** 1 Yard = 91,44cm\n** 1 Meile = 1,609344 km\n** 1 Acre = 4046,873 m\u00b2\n\n* Masse und Gewicht:\n** 1 Unze = 28,35g\n** 1 Pfund = 453,59g\n(Quelle: https://en.wikipedia.org/wiki/United_States_customary_units)\n\n\n==== Das Gewicht der Seele (21 Gramm) ====\n1907 versuchte der Arzt Duncan MacDougall, die Existenz der menschlichen Seele zu beweisen - wissenschaftlich. Er w\u00e4hlte sechs Personen aus, die kurz vor dem Tod standen, und stellte sie auf eine pr\u00e4zise Waage. In dem Moment, in dem sie starben, so behauptete er in der Ver\u00f6ffentlichung seiner Ergebnisse, verloren sie 21 Gramm an K\u00f6rpergewicht, was durch keine anderen Vorg\u00e4nge erkl\u00e4rbar war, als dass die Proband*innen beim Sterben ihre Seele verloren. Nat\u00fcrlich gab es Kritik an dieser Studie, und es wurden andere Erkl\u00e4rungen angef\u00fchrt. Doch die Idee, dass die menschliche Seele 21 Gramm wiegt - und somit existiert - wurde popul\u00e4r und beeinflusste sogar den Filmregisseur Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu, der einen Film aus dem Jahr 2003 genau so benannte: \"21 Gramm\". Bis heute wirft das Experiment die Frage auf, was die Wissenschaft wissen kann und was nicht. (Quelle: https://www.thevintagenews.com/2018/11/30/21-grams-experiment/)\n\n\n== Ma\u00dfeinheiten und Messger\u00e4te ==\n==== Wassertiefe und Knoten ====\nDie Tiefenlotung ist eine Methode zur Messung der Wassertiefe. Sie wurde bereits im antiken Griechenland und Rom angewandt und findet auch heute noch Verwendung beim Hobbyangeln und als Backup f\u00fcr das heute \u00fcbliche Echolot im Falle einer St\u00f6rung.\nBeim 'Depth Sounding' (das nichts mit 'sound' = 'Ger\u00e4usch' zu tun hat) wird ein d\u00fcnnes Seil mit einem Lot, d.h. einem Gewicht, das am Ende des Seils h\u00e4ngt, kombiniert. Dieses Seil wird dann ins Wasser fallen gelassen, bis es die Grundoberfl\u00e4che erreicht. Anhand von Markierungen, die am Seil angebracht sind, kann man schnell die Tiefe des Wassers messen.\n\nInteressanterweise hat der Schriftsteller Mark Twain seinen K\u00fcnstlernamen von dieser Technik erhalten. Beim Depth Sounding kann die Wassertiefe angegeben werden, indem man \"By the mark\" sagt und die Zahl addiert, die von der Wasseroberfl\u00e4che getroffen wird. In Mississippi in den 1850er Jahren wurde die Zahl \"zwei\" als \"twain\" bezeichnet. Wenn die Lotsen also die Tiefe des Flusses ma\u00dfen und sie zwei Faden betrug (ein Faden ist 1,8288m), sagten sie \"by the mark twain\". (Quelle: https://en.wikipedia.org/wiki/Depth_sounding)\n\n\n==== Thermometer ====\nEin Thermometer basiert auf der Tatsache, dass sich Fl\u00fcssigkeiten (und Gase) in Abh\u00e4ngigkeit von der Temperatur ausdehnen und zusammenziehen. W\u00e4hrend die ersten Schritte in Richtung moderner Thermometer im antiken Griechenland und sp\u00e4ter wieder im 16. und 17. Jahrhundert gemacht wurden, waren die von Fahrenheit 1714 und Celsius 1742 erfundenen Thermometer die gr\u00f6\u00dften Durchbr\u00fcche (siehe Celsius vs. Fahrenheit vs. Kelvin).\nIm Laufe der Zeit gab es verschiedene Skalen, Einheiten und Fl\u00fcssigkeiten (oder Gase), die in Thermometern verwendet wurden; heute basieren die meisten klassischen Thermometer auf Quecksilber, und trotz der Unterschiede in den Messeinheiten gibt es einen international vereinbarten Bereich von Temperaturen, die von einem Thermometer angezeigt werden. (Quelle: https://en.wikipedia.org/wiki/Thermometer)\n\n\n==== Sextant ====\n[[File:To Rule and To Measure - Sextant.png|250px|thumb|left|'''Ein Sextant'''. Quelle: [https://www.brighthubengineering.com/seafaring/31615-marine-navigational-equipment-the-sextant Bright Hub Engineering]]]\nEin Sextant ist ein nautisches Werkzeug zur Bestimmung der Position eines Schiffes. Er basiert auf der Messung des Winkelabstands zwischen astronomischen Objekten und wurde um 1700 von Newton und Hadley erfunden. Ein Sextant besteht aus einem Kreissektor mit einem Spiegel, einem Halbspiegel und einem Teleskop. Man schaut durch das Fernrohr auf einen bestimmten Punkt (auf einen Stern, die Sonne oder den Horizont) und bewegt den Spiegel so lange, bis ein zweiter Bezugspunkt auf den Halbspiegel reflektiert wird, so dass sich die Objekte schlie\u00dflich \u00fcberlappen. Anhand der Bewegung des Spiegels kann man den Winkel auf der Skala des Sextanten ablesen. Der Winkel zwischen einem Stern \u00fcber dem Horizont hilft in Verbindung mit der Tageszeit, die geografische Breite mit Hilfe von astronomischen Tabellen zu bestimmen. Der Winkel zwischen dem Mond und einem Stern f\u00fchrt zur geografischen H\u00f6he.\n(Quelle: https://www.brighthubengineering.com/seafaring/31615-marine-navigational-equipment-the-sextant/, https://www.spektrum.de/lexikon/physik/sextant/13240)\n\n\n==== Trigonometrie ====\n[[File:To Rule and To Measure - Trigonometry.png|400px|thumb|right|'''Die Sechs Trigonometrischen Funktionen.''' Quelle: [http://www.gradeamathhelp.com/image-files/trigonometry-functions.gif Grade A Math Help]]]\nDie Trigonometrie ist eine der \u00e4ltesten Formen des Rechnens und Messens, deren Urspr\u00fcnge sich \u00fcber Sumer, Babylon, Griechenland, \u00c4gypten, Indien und andere fr\u00fche Zivilisationen bis vor 4000 Jahren erstreckten. Das Wort bedeutet \"Dreiecksmessung\" und bezieht sich auf die Verwendung von Seitenl\u00e4ngen und Winkeln von Dreiecken f\u00fcr Berechnungen. Wenn man die Gr\u00f6\u00dfe von gen\u00fcgend Seiten und Winkeln eines Dreiecks kennt, ist es m\u00f6glich, die Gr\u00f6\u00dfe oder L\u00e4nge der restlichen Seiten und Winkel zu berechnen. Die Trigonometrie wurde historisch in geod\u00e4tischen Messungen, in der Astronomie, in der Navigation und in vielen anderen Anwendungsbereichen eingesetzt und erm\u00f6glichte die Entwicklung vieler relevanter Technologien und Erfindungen im Laufe der Geschichte.\n\nW\u00e4hrend die Trigonometrie recht aufw\u00e4ndig werden kann, basiert die grundlegendste planare Trigonometrie, wie wir sie heute kennen, auf der Arbeit von Rheticus und seinem Sch\u00fcler Otho im 16. Jahrhundert und umfasst sechs grundlegende trigonometrische Funktionen: Sinus, Cosinus, Tangens sowie Kosekante, Sekante und Kotangens. Wenn man zwei Variablen des Dreiecks kennt, kann die dritte mit Hilfe eines Taschenrechners oder historisch mit einem Tabellenbuch oder einem Rechenschieber berechnet werden. (Quelle: https://math.wikia.org/wiki/History_of_trigonometry)\n\n\n==== LiDAR ====\n[[File:To Rule and To Measure - LiDAR.png|300px|thumb|right|'''Das Prinzip der LiDAR-Technologie.''' Quelle: [https://archaeologicalphotorelief.files.wordpress.com/2014/07/how_lidar_works.jpg Archeological Photo Relief]]]\nLiDAR ist die Abk\u00fcrzung f\u00fcr \"light detection and ranging\". Es ist eine Technologie, die den Abstand zwischen dem LiDAR-Sensor und jedem Objekt in seinem sichtbaren Bereich misst. LiDAR kann sowohl flugzeug- oder satellitengest\u00fctzt als auch in der N\u00e4he der Oberfl\u00e4che eingesetzt werden. Der Sensor sendet mit einem Laser einen Lichtimpuls aus und erfasst das Licht, das von jedem Objekt reflektiert wird. Basierend auf der Zeit, die das Licht ben\u00f6tigt hat, berechnet er die Entfernung zum Objekt. Die Objekte k\u00f6nnen fest, aber auch fl\u00fcssig oder sogar gasf\u00f6rmig sein. Auf diese Weise ist der LiDAR-Sensor in der Lage, ein 3D-Bild z.B. der Erdatmosph\u00e4re oder -oberfl\u00e4che zu erstellen. Die LiDAR-Technologie kann bei der Erstellung von Landkarten helfen und wurde in j\u00fcngster Zeit eingesetzt, um Aerosole in der Luft, Temperaturprofile oder Spurengaskonzentrationen zu identifizieren. (Quelle: https://www.dlr.de/rd/desktopdefault.aspx/tabid-5626/9178_read-17527/)\n\n\n==== Chinesisches Fingerrechnen ====\n[[File:To Rule and To Measure - Chinese Finger Counting.png|400px|thumb|center|'''Chinesisches Fingerrechnen'''. Quelle: [https://en.wikipedia.org/wiki/Chinese_number_gestures Wikipedia]]]\nIn China gibt es ein System, die Zahlen von 1-10 mit nur einer Hand anzuzeigen (im Gegensatz zum westlichen Brauch, eine Einheit pro Finger anzuzeigen, also nur f\u00fcnf pro Hand zu erreichen). Dieses System ist wahrscheinlich aufgrund der chinesischen Sprachvielfalt entstanden, oder auch aus dem Wunsch heraus, Informationen privat zu vermitteln. (Quelle: https://en.wikipedia.org/wiki/Chinese_number_gestures)\n\n\n==== Das Lineal ====\n[[File:To Rule and To Measure - Ruler.png|300px|thumb|left|'''Ein Lineal aus Stahl'''. Quelle: [https://en.wikipedia.org/wiki/Ruler#/media/File:Steel_ruler_closeup.jpg Wikipedia]]]\nLineale sind der Archetyp der Messwerkzeuge. Sie helfen, den Abstand zwischen Punkten oder die L\u00e4nge von Linien zu bestimmen (was dasselbe ist, wenn man dar\u00fcber nachdenkt), oder k\u00f6nnen als geometrisches Hilfsmittel verwendet werden, um festzustellen, ob eine Linie gerade oder gekr\u00fcmmt ist, und um bei Bedarf gerade Linien zu zeichnen. Lineale werden \u00fcblicherweise aus Holz, Kunststoff, Stahl, flexiblem Band oder \u00e4hnlichem hergestellt, je nach Einsatzgebiet, Benutzerpr\u00e4ferenz und notwendiger L\u00e4nge. Letztere kann unterschiedlich sein: Antike Lineale hatten unterschiedliche L\u00e4ngen, Lineale, die man in der Schule antrifft, messen typischerweise 30 cm L\u00e4nge, und flexiblere (manchmal faltbare) Lineale, die in der Architektur, im Bauwesen oder anderen solchen Bereichen verwendet werden, messen oft mehrere Meter. Historisch gesehen wurden Lineale h\u00e4ufiger verwendet, nachdem das metrische System (siehe oben) ab dem sp\u00e4ten 18. Jahrhundert popul\u00e4r geworden war. Das \u00e4lteste erhaltene Lineal (aus einer Kupferlegierung) stammt jedoch aus dem Jahr 2650 v. Chr. aus Nippur in Sumer, und Lineale aus Elfenbein wurden im Indus-Tal vor 1500 v. Chr. verwendet. (Quelle: https://en.wikipedia.org/wiki/Ruler)\n\n\n==== Weltraumteleskop ====\n[[File:To Rule and To Measure - Hubble Space Telescope.png|250px|thumb|right|'''Das ''Hubble Space Telescope'''''. Quelle: [https://en.wikipedia.org/wiki/Space_telescope Wikipedia]]]\nEin Weltraumteleskop ist ein Teleskop im Weltraum. Anders als bei Satelliten, die die Erde abbilden, zielt es in den Weltraum, um entfernte Sterne, Planeten und andere astronomische Objekte zu kartieren oder zu vermessen. Weltraumbeobachtungen haben den Vorteil, dass sie nicht durch Filterung und Verzerrungen durch die Erdatmosph\u00e4re sowie durch Lichtverschmutzung auf der Erde beeintr\u00e4chtigt werden. Daher erm\u00f6glichen sie den Wissenschaftler*innen eine bessere Beobachtung des Weltraums trotz der Schwierigkeiten bei der Wartung und der hohen Kosten f\u00fcr den Bau von Weltraumteleskopen. (Quelle: https://en.wikipedia.org/wiki/Space_telescope)\n\n\n==== Gas- und Fl\u00fcssigchromatographie ====\nGaschromatographie (GC) und Fl\u00fcssigkeitschromatographie (LC) sind die beiden Hauptformen der Chromatographie, einer Analysetechnik zur Quantifizierung von Verbindungen in einem Gemisch, z.B. bei der Qualit\u00e4tskontrolle in der Industrie, in der Forensik oder in den Umweltwissenschaften. Der Begriff \"Chromatographie\" wurde 1906 vom russischen Botaniker Tswett gepr\u00e4gt, w\u00e4hrend die Methode selbst erstmals 1952 verwendet wurde. \n\nEine Chromatographie besteht aus zwei Hauptkomponenten: einer station\u00e4ren Phase und einer mobilen Phase, die sich entlang des Mediums im Chromatographen bewegt. Bei der Gaschromatographie besteht die mobile Phase aus einem Inertgas, wie Helium oder Stickstoff, w\u00e4hrend die station\u00e4re Phase aus einer Fl\u00fcssigkeit besteht. Bei der Fl\u00fcssigkeitschromatographie ist die mobile Phase eine Fl\u00fcssigkeit und die station\u00e4re Phase ein Feststoff. Die zu analysierende Probe wird in diese Phasen eingebracht. Diejenigen Verbindungen, die st\u00e4rker mit der station\u00e4ren Phase wechselwirken (\"like-dissolves-like\"-Regel, d.h. chemisch \u00e4hnliche Verbindungen bleiben zusammen), bleiben l\u00e4nger in dieser Phase, w\u00e4hrend die Verbindungen, die weniger mit der station\u00e4ren Phase wechselwirken, sich mit der mobilen Phase bewegen. Die Verteilung der Verbindungen zwischen den beiden Phasen kann mit verschiedenen Formen von Detektoren analysiert werden, was zu Erkenntnissen \u00fcber die Zusammensetzung der Ausgangsprobe f\u00fchrt. (Quelle: http://www.chem.ucla.edu/~bacher/Allgemein/30BL/gc/theorie.html, https://microbenotes.com/chromatography-principle-types-and-applications/)\n\n[[File:To Rule and To Measure - Chromatography.png|400px|thumb|center|'''Das Prinzip der Chromatographie'''. Quelle: [https://microbenotes.com/chromatography-principle-types-and-applications/ Microbenotes]]]\n\n\n== Abschlie\u00dfende Bemerkungen ==\nViele dieser Ma\u00dfeinheiten und Messger\u00e4te werden von Forschenden oft als wissenschaftliche Methoden definiert. Wir w\u00fcrden dem widersprechen, denn sie erzeugen kein Wissen an sich, sind aber oft eine wichtige Grundlage f\u00fcr die Sammlung von Daten oder die Strukturierung des erworbenen Wissens. Wir m\u00fcssen anerkennen, dass es eine F\u00fclle von Daten gibt, und die hier vorgestellten Beispiele zeigen deren gro\u00dfe Vielfalt und die entstehenden Herausforderungen, verschiedene Daten zu integrieren. Dies wird eine der wichtigsten methodischen Herausforderungen des 21. Jahrhunderts sein und unterstreicht sowohl die Bedeutung der Anerkennung qualitativer Methoden als auch die tiefe Normativit\u00e4t, die Teil der vielf\u00e4ltigen Datens\u00e4tze ist, die wir haben.\n\n\n== Weiterf\u00fchrende Informationen ==\n* [https://www.iqmindware.com/wiki/what-does-my-iq-score-mean IQ]: Eine Erkl\u00e4rung\n* [https://www.youtube.com/watch?v=7p2a9B35Xn0 IQ]: Misst er wirklich, wie schlau man ist?\n* [https://www.youtube.com/watch?v=7p2a9B35Xn0 IQ]: Eine kritische Reflexion\n* [https://www.investopedia.com/terms/g/gdp.asp BIP]: Eine detaillierte Erkl\u00e4rung\n* [https://www.factmonster.com/math-science/weights-measures/metric-weights-and-measures Ma\u00dfeinheiten]: Eine Reflexion \u00fcber verschiedene Messsysteme\n* [http://hdr.undp.org/en/content/human-development-index-hdi Der Human Development Index]: Eine Alternative zum BIP?\n* [https://www.investopedia.com/terms/g/gini-index.asp Der GINI Index]: Ein Ma\u00df f\u00fcr Ungleichheit\n----\n[[Category:Normativity_of_Methods]]\n\nThe [[Table_of_Contributors|authors]] of this entry are Christopher Franz and Henrik von Wehrden."
                    },
                    "sha1": "9us69h3wmh1avuvacrvk1au1ipqa01p"
                }
            },
            {
                "title": "Transcribing Interviews",
                "ns": "0",
                "id": "783",
                "revision": {
                    "id": "6310",
                    "parentid": "6205",
                    "timestamp": "2021-09-01T07:11:54Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "12521",
                        "#text": "'''In short:''' This entry revolves around Transcription, which is the reconstruction of recorded audio into written text for the analysis of Interviews. For more on Interview Methodology, please refer to the [[Interviews|Interview overview page]].\n\n== What is Transcription? ==\nTranscription is not a scientific method in itself, but an important step between the conduction of Interviews and the Coding process and subsequent analysis. '''Transcription entails the process of writing down what has been said in an Interview based on video or audio recordings.''' Transcription has become common practice in qualitative research (1). The transcription process is an analytical and interpretative act, which influences how the transcript represents what has actually been said or done by the interviewee(s) (1, 3). A well-conceived transcription process will support the analysis, and a bad transcription may lead to an omission or distortion of important data. This entry revolves around important elements of the transcription process. For more details on Interviews and the subsequent analysis of the transcripts, please refer to [[Interviews]] and [[Content Analysis]]. \n\n'''Transcriptions are mostly relevant for qualitative Interview approaches''', such as [[Open Interview]]s, [[Semi-structured Interview]]s, [[Narrative Research|Narrative Interviews]], [[Ethnography|Ethnographic Interviews]] or [[Focus Groups]]. It may also be relevant for [[Video Research]], or serve as a source for supplementary data when conducting [[Surveys]]. Transcripts enable [[Glossary|researchers]] to analyze what Interviewee's or observed individuals said in a given situation without the risk of missing crucial content. Knowing that the data will be there later on allows for the researcher to engage with the Interview situation instead of imposing the need to write everything down right away. Recorded material can be re-visited long after the data gathering process and transcribed.\n\n\n== Denaturalized and naturalized transcription ==\n'''In a denaturalized approach, the recorded speech is written down word by word.''' Denaturalized transcription revolves mostly around the informational content of the interviewee's speech. Denaturalism \"(...) suggests that within speech are meanings and perceptions that construct our reality\" (Oliver et al. 2005, p.1274). '''This approach is more about ''what'' is said, than ''how'' it is said,''' and is mostly relevant to research interested in how people conceive and communicate their world, for example in Grounded Theory research (although there are also exceptions here) (Oliver et al. 2005, p.1278).\n\n'''Naturalized transcription is as detailed as possible,''' including stutters, pauses and other idiosyncratic elements of speech. It attempts to provide more details and a more 'realistic' representation of the interviewee's speech. The idea is to reduce a loss of information, be more 'objective' and true to the Interviewee(s), and thus impose less assumptions through the researcher. Naturalized Transcription is, for example, of interest when a conversation between individuals is recorded (in a group interview, or a Focus Group), and the way in which these individuals interact with each other is of interest (overlapping talk, turn-taking etc.) (Oliver et al. 2005, p.1275.) Grammatical or spelling mistakes are not corrected in the transcript for naturalistic transcription (1). Also, verbal cues that support the spoken word may elicit more insights for the researchers, such as dialects, increasing or decreasing volumes and specific emphases for individual words, or pauses. For this purpose, the transcript may include textual symbols to provide more information despite the words themselves:\n\n[[File:Transcribing Interviews - Naturalized textual symbols.png|700px|thumb|center|'''Textual symbols in naturalized transcription.''' Source: Oliver et al. 2005, p.1276]]\n\n'''Naturalized transcription concerns itself more with ''how'' individuals speak and converse.''' However, the researcher needs to weigh whether this level of detail is necessary for the specific research intent, since it leads to a considerable increase in transcription effort.\n\n\n== Normativity of transcriptions ==\nNo matter the approach to transcription, it is important to acknowledge that any transcript is a re-construction of the Interview done by the researcher. As Skakauskaite (2012, p.24) highlights: \"Transcribing (...) is a form of analysis that is shaped by the researchers' examined and unexamined theories and assumptions, ideological and ethical stances, relationships with participants, and the research communities of which one is a member\". '''A researcher that bases his or her analysis on Interview transcripts needs to acknowledge this role he or she imposes on the data in the subsequent analysis and interpretation.''' A lack of reflexivity in this regard may distort the research results, and impede interpretation especially in-between different research communities. Oliver et al. (2005) therefore suggests researchers to reflect upon the chosen approach and which challenges emerge based on it. \n\nLikewise, Skakauskaite (2012, p.25) calls for more transparent information on the construction of transcripts in research publications: \"What we can learn and know about human activity and interaction depends on how we use language data and what choices we make of how to turn audio (and/or video) records into written texts, what to represent and not represent, and how to represent it. Given that there is no single way of transcribing, making transparent transcribing decisions and theories guiding those decisions, can provide grounded warrants for claims researchers make about observed (and recorded) human actions and interactions.\" Therefore, a proper documentation of the chosen approach, the underlying rationale, and the challenges that were recognized is advisable. Also, asking peers to assess the transcript, or attempting member checking - i.e., interviewees check the transcripts for accuracy - may improve the transcript to this end (1).\n\n\n== Challenges in the transcription process ==\nOliver et al. (2005) list a range of challenges in the translation of recorded speech to text. These include technical issues, such as bad recording quality. Further, transcribers may mis-interpret pronunciation, slang or accents, and write down incorrect words. Also, involuntary vocalizations, such as coughing or laughing, may provide additional information to supplement speech, but also not be of meaning, which may be hard to decide. Lastly, non-verbal cues such as waving or gesticulations may add information to the spoken word, but are only analyzable if recorded on video and/or noted down during the interview, and subsequently added to the transcript.\n\n'''The act of transcription may prove cumbersome.''' Typically, the transcription of recorded text may take between 5-10 times as long as the Interview, although this may differ between researchers and depending on the transcription approach. In any case, the time needed for transcription needs to be factored into the research schedule. Further, long sessions of transcribing Interviews are exhaustive, and may lead to inaccuracies in later datasets. Researchers should therefore consider transcribed Interviews right after they were conducted, if possible.\n\nFor larger research projects, with several interviews conducted possibly in different regions, there will typically be more than one transcriber, and '''the transcriber may not always be the researcher'''. Therefore, to maintain accurate and comparable transcripts, and provide intersubjective data readibility, the approach to transcribing and a clear denomination of what is important, what is not, and how to deal with specific data, is important to define and discuss prior to the process, and whenever issues emerge (Oliver et al. 2005).\n\n\n== Technical tips ==\nThe most crucial point for a smooth transcription is the '''quality of recording'''. The researcher should pay great attention to producing a high quality recording during the interview situation, or later in working with the recorded material, to save time in the transcription process. This includes, for example, proper handling of the recording device, avoiding or filtering out background noises, or, if necessary, asking interviewees to repeat phrases that were not understandable during the interview itself.\n\nGenerally, it is advisable to transcribe data not too long after gathering it, in case certain phrases are not understandable in the recording, and memory can better help recall. However, a '''time lag between recording and transcription''' can help the researcher distancing themselves from the interview situation and allow for greater objectivity in listening. If the data gathering took time long before the transcription, it is recommendable for the transcriber to listen to the complete recording again before starting the transcription in order to recall the conversation as a whole, and be able to pay attention to details and the informational content, which can be helpful to properly create the transcript (1).  \n\nWhen transcribing, it is advisable to '''play the recorded text at a slower speed and type simultaneously''' - the author of this entry has made good experiences with a 0.7x speed for denaturalistic transcription. However, the preferable speed depends on the speed that the transcriber can type at (see [[Speed Typing]]), and while slower replays of the recordings will extend the transcription process, a quicker replay is more prone to mistakes and misunderstandings. This is especially relevant for fast speaking interviewees, heavily accented speech, or naturalistic transcription approaches, where more time will be needed to perceive all details. The replay should be done using a device or software that can easily be accessed, like a phone, recording device, or second screen on the computer. This is because sections of the recording may be necessary to replay several times. A single skip should be possible by the press of a button, and adjusted to a preferable time span, e.g. 2 seconds back, to simplify this. \n\nWhile typing, the text may be structured and punctuations may be added to improve legibility, while the impact this act has on the transcript should be acknowledged. The same is true for the use of paragraphs and line-breaks in the text, which are advisable for turn-by-turn dialogic interactions or longer monologues, but also influence the way the expressed arguments are represented in the written text. To allow for quotations, the lines of '''the transcript should be numbered''' or time codes should be added at the end of each paragraph (2). A transcript template may be advisable to help standardize the transcripts (1).\n\nPrograms like e.g., f4transkript or the transcription function of MAXQDA offer a range of settings to easily handle different of the above mentioned aspects. Autotranscription programs can also be used, albeit only for the denaturalistic approach. However, the time saver effect is limited because training the program for voice recognition takes time, and transcription accuracy still has to be validated and corrected by hand.\n\nFor ethical reasons, the '''identity of the interviewee should be secured''' in the transcribed text by attributing a number, pseudonym or code to the interview (1). This should be done in a way that supports easy data management and file access. If member checking is planned, the transcriber should leave space in the transcript file for potential comments (1).\n\n\n== References ==\n(1) Widodo, H.P. 2014.\u00a0''Methodological Considerations in Interview Data Transcription.''\u00a0International Journal of Innovation in English Language 3(1). 101-107.\n\n(2) Dresing, T., Pehl, T. 2015.\u00a0''Praxisbuch Interview, Transkription & Analyse. Anleitungen und Regelsysteme f\u00fcr qualitativ Forschende.''\u00a06th edition.\n\n(3) Skakauskaite, A. 2012. ''Transparency in Transcribing: Making Visible Theoretical Bases Impacting Knowledge Construction from Open-Ended Interview Records.'' Forum Qualitative Social Research 13(1,14).\n\n(4) Oliver, D.G. Serovich, J.M. Mason, T.L. 2005. ''Constraints and Opportunities with Interview Transcription: Towards Reflection in Qualitative Research.'' Social Forces 84(2). 1273-1289.\n----\n[[Category: Normativity of Methods]]\n[[Category: Skills and Tools]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "1ky9nagbds8znusxcgc0o165jgz4zwi"
                }
            },
            {
                "title": "Transdisciplinarity",
                "ns": "0",
                "id": "447",
                "revision": {
                    "id": "6350",
                    "parentid": "5878",
                    "timestamp": "2021-09-10T11:52:46Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23752",
                        "#text": "'''Note:''' The German version of this entry can be found here: [[Transdisciplinarity (German)]]\n\n'''In short:''' Transdisciplinarity is a research approach that is commonly applied in Sustainability & Transformation Research, focusing on the collaboration of academic and non-academic actors with a solution-oriented perspective. The following entry introduces the approach, its characteristics and prevalent challenges, and illustrates the use of methods with examples.\n\n== Multi-, Inter- and Transdisciplinarity ==\nTo understand transdisciplinarity, it should first be distinguished from multidisciplinarity and interdisciplinarity. This is especially relevant when comparing German-speaking and English-speaking resources: the US-American understanding of transdisciplinarity is rather comparable to interdisciplinarity, while the German-speaking discourse is based on the following distinctions.\n\n[[File:Disciplinary.png|700px|thumb|right|'''Different forms of disciplinary cooperation.''' [http://makinggood.design/thoughts/tasty/ Source.]]]\n* '''Multidisciplinarity''' revolves around the cooperation between different academic disciplines for one research endeavour. The researchers study the same topic in a parallel structure and all have their distinct goals and research questions. They share knowledge and compare their findings and may also combine their results in one big report, but the disciplinary boundaries generally remain intact. (Stock & Burton 2011)\n* '''Interdisciplinarity''' is a research mode that deploys a higher level of cooperation between researchers from different, often rather unrelated disciplines. Disciplinary viewpoints are crossed and integrated to develop new perspectives in order to create new knowledge or re-assess existing, disciplinary knowledge. As an example, political scientists and ecologists may come together to jointly study a eco-political issue.  (Stock & Burton 2011)\n* '''Transdisciplinarity''', then, can be seen as the next higher step. Transdisciplinarity is \"a reflexive, integrative, method-driven scientific principle aiming at the solution or transition of societal problems and concurrently of related scientific problems by differentiating and integrating knowledge from various scientific and societal bodies of knowledge.\" (Lang et al. 2012, p.26f). The most important features here are\n** the collaboration of academia and society in a mutual learning and research process where stakeholders (politicians, entrepreneurs, NGO representatives, citizens etc.) are involved at all stages, i.e. in the framing of the problem, the development of possible solutions and the creation of new scientific knowledge, with a focus on these stakeholders contributing their specific knowledge and experiences to broaden the research perspective; \n** the focus on real-world problems, such as questions of sustainable development, that strongly affect and interest not only the academic sphere, but also the economy, culture and politics; as well as \n** the development of solutions to the studied problems that are transferable both into scientific and practical discourses and action. (Stock & Burton 2011, Lang et al. 2012, Arnold & Piontek 2018, Mauser et al. 2013; Ruppert-Winkel et al. 2015).  \n\n\n== Why should (Sustainability) Science engage with the public? ==\nThroughout the last centuries and decades, the differentiation and [[Glossary|institutionalisation]] of scientific disciplines allowed academia to develop and deepen specified conceptual and methodological expertise and create distinct language and topics that have enabled these disciplines to provide plenty of insightful knowledge and guidance for society (Stock & Burton 2011). However, '''there has been an increasing recognition that disciplinary viewpoints and approaches may no longer be sufficient to solve prevalent challenges''' that span across several scientific and societal spheres. Examples for this are ecological threats such as biodiversity loss or socio-technological challenges such as digitalisation which demand the creation of new theoretical and empirical scientific insights, but also action-oriented solutions that can be applied in policy, business, education, culture and such. With his landmark 1994 publication, Michael Gibbons introduced the term \"Mode 2 research\" which highlights this emerging form of scientific inquiry that is \"characterised by closer interaction between scientific, technological and industrial modes of knowledge production, by the weakening of disciplinary and institutional boundaries, by the emergence of more or less transient clusters of experts, often grouped around large projects of various kinds, and by the broadening of the criteria of quality control and by enhanced social accountability.\" (Gibbons 1994, p.68)\n\nTo this end, multi- and interdisciplinary research modes (see above) are research modes that already cross disciplinary boundaries to some extent, sharing knowledge or jointly creating it, but transdisciplinary research best represents this new form of knowledge production. Mode 2 research and transdisciplinary research have therefore often been used synonymously. Transdisciplinary research facilitates the creation of not only system knowledge (the current state) and target knowledge (the intended state), but especially of transformational knowledge (how to get there) that considers the demands and preconditions of all relevant stakeholders. Thus, transdisciplinary research is of particular interest where there is a lack of practical knowledge about a specific problem, solution pathways are not yet known and there is a need for societal negotiation throughout the development of solutions. (Arnold & Piontek, p.145f)\n\n'''Transdisciplinary research is of special importance to Sustainability Science and has received immense recognition in this field in the last years.''' This is because \"[s]ustainability is also inherently transdisciplinary\" (Stock & Burton 2011, p.1091), as it builds on the premise of solving real-world problems which are deeply nestled in ecological, political, economic and social processes and structures and therefore cannot be understood and solved without engaging with these spheres (Kates et al. 2015). Transdisciplinary research is a suitable approach for Sustainability Science: it allows to incorporate the knowledge of relevant stakeholders; it considers the normative dimensions involved in societal endeavors (that is, diverging norms, goals and visions of different societal spheres); and it increases [[Glossary|legitimacy]], ownership and accountability for the jointly developed solutions (Lang et al. 2012; Stock & Burton 2011). The integrative transdisciplinary approach highlights systemic interdependencies, enables a better understanding of complex issues and provides better knowledge to develop socially robust and applicable, effective solutions (Lang et al. 2012; Mauser et al. 2015).\n\n\n== How to do TD in Sustainability Science ==\nHere, we will refer to the ideal-typical model presented by Lang et al (2012). It introduces a conceptual guide, structured into three phases, on how transdisciplinary research for sustainability science should ideally be conducted. The relevant steps of each phase as well as general principles for the whole process will be listed below. The presented process is recursive: Problems emerging in scientific as well as societal practice are integrated in the transdisciplinary research approach. After its three phases have been realized, the newly generated knowledge and solutions are re-integrated into these two spheres, where they are applied and consolidated. Ultimately, new challenges emerge that demand renewed transdisciplinary collaboration.\n\n==== The ideal-typical model ====\n[[File:TDmodel.png|500px|thumb|left|'''The ideal-typical model for TD research in Sustainability Science.''' Source: Lang et al. 2012]]\n\n''Phase A''\n* Build a collaborative research team.\n* Create joint understandings and definitions of the problem.\n* Collaboratively define research objects and objectives, research questions, and success criteria.\n* Design a methodological [[Glossary|framework]].\n\n''Phase B''\n* Assign roles and responsibilities.\n* Apply research methods and settings to create the intended knowledge.\n\n''Phase C''\n* Realize two-dimensional integration.\n* Generate products for both parties (societal and scientific).\n\n''General principles''\n* Facilitate continuous evaluation\n* Mitigate conflict constellations\n* Enhance capabilities for and interest in participation\n<br>\n\n==== Challenges ====\nThere is a wide range of potential pitfalls challenging the achieval of the ideal-typical model.\n\n* Mauser et al. (2013) remark that the roles and responsibilities of all stakeholders involved need to be clarified from the beginning and existent inequalities need to be removed. Further, they comment that integrated research modes demand new methods and concepts, new institutional arrangements, organizational [[Glossary|competencies]] and tailored funding possibilities. The cooperation between science and society requires reflexive learning processes, appropriate communication tools and also needs to reflect on the role of science in the overall endeavour. Also, the authors highlight that there may be inertia to change in the given system which needs to be overcome.\n* Lang et al. (2012) name various issues in the framing of problems (lack of problem awareness, unbalanced problem ownership), processual issues (insufficient legitimacy of the actors involved, conflicting methodological standards, lack of integration (knowledge, structures, [[Glossary|communication)]], discontinuous [[Glossary|participation]]), as well as challenges concerning the development of solutions (ambiguity of results, fear to fail, limited solution options, lack of legitimacy of transdisciplinary outcomes, capitalization on distorted research results, issues when tracking impacts). Also, \"[a]rguing from a more conventional research perspective, scientists might be skeptical with respect to reliability, validity, and other epistemological and methodological aspects of collaborative research (\u2018\u2018credibility\u2019\u2019). Practitioners and stakeholders, on the other hand, might be skeptical regarding the practical relevance of the results (\u2018\u2018salience\u2019\u2019).\" (Lang et al. 2012)\n* Hall & Rourke (2014) expand on communicative challenges, claiming that \"[n]ot all of the challenges that threaten TDSS [transdisciplinary sustainability science] are communication challenges, but communication breakdown can exacerbate any of them. Because of its centrality, care must be taken by collaborators to cultivate a healthy communication dynamic; however, given the many perspectives involved in a typical TDSS project, this will not be easy. These projects meet complex problems with complex responses, entailing the need to remain flexible and responsive to participant requirements and the need to modify the approach if new information and values arise.\" They highlight communicative challenges especially in the framing of problems (exclusion of important perspectives, different views of the problem) and the conduction of the research (unwillingness to share personal perspectives, failure to recognize or articulate differences in individual assumptions, uncertainties and incomplete or incompatible knowledge, limited cognitive abilities to integrated individual partial knowledge).\n\n==== Methods of transdisciplinary research ====\nSeveral scientific methods are useful for transdisciplinary work. Many methods used in TD research represent single elements of a broader transdisciplinary inquiry, such as [[Interviews]] which may be used in a first step of [[Glossary|consultation]]. Other methods, however, originate from transdisciplinary endeavours or are strongly associated to them. Among these, noteworthy methods include:\n\n* [[Visioning & Backcasting|Visioning]] as well as [[Scenario Planning]], which strongly favor transdisciplinary research.\n* [[Living Labs & Real World Laboratories|Living Labs & Real-World Laboratories]] as research environments that facilitate transdisciplinary research.\n* Multi-Attribute Utility Theory (MAUT) can be used to evaluate scenarios.\n* [[Citizen Science]] can also be transdisciplinary.\n\n==== Skills & Tools ====\nVarious Skills & Tools may facilitate transdisciplinary research and help solve some of the challenges mentioned above. Among these are:\n\n* [[Design Thinking]]\n* [[Disney Method]]\n* [[Lego Serious Play]]\n* [[Stakeholder Mapping]]\n* [[World Caf\u00e9]]\n* [[The Art of Hosting | Workshops]]\n\n==== Agency ====\nThe choice of methods for TD research is also relevant for the level of [[Glossary|agency]] and participation that is provided to the non-scientific stakeholders in the process. One may differentiate four levels of [[Agency, Complexity and Emergence|agency]] on which societal actors are involved in scientific research (see also Brandt et al. 2013):\n\n* ''Information'': Stakeholders are informed about scientific insights, possibly in form of policy recommendations that make the knowledge actionable. This is the most common form of science-society cooperation.\n* ''Consultation'': A one-directional information flow from practice actors (stakeholders) to academia, most commonly in form of questionnaires and interviews, which provides input or feedback to proposed or active research. Stakeholders provide information, which is of interest to the researchers, but are not actively involved in the research process.\n* ''Collaboration'': Stakeholders cooperate with academia, e.g. through one of the aforementioned methods, in order to jointly frame and solve a distinct issue.\n* ''Empowerment'': The highest form of involvement of non-scientific actors in research, where marginalized or suppressed stakeholders are given authority and ownership to solve problems themselves, and/or are directly involved in the decision-making process at the collaboration level. Empowerment surpasses mere collaboration since stakeholders are enabled to engage with existing problems themselves, rather than relying on research for each individual issue anew.\n\nTo illustrate the choice of methods in transdisciplinary research, how the selected methods can be combined, and how they relate to agency, see one of the following examples.\n\n\n== Examples ==\n[[File:Besatzfisch.png|450px|thumb|center|The research project \"Besatzfisch\". [http://besatz-fisch.de/content/view/90/86/lang,german/  Source]]]\n* The research project [http://besatz-fisch.de/content/view/34/57/lang,german/ \"Besatzfisch\"] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to '''understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.''' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (''Cooperation''). Second, anglers were questioned about fish population sizes and their economic implications (''Consultation'') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (''Consultation''). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (''Empowerment''). Fifth, social-ecological models were developed based on the previous empirical results. (''Consultation'') Sixth, the project results are published in different forms of media for different target groups (''Information'').\n\n* Another interesting example is the article [https://www.ecologyandsociety.org/vol23/iss2/art9/ \"Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau\"]. In order to '''assess potential future social-ecological developments in Mongolia''', researchers first held a workshop with stakeholders, mostly from academia but with diverse disciplinary backgrounds, and a few non-scientific stakeholders. In this workshop, first, key elements that influence sustainable development in the region were identified. Step by step, these were then ranked to identify critical uncertainties, which led to the development of potential future scenarios (''Consultation''). Also, the stakeholders' opinions on the workshop were later assessed through interviews, indicating positive impacts on their work and perspectives (''Empowerment''). The insights from the workshops were translated into system dynamics models by the researchers that were iteratively feedbacked by the stakeholders (''Consultation''), which led to a final model (''Information''). This approach was not purely transdisciplinary but illustrates a transdisciplinary workflow.\n\n* In the article \"[https://www.researchgate.net/publication/329789267_Developing_an_optimized_breeding_goal_for_Austrian_maternal_pig_breeds_using_a_participatory_approach Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach]\" the authors describe a '''participatory approach conducted in order to develop new indicators (traits) for animal health in pig breeding in Austria'''. First, propositions for new indicators to revise current policy were collected by the directors of the coalition of pig breeders, as well as in a science-led workshop with breeders and staff of the coalition (''Consultation''). The results were assessed based on a literature study and academic consultation. Then, the results were communicated back for feedback, tested on farms, and refined in further transdisciplinary workshops (''Consultation''). Next, pig breeders were asked and trained to record the new traits for one year (''Collaboration'') in a [[Citizen Science]]-like approach. This process showed to improve the quality of research data while generating new knowledge and skills for the breeders (''Empowerment''). The gathered data was feedbacked by the scientists and was set to lead to policy recommendations (''Information'').\n\n\n== Open questions and future directions ==\n[[File:Ivory Tower.jpg|300px|thumb|right|'''The proverbial Ivory Tower is challenged through transdisciplinary research.''' Source: [http://sciblogs.co.nz/app/uploads/2017/03/academias-ivory-tower.jpg Sciblogs].]]\nThe future of transdisciplinary research is promising. However, major hurdles still need to be overcome. These include\n* the question how willing academia is to leave the proverbial 'ivory tower' and get in contact with political, economic and societal actors\n* the need for more experiences and practical guidance on how the aforementioned challenges in the conduction of transdisciplinary research can be solved (integration of knowledge, balancing of interests and methods, communication and mutual learning, joint development of solutions)\n* a lack of educational approaches that support transdisciplinary thinking and working\n\n\"As the world\u2019s natural resources dwindle and the climate changes, the need for sustainability research will increase\u2014as will the need for both integrated research and clear frameworks for conducting such research. Research on sustainability is vital to our collective interests and will require more and more collaboration across various boundaries. The more clearly we can articulate the bridges between those boundaries, the easier those novel collaborations will be.\" (Stock & Burton 2011, p.1106)\n\n\n== Key Publications ==\nLang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges.''\n\nDefila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\nBrandt et al. 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological Economics 92. 1-15.\n\nGAIA Special Episode ''Labs in the Real World - Advancing Transdisciplinarity and Transformations''. \n\nGibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\nWiek, A. and Lang D.J., 2016.Transformational Sustainability Research Methodology\u201d in Heinrichs, H. et al. (eds.), 2016. Sustainability Science, Dordrecht: Springer Netherlands. Available at: http://link.springer.com/10.1007/978-\u201094-\u2010017-\u20107242-\u201062.   \n\n\n== References ==\n* Lang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges''.\n\n* Kates et al. 2015. ''Sustainability Science''.\n\n* Stock, P. Burton, R.J.F. 2011. ''Defining Terms for Integrated (Multi-Inter-Trans-Disciplinary Sustainability Research)''. Sustainability 3. 1090-1113.\n\n* Arnold, A. Piontek, F. ''Zentrale Begriffe im Kontext der Reallaborforschung.'' in: Defila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\n* Gibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\n* Mauser et al. 2013. ''Transdisciplinary global change research: the co-creation of knowledge for sustainability.'' Current Opinion in Environmental Sustainability 5. 420-431.\n\n* Ruppert-Winkel et al. 2015. ''Characteristics, emerging needs, and challenges of transdisciplinary sustainability science: experiences from the German Social-Ecological Research Program.'' Ecology and Society 20(3). 13-30.\n\n* Hall, T. E. O'Rourke, M. 2014. ''Responding to communication challenges in transdisciplinary sustainability science. Heuristics for transdisciplinary sustainability studies: Solution-oriented approaches to complex problems.'' 119-139.\n\n* Allington, G. R. H., M. E. Fernandez-Gimenez, J. Chen, and D. G. Brown. 2018. ''Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau.'' Ecology and Society 23(2):9. \n\n* Pfeiffer, C. Schodl, K. Fuerst-Waltl, B. Willam, A. Leeb, C. Winckler, C. 2018. ''Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach.'' Journal fo Central European Agriculture 19(4). 858-864.\n\n\n== Further Information ==\n* The [http://intrepid-cost.ics.ulisboa.pt/about-intrepid/ INTREPID] network revolves around transdisciplinary and interdisciplinary research and provides useful actors and learnings in this field.\n* [http://www.transdisciplinarity.ch/td-net/Aktuell/td-net-News.html TD-NET] is a resourceful Swiss platform that organizes and presents activities in the field. The same can be said about the [https://complexitycontrol.org/methods-of-transdisciplinary-research/ Complexity or Control blog], hosted at Leuphana University.\n* The [https://www.reallabor-netzwerk.de/ Reallabor-Netzwerk] also hosts useful information about real world laboratories and transdisciplinary research.\n* You can find a lot of considerations concerning transdisciplinarity on [https://i2insights.org/tag/transdisciplinarity-general-relevance/ this part of the 'Integration and Insights' blog]. \n* Mauser et al. (2013, p.420) present the ''[https://futureearth.org/initiatives/ Future Earth Initiative]'', which emerged from Rio+20 and \"will provide a new platform and paradigm for integrated global environmental change research that will be designed and conducted in partnership with society to produce the knowledge necessary for societal transformations towards sustainability\".\n\n----\n[[Category:Normativity of Methods]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "nvvn51yasu7bpbrnc1ect11ca6jvjrl"
                }
            },
            {
                "title": "Transdisciplinarity (German)",
                "ns": "0",
                "id": "499",
                "revision": {
                    "id": "6519",
                    "parentid": "6518",
                    "timestamp": "2022-01-27T18:32:02Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Offene Fragen und zuk\u00fcnftige Entwicklungen */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "26614",
                        "#text": "'''Note:''' This is the German version of this entry. The original, English version can be found here: [[Transdisciplinarity]]. This entry was translated using DeepL and only adapted slightly. Any etymological discussions should be based on the English text.\n\n'''Kurz und knapp:''' Transdisziplinarit\u00e4t ist ein Forschungsmodus, der h\u00e4ufig in der Nachhaltigkeits- und Transformationsforschung angewendet wird. Der folgende Eintrag stellt den Ansatz, seine Charakteristika und die vorherrschenden Herausforderungen vor und veranschaulicht die Anwendung anhand von Beispielen.\n\n__TOC__ \n\n== Multi-, Inter- und Transdisziplinarit\u00e4t ==\nUm Transdisziplinarit\u00e4t zu verstehen, ist sie zun\u00e4chst von Multidisziplinarit\u00e4t und Interdisziplinarit\u00e4t zu unterscheiden. Dies ist insbesondere beim Vergleich deutschsprachiger und englischsprachiger Ressourcen relevant: Das US-amerikanische Verst\u00e4ndnis von Transdisziplinarit\u00e4t ist eher mit Interdisziplinarit\u00e4t vergleichbar, w\u00e4hrend der deutschsprachige Diskurs auf folgenden Unterscheidungen beruht.\n\n[[File:Disciplinary.png|500px|thumb|right|'''Verschiedene Formen von disziplin\u00e4rer Kooperation.''' [http://makinggood.design/thoughts/tasty/ Quelle.]]]\n* '''Multidisziplinarit\u00e4t''' dreht sich um die Zusammenarbeit zwischen verschiedenen akademischen Disziplinen f\u00fcr ein Forschungsvorhaben. Die Forscher*innen untersuchen dasselbe Thema in einer parallelen Struktur und haben alle ihre unterschiedlichen Ziele und Forschungsfragen. Sie teilen ihr Wissen und vergleichen ihre Ergebnisse und k\u00f6nnen ihre Ergebnisse auch in einem gro\u00dfen Bericht zusammenfassen, aber die disziplin\u00e4ren Grenzen bleiben grunds\u00e4tzlich bestehen. (Stock & Burton 2011)\n* '''Interdisziplinarit\u00e4t''' ist ein Forschungsmodus, der ein h\u00f6heres Ma\u00df an Zusammenarbeit zwischen Forscher*innen aus verschiedenen, oft eher unverbundenen Disziplinen vorsieht. Disziplin\u00e4re Standpunkte werden gekreuzt und integriert, um neue Perspektiven zu entwickeln, um neues Wissen zu schaffen oder bestehendes, disziplin\u00e4res Wissen neu zu bewerten. So k\u00f6nnen beispielsweise Politikwissenschaftler*innen und \u00d6kolog*innen zusammenkommen, um gemeinsam ein umweltpolitisches Thema zu untersuchen.  (Stock & Burton 2011)\n* '''Transdisziplinarit\u00e4t''' kann als der n\u00e4chst h\u00f6here Schritt angesehen werden. Transdisziplinarit\u00e4t ist \"ein reflexives, integratives, methodengesteuertes wissenschaftliches Prinzip, das auf die L\u00f6sung oder den \u00dcbergang gesellschaftlicher Probleme und gleichzeitig verwandter wissenschaftlicher Probleme durch Differenzierung und Integration von Wissen aus verschiedenen wissenschaftlichen und gesellschaftlichen Wissensbest\u00e4nden abzielt\" (Lang et al. 2012, S.26f). Die wichtigsten Merkmale sind hier\n** die Zusammenarbeit von Wissenschaft und Gesellschaft in einem wechselseitigen Lern- und Forschungsprozess, in dem Interessenvertreter*innen (Politiker*innen, Unternehmer*innen, NGO-Vertreter*innen, B\u00fcrger*innen usw.) in allen Phasen einbezogen werden, d.h. bei der Formulierung des Problems, der Entwicklung m\u00f6glicher L\u00f6sungen und der Schaffung neuer wissenschaftlicher Erkenntnisse, wobei der Schwerpunkt darauf liegt, dass diese Interessenvertreter*innen ihre spezifischen Kenntnisse und Erfahrungen einbringen, um die Forschungsperspektive zu erweitern; \n** die Konzentration auf Probleme der realen Welt, wie z.B. Fragen nachhaltiger Entwicklung, die nicht nur die akademische Sph\u00e4re, sondern auch die Wirtschaft, Kultur und Politik stark beeinflussen und interessieren; sowie \n** die Entwicklung von L\u00f6sungen f\u00fcr die untersuchten Probleme, die sowohl in wissenschaftliche als auch in praktische Diskurse und Handlungen \u00fcbertragbar sind (Stock & Burton 2011, Lang et al. 2012, Arnold & Piontek 2018, Mauser et al. 2013; Ruppert-Winkel et al. 2015).\n\n== Warum sollte sich (Nachhaltigkeits-)Forschung mit der \u00d6ffentlichkeit auseinandersetzen? ==\nIm Laufe der letzten Jahrhunderte und Jahrzehnte hat die Differenzierung und Institutionalisierung der wissenschaftlichen Disziplinen es der Wissenschaft erm\u00f6glicht, spezifizierte konzeptuelle und methodische Fachkenntnisse zu entwickeln und zu vertiefen und eine eigene Sprache und eigene Themen zu schaffen, die es diesen Disziplinen erm\u00f6glicht haben, eine F\u00fclle von aufschlussreichem Wissen und Orientierungshilfen f\u00fcr die Gesellschaft bereitzustellen (Stock & Burton 2011). '''Es wurde jedoch zunehmend erkannt, dass disziplin\u00e4re Standpunkte und Ans\u00e4tze m\u00f6glicherweise nicht mehr ausreichen, um die vorherrschenden Herausforderungen zu l\u00f6sen''', die sich \u00fcber mehrere wissenschaftliche und gesellschaftliche Bereiche erstrecken. Beispiele hierf\u00fcr sind \u00f6kologische Bedrohungen wie der Verlust der biologischen Vielfalt oder sozio-technologische Herausforderungen wie die Digitalisierung, die die Schaffung neuer theoretischer und empirischer wissenschaftlicher Erkenntnisse, aber auch handlungsorientierte L\u00f6sungen erfordern, die in Politik, Wirtschaft, Bildung, Kultur usw. Anwendung finden k\u00f6nnen. Mit seiner bahnbrechenden Ver\u00f6ffentlichung aus dem Jahr 1994 f\u00fchrte Michael Gibbons den Begriff der \"Mode-2\"-Forschung ein, der diese neu entstehende Form der wissenschaftlichen Untersuchung hervorhebt, die \"gekennzeichnet ist durch eine engere Interaktion zwischen wissenschaftlichen, technologischen und industriellen Modi der Wissensproduktion, durch die Schw\u00e4chung disziplin\u00e4rer und institutioneller Grenzen, durch das Entstehen mehr oder weniger vor\u00fcbergehender Cluster von Experten, die sich oft um gro\u00dfe Projekte verschiedener Art gruppieren, und durch die Erweiterung der Kriterien der Qualit\u00e4tskontrolle und durch eine verst\u00e4rkte soziale Verantwortlichkeit\" (Gibbons 1994, S.68).\n\nMulti- und interdisziplin\u00e4re Forschungsmodi (siehe oben) l\u00f6sen sich bereits teilweise von disziplin\u00e4ren Grenzen, indem Wissen geteilt und gemeinsam erzeugt wird, aber transdisziplin\u00e4re Forschung repr\u00e4sentiert diese neue Form der Wissensproduktion am besten. Modus-2-Forschung und transdisziplin\u00e4re Forschung werden daher oft synonym verwendet. Transdisziplin\u00e4re Forschung erm\u00f6glicht nicht nur die Schaffung von Systemwissen (Ist-Zustand) und Zielwissen (Soll-Zustand), sondern vor allem von Transformationswissen (wie man dorthin gelangt), das die Anforderungen und Voraussetzungen aller relevanten Akteur*innen ber\u00fccksichtigt. Transdisziplin\u00e4re Forschung ist daher besonders dort von Interesse, wo es an praktischem Wissen \u00fcber ein spezifisches Problem mangelt, L\u00f6sungswege noch nicht bekannt sind und w\u00e4hrend der gesamten Entwicklung von L\u00f6sungen gesellschaftlicher Aushandlungsbedarf besteht. (Arnold & Piontek, S.145f)\n\n'''Transdisziplin\u00e4re Forschung ist f\u00fcr die Nachhaltigkeitswissenschaft von besonderer Bedeutung und hat auf diesem Gebiet in den letzten Jahren immense Anerkennung erfahren.''' \"Nachhaltigkeit ist von Natur aus auch transdisziplin\u00e4r\" (Stock & Burton 2011, S.1091), da sie auf der Pr\u00e4misse der L\u00f6sung von Problemen der realen Welt aufbaut, die tief in \u00f6kologische, politische, wirtschaftliche und soziale Prozesse und Strukturen eingebettet sind und daher ohne die Auseinandersetzung mit diesen Sph\u00e4ren nicht verstanden und gel\u00f6st werden k\u00f6nnen (Kates et al. 2015). Transdisziplin\u00e4re Forschung ist ein geeigneter Ansatz f\u00fcr die Nachhaltigkeitswissenschaft, da sie es erm\u00f6glicht, das Wissen relevanter Interessengruppen einzubeziehen, die normativen Dimensionen gesellschaftlicher Bestrebungen - n\u00e4mlich divergierende Normen, Ziele und Visionen verschiedener gesellschaftlicher Bereiche - zu ber\u00fccksichtigen und die Legitimit\u00e4t, Eigenverantwortung und Rechenschaftspflicht f\u00fcr die gemeinsam entwickelten L\u00f6sungen zu erh\u00f6hen (Lang et al. 2012; Stock & Burton 2011). Der integrative transdisziplin\u00e4re Ansatz hebt systemische Interdependenzen hervor, erm\u00f6glicht ein besseres Verst\u00e4ndnis komplexer Fragen und liefert besseres Wissen, um sozial robuste und anwendbare, effektive L\u00f6sungen zu entwickeln (Lang et al. 2012; Mauser et al. 2015).\n\n== Wie funktioniert transdisziplin\u00e4re Nachhaltigkeitsforschung? ==\nWir werden uns hier auf das idealtypische Modell beziehen, das von Lang et al (2012) vorgestellt wurde. Es f\u00fchrt einen in drei Phasen gegliederten konzeptionellen Leitfaden ein, wie transdisziplin\u00e4re Forschung f\u00fcr die Nachhaltigkeitswissenschaft idealerweise durchgef\u00fchrt werden sollte. Im Folgenden werden die relevanten Schritte jeder Phase sowie allgemeine Prinzipien f\u00fcr den gesamten Prozess aufgef\u00fchrt. Der vorgestellte Prozess ist rekursiv: Sowohl in der wissenschaftlichen als auch in der gesellschaftlichen Praxis auftretende Probleme werden in den transdisziplin\u00e4ren Forschungsansatz integriert. Nach der Realisierung der drei Phasen werden die neu generierten Erkenntnisse und L\u00f6sungen wieder in diese beiden Sph\u00e4ren integriert, wo sie angewendet und konsolidiert werden. Letztlich ergeben sich neue Herausforderungen, die eine erneute transdisziplin\u00e4re Zusammenarbeit erfordern.\n\n==== Das ideal-typische Modell ====\n[[File:TDmodel.png|600px|thumb|left|'''Das ideal-typische Modell transdisziplin\u00e4rer Nachhaltigkeitsforschung.''' Quelle: Lang et al. 2012]]\n\n''Phase A''\n* Ein gemeinschaftliches Forschungsteam aufbauen.\n* Gemeinsame Verst\u00e4ndnisse und Definitionen des Problems schaffen.\n* Gemeinsam Forschungsobjekte und -ziele, Forschungsfragen und Erfolgskriterien definieren.\n* Einen methodischen Rahmen entwerfen.\n\n''Phase B''\n* Rollen und Verantwortlichkeiten zuweisen.\n* Forschungsmethoden und -einstellungen anwenden, um das beabsichtigte Wissen zu schaffen.\n\n''Phase C''\n* Zweidimensionale Integration realisieren.\n* Produkte f\u00fcr beide Parteien (gesellschaftlich und wissenschaftlich) erzeugen.\n\n''Allgemeine Grunds\u00e4tze''.\n* Eine kontinuierliche Evaluierung ber\u00fccksichtigen.\n* Konfliktkonstellationen entsch\u00e4rfen.\n* Zugang und Interesse f\u00fcr die Teilnahme aller Akteur*innen sicherstellen.\n\n==== Herausforderungen ====\nEs gibt eine Vielzahl potenzieller Fallstricke, die das Erreichen des idealtypischen Modells erschweren.\n\n* Mauser et al. (2013) merken an, dass die Rollen und Verantwortlichkeiten aller beteiligten Akteur*innen von Anfang an gekl\u00e4rt und bestehende Ungleichheiten beseitigt werden m\u00fcssen. Ferner merken sie an, dass integrierte Forschungsmodi neue Methoden und Konzepte, neue institutionelle Arrangements, organisatorische Kompetenzen und ma\u00dfgeschneiderte Finanzierungsm\u00f6glichkeiten erfordern. Die Zusammenarbeit zwischen Wissenschaft und Gesellschaft erfordert reflexive Lernprozesse, geeignete Kommunikationsmittel und auch eine Reflexion \u00fcber die Rolle der Wissenschaft im Gesamtunternehmen. Sie machen auch deutlich, dass es m\u00f6glicherweise eine allgemeine systemische Tr\u00e4gheit gegen\u00fcber Ver\u00e4nderungen gibt, die es zu \u00fcberwinden gilt.\n* Lang et al. (2012) nennen verschiedene Probleme bei der Formulierung von Problemen (mangelndes Problembewusstsein, unausgewogene Verantwortung f\u00fcr Probleme), prozessuale Probleme (unzureichende Legitimit\u00e4t der beteiligten Akteur*innen, widerspr\u00fcchliche methodische Standards, mangelnde Integration (Wissen, Strukturen, Kommunikation), diskontinuierliche Beteiligung) sowie Herausforderungen bei der Entwicklung von L\u00f6sungen (Mehrdeutigkeit der Ergebnisse, Angst vor dem Scheitern, begrenzte L\u00f6sungsm\u00f6glichkeiten, mangelnde Legitimit\u00e4t transdisziplin\u00e4rer Ergebnisse, Ausnutzung verzerrter Forschungsergebnisse, Probleme bei der Nachverfolgung von Auswirkungen). Auch \"aus einer konventionelleren Forschungsperspektive k\u00f6nnten Wissenschaftler*innen skeptisch sein in Bezug auf Reliabilit\u00e4t, Validit\u00e4t und andere erkenntnistheoretische und methodologische Aspekte kollaborativer Forschung (''Glaubw\u00fcrdigkeit''). Praktiker*innen und Interessenvertreter*innen hingegen k\u00f6nnten hinsichtlich der praktischen Relevanz der Ergebnisse skeptisch sein (''Salienz'')\". (Lang et al. 2012)\n* Hall & Rourke (2014) erl\u00e4utern die kommunikativen Herausforderungen und behaupten, dass \"nicht alle Herausforderungen, die TDSS [transdisziplin\u00e4re Nachhaltigkeitswissenschaft] bedrohen, Kommunikationsherausforderungen sind, aber Kommunikationszusammenbr\u00fcche jede von ihnen verschlimmern k\u00f6nnen. Aufgrund ihrer zentralen Bedeutung m\u00fcssen die Mitarbeiter*innen darauf achten, eine gesunde Kommunikationsdynamik zu kultivieren; angesichts der vielen Perspektiven, die in einem typischen TDSS-Projekt involviert sind, wird dies jedoch nicht einfach sein. Diese Projekte begegnen komplexen Problemen mit komplexen Antworten, was die Notwendigkeit mit sich bringt, flexibel zu bleiben und auf die Anforderungen der Teilnehmer*innen zu reagieren, sowie die Notwendigkeit, den Ansatz zu modifizieren, wenn neue Informationen und Werte auftauchen\". Sie benennen kommunikative Herausforderungen insbesondere bei der Formulierung von Problemen (Ausschluss wichtiger Perspektiven, unterschiedliche Sichtweise auf das Problem) und der Durchf\u00fchrung der Forschung (Unwilligkeit, pers\u00f6nliche Perspektiven zu teilen, Unverm\u00f6gen, Unterschiede in den individuellen Annahmen zu erkennen oder zu artikulieren, Unsicherheiten und unvollst\u00e4ndiges oder inkompatibles Wissen, begrenzte kognitive F\u00e4higkeiten zur Integration individuellen Teilwissens).\n\n==== Methoden transdisziplin\u00e4rer Forschung ====\nF\u00fcr transdisziplin\u00e4re Arbeit sind mehrere wissenschaftliche Methoden n\u00fctzlich. Viele Methoden, die in der TD-Forschung verwendet werden, stellen einzelne Elemente einer breiteren transdisziplin\u00e4ren Untersuchung dar, wie z.B. Interviews, die in einem ersten Schritt eingesetzt werden k\u00f6nnen. Andere Methoden stammen jedoch spezifisch aus transdisziplin\u00e4ren Bestrebungen oder sind stark mit diesen verbunden. Zu diesen geh\u00f6ren:\n\n* [[Visioning & Backcasting|Visioning]] sowie [[Scenario Planning]], die transdisziplin\u00e4r angelegt sind.\n* [[Living Labs & Real World Laboratories|Reallabore]] sind Forschungsumgebungen, die transdisziplin\u00e4re Forschung erleichtern. \n* [[Social Network Analysis|Soziale Netzwerkanalysen]] k\u00f6nnen helfen, Strukturen und Herausforderungen in Echtwelt-Kontexten zu erkennen.\n* [[Citizen Science]] kann auch transdisziplin\u00e4r angelegt sein.\n\n==== Skills & Tools ====\nVerschiedene [[Skills & Tools]] k\u00f6nnen transdisziplin\u00e4re Forschung unterst\u00fctzen und dabei helfen, einige der zuvor genannten Herausforderungen zu umgehen. Dazu geh\u00f6ren:\n\n* [[Design Thinking]]\n* [[Disney Method]]\n* [[Lego Serious Play]]\n* [[Stakeholder Mapping]]\n* [[World Caf\u00e9]]\n* [[The Art of Hosting | Workshops]]\n\n==== Agency ====\nDie Wahl der Methoden f\u00fcr TD-Forschung ist auch relevant f\u00fcr das Level an [[Agency, Complexity and Emergence (German)|Agency]] und Teilhabe, das den nichtwissenschaftlichen Akteur*innen im Prozess erm\u00f6glicht wird. Man kann vier Handlungsebenen unterscheiden, auf denen gesellschaftliche Akteure an der wissenschaftlichen Forschung beteiligt sind (siehe auch Brandt et al. 2013): \n\n* ''Information'': Stakeholder*innen werden \u00fcber wissenschaftliche Erkenntnisse informiert, beispielsweise in Form von Politikempfehlungen, die das Wissen umsetzbar machen. Dies ist die h\u00e4ufigste Form der Zusammenarbeit zwischen Wissenschaft und Gesellschaft.\n* ''Konsultation'': Ein eindirektionaler Informationsfluss von Praxisakteur*innen (Stakeholder*innen) zur Forschung, zumeist in Form von Frageb\u00f6gen oder Interviews, als Input oder Feedback zu geplanter oder laufender Forschung. Stakeholder*innen liefern Informationen, die f\u00fcr die Forschenden von Interesse sind, aber sie sind nicht aktiv am wissenschaftlichen Prozess beteiligt.\n* ''Kollaboration'': Stakeholder*innen kooperieren mit der Wissenschaft, z.B. durch eine der oben genannten Methoden, um gemeinsam ein bestimmtes Problem zu formulieren und zu l\u00f6sen.\n* ''Empowerment'': Die h\u00f6chste Form der Einbindung nicht-wissenschaftlicher Akteur*innen in der Forschung, bei der marginalisierte oder unterdr\u00fcckte Stakeholder*innen in die Lage versetzt werden, selbstst\u00e4ndig Entscheidungen zu treffen und L\u00f6sungen entwickeln. Empowerment geht \u00fcber die einfache Zusammenarbeit hinaus, da es den Stakeholder*innen die F\u00e4higkeit vermittelt, sich selbst mit Problemen auseinanderzusetzen, anstatt die Wissenschaft bei jedem auftretenden Problem neu miteinzubeziehen.\n\nDie folgenden Beispiele zeigen auf, wie die ausgew\u00e4hlten Methoden kombiniert werden k\u00f6nnen und wie sie sich auf die Agency beziehen.\n\n== Beispiele ==\n[[File:Besatzfisch.png|450px|thumb|center|Das Forschungsprojekt \"Besatzfisch\". [[http://besatz-fisch.de/content/view/90/86/lang,german/  Quelle]]]]\n* Das Forschungsprojekt [http://besatz-fisch.de/content/view/34/57/lang,german/ \"Besatzfisch\"] ist ein gutes Beispiel f\u00fcr ein langfristiges transdisziplin\u00e4res Forschungsprojekt, das sich mit unterschiedlichen methodischen Ans\u00e4tzen besch\u00e4ftigt. In diesem vierj\u00e4hrigen Projekt wurde versucht, '''die \u00f6kologische, soziale und wirtschaftliche Rolle und die Auswirkungen des Besatzfischs in nat\u00fcrlichen \u00d6kosystemen zu verstehen'''. Zun\u00e4chst wurden Fische in die \u00d6kosysteme eingef\u00fchrt und die nachfolgende Populationsdynamik qualitativ und quantitativ gemessen, vieles davon gemeinsam mit den kooperierenden Anglern (\"Kooperation\"). Zweitens wurden die Angler*innen \u00fcber die Gr\u00f6\u00dfe der Fischpopulationen und ihre wirtschaftlichen Auswirkungen befragt (''Konsultation''), bevor die Daten mit Hilfe von monet\u00e4ren Modellen analysiert wurden. Drittens wurden Entscheidungsprozesse auf der Grundlage von Gespr\u00e4chen mit den Angler*innen modelliert und ihre mentalen Modelle \u00fcber das Angeln ausgewertet (''Konsultation''). Viertens wurden partizipative Workshops durchgef\u00fchrt, um die Angler bei der Optimierung ihrer Fischgr\u00fcnde zu unterst\u00fctzen (''Empowerment''). F\u00fcnftens wurden auf der Grundlage der bisherigen empirischen Ergebnisse sozial-\u00f6kologische Modelle entwickelt (''Konsultation''). Zuletzt werden die Projektergebnisse in verschiedenen Medien f\u00fcr unterschiedliche Zielgruppen ver\u00f6ffentlicht (''Information'').\n\n* Ein weiteres interessantes Beispiel ist der Artikel [https://www.ecologyandsociety.org/vol23/iss2/art9/ \"Kombination von partizipativer Szenarioplanung und Systemmodellierung zur Identifizierung von Treibern der zuk\u00fcnftigen Nachhaltigkeit auf der mongolischen Hochebene\"]. Um '''m\u00f6gliche zuk\u00fcnftige sozial-\u00f6kologische Entwicklungen in der Mongolei abzusch\u00e4tzen''', f\u00fchrten die Forscher*innen zun\u00e4chst einen Workshop mit Interessenvertreter*innen durch, haupts\u00e4chlich aus dem akademischen Bereich, aber mit unterschiedlichen disziplin\u00e4ren Hintergr\u00fcnden, und einigen wenigen nicht-wissenschaftlichen Akteur*innen. In diesem Workshop wurden zun\u00e4chst die Schl\u00fcsselelemente identifiziert, die die nachhaltige Entwicklung in der Region beeinflussen. Schritt f\u00fcr Schritt wurden diese dann in eine Rangfolge gebracht, um kritische Unsicherheiten zu identifizieren, die zur Entwicklung potenzieller Zukunftsszenarien f\u00fchrten (\"Konsultation\"). Auch die Meinungen der Interessenvertreter*innen zum Workshop wurden sp\u00e4ter durch Interviews bewertet, was auf positive Auswirkungen auf ihre Arbeit und Perspektiven hinwies (\"Empowerment\"). Die Erkenntnisse aus den Workshops wurden von den Forscher*innen in systemdynamische Modelle \u00fcbersetzt, die von den Interessenvertreter*innen iterativ r\u00fcckgekoppelt wurden (''Konsultation''), was zu einem endg\u00fcltigen Modell f\u00fchrte (''Information''). Dieser Ansatz war nicht rein transdisziplin\u00e4r, sondern illustriert vor allem einen beispielhaften transdisziplin\u00e4ren Arbeitsablauf.\n\n* In dem Artikel \"[https://www.researchgate.net/publication/329789267_Developing_an_optimized_breeding_goal_for_Austrian_maternal_pig_breeds_using_a_participatory_approach \"Entwicklung eines optimierten Zuchtziels f\u00fcr \u00f6sterreichische Mutterschweinrassen unter Verwendung eines partizipativen Ansatzes\"] beschreiben die Autor*innen einen '''partizipativen Ansatz, der durchgef\u00fchrt wurde, um neue Indikatoren (Merkmale) f\u00fcr die Tiergesundheit in der Schweinezucht in \u00d6sterreich zu entwickeln'''. Zun\u00e4chst wurden Vorschl\u00e4ge f\u00fcr neue Indikatoren zur \u00dcberarbeitung der aktuellen Politik von den Direktor*innen der Koalition der Schweinez\u00fcchter*innen sowie in einem wissenschaftlich geleiteten Workshop mit Z\u00fcchter*innen und Mitarbeiter*innen der Koalition gesammelt (''Konsultation''). Die Ergebnisse wurden auf der Grundlage einer Literaturstudie und einer wissenschaftlichen Konsultation bewertet. Anschlie\u00dfend wurden die Ergebnisse zur R\u00fcckmeldung zur\u00fcck \u00fcbermittelt, in den Betrieben getestet und in weiteren transdisziplin\u00e4ren Workshops (\"Konsultation\") verfeinert. Als n\u00e4chstes wurden die Schweinez\u00fcchter*innen gebeten und geschult, die neuen Merkmale ein Jahr lang (''Zusammenarbeit'') in einem [[Citizen Science]]-\u00e4hnlichen Ansatz zu erfassen. Es zeigte sich, dass dieser Prozess die Qualit\u00e4t der Forschungsdaten verbessern und gleichzeitig neues Wissen und neue F\u00e4higkeiten f\u00fcr die Z\u00fcchter*innen generieren konnte (''Empowerment''). Die gesammelten Daten wurden von den Wissenschaftlern r\u00fcckgekoppelt und sollten zu politischen Empfehlungen f\u00fchren (''Information'').\n\n== Offene Fragen und zuk\u00fcnftige Entwicklungen ==\n[[File:Ivory Tower.jpg|300px|thumb|right|'''Der sprichw\u00f6rtliche Elfenbeinturm wird durch transdisziplin\u00e4re Forschung hinterfragt.''' Quelle: [http://sciblogs.co.nz/app/uploads/2017/03/academias-ivory-tower.jpg Sciblogs].]]\n\nDie Zukunft der transdisziplin\u00e4ren Forschung ist vielversprechend. Allerdings sind noch gro\u00dfe H\u00fcrden zu \u00fcberwinden. Dazu geh\u00f6ren\n* die Frage, wie bereit die Wissenschaft ist, den sprichw\u00f6rtlichen \"Elfenbeinturm\" zu verlassen und mit politischen, wirtschaftlichen und gesellschaftlichen Akteur*innen in Kontakt zu treten,\n* die Notwendigkeit von mehr Erfahrungen und praktischer Anleitung, wie die oben genannten Herausforderungen bei der Durchf\u00fchrung transdisziplin\u00e4rer Forschung gel\u00f6st werden k\u00f6nnen (Integration von Wissen, Ausgleich von Interessen und Methoden, Kommunikation und gegenseitiges Lernen, gemeinsame Entwicklung von L\u00f6sungen),\n* p\u00e4dagogische Ans\u00e4tze, die transdisziplin\u00e4res Denken und Arbeiten unterst\u00fctzen.\n\n\"In dem Ma\u00dfe, wie die nat\u00fcrlichen Ressourcen der Welt schwinden und sich das Klima ver\u00e4ndert, wird der Bedarf an Nachhaltigkeitsforschung zunehmen, ebenso wie der Bedarf sowohl an integrierter Forschung als auch an klaren Rahmenbedingungen f\u00fcr die Durchf\u00fchrung solcher Forschung. Die Forschung zur Nachhaltigkeit ist f\u00fcr unsere gemeinsamen Interessen von entscheidender Bedeutung und wird mehr und mehr Zusammenarbeit \u00fcber verschiedene Grenzen hinweg erfordern. Je klarer wir die Br\u00fccken zwischen diesen Grenzen artikulieren k\u00f6nnen, desto einfacher werden diese neuartigen Kooperationen sein\" (Stock & Burton 2011, S.1106).\n\n== Wichtige Ver\u00f6ffentlichungen ==\nLang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges.'' Sustainability Science 7. 25-43.\n\nDefila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\nBrandt et al. 2013. ''A review of transdisciplinary research in sustainability science.'' Ecological Economics 92. 1-15.\n\nGAIA Special Episode ''Labs in the Real World - Advancing Transdisciplinarity and Transformations''. \n\nGibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\nWiek, A. and Lang D.J., 2016. ''Transformational Sustainability Research Methodology''. In: Heinrichs, H. et al. (eds.). 2016. ''Sustainability Science''. Dordrecht: Springer Netherlands.\n\n\n== Quellen ==\n* Lang et al. 2012. ''Transdisciplinary research in sustainability science: practice, principles, and challenges''.\n\n* Kates et al. 2015. ''Sustainability Science''.\n\n* Stock, P. Burton, R.J.F. 2011. ''Defining Terms for Integrated (Multi-Inter-Trans-Disciplinary Sustainability Research)''. Sustainability 3. 1090-1113.\n\n* Arnold, A. Piontek, F. ''Zentrale Begriffe im Kontext der Reallaborforschung.'' in: Defila, R. Di Giulio, A. (eds). 2018. ''Transdisziplin\u00e4r und transformativ forschen. Eine Methodensammlung.'' Springer VS.\n\n* Gibbons, M. (ed.) 1994. ''The new production of knowledge: The dynamics of science and research in contemporary societies.'' SAGE.\n\n* Mauser et al. 2013. ''Transdisciplinary global change research: the co-creation of knowledge for sustainability.'' Current Opinion in Environmental Sustainability 5. 420-431.\n\n* Ruppert-Winkel et al. 2015. ''Characteristics, emerging needs, and challenges of transdisciplinary sustainability science: experiences from the German Social-Ecological Research Program.'' Ecology and Society 20(3). 13-30.\n\n* Hall, T. E. O'Rourke, M. 2014. ''Responding to communication challenges in transdisciplinary sustainability science. Heuristics for transdisciplinary sustainability studies: Solution-oriented approaches to complex problems.'' 119-139.\n\n* Allington, G. R. H., M. E. Fernandez-Gimenez, J. Chen, and D. G. Brown. 2018. ''Combining participatory scenario planning and systems modeling to identify drivers of future sustainability on the Mongolian Plateau.'' Ecology and Society 23(2):9. \n\n* Pfeiffer, C. Schodl, K. Fuerst-Waltl, B. Willam, A. Leeb, C. Winckler, C. 2018. ''Developing an optimized breeding goal for Austrian maternal pig breeds using a participatory approach.'' Journal fo Central European Agriculture 19(4). 858-864.\n\n== Weitere Informationen ==\n* Das [http://intrepid-cost.ics.ulisboa.pt/about-intrepid/ INTREPID]-Netzwerk dreht sich um transdisziplin\u00e4re und interdisziplin\u00e4re Forschung und f\u00fchrt hilfreiche Akteure und Erkenntnisse aus diesem Feld an.\n* [http://www.transdisciplinarity.ch/td-net/Aktuell/td-net-News.html TD-NET] ist eine umfangreiche Schweizer Plattform, die Aktivit\u00e4ten in diesem Feld organisiert und pr\u00e4sentiert. Dasselbe trifft auf den [https://complexitycontrol.org/methods-of-transdisciplinary-research/ \"Complexity or Control\"-Blog] zu, der an der Leuphana angesiedelt ist.\n* Das [https://www.reallabor-netzwerk.de/ Reallabor-Netzwerk] bietet ebenfalls n\u00fctzlilche Informationen \u00fcber Reallabore und TD-Forschung an.\n* Mauser et al. (2013, p.420) nennen in ihrem Paper die \"[https://futureearth.org/initiatives/ Future Earth Initiative]\", die aus der Rio+20 Konferenz heraus entstand und \"eine neue Plattform und Paradigmen f\u00fcr integrierte globale Umweltver\u00e4nderungsforschung anbieten wird, die in Partnerschaft mit der Gesellschaft designed und durchgef\u00fchrt wird, um das f\u00fcr gesellschaftliche Transformation hin zur Nachhaltigkeit notwendige Wissen zu erzeugen.\"\n----\n[[Category:Normativity of Methods]]"
                    },
                    "sha1": "dmnvuqbp8ebs4erwlcss2z9aod8453p"
                }
            },
            {
                "title": "Treemap",
                "ns": "0",
                "id": "804",
                "revision": {
                    "id": "5769",
                    "parentid": "5616",
                    "timestamp": "2021-06-13T14:03:52Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Definition */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "6079",
                        "#text": "'''Note:''' This entry revolves specifically around Treemaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br>\n<br>\n'''In short:''' A treemap is a rectangle-based visualization method for large, hierarchical data sets. Originally designed to visualize files on a hard drive and developed by Shneiderman and Johnson. They capture two types of information in the data: (1) the value of individual data points; (2) the structure of the hierarchy.\n__TOC__\n<br>\n\n== Definition ==\nTreemaps display hierarchical (tree-structured) [[Glossary|data]]. They are composed of a series of nested rectangles (tiles) whose areas are proportional to the data they represent. Each branch of the tree is given a rectangle, which is then subdivided into smaller rectangles representing sub-branches. The conceptual idea is to break down the data into its constituent parts and quickly identify its large and small components.\n<br>\n<br/>\n[[File:Switzerlandtreemap.png|400px|thumb|right|Fig.1: Switzerland imports in 2017. Source: commons.wikimedia.org]]\n'''Treemaps are used:''' <br>\n1. To study data with respect to two quantitative values: <br>\n\u2013 positive quantitative value standing for the size of the rectangle (area cannot be negative) and<br>\n\u2013 second or categorical quantitative value standing for the color of the individual rectangles.<br>\n2. To display very large amount of hierarchial data in a limited space.<br> \n3. To make a quick, high-level summary of the similarities and differences within one category as well as between multiple categories (not precise comparisons).\n<br>\n<br>\nThe efficient use of physical space and the intelligent color management make treemaps powerful visualization technique applied to a wide variety of domains. They are used to display significant amounts of information in financial, commercial, governmental and similar fields. The treemap on Fig.1 shows Switzerland imports in 2017.\n[[File:Motorbikestreemap.png|300px|thumb|right|Fig.2: Category-wise sales figure for motorbikes. Source: www.fusioncharts.com]]\n'''Adding new Dimensions.''' With the intelligent use of colors, new dimensions can be added to the diagram. The usual practice is to use color in different rectangles to indicate a second categorical or quantitative value. If color is used to express a quantitative value, it\u2019s strongly encouraged to use only one color (if all the numbers are positive) or two colors (one for negative and one for positive), and vary the intensity of the color to express precise value.\n<br>\n<br>\nThe following treemap (Fig.2) illustrates the category-wise (Street, Cruiser and etc.) sales figure for motorbikes. The size of the rectangles within each category indicates the relative number of sales. Different colors and color intensities show growth and declines of the motorbike sales. \u201cStatic\u201d shows that sales neither grew nor declined. Very intense orange indicates a big shift downward, and very intense green indicates a big shift upwards.\n\nFrom Fig.2 it can be concluded that appropriate use of color enables us to use tree maps to represent losses, declines in sales or other non-positive values. The second quantitative value is not represented by the area of the rectangle.\n<br>\n<br>\nThe way the rectangle is divided and arranged into sub-rectangles depends on '''the tiling algorithm''' used.\n<br>\n<br>\nMany tiling algorithms have been developed and here are some of them:\n<br>\n<br>\n'''Squarified''' - keeps each rectangle as square as possible. It also tries to order the consecutive elements of the dataset (blocks, tiles) in descending order from the upper left corner to the lower right corner of the graph.\n<br>\n<br>\n'''Slice and Dice''' uses parallel lines to divide a root into branches (large rectangles). Then they are subdivided into smaller rectangles representing sub-branches again by using parallel lines. At each level of the hierarchy the orientation of the lines is switched (vertical vs. horizontal).\n\n== R Code ==\nImagine you have book A, consisting of 200 pages, which you use in your statistics course. This book contains of 2 main sections: B (80pages) and C (120pages). B section covers topics of Descriptive Statistics and C section covers topics of Inferential Statistics.\n<br>\n<br>\nTopics of B section are: D(30pages) and E(50pages). D is about sample mean and sample standard deviation while E is about Skewness and Kurtosis.\n<br>\n<br>\nTopics of C section are: F(20pages), G(40pages) and H(60pages). F is about Hypothesis Testing, G covers Confidence Intervals and H focuses on Regression Analysis.\n<br>\n<br>\nYou have tree-structured data and want to make a treemap for displaying the constituent sections of book and make comparisons of its\nsmall and large components.\n[[File:Customtreemap.png|300px|thumb|right|Fig.3]]\n<syntaxhighlight lang=\"R\">\n#Fig.3\nlibrary(treemap) \ngroup = c(rep(\"B\",2), rep(\"C\",3)) \nsubgroup = c(\"D\",\"E\",\"F\",\"G\",\"H\") \nvalue = c(30,50,20,40,60) \ndata= data.frame(group,subgroup,value) \ntreemap(data,index=c(\"group\",\"subgroup\"),\n        vSize = \"value\",\n        palette = \"Set2\",\n        title=\"A\",\n        type=\"index\",\n        bg.labels=c(\"white\"),\n        align.labels=list(c(\"center\", \"center\"), \n                          c(\"right\", \"bottom\")))\n</syntaxhighlight>\n\n==References and further reading material==\n# Ben Shneiderman (1992). \u201cTree visualization with tree-maps: 2-d space-filling approach\u201d. ACM Transactions on Graphics. 11: 92\u201399.\n# Ben Shneiderman, April 11, 2006, Discovering Business Intelligence Using Treemap Visualizations, http://www.perceptualedge.com/articles/b-eye/treemaps.pdf\n# https://towardsdatascience.com/treemaps-why-and-how-cfb1e1c863e8\n# https://www.nngroup.com/articles/treemaps/\n# https://www.fusioncharts.com/resources/chart-primers/treemap-chart/\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Shahlo Hasanova."
                    },
                    "sha1": "ioztt7kc3ok1j0etjc57fj1bipqnace"
                }
            },
            {
                "title": "Types, Expressions, and Variables in Python",
                "ns": "0",
                "id": "1131",
                "revision": {
                    "id": "7278",
                    "parentid": "7272",
                    "timestamp": "2023-08-29T09:32:12Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "comment": "This article explains the role of data types, expressions, variables and comments in Python and how to use them.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "7641",
                        "#text": "Types, Expressions, and Variables in Python\nTHIS ARTICLE IS STILL IN EDITING MODE\n== Introduction ==\nWelcome to this tutorial on how to use Python! In this tutorial, we will explore and learn about some basics needed to get along in Python. This includes different data types, what you can do with them and how to store them. By the end of the tutorial, you will be able to: make comments in Python, know the different data types, use expressions, characters and strings, and select elements of strings.\n== Hello World ==\nWhen learning a new programming language, normally we start with a simple \u201chello world\u201d example. This one line of code will ensure that the environment is set up correctly, and we know how to print a string. \n\n<syntaxhighlight lang=\"Python\" line>\nprint(\u2019hello, world!\u201d)\n</syntaxhighlight>\n\nAfter executing the line above, you should see the Python prints hello, world! Yay!\n\nNote: the \u201cprint ()\u201d above is a function, and you passed the argument \u2018hello, world!\u2019 inside the print function to tell Python what to print. You just learned new keywords: function (\"print()\", argument(\"hello, world\"), and statement (the whole line). \n\n== Comments ==\nBesides writing code, it is always recommended to put a comment on your line of code. Not only that it helps other people to understand your code better, but more importantly it acts as a reminder for you of what you have written! Trust me, even experienced programmers will forget their lines of code after a few weeks. \n\nTo write comments in Python, use the hashtag symbol # before writing your comment. When you run your code, Python will ignore everything past the # on that particular line.\n\n<syntaxhighlight lang=\"Python\" line>\n#Practice writing a comment\nprint (\u2019hello, Eren\u2019) #this prints my name\n#print (\u2019asdf\u2019)\n</syntaxhighlight>\n\n==Data Types==\nData types are how Python understand the types of each data and therefore how to handle them. In many situations, it seems intuitive to humans which data type we are dealing with. However, it is not for Python. Because some commands only work with some data types, it is important to always know which data types are involved in your analysis to use Python. \n\nPopular data types in Python are:\n{| class=\"wikitable\"\n|-\n! Type name!! Example !! Short explanation\n|-\n| int (integer) || 123 || simple numbers (including negative)\n|-\n| float || 1.23 || numbers between integers\n|-\n| boolean || True/False || Can only be True or False (capital T and F)\n|-\n| str (string) || \"hello\" || text\n|-\n| datetime || 2022-04-12 14:05:59.420606 || date and time\n|}\n\nType \u201ccasting\u201d is where a variable can be forced to change to the type that we want. Of course, we want to be careful when doing that, because there could be some information loss in the process. \n\nExample: if we want to know just the integer from a float.\n\n<syntaxhighlight lang=\"Python\" line>\nint(1.6) \n##result will be 1\n</syntaxhighlight>\n\n==Expressions and Variables==\n\nExpressions are operations that Python performs. \n\nExample: 1+2+3 \u2192 the operations here are addition. Popular math operators are: addition (+), subtraction (-), multiplication (*), division (/), and power (**)\n\nVariables are used to store values. \n\nExample: \n\n<syntaxhighlight lang=\"Python\" line>\nmy_number = 1 \n</syntaxhighlight>\n\nIn the code above, we store the value 1 to the variable \u201cmy_number\u201d. \nIt is also possible to update the value of the variable by re-assigning it. \nExample: I failed a math test the first time and I got a perfect score after I retake the exam. \n\n<syntaxhighlight lang=\"Python\" line>\nmy_math_score = 30 #old value\n\nmy_math_score = 100 #new value\n</syntaxhighlight>\n\nNow, we can combine both what we have learned from expression and variables into one. \n\nExample: I have a height of 180 cm, and my brother\u2019s height is 20 cm less than my height. How would this in code look like?\n\n<syntaxhighlight lang=\"Python\" line>\nmy_height = 180\n\nbrothers_height = my_height - 20 \n</syntaxhighlight>\n\n\nAnd to combine what we have learned about data types and variables, we can also check a type of a certain variable using the function type. \n\n<syntaxhighlight lang=\"Python\" line>\ntype(my_height)\n</syntaxhighlight>\n\n\n\nAs we can choose any name for a variable, it is recommended to have a meaningful name for a variable name. A good programmer can take up to two minutes to stop and think about a perfect variable name before programming a complex algorithm.\n\n==Strings==\nTexts, stored as string in Python, are not as simple as it seems. In Python (and others programming languages) it has unique properties that we can take advantage of. \n\nTo define string in Python, use quotation marks, or single quotation. \n\n\u201cStephen\u201d\n\n\u2018Stephen\u2019\n\nA string can be a combination of spaces, digits, and most of symbols / special characters. \n\n\u20181 2 3 4 5 !\u2019\n\nWe can print out string directly using print statement: \n\n<syntaxhighlight lang=\"Python\" line>\nprint(\u201dHello World!\u201d)\n</syntaxhighlight>\n\nWe can bind or assign a string to a variable: \n\n<syntaxhighlight lang=\"Python\" line>\nname = \u201cStephen Kurniawan\u201d\nprint(name)\n</syntaxhighlight>\n\n\nOne unique property of a string is that it is a sequence of characters.\n\nImagine the word: \u201cCAT\u201d, it is made by 3 characters: C, A, and T. \n\nEach character in a string can be called by using its \u201c**index**\u201d.\n{| class=\"wikitable\"\n|-\n! **Char** !! **C**!! **A** !! **T**\n|-\n| **index** || **0** || **1** || **2**\n|}\nThe index of \u201cC\u201d, \u201cA\u201d, and \u201cT\u201d is 0,1,2 respectively. This is called \"zero indexing\" and important to consider whenever you want to analyze data in Python. This works not only for strings but all other data types.\n\nThere is a special way to get the character that we want using an index. \n\nExample: I want to get the \u201cA\u201d from \u201cCAT\u201d using the index []. \n\n<syntaxhighlight lang=\"Python\" line>\npet = \u201cCAT\u201d\n</syntaxhighlight>\n\n<syntaxhighlight lang=\"Python\" line>\nprint(pet[1]) # use square brackets [ ] to specify index. \n</syntaxhighlight>\n\nAnd if we want a sub-string or a set of characters from the string, we can use the slice operator \u201c:\u201d.\n\nExample: I want to get the \u201cAT\u201d from \u201cCAT\u201d\n\n<syntaxhighlight lang=\"Python\" line>\npet = \u201cCAT\u201d\nprint( pet[1:3] ) \n</syntaxhighlight>\nHere again, you learn how Python counts. When using the slice operator, the lower bound (in this case 1) is included and the upper bound (in this case three) is not. In words this means: From the variable \"pet\", print all indexes from the index 1 (second element) to three (fourth element), excluding three. \n\n==Quiz==\n1. my_minutes = 200\nmy_hours = my_minutes / 60\nwhat will be the result of my_hours? \n2. Use the type function to check the type of the variable: my_minutes\n3. What happens when you put \u201c-1\u201d when you index a string? Example: \n<syntaxhighlight lang=\"Python\" line>\npet[-1]  \n</syntaxhighlight>\n4. What happens when you add 2 strings together? \nExample: \n<syntaxhighlight lang=\"Python\" line>\npet_1 = \u201ccat\u201d\npet_2 = \u201cdog\u201d\npet_3 = pet_1 + pet_2\n</syntaxhighlight>\n5.\nKim has three apple trees. From each tree, they can harvest 20 apples. They plant a new tree every year but can only harvest apples one year after it has been planted. Kim has 5 housemates who each want two apples every year and three family members who want 3 apples a year. To pay her rent they need to sell 53 apples a year. Are the apple trees enough to pay the rent and give their friends the apples? If no, when is this the case?\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "rkdefaseansyimcwhrn6dmkjr80rdz4"
                }
            },
            {
                "title": "Venn Diagram",
                "ns": "0",
                "id": "767",
                "revision": {
                    "id": "5960",
                    "parentid": "5383",
                    "timestamp": "2021-06-30T20:41:12Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2308",
                        "#text": "'''Note:''' This entry revolves specifically around Venn Diagrams. For more general information on quantitative data [[Glossary|visualisation]], please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n__TOC__\n<br/>\n== Venn Diagrams ==\n===Discription===\nVenn diagrams are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. This kind of visualisation allows to see logical relationships between data sets, where each is represented by a circle. The overlaps indicate elements common to both data sets. \n\n===R Code===\n\nVenn Diagrams are the most useful to visualise relationships between 2-3 datasets, otherwise the diagram becomes difficult to read if used to represent more groups.\n\nThe <syntaxhighlight land=\"R\" inline>VennDiagram</syntaxhighlight> package in R allows to build Venn Diagrams with the help of its in-built function <syntaxhighlight land=\"R\" inline>venn.diagram()</syntaxhighlight>.\n[[File:Insects Vann diagram1.png|250px|frameless|right]]\n<syntaxhighlight lang=\"R\">\n\n#First download and install the VennDiagramm package\ninstall.packages(\"VennDiagram\")\nlibrary(VennDiagram)\n\n# let's generate three datasets. Each set would represent a sample \n# with identified insect species. The first sample includes 70 insects, \n# the second samples includes 63 insects, and the third one includes\n# 86 insects. \nsample1 <- paste(rep(\"species_\", 70), sample(c(1:100), 70, replace=F), sep=\"\")\nsample2 <- paste(rep(\"species_\", 63), sample(c(1:100), 63, replace=F), sep=\"\")\nsample3 <- paste(rep(\"species_\", 86), sample(c(1:100), 86, replace=F), sep=\"\")\n\n# Now let's create a venn diagram visualising these three data sets and the number of items \n# they have in common. The file with your Venn Diagram will be saved on your hard disk, \n# so you can view and use it seperately.\nvenn.diagram(\n  x <- list(sample1, sample2, sample3),\n  category.names = c(\"Sample 1\" , \"Sample 2 \" , \"Sample 3\"),\n  file = \"Insects_Vann_diagram1.PNG\", \n  fill = c(\"red\", \"green\", \"blue\"),\n  alpha = c(0.5, 0.5, 0.5), \n  cex = 2, \n  cat.fontface = 2, \n  lwd = 2,\n)\n</syntaxhighlight>\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Olga Kuznetsova."
                    },
                    "sha1": "l8de0dgtf3wqxejzh5x01af6hpyagpb"
                }
            },
            {
                "title": "Video Research",
                "ns": "0",
                "id": "651",
                "revision": {
                    "id": "5790",
                    "parentid": "5771",
                    "timestamp": "2021-06-13T23:19:24Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Strengths & Challenges */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11120",
                        "#text": "[[File:ConceptVideoResearch.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Video Research]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Video Research extracts information based on video material of social interactions.\n\n== Background ==\n[[File:Video-based Research.png|thumb|400px|right|'''SCOPUS hits per year for Video-based Research until 2019.''' Search term: 'video analysis' in Title, Abstract, Keywords. Source: own.]]\n\nIn the beginning of the 20th Century, [[Ethnography|ethnographers]] made use of early video technology for so-called 'film studies' that had a focus on \"human behavior and conduct, interaction and communication\" (Jan\u00edk et al. 2009, p.7; 1). Yet, video- or audio-taping was also often used merely to help produce more precise transcriptions of speech in qualitative research and to identify and describe the speaking individuals (4). \n\n'''During the second half of the 20th Century, however, the rising availability and fidelity of video technology led to an increase of its usage and to advancements of the methodology'''. Being able to videotape events, such as classroom sessions, offered researchers new, rich possibilities of qualitative analysis compared to solely relying on taking notes and conducting interviews (1). At the same time, this deep insight limited in turn the potential sample size in the respective studies (4). So while purely qualitative approaches - focusing on single or small numbers of classes - had become a common form of educational analysis by the 1990s, they did not allow for generalizations that were necessary for changes in educational policies. \n\nAs a consequence, quantitative approaches in form of Video Surveys were introduced at the end of the 1980s (1, 4). The most notable research endeavor in this regard was the ''TIMSS Videotape Classroom Study'' in the 1990s (see Key Publications). This study relied on 'Video Surveys', which include bigger case numbers and which were made possible through improvements in multimedia computer technology (1, 4). They combine the potential for statistical analysis and thus more generalizable findings while maintaining the validity provided by the video technology. Due to the technological improvements during the last decades, video-based research was subsequently applied more often (1, 3, 6). Today, video-based methodologies are used mainly in educational research, often in the field of cross-national comparisons, but also in psychology and sociology (1, 4).\n\n\n== What the method does ==\nVideo-based research refers to \"(...) research of social or educational reality based on analysis of video recordings\" (Jan\u00edk et al. 2009, p.7). '''Individual events, such as classroom sessions or cultural ceremonies, are recorded on video. The generated video footage then serves as the basis for the analysis of the respective situation.''' Video-based research is thus a method that includes both [[Glossary|data]] gathering and data analysis (2). The video data is coded in a process similar to [[Content Analysis]]: the events happening in the video and the statements made by, for example, teachers and students are coded according to theory-led, pre-determined or inductively and iteratively developed categories (4). Often, additional data is adduced to provide contextual information (see Challenges).\n\n[[File:Video Research example study.png|600px|thumb|center|'''The design of a study in education research, combining video-taping with additional forms of data gathering.''' Source: (Br\u00fcckmann & Knierim 2008, p.193)]]\n\nThe scope of the research may range from qualitative, unstructured, small-scale observations to more quantitative, large-scale, structured studies, depending on the research questions and theoretical approach (2). Quantitative approaches allow for the identification of trends and variations, while more qualitative approaches strengthen the understanding of prevalent phenomena (1). The possibility of combining both approaches is special about video-based research and enables researchers to quantitatively validate comprehensive qualitative findings (1, 4).\n\n== Strengths & Challenges ==\n* The biggest strength of the method stems from the nature of the data. '''Video material can be slowed down, stopped, rewound, re-watched and stored for a long time.''' This makes video-based research \"(...) a tool for social scientists to observe phenomena that are too complex to be noticed by the naked eye.\" (Jan\u00edk et al. 2009, p.7). The researchers are not limited to what they were able to note down during the event itself, but can assess everything that happened as often as they like (1, 4, 6). At the same time, this complexity can be reduced to a specific aspect of interest for a first analysis, after which the researchers can come back to the data at any later point for further inquiries (2). Long-time comparisons are also possible if similarly taped videos are produced over a span of time. Several researchers - potentially from different disciplinary backgrounds - can code and analyze the material at once, and regularly discuss their insights and exchange perspectives which can lead to the [[Glossary|emergence]] of new ideas and analytic categories (1, 3, 4, 6). This increases inter-coder reliability without having to determine a narrow focus of the research prior to the data collection (3, 4). In addition, the data format facilitates the [[Glossary|communication]] of results since exemplary scenes or images can be taken from the video (4).\n* '''The complexity of the respective situation as captured by the video material allows for both qualitative and quantitative analysis and the application of a range of different research questions and perspectives.''' Also, as mentioned before, qualitative and quantitative results can be integrated because of the rewatchability of the data.\n\nSome of the beneficial methodological elements also pose challenges. \n* Compared to - for example - the act of taking notes, the taping of videos is more complicated, needs more (expensive) video, audio, computer and data storage equipment as well as more technological knowledge and training (3).\n* '''The video data suggests objectivity but may not be objective due to the focus of the camera during the recording.''' If one individual or one perspective is favored, the subsequent analysis has an inherent focus on this element (1, 3, 4). Also, without additional data on the analyzed situation to contextualize the video material that may be gathered, e.g. through [[Ethnography|observations]] or [[Open Interview|Interviews]], researchers may develop a false feeling of understanding - especially foreign-country - (teaching) practices (1).\n* This supplementary data, which may be collected in addition to the videotaping, is also rather essential for a postponed analysis of the video material. If neglected, it may be impossible to properly analyze the videotaped situation some time after the initial research (1, 3).\n* With a camera being present, the subjects / participants may act differently than they usually would (3, 4).\n* Lastly, the act of videotaping individuals brings along moral implications of privacy and confidentiality that need to be addressed beforehand, e.g. by establishing an open relationship with the subjects of research and ensuring agreement on the usage of the video material (1, 3; see Normativity).\n\n== Normativity ==\n* The gathering of video material is very closely related to methods of [[Ethnography]], while the analysis of the gathered data is basically a special form of [[Content Analysis]].\n* As mentioned above, the video data may - or even should - be supplemented with additional qualitative and/or quantitative data, gathered e.g. by the use of a questionnaire, participant observation or through the analysis of documents such as worksheets or work samples in classrooms (1, 4).\n* \"Collecting videotape data requires a careful consideration of ethical and legal obligations regarding the protection of the confidentiality and privacy rights of those individuals who are filmed. If conducting cross-national studies, researchers should be aware that some countries have more fully developed laws and regulations than others, often with specific considerations for minors\" (Jacobs et al. 2007, p.290)\n\n== Key publications ==\nStigler et al. 1999. ''THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.'' U.S. Department of Education. National Center for Education Statistics Washington.\n* The TIMSS study was an empirical milestone for the integration of (comparative) video-based research, comparing the teaching practices in 231 classrooms in three countries.\n\n== References ==\n(1) Ulewicz, M. Beatty, A. (eds.) 2001. ''The Power of Video Technology in International Comparative Research in Education.'' National Academy Press Washington.\n\n(2) Jan\u00edk, T. Seidel, T. Najvar, P. ''Introduction: On the Power of Video Studies in Investigating Teaching and Learning.'' In: Jan\u00edk, T. Seidel, T. (eds). 2009. ''The Power of Video Studies in Investigating Teaching and Learning in the Classroom.'' Waxmann Verlag GmbH M\u00fcnster. 7-19.\n\n(3) Jacobs, J.K. Hollingsworth, H. Givvin, K.B. 2007. ''Video-Based Research Made \"Easy\": Methodological Lessons Learned from the TIMSS Video Studies.'' Field Methods 19. 284-299.\n\n(4) Stigler, J. Gallimore, R.G. 2000. ''Using Video Surveys to Compare Classrooms and Teaching Across Cultures: Examples and Lessons from the TIMSS Video Studies''. Educational Psychologist 35(2). 87-100.\n\n(5) Stigler et al. 1999. ''THE TIMSS VIDEOTAPE CLASSROOM STUDY. Methods and Findings from an Exploratory Research Project in Eighth-Grade Mathematics Instruction in Germany, Japan, and the United States.'' U.S. Department of Education. Natoinal Center for Education Statistics Washington.\n\n(6) Br\u00fcckmann, M. Knierim, B. ''Teaching and learning processes in physics instruction - chances of videotape classroom studies.'' In: Mikelskis-Seifert, S. Ringelband, U. Br\u00fcckmann, M. (eds.) 2008. ''Four Decades of Research in Science Education - from Curriculum Development to Quality Improvement.'' Waxmann Verlag M\u00fcnster.\n\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "f90eh5ssf94gbpny4frso51oj8x31g4"
                }
            },
            {
                "title": "Visioning & Backcasting",
                "ns": "0",
                "id": "589",
                "revision": {
                    "id": "6246",
                    "parentid": "6245",
                    "timestamp": "2021-08-16T12:20:27Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "23514",
                        "#text": "[[File:ConceptVisioning.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization for Visioning]]]]\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| '''[[:Category:Quantitative|Quantitative]]''' || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| [[:Category:Deductive|Deductive]]\n|-\n| style=\"width: 33%\"| [[:Category:Individual|Individual]] || style=\"width: 33%\"| '''[[:Category:System|System]]''' || '''[[:Category:Global|Global]]'''\n|-\n| style=\"width: 33%\"| [[:Category:Past|Past]] || style=\"width: 33%\"| [[:Category:Present|Present]] || '''[[:Category:Future|Future]]'''\n|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' In a Visioning process, one or more desirable future states are developed in a panel of scientific and non-scientific stakeholders. In the process of Backcasting, potential pathways and necessary policies or measures to achieve these future states are developed.\n\n== Background ==\n[[File:Visioning.png|400px|thumb|right|'''SCOPUS hits per year for Visioning until 2019.''' Search term: 'visioning' in Title, Abstract, Keywords. Source: own.]]\nVisioning and Backcasting are historically connected and each of them cannot be thought without the other. '''Backcasting emerged earlier than Visioning during the 1970s and first in the field of energy planning,''' \"(...) growing out of discontent with regular energy forecasting that was based on trend extrapolation, an assumed ongoing increase in the energy demand and a disregard for renewable energy technologies and energy conservation\" (Vergragt & Quist 2011, p.748). Subsequently, Backcasting was furthered in this field in the USA, Canada and Sweden (1, 8). The topics approached through Backcasting shifted towards the field of sustainability after the \"Our Common Future\" report in 1987 (8). \n\n'''Modern Visioning approaches emerged later during the 1980s and 1990s''' with the incorporation of [[System Thinking & Causal Loop Diagrams|System Thinking]] and participatory engagement (1). Since its emergence and due to a rising role of participatory approaches, different versions of Visioning have been developed, including future workshops, community visioning, sustainability solution spaces, future search conference, visioneering and others (1). \n\nToday, Visioning is used most prominently within planning and planning research where it helps guide investments, politics and action programs (1). Examples for this are energy planning or urban planning (2, 3, 5, 7). '''It has also become particularly important in transformational sustainability science''' that tries to directly contribute to real-world sustainability transitions (1). In this field, especially the research body on transition management argues for a combination of social and technological innovation and multi-stakeholder-approaches for sustainable development (3). Still, Visioning research is still at a \"nascent stage\" (Iwaniec & Wiek 2014, p.544), and has not yet fully been established as a method.\n\n\n== What the method does ==\nIn a Visioning process, multidisciplinary stakeholders (most commonly scientific and non-scientific experts on the topic, or also non-experts in participatory approaches) are brought together for a workshop to collect ideas and finally formulate a joint vision as an answer to a previously asked question (1, 8). A vision provides \u201ca key reference point for developing strategies to transition from the current state to a desirable future state, actively avoiding undesirable developments\u201d (Wiek & Iwaniec 2014, p.498). This vision can take the form of qualitative or quantitative goals and targets (1). For example, such a vision could be ''a society based entirely on renewable resources'' or ''a technological process that causes minimum environmental impact'' (2). In theory, a vision can cover every spatial and temporal scale. Visions for the planet in hundreds of years are just as conceivable as a vision for a small company in five years. The exact dimensions depend on the intended goal of the Visioning process. The vision itself may exist on a very basic level, e.g. 'a world without hunger'. However, by adding more specific qualitative and especially quantitative targets and elements, the vision may become more complex. This complicates the subsequent Backcasting process which, based on a systemic perspective, needs to consider various societal and technological elements that influence the intended process of policy-making (2, 6).\n\n'''Visioning combines data gathering, data analysis & interpretation:''' In preparation, sound knowledge of the issues at hand is mandatory which may be developed by analyzing and interpreting existent data on the current state of a particular system (e.g. a country, a company, a landscape) (3, 6). Based on this, a vision for said system is created, generating new qualitative and/or quantitative data. This process can be structured in four steps according to Wiek & Iwaniec (2014, p.504). \n# Framing the Visioning process\n# Creating initial vision material (vision pool)\n# Decomposing and analyzing this material, and finally\n# Revising and recomposing the vision \nThe Visioning process should continuously be reflected upon and revised iteratively (1).\n\nA wide range of possible techniques and settings for vision development is available (6). Among these are abstract forms (vision maps, solution spaces), more realistic visualisations of landscape and city visions using GIS or video techniques; and a set of other visual and digital solutions (decision theaters, digital workshops) (1). More generally, ideas may be collected using e.g. [[Design Thinking]] approaches or [[Glossary|brainstorming]], and sorted through clustering and rating procedures, among others (3). Discussions and thoughts may be secured in form of notes, drawings or voice recordings (5). As per setting, a 'neutral' location (i.e. unaffiliated to specific stakeholders) may be preferable in order not to influence the Visioning process (3). Virtual solutions are possible - bringing people together physically, however, may provide benefits to their interaction that virtual solutions lack (3).\n\nComplementary to the Visioning approach, Backcasting describes the process of developing pathways to reach the vision. While it is thus a method in its own right and not to be understood synonymously with Visioning, these two processes are directly connected to each other.\n\n==== An example of Visioning and Backcasting ====\nThe '1.5\u00b0C' goal that was developed at the 2015 COP is a good example of a vision. It is both qualitative (with regards to the connection between humanity and nature) and quantitative (carbon emissions being at net zero). It may not be entirely realistic or probable, but it serves as a benchmark to be achieved. It was developed in close cooperation between policy actors and academia and implicates transformational political, economic and societal action. The latter is done in form of development pathways that can be seen as a result of the subsequent Backcasting process.\n\n\n== Strenghts & Challenges ==\nVisioning and Backcasting are approaches to thinking about the future, and are to this end comparable to [[Scenario Planning]] (the development of possible futures) and Forecasting (the extrapolation of probable future states). However, Visioning and Backcasting provide advantages - and disadvantages - over these methods:\n* Scenario Planning as well as Forecasting are based on the idea of extrapolating current trends to imagine potential or likely futures. This may be seen as rather unlikely to spur holistic societal change, since it accepts the given trajectories instead of thoroughly questioning them (3, 6). Backcasting that is based on a vision, however, works with desirable futures (visions) and develops policies that ought to lead there. Due to this normative premise (see Normativity), it allows for a stronger connection between the envisioned future and conceivable policy measures (2, 6). Backcasting \"permits a better feel for the effects of different policies\" and \"reduces the tendency (...) for the results of an analysis to be rendered instantly obsolete by the response to it\" (Robinson 1982, p.338). A vision may provide insights into \"(...) associated uncertainties, different perspectives, range of options and strategies to move forward\" (5, p.1922).\n* Backcasting may be the preferable approach when the issue at hand is very complex and influenced by external forces, current trends are part of the problem, major change is needed and the time horizon involved is long enough to allow for considerable deliberate choice. This illustrates the difference between Scenario Planning and Visioning as different approaches to thinking about the future (6).\n* Additionally, insights from psychology indicate that positive, inspirational visions have a stronger motivational impact than a \"what-we-should-be-doing\" (1, p.498) set of push factors, as they may arise during more traditional planning approaches.\n* Aside from the created vision, Visioning can offer further benefits: \u201c(...) [P]articipatory visioning activities fulfill several process-level functions, including building capacity, empowering stakeholders, creating ownership, and developing accountability.\u201d (Wiek & Iwaniec 2014, p.498) It can bring together actors who have never before spoken to each other. A successful Visioning process may thus also be measured based on \"(...) how it subsequently affects the participants' minds and behavior\" (Davies et al. 2012, p.57). It is presumed that even values and convictions may be altered during the process (3).\n\n\n== Normativity ==\n==== Connectedness ====\nVisioning is connected to various other methodological approaches. \n* First, it is based on a [[System Thinking & Causal Loop Diagrams|System Thinking]] approach, recognizing the interconnectedness and causal interference of elements within a system.\n* Visioning and Backcasting processes may take place in a [[Living Labs & Real World Laboratories]] environment.\n* In preparation to the Visioning process, a [[Content Analysis]], [[Open Interview|Interviews]], [[Ethnography|Ethnographic Observations]], [[Survey|Surveys]], Problem Mapping, Supply Chain Analyses, among other approaches, may be undertaken to better understand the current state of the examined system and its prevalent issues (3, 7).\n* A [[Stakeholder Analysis]] may be conducted to identify relevant individuals and actors for the Visioning process.\n* As mentioned before, Visioning processes are commonly combined with a Backcasting process during which the strategies and pathways are developed that are necessary to reach the goals and targets included in the vision. Sometimes, the line between mere Visioning and the development of strategies by Backcasting fades, leading to explicit strategies being developed as a part of the Visioning process (see e.g. (7)).\n* Also, a Visioning process may be followed by Scenario Planning: after envisioning what is desirable, stakeholders may think about what is actually realistic (5).\n* The envisioned futures may be reviewed by further means. If the vision includes quantitative goals for a product, they may be examined using a [[Life Cycle Analysis]]. The visions may also further be verified by [[Interviews]] with relevant individuals whose feedback is further taken into consideration. [[Serious Gaming]] approaches and [[Simulation Games]], [[Thought Experiments]] as well as System Models are conceivable approaches to simulate them and identify potential flaws. This variety highlights the fact that visions are foremost constructs that need to be further implemented in methodological designs to lead to practice-oriented solutions.\n* If desired, the impact that the Visioning process had on the actors involved may be assessed by subsequently conducting Interviews (3).\n\n==== Everything normative about this method ====\nVisioning is normative from its core - it revolves around thinking about desirable futures (1, 2, 8). What is ''desirable'' is a question of cultural, societal and political negotiation (4). This is further complicated by the fact that, in order to create a vision that is shared by and respects all relevant actors, the Visioning process should be participatory and open towards diverse stakeholders' perspectives (1). The question of what is desirable is therefore a very complex one. For more thoughts on the role of normativity in transdisciplinary methods, see the entry on [[Transdisciplinarity]].\n\nThe idea of a vision of a desirable future has been criticized in academia as being too impractical and unrealistic (1, 3, 4). However, this difference to probable scenarios may be seen as an advantage, spurring [[Glossary|creativity]] and opening up mindsets (6). It has therefore been claimed that \u201cvisions ought to be idealistic, free, open, innovative, and, in fact, not (too) realistic\u201d (Wiek & Iwaniec 2014, p.498). At the same time, they should be internally coherent, plausible and motivational in order to serve as helpful guidance for policy-making (1, 6).\n\n\n== Outlook ==\nDue to the method's novelty, there is a lack of sound theoretical base and methodology. Visioning is still in its \u201cnascent stage\u201d (Wiek & Iwaniec 2014, p.509) and there needs to be more agreement on its methodological elements. This development of the method may best be furthered in close collaboration between academia and practice (1).\n\nIt can however be stated that Visioning and Backcasting may provide essential benefits for Sustainability research in the future. Sustainability Science has been framed to aim for System, Target and Transformation Knowledge and targets real-world problems and their solutions in integrated, participatory modes (see Brandt et al. 2013. ''A review of transdisciplinary research in sustainability science''). To this end, a Visioning process builds on system knowledge, and aims for target knowledge, while Backcasting provides transformation knowledge, all in transdisciplinary environments.\n\n\n== An exemplary study ==\n[[File:Visioning exemplary study title.png|450px|frameless|center|The title of Iwaniec & Wiek 2014]]\nIn their 2014 study, '''Iwaniec & Wiek present the process and results of a case study from Phoenix, Arizona, USA.''' Here, the City's Planning Manager approached Arizona State University, which resulted in new planning framework for sustainable urban planning and related educational outcomes. Part of this collaboration was a Visioning workshop, in which a sustainability vision for Phoenix in 2050 was set to be developed with public participation, and later be incorporated in the city's planning processes.\n\n'''The Visioning process followed six stages''' (p.546):<br>\n''1) Framing the process''<br>\nIn the first step, the temporal, spatial, methodological and thematical scope of the visioning process was determined by the legislative requirements and demands of the City of Phoenix Planning Department. They wanted to focus on the pillars Environment, Community, Economy and Infrastructure as well as the year 2050, decided for public participation in the process, and the visioning procedure was intended to become a new paradigm of sustainability-oriented city planning.\n\n[[File:Visioning Example.png|400px|thumb|right|'''The identified main vision elements''' for the whole city arranged in the four domains ''Environment'', ''Community'', ''Economic'' and ''Infrastructure'' according to Step 1. More central elements were more highly prioritized in the initial participatory meetings. Source: Iwaniec & Wiek 2014, p.554.]]\n\n''2) Creating visiong statements and priorities''<br>\nIn 30 small participatory meetings (two in every of the 15 city villages of Phoenix), 13-40 individuals each were introduced to the visioning process, and asked for vision statements, i.e. answers to the question \"Imagine Phoenix as the best it can be in 2050 - What do you see?\". The created statements (759 unique statements across the meetings) were prioritized in a voting activity, which resulted in 15 lists of prioritized vision statements.\n\n''3) Analyzing the vision drafts''<br>\nIn the next step, [[Content Analysis]] was used to deconstruct all vision statements into a standardized element (what should be there in 2050?) and a value proposition (how should it be?) each, which additional descriptive codes \"to specify actors' role, impact, location and spatial scale.\" (p.548). All identified 'vision elements' (1717 in total) then were analyzed using [[System Analysis]], [[Cluster Analysis]], [[Network Analysis]] and further statistical measures. This was done to assess for each village how the elements relate to each other, if there are missing elements, and if elements are contradictory, as well as across the villages to compare the different visions and to see how consistent the overall city vision was. In the end, topical subsystems for the city were extracted, and an initial narrative for the city vision was created based on the results, which was visualized in form of maps for each village and the whole city. The process in step 3 was iterative and a cooperation of the city planners and the researchers.\n\n[[File:Visioning exemplary study result 2.png|400px|thumb|right|'''The revised narrative for subsystem 'mass transit', with images and a story \"In the life of a Phoenician...\".''' Source: Iwaniec & Wiek 2014, p.561]]\n''4) Reviewing and revising the drafts''<br>\nThe fourth phase consisted of a workshop, to which all participants of the 30 smaller meetings were invited, and additional citizens, with a focus on inclusive participation (Hispanic, youth). In a plenary session, all participants were introduced to the results of steps 2-3, i.e. the preliminary vision in form of maps, visuals and a narrative. Then, everyone was distributed into 12 groups of 9-10 participants, with two groups working on one of six urban subsystems each. These subsystems were: Open Governance, Enhanced Roads/ Highways, Responsible Water use, Comprehensive Mass Transit, Lots of Open Space, Dense Urban Cores, Abundant Vegetation. For each subsystem, vision elements were physically presented to the group on gameboards, with replaceable arrows indicating relationships between these elements.\n\nThen, each participant was asked to illustrate all vision elements, and the group created a collage and narrative for their respective subsystem. The physical systemic representation of the subsystem was discussed and adjusted by the group, and vision elements were reprioritized. The new version of the subsystem vision was again discussed, also in terms of its sustainability concerning resources, environmental impact, and social justice. In the end, a concluding narrative for the subsystem was developed.\n\nIn the final plenum, all subsystem narratives were presented, leading to a range of results for the whole city visioning: \"illustrative collages, various photo-documented game-board configurations and descriptions and the final narrations of the visions.\" (p550). Post-workshop feedback was also gathered.\n\n''5) Finalizing the vision''<br>\nNow that the initial vision elements (Step 2) and the results of their systemic analysis (Step 3) were revised (Step 4), the \"revised vision subsystems were reintegrated into a new system map of the city vision.\" (p.550) All visioning results so far were compiled and analyzed again, and a final report was created that \"represented the vision for Phoenix 2050 as a systemic, coherent and sustainable city vision.\" (p.551). The main results of this report found their way into the city's planning documents.\n\n''6) Final review and dissemination''<br>\nThe plan for the final step in this case study was to have the public vote for or against the document which the newly created vision had become a part of. However, due to political reasons, in the end another visioning process was started which built on the results of the presented first process. However, as the researchers highlight, this is in line with the iterative and adaptive underlying visioning methodology. Also, they state that the presented visioning process already helped build capacity in terms of visioning both for the planners and the public, as suggested by a post-workshop survey.\n\n\n== Key Publications ==\n'''Theoretical'''\n* Robinson, J.B. 1982. ''Energy backcasting: A proposed method of policy analysis.'' Energy Policy 10(4). 337-344.\nExplains the Backcasting process and the role of a vision in this process.\n\n* Constanza, R. 2000. ''Visions of alternative (unpredictable) futures and their use in policy analysis.'' Conservation Ecology 4(1).\nIllustrates four visions for the year 2100 and outlines how one might choose from these.\n\n* Wiek, A. Iwaniec, D.M. 2014. ''Quality criteria for visions and visioning in sustainability science.'' Sustainability Science 9. 497\u2013512.\nPresent ten quality criteria and a set of tools and techniques for Visioning, including exemplary publications.\n\n'''Empirical'''\n* Iwaniec, D.M. Wiek, A. 2014. ''Advancing Sustainability Visioning Practice in planning - The General Plan Update in Phoenix, Arizona.'' Planning Practice and Research.\nDescribes the Visioning process step by step in an exemplary project and draws conclusions on how the method may better be implemented in planning practice.\n\n* Gaziulusoy, a.I. Ryan, C. 2017. Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning. The Design Journal 20(1). 1916-1926.\nPresents a project combining Visioning with Scenario Planning and other approaches in order to develop pathways for low-carbon cities in Australia.\n\n* Elkins, L.A. Bivins, D. Holbrook, L. 2009. ''Community Visioning Process. A Tool for Successful Planning.'' Journal of Higher Education Outreach and Engagement 13(4). 75-84.\nIllustrates a project in a small US town where citizens were invited to develop visions and, subsequently, strategies for the future of their community.\n\n\n== References ==\n(1) Wiek, A. Iwaniec, D.M. 2014. ''Quality criteria for visions and visioning in sustainability science.'' Sustainability Science 9. 497\u2013512.\n\n(2) Robinson, J.B. 1982. ''Energy backcasting: A proposed method of policy analysis''. Energy Policy 10(4). 337-344.\n\n(3) Davies, A.R. Doyle, R. Pape, J. 2012. ''Future visioning for sustainable household practices: spaces for sustainability learning?'' Area 44(1). 54-60.\n\n(4) Constanza, R. 2000. ''Visions of alternative (unpredictable) futures and their use in policy analysis.'' Conservation Ecology 4(1).\n\n(5) Gaziulusoy, a.I. Ryan, C. 2017. ''Shifting Conversations for Sustainability Transitions Using Participatory Design Visioning.'' The Design \nJournal 20(1). 1916-1926.\n\n(6) Dreborg, K.H. 1996. ''Essence of backcasting''. Futures 28(9). 813-828.\n\n(7) Elkins, L.A. Bivins, D. Holbrook, L. 2009. ''Community Visioning Process. A Tool for Successful Planning.'' Journal of Higher Education \nOutreach and Engagement 13(4). 75-84.\n\n(8) Vergragt, P.J. Quist, J. 2011. ''Backcasting for sustainability: Introduction to the special issue.'' Technological Forecasting & Social Change 78. 747-755.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Inductive]]\n[[Category:Individual]]\n[[Category:System]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "46uylpbi7i7du24himivatq49j1xm0j"
                }
            },
            {
                "title": "Walking Exercise",
                "ns": "0",
                "id": "1013",
                "revision": {
                    "id": "7076",
                    "parentid": "7075",
                    "timestamp": "2023-04-18T14:23:42Z",
                    "contributor": {
                        "username": "Oskarlemke",
                        "id": "63"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "14620",
                        "#text": "[[File:ConceptWALKINGEXERCISE.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[WALKING EXERCISE]]]]\n\n<br/>\n{|class=\"wikitable\" style=\"text-align: center; width: 50%\"\n! colspan = 3 | Method categorization\n|-\n| [[:Category:Quantitative|Quantitative]] || colspan=\"2\" | '''[[:Category:Qualitative|Qualitative]]'''\n|-\n| '''[[:Category:Inductive|Inductive]]''' || colspan=\"2\"| '''[[:Category:Deductive|Deductive]]'''\n|-\n| style=\"width: 33%\"| '''[[:Category:Individual|Individual]]''' || style=\"width: 33%\"| [[:Category:System|System]] || [[:Category:Global|Global]]\n|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/><br/>\n\n\n''' In short:''' Mental maps are a visual representation of how people perceive their daily environment and how people orient themselves in it. The ''Walking Exercise'' makes use of this strategy to initially build sustainability competencies in higher education settings.\n\n== Background ==\n[[File:VisualisationMentalMapsWALKINGEXERCISE.png|thumb|right|mental maps of Phoenix (a + b) and Hamburg (c + d). Source: (1)]]\nThe world and environment are in a critical state as there are certain sustainability challenges, such as biodiversity loss, global warming, limited resources, and increased inequalities. From this, the need to react to them arises, both locally and globally. Developing certain sustainability competencies (skills, abilities) can be a start to learn how to do so. Wiek et al. (1) have sketched out five competencies that should be considered for academic program development. These are as follows: systems thinking, anticipatory competence, normative competence, strategic competence, and interpersonal/collaborative competence. Based on these, Caniglia et al. (2) have worked out a method, aiming at building some of the aforementioned competencies, which is called ''Walking exercise''. The method combines mental mapping and exploratory walking.\n\n[[File:Transect-walk.png|thumb|right|source: https://www.spool.ac/index.php/spool/article/view/35]]\n'''Mental mapping''' stems from the field of behavioural geography and was especially coined by Kevin Lynch and his work \u201cThe Image of the City\u201d (3). It captures how people perceive their urban environment. Practically speaking, a person\u2019s image of a city is their mental map of it. The map usually entails the following characteristics:\n* paths: routes along which people move throughout the city; \n* edges: boundaries and breaks in continuity; \n* districts: areas characterized by common characteristics; \n* nodes: strategic focus points for orientation like squares and junctions;\n* landmarks: external points of orientation, usually an easily identifiable physical object in the urban landscape.\n\n'''Exploratory walking''' (or transect walking) is a method from the field of city planning often used for observation-based community improvement. Its aim is the gathering of data and experience of one\u2019s own daily environment in a systematic way that transform one\u2019s own perception of it, thereby gaining deeper understanding. This means walking through the environment along a defined path across an area and taking notes on what stands out. Often, it is done in small groups in order to be able to exchange with others.\n\n\n==What the method does==\n\n[[File:Goals.png|thumb|right|Goals of the Walking exercise. Source: (1)]]\n[[File:Walking exercise spiral.png|thumb|right|Phases and steps of the Walking Exercise. Source: (1)]]\n'''The ''walking exercise'' is a bottom-up, student-centered, and experience-based method in higher education settings to develop sustainability competencies in local contexts.''' It is meant for students with no or little previous knowledge in sustainability science, for example first-semester students in the environmental sciences realm and spans over one semester.\n\nThe goal is to actively engage with sustainability problems in one\u2019s surroundings from the beginning on and thereby understand concepts, principles, methods of sustainability and think about solution options.\n\nEssential for this is the '''development of [https://www.youtube.com/watch?v=1vHY2RJnr3A sustainability competencies]''', especially systems thinking, normative, and collaborative competencies as named by Wiek et al. (2011).\n\nSystems thinking means the ability to analyze complex systems and problems across different domains (society, economy, environment) and scales (local to global) in order to engage with and tackle them. \nNormative competencies, or \u201cvalue-focused thinking\u201d, stands for the evaluation of sustainability through different sustainability principles and the ability to discuss and apply values, habits, perceptions and experiences. It is tightly connected with ethics and touches upon reflecting on one\u2019s own position as well.\nBeing able to motivate people and facilitate group processes using non-violent and empathetic communications, as well as actively listening, is the essence of collaborative competencies. As it describes practices between people, it is called interpersonal competence also.\n\nIn order to foster these competencies, students shall perceive and explore their urban environment using mental mapping activities and walking activities (connecting the learning objectives directly with one\u2019s own experience). \nTo do so, both phases (mapping and walking) are performed after one another and share the same four sub-steps: preparation, data gathering and analysis, interpretation and reflection, and lastly, sharing. \n\nIn their learning experience, students are supported by an instructor who guides them by preparing methodological/theoretical inputs, reflection questions, and facilitates in-class discussions. However, it is by no means a frontal teaching style but rather a source of support if needed, as students should learn from their own experiences.\n\n\n\n\n\nThe table below displays an '''ideal-typical process''' of the ''Walking exercise''. \n{| class=\"wikitable\"\n|-\n! phase !! step !! description !! output\n|-\n| '''mental mapping''': perceiving sustainability || preparation || After the class instructor has given a short input on theoretical and methodological foundations on how to visualize and map spaces, students develop a sketch of how they perceive their urban environment and what meaning or feeling certain places hold for them. They complement their sketches with a short questionnaire. Students then compare similarities and differences between their sketches using Lynch\u2019s elements (paths, edges, districts, nodes, landmarks) and discuss what might be the reason for this. ||individual sketch of map; short questionnaire\n|-\n|   || data gathering and analysis || Students learn basic principles about analyzing qualitative data (here: sketches and answers to questionnaires) based on input given by the instructor. They then proceed to interview other people outside of class with their prior developed questionnaires in teams. Afterwards, they code their data on recurring places, feelings, and meanings. In a spreadsheet, students compare the collected answers with their own sketches. || spreadsheet \n|-\n|   || interpretation and reflection || In an attempt to interpret their findings, students specifically look at how different places are connected to one another in people\u2019s experience of the urban environment. They finally visualize their findings into one single image called a shared mental map. Afterwards, they learn about concepts and principles of sustainability and identify a specific local sustainability issue. They assess this selected area or community regarding (un)sustainability. Taking the personal background of their interviewees into account, students reflect on how this might have shaped the answers they received, as well as their own image of the city. Optionally, they visualize (non-)linear cause-effect structures and feedback loops in a causal systems diagram. This is especially recommended if the goal is to develop systems thinking competencies. || shared mental map; written reflection on how people\u2019s background influences their perception of their environment\n|-\n|   || sharing|| In class, students present their results and discuss how social, disciplinary or cultural backgrounds influence people\u2019s image of the city afterwards. Thereby, they learn to do a presentation and to actively listen to and discuss with others. What shall be mentioned here is that this can be a good way of introducing students to the concept of [https://sustainabilitymethods.org/index.php/Gender_First_Aid_Kit intersectionality] as well. In a final reflection paper, they record their own learning process. || presentation; written reflection on own learning process\n|-\n| '''exploratory walking''': exploring sustainability || preparation|| In the second phase, called exploratory walking or transect walking, students learn about walking methods as a scientific approach. The instructor continues with presenting and distributing the transects, meaning the beginning and final point of the walks. Students then gather in pairs to perform the walks. || -\n|-\n|  || data gathering and analysis|| In the aforementioned pairs, students decide how they want to record their walks, for example photos, videos or notes and proceed to collect data in the chosen format while walking the transects. In addition, students supplement their material with a few evaluative, location-specific questions. If wanted, they can focus their walk on specific issues such as ecosystem services, distribution of environmental benefits, burdens with social justice or others. After the walk, students bring together all their collected data and visualize it in one format, creating a virtual version of their walks referred to as \u201cthe narrative of the walk\u201d. || narrative of walk with the help of media formats students find appropriate\n|-\n|  || interpretation and reflection|| As a next step, students look at their observations and select some sustainability issues they have encountered. They then assess their selected issue with sustainability principles (4) they have learned about earlier and create a basic map of it. Optionally, students can create a causal system map where they anticipate future scenarios and identify intervention points. As part of their reflection process, students write down their own individual experiences and how their perceptions might have changed before and after the walk. In addition, they can reflect on how other people\u2019s background affect their perception and how unevenly sustainability issues are distributed in the city. || basic system map of encountered sust. problems\n|-\n|  || sharing|| The last step in the ''Walking Exercise'' framework is presenting the results to class and discussing similarities and differences between the areas they have walked and explored. || presentation\n|}\n\n==Strengths & Challenges==\n'''Strengths'''\n* There is little to no knowledge required beforehand, making it low-threshold and an easy entry point into academic methods, especially for people just starting their path in academia.\n* However, although it is quite a simple method, it helps identify core problems of sustainability in urban environments and visualizes underlying cause-effect structures.\n* Rather than learning passively in a classroom only, students actively engage within this framework, therefore directly tying learning objectives with experience.\n* Because of this, students have higher learning results meaning better acquisition of sustainability competencies. These are of importance for any sustainability-related and solution-oriented work.\n* In a less structured way, mental mapping or exploratory walking can be applied in daily life as well, for instance when roaming around a prior unknown city.\n\n'''Challenges'''\n* Since the method focuses on the visual sense as a way of orienting oneself, it neglects other factors and senses such as hearing or smelling.\n* Moreover, it only takes into account the currently observable situations and features, serving only as an entry point for a more in-depth analysis.\n\n==Normativity==\n*The ''Walking Exercise'' heavily builds on how people perceive their environment and what they consider important, which is individual and subjective to every person and their background, experiences, and biases. During the different steps within the framework and especially in collective decision processes, there can be potential for conflict because of these.\n* Moreover, by making one\u2019s own perception explicit, engaging with it and exchanging with others about theirs, one\u2019s own understanding and perception of surroundings may change in the process.\n* The method therefore holds potential for reflection on ethics/values as well, adding to its normative character.\n\n==Outlook==\n\nCaniglia et al. suggest that, although the learning framework has been developed in a special context, it could be used in different higher education settings (e.g. large introductory classes and campus initiatives), as well as in high-school educational programs and other program-based learning projects. \n\nConcerning the effectiveness of the ''Walking Exercise'' they propose a summative assessment that evaluates to what extent the activities (mapping, walking, discussions, reflection) in the framework actually lead to the achievement of learning objectives, i.e. the initial development of systems thinking, normative and collaborative competencies\n\n==Key publications and References==\n\n(1) [https://link.springer.com/article/10.1007/s11625-011-0132-6 Wiek et al. (2011). Key competencies in sustainability: a reference framework for academic program development.]\n\n(2) [https://www.researchgate.net/publication/309728517_An_experience-based_learning_framework_Activities_for_the_initial_development_of_sustainability_competencies Caniglia et al. (2015): An experience-based learning framework - Activities for the initial development of sustainability competencies.]\n\n(3)  [https://mitpress.mit.edu/9780262620017/the-image-of-the-city/ Lynch, K. (1960). The image of the city. MIT Press.]\n\n(4) [https://www.worldscientific.com/doi/abs/10.1142/S1464333206002517 Gibson, R. B. (2006). Sustainability assessment: basic components of a practical approach.]\n\n----\n[[Category:Qualitative]]\n[[Category:Inductive]]\n[[Category:Deductive]]\n[[Category:Individual]]\n[[Category:Past]]\n[[Category:Present]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Oskar Lemke."
                    },
                    "sha1": "0c0sauz9nsmeqimdcjtdekh4crofoys"
                }
            },
            {
                "title": "Web Scraping in Python",
                "ns": "0",
                "id": "1037",
                "revision": {
                    "id": "7248",
                    "parentid": "7029",
                    "timestamp": "2023-06-29T05:48:40Z",
                    "contributor": {
                        "username": "Milan",
                        "id": "172"
                    },
                    "minor": null,
                    "comment": "This article provides an introduction to web scraping in Python, which allows you to collect data for your research from webpages.",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "11260",
                        "#text": "THIS ARTICLE IS STILL IN EDITING MODE\n==Web Scraping: The Definition==\nWeb Scraping is the process of extracting information from websites in an automated manner. It thus primarily deals with the building blocks of these webpages, (Extensible) Hypertext Markup Language ((X)Html). Many people consider Data Scraping and Web Scraping to be synonyms. However, the former considers extracting data in general, regardless of its storage location, while the latter deals with the data from websites. There are other types of data scraping techniques, such as Screen Scraping and Report Mining that are also of interest but cannot be engaged with here.\n\n==When Do We Use Web Scraping?==\nIn general, we can fetch data from websites by making API calls or by subscribing to RSS feeds. An API is an Interaction Programming Interface. You can imagine it to be two programs talking to each other via this interface. If you make an API call, you ask for permission that one program provides the other program information, in this case, data. An RSS (Rich Site Summary) Feed is basically an automated newsletter, informing you when there is new data on the webpage you are interested in. However, it is not necessary for a website to provide the data via these methods. In such cases, web scraping is a reliable way to extract data. Some practical scenarios where web scraping is useful are listed as follows, \n\n* '''Market Research''': To keep an eye on the competitors without performing time-consuming manual research\n* '''Price Tracking''': Tracking product prices on websites like Amazon as showcased by [https://uk.camelcamelcamel.com/ caramelcaramel]\n* '''News Monitoring''': Periodically extracting news related to selected topics from various websites\n* '''Real Estate Aggregation: Web scraping can be used to aggregate the real estate listings across multiple websites into a single database, see for example [https://www.zillow.com/ Zillow]\n\n==Guidelines for Web Scraping==\nFollowing guidelines should always be considered to perform appropriate and considerate web scraping.\n\n* Always read through the terms and conditions of a website and see if there are any particular rules about scraping.\n* Verify whether the content has been published in the public domain or not. If it has been published in the public domain, there should be no problem in performing web scraping. The easiest way is to use public domain resources, which are often marked such as with the Creative Commons Zero (CCO) license. If you are unsure, consult someone who is in charge of the domain.\n* Never scrape more than what you need and only hit the server at regular and acceptable intervals of time as web scraping consumes a lot of resources from the host server. This means trying to avoid a lot of requests in a short time period. You can also track the response time, as we will see later.\n\n==Web Scraping Using BeautifulSoup==\nWe now focus our attention on the actual process of web scraping using the BeautifulSoup library in Python. Alternatives to this library are also available and have been mentioned in the Further Reading section.\n\nIn this tutorial, we attempt to scrape the data from the [https://www.imdb.com/chart/top/ IMDB Top 250] Movies webpage. Specifically, we will be extracting the following movie details.\n* Rank\n* Rating\n* Crew Details\n* Release Year\n* Number of user ratings\n\nThe prerequisites to follow the tutorial have been listed here:\n\n* Python (Version 3.9.8 is used in this tutorial\n* Requests library\n* BeautifulSoup library\n* pandas library\n\nThe entire process has been detailed in a stepwise manner as seen below.\n\n==Step 1: Inspect Webpage==\nFirst, we use the \"Inspect\" option on the selected webpage by right-clicking somewhere on the webpage and choosing \"inspect\" to open the developer tools window and see the Html structure. Here's a glimpse of how that looks for our IMDB webpage.\n[[File:Image 1.png|centre| 900 px]]\n\n\nNow, we search for the Html element where our data is present. We can do so by using the tool represented by this symbol in Chrome:\n[[File:Chrome search HTML.png|cente right]]\n\nWith this tool, you can just hover any element in the web page and it will show you the respective Html code.\n\nBy using this tool in the IMDB webpage we find that our data is present in various td elements with the classes \"titleColumn\", \"ratingColumn imdbRating\". A sample is shown below:\n[[File:Image 4.png|centre|Data Information IMDB Top 250]]\n\n==Step 2: Fetching the Html Document==\nTo extract the information from the Html document, we need to first fetch it into our code. We use the requests library here. \n<syntaxhighlight lang=\"Python\" line>\nimport requests \n\nurl = \"http://www.imdb.com/chart/top\"\nheaders = {'accept-language': 'en-US'}\nresponse = requests.get(url, headers=headers)\n</syntaxhighlight>\n\nBasically, we provide a URL from which the Html document needs to be fetched to the requests library and ask it to send a GET-type HTTP request to that URL since we only want to fetch the content and not modify it. The 'headers' object here refers to all the HTTP headers we are providing to the request we are about to make. Since we want our movie titles in English, we state that we are only going to accept the language English (en-US) in the HTTP response via the HTTP header accept-language. \n\nFurther information on HTTP requests has been provided in the ''Further Reading'' section.\n\n==Step 3: Processing Html Using Beautifulsoup==\nNow, we extract the required information from the Html document (stored in the 'response' object). For this, we use the BeautifulSoup library.\n<syntaxhighlight lang=\"Python\" line>\nimport time\nstart_time = time.time()\n\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\nmovies = soup.select('td.titleColumn')\nratings = [rating.attrs.get('title') for rating in soup.select('td.ratingColumn.imdbRating strong')]\nmovie_data = []\nfor i in range(len(movies)):\n    rating_info = ratings[i].split(\" \")\n    data = {'rank': movies[i].get_text().strip().split(\".\")[0], \n    'title': movies[i].a.get_text(), \n    'crew': movies[i].a.attrs.get('title'), \n    'release_year': movies[i].span.get_text()[1:-1],\n    'rating': rating_info[0],\n    'num_user_ratings': rating_info[3].replace(\",\", \"\")}\n    movie_data.append(data)\n\nend_time = time.time()\nprint(\"Execution time:\", end_time - start_time, \"seconds\")\n</syntaxhighlight>\n\nLet us now dissect this code snippet.\n\n* First we install BeuatifulSoup which we need for the web scraping, and time, to track the response time. We have also set the start time in the beginning.\n*Then, we create an Html parser to parse the Html document. Parsing means that the Html code is transformed in a way that we can use it in Python\n* We then fetch the Html elements of interest using 'CSS selectors' which run in the backend of the select() function. By passing the 'td.titleColumn' argument, we ask the selector to fetch the td elements with the class 'titleColumn'. This data is stored in 'movies'.\n* We have already seen that the information related to the ratings is present in a different element altogether. So, we use a CSS selector here with a different keystring 'td.ratingColumn.imdbRating strong'. This asks the selector to fetch those td elements which have the classes 'ratingColumn' and 'imdbRating'. Then, we ask the selector to fetch the 'strong' element within each of the chosen td elements (see the Html code in the last figure). Finally, we would like to fetch the content of the 'title' attribute for each selected strong element. This data is stored in 'ratings'. \n\n* We now iterate over 'movies' and 'ratings' objects to fetch the individual strings and extract the required data per movie. The following explains what exactly is happening in each iteration:\n\n    # Since each string in 'ratings' object is formatted as \"(rating)     based on (count) user ratings\", we split the string of the rating column at each space character and pick the first and fourth elements of the split string to get the rating and the number of user ratings. Also, we format the number of user ratings by removing \",\" in the string (the 2nd- 3rd line in the loop)\nRank details are at the very beginning of the text in each movie string (2nd-3rd last line in the loop). \n    # Crew details are present in the title attribute of the anchor tag of the string and the movie title is embedded between the anchor tags. Thus, we use the relevant code to extract these details\n    # Release year is present between the span tags and is formatted as (year). Hence we fetch the content from the span tags and select the substring after removing the first and last elements of the original string.\n* All the extracted details are put into a dictionary object 'data' and a list of all the dictionaries prepared in this manner is created.\n* Lastly, we have set the end time so that we can print the response time. Have an eye on this if you send several requests.\nTo understand this code better, have the inspection tool open and look at the title and the rating column.\n\n==Step 4: Storing the Data==\nWe now look at how to store the data prepared thus far. This is rather simple in Python as we simply store the data in a pandas DataFrame, with the appropriate column names provided, and then convert that to a .csv file.\n\n<syntaxhighlight lang=\"Python\" line>\nimport pandas as pd \n\ndf = pd.DataFrame(movie_data, columns = ['rank', 'title', 'crew', 'release_year', 'rating', 'num_user_ratings'])\ndf.to_csv('movie_data.csv', index=False)\n</syntaxhighlight>\n\nWe have successfully stored our data in a file titled 'movie_data.csv'.\n\n== External Links==\n* [https://www.okta.com/identity-101/data-scraping/ Data Scraping- Okta.com]\n* [https://en.wikipedia.org/wiki/Data_scraping Data Scraping - Wikipedia]\n* [https://kinsta.com/knowledgebase/what-is-web-scraping/ Web Scraping Uses- Kinsta.com]\n* [https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/ Dataquest tutorial For Web Scraping:] Legal aspects of web scraping are also mentioned here\n* [https://realpython.com/beautiful-soup-web-scraper-python/ Real Python Tutorial On BeautifulSoup]\n* [https://www.geeksforgeeks.org/scrape-imdb-movie-rating-and-details-using-python/ GeeksForGeeks Tutorial For IMDB Ratings and Details]\n\n==Further Readings==\n* [https://www.w3schools.com/html/html_xhtml.asp What is Extensible Hypertext Markup Language?]\n* [https://rss.com/blog/how-do-rss-feeds-work/ RSS feed and their uses]\n* [https://www.cloudflare.com/learning/security/api/what-is-api-call/#:~:text=Application%20programming%20interfaces%20(APIs)%20are,provide%20a%20service%20or%20information What is API?]\n* [https://www.crummy.com/software/BeautifulSoup/bs4/doc/ BeautifulSoup Documentation:] For more on CSS Selectors and other ways of using BeautifulSoup\n* [https://www.projectpro.io/article/python-libraries-for-web-scraping/625 BeautifulSoup Alternatives]\n* [https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods Types of HTTP Requests]\n* [https://developer.mozilla.org/en-US/docs/Web/HTTP/Messages HTTP Messages (Requests and Responses)]\n\n[[Category:Statistics]]\n[[Category:Python basics]]\n\nThe [[Table of Contributors|author]] of this entry is XX. Edited by Milan Maushart"
                    },
                    "sha1": "00zjli0auo9igocscsih8ibge0ute82"
                }
            },
            {
                "title": "Why statistics matters",
                "ns": "0",
                "id": "6",
                "revision": {
                    "id": "7025",
                    "parentid": "7013",
                    "timestamp": "2023-04-12T13:45:30Z",
                    "contributor": {
                        "username": "HvW",
                        "id": "6"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "30443",
                        "#text": "'''In short:''' This entry provides a very brief overview on why you should care about statistics.\n\n== The Power of Statistics ==\n'''I consider statistics to be very powerful.''' Statistics became established over the last decades as one of the most important and abundantly used methods in modern science. Due to the development of modern computers and the manifestation of smartphones in the individual's daily life, there is a wealth of information available today. This triggered an almost co-evolutionary pattern where statistics evolved further in tandem with the increase of available data. While initially statistics focused on the calculation in astronomy and was widely preoccupied with testing hypotheses, this has vastly changed today. Much of the data that is available today is not harvested through [[Experiments|systematic experiments]]. Instead we all generate data. There is a swarm intelligence of data being formed. The most popular example is the internet which is basically one big junk of data including many datasets about us people as well as our planet that is not analysed to date. More analysis emerge out of the wealth of information and many of them are based on statistics. \n\n[[File:Bridge.jpg|thumb|right|400px|Building bridges is essential within science.]]\nStatistics thus enable us to find [[Glossary|patterns]], building on large datasets we can aim to find relation and meaning within complex issues and maybe even sometimes reason and explanation. While it is widely a question of the epistemological development of science as well as a clear domain of philosophy the approximation of facts is something that is at the heart of statistics as well. It is within the power of statistics to unravel what we can understand of a given system or dataset. However, it should also be clear what we cannot understand. Very often this is a crucial point: Statistics does not only allow us to be critical of our analysis but also of our understanding and ultimately ourselves. The power of statistics can thus be also framed as the power of understanding our own limitations. Very often the most vital understanding is that we do not understand a lot after all. Here qualitative science and some other would argue first and foremost statistics are often associated with a certain arrogance. Out of that arrogance of statistics to provide answers and, I would argue, also the missing opportunity of researchers to experience societal phenomena, there is often a large scepticism towards statistics. It should be in our power now to hear these misunderstandings and to start building bridges between the conflicting parties. Scientists building on statistics are not only widely disconnected from qualitative researchers, but also from the dominating discourses in society. Only if statistics is integrated into the broader diversity of knowledge that science currently unfolds, we can unleash its full potential. \n\n'''We should aim to maximise our understandings based on statistics if it is possible, but we also need to be aware of the mere fact that statistics will not provide us with all the answers.''' Most statisticians are well aware of that, but science went a little bit on the wrong track when results derived from statistical analysis were communicated. The confidence of the statistical results is overall very clear when it comes to numbers, although these numbers mean different things to different people. Furthermore, numbers have context in certain settings, and this makes it highly implausible that the same number will always mean the same under any given circumstances. If I would proclaim a thought experiment where you have a 10% chance of dying then everybody would be quite unhappy to hear that they have a certain chance of sudden death. If you have on the other hand a lethal disease but a 10% chance of survival, you would probably want to hold onto that 10%. In our desperation,  many would hope and take the 10% chance. Quite often our recognition of numbers is flawed. Imagine that you could join a lottery for a fantastic new computer. 10 Euros today buy you a 10% chance to win this computer tomorrow. Many would probably join in and try to take their chance in this thought experiment. However, not too many would join in if they could get a 10% chance with the same amount of money in 10 years. The chances in this example did not change at all. However a 10% chance tomorrow is just different to most people then a 10% chance in 10 years. \n\nHumans are constructed in a sense where the distant future, the distant past, but also distant places are unreasonably less important to us then they should be. On the other hand, are things close to us unreasonably more important for most people (Parfit, 2013, pp. 56\u201357). Statistics do not have that floor. They are of course [[Bias in statistics|biased]] through the people that conduct them. However, the predictive power of statistics may enable us to do more guided actions. If more people would be able to find structures and patterns in data, they could take more informed decisions. This might be one steppingstone to overcome unreasonable behaviour at least partially. If we would be able to route our decisions more deeply in the rigorous analysis of statistics, then many of their perceptions and approximations that people have of reality might be quite different, and this may influence our behaviour. Right now, the actions of the majority of people are mostly disconnected from statistical data. If more people would have the possibility to understand at least a glimpse of statistics, then transparency and reflexivity could be potentially achieved. It is within our power now to cease the possibilities of statistics, and to translate them into a better understanding of our world.\n\n==Statistics as a part of science==\n\n====Challenges of statistics====\n'''Science creates knowledge.''' This knowledge production follows certain principles. Being a statistician with all the experience I had the privilege to perceive to date, I recognize this as one of the biggest challenge of statistics. Many people, especially those new to statistics, feel a strong urge towards freedom. I think this is wonderful. Radical thinking moves us all forward. However, because of this urge for freedom already existing knowledge is often not valued, as the radical thought for the new is stronger than the appreciation of the already established. This is, in my experience, the first grave challenge that the field of statistics faces. People reject the principles of statistics out of the urge to be radical. I say, be radical, but also [https://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants stand] [https://www.bbc.co.uk/worldservice/learningenglish/movingwords/shortlist/newton.shtml on the shoulders] [https://www.bookbrowse.com/expressions/detail/index.cfm/expression_number/504/on-the-shoulders-of-giants of giants].\n\nThe second grave challenge of statistics is numbers per se. Most people are not fond of numbers. Some are even afraid. Fear is the worst advisor. Numbers are nothing to be afraid of, and likewise, statistics are nothing to be afraid of either. Yet learning statistics is no short task, and this is the third obstacle I see. \n\nLearning a scientific method, any scientific method, takes time. It is like learning an instrument or learning martial arts. Learning a method is not just about a mere internalisation of knowledge, but also about gaining experience. Be patient. Take the time to build this experience. Within this Wiki, I give you a basic introduction of how statistics can help you to create knowledge, and how you can build this experience best.\n\n====Occam's razor====\n[[File:William of Ockham.jpg|thumb|left|William of Occam]]\n\"Entities should not be multiplied without necessity.\"\n\nThe Franciscan friar [https://en.wikipedia.org/wiki/William_of_Ockham William of Occam] almost single-handedly came up with one of the most fundamental principles to date in science. He basically concluded, that \"everything should be as simple as possible, but as complex as necessary.\" Being a principle, it is suggested that this thought extends to all. While in his time it was rooted in philosophy or more specifically in logic,  [https://science.howstuffworks.com/innovation/scientific-experiments/occams-razor.htm Occam's razor] turned out to propel many scientific fields later on, such as physics, biology, theology, mathematics and many more. It is remarkable how this principle purely rooted in theoretical consideration generated the foundation for the scientific method, which would surface centuries later out of it. It also poses as one of the main building blocks of modern statistics as William of Occam came up with the principle of parsimony. While this is well known in science today, we are up until today busy discussing whether things are simple or complex. Much of the scientific debate up until today is basically a pendulum swing between these two extremes, with some people oversimplifying things, while others basically saying that everything is so complex we may never understand it. Occam's razor concludes that the truth is in between.\n\n====The scientific method====\n[[File:Francis Bacon.jpg|thumb|left|Francis Bacon]]\nThe [https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology scientific method] was a true revolution since it enabled science to inductive observations and thus indirectly paved the road towards the testing of hypotheses through observation. Before, science was vastly dominated by theorizing -that is developing theories- yet testing theories proved to be more difficult. While people tended to observe since the dawn of humans, making such observations in a systematic way opened a new world in science. Especially [https://www.britannica.com/science/Baconian-method Francis Bacon] influenced this [https://www.khanacademy.org/humanities/monarchy-enlightenment/baroque-art1/beginners-guide-baroque1/a/francis-bacon-and-the-scientific-revolution major shift], for which he laid the philosophical foundation in his \"Novum Organon\". \n\n'''All observations are normative, as they are made by people.''' This means that observations are constructs, where people tend to see things through a specific \"lens\". A good example of such a specific normative perspective is the number zero, which was kind of around for a long time, but only recognised as such in India and Arabia in the 8th-9th century ([https://en.wikipedia.org/wiki/History_of_the_Hindu\u2013Arabic_numeral_system 0]). Today, the 0 seems almost as if it was always there, but in the antique world, there was no certainty whether the 0 is an actual number or not. This illustrates how normative perspectives change and evolve, although not everybody may be aware of such radical developments as [https://www.sciencealert.com/more-than-half-of-americans-could-be-confused-about-arabic-numbers Arabic numbers].\n\n==A very short history of statistics==\nBuilding on Occam's razor and the scientific method, a new mode of science emerged. Rigorous observation and the testing of hypotheses became one important building block of our civilisation. One important foundation of statistics was [https://www.youtube.com/watch?v=XQoLVl31ZfQ probability theory], which kind of hit it off during the period known as ''[https://en.wikipedia.org/wiki/Age_of_Enlightenment Enlightenment]''. \n\nProbability was important as it enabled differentiation between things happening by chance, or following underlying principles that can be calculated by probability. Of course, probability does not imply that it can be understood why something is not happening by chance, but it is a starting point to get out of a world that is not well understood. Statistics, or more importantly probability, was however not only an important scholar development during the enlightenment, but they also became a necessity.\nOne of the first users of probability was [https://www.britannica.com/science/probability/Risks-expectations-and-fair-contracts Jan de Wit], leader in the Netherlands from 1652 to 1672. He applied the probabilistic theory to determine proper rates of selling annuities. Annuities are payments which are made yearly but back in the days states often collected them during times of war. He stated that annuities and also life insurances should be connected to probability calculations and mortality records in order to determine the perfect charge of payment. \n\nFollowing the [https://www.youtube.com/watch?v=c-WO73Dh7rY Peace of Westphalia], nations were formed at a previously unknown extent, effectively ending the European religious wars. Why is that relevant, you wonder? States need governance, and governance builds at least partly on numbers. Statistics enabled states to get information on demographics and economic development. Double-entry bookkeeping contributed as well. Numbers became a central instrument of control of sovereigns over their nations. People started showing graphs and bar charts to show off their development, something we have not quite recovered from ever since. For example in 1662 [https://en.wikipedia.org/wiki/John_Graunt John Graunt] estimated the population of London by using records on the number of funerals per year, the death rate and the average family size and came to the conclusion that London should have about 384 000 citizens.\nStatisticians were increasingly in demand to account for data, find relations between observations, and basically to find meaning in an increasing plethora of information. The setup and calculation of experiments created another milestone between the two world wars, effectively propelling modern agriculture, [https://www.verywellmind.com/history-of-intelligence-testing-2795581#stanford-binet-intelligence-test psychology], [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1181278/ clinical trials] and many other branches of science.\n[[File:Fisher.jpg|thumb|Ronald Fisher propelled agriculture in the 20th century due to his systemic approach on data analysis. Moreover, his statistical approach found its way in many other scientific fields.]]\n\nHence, statistics became a launchpad for much of the exponential development we observe up until today. While correlation and the Analysis of Variance ([[ANOVA]]) are a bit like the daily bread and butter of statistics, the research focus of the inventor of the ANOVA -[https://www.youtube.com/watch?v=A8gD3CcbDTY Ronald Fisher] - on Eugenics is a testimony that statistics can also be misused or used to create morally questionable or repugnant assumptions. Despite these drawbacks, statistics co-evolved with the rise of computers and became a standard approach in the quantitative branch of science.\n\nWith computers, long-standing theories such as [https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors Bayes Theorem] could be tested, and many new statistical methods developed. Development of statistics led also to new developments in science, as for instance, multivariate statistics paved the road to look at messy datasets, and machine learning revolutionised the way we approach information in the age of computers.\n\nFor another perspective on the history of statistics, we can highly recommend you the [https://www.wnycstudios.org/podcasts/radiolab/episodes/91684-stochasticity Radiolab Podcast about Stochasticity] as well as the longer entry on the [[History of Methods]].\n\n==Key concepts of statistics==\n\n====Models as simplifications====\n\nStatistics is about simplification of complex realities, or about finding patterns in more or fewer complex data. Such simplifications are often called [https://jamesclear.com/all-models-are-wrong models]. \n\nAccording to the statistician Box \"..all models are wrong, but some models are useful. However, the approximate nature of the model must always be borne in mind.\" This famous quotation (Box and Draper 1987, page 424) is often misunderstood. Box talked about specific - in this case polynomial - models. These represent approximations or simplifications and are hence inherently all wrong. Still, some of these specific models can be useful. With this, Box highlights the previously known ''principle of parsimony''. \n\nThe quote is, however, often used to reject the idea of models altogether, which is simply a mistake. There is another famous quote that is associated with Winston Churchill [http://howtogetyourownway.com/statistics/deception_with_statistics.html \"I only believe in statistics that I doctored myself\"]. Most likely this sentence was never said by Churchill but was associated with him by Goebbels to discredit him (Barke, 2011). This showcases that the discussion about the approximation of truth through statistics is a controversial one. Never forget that statistics can generate approximations of truths, but I would argue it cannot generate fundamental truths. \n\nIs that a problem? NO! '''In most cases, approximations will do.''' In other words, it is quite easy to prove something as a fact, but it is much harder to prove that something is not a fact. This is what any good conspiracy theory is built upon. Try to prove that flying saucers do not exist. Yet if one would land in front of you, it is quite evident that there are flying saucers. \n\nThis debate about truths has preoccupied philosophy for quite some time now, and we will not bother ourselves with it here. If you look for generalisable truths, I advise going to [https://www.philosophynews.com/post/2015/01/29/What-is-Truth.aspx philosophy]. But if you look for more tangible knowledge, you are in the right place. Statistics is one powerful method to approximate truths.\n\n====Samples====\n[[File:Beech forest.jpg|thumb|left|400px|Want to sample them all?]]\nIf we would want to understand everything through statistics, we would need to sample everything. Clearly, this is not possible. Therefore, statisticians work with samples. [https://www.thoughtco.com/what-is-statistical-sampling-3126366 Samples] allow us to just look at a part of a whole, hence allowing to find patterns that may represent a whole. \n\nTake an example of leaf size in a forest that consists of Beech trees. You clearly do not need to measure the leaves of all trees. However, your sample should be ''representative''. So, you would want to sample maybe small trees and large trees, trees at the edge of the forest, but also in the center. This is why in statistics people tend to take a ''[https://www.youtube.com/watch?v=A7fcdRhSp8k&feature=youtu.be random sample]''. \n\nThis is quite important, since otherwise, your analysis may be flawed. Much in statistics is dealing with sample designs that enable representative samples, and there are also many analytics approaches that try to identify whether samples are flawed, and what could be done if this is the case. Proper sampling is central in statistics. Therefore, it is advisable to think early within a study about your sample design.\n\n====Analysis====\n\n[[File:R Logo.png|thumb|right|300px|R is a powerful tool for statistical analysis.]]\n\n'''Analysing data takes experience.''' While computers and modern software triggered a revolution, much can go wrong. There is no need to worry, as this can be avoided through hard work. You have to practise to get good at statistical analysis. The most important statistical test can be learned in a few weeks, but it takes time to build experience, enabling you to know when to use which test, and also how to sample data to enable which analysis. \n\nAlso, statistical analysis is made in specific software tools such as R, SPSS or Stata, or even programming languages such as Python or C++. Learning to apply these software tools and languages takes time. If you are versatile in statistical analysis, you are rewarded, since many people will need your expertise. There is more data than experienced statistical analysts in the world. But more importantly, there are more open questions than answers in the world, and some of these answers can be generated by statistical analysis.\n\n====Presenting statistical results====\n[[File:Bildschirmfoto 2020-03-31 um 10.35.59.png|right|400px|thumb|left|The \"Sonntagsumfrage\" in Germany asks on weekly basis which party citizens would vote for if there would be elections on Sunday. The results are always presented in '''barplots'''.]]\n\n[[File:Bildschirmfoto 2020-03-31 um 10.35.47.png|thumb|400px|Another form of presenting election results is the '''pie chart'''. In this example it visualizes the results of the state election in Hessen in 2009.]]\n\nWithin current media as well as the Internet, the most dominating forms of presenting statistical results are the barplot and the pie chart. The first can have some merits in illustrating simple numbers; the second one is often considered to be misleading if not manipulative. However, these two graphics represent the current baseline of knowledge of civil society when it comes to the [https://www.datapine.com/blog/misleading-statistics-and-data/ presentation of statistical results]. Sometimes certain newspapers as well as the Internet present simple correlation plots that are rather intuitive and therefore also understandable for people with some background in science. A prominent example is the [https://www.investopedia.com/ask/answers/042215/what-are-some-examples-positive-correlation-economics.asp Economist], which due to its topical focus quite often relies on correlation plots. \n\nBeyond that, most mainstream graphics that represent statistical results have more of an [https://www.open.edu/openlearn/science-maths-technology/mathematics-and-statistics/statistics/statistics-and-the-media aesthetic value] instead of a true representation of statistical results. When it comes to the actual numbers, only people with an education in science are more or less able to interpret terms such as significance, estimate or correlation. Here, it can be hoped that the landscape will change in the mid-term. Since the dawn of the personal computer and with a broader access to [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4008059/ knowledge] through our mobile phones and the Internet, many people have the possibility to explore statistical results, or at least get in touch with statistical results, even if indirectly. \n\nWhile software for analysing statistical data was in the past widely restricted to experts, the computer revolution even offers [https://www.r-project.org/ open source solutions] available to many people. Hence the proportion of people that are able to code, do statistical analysis, and understand the presentation of statistical results is dramatically increasing. If everybody would be able to interpret correlations and box plots, and would have clear understanding of statistical significance, we would be able to talk of progress.\n\n====Generalisations on statistics====\n\n[https://fs.blog/2015/11/map-and-territory/ \"The map is not the territory\"] basically means that maps are a [http://www.nlpls.com/articles/mapTerritory.php generalisation] and therefore reduce the detailed richness/the richness of details of the territory. It took me very long to get this sentence. While it simply suggests that reality is different from the representation in a map, I think the sentence puzzled me because in my head it is kind of the other way around. The territory is not the [[Geographical Information Systems|map]]. This is how a statistician would probably approach this matter. Statistics are about generalisation, and so are maps. If you would have a map that would contain every detail of the [https://wiki.lesswrong.com/wiki/The_map_is_not_the_territory reality] how you perceive it, it would not only be a gigantic map, but it would be completely useless for orientation in the territory. Maps are so fantastic at least to me because they allow us through clever and sometimes not so clever representation of the necessary details to orientate ourselves in unknown terrain. Important landmarks such as mountains and forests, rivers or buildings allow us to locate ourselves within the territory. \n\nMany maps often coined thematic maps include specific information that is represented in the map and that follows again a generalisation. For example, land-use maps may contain information about agriculture pastures, forests and urbanisation, but for the sake of being understandable do not differentiate into finer categories. Of course for the individual farmer this would not be enough in order to allow for nuanced and contextual land use strategy. However, through the overview we gain an information that we would not get if the map would be too detailed. \n\nStatistics works kind of in the same way. If we looked at any given data point within a huge dataset we would probably be incapable of understanding general patterns. Likewise statistics does draw conclusion from the complicated if not complex data and are able to find meaning or explanations in the plateau of data. Hence I agree that if I want to get a feeling about a specific city that I visit then a map would probably not bring me very far in terms of absorbing the atmosphere of the city. Yet without a map it would be tricky to know where I would be going and how I find my way home again. Our individual data points are often very important to -you guessed it- individuals but the overall patterns are important to many of us. Or at least they should be, I think. But this is [[Big problems for later|another matter]]. [https://www.youtube.com/watch?v=x9BCsy77mlU The map is not the territory], but the territory is also not the map.\n\n==References==\n\nParfit, D. (2013). ''On what matters'' (S. Scheffler, Ed.). Oxford University Press.\n\n[https://en.wikipedia.org/wiki/History_of_statistics History of statistics] by Wikipedia\n\n[http://homepage.divms.uiowa.edu/~dzimmer/alphaseminar/Statistics-history.pdf A Brief History of Statistics]\n\n[https://www.destatis.de/GPStatistik/servlets/MCRFileNodeServlet/BWMonografie_derivate_00000083/8055_11001.pdf Barke, W. (2011).] ''Ich glaube nur der Statistik: Was Winston Churchill \u00fcber Zahlen und die Statistik wirklich sagte und was er gesagt haben soll'' (6. Aufl.). Statistisches Landesamt Baden-W\u00fcrttemberg. \n\n==External links==\n\n====Videos====\n[https://www.youtube.com/watch?v=XQoLVl31ZfQ Bayes Theorem] Here you can watch one of the most popular examples of probabilistic theory\n\n[https://www.youtube.com/watch?v=A8gD3CcbDTY Ronald Fisher] The video is about the life and work of one of the most famous statisticians\n\n[https://www.youtube.com/watch?v=A7fcdRhSp8k&feature=youtu.be Different sampling methods] This video explains the most common sampling methods\n\n[https://www.youtube.com/watch?v=x9BCsy77mlU The map is not the territory] This video explains what we think the world is, and why it is different from what the world really is.\n\n====Articles====\n\nMore about the Quote \"Standing on the shoulders of giants\":[https://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants Wikipedia],[https://www.bbc.co.uk/worldservice/learningenglish/movingwords/shortlist/newton.shtml BBC] \n\n[https://en.wikipedia.org/wiki/William_of_Ockham William of Occam]: Life and Work by Wikipedia\n\n[https://science.howstuffworks.com/innovation/scientific-experiments/occams-razor.htm How Occam's Razor Works]: A comprehensive but well understandable treatise\n\n[https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-the-scientific-method/a/the-science-of-biology What is the scientific method?]: Step by Step\n\n[https://www.britannica.com/science/Baconian-method The \"Baconian Method\"]: A very brief overview\n\n[https://www.khanacademy.org/humanities/monarchy-enlightenment/baroque-art1/beginners-guide-baroque1/a/francis-bacon-and-the-scientific-revolution Francis Bacon and the Scientific Revolution]: A historic perspective (brief)\n\n[https://www.sciencealert.com/more-than-half-of-americans-could-be-confused-about-arabic-numbers 0]: Alarming news\n\n[https://en.wikipedia.org/wiki/History_of_the_Hindu%E2%80%93Arabic_numeral_system History of our numerical system] by Wikipedia\n\n[https://en.wikipedia.org/wiki/Age_of_Enlightenment The Enlightenment] by Wikipedia\n\n[https://en.wikipedia.org/wiki/John_Graunt Who was John Graunt?]: A short biography\n\n[https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors Han Solo and Bayes Theorem]: An enjoyable explanation\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1181278/ One of the first clinical trials]: The work by the statistician Austin Bradford Hill\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2085438/pdf/brmedj03396-0011.pdf Smoking cigarettes causes cancer and heart disease]: A short paper\n\n[http://howtogetyourownway.com/statistics/deception_with_statistics.html I only believe in statistics that I doctored myself]: Being deceptive with statistics\n\n[https://jamesclear.com/all-models-are-wrong All models are wrong. Some models are useful]: A warning.\n\n[https://www.philosophynews.com/post/2015/01/29/What-is-Truth.aspx What is truth?]: An insight into philosophy\n\n[https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/sampling-in-statistics/ Sampling]: An explanation of different methods, types & errors\n\n[https://fs.blog/2015/11/map-and-territory/ The map is not the territory]: A very detailed article\n\n[https://wiki.lesswrong.com/wiki/The_map_is_not_the_territory The map is not the territory]: Differences between belief and reality\n\n[http://www.nlpls.com/articles/mapTerritory.php The map is not the territory]: A brief explanation\n\n[https://www.investopedia.com/ask/answers/042215/what-are-some-examples-positive-correlation-economics.asp The Role of Correlation in Economy]: Examples for presenting statistical results\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4008059/ Presenting statistical results]: A quick paper to read\n\n[https://www.open.edu/openlearn/science-maths-technology/mathematics-and-statistics/statistics/statistics-and-the-media Statistics and the media]: A warning.\n\n[https://www.datapine.com/blog/misleading-statistics-and-data/ Misleading Statistics and Data]: An informative article on the misuse od statistics\n----\n[[Category:Statistics]]\n[[Category:Normativity of Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden."
                    },
                    "sha1": "3qzv83hlijuh675sin1ri1ucfjrirqo"
                }
            },
            {
                "title": "Wordcloud",
                "ns": "0",
                "id": "831",
                "revision": {
                    "id": "6579",
                    "parentid": "5998",
                    "timestamp": "2022-03-21T07:45:28Z",
                    "contributor": {
                        "username": "Ollie",
                        "id": "32"
                    },
                    "minor": null,
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "9748",
                        "#text": "'''Note:''' This entry revolves specifically around Wordclouds. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]]. Another example of a worldcloud can be found in this [[Price_Determinants_of_Airbnb_Accommodations#Wordcloud|entry]].\n<br>\n__TOC__\n\n== Definition ==\nWordcloud is a cluster of words or visualisation of the most commonly or frequently used terms within a given dataset of words. The basic principle is that the higher the frequency, the more prominent the word is in the figure. Wordcloud is a relatively easy and quick text-mining tool which allows to get a rough overview of the word usage within the dataset, which can give further ideas when studying word usage frequency. Wordclouds can also help get first impression of the differences or similarities when working with two textual datasets.\n\n'''Advantages'''\n* Due to their simple design, wordclouds rarely need additional explanation of the visualised results.\n* Straightforward and colourful figure can be often used for aesthetic purposes and in order to quickly get one\u2019s point across.\n[[File:HarryWordCloud.png|300px|thumb|right|'''A Wordcloud of the most commonly used words in the Harry Potter book series.''' Unfortunately, this gives little insight since the result of the visualisation is quite predictable. [https://public.tableau.com/app/profile/dave.andrade/viz/WordCloudsandWizardry/HarryPotterWordCloudDashboard Source]]]\n'''Disadvantages'''\n* If your data set is not prepared and cleaned you might get little to no insight into the data. In this context, \"cleaning\" refers to the deletion of double spaces, numbers, punctuation, fill words (\"and\", \"or\", \"but\", \"is\", \"at\" etc.) and shifting all words to lowercase, among other measures. \n* It is important to have a clear idea why this method is being applied and for which purposes. Otherwise the wordcloud will simply visualise evident or common words with no use for research.\n* A general disadvantage of wordclouds is caused by the so-called [https://www.data-to-viz.com/caveat/area_hard.html \u201carea is a bad metaphor\u201d] phenomenon. Human eyes can have difficulty visually identifying the size of an area, in this context, a word\u2019s size compared to the other words around it. Analytical accuracy of such visualisations can be especially compromised if the words have different lengths: wordclouds tend to emphasise long words over short words as shorter words need less space and thus might appear smaller.\n* Wordclouds consider one word at a time. So, for example, if customers often describe the product they purchased as \u201cnot good\u201d in their reviews, the wordcloud will be likely to visualise it as \u201cnot\u201d and \u201cgood\u201d separately. Taken into account that the words are allocated randomly in the cloud, this only further impairs an accurate analysis of the visualisation.\n\nAll in all, wordclouds can be a good intro exercise when getting familiar with text mining and neural language processing. It offers a simple and comprehensible visualisation, however its features are quite limited if a thorough textual analysis is required. As an alternative, it might be better to use a [[Barplots,_Histograms_and_Boxplots#Barplots|barplot]] or [https://www.data-to-viz.com/graph/lollipop.html a lollipop chart], since they also make it possible to rank words by usage frequency. For example, you can easily allocate the tenth most common word in a barplot, whereas in a wordcloud you might struggle to see the difference in sizes after identifying the first 5 most commonly used terms.\n\n== R Code ==\nLet's try and create our own wordclouds. For the exercise we will use the transcripts of the inaugural speeches of the President of Russia ([http://en.kremlin.ru/events/president/news/57416 from 2016]) and the President of the USA ([https://www.whitehouse.gov/briefing-room/speeches-remarks/2021/01/20/inaugural-address-by-president-joseph-r-biden-jr/ from 2021]). Creating two wordclouds can help us obtain a general understanding of which topics were in the centre of attention for the presidents during their speeches.\n\nBefore starting, we copy the text form the source and save it as a plain text with UTF-8 encoding. Once this is done, we can open our IDE to proceed with text mining.\n\n<syntaxhighlight lang=\"R\">\n# First we activate the necessary libraries\nlibrary(tidyverse) # loads core packages for data manipulation and functional programming\nlibrary(tm) # for text mining\nlibrary(SnowballC) # to get rid of stopwords in the text\nlibrary(wordcloud2) # to visualise as a wordcloud\n\nx = readLines(\"/PresidentRussia.txt\", encoding=\"UTF-8\") #your pathway to the text file\n\n# Transform the vector into a \"list of lists\" corpus for text cleaning in the next step\nx = Corpus(VectorSource(x)) \n\n#Now we are ready to clean the text with the help of the SnowballC library\nx = x %>% \n        tm_map(removeNumbers)%>% # get rid of numbers\n        tm_map(removePunctuation)%>% # get rid of punctiation\n        tm_map(stripWhitespace)%>% # get rid of double spaces if present\n        tm_map(content_transformer(tolower))%>% # switch words to lowercase\n        tm_map(removeWords, stopwords(\"English\"))%>% # remove stopwords in English, such as such as \n                                                        #\"the\", \"is\", \"at\", \"which\", and \"on\".\n        tm_map(removeWords, \n               c(\"and\", \"the\", \"our\", \"that\", \"for\", \"will\")) # remove customer stopwords\n\n# Once we cleaned the text, we are ready to create a table with each word\n# and its calculated frequency and transform it into a matrix \ndtm = TermDocumentMatrix(x)\nmatriz = as.matrix(dtm)\n\n# Next, we create a vector from the matrix rows with quantities and frequencies of the words\nwordsvector = sort(rowSums(matriz), decreasing = TRUE)\n\n# Now we create the final dataframe featuring two variable, word and frequency, to use for visualisation\nx_df = data.frame(\n        row.names = NULL, \n        word = names(wordsvector), \n        freq = wordsvector)\n\n# Once this is done, here is again the whole code used above but in a form of one eloquent function\nnube = function(x){\n        \n        x = Corpus(VectorSource(x))\n        x = x %>% \n                tm_map(removeNumbers)%>% \n                tm_map(removePunctuation)%>% \n                tm_map(stripWhitespace)%>%\n                tm_map(content_transformer(tolower))%>%\n                tm_map(removeWords, stopwords(\"English\"))%>% \n                tm_map(removeWords, \n                       c(\"and\", \"the\", \"our\", \"that\", \"for\", \"will\"))\n        dtm = TermDocumentMatrix(x)\n        matriz = as.matrix(dtm)\n        wordsvector = sort(rowSums(matriz), decreasing = TRUE)\n        x_df = data.frame(\n                row.names = NULL,\n                word = names(wordsvector), \n                freq = wordsvector)\n        return(x_df)\n}\n\n# Now with the help of the function which creates a clean dataframe, we just need to load the .txt file, \n# run it through the function and graph the wordcloud. For the latter we are using our dataframe \n# with 2 columns of words and frequency of words, with an additional setting for the size and colour of words.\n\n#Fig.2 \nPutin = readLines(\"/PresidentRussia.txt\", encoding = \"UTF-8\") \ndf_putin = nube(Putin)\nwordcloud2(data = df_putin[1:2], size = 0.8, color = 'random-dark')\n\n\n# For comparison we can also look at the inaugural speech of US President Joe Biden from 2021\n# \nBiden = readLines(\"/PresidentUS.txt\", encoding = \"UTF-8\") \ndf_biden = nube(Biden)\nwordcloud2(data = df_biden[1:2], size = 0.8, color = 'random-dark')\n</syntaxhighlight>\n\n[[File:PutinSpeech.png|500px|thumb|left|Fig.2: '''Wordcloud of the most used words during Vladimir Putin's inauguration speech in 2016''']]\n[[File:BidenSpeech.png|500px|thumb|center|Fig.3: '''Wordcloud of the most used words during Joe Biden's inauguration speech in 2021''']]\n<br>\n\nAbove we see two wordclouds resulting from out text mining performed on the transcripts of Biden and Putin during their inauguration ceremonies. Based on the visualisations, we can conclude, that the most used word for both presidents were the names of their countries. Both presidents addressed the people by using different words, such as \"people\" and \"citizens\" in case of Vladimir Putin's speech, and \"nation\" and \"Americans\" in case of Biden's speech. Among other more prominent terms are such words as \"democracy\", \"story\", \"war\", \"justice\", \"unity\" and \"work\" used by the American president, and \"challenges\", \"time\", \"history\", \"work\", \"towards\", \"fatherland\", \"achieve\" and \"life\" used by the Russian president.\n\n==References and further reading material==\nThe code was inspired by\n# [https://anamumaq.medium.com/easy-word-cloud-with-r-perus-presidential-debate-version-a1911aa0f1e Easy word cloud with R \u2014 Peru\u2019s presidential debate version] by Ana Mu\u00f1oz Maquera\n# [https://medium.com/swlh/my-first-data-science-project-a-word-cloud-comparing-the-2020-presidential-debates-c4a2c705dae2 \"My First Data Science Project: A Word Cloud Comparing the 2020 Presidential Debates\"] by Tori White\n\nFurther sources\n# [http://en.kremlin.ru/events/president/news/57416 The official transcript of the inaugural speech of the President of Russian (2016)] \n# [https://www.whitehouse.gov/briefing-room/speeches-remarks/2021/01/20/inaugural-address-by-president-joseph-r-biden-jr/ Inaugural Address by President Joseph R. Biden, Jr. (2021)]\n# [https://www.r-graph-gallery.com/196-the-wordcloud2-library.html Guide to the Wordcloud2]\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table_of_Contributors|author]] of this entry is Olga Kuznetsova."
                    },
                    "sha1": "f7r9xx71745ae7tap1ltwz599xe86b8"
                }
            },
            {
                "title": "World Caf\u00e9",
                "ns": "0",
                "id": "304",
                "revision": {
                    "id": "3275",
                    "parentid": "2270",
                    "timestamp": "2020-11-04T10:32:06Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2831",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || [[:Category:Team Size 2-10|2-10]] || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\nThe World Caf\u00e9 is a method for facilitating discussions in big groups. With many participants, discussion rounds tend to be sprawling, slow and dominated by strong speakers. If you want to facilitate a discussion that is more effective, energetic, and inclusive, the World Caf\u00e9 is a helpful technique. It divides participants into moderated subgroups, who then wander together through a parcours of stations with different questions, all the while the atmosphere is relaxed and casual like in a caf\u00e9.\n\n== Goals ==\nSplitting big groups into subgroups fosters inclusive, energetic, effective and in-depth discussions: \n* reserved speakers can feel more comfortable speaking in a smaller group\n* the parcours format allows people to physically move through the room in between discussions\n* the moderator can steer the discussion towards unexplored issues with every new subgroup\n* every participant contributed to the collective results in the end\n\n== Getting started ==\nDepending on group size, room capacities and questions you want to discuss, different stations are set up (can be tables, boards, flipcharts etc.) with a moderator who will introduce the question and lead the discussion. The participants will be divided into as many subgroups as there are stations. Each subgroup will visit every station. The moderator welcomes the subgroup participants and introduces the question. Within a given time slot, the subgroups will discuss the question and write down their ideas and insights, before they then wander to the next station. The moderators remain with their station and welcome the next group. They present the question plus a broad overview of the insights of the former group and deepen the discussion with the new group. After the parcours has been completed by all subgroups, the moderators present the collective discussion results of each question to the full group. \n\nIt is helpful to have one moderator who is in charge of the clock and who manages the parcours direction.\n\n== Links & Further reading ==\nhttp://www.theworldcafe.com/\n\nhttps://www.methodenkartei.uni-oldenburg.de/uni_methode/world-cafe/\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Dagmar M\u00f6lleken."
                    },
                    "sha1": "5aoibnex9pskb36llxz6nxbf7kjsuhz"
                }
            },
            {
                "title": "Writing a Protocol",
                "ns": "0",
                "id": "305",
                "revision": {
                    "id": "2271",
                    "parentid": "2207",
                    "timestamp": "2020-08-03T12:42:04Z",
                    "contributor": {
                        "username": "Christopher Franz",
                        "id": "14"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "733",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== Why & When ==\n\n\n== Goal(s) ==\n\n\n== Getting started ==\n\n\n== Links & Further reading ==\n\n\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]"
                    },
                    "sha1": "brr0o70ojnw1b8mkdmbcutj56jbljw3"
                }
            },
            {
                "title": "Writing a journal article",
                "ns": "0",
                "id": "987",
                "revision": {
                    "id": "6830",
                    "parentid": "6829",
                    "timestamp": "2022-10-29T20:45:05Z",
                    "contributor": {
                        "username": "CarloKr\u00fcgermeier",
                        "id": "11"
                    },
                    "comment": "/* Links & further reading */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "61329",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| [[:Category:Collaborative Tools|Collaborative Tools]] || [[:Category:Software|Software]] || '''[[:Category:Personal Skills|Personal Skills]]''' || [[:Category:Productivity Tools|Productivity Tools]] || '''[[:Category:Team Size 1|1]]''' || ''' [[:Category:Team Size 2-10|2-10]]''' || [[:Category:Team Size 11-30|11-30]] || [[:Category:Team Size 30+|30+]]\n|}\n\nIn this article, you will find a range of material that is hopefully useful to you when you prepare your paper.\n\nThe target audience is research students and other early career researchers, especially (but not only) in the environmental sciences and sustainability. My own background is in the natural sciences, but I have also worked with social scientists a fair bit. My suggestions are necessarily biased by my own experience. If you work in a field that is very different from mine, you may find that some of my suggestions are not as useful to you as others: but hopefully you will still find that some of the material is useful.\n\n==First Steps==\nBefore you write an article for publication in a journal, there are a few basic things that are good to think about.\n\n''Content:'' You need to know what it is that you are planning to write about. Once you know what you want to write about, there is still the question of how to best present the content. Many papers with solid content could have been published in much more highly ranked journals if they were presented slightly differently. The issue in such cases is not so much the main content as such, but how it is prioritized, structured, at which length and level of depth it is communicated, how it is embedded in current debates, and how it is framed.\n\n''Your target journal:'' You need to understand your target journal \u2013 what will appeal to the readers, to the editors, and what kinds of particular policies does the journal have. There are papers that are very good but are rejected because they are submitted to the \u2018wrong\u2019 journal. Moreover, there are brilliant papers that are accepted in a good journal \u2013 but then nobody reads them! That can happen if the paper is a poor fit relative to what the journal overall is about. Study your journals a bit, flick through recent tables of contents, find out something about the editors, read how the journal presents itself in its \u2018scope\u2019 section, and so on. You will find that the same content might fit quite different journals, depending on how it is structured and presented.\n\n===Framing your paper===\nFor a given quality of science, how the paper is framed probably makes the biggest difference for where it is published, and how well read and cited it will be. Framing, essentially, is how you fit your paper into the \u2018bigger picture\u2019, or if you prefer a less neutral term, it is how you sell your paper. My general advice is that you choose the largest plausible frame for your paper that is reasonable. In other words, to make sure you\u2019re widely read it makes sense to find a big hook off which to hang your paper \u2013 but you have to be careful not to oversell your work. This is subjective.\n\nTypical choices for framing papers are current global issues, big theories and new theories, particularly those that are controversial. The trick then is to make the link from the \u2018big issue\u2019 to your study as swiftly and directly as possible at the beginning; and back to the same big issue (plus sometimes additional ones) near the end of the study. A good frame is one that appears reasonable and can be directly linked to your work, and one that your work directly speaks to. A bad frame is one that takes a lot of imagination to see how your study fits in with it, and that your results don\u2019t have a lot to do with.\n\nOften but not always your framing is partly determined by how you planned your study (as opposed to your paper) in the first instance.\n\n''Possible exercises:''\n\n* Brainstorm a list of possible frames. Initially, don\u2019t be too selective \u2014 just write down lots of possible frames  that might be connected with your paper.\n* Read your discipline: where are things at? What\u2019s currently a hot topic? What kinds of topics have recently been published in the most prestigious journals? Does your work fit in with any of those topics?\n* Generally: can you use a certain bandwagon for your benefit, and jump on it for a bit? Don\u2019t actually change your focus too much \u2014 but can your focus be twisted ever so slightly so that it becomes interesting to a broader audience?\n===Planning your paper===\nI suggest several phases to a paper:\n\n# Think about what you want to write. When I say \u2018think about\u2019, this thinking could take many different forms. Ironically, some people only think clearly when they write, others need a whiteboard, others need to talk things through with their friends. No problem! The strategy does not matter, as long as you come up with a sound logic. Note, however, that if you do write at this stage, don\u2019t become too attached to your writing. This first step is really about clarifying your flow of the argument in your head. Communicating it effectively on paper is a separate step.\n# Think about where you want to publish your work. This is an iterative process, with two main consequences. One, you may need to re-think how you choose to frame your paper (see section on framing), and therefore you might have to go back to the previous step. Two, you will find that there is a particular word count you need to fit within.\n# Now that you know the word count, you need to fit the content to the length specifications that you need to work with.\n===Prioritising content===\nIt is important that you prioritise what goes into your paper, and what you leave out. Especially when you start out in research, it seems appealing to pack in as much information as possible into your first few papers. After all, you did a lot of research, which was hard work \u2013 and you gained a lot of insights! Now surely it\u2019s worthwhile communicating these insights as completely as possible.\n\nThat logic holds only in part. Especially if you need to write a short paper (e.g. because that\u2019s what the journal requirements are) it is critically important you set yourself some clear priorities about what matters, and what doesn\u2019t.\n\nBut even when the word count is not a problem, more is not necessarily better. Imagine a paper with three really good points (each of which makes you go \u201cwow, very clever\u201d), two quite good points (which make you go \u201cthat\u2019s a fair point\u201d), and two reasonable points (which make you go \u201csure, but not particularly exciting\u201d). And now imagine an alternative, shorter paper, which just makes the three really good points. Which paper will leave a stronger impression on you?\n\nThis might depend on what sort of a reader you are. But it stands to reason that the short paper, with only really good points, will keep a reader engaged for every single paragraph. The longer version of the same thing, by contrast, might have the reader drift off, flick pages, or skip on to a different paper. So, don\u2019t just think about the marginal contribution of an extra thought or extra text: of course, more text always means more insight. But also think about the average quality of your thoughts: sometimes, additional thoughts that are not particularly interesting just dilute the quality of your work, leaving the reader to feel your paper is \u201cjust average\u201d when in fact you had some very good points \u2013 but those may have got lost among the less good points.\n\n===Fitting the content to the length===\nIt is important that those bits of content that are really important to you are not overshadowed by other bits of content, which actually are less important to you. If the important parts are overshadowed, this effectively means they get lost among other, less important parts. Overshadowing of important content most commonly happens because the good points receive too little space in the paper relative to other, less important points.\n\nTwo main strategies can help with this problem at a general level. First, see my suggestions for prioritizing your content \u2013 the gist is to focus on your most exciting ideas, and perhaps just leave some of your other thoughts untold. Second, you can sometimes use supplementary online material to explain things that are not of central interest to all readers.\n\nIn practical terms, I suggest you write a dot-point outline of your paper, with all headings and sub-headings included; and dot point summaries of the main content below. Such a skeleton outline can be very helpful because it\u2019s all about the logic and the flow. The wording is completely irrelevant at this stage, because this outline needs to make sense only to you (and perhaps your co-authors).\n\nOnce you have an outline, think about which sections are particularly important to you. The relative importance of a section should \u2013 to the extent possible \u2013 be mirrored by its length. In other words, it is not a good idea to have 2000 words of background material, followed by 200 words of brilliance, before a 500 word conclusion. Much better, in this example, would be to have 700 words of background material, followed by (well-structured) 1000 words of brilliance, followed by a 250 word conclusion.\n\nGo through these steps, and the general steps for planning a paper, several times in an iterative fashion. What you end up with is a dot-point outline summarizing your headings and main argument, including an indicative word count that each section will occupy. You can also add key references to this outline.\n\nSuch an outline is an incredibly powerful way to start \"writing\" properly. The idea is to leave the actual writing until the content is completely clear. This has the advantage that you now know precisely what you want to say \u2013 you just have to put it into words. Sections that are too long, rambling, or don\u2019t fit become virtually impossible if you follow this approach.\n\nNote, again, that if you are the kind of person who develops their thoughts as they write, that\u2019s not a problem. But don\u2019t think your initial writing (even if it\u2019s nicely worded prose) should be the same as the paper in the end. Once you have done your first bout of writing, it\u2019s still a good idea to condense things back into an outline, and then write again.\n\n(Of course, with practice, you may not need this elaborate process, but it is a pretty fail-safe approach.)\n\n''Possible exercises:''\n\n* Write and re-write an outline for your paper until the logic is smooth\n* As you go, consider different journal options \u2014 what does this mean for which content you may have to leave out, and how many words would you devote to different sections if you were to submit to different journals?\n\n===Content-to-length-ratio===\nThe content-to-length ratio accurately summarises what more and more high-end journals are all about: cramming as much exciting, new information into as little space as possible. Not all journals make this a high priority, but it is increasingly common. Journals that pride themselves of rapid turnaround, have short papers, have \u2018Letters\u2019 in the journal name, regularly use supplementary online material, and have high impact factors are very likely to like a high content-to-length ratio \u2013 whether they explicitly state this or not. If you get the sense that a high content-to-length ratio is expected of you, it is particularly important that you prioritise your content and fit your content to the prescribed length. Your writing style also becomes more important, because there is no room for extra words.\n==Structure==\nThroughout the writing process, you need to think about structure. Most people think of structure as something that applies to the paper as a whole. However, structure is also important at finer scales, such as the level of sections, paragraphs, and sentences.\n===Structure: the paper===\nNo matter what type of paper you write, it needs to have a clear thread through it, and sections need to clearly link. One of the challenges is that writing is linear \u2013 it has a start point and an end point. By contrast, much academic content is complex \u2013 more like a website, where things are related in many different directions. The challenge of writing is to turn the multi-facetted nature of the content (where everything is related and linked to everything else, like the internet) into a simple, one-directional argument.\n\nSome general principles hold. At the most general level, it\u2019s a good idea to start your paper broad, have specific aims at the end of your introduction, and then have a detailed \u2018meaty\u2019 part in the middle. Towards the end, you need to get back to the big picture, preferably the same context that you started with. The other general principle is that you must not assume background knowledge beyond the obvious in your discipline. In other words, your chain of argument must not leave out steps that are actually important for the reader. Ask yourself how you would need to explain the general gist of your paper to your friend who studies a different discipline, or to your aunt. A good structure tends to make sense even to \u2018uninformed\u2019 people. If they don\u2019t get the basic structure you\u2019re proposing, it\u2019s likely that you have left out important steps in your logic.\n\nI distinguish between a few different types of papers. Standard empirical papers tend to follow a standard structure: introduction, methods, results, discussion, conclusion, acknowledgements, references. Sub-headings are common (and useful) within methods and discussion, in particular, but sometimes also in the results section.\n\nEssay papers are fundamentally different. They have no set rules, which makes it even more important that they follow a logical, and clearly understandable thread. Often, it\u2019s a good idea to outline what this thread is specifically at the end of the Introduction. Also, even though it\u2019s an essay, sub-headings can be immensely useful. Check your subheadings \u2013 if, without any further information, they tell a logical story, you\u2019ve probably worked out a useful structure.\n\nReview papers are different again. Again, there is a lot of flexibility for how exactly those are prepared, and it depends a lot on which journal you\u2019re writing for. It\u2019s worth noting that in ecology and conservation science, journals are increasingly keen on quantitative reviews or meta-analyses (a statistical approach to extract patterns out of multiple case studies).\n\nNo matter which kind of paper you write, headings are critically important. Typically, you will want to use 2-3 heading levels. You need to clearly differentiate these in style, so that it\u2019s easy for readers to see what you\u2019re doing. E.g. use a bold 15 point font for main headings, a bold 13 point font for subheadings, and an italics 12 point font sub-sub-headings. It\u2019s a good idea to match the journal style.\n\n''Possible exercises:''\n* What type of paper are you planning to write? \n* What is a logical sequence of headings?\n\n===Structure: section and paragraph===\nA section consists of several paragraphs. The \u2018rules\u2019 that apply to structuring sections and paragraphs are pretty similar.\n\nThe first sentence and last sentence are disproportionately important. The first sentence needs to be as clear as possible about what the paragraph is about. Avoid first sentences that are just background, or pick up the idea from the previous paragraph. If, for example, your paragraph is about limitations of a particular conceptual model, a good first sentence might be:\n\u201cThe xxx conceptual model has several inherent limitations.\u201d Or, if your heading already says as much, your first sentence can potentially go one step further, such as: \u201cInherent limitations of the xxx conceptual model arise from factor X, factor Y, and factor Z.\u201d In both cases, the first sentence makes it very clear what must come next, namely details about the limitations. This helps the reader in several ways. First, she will know what the section will be all about; and second, this helps her decide whether to read the paragraph or skip straight to the next one. The importance of allowing readers to skim-read cannot be overstated. A good document is one that enables us to quickly grasp the content. First sentences of paragraphs help immensely, if they are well written.\n\nThe last sentence of a paragraph or section has to fully wrap up the content. Make sure that your thought is not left hanging, only 90 % complete. You need to fully finish your thought so the reader has no doubt about your intended \u2018so what\u2019. So, when you think your paragraph is finished, ask yourself: \u2018So what?\u2019 If you just need to read out the last sentence again, you included the so-what. If, however, you need to come up with a new, additional sentence, it shows you hadn\u2019t quite finished your thought. This little exercise won\u2019t always work, but it may help you to test yourself whether your take-home message would actually get through to the reader.\n\nApart from the first and last sentences, many paragraphs list things, explain things, discuss causal relationships, or contrast things. In all cases, simple phrases can help to make the argument clearer. Don\u2019t be afraid of listing things as \u201cFirst, \u2026\u201d and so on, or starting sentences with \u201cAlthough \u2026\u201d. Simple words like these at the beginning of sentences can make it much easier for your reader to find the thread of your argument.\n\n''Possible exercises:''\n* Both in existing papers, and in your own draft, check the first and last sentences of several paragraphs and sections. Do they introduce the content, and wrap up the content? Or are thoughts left hanging?\n* Can you structure your argument within a paragraph more clearly? For example, by using numbered lists, or simple words that link your sentences causally?\n\n===Structure: the sentence===\nDirect sentences are easy to understand, whereas indirect ones are difficult to understand.\n\nThis was an example of a very direct sentence. It has a few properties:\n\n# The most important notion comes first and tells you what the sentence is about.\n# It has a simple structure, in this case two parallel phrases with identical grammar.\n# It uses simple words and avoids jargon. I used \u2018direct\u2019, because it is a short and simple word. And to contrast it with something else, I used \u2018indirect\u2019. I could also have used \u2018convoluted\u2019 or some other word, but the simplest word is the one that creates the sharpest, clearest, black-and-white contrast to the other one. Note that the era of big words is finished, at least in scientific writing! The simpler the better. You don\u2019t come across as smart if you use (unnecessarily) big words, but as overly complicated.\n# It uses unambiguous words. For example, I used \u2018whereas\u2019, which has only one meaning \u2013 I avoided \u2018while\u2019 because this has a temporal meaning as well as a contrasting meaning. Similarly, I used \u2018difficult\u2019 and not \u2018hard\u2019, because hard has multiple meanings.\n# It is short! Many short sentences are much easier to follow than few long ones.\nTwo additional questions concern tense and voice. Most academic writing is in past tense, unless you report generic facts not related to your findings or other recently reported empirical findings. Most importantly, make sure you do not accidentally jump between tenses.\n\nThe use of passive voice was common in academic writing in the past. For example, an ecologist might have reported \u201cAll animals heard or seen were recorded by two experienced observers\u201d. Today, this sentence would probably read \u201cWe counted all animals heard or seen\u201d. Although writing in active voice is now preferred by many journals (some have it in their style guide!), this is not to be mixed up with necessarily using the first person. In the example above, \u2018we\u2019 is the first person plural, and so in this case, indeed, the active voice wording also uses a first person perspective. But take a typical sentence from a results section, such as \u201cCompanies used a variety of strategies to market their green credentials\u201d. This is an active voice sentence (in third person plural), and there is absolutely no need to somehow try to turn it into a first-person sentence. For example, adding \u201cWe found that\u2026\u201d at the beginning of sentences is not necessary (though you will find that some authors now do just that) \u2013 in fact, it makes the sentence longer and unnecessarily complicated.\n\nFinally, a warning for those operating in a German academic context\u2026: modern academic writing in English is very different from classic academic writing in German. Avoid nested sentences, long sentences, hidden meanings or clever metaphors.\n\n===Phrases to use and to avoid===\nThrough time, you will develop your own writing style, and that\u2019s a good thing. However, there are certain phrases that help in writing, and others that should be avoided. Here is a short list of them; presumably very incomplete, but nevertheless a starting point.\n\n{| class=\"wikitable\" style=\"text-align: center; background-color: white\"\n|-\n! Avoid !! Use\n|-\n|Like xyz || such as xyz <br/> (e.g. xyz) \n|-\n|Etc. || spell out and end the list with \u2018and\u2019 or \u2018or\u2019\n|-\n|A number of || Several or many; ideally list the actual number if possible\n|-\n|But (at beginning of sentence)|| However, \u2026\n|-\n|\u2026 while \u2026 || \u2026 whereas \u2026 (use \u2018while\u2019 only in a temporal context)\n|- \n|\u2026 as \u2026 || \u2026 because \u2026 (use \u2018as\u2019 only in a temporal context)\n|-\n|In contrast, \u2026 || By contrast, \u2026 (though \u2018in contrast\u2019 is not technically wrong)\n|-\n| || For example, \u2026For instance, \u2026\n|- \n| || Furthermore, \u2026 \n|- \n| ||In addition, \u2026 \n|- \n| ||Notably, \u2026\n|-\n| || Surprisingly, \u2026\n|-\n| About, roughly || Approximately\n|}\n\n===Who to cite, when and where?===\nOne important aspect of academic work (as opposed to popular writing) is that you need to acknowledge where various ideas came from. As a tutor of mine once said: whatever you don\u2019t wake up just knowing one day, needs to be either referenced or arise directly from your results. But with so many papers out there now \u2013 what does this mean? What about citing multiple papers for the one statement? When should you cite how many papers, and which?\n\nOne approach to this is to cite a lot of other papers. The idea is (I guess) that you somehow \u2018prove\u2019 that you really know the literature. In this school of thought, even relatively obvious statements often get three or even more references to other work. This approach is very common, but I don\u2019t think it\u2019s very elegant.\n\nAn alternative approach is to focus on the most important citations. A good choice is to focus on the \u2018classic\u2019 papers on a given topic, as well as the most recent, high-impact papers on that topic. A good way of finding the \u2018classics\u2019 on a given topic is to search in online databases, and sort the findings by the number of times a paper has been cited. The \u2018classics\u2019 tend to be highly cited.\n\nIn addition to the classics, the latest \u2018high impact\u2019 papers make for good sources to cite. Those are basically the ones that will be relatively highly cited in the future, but are not highly cited yet \u2013 it typically takes about 1-2 years for a paper to get cited after it is published. To find the latest important papers in your discipline, you need to be aware of which are the most important journals in your field, and it\u2019s a good idea to (regularly) read the table of contents of these journals, so that you\u2019re up to date.\n\nKey parts of a paper where most of the citations end up are the introduction (where you demonstrate how your work fits in with others, and that you know who else publishes in your domain), and the discussion (where you relate your findings to other people\u2019s findings). Note that with very few exceptions, there are no citations in the abstract.\n\n==Sections relevant to all papers==\nAlthough there are many different types of papers, some types of sections occur in most papers. \n===Abstract===\nThe abstract is the first thing your reader will see. It\u2019s also the first impression that editors will get of your paper. A good abstract will leave people satisfied that they know what you did, why you did it, and what you found out. A good abstract most likely means people will want to read the rest of the paper, and it greatly increases the chances of people remembering your paper later on. This, in turn, means a good abstract is important for people to cite your paper. If your main message is clear from the abstract, others are much more likely to recall what your paper was all about when they pick it up a year after first reading it \u2013 for example when they are in the process of writing a paper of their own and are looking for appropriate citations.\n\nAbstracts differ greatly between journals, in both style and length. It is critically important that you follow the instructions for authors for your particular journal. Here is an overview of some of the differences that you might encounter.\n\nDescriptive abstracts give some background information, and summarise some of the argument. Their analogue in the movie world is a trailer. They tell you enough about what the paper is all about so that you want to keep reading it \u2013 but they don\u2019t give it all away. They often don\u2019t give you the final take-home messages, but rather, they end in statements such as: \u201cThe implications of these findings for policy development are discussed.\u201d Some disciplines use these kinds of abstracts a lot, but personally, I find them frustrating. Take my previous example just above. If there are important policy implications, wouldn\u2019t it be much nicer to know what they actually are, rather than just knowing that there are implications? My sense is that descriptive abstracts should be avoided, unless you are dealing with a journal and discipline where it is expected of you to write such an abstract. Otherwise, readers get a lot more out of \u2018real summaries\u2019.\n\nReal summaries is my slightly clumsy term for the more common type of abstract, which tells you about what you did, why you did it, what you found, and what the implications are. It\u2019s really the latter that sets them apart from descriptive abstracts. Real summaries fully reflect the scope of the paper from motivation to take-home-messages. So, for example, a last sentence of a real summary might be: \u201cOur findings suggest that a more participatory approach is needed to improve citizen acceptance of the suggested reforms to water policy.\u201d This doesn\u2019t just tell you that there are policy implications, but it tells you something (of course only very briefly) about what those implications actually are: in this case, the need for more citizen participation.\n\nStructured abstracts are a type of abstract that some journals use. They tend to use a series of sub-headings or numbers; and authors are supposed to follow this structure when they write their summaries. The Journal of Applied Ecology is a good example of such a format. Typically, the different parts of the abstract refer to different sections of the paper. For example, there might be one part of the abstract that summarises the background and motivates the paper, one point that summarises the aims and study location, one for the methods, and so on. In the case of the Journal of Applied Ecology, the last sentence needs to be a clear summary of the take-home messages.\n\nAlthough structured abstracts are relatively uncommon, almost all abstracts work well if they are first written as if they were structured abstracts. Typically, abstracts will mirror the structure of the overall paper. It is particularly important to have a clear first sentence that motivates the need for the paper and gives background information; and it is particularly important to have clear take-home messages. Perhaps the least important part of an abstract (in terms of length) is the methods. A summary of methods should be included, but often this can be quite short. Given that not everything can be included in an abstract, the methods can be dealt with relatively briefly, whereas it is critical that the motivation and take-home-messages are long enough that they are clear.\n\nAbstracts vary widely in length. Some journals allow as little as 120 words, whereas others go up to 400 or so. Longer is not necessarily better, or easier to write. In all cases, it\u2019s important to try to appropriate fit the content within the prescribed length.\n\nFinally, it works best if you write your abstract last \u2013 to summarise your document, you first need to have your document finished.\n\n''Possible exercises:''\n\n* Collect several abstracts from existing papers, both within your own expertise and outside your own expertise.\n* Analyse these abstracts: Do they follow a clear structure? Which abstracts tell you a lot, and which tell you very little? Which do you like, and why? Which are easy to follow, and why? Is the \u2018so-what\u2019 clear?\n* Write a short list of attributes that you found frustrating in other people\u2019s abstracts, and a list of things you found really useful.\n* When you write your own abstract, use this list as a reference point.\n\n===Introduction===\nThe introduction of a paper is critically important. Even if your results are quite good, unless you introduce your work well, interesting results can come across as boring or meaningless. This issue is related to that of framing, which I discuss separately elsewhere.\n\nIntroductions need to start with a broad motivating statement, which takes up one or two sentences. The scheme for these sentences typically is something like this: \u201cXYZ is a really important issue because of A, B, and C.\u201d\n\nFollowing such a broad statement, there is usually some review of existing knowledge. This serves two purposes. One, you need to embed your work in the context of other people\u2019s work. Two (a more political reason), you need to demonstrate that you know about other important players in the area in which you are publishing. Reviewers get very annoyed if you have missed their very important work, even though it\u2019s on a related topic. You can\u2019t (and shouldn\u2019t!) cite everything, but it\u2019s a good idea to be familiar with who else is working in your area, and give them credit for their work.\n\nIntroductions then become increasingly specific, like a funnel. At the very end, you present specific aims. The logic of an introduction thus typically flows something like this:\n\n* XY is an important issue\n* For example, it has these effects, and these other effects\n* These have been investigated in a number of ways\n* Author A came up with this explanation\n* Author B proposed an alternative explanation\n* To date, it is unknown what the role of the phenomenon Z is.\n* Phenomenon Z could be important because of this, that, and something else.\n* Here, we investigate the role of phenomenon Z in the context of \u2026\n* Specifically, we addressed three aims:\n** First, we tested whether \u2026\n** Second, we compared our findings \u2026\n** Third, we applied our insights to \u2026\n* Optional last sentence to summarise the key finding: \u201cWe show that \u2026\u201d\n\nThe last optional sentence wraps up the introduction. It basically states the main finding. While that might seem unusual, it gives a strong ending to the introduction, and makes it very clear for the reader in which direction you are heading. Some leading journals now encourage a summary sentence at the end of the introduction.\n\nHow long should an introduction be? This depends on the discipline and journal you are writing for, and on your overall strategy to fit your content within the prescribed length. You might want to copy and paste the introductions from some papers that you like (which are broadly similar), and do a word count on them to get a rough idea for what is appropriate.\n\n''Possible exercises:'' \n\n* Collect the introductions from several published papers, both from your own area of expertise and from outside your own expertise.\n* Analyse these introductions: Do they follow a clear structure? Which introductions lead you clearly to key aims? Which take big detours? Do you think those detours are useful (because they add depth), or are they a distraction? Why? After the introduction, are you inspired to read the rest of the paper? Are the aims clear \u2014 do you know what\u2019s coming?\n* Write a short list of attributes that you found frustrating in other people\u2019s introductions, and a list of things you found really useful.\n* When you write your own introduction, use this list as a reference point.\n\n===Conclusion=== \nMany papers don\u2019t have conclusions. My personal view is that this is not a good trend \u2013 I think it\u2019s a good idea to have a short conclusion that summarises your key finding, and key take-home message. This shouldn\u2019t be too long or repetitive, but is worth having so that your argument is not left unfinished. Importantly, don\u2019t start any new thoughts in your conclusion.\n\n''Possible exercises:''\n\n* Find recent papers that are relevant to your own work. Do they even have conclusions?\n* If there was just one thing that you want your reader to remember from your work, what is it? What if there were two, or three things? List these things: they probably should be in your conclusion.\n\n===References===\nThe referencing style differs between different journals. No matter which journal you submit to, you save yourself a lot of work if you use specialized software for managing your references and for formatting them (such as Endnote or RefWorks). This software can do three things for you: (1) make sure you don\u2019t cite things that then are missing in the reference list, and (2) format all your references consistent with a particular journal style, and (3) re-format your references for a different journal style should you want (or have) to submit your work elsewhere. Like all software this takes a little time to learn, but if you intend to write several papers, the learning curve is absolutely worth it.\n\n===Acknowledgements===\nBefore your references section, you should list your acknowledgements. This includes funding sources, colleagues who helped you (but are not authors), and it also includes referees if they gave you useful suggestions for how to improve your paper. For example: \u201cComments by two anonymous reviewers greatly helped to improve an earlier version of this manuscript.\u201d\n\n==Additional sections to empirical papers==\nSome sections are particularly relevant to empirical papers.\n\n===Methods===\nFor non-essay papers, the methods section describes everything the reader needs to know to replicate what you did. This means that all procedures need to be described in sufficient detail that somebody else could implement them again. Methods are usually written in past tense. The only references in the methods section tend to be to previous, related work (which the reader needs to know about) or technical papers outlining particular protocols. In the latter case, it is appropriate to summarise the technical procedure briefly, as well as point to a specific reference for further details. The only procedures that need no references are those that are accepted as \u2018standard techniques\u2019. It will depend on your discipline what is considered a standard method. An example might be simple linear regression. If you say that you used simple linear regression in an empirical science context, nobody will expect you to provide a reference. If you say, for example, that you used canonical correspondence analysis, however, people probably will want you to give a reference, so they can look up the details. So what is considered a standard technique and what is not is context-dependent.\n\n''Possible exercises:''\n\n* Do you have  lot of methods? If so, you probably need to think of a series of sub-headings.\n* What can you assume is general background knowledge in your discipline, and what requires explanation? Hence, which of your methods can you describe briefly (perhaps with a reference), and which need more space?\n\n\n===Results===\nThe results section of a (non-essay) paper is where the findings are presented. This is typically done in past tense, and without any references to other work. You simply summarise key points from your figures and tables, and explain additional findings in the text. You provide no interpretation of findings. The results section is purely descriptive.\n\n''Possible exercises:''\n\n* List your results, prioritise them, and decide which to include in your paper\n* How can you best communicate your particular results: in text, tables, or figures? (Figures work best for many readers.)\n===Discussion===\nStandard (non-essay) papers have discussion section after the results. This section is about interpreting the findings, placing them in a bigger context, relating them to other work, and presenting take-home messages.\n\nUnlike for methods and results, you have a lot of freedom how you write your Discussion section \u2013 probably even more so than for the Introduction. Broadly speaking, there are two types of discussion.\n\nThe \u2018boring\u2019 type of discussion goes through your main results, one after the other, interprets the result, and relates it to other literature. The method goes something like this:\n\n* (Short re-statement of main result, e.g.:) \u201cMany large companies integrated sustainability considerations in their daily operations.\u201d\n* (Interpretation, e.g.:) \u201cThis may be because of X, or because of Y.\u201d\n* (Relate to other people\u2019s work, e.g.:) \u201cThese findings underline the importance of ZZZ (reference), as also shown in Japan (reference) and the USA (reference).\u201d\n* (then the next main result, and so on)\n\nMost discussions have at least some sections that follow this boring format. There\u2019s nothing wrong with this, other than that it is, well, boring\u2026\n\nTowards the end of your discussion, and if you\u2019re bold all the way through, you need a more creative way of putting things together. Even if you\u2019re using the \u2018boring\u2019 format, you still need to get to the bigger picture, the big so-what messages. That is very hard to do unless at some point, you go for a more creative format.\n\nIt is harder, but more interesting, to start with a more creative format from the outset. A freely structured discussion needs to make a clear argument, and it can be very useful to use sub-headings to structure this argument. Even though the structure might be free and creative, you still need to draw on your own results under each sub-heading, not just on other literature \u2013 a discussion is NOT a literature review, but your work must be at the centre of the argument.\n\nA discussion often picks up similar themes as discussed in the Introduction, and thus is a bit of a reverse funnel. Whereas an Introduction starts broad and becomes focused, a Discussion focuses on your results initially, but then ultimately must leave a clear impression with the reader how the results contributed to the resolving the broad issues that you outlined at the beginning of the Introduction.\n\n==Visual Items and extra information==\nHere, I briefly outline some of the main considerations for the use of figures, tables, boxes ans suplementary online material\n\n===Figures===\nMost people find it easier to read figures than tables. Figures therefore are an effective means of summarizing key findings. A convention is to describe the overall pattern in your findings in the text, and then refer to a figure for details.\n\nFor example, you might write: \u201cSpecies composition changes systematically along a gradient of land use intensity (Fig. 2)\u201d \u2013 and Figure 2 would then show the details of this gradient.\n\nOr you might write: \u201cLarge companies often had explicit policies in place to enhance sustainability (Fig. 3a). By contrast, many small companies had policies for various aspects of sustainability (e.g. governing social issues) but these were rarely combined in an explicit framework for sustainability (Fig. 3b).\u201d\n\nIn both of these examples, the text tells you the basics, and you can look at the figures to find out more.\n\nHere\u2019s a couple of bad examples:\n\u201cFigure 2 shows changes in species composition\u201d \u2013 Figure 2 should not be the central part of the sentence, but something referred to at the end.\n\u201cLarge companies had different ways of dealing with sustainability than small companies (Figure 3)\u201d \u2013 this doesn\u2019t tell you enough about what those different ways actually were.\nIn both cases, there is too little emphasis on writing good text to go with a good figure.\n\nThere are a few things to consider when preparing the figures themselves:\n\n* Use simple fonts (sans serif) and avoid \u2018graph clutter\u2019. So, for example, Times New Roman is not a good font for graphs, whereas Arial is a good font. Similarly, three-dimensional bar charts like Microsoft Excel can produce are full of graph clutter. Keep things minimal \u2013 whatever doesn\u2019t contain information shouldn\u2019t be in the graph.\n* Keep your things black and white, unless you are writing for a journal that uses colour figures (and note those often cost extra!)\n* Keep in mind the size of your figures. Make your figures as small as possible, and make sure that the fonts are readable in the size the figure will ultimately be printed in. Also, think about how many columns the table should take up. Journals tend to arrange their text in one, two or three columns. If you write for a two-column journal, it makes sense to come up with a figure that either takes up one column in width, or two columns in width.\n\nFinally, figures need to have informative legends. Good legends are often quite long. Try to write legends, which (together with the figure) can pretty much stand alone. Legends that are very short often contain too little information. Especially for somebody who reads your paper quickly, it is very useful if your legend contains quite a lot of information. Note that legends for figures are placed below the figure. In a manuscript, they typically go on a separate page from the figures themselves.\n\n''Possible exercises:''\n\n* Initially, don\u2019t worry about software. Draw by hand what you want your figure to look like.\n* Then ask yourself if you can implement this figure yourself, or if you would need additional technical skills, or if there is a person you can ask to help with the figure.\n* If your ideal figure can\u2019t be done easily, ask yourself if there is a slightly simpler version that you can do, which is almost as good. Don\u2019t waste too much time on \u2018the perfect figure\u2019!\n\n===Tables===\n\nTables are an effective means to summarise summary information, both numerical and otherwise. When you prepare a table, also consider the alternative \u2013 figures. Figures tend to take up more space for the same information, but some patterns can be summarized much more effectively in pictures than in a table. But especially when the data you are trying to visualize is quite simple, tables are sometimes almost quite as good as figures, but take up a lot less space.\n\nOnce you have decided a table is what you want, think about how to present your information within the table.\n\nMost journals will allow multiple nested column names, but otherwise, note that the formatting in published tables is usually pretty minimal. So, while in your word processor you can use different types of highlighting or colours, this is not the case in most journals. The best is to format your table using only horizontal lines (like in published tables) \u2013 and then make sure the information is clearly understandable.\n\nKeep in mind the size of your tables. While you can fit a lot into a table in your word processor, journals typically have less space \u2013 so make your tables as small as possible. Also, you might want to think about how many columns the table should take up. Journals tend to arrange their text in one, two or three columns. If you write for a two-column journal, it makes sense to come up with a table format that either takes up one column in width, or two columns in width.\n\nFinally, tables need to have informative legends. Good legends are often quite long. Try to write legends, which (together with the table) can pretty much stand alone. Legends that are very short often contain too little information. Especially for somebody who reads your paper quickly, it is very useful if your legend contains quite a lot of information. Note that table legends go above the tables (unlike figure legends).\n\n===Boxes===\nNot many journals allow boxes. In case you write for a journal that does have boxes, they are really useful for a few things.\n\nExamples. A classic use of boxes is to provide short case studies or examples of a concept, theory or situation that you outline in the text.\n\nAdditional depth. If there is something you find exciting, but it\u2019s not really central to your argument, a box can be good for that.\n\nCross-linkages throughout your text. Sometimes boxes can help to link different sections of a paper in the one point. For example, if you have several sections that build different parts of an overall argument or theory, a box can be an effective way to summarise how those parts come together.\n\nIf your journal does allow boxes, check carefully how they are formatted. Some contains figures and tables as well, others include just text. Also check the typical word count in a box in your target journal; this varies widely, depending on the page layout of the particular journal.\n\n===Suplementary online material===\nSupplementary online material is now being used by many journals for things that will only be of central interest to a small number of readers, but nevertheless should not be left out. It is essentially what used to be appendices. Supplementary material often can be several additional pages of text, and sometimes up to 10 additional figures. Different journals have different restrictions on this.\n\nTypical items that can go into the supplementary material are things such as (1) in-depth methods, (2) additional results, (3) further justification for a particular approach, or (4) photographs related to the study. In all cases, the main text needs to refer to the supplementary material (e.g. \u201csee supplementary online material for further details\u201d, or by referring to \u201cFig. S1\u201d). It is also a good idea to summarise briefly the supplementary material in the text. For example \u201cA null model was used to test whether communities were randomly assembled (see supplementary online information for details)\u201d, or \u201cAdditional scenarios were explored and showed broadly similar patterns (Figures S1-S5)\u201d, or \u201cOur study assumes that communities are influenced by institutions at multiple scales, including local, state and federal governments (Fig. S1)\u201d.\n\nSupplementary online material should not be seen as a deposit for everything that didn\u2019t otherwise fit. Rules of prioritizing your material and fitting the content to the prescribed length still apply. Rather, supplementary material is for things that are necessary or informative, but not of central interest to the majority of readers.\n\n''Possible exercises:''\n\n* Collect supplementary material from a few different journals that are relevant to you. Get a feeling for what people put into their supplementary material. Which kind of format is useful for you, the reader, to understand how the material is organised?\n\n==The process of publishing==\nIt\u2019s helpful to understand the process of publishing. Writing a good paper is probably the most important, but it\u2019s also helpful to understand some of the peripheral issues surrounding the process of publishing.\n\n===Formatting your manuscript===\nMost journals expect that your manuscript is formatted in the following way:\n\nFirst page is a cover page, containing title, keywords, author names and institutional affiliations, plus the email address of the corresponding author.\n\nSecond page is where the abstract should start.\n\nThird page onwards comes the manuscript text, WITHOUT any tables, figures, or table or figure legends.\n\nAt the end of the text, list your Acknowledgements, followed by References.\n\nOn a new page after the references, list your figure legends.\n\nOn separate new pages after that, present your tables. The table legends go directly above the tables.\n\nOn separate new pages after that, list your figures, one figure per page. Some journals also want you to upload the figure files separately (in which case you need to follow their instructions file type and resolution).\n\nIn addition, your entire manuscript should be double-space. It is also helpful if you add line numbers, for example every fifth line, consecutively throughout the entire manuscript. This makes it much easier for referees to comment on your work.\n\n===Should I care about impact factors?===\nIf you find your work worthwhile, you will have an inherent interest that many people read it. This means you need to place your paper in the right journal (e.g. matching the topic with what else is in the journal), and preferably in a widely read journal. The impact factor of a journal is a measure of how widely cited recent articles from the particular journal are. In this sense, it does matter. In addition, a high impact factor publication will look better on your CV than a low impact one.\n\nAlthough the impact factor matters, the most important thing is that you publish in a journal that is highly visible to your target audience. A certain journal might have a slightly lower impact factor than another one, but if it is still among the \u2018standard leading journals\u2019 in your field, it\u2019s not worthwhile worrying too much about the impact factor.\n\nIdeally, think of the impact factor as a means \u2013 to measure if a journal is widely read. Publishing in a good journal, by contrast, is the ultimate objective. The impact factor will help you make an informed decision where to submit your paper, but it should not be the only criterion.\n\n===Cover letter===\nCover letters have become quite important in some academic disciplines. Ask around in your discipline how common it is for papers to be rejected without peer review. If that is a common phenomenon, this suggests that editors themselves have a lot of power \u2013 they might decide that a lot of papers don\u2019t even go for peer review. Rejections without peer review tend to be quick, but are frustrating because you get virtually no feedback why your paper was rejected.\n\nCover letters are intended to convince the editors that they should send the paper out for peer review. If you are dealing with a journal that sends almost everything out for peer review, a minimalistic cover letter is fine. But if you are dealing with journals that reject a lot of work without peer review, you need to explain why your particular paper is worth considering.\n\nIn a (more than minimalistic) cover letter, you might want to explain:\n\n* That you\u2019re submitting the work to the journal for consideration for publication;\n* Why the work is new, exciting and important;\n* Why you think the particular journal is suitable;\n* Any additional information that is important to the particular journal.\n\nMany journals have additional specific requirements for their cover letters. For example, they may require a statement that all co-authors have read the manuscript and agree to its submission; or they may ask you to suggest potential referees. Check the journal\u2019s instructions for authors to make sure you follow the instructions.\n\nIn some cases, it\u2019s a good idea to contact the editor by email before you submit your paper to ask if the paper might be suitable for a journal. This is not something that all journals are happy with, but some actually encourage such pre-submission enquiries. If you do write a pre-submission enquiry, it\u2019s a good idea to keep it brief; send the abstract, and maybe a key figure; and explain why you think it might be suitable for a particular journal. Make sure you specifically ask whether the editor thinks the work may be suitable for the particular journal. Sometimes editors get back to you with a helpful hint: for example, they might say that your work sounds interesting, but needs to have a clearer international relevance for it to be considered. If that\u2019s the case, you already know that there\u2019s something you should work on some more before even submitting to that particular journal.\n\nPersonally, I think a cover letter should be about one page, and if possible, it looks better if you write it on the official letterhead of your institution.\n\n===Open access===\nFor many journals, you need a subscription to read the articles. This costs money, and it is one of the key reasons why people in poor countries do not have very good access to primary scientific literature.\n\nIn an effort to make scientific literature available more widely, an increasing number of journals offers an \u2018open access\u2019 option, and some journals are entirely open access. Both typically require the author to pay money (out of research grants, typically) when a paper is accepted for publication. However, both types of model typically also have an option that stipulates that if you truly cannot afford the so-called page charges, they will still take your paper.\n\nSome people have speculated that making your paper open access might increase your citation rates. Whether that\u2019s true or not, it is certainly easier to access a paper if it is open access. I suggest whether you choose an open access journal or not should depend on who you think your main audience is (and hence what the benefits of open access are), and whether you can find a journal that has a good reputation and is open access in your field. My personal view is this: If you have two equally prestigious journals, one allows you to make your paper open access while the other does not, and you have funds to pay for this \u2013 then you may as well make your paper open access because you have nothing to lose, and you might gain extra readers. If your work is directly relevant to poor countries or to institutions who typically do not have access to journals (e.g. in the business sector), you might place an even higher priority on publishing your work open access.\n\n===Revising a paper after review===\n\nWhen you first write your paper, of course you hope for it to be accepted. But reality is that many papers are rejected, and those of those that are not, many require revisions, often major. As an example, leading international journals in applied ecology now reject 80% of papers \u2013 if you look at leading journals like Nature, the percentage of rejections is much higher still. Given this situation, being asked to submit a revised version, even if the revisions are major, is often a \u2018success\u2019. How can you make sure that when you are asked to submit a revised version, you maximize the chances of your revised paper being accepted?\n\nThe first general rule is that you take the criticisms of all reviewers seriously. As the author, it\u2019s your job to communicate your ideas clearly. Even in cases where the criticism you received seems unjustified, ask yourself \u201cWhat can I do better so that this same reviewer will be more likely to buy into my argument next time around?\u201d Even if you are completely satisfied that your argument holds, ask yourself if you can do a better job of communicating it clearly.\n\nYou then need to go through the comments by the editor and all reviewers one by one, and address every single one of them. Broadly speaking, you have three options for how to respond to a particular criticism:\n\n# When, upon reflection, you agree with the reviewer: You implement a change that does what the reviewer asked for. For example, she may have asked for additional explanation, an additional reference, or the complete re-write of a section, including a different conclusion.\n# When, despite reflection, you disagree with the reviewer: You might not want to implement some particular suggestions, especially if they relate to subjective matters, which you simply feel differently about. (For minor suggestions, I suggest just do as the reviewers suggest, because there\u2019s nothing to lose; but if you truly disagree, just implementing the suggestions is not a satisfactory solution.) Still, in this case, you can often communicate more clearly what your position on the issue is. So re-write your text in a way that might convince the reviewer of your position, lends more weight to your argument, or is simply easier to follow. In this case, you\u2019re still making the same argument as before, but you explain it more clearly.\n# When, despite reflection, you think a reviewer is simply wrong: In this case, you change nothing in the text, but you explain in your cover letter to editor why you did not make the change.\n\nAs a general rule of thumb, if you want your paper to be accepted, and especially if it goes back to the same reviewers, your response to most criticisms needs to be type 1, followed by type 2 \u2013 only rarely will you get away with response type 3 above.\n\nYour revision ultimately consists of the changed manuscript, as well as a detailed letter outlining point by point your responses to the reviewers. Put yourself in the position of the editor: she wants to understand as easily as possible what you changed or did not change, and why. Making this job as easy as possible for the editor is what your response letter needs to do.\n\nThe following format works well:\n\n* An introductory cover note, which follows a logic like this:\n** Thank you for your decision letter from xyz.\n** We greatly appreciate the constructive comments.\n** We have now addressed these comments, and this has strengthened the paper.\n** On the following pages, we outline point by point responses to the comments by the reviewers (and the associate editor, if applicable).\n** We hope our revised version will be received favourably and look forward to hearing from you in the near future.\n* On the following pages, detailed point by point responses, to each of the comments, in a format as follows:\n** Paste in a given reviewer\u2019s comment in italics\n** Then explain how you have addressed it, e.g.:\n*** ''\u201cThe authors assert that biodiversity is declining in their study area, but they do not substantiate this claim. At the very least, this statement requires a reference.\u201d'' <br />Response: We appreciate this concern and have now added a reference (Smith et al. XXX; see page 4 of the revised manuscript)\n*** \u201cThe methods used on page 6 are unclear. Which type of regression model was used?\u201d <br />Response: In the first paragraph of page 6 of the original manuscript, we had actually specified that we used generalized linear models. Arguably, our statement was hidden among other information, which is why the reviewer may have missed it. We have now moved our statement to a more prominent position at the beginning of the section on \u2018Data analysis\u2019 (page 5 of the revised manuscript). This should make it more obvious which methods we used.\n\nDon\u2019t be surprised that response letters sometimes end up very long! Still, it is better to explain too carefully what you have changed and why, than not carefully enough. The objective of a response letter is to show the editor that you are taking the review process seriously, and that you have put in a very genuine effort to address the comments, including the difficult ones. It\u2019s a good idea to approach this process from a position of intellectual humility, while also making sure that you stay true to your core argument.\n\n==Topics not covered==\nThere are additional topics that could be listed here, but which for now, I have not included. Here is a short summary of some of those issues.\n\nStatement of author contributions: a short summary saying who did what, such as in Proceedings of the National Academy of Sciences USA (PNAS).\n\nWriting for Nature or Science, or other journals with unusual formats: Nature and Science often mix results and discussions, and journals such as PNAS put the methods in small print at the back of the paper. To familiarize yourself with how to write for these journals, it\u2019s best to check a number of recent articles, as well as read the author instructions in detail.\n\nReporting statistics: Different disciplines have slightly different conventions on how to report statistics, such as significance levels or which methods were used. It\u2019s best to check other recent papers from the particular journal you are submitting your manuscript to for examples.\n\n==Links & further reading==\nHere are some other sources that you might find helpful.\n\n[https://dbem.org/WritingArticle.pdf  For social scientists]\n\n\n---- \n[[Category:Skills_and_Tools]]\n\n[[Category:Personal Skills]]\n\n[[Category: Team Size 1]]\n\n[[Category:Team Size 2-10]]\nThe [[Table_of_Contributors | author]] of this entry is Joern Fisher. This article was originally published on Joern Fischer's [https://writingajournalarticle.wordpress.com/ blog]."
                    },
                    "sha1": "ezc9i1ump1j4vpxfua3284v9yq2616x"
                }
            },
            {
                "title": "Writing an outline",
                "ns": "0",
                "id": "1038",
                "revision": {
                    "id": "7070",
                    "parentid": "7063",
                    "timestamp": "2023-04-18T14:16:36Z",
                    "contributor": {
                        "username": "Lya",
                        "id": "131"
                    },
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "21350",
                        "#text": "With the completion of an outline you take one first step of what your research could be all about. One should never forget that research is an evolving and iterative process, though. Still, writing an outline makes you settle for what you want to focus on in this moment, and more importantly, also allows your supervisors as well as your peers to give you structured feedback. This is why any research project should start with the landmark of writing an outline. Different branches in science have different focal points and norms that an outline is build upon. Here, we present an approach that tries to do justice to the diversity of approaches that are out there, yet it is always advisable to ask your supervisors for modification if need be. \n\n===== Working Title=====\nAll research starts with a  title. Personally, I read hundreds of titles of research papers each month, and only a small portion are appealing to my specific focus and interest to invite me to read further. Titles are the door-opener for most researchers, which is why titles should be simple and to the point. If a title is too long it will lack clarity and crispiness. If a title is too short, it will surely not give away enough information needed to know what the research is all about. Often people try to make a fancy title that is supposed to be witty or funny and contains some sort of wordplay or inside joke. Avoid this. You may go for such a title later once the research is done and the paper is written, yet such titles need to be earned. Hence especially in an outline it is best for a title that is walking the fine line of giving away enough information but not too much. \n\n===== Participants =====\nIn the end, it is going to be you who will do the research. You will sit at your desk, you will gather the data and look at the literature, you will get deeper into the topic, thus it is you who will basically write the research. To this end, it has proven of immense value to have a network of peers. More often than not, this is an informal network for critical reflection, but also for support. Such a peer network will not be mentioned here if it is not actively involved in conducting the research. Beside your supervisors actually very few people will be involved in your research. You may have someone helping with the analysis or being experienced in the topic if you are a PhD student, yet in a Bachelor or Master thesis the focus is stronger on proving that you can conduct independent research. While in a PhD this is also the goal, it is on a more sophisticated level, where because of the longer timeline and thus deeper focus collaboration may be of greater importance. It is also quite important to clarify roles and expectations at the beginning. Remember that a thesis is your work. \n\n===== Background and topic =====\nThe topical focus and its background are often what drives people. Most researchers are very exited about their topics, and it is valuable to have something that can fuel your energy while you work on your thesis. There will be ups and downs surely, yet it is still good to focus on something that not only drives you, but also current research. Timely topics are ideally embedded into a longer development that led to the emergence of the topic. You do not want to work on a topic where thousands and thousands of paper were already published, but ideally you should also not work on something that is so new that there are no shoulders to stand on. Researchers do really stand on the shoulders of giants, and despite the thrive to make innovative and timely research, we should never forget that we are part of a bigger movement. Within this movement, we will add one tiny step. A friend once said that we are drops in a wave - a fitting allegory when looking how research evolves. Still, our contribution matters, and may move the research landscape forward. What it does however more importantly is that it moves us forward. A thesis is first and foremost a prove that you can conduct research, and that you are thus able to contribute to the scientific community. Because of this, the topic is less important than most people think it is, because research is a transformational experience for the research. Do not misunderstand me, all topics of past people I supervised were really important to me. What is however even more important is that they learned to overcome themselves, and slay the dragon that is their thesis. \nOnce you found a topic that excites you, it is advisable to iterate it with your peers. Write down why your topic is timely, how it contributes to the wider research, and what  the current state of the art is. You want to make a thorough assessment yet have to be careful because reading too much may confuse you. Do not expect that everything is coherent and there are no contradictions. Research is discourse, and these discourses evolve over time. Often researchers disagree, and not everything makes sense. Be prepared to be confused. It is your job to evolve a critical perspective of the state of the art of the literature, and identify landmark papers. Also, try to find previous research that aimed in the same direction and learn how they approached the topic. What were the methodologies, where did the researchers struggle, and what were their recommendations. Lastly, try to develop an elevator pitch on why this topic is important to you. If you can explain in short why you think this research needs to be done, you are onto something.\n\n===== Key supporting theories =====\nJust as topics are embedded in the current scientific discourse, such research looks at partials of reality. In order to generate such a specific view of reality, research builds on theories. Theories in science can be operationalized at different scales. Some theories are on a conceptual level, which is basically very theoretical and often restricted to non-empirical research such as much of philosophy. Other researchers may build on frameworks, and is very applied. In between are paradigms, which are the types of theories that branches of research are for some time built upon. Examples for concepts would justice or peace, examples of paradigms would be effective altruism or ecosystem services, and examples for frameworks would be concrete assessment schemes or the sustainable development goals. Much confusion in theory work is because these three levels are confused, which is why it is important to localise your theory in your work clearly. \nAnother common problem when writing an outline is to settle for a low number of theories. If you ask me, one theory can be enough, and two can be exciting. More than two theories are more often than not impossible to tame. Hence you need to remember that you look at a version of reality, and that theories enable you to either test hypothesis or create research questions that are open and specific at the same time. You may not want to know how everything works, but how it works under a specific viewpoint or theory of reality. This is what theories are all about. Researchers theorize how mechanisms in the world might work, how patterns emerge, why people act, and why societies fail. There is a plethora of theories, and it is important to be familiar with the literature of the theories that are within your realm. \n\nMost topics are associated with certain theories, such as mainstream economics often resolves around utilitarianism. While you can operate on safe ground if you follow suit with these associations, it can also be exciting yet daring to superimpose a theory onto a topic that has never been exposed to it. Yet this should ideally be discussed with your supervisor, because while it can be exciting, it may be prone to challenges that are hard to anticipate by the uninitiated. This is one of the key points where you should build on the experience of others. Lastly, while it may be exciting to work on a scientific theory, always remember that you look at a snapshot. Theories are often proposed, and then in subsequent years applied. Based on this, they are revised, scrutinized, and often the research community becomes divided about specific theories. While many researchers question or even reject evolutionary altruism for example, others may work based on this theory for their whole career. Choosing a theory is a normative responsibility, you decide to take on a specific perspective, and this is a key responsibility as a researcher. While you should choose wisely, I would also say: Do not overthink it. Most early researchers tend to think way too much about it, in fact I never saw anyone thinking too little about it. And after all, at this point your supervisors should give you feedback. \n\n===== Main references =====\nIdeally, you can at this point already identify the main references that your research is being based upon. Make sure to reference the sources that are most recognized by the research community, but also closest to what you think are the best sources. Often this may be one and the same, but sometimes it differs. What is crucial is to focus on the main sources. Especially early career researchers are so happy that they read so much stuff that they want to quote it all. While this sounds like a great idea, remember that not everybody is as deeply embedded as your are, thus focus on the most important sources. On a less dramatic note, make sure that your quotations are in the same style. Settle early on one citation style, use a software to guarantee coherence, and never change it from then on.\n\n===== Research question or hypothesis =====\nThe next point is a very delicate one: Research questions vs. hypotheses. Let us start with the more simple point. How much of those should you have? It is really difficult to answer one question. This is something that philosophers may do, but most research demands more questions that build on each other. See it from a structural point of view, answering one question pre-structures your introduction into one big blob of text. That is certainly not convenient. 2-5 questions seem ideal, because -let's face it- it is really hard to remember more than 5 things intuitively. We then tend to forget, and it also has the drawback to make your research seem \u00fcber-structured. Hence it can be a good heuristic to have 2-5 research questions or hypotheses. Now let us move to the more troubling part, which is the question whether it is research questions or hypotheses. From a philosophy of science standpoint, the two should be mutually exclusive. While hypotheses are surely deductive and demand a clear confirmation or rejection, research questions are somewhat more open and hence inductive. Thus while the latter has never been clear cut, it can be helpful to draw such a clear line. Some folks may now raise the streetwise question whether we should not all be thinking abductively anyway? In a sense this is where all science moves anyway, because we need both inductive and deductive approaches, and ideally combines the benefits of both in the long run. Yet this is mainly a questions of both experience and temporal grain. Working abductively means that you can clearly remark between hypotheses and research questions, because the first demand a very clear structure, while the latter are way more open. There is an underlying relation that hypotheses tend to be more quantitative, while research questions are often more qualitative. Still, this is a mere correlation and not a really causal relation, but it confuses a great many people. Chasing whether to write hypotheses or research questions is in addition often a question of the tradition of the respective scientific discipline. Many folks in natural science are still proud of their razor sharp hypotheses, and other fellow within socials science lean clearly towards research questions. Ideally, find out what your supervisors demand, which is the most simple heuristic to this end. Still, this point underlines that from a philosophy of science standpoint the silo-mentality of disciplines has its reason, but does not always make sense. What is most important for you is that an adductive procedure may be most desirable, yet only in the long run. It is part of the tradition of many sciences the postulate, test, and adjust. This works more on a time-scale of years or decades, but thus demands a temporal grain of a longer research agenda. Hence an abductive is not suitable for a shorter project such as a thesis. \n\n===== Study area =====\nThe next thing you want to focus on is the study area. This does not necessarily need to be a concrete space, but can also be something of a less concrete system. In a systematic review or critical content analysis this can be a branch of the literature. Within an image analysis this can be a set of paintings. In an ethnographic study this can be a specific group of people. Within a lab experiments it can be a completely artificially designed study, such as a set of planting pots that are irrigated, shaded and endure different temperature settings. This the study area is the specific system or setting that is being investigated. Again, there is a certain tradition that deductive studies are more tamed or have a deeper control or understanding of their study area, while inductive studies are more open minded and less clear cut when it comes the the pre-study understanding of the study area. Yet do not be fooled, inductive studies can be very clear when it comes to looking at something specific, these studies do it just with a different kind of open-mindedness. What is the benefit for you is to be clear in defining where you want to work in, may it be inductive or deductive. It is helpful to have a clear definition, because otherwise you will be either overworking yourself or have a sample set that is too small. The study area should ideally be chosen to make it very clear how your research represents some dynamics, pattern or mechanisms that can either serve as a basis to approximate reproducible knowledge, or at least knowledge where the path towards it can be clearly documented. Hence the study area is something that allows you to make your research specific and thus tamed. Your choice remarks a start as well as an end. Ideally, it should be exciting and represent a clear knowledge gap, but in addition it also demands to represent a bigger picture that is well represented by the chosen system. If you want to work on small business dynamics you need to work with organization that can be seen as representing the overall dynamics. If you want to make a survey, you certainly do not want to focus on outliers if you try to represent a bigger group. Ideally you build on the experience or already established researchers to learn to make the right choice concerning your study area. \n\n===== Data gathering =====\nIt is somewhat beyond an introductory entry to tame the whole world of data, which includes gathering data, analyzing it, and interpreting it. Yet we may take a peek at what is relevant, and can and should be achieved in the realms of an outline. The core goal to this end is probably the sweet spot between innovation and feasibility. Innovation in terms of your research design means that you read the relevant literature and, integrated it into your considerations on how to gather and analyze data. Start with textbook knowledge, and then go from the most highly cited general papers on the subject into more specific papers that are closer to the context you focus on. There is a lot of experience out there that is relevant to your specific methodological design, you just need to adapt it to your focus. This is exactly the reason why divergent thinking is a key skill in academia. Instead adding to the pile of literature you read you may want to consult an expert, however only with specific questions. You can get the basics on simple methods from textbooks, yet your supervisor may know best if there is anything else to consider. More often than not this may not be the case, yet there are things to consider. For instance is the sample size often relevant on both quantitative and qualitative science, and other questions such as heterogeneity of the sample are often of considerable importance. Questions of bias often already play a central role when gathering data, and then there are the simple mechanics. For you to document the gathering process, are there any machines or tools needed, and are there other technical aspects to consider such as software solutions? Yet the key question when getting data is whether if it is feasible or not. No data can be gathered if it is impossible to get it. Early career researchers often overplay their hand to this end. Make a pretest and see how long it takes you. Also remember that over time you may get faster concerning some mechanics, but also new and unanticipated problems may arise. Plan at least a third more time than the most conservative estimate. If you do not need it, fine, yet much research already took longer at this stage. Lastly, research needs to add to existing knowledge, hence it should be innovative. We all stand on the shoulders of giants. At an early stage of your career this is often contextual knowledge, not fundamental one. You may not want to reinvent the wheel, but will certainly not revolutionize levitation. You hover somewhere in between, floating between the solid ground of existing knowledge and the stars overhead. Make sure that you always remember that we all started somewhere. If you read this text about how to write an outline, it is most likely that you are only starting in your career. Remember that we all started somewhere, and in research that is often done by learning to gather data.\n\n===== Data analysis =====\nThe next step is analysis. Analyzing data equals experience. There may be some simple recipes how to analyze data, after all this is what normal-science text books are made for. Yet, from a methodological standpoint this does not seem to be very innovative. Yet again, also when it comes to learning methods, you need to start somewhere. One would not start to learn making food with a lasagne. Making simple pasta is a good start, yet the multilayered mixedness of a lasagne takes some experience. With methods it is quite the same. Learn the basics first, and then move on to get into the more complicated stuff. You need to get the initial stuff under your belt in order to go into advanced analysis, may it be qualitative or quantitative. No one can take this away from you, you will have to learn this yourself. Hence, doing analysis is first and foremost a matter of experience. Your supervisors and others are not there to help you to this end, because this would take the experience away from you. They may stop you if you made a mistake, but learning to make a content analysis -as an example- takes a long time. Read other research and then get at it. You can only evolve into an experienced researcher yourself, because experience is tough to teach. This with your outline you just need to write the main steps, and then your supervisor will make you aware of any biases, flaws or sources or problems. If you hear nothing, great. Then all should be fine, otherwise you will find out on your own. \n\n===== Data interpretation =====\nData interpretation is the toughest point to anticipate in an outline, because this would take a deep command of the available literature, and the cross-sectional mechanisms that are relevant within this literature. One could fall into the trap of writing a generic critique before the study was conducted. There is a tendency that researchers write such simple truths as \"this research is biased by a perspective of the global north\", which may be true, but is a bit of a one-size-fits-all category that helps no one, the least any suppressed groups. Hence make sure to be specific if you write anything at all. Yet this is a section often best led intentionally left blank. \n\n===== Potential pitfalls =====\nPotential pitfalls is again something that is better if aimed at concrete problems, yet should not be lofty or speculative. What may be challenges that are potentially relevant for your very context? Are these about you (e.g. concerning time scheduling issues), about the research (such as sample availability), or about the broader context (i.e. a global pandemic)? Write only what you know that has been proven as a problem in the past, do not speculate way beyond your current sphere of knowledge. This may be a moment to consult experienced researchers again, because they may have bumped into problems that are rooted in experience, which is a much more solid knowledge base compared to speculative anticipation. Always remember that research is also adaptation, you face problems, and then you overcome them. \n\n===== Time frame =====\nFor many people making a time planing is beneficial, as they claim that they need the pressure. Others make a timeline that is just for breaking it, much in the spirit of Douglas Adams: \"I love deadlines, I love the whooshing sounds they make when they fly by.\"\nI wish I could claim that you just make a timeline and then make it work, but this is also not really functional. Ultimately, I believe we make deadlines and timelines to learn about our own imperfections."
                    },
                    "sha1": "g6i2yh4kn01uzpp6u0arsmb2yvep68o"
                }
            },
            {
                "title": "Yes, and",
                "ns": "0",
                "id": "316",
                "revision": {
                    "id": "5064",
                    "parentid": "5049",
                    "timestamp": "2021-04-09T15:15:34Z",
                    "contributor": {
                        "username": "Imi",
                        "id": "37"
                    },
                    "minor": null,
                    "comment": "/* Goals */",
                    "model": "wikitext",
                    "format": "text/x-wiki",
                    "text": {
                        "@xml:space": "preserve",
                        "@bytes": "2981",
                        "#text": "{|class=\"wikitable\" style=\"text-align: center; width: 100%\"\n! colspan = \"4\" | Type !! colspan = \"4\" | Team Size\n|-\n| '''[[:Category:Collaborative Tools|Collaborative Tools]]''' || [[:Category:Software|Software]] || [[:Category:Personal Skills|Personal Skills]] || [[:Category:Productivity Tools|Productivity Tools]] || [[:Category:Team Size 1|1]] || '''[[:Category:Team Size 2-10|2-10]]''' || '''[[:Category:Team Size 11-30|11-30]]''' || '''[[:Category:Team Size 30+|30+]]'''\n|}\n\n== What, Why & When ==\n\"Yes, and...\"-thinking is a technique used in improvisation. It originates from and is most commonly known in improvisational theater, but it can also be applied in any other context where new ideas shall be developed. Spontaneous improvisation building on \"Yes, and...\" can improve [[Glossary|brainstorming]] processes and encourage team members to better cooperate and listen to each other in the process of idea generation. You may call it a proper philosophy on how to work and communicate with each other.\n\n== Goals ==\n* Generation of new ideas\n* Improved [[Glossary|communication]]\n* More engaging, productive teamwork\n\n== Getting started ==\n[[File:Yes-1137274 1920.jpg|450px|thumb|right|If you say \"Yes, and...\", everyone profits]]\nThere are two parts to \"Yes, and...\": the \"Yes!\" and the \"and...\".\nThe \"Yes\" means not to immediately judge other people's ideas and decline them if they do not seem suitable, but rather be positive about them. The \"and...\"-element encourages the addition of new information and thoughts which can lead to more complex ideas.\n\n\"Yes, and...\" does not necessarily mean that every new sentence has to begin with the words \"Yes, and\". Instead, it is about the mental attitude of accepting whatever other people offer and continuing or expanding upon their line of thought. It means going with the flow instead of blocking ideas and losing the dynamics of an ongoing discussion. \"Yes, and\" is about losing one's ego and fostering positivity in a group. For a theatre audience, this is more fun to watch; and for team members at work (in any field, really), it is both more fun to engage with and more productive than being overly critical and blocking ideas as soon as they are formulated.\n\n== Links & Further reading ==\n\n''Sources:''\n* ThoughtCo.- [https://www.thoughtco.com/yes-and-improv-game-2713213 Improve Acting Instincts and Performance With This Clever Improv Game]\n* Wikipedia - [https://en.wikipedia.org/wiki/Yes,_and... Yes, and]\n* Medium - [https://medium.com/improv4/saying-yes-and-a-principle-for-improv-business-life-fd050bccf7e3 Saying \"Yes, and\" - A Principle for improv, business & life]\n\nYoutube - [https://www.youtube.com/watch?v=DphjhudlZis Big Think video] on the Yes, and - principle\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n[[Category:Team Size 30+]]\n\nThe [[Table_of_Contributors|author]] of this entry is Christopher Franz."
                    },
                    "sha1": "0rc22dr1ruziifr0xf5jgcqrwsgd5zj"
                }
            }
        ]
    }
}