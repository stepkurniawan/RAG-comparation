{"question":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA a powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is the purpose of ANCOVA in statistical analysis?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram and how is it created?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"Reducing variance in field experiments or complex laboratory experiments","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA is used to compare group means while controlling for the effect of a covariate.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"contexts":{"0":["==Advantages and Limitations of A\/B Testing==\n'''Advantages'''\nA\/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment.","'''Limitations'''\nDespite its advantages, A\/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A\/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A\/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented."],"1":["The ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.","====Analysis of Variance====\n'''The [https:\/\/www.investopedia.com\/terms\/a\/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment."],"2":["'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while [https:\/\/365datascience.com\/bayesian-vs-frequentist-approach\/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this","There is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future."],"3":["'''What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency.''' Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy's law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of agency. There is thus a metaphysical problem when we try to differentiate the agent, their actions, and the consequences of their actions. One could claim that this can be solved by focussing on the consequences of the actions of agents alone. However, this consequentialist view is partly a theoretical consideration, as this view can create many interesting experiments, but does not really help us to solve the problem of unintentional acts per se. Still, consequentialism and a focus on mere actions is also relevant","What is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https:\/\/www.britannica.com\/science\/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations."],"4":["== At least one categorical variable ==\n'''Your dataset does not only contain continuous data.''' Does it only consist of categorical data, or of categorical and continuous data?\n<imagemap>Image:Statistics Flowchart - Data Formats.png|650px|center|\npoly 288 2 151 139 289 271 424 138 [[Data formats]]\npoly 137 148 0 285 138 417 273 284  [[An_initial_path_towards_statistical_analysis#Only_categorical_data:_Chi_Square_Test|Only categorical data: Chi Square Test]]\npoly 436 151 299 288 437 420 572 287 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data|Categorical and continuous data]]\n<\/imagemap>","Ordination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through"],"5":["|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.","'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes\/no (1\/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()<\/code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions."],"6":["|}\n<br\/>\n<br\/>\n<br\/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics.","== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as ''similarity measure'' or ''distance measure'', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named \"unsupervised learning\", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term \"clustering\" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article."],"7":["'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]","* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition."],"8":["'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients","Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the"],"9":["== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.","'''In short:'''\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the \"noise\" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n'''Prerequisite knowledge'''\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients"],"10":["== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test.","one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test."],"11":["1989, p.403). Because of this, Content Analysis is a potent method to identify trends and patterns in (text) sources, to determine authorship, or to monitor (public) opinions on a specific topic (3).","Apart from text, a diverse set of data can be analyzed using Content Analysis. \"Anything that occurs in sufficient numbers and has reasonably stable meanings for a specific group of people may be subjected to content analysis.\" (Krippendorff 1989, p.404). The data must convey a message to the receiver and be durable (2, 3). Often, Content Analysis focuses on data that are difficult or impossible to interpret with other methods (3). The data may exist 'naturally' and be publicly available, for example verbal discourse, written documents, or visual representations from mass media (newspapers, books, films, comics etc.); or be rather unavailable to the public, such as personal letters or witness accounts. The data may also be generated for the research purpose (e.g. interview transcripts) (1, 2, 4)."],"12":["'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method.","'''There are different forms of correlation analysis.''' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https:\/\/365datascience.com\/students-t-distribution\/ Student's t-distribution]. Alternative correlation measures like [https:\/\/www.statisticssolutions.com\/kendalls-tau-and-spearmans-rank-correlation-coefficient\/ Kendall's tau and Spearman's rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data."],"13":["You can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=\"R\" inline>corrplot<\/syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=\"R\" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n<\/syntaxhighlight>","\"Displacement\", \"Horsepower\", \"Rear Axle Ratio\",\n                            \"Weight\", \"1\/4 Mile Time\", \"Engine\", \"Transmission\",\n                            \"Gears\", \"Carburetors\")\n<\/syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=\"R\" line>\n# Fig.5\ncorrplot(correlations,\n         method = \"circle\",\n         type = \"upper\",\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         tl.cex = 0.6)\n<\/syntaxhighlight>"],"14":["Telemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is \u201cthe system of determining information about an animal through the use of radio signals from or to a device carried by the animal\u201d (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===","If you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification\/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc."],"15":["The most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https:\/\/link.springer.com\/content\/pdf\/10.1186\/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.","A second case of a [https:\/\/statisticsbyjim.com\/regression\/check-residual-plots-regression-analysis\/ skewed distribution] is when the residuals are showing any sort of clustering. Residuals should be distributed like stars in the sky. If they are not, then your error is not normally distributed, which basically indicates that you are missing some important information."],"16":["In the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https:\/\/data.library.virginia.edu\/the-wilcoxon-rank-sum-test\/ Wilcoxon rank sum test] may be more appropriate.","You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n'''If you would like to know how one can create the diagrams which you see here, this is the R code:'''\n\n<syntaxhighlight lang=\"R\" line>"],"17":["In 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).","== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5)."],"18":["'''Transdisciplinary research is of special importance to Sustainability Science and has received immense recognition in this field in the last years.''' This is because \"[s]ustainability is also inherently transdisciplinary\" (Stock & Burton 2011, p.1091), as it builds on the premise of solving real-world problems which are deeply nestled in ecological, political, economic and social processes and structures and therefore cannot be understood and solved without engaging with these spheres (Kates et al. 2015). Transdisciplinary research is a suitable approach for Sustainability Science: it allows to incorporate the knowledge of relevant stakeholders; it considers the normative dimensions involved in societal endeavors (that is, diverging norms, goals and visions of different societal spheres); and it increases [[Glossary|legitimacy]], ownership and accountability for the jointly developed solutions (Lang et al. 2012; Stock & Burton 2011). The integrative transdisciplinary approach highlights systemic interdependencies, enables a better understanding of","approaches developed over the last decades, which illustrated the importance of joined problem framing of science together with society, and the opportunities that arise out of such a mutual learning approach. '''All in all, Sustainability Science has shifted repeatedly in terms its underlying paradigms in order to arrive at its current agenda,''' and is - following [https:\/\/www.simplypsychology.org\/Kuhn-Paradigm.html Kuhn] - truly emerging out of the recognition of a crisis. This article provides a brief overview on the developments that contribute to the methodological approaches that are prominent within the wide arena of Sustainability Science today. Since this arena is emerging, this overview cannot be conclusive, and is certainly not comprehensive."],"19":["'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research.","as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality."],"20":["'''The system is at the basis of System Thinking.''' System Thinking is a form of scientific approach to organizing and understanding 'systems' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of 'holistic' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that \"(...) all system dynamics are in principle non-linear\" and that \"(...)","'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries."],"21":["== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:","# '''Have a look at your notes and try to find more information.''' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!"],"22":["Within [[ANOVA]] designs, the question whether a variable is a [https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To","==== Random factors ====\nThis brings us to the next factor that can affect our model validity. There are many things we might want to know when creating a statistical model, but there may also be things that we do not want to know. Statistically speaking, these are ''random factors'' or ''random variables''. Regarding these, we explicitly exclude the variance that is created by these parts of the dataset, because we want to minimise their effects. An example would be a medical study that wants to investigate the effect of a certain drug on the recovery rate of some diseased patients. In such studies, the information whether a patient is a smoker or not is often included as a random factor, because smoking negatively affects many diseases and the respective recovery rates. We know that smoking makes many things worse (from a medical standpoint) and this is why such variables are excluded. More on random factors can be found [[ Field_experiments#Fixed_effects_vs._Random_effects | here]]."],"23":["resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can","a deeper understanding of the specific context and case. While real world experiments emerged some decades ago already, they are only starting to gain wider recognition. All the while, the reproducibility crisis challenges the classical laboratory and field experiments, as a wider recognition that many results - for instance from psychological studies - cannot be reproduced. All this indicates that while much of our scientific knowledge is derived from experiments, much remains to be known, also about the conduct of experiments themselves."],"24":["== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> ''Is everyone on the same page?''\n<br> ''Are there important issues that have been neglected so far?''\n<br> ''Is there unspoken dissonance?''\n<br> ''Is there an elephant in the room?''\n<br> ''What are we actually talking about?''","===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes."],"25":["== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check\/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician.","unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."],"26":["'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed.","It also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row\/column will not have the same value behind it or belong to the same bin."],"27":["expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''","Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https:\/\/www.britannica.com\/biography\/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many"],"28":["Multivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests.","* brand -- Pizza brand (class label)\n* id -- Sample analysed\n* mois -- Amount of water per 100 grams in the sample\n* prot -- Amount of protein per 100 grams in the sample\n* fat -- Amount of fat per 100 grams in the sample\n* ash -- Amount of ash per 100 grams in the sample\n* sodium -- Amount of sodium per 100 grams in the sample\n* carb -- Amount of carbohydrates per 100 grams in the sample\n* cal -- Amount of calories per 100 grams in the sample\n\nHow can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen\/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality."],"29":["The trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].","__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and\/or the nature of the outputs is well understood."],"30":["== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a \"brute force\" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.","The trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3]."],"31":["=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and\/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps.","== What are scientific methods? =="],"32":["== Goals ==\nSince the goal of mindfulness is basically having \"no mind\", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free.","If you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions \u2013 just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that"],"33":["'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information.","== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan."],"34":["Mixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as","While psychology, medicine, agricultural science, biology and later ecology thus thrived in their application of experimental designs and studies, there was also an increasing recognition of information that was creating [[Bias and Critical Thinking|biases]] or otherwise falsely skewed the results. '''The ANOVA hence became amended by additional modifications, ultimately leading to more advanced statistics that were able to focus on diverse statistical effects''', and reduce the influence of skews rooted, for instance, in sampling bias, statistical bias or other flaws. Hence, [[Mixed-Effect Models]] became an advanced next step in the history of statistical models, leading to more complex statistical designs and experiments, taking more and more information into account. In addition, meta-analytical approaches led to the combination and summarising of several case studies into a systematic overview. This was the dawn of a more integrational understanding of different studies that were combined into a [[Meta-Analysis]], taking different contexts of the numerous studies into account as well. In"],"35":["Typically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs.","== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know."],"36":["The most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.","== '''Starting to engage with model reduction - an initial approach''' =="],"37":["=== Redundancy analysis ===","== '''Starting to engage with model reduction - an initial approach''' =="],"38":["'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007).","Narrative Research is \"(...) the study of stories\" (Polkinghorne 2007, p.471) and thus \"(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience.\" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the 'narratology', i.e. the structure and grammar of a story; the 'narrative content', i.e. the themes and meanings conveyed through the story; and\/or the 'narrative context', which revolves around the effects of the story (Squire et al. 2014)."],"39":["Over the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer [[Causality|causality]] when explaining the","|-\n| style=\"width: 33%\"| '''[[:Category:Past|Past]]''' || style=\"width: 33%\"| '''[[:Category:Present|Present]]''' || '''[[:Category:Future|Future]]'''\n|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented."],"40":["Poisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https:\/\/studiousguy.com\/poisson-distribution-examples\/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===","|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}"],"41":["== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:","There are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to ''25 minutes = 1 \"Pomodoro\"'').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15\u201330 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ===="],"42":["=== Curse of dimensionality ===\nThis term was coined by Richard R. Bellman, an American applied mathematician. As the number of features \/ dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. '''Principle Component Analysis helps to alleviate this problem'''.","=== Non metric (multi)dimensional scaling ==="],"43":["<syntaxhighlight lang=\"Python\" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n<\/syntaxhighlight> \n\n<syntaxhighlight lang=\"Python\" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n<\/syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term \u2018residual\u2019 refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.","Looking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=\"Python\" line>\nsns.pairplot(data, hue=\"group\", diag_kind=\"hist\")\n<\/syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]"],"44":["'''Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.''' At the time, corporate planning was traditionally \"(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude.\" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the \"Unified Planning Machinery\". The idea was to listen to planners' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell's management to deal with the 1973 and 1981 oil crises (1). Shell's success popularized the method. By 1982,","oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2)."],"45":["Romanian-American psychosociologist '''Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s''' with his - and his collaborator Helen Jennings' - 'sociometry', which served \"(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction.\" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).","'''One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel'''. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued \"against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions.\" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|'''Moreno's original work on Social Networks.''' Source: Borgatti et al. 2009, p.892]]"],"46":["==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.","'''Note:''' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nThis entry aims to introduce Stacked Area Plot and its visualization using R\u2019s <syntaxhighlight lang=\"R\" inline>ggplot2<\/syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview=="],"47":["|}\n<br\/>__NOTOC__\n<br\/><br\/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world.","== What the method does ==\nThought Experiments are the philosophical method that asks the \"What if\" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word \"experiment\" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results."],"48":["We can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.","== Temporal grain and measures of time =="],"49":["* The research project [http:\/\/besatz-fisch.de\/content\/view\/34\/57\/lang,german\/ \"Besatzfisch\"] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to '''understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.''' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (''Cooperation''). Second, anglers were questioned about fish population sizes and their economic implications (''Consultation'') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (''Consultation''). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (''Empowerment''). Fifth, social-ecological models","* Das Forschungsprojekt [http:\/\/besatz-fisch.de\/content\/view\/34\/57\/lang,german\/ \"Besatzfisch\"] ist ein gutes Beispiel f\u00fcr ein langfristiges transdisziplin\u00e4res Forschungsprojekt, das sich mit unterschiedlichen methodischen Ans\u00e4tzen besch\u00e4ftigt. In diesem vierj\u00e4hrigen Projekt wurde versucht, '''die \u00f6kologische, soziale und wirtschaftliche Rolle und die Auswirkungen des Besatzfischs in nat\u00fcrlichen \u00d6kosystemen zu verstehen'''. Zun\u00e4chst wurden Fische in die \u00d6kosysteme eingef\u00fchrt und die nachfolgende Populationsdynamik qualitativ und quantitativ gemessen, vieles davon gemeinsam mit den kooperierenden Anglern (\"Kooperation\"). Zweitens wurden die Angler*innen \u00fcber die Gr\u00f6\u00dfe der Fischpopulationen und ihre"]},"context_precision":{"0":0.9999999999,"1":0.0,"2":0.9999999999,"3":0.0,"4":0.9999999999,"5":1.0,"6":1.0,"7":1.0,"8":1.0,"9":1.0,"10":0.9999999999,"11":0.0,"12":1.0,"13":1.0,"14":0.9999999999,"15":1.0,"16":1.0,"17":0.5,"18":0.0,"19":0.9999999999,"20":0.9999999999,"21":0.9999999999,"22":0.9999999999,"23":1.0,"24":1.0,"25":0.9999999999,"26":0.9999999999,"27":0.5,"28":0.9999999999,"29":0.5,"30":0.9999999999,"31":0.9999999999,"32":0.9999999999,"33":0.0,"34":0.9999999999,"35":1.0,"36":0.9999999999,"37":0.9999999999,"38":0.5,"39":0.9999999999,"40":0.9999999999,"41":0.5,"42":0.9999999999,"43":0.9999999999,"44":1.0,"45":0.9999999999,"46":0.0,"47":1.0,"48":0.9999999999,"49":1.0},"context_recall":{"0":1.0,"1":0.75,"2":1.0,"3":1.0,"4":1.0,"5":1.0,"6":1.0,"7":1.0,"8":1.0,"9":1.0,"10":1.0,"11":1.0,"12":1.0,"13":1.0,"14":1.0,"15":1.0,"16":1.0,"17":1.0,"18":0.5,"19":1.0,"20":1.0,"21":1.0,"22":1.0,"23":1.0,"24":0.6666666667,"25":1.0,"26":1.0,"27":1.0,"28":1.0,"29":1.0,"30":0.8,"31":0.6666666667,"32":1.0,"33":1.0,"34":1.0,"35":1.0,"36":1.0,"37":1.0,"38":1.0,"39":1.0,"40":1.0,"41":1.0,"42":1.0,"43":1.0,"44":1.0,"45":0.5,"46":0.0,"47":1.0,"48":0.0,"49":0.8333333333}}