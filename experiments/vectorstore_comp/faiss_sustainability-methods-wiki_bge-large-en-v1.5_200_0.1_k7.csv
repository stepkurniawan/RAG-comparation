|question|ground_truths|contexts|context_precision|context_recall
0|What is the advantage of A/B testing?|The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.|"[""==Advantages and Limitations of A/B Testing==\n'''Advantages'''\nA/B testing has several advantages over traditional methods of evaluating the effectiveness of a product or design. First, it allows for a more controlled and systematic comparison of the treatment and control version. Second, it allows for the random assignment of users to the treatment and control groups, reducing the potential for bias. Third, it allows for the collection of data over a period of time, which can provide valuable insights into the long-term effects of the treatment."", ""'''Limitations'''\nDespite its advantages, A/B testing also has some limitations. For example, it is only applicable to products or designs that can be easily compared in a controlled manner. In addition, the results of an A/B test may not always be generalizable to the broader population, as the sample used in the test may not be representative of the population as a whole. Furthermore, it is not always applicable, as it requires a clear separation between control and treatment, and it may not be suitable for testing complex products or processes, where the relationship between the control and treatment versions is not easily defined or cannot be isolated from other factors that may affect the outcome.\n\nOverall, A/B testing is a valuable tool for evaluating the effects of software or design changes. By setting up a controlled experiment and collecting data, we can make evidence-based decisions about whether a change should be implemented."", '[[File:AB_Test.jpg|500px|thumb|center]]\n\n\nAn important advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process. To ensure the trustworthiness of the results of A/B tests, the scheme of [[Experiments and Hypothesis Testing|scientific experiments]] is followed, consisting of a planning phase, an execution phase, and an evaluation phase.\n\n==Planning Phase==\nDuring the planning phase, a goal and hypothesis are formulated, and a study design is developed that specifies the sample size, the duration of the study, and the metrics to be measured. This phase is crucial for ensuring the reliability and validity of the test.', ""'''THIS ARTICLE IS STILL IN EDITING MODE'''\n==A/B Testing in a nutshell==\nA/B testing, also known as split testing or bucket testing, is a method used to compare the performance of two versions of a product or content. This is done by randomly assigning similarly sized audiences to view either the control version (version A) or the treatment version (version B) over a set period of time and measuring their effect on a specific metric, such as clicks, conversions, or engagement. This method is commonly used in the optimization of websites, where the control version is the default version, while the treatment version is changed in a single variable, such as the content of text, colors, shapes, size or positioning of elements on the website.\n\n[[File:AB_Test.jpg|500px|thumb|center]]"", 'In most cases, it is not known a priori whether the discrepancy in the results between A and B is in favor of A or B. Therefore, the alternative hypothesis should consider the possibility that both versions A and B have different levels of efficiency. In order to account for this, a two-sided test is typically preferred for the subsequent evaluation.\n\n\'\'\'For example:\'\'\'\n\n""To fix the problem that there are hardly any subscriptions for my newsletter, I will put the sign-up box higher up on the website.""\n\nGoal: Increase the newsletter subscriptions on the website.\n\nH0: There are no significant changes in the number of new newsletter subscribers between the control and treatment versions.\n\nH1: There are significant changes in the number of new newsletter subscribers between the control and treatment versions.', '===Minimizing Confounding Variables===\nIn order to obtain accurate results, it is important to minimize confounding variables before the A/B test is conducted. This involves determining an appropriate sample size, tracking the right users, collecting the right metrics, and ensuring that the randomization unit is adequate.', 'Furthermore, it is important to analyze only the subset of the population/users that were potentially affected. For example, in an A/B test aimed at optimizing newsletter subscriptions, it would be appropriate to exclude individuals who were already subscribed to the newsletter, as they would not have been affected by the changes made to the subscription form.\n\nAdditionally, the metrics used in the experiment should be carefully chosen based on their relevance to the hypotheses being tested. For example, in the case of an e-commerce site, metrics such as newsletter subscriptions and revenue per user may be of interest, as they are directly related to the goal of the test. However, it is important to avoid considering too many metrics at once, as this can increase the risk of miscorrelation.']"|0.7861904761747525|1.0
1|What is the ANOVA a powerful for?|Reducing variance in field experiments or complex laboratory experiments|"['The ANOVA can be a powerful tool to tame the variance in field experiments or more complex laboratory experiments, as it allows to account for variance in repeated experimental measures of experiments that are built around replicates. The ANOVA is thus the most robust method when it comes to the design of deductive experiments, yet with the availability of more and more data, also inductive data has increasingly been analysed by use of the ANOVA. This was certainly quite alien to the original idea of Fisher, who believed in clear robust designs and rigid testing of hypotheses. The reproducibility crisis has proven that there are limits to deductive approaches, or at least to the knowledge these experiments produce. The 20th century was certainly fuelled in its development by experimental designs that were at their heart analysed by the ANOVA. However, we have to acknowledge that there are limits to the knowledge that can be produced, and more complex analysis methods evolved with the wider availability of computers.', ""====Analysis of Variance====\n'''The [https://www.investopedia.com/terms/a/anova.asp ANOVA] is one key analysis tool of [[Experiments|laboratory experiments]]''' - but also other experiments as we shall see later. This statistical test is - mechanically speaking - comparing the means of more than two groups by extending the restriction of the [[Simple_Statistical_Tests#Two_sample_t-test|t-test]]. Comparing different groups became thus a highly important procedure in the design of experiments, which is, apart from laboratories, also highly relevant in greenhouse experiments in ecology, where conditions are kept stable through a controlled environment."", ""'''Taken together, the ANOVA is one of the most relevant calculation tools to fuel the exponential growth that characterised the 20th century.''' Agricultural experiments and medical trials are widely built on the ANOVA, yet we also increasingly recognise the limitations of this statistical model. Around the millennium, new models emerged, such as [[Mixed Effect Models|mixed effect models]]. But at its core, the ANOVA is the basis of modern deductive statistical analysis."", ""== What the method does ==\nThe ANOVA is a deductive statistical method that allows to compare how a continuous variable differs under different treatments in a designed experiment. '''It is one of the most important statistical models, and allows for an analysis of data gathered from designed experiments.''' In an ANOVA-designed experiment, several categories are thus compared in terms of their mean value regarding a continuous variable. A classical example would be how a certain type of barley grows on different soil types, with the three soil types loamy, sandy and clay (Crawley 2007, p. 449). If the majority of the data from one soil type differs from the majority of the data from the other soil type, then these two types differ, which can be tested by a t-test. The ANOVA is in principle comparable to the t-test, but extends it: it can compare more than two groups, hence allowing to compare data in more complex, designed experiments."", 'In addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs.', 'Furthermore, many researchers use the ANOVA today in an inductive sense. With more and more data becoming available, even from completely undersigned sampling sources, the ANOVA becomes the analysis of choice if the difference between different factor levels is investigated for a continuous variable. Due to the [[Glossary|emergence]] of big data, these applications could be seen critical, since no real hypothesis are being tested. Instead, the statistician becomes a gold digger, searching the vastness of the available data for patterns, [[Causality#Correlation_is_not_Causality|may these be causal or not]]. While there are numerous benefits, this is also a source of problems. Non-designed datasets will for instance not be able to test for the impact a drug might have on a certain diseases. This is a problem, as systematic knowledge production is almost assumed within the ANOVA, but its application these days is far away from it. The inductive and the', ""The general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.""]"|0.26785714284375|0.8571428571428571
2|What is the difference between frequentist and Bayesian approaches to probability, and how have they influenced modern science?|Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.|"[""'''Centuries ago, Thomas Bayes proposed a dramatically different approach'''. Here, an imperfect or a small sample would serve as basis for statistical interference. Very crudely defined, the two approaches start at exact opposite ends. While frequency statistics demand preconditions such as sample size and a normal distribution for specific statistical tests, Bayesian statistics build on the existing sample size; all calculations base on what is already there. Experts may excuse my dramatic simplification, but one could say that frequentist statistics are top-down thinking, while [https://365datascience.com/bayesian-vs-frequentist-approach/ Bayesian statistics] work bottom-up. The history of modern science is widely built on frequentist statistics, which includes such approaches as methodological design, sampling density and replicates, and diverse statistical tests. It is nothing short of a miracle that Bayes proposed the theoretical foundation for the theory named after him more than 250 years ago. Only with the rise of modern computers was this"", 'There is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].\n\n== Outlook ==\nBayesian methods have been central in a variety of domains where outcomes are probabilistic in nature; fields such as engineering, medicine, finance, etc. heavily rely on Bayesian methods to make forecasts. Given that the computational resources have continued to get more capable and that the field of machine learning, many methods of which also rely on Bayesian methods, is getting more research interest, one can predict that Bayesian methods will continue to be relevant in the future.', '== Strengths & Challenges ==\n* Bayesian approaches incorporate prior information into its analysis. This means that any past information one has can be used in a fruitful way.\n* Bayesian approach provides a more intuitive and direct statement of the probability that the hypothesis is true, as opposed to the frequentist approach where the interpretation of p-value is convoluted.\n\n* Even though the concept is intuitive to understand, the mathematical formulation and definitions can be intimidating for beginners.\n* Identifying correct prior distribution can be very difficult in real life problems which are not based on careful experimental design.\n* Solving complex models with Bayesian approach is still computationally expensive.\n\n\n== Normativity ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. Just like the examples presented above, this has several flipsides of the same coin.', 'There is no doubt that Bayesian statistics surpass frequentists statistics in many aspects, yet in the long run, Bayesian statistics may be preferable for some situations and datasets, while frequentists statistics are preferable under other circumstances. Especially for predictive modeling and small data problems, Bayesian approaches should be preferred, as well as for tough cases that defy the standard array of statistical distributions. Let us hope for a future where we surely know how to toss a coin. For more on this, please refer to the entry on [[Non-equilibrium dynamics]].', 'While frequentist statistics evolve around [[A matter of probability|probability]], there are other ways to calculate the value of models. Information theory is - in a nutshell - already focusing on diverse approaches to evaluate information gained through statistical analysis. The shortcoming of [[Simple Statistical Tests|p-values]] have been increasingly moved in the focus during the last one or two decades, yet we are far away from alternative approaches (e.g. AIC) being either established or accepted. Instead, statistics are scattered when it comes to analysis pathways, and model reduction is currently at least in the everyday application of statistics still closer to philosophy than our way of conducting statistics. The 20th century was somewhat reigned by numbers, and probability was what more often than not helped to evaluate the numbers. New approaches are emerging, and probability and other measures may be part of the curriculum of high school students in the future, leaving the more advanced stuff that we have no idea about today to higher education.', ""== Bayes Theorem ==\nIn contrast to a Frequentist approach, the Bayesian approach allows researchers to think about events in experiments as dynamic phenomena whose probability figures can change and that change can be accounted for with new data that one receives continuously. However, this coin comes with a flip side\n\nOn the one hand, Bayesian Inference can overall be understood as a deeply [[:Category:Inductive|inductive]] approach since any given dataset is only seen as a representation of the data it consists of. This has the clear benefit that a model based on a Bayesian approach is way more adaptable to changes in the dataset, even if it is small. In addition, the model can be subsequently updated if the dataset is growing over time. '''This makes modeling under dynamic and emerging conditions a truly superior approach if pursued through Bayes' theorem.''' In other words, Bayesian statistics are better able to cope with changing condition in a continuous stream of data."", ""This does however also represent a flip side of the Bayesian approach. After all, many data sets follow a specific statistical [[Data distribution|distribution]], and this allows us to derive clear reasoning on why these data sets follow these distributions. Statistical distributions are often a key component of [[:Category:Deductive|deductive]] reasoning in the analysis and interpretation of statistical results, something that is theoretically possible under Bayes' assumptions, but the scientific community is certainly not very familiar with this line of thinking. This leads to yet another problem of Bayesian statistics: they became a growing hype over the last decades, and many people are enthusiastic to use them, but hardly anyone knows exactly why. Our culture is widely rigged towards a frequentist line of thinking, and this seems to be easier to grasp for many people. In addition, Bayesian approaches are way less implemented software-wise, and also more intense concerning hardware demands.""]"|0.9999999999|1.0
3|Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?|Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.|"[""'''What is relevant to consider is that actions of agents need to be wilful, i.e. a mere act that can be seen as serendipity is not part of agency.''' Equally, non-anticipated consequences of actions based on causal chains are a problem in agency. Agency is troubled when it comes to either acknowledging serendipity, or Murphy's law. Such lucky or unlucky actions were not anticipated by the agents, and are therefore not really included in the definition of agency. There is thus a metaphysical problem when we try to differentiate the agent, their actions, and the consequences of their actions. One could claim that this can be solved by focussing on the consequences of the actions of agents alone. However, this consequentialist view is partly a theoretical consideration, as this view can create many interesting experiments, but does not really help us to solve the problem of unintentional acts per se. Still, consequentialism and a focus on mere actions is also relevant"", 'What is however clear is that the three concepts - agency, complexity and emergence - have consequences about our premises of empirical knowledge. What if ultimately nothing is generalisable? What if all valid arguments are only valid for a certain time? And what if some strata will forever escape a truly reliable measurement? We [[Big problems for later|cannot answer]] these problems here, yet it is important to differentiate what we know, what we may be able to know, and what we will probably never know. The [https://www.britannica.com/science/uncertainty-principle uncertainty principle of Heisenberg] in Quantum mechanics which refers to the the position and momentum of particles illustrates that some things cannot be approximated, observed or known. Equal claims can be made about larger phenomena, such as personal identity. Hence, as much as agency, complex systems and emergence can be boundary objects for methods, they equally highlight our (current) limitations.', 'The investigation of complex system has thrived in the last decades, both from an empirical as well as from a conceptual perspective. Many methods emerged or were subsequently adapted to answer questions as well as explore relations, and this thrive towards a deeper understanding of systems is at least one important difference to agency from a methodological standpoint. Much novel data is available, and often inductively explored. The scale of complex systems makes an intervention with a focus on [[Causality and correlation|causality]] a challenge, hence many investigated relations are purely correlative. Take for instance social media, or economic flows, which can be correlatively investigated, yet causality is an altogether different matter. This creates a methodological challenge, since many of our questions regarding human systems are normative, which is why many researchers assume causality in their investigations, or at least discuss relations as if these are causal. Another methodological problem related to causality are non-linear relations, since much of the statistical canon', 'Being happy is a serendipity. Those who can be optimistic can consider themselves lucky, and have a responsibility towards others. This responsibility is a truly societal responsibility, because only if we can all enable everybody to be at peace may we overcome our deeply rooted problems. Our first and foremost priority will be to learn ways to marginalise minorities less, and help humans less privileged than us. We should never forget that optimism can obviously be also part of a privilege. I was privileged enough to see many different cultures, and remember that I saw optimistic and pessimistic world views in rich and poor. Yet this is easier said then lived, and only when all people can grow up with the capability to thrive in this world, and have their rights guarded will we surely and maybe finally be able to make the case for optimism. I for myself thrive towards this lack of inequality, and we have gotten closer over time. When my grandmother was born more than 100 years ago two thirds of all people lived', ""Ever since Aristotle and his question ”What is its nature?”, we are nagged by the nitty-grittiness of true causality or deep causality. I propose that there is a '''high road of causality''', and a '''low road of causality'''. The high road allows us to explain everything on how two things or phenomena are linked. While I reject this high road, many schools of thought consider it to be very relevant. I for myself prefer the low road of causality: May one thing or phenomena be causally linked to another thing or phenomena; if I take one away, will the other not happen? This automatically means that I have to make compromises of how much I understand about the world."", ""Ever since Aristotle and his question ”What is its nature?”, we are nagged by the nitty-grittiness of true causality or deep causality. I propose that there is a '''high road of causality''', and a '''low road of causality'''. The high road allows us to explain everything on how two things or phenomena are linked. While I reject this high road, many schools of thought consider it to be very relevant. I for myself prefer the low road of causality: May one thing or phenomena be causally linked to another thing or phenomena; if I take one away, will the other not happen? This automatically means that I have to make compromises of how much I understand about the world."", '==== The high and the low road of causality ====']"|0.0|1.0
4|What is the recommended course of action for datasets with only categorical data?|For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.|"[""== At least one categorical variable ==\n'''Your dataset does not only contain continuous data.''' Does it only consist of categorical data, or of categorical and continuous data?\n<imagemap>Image:Statistics Flowchart - Data Formats.png|650px|center|\npoly 288 2 151 139 289 271 424 138 [[Data formats]]\npoly 137 148 0 285 138 417 273 284  [[An_initial_path_towards_statistical_analysis#Only_categorical_data:_Chi_Square_Test|Only categorical data: Chi Square Test]]\npoly 436 151 299 288 437 420 572 287 [[An_initial_path_towards_statistical_analysis#Categorical_and_continuous_data|Categorical and continuous data]]\n</imagemap>"", 'Ordination are one of the pillars of pattern recognition, and therefore play an important role not only in many disciplines, but also in data science in general. The most fundamental differentiation in which analysis you should choose is rooted in the data format. The difference between continuous data and categorical or nominal data is the most fundamental devision that allows you to choose your analysis pathway. The next consideration you need to review is whether you see the ordination as a string point to inspect the data, or whether you are planning to use it as an endpoint or a discrete goal within your path of analysis. Ordinations re indeed great for skimming through data, yet can also serve as a revelation of results you might not get through other approaches. Other consideration regarding ordinations are related to deeper matters of data formats, especially the question of linearity of continuous variables. This already highlights the main problem of ordination techniques, namely that you need a decent overview in order to choose the most suitable analysis, because only through', 'If your answer is YES, you are heading way below. Click [[An_initial_path_towards_statistical_analysis#Is_there_a_categorical_predictor?|HERE]].\n\n\n== Only continuous variables ==\nSo your data is only continuous.<br/>\nNow, you need to check if there dependencies between the variables.\n<imagemap>Image:Statistics Flowchart - Continuous - Dependencies.png|650px|center|\npoly 383 5 203 181 380 359 559 182 [[Causality]]\npoly 182 205 2 381 179 559 358 382 [[An_initial_path_towards_statistical_analysis#No_dependencies:_Correlations|No dependencies]]\npoly 585 205 405 381 582 559 761 382 [[An_initial_path_towards_statistical_analysis#Clear_dependencies|Clear dependencies]]\n</imagemap>', 'Instead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values.', ""</imagemap>\n'''How do I know?'''\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric (including continuous) variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is ''categorical''."", ""'''How do I know?'''\n* Investigate your data using <code>str</code> or <code>summary</code>. ''integer'' and ''numeric'' data is not ''categorical'', while ''factorial'', ''ordinal'' and ''character'' data is categorical.\n\nIf your answer is NO, you should stick to the ANOVA - more specifically, to the kind of ANOVA that you saw above (based on regression analysis, or based on a generalised linear model). An ANOVA compares the means of more than two groups by extending the restriction of the t-test. An ANOVA is typically visualised using [[Introduction_to_statistical_figures#Boxplot|Boxplots]].</br> The key R command is <code>aov()</code>. Check the entry on the [[ANOVA]] to learn more."", ""===== Is there a categorical predictor? =====\n'''You have several predictors (= independent variables) in your dataset.''' But is (at least) one of them categorical?\n<imagemap>Image:Statistics Flowchart - Categorical predictor.png|650px|center|\npoly 387 1 208 184 385 359 563 183 [[Data formats]]\npoly 180 197 1 380 178 555 356 379 [[An_initial_path_towards_statistical_analysis#ANCOVA|ANCOVA]]\npoly 584 196 405 379 582 554 760 378 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Multiple Regression]]\n</imagemap>""]"|0.6961904761765524|1.0
5|What is a Generalised Linear Model (GLM)?|A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.|"['|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.', ""'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions."", ""unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."", ""== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician."", ""== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling."", 'Since not all researchers build their analysis on a deep understanding of diverse statistical distributions, GLMs can also be seen as an educational means that enable a deeper understanding of the diversity of approaches, thus preventing wrong approaches from being utilised. A sound understanding of the most important GLM models can bridge to many other more advanced forms of statistical analysis, and safeguards analysts from compromises in statistical analysis that lead to ambiguous results at best.', '== Outlook ==\nIntegration of the diverse approaches and parameters utilised within GLMs will be an important stepping stone that should not be sacrificed just because even more specific analysis are already gaining dominance in many scientific disciplines. Solving the problems of evaluation and model selection as well as safeguarding the comparability of complexity reduction within GLMs will be the frontier on which these approaches will ultimately prove their worth. These approaches have been available for more than half of a century now, and during the last decades more and more people were enabled to make use of their statistical power. Establishing them fully as a part of the standard canon of statistics for researchers would allow for a more nuanced recognition of the world, yet in order to achieve this, a greater integration into students curricular programs will be a key goal.\n\n== Key Publications ==']"|0.9166666666361111|1.0
6|What is Cluster Analysis?|Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).|"[""|}\n<br/>\n<br/>\n<br/>\n'''In short:''' Clustering is a method of data analysis through the grouping of unlabeled data based on certain metrics."", '== What the method does ==\nClustering is a method of grouping unlabeled data based on a certain metric, often referred to as \'\'similarity measure\'\' or \'\'distance measure\'\', such that the data points within each cluster are similar to each other, and any points that lie in separate cluster are dissimilar. Clustering is a statistical method that falls under a class of [[Machine Learning]] algorithms named ""unsupervised learning"", although clustering is also performed in many other fields such as data compression, pattern recognition, etc. Finally, the term ""clustering"" does not refer to a specific algorithm, but rather a family of algorithms; the similarity measure employed to perform clustering depends on specific clustering model.\n\nWhile there are many clustering methods, two common approaches are discussed in this article.', ""'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between."", '\'\'\'Cluster Map\'\'\'<br>\nIn a second step, \'\'\'all statements (= data points) are grouped into clusters\'\'\' ""which represent higher order conceptual groupings of the original set of statements."" (Trochim 1989). This is done - as proposed by Trochim (1989) - by using the X-Y coordinates of each statement after the multidimensional scaling as input for a Ward\'s hierarchical cluster analysis (HCA), which ""partitions the multidimensional scaling map into any number of clusters"" (Trochim 1989, 8). The analyst has to decide how many clusters the statements should be grouped into, based on what makes sense with regards to the topic and statements. You will find more on cluster analysis in [[Clustering Methods|this entry]].', '\'\'\'Weaknesses\'\'\'\n* Even though the number of clusters does not have to be pre-specified, users still need to specify the \'\'distance metric\'\' and the \'\'linkage criteria\'\'.\n* This method becomes very slow when the size of the data set is large.\n\n== See Also ==\nAlthough this article only focused on two clustering algorithms. There are many other clustering methods that employ different strategies. Readers are suggested to investigate expectation maximization algorithm, and  density based clustering algorithms, and latent class analysis specifically.\n\n== Key Publications ==\n\n\'\'\'k-Means Clustering\'\'\'\n* MacQueen, James. ""Some methods for classification and analysis of multivariate observations."" Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. No. 14. 1967.', ""== Background ==\n[[File:Clustering SCOPUS.png|400px|thumb|right|'''SCOPUS hits for Clustering until 2019.''' Search terms: 'Clustering', 'Cluster Analysis' in Title, Abstract, Keywords. Source: own.]]\nCluster analysis shares history in various fields such as anthropology, psychology, biology, medicine, business, computer science, and social science. \n\nIt originated in anthropology when Driver and Kroeber published Quantitative Expression of Cultural Relationships in 1932 where they sought to clusters of [[Glossary|culture]] based on different culture elements. Then, the method was introduced to psychology in the late 1930s."", ""=== k-Means Clustering ===\n\nThe k-means clustering method assigns '''n''' examples to one of '''k''' clusters, where '''n''' is the sample size and  '''k''', which needs to be chosen before the algorithm is implemented, is the number of clusters. This clustering method falls under a clustering model called centroid model where centroid of a cluster is defined as the mean of all the points in the cluster. K-means Clustering algorithm aims to choose centroids that minimize the within-cluster sum-of-squares criterion based on the following formula:\n\n[[File:K-Means Sum of Squares Criterion.png|center]]\n\nThe in-cluster sum-of-squares is also called inertia in some literature.\n\nThe algorithm involves following steps:""]"|0.8541666666453125|1.0
7|What is the purpose of Network Analysis?|Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.|"[""'''In short:''' Social Network Analysis visualises social interactions as a network and analyzes the quality and quantity of connections and structures within this network.\n\n== Background ==\n[[File:Scopus Results Social Network Analysis.png|400px|thumb|right|'''SCOPUS hits per year for Social Network Analysis until 2019.''' Search terms: 'Social Network Analysis' in Title, Abstract, Keywords. Source: own.]]"", ""* '''Data Analysis''': When it comes to analyzing the gathered data, there are different network properties that researchers are interested in in accordance with their research questions. The analysis may be qualitative as well as quantitative, focusing either on the structure and quality of connections or on their quantity and values. (Marin & Wellman 2010, p.16; Butts 2008, p.21f). The analysis can focus on \n** the quantity and quality of ties that connect to individual nodes\n** the similarity between different nodes, or\n** the structure of the network as a whole in terms of density, average connection length and strength or network composition."", ""==== Step by Step ====\n* '''Type of Network:''' First, Social Network Analysts decide whether they intend to focus on a holistic view on the network (''whole networks''), or focus on the network surrounding a specific node of interest (''ego networks''). They also decide for either ''one-mode networks'', focusing on one type of node that could be connected with any other; or ''two-mode networks'' where there are two types of nodes, with each node unable to be connected with another node of the same type (Marin & Wellman 2010, 13). For a two-mode network, you could imagine an analysis of social events and the individuals that visit these, where each event is not connected to another event, but only to other individuals; and vice-versa."", ""'''How do I know?'''\n* Classification is performed when you have (X, y) pair of data (where X is a set of independent variables and y is the dependent variable). Hence, you can map each X to a y. Clustering is performed when you only have X in your dataset. So, this decision depends on the dataset that you have.\n\n\n== Network Analysis ==\nWORK IN PROGRESS\n'''You have decided to do a Network Analysis.''' In a Network Analysis, you arrange your data in a network structure to understand their connections and the distance between individual data points. Check the entry on [[Social Network Analysis]] and the R example entry MISSING to learn more. Keep in mind that network analysis is complex, and there is a wide range of possible approaches that you need to choose between."", '* \'\'\'Data Collection\'\'\': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a \'\'prompted recall\'\' approach, they are asked which people they would think of with regards to a specific topic (e.g. ""To whom would you go for advice at work?"") while they are shown a pre-determined list of potentially relevant individuals. In the \'\'free list\'\' approach, they are asked to recall individuals without seeing a list (Butts', '\'\'System Analysis\'\' ""(...) is about discovering organisational structures in systems and creating insights into the organisation of causalities. It is about taking a problem apart and reassembling it in order to understand its components and feedback relationships."" (Haraldsson 2004, p.5). \'\'System Analysis\'\', thus, focuses on understanding the system and being able to recreate it. This is often done through the application of Causal Loop Diagrams, which will be explained below. For more information, refer to the entry on [[System Analysis]].', '* An important element of the analysis is not just the creation of quantitative or qualitative insights, but also the \'\'\'visual representation\'\'\' of the network. For this, the researcher ""(...) will naturally seek the clearest visual arrangement, and all that matters is the pattern of connections."" (Scott 1988, p.113) Based on the structure of the ties, the network can take different forms, such as the Wheel, Y, Chain or Circle shape.\nImportantly, the actual distance between nodes is thus not equatable with the physical distance in a [[Glossary|visualisation]]. Sometimes, nodes that are visually very close to each other are actually very far away. The actual distance between elements of the network should be measured based on the ""number of lines which it is necessary to traverse in order to get from one point to another."" (Scott 1988, p.114)']"|0.99999999995|1.0
8|What is the purpose of ANCOVA in statistical analysis?|ANCOVA is used to compare group means while controlling for the effect of a covariate.|"['\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients', 'Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the', ""== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test."", '\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. Pay attention to the data format of your independent variable(s). \'\'integer\'\' and \'\'numeric\'\' data is not \'\'categorical\'\', while \'\'factorial\'\', \'\'ordinal\'\' and \'\'character\'\' data is categorical.\n\n\n===== ANCOVA =====\n\'\'\'If you have at least one categorical predictor, you should do an ANCOVA\'\'\'. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more.', '==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]', 'library(car)\n\nmodel_2 <- lm(t3~t1+group+group:t1, data= data)\nAnova(model_2, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_test_for_model2.png|300px|frameless|center]]\nInteraction is not significant(p = 0.415), so the slope across the groups is not different.\n\n===Computation===\nWhen running the ANCOVA test in R attention should be paid on the orders of variables, because our main goal is to remove the effect of the covariate first. This notion is based on the general ANCOVA steps:\n\n1) Run a regression between the independent(covariate) and dependent variables.\n\n2) Identify the residual values from the results.\n\n3) Run an ANOVA on the residuals.', '3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=""R"" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]']"|0.9999999999833333|1.0
9|What are the key principles and assumptions of ANCOVA?|ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.|"[""== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test."", '\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients', 'one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.', 'Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the', '==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]', '\'\'\'How do I know?\'\'\'\n* Check the entry on [[Data formats]] to understand the difference between categorical and numeric variables.\n* Investigate your data using <code>str</code> or <code>summary</code>. Pay attention to the data format of your independent variable(s). \'\'integer\'\' and \'\'numeric\'\' data is not \'\'categorical\'\', while \'\'factorial\'\', \'\'ordinal\'\' and \'\'character\'\' data is categorical.\n\n\n===== ANCOVA =====\n\'\'\'If you have at least one categorical predictor, you should do an ANCOVA\'\'\'. An ANCOVA is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. Check the entry on [[Ancova]] to learn more.', '===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data']"|0.8541666666453125|1.0
10|What are the assumptions associated with ANCOVA?|ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.|"[""== Assumptions ==\n'''Regression assumptions'''  \n# The relationship between dependent and independent variables must be linear for each treatment group.\n'''ANOVA assumptions'''\n# Variances between groups are homogeneous.\n# Residuals are randomly and normally distributed.\n'''Specific ANCOVA assumptions'''\n# A further specific (but optional) assumption is homogeneity of slopes. It is optional because it is only required to simplify the model for estimation of adjusted means.\n\n\n==What is One-way ANCOVA?==\nOne-way ANCOVA compares the variance in the group means within a sample with only one independent variable or factor that has three or more than three categorical groups whilst considering the coavriate. Since ANCOVA is hypothesis-based test, we need to have a understanding and well developed question about our data that we want an answer to, before we can generate a hypothesis and run the test."", 'one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the ANCOVA test.', '\'\'\'In short:\'\'\'\nAnalysis of covariance (ANCOVA) is a statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable.\n\n== Prerequites ==\n\n\'\'\'Prerequisite knowledge\'\'\'\nAn understanding of the [[ANOVA]]\n* Main effects and interaction effects\n* Sum of squares\n* Mean squares\n* ANOVA tables\n* F-statistics and significance values\n* Post-hoc analysis (Tukey, Bonferonni, etc.)\nAn understanding of Linear Regression\n* Regression slopes\n* p-values\n* Coefficients', 'Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the', '==What is two-way ANCOVA?==\nA two-way ANCOVA is, like a one-way ANCOVA, however, each sample is defined in two categorical groups. The two-way ANCOVA therefore examines the effect of two factors on a dependent variable – and also examines whether the two factors affect each other to influence the continuous variable.\n\n----\n[[Category:Statistics]]\n[[Category:R examples]]', '===Data preparation===\nIn order to demonstrate One-way ANCOVA test we will refer to balanced dataset ""anxiety"" taken from the ""datarium"" package. The data provides the anxiety score, measured at three time points, of three groups of individuals practicing physical exercises at different levels (grp1: basal, grp2: moderate and grp3: high). The question is ""What treatment type has the most effect on anxiety level?""\n\n<syntaxhighlight lang=""R"" line>\ninstall.packages(""datarium"")\n#installing the package datarium where we can find different types of datasets\n</syntaxhighlight>\n\n\n===Exploring the data===\n<syntaxhighlight lang=""R"" line>\ndata(""anxiety"", package = ""datarium"")\ndata = anxiety\nstr(data)\n#Getting the general information on data', '3) Run an ANOVA on the residuals.\n\nBefore running ANCOVA test with adjusted before treatment anxiety score (t1 = covariate) let us run the ANOVA test only on groups and after treatment anxiety score(t3) in order to see the impact of ANCOVA test on Sum of Squares of Errors.\n\n<syntaxhighlight lang=""R"" line>\nmodel_3<- lm(t3~group, data = data)\nAnova(model_3, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model3.png|300px|frameless|center]]\n\n<syntaxhighlight lang=""R"" line>\nmodel_1 <- lm(t3~t1+group, data = data)\nAnova(model_1, type = ""II"")\n</syntaxhighlight>\n[[File:Anova_model1.png|300px|frameless|center]]']"|0.8055555555287036|1.0
11|What are the strengths and challenges of Content Analysis?|Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.|"['1989, p.403). Because of this, Content Analysis is a potent method to identify trends and patterns in (text) sources, to determine authorship, or to monitor (public) opinions on a specific topic (3).', 'Apart from text, a diverse set of data can be analyzed using Content Analysis. ""Anything that occurs in sufficient numbers and has reasonably stable meanings for a specific group of people may be subjected to content analysis."" (Krippendorff 1989, p.404). The data must convey a message to the receiver and be durable (2, 3). Often, Content Analysis focuses on data that are difficult or impossible to interpret with other methods (3). The data may exist \'naturally\' and be publicly available, for example verbal discourse, written documents, or visual representations from mass media (newspapers, books, films, comics etc.); or be rather unavailable to the public, such as personal letters or witness accounts. The data may also be generated for the research purpose (e.g. interview transcripts) (1, 2, 4).', '== Normativity ==\n==== Quality Criteria ====\n* Reliability is difficult to maintain in the Content Analysis. A clear and unambiguous definition of codes as well as testings for inter-coder reliability represent attempts to ensure inter-subjectivity and thus stability and reproducibility (3, 4). However, the ambiguous nature of the data demands an interpretative analysis process - especially in the qualitative approach. This interpretation process of the texts or contents may interfere with the intended inter-coder reliability.', '|}\n<br/>__NOTOC__\n<br/><br/><br/>\n\'\'\' In short:\'\'\' Content Analysis relies on the summarizing of data (most commonly text) into content categories based on pre-determined rules and the analysis of the ""coded"" data.', 'While there is a wide range of qualitative Content Analysis approaches, this entry will focus on joint characteristics of these. For more information on the different variations, please refer to Schreier (2014) in the Key Publications.', '== Strengths and Challenges ==', ""* Last, the coded data are ''analyzed and interpreted'' whilst taking into account the theoretical constructs that underlie the research. Inferences are drawn, which - according to Mayring (2000) - may focus on the communicator, the message itself, the socio-cultural context of the message, or on the message's effect. The learnings may be validated in the face of other information sources, which is often difficult when Content Analysis is used in cases where there is no other possibility to access this knowledge. Finally, new hypotheses may also be formulated (all from 1, 2).""]"|0.19999999998|1.0
12|What are the three main methods to calculate the correlation coefficient and how do they differ?|The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.|"[""'''A note on calculating the correlation coefficient:'''\nGenerally, there are three main methods to calculate the correlation coefficient: Pearson's correlation coefficient, Spearman's rank correlation coefficient and Kendall's rank coefficient. \n'''Pearson's correlation coefficient''' is the most popular among them. This measure only allows the input of continuous data and is sensitive to linear relationships. \nWhile Pearson's correlation coefficient is a parametric measure, the other two are non-parametric methods based on ranks. Therefore, they are more sensitive to non-linear relationships and measure the monotonic association - either positive or negative. '''Spearman's rank correlation''' coefficient calculates the rank order of the variables' values using a monotonic function whereas '''Kendall's rank correlation coefficient''' computes the degree of similarity between two sets of ranks introducing concordant and discordant pairs.\nSince Pearson's correlation coefficient is the most frequently used one among the correlation coefficients, the examples shown later based on this correlation method."", ""'''There are different forms of correlation analysis.''' The Pearson correlation is usually applied to normally distributed data, or more precisely, data that shows a [https://365datascience.com/students-t-distribution/ Student's t-distribution]. Alternative correlation measures like [https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ Kendall's tau and Spearman's rho] are usually applied to variables that are not normally distributed. I recommend you just look them up, and keep as a rule of thumb that Spearman's rho is the most robust correlation measure when it comes to non-normally distributed data."", ""* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (“circle”, “square”, “ellipse”, “number”, “shade”, “color”, “pie”), “circle” is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* '''type''': how the correlation matrix will be displayed. It can either be “upper”, “lower” or “full”. Full is called by default.\n* '''order''': order method for the correlation coefficients. The “hclust” method orders them in hierarchical order, but it also possible to order them alphabetical (“alphabetical”) or with a [[Principal_Component_Analysis|principal component analysis]] (“PCA”).\n* '''tl.col''': color of the labels.\n* '''tl.srt:''' rotation of the labels in degrees.\n* '''tl.cex:''' size of the labels."", ""== What the method does ==\nCorrelation analysis examines the relationship between two [[Data formats|continuous variables]], and test whether the relation is statistically significant. For this, correlation analysis takes the sample size and the strength of the relation between the two variables into account. The so-called ''correlation coefficient'' indicates the strength of the relation, and ranges from -1 to 1. A coefficient close to 0 indicates a weak correlation. A coefficient close to 1 indicates a strong positive correlation, and a coefficient close to -1 indicates a strong negative correlation. \n\nCorrelations can be applied to all kinds of quantitative continuous data from all spatial and temporal scales, from diverse methodological origins including [[Survey]]s and Census data, ecological measurements, economical measurements, GIS and more. Correlations are also used in both inductive and deductive approaches. This versatility makes correlation analysis one of the most frequently used quantitative methods to date."", ""==== Calculating Pearson's correlation coefficient r ====\nThe formula to calculate [https://www.youtube.com/watch?v=2B_UW-RweSE a Pearson correlation coefficient] is fairly simple. You just need to keep in mind that you have two variables or samples, called x and y, and their respective means (m). \n[[File:Bildschirmfoto 2020-05-02 um 09.46.54.png|400px|center|thumb|This is the formula for calculating the Pearson correlation coefficient r.]]\n<br/>\n\n=== Conducting and reading correlations ===\nThere are some core questions related to the application and reading of correlations. These can be of interest whenever you have the correlation coefficient at hand - for example, in a statistical software - or when you see a correlation plot.<br/>"", ""\ufeffBabbie, Earl. 2016. ''The Practice of Social Research.'' 14th ed. Boston: Cengage Learning.\n\nNeuman, W. Lawrence. 2014. ''Social Research Methods: Qualitative and Quantitative Approaches.'' 7th ed. Pearson.\n\n\n\n== Further Information ==\n* If you want to practise recognizing whether a correlation is weak or strong I recommend spending some time on this website. There you can guess the correlation coefficients based on graphs: http://guessthecorrelation.com/\n\n* [https://online.stat.psu.edu/stat501/lesson/1/1.6 The correlation coefficient]: A very detailed and vivid article\n\n* [https://online.stat.psu.edu/stat501/lesson/1/1.7 The relationship of temperature in Celsius and Fahrenheit]: Several examples of interpreting the correlation coefficient"", '== Partial Correlation vs. Multiple Linear Regression ==\nWhen thinking about partial correlation, one might notice that the basic idea is similar to a multiple linear regression. Even though a correlation is bidirectional while a regression aims at explaining one variable by another, in both approaches we try to identify the true and unbiased relationship of two variables. So when should we use which method? In general, partial correlation is a building step for multiple linear regression. Accordingly, linear regression has a broader range of applications and is therefore in most cases the method to choose. Using a linear regression, one can model interaction terms and it is also possible to loosen some']"|0.8333333333055556|1.0
13|What is the purpose of a correlogram and how is it created?|A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction. It is created using the R package corrplot. Correlation coefficients can be calculated and stored in a variable before creating the plot for clearer code.|"['You can see that this plot looks much more informative and attractive.\n\n\n== Correlogram ==\n=== Definition ===\nThe correlogram visualizes the calculated correlation coefficients for more than two variables. You can quickly determine whether there is a relationship between the variables or not. The different colors give you also the strength and the direction of the relationship. To create such a correlogram, you need to install the R package <syntaxhighlight lang=""R"" inline>corrplot</syntaxhighlight> and import the library. Before we start to create and customize the correlogram, we can calculate the correlation coefficients of the variables and store it in a variable. It is also possible to calculate it when creating the plot, but this makes your code more clear.\n\n=== R Code ===\n<syntaxhighlight lang=""R"" line>\nlibrary(corrplot)\ncorrelations <- cor(mtcars)\n</syntaxhighlight>', '""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                            ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                            ""Gears"", ""Carburetors"")\n</syntaxhighlight>\n[[File:correlogram.png|500px|thumb|right|Fig.5]]\nNow, we are ready to customize and plot the correlogram.\n<syntaxhighlight lang=""R"" line>\n# Fig.5\ncorrplot(correlations,\n         method = ""circle"",\n         type = ""upper"",\n         order = ""hclust"",\n         tl.col = ""black"",\n         tl.srt = 45,\n         tl.cex = 0.6)\n</syntaxhighlight>', ""'''Note:''' This entry revolves specifically around Correlation Plots, including Scatter Plots, Line charts and Correlograms. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]]. For more info on Data distributions, please refer to the entry on [[Data distribution]].\n<br/>\n'''In short:''' Correlations describe the mutual relationship between two variables. They provide the possibility\nto measure the relation between any kind of quantitative data - may it be continuous, discrete and interval data, yet there are even correlations for ordinal data, although these shall be less relevant for us. In this entry you will learn about how to build correlation plots in R. To learn more about correlations in theoretical terms, please refer to the entry on [[Correlations]]. To learn about Partial Correlations, please refer to [https://sustainabilitymethods.org/index.php/Partial_Correlation this entry].\n__TOC__\n<br/>"", 'Analysis of covariance (ANCOVA) is the statistical test that compares the means of more than two groups by taking under the control the ""noise"" caused by covariate variable that is not of experimental interest. This is done in order to see the true effect of the variable of interest on a dependent variable. Fundamental idea of ANCOVA is based on hypothesis testing, its goal is to evaluate multiple mutually exclusive theories about our [[Glossary|data]], where null hypothesis claims that the means of the groups are equal, and alternative [[Glossary|hypothesis]] rejects that based on p-value. Additionally, it is important to highlight that ANCOVA also gives information on variables by means of Sum of Squares partitioning that shows the size of error or unexplained information in relation. Since ANCOVA historically comes from the combination of Linear regression and ANOVA, one should meet the assumptions inherent from them as well as specific to ANCOVA before proceeding to the', ""* '''method''': which method should be used to visualize your correlation matrix. There are seven different methods (“circle”, “square”, “ellipse”, “number”, “shade”, “color”, “pie”), “circle” is called by default and shows the correlation between the variables in different colors and sizes for the circles.\n* '''type''': how the correlation matrix will be displayed. It can either be “upper”, “lower” or “full”. Full is called by default.\n* '''order''': order method for the correlation coefficients. The “hclust” method orders them in hierarchical order, but it also possible to order them alphabetical (“alphabetical”) or with a [[Principal_Component_Analysis|principal component analysis]] (“PCA”).\n* '''tl.col''': color of the labels.\n* '''tl.srt:''' rotation of the labels in degrees.\n* '''tl.cex:''' size of the labels."", '==== Covariance matrix ====\n\nThe covariance matrix is a square d x d matrix, where each entry represents the covariance of a possible pair of the original features. It has the following properties:\n* The size of the matrix is equal to the number of features in the data\n* The main diagonal on the matrix contains the variances of each initial variables.\n* The matrix is symmetric, since Cov(d1, d2) = Cov(d1, d2)\n\nThe covariance matrix gives you a summary of the relationship among the initial variables.\n* A positive value indicate a directly proportional relationship (as d1 increases, d2 increases, and vice versa)\n* A negative value indicate a indirectly proportional relationship (as d1 increases, d2 decreases, and vice versa)', 'With a bit experience, you can recognize quite fast, if there is a relationship between the variables. It is also possible to see, if the relationship is weak or strong and if there is a positive, negative or sometimes even no relationship. You can visualize correlation in many different ways, here we will have a look at the following visualizations:\n\n* Scatter Plot\n* Scatter Plot Matrix\n* Line chart\n* Correlogram']"|0.8928571428348213|1.0
14|What is telemetry?|Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.|"['Telemetry is another method that was further developed in recent years, although it has been used already for decades in wildlife ecology. Telemetry is “the system of determining information about an animal through the use of radio signals from or to a device carried by the animal” (11). For birds, this method can be applied in areas ranging in size from restricted breeding territories of resident bird species to movement patterns of international migratory species. Also, the distribution patterns of infectious diseases of migratory species can be tracked (11). However, for some birds, negative effects on nesting behavior were observed (12). \n\n== Key publications ==\n=== Theoretical ===', ""If you deconstruct this definition, here is what you have:\n* '''Task (T)''' is what we want the Machine Learning algorithm to be able to perform once the learning process has finished. Usually, this boils down to predicting a value ([[Regression Analysis|regression]]), deciding in which category or group a given data falls into (classification/clustering), or solve problems in an adaptive way (reinforcement learning).\n* '''Experience (E)''' is represented by the data on which the learning is to be based. The data can either be structured - in a tabular format (eg. excel files) - or unstructured - images, audio files, etc."", ""== What, Why & When ==\nLoom is a '''video recording software which lets you easily record your screen and yourself at the same time''' right in your browser. This way, you can show your work or ideas to colleagues or others, and not waste time and harddrive space on recording videos right on your computer.\n\n== Goals ==\n* Communicate your work to colleagues, collaborators, students in a way that is precise and engaging.\n* Better enable others to prepare for meetings."", 'THIS ARTICLE IS STILL IN EDITING MODE\n==What is Time Series Data==\nTime series data is a sequence of data points collected at regular intervals of time. It often occurs in fields like engineering, finance, or economics to analyze trends and patterns over time. Time series data is typically numeric data, for example, the stock price of a specific stock, sampled at an hourly interval, over a time frame of several months. It could also be sensory data from a fitness tracker, sampling the heart rate every 30 seconds, or a GPS track of the last run.\nIn this article, we will see examples from a household energy consumption dataset having data for longer than a year, obtained from [https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data Kaggle] (Download date: 20.12.2022).', '===== Data gathering =====', ""'''In short:''' This entry revolves around Transcription, which is the reconstruction of recorded audio into written text for the analysis of Interviews. For more on Interview Methodology, please refer to the [[Interviews|Interview overview page]]."", '====Continuous data====']"|0.9999999999|1.0
15|What is a common reason for deviation from the normal distribution?|A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.|"['The most abundant reason for a deviance from the normal distribution is us. We changed the planet and ourselves, creating effects that may change everything, up to the normal distribution. Take [https://link.springer.com/content/pdf/10.1186/1471-2458-12-439.pdf weight]. Today the human population shows a very complex pattern in terms of weight distribution across the globe, and there are many reasons why the weight distribution does not follow a normal distribution. There is no such thing as a normal weight, but studies from indigenous communities show a normal distribution in the weight found in their populations. Within our wider world, this is clearly different. Yet before we bash the Western diet, please remember that never before in the history of humans did we have a more steady stream of calories, which is not all bad.', 'A second case of a [https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/ skewed distribution] is when the residuals are showing any sort of clustering. Residuals should be distributed like stars in the sky. If they are not, then your error is not normally distributed, which basically indicates that you are missing some important information.', 'Reality is complex as people like to highlight today, but in a statistical sense that is often true. Most real datasets do not show a perfect model fit, but instead may individual data points deviate from the perfect fit the model assumes. What is now critical, is to assume that the error reflected by the residuals is normally distributed. Why, you ask? Because if it is not normally distributed, but instead shows a flawed pattern, then you missed an important variable that influences the distribution of the residuals. The distance from the points from the line should be normally distributed, which can be checked through a histogram.', ""'''What are reasons for outliers?''' There are many possible reasons why outliers may occur in a dataset. Some common reasons include: 1. Measurement errors: Outliers can occur due to errors in the measurement process, such as incorrect readings or faulty equipment. 2. Data entry errors: Outliers can also occur due to errors in data entry, such as transposing numbers or mistyping data. 3. Natural variations: In some cases, outliers may occur naturally as a result of natural variations in the data. For example, extreme weather events or rare occurrences may result in data points that are significantly different from the rest of the data. 4. Fraud: Outliers can also occur as a result of fraudulent activity, such as attempts to manipulate data or hide certain transactions. 5. Sampling errors: In some cases, outliers may occur due to sampling errors, such as a sample that is not representative of the population being studied."", ""==== Sample size matters ====\n[[File:NormalDistributionSampleSize.png|thumb|500px|right|'''Sample size matters.''' As these five plots show, bigger samples will more likely show a normal distribution.]]\n\nMost things in their natural state follow a normal distribution. If somebody tells you that something is not normally \ndistributed, this person is either very clever or not very clever. A [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/ small sample] can hamper you from finding a normal distribution. '''If you weigh five people you will hardly find a normal distribution, as the sample is obviously too small.''' While it may seem like a magic trick, it is actually true that many phenomena that can be measured will follow the normal distribution, at least when your sample is large enough. Consequently, much of the probabilistic statistics is built on the normal distribution."", ""'''Most phenomena we can observe follow a normal distribution.''' The fact that many do not want this to be true is I think associated to the fact that it makes us assume that the world is not complex, which is counterintuitive to many. While I believe that the world can be complex, there are many natural laws that explain many phenomena we investigate. The Gaussian [https://www.youtube.com/watch?v=mtbJbDwqWLE normal distribution] is such an example. [https://studiousguy.com/real-life-examples-normal-distribution/ Most things] that can be measured in any sense (length, weight etc.) are normally distributed, meaning that if you measure many different items of the same thing, the data follows a normal distribution."", 'The probability can be best explained with the [https://www.stat.colostate.edu/~vollmer/stat307pdfs/LN4_2017.pdf normal distribution]. The normal distribution basically tells us through probability how a certain value will add to an array of values. Take the example of the height of people, or more specifically people who define themselves as males. Within a given population or country, these have an average height. This means in other words, that you have the highest chance to have this height when you are part of this population. You have a slightly lower chance to have a slightly smaller or larger height compared to the average height. And you have a very small chance to be much smaller or much taller compared to the average. In other words, your probability is small to be very tall or very small. Hence the distribution of height follows a normal distribution, and this normal distribution can be broken down into probabilities. In addition, such a distribution can have a variance, and']"|0.8874999999778125|1.0
16|How can the Shapiro-Wilk test be used in data distribution?|The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.|"[""In the case of continuous metrics, such as average revenue per user, the [[T-Test|t-test or Welch's t-test]] may be used to determine the significance of the treatment effect. However, these tests assume that the data is normally distributed, which may not always be the case. In cases where the data is not normally distributed, nonparametric tests such as the [https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/ Wilcoxon rank sum test] may be more appropriate."", 'You can also use the Shapiro-Wilk test to check for normal distribution. If the test returns insignificant results (p-value > 0.05), we can assume normal distribution.\n\nThis barplot (at the left) represents the number of front-seat passengers that were killed or seriously injured annually from 1969 to 1985 in the UK. And here comes the magic trick: If you sort the annually number of people from the lowest to the highest (and slightly lower the resolution), a normal distribution evolves (histogram at the left).\n\n\'\'\'If you would like to know how one can create the diagrams which you see here, this is the R code:\'\'\'\n\n<syntaxhighlight lang=""R"" line>', '===Distribution Based===', '<syntaxhighlight lang=""R"" line>\n\n# we will work with the swiss() dataset.\n# to obtain a histogram of the variable Education, you type\n\nhist(swiss$Education)\n\n# you transform the data series with the natural logarithm by the use of log()\n\nlog_edu<-log(swiss$Education)\nhist(log_edu)\n\n# to make sure, that the data is normally distributed, you can use the shapiro wilk test\n\nshapiro.test(log_edu)\n\n# and as the p-value is higher than 0.05, log_edu is normally distributed\n\n</syntaxhighlight>\n\n====The Pareto distribution====\n[[File:Bildschirmfoto 2020-04-08 um 12.28.46.png|thumb|300px|\'\'\'The Pareto distribution can also be apllied when we are looking at how wealth is spread across the world.\'\'\']]', '<syntaxhighlight lang=""R"" line>\n#Shapiro-Wilk normality test\nshapiro.test(residuals(model_1))\n## Output:\n## data:  residuals(model_1)\n## W = 0.96124, p-value = 0.1362\n</syntaxhighlight>\n<br>\nHistogram of residual values is ""bell shaped"" and the Shapiro-Wilk normality test show p value of 0.1362 which is not significant (p>0.05) as a result we can concludevthat it is normally distributed.\n\n4. Assumption of Homogeneity of regression slopes checks that there is no significant interaction between the covariate and the grouping variable. This can be assessed as follow:\n\n<syntaxhighlight lang=""R"" line>\noptions(contrasts = c(""contr.treatment"", ""contr.poly""))\n\nlibrary(car)', ""'''How do I know?'''\n* Inspect the data by looking at [[Introduction_to_statistical_figures#Histogram|histograms]]. The key R command for this is <code>hist()</code>. Compare your distribution to the [[Data distribution#Detecting_the_normal_distribution|Normal Distribution]].  If the data sample size is big enough and the plots look quite symmetric, we can also assume it's normally distributed.\n*  You can also conduct the Shapiro-Wilk test, which helps you assess whether you have a normal distribution. Use <code>shapiro.test(data$column)</code>'. If it returns insignificant results (p-value > 0.05), your data is normally distributed.\n\nNow, let us have another look at your variables. '''Do you have continuous and categorical independent variables?'''"", ""#The data [[Data formats|should be continuous or on an ordinal scale.]]\n#We assume normally distributed data. While this is often not the case with smaller samples, according to the central limit theorem, the empirical distribution function of the standardised means converges towards the standard normal distribution with growing sample size. Hence, this prerequisite can be assumed to be met with a sufficiently large sample size (≥ 30). \n#The data must be drawn randomly from a representative sample.\n#For a '''Student’s t-test,''' equal variances in the two groups are required. However, by default, the built in function '''t.test()''' in R assumes that variances differ ('''Welch t-test'''). If it is known that variances are equal, one can set '''var.equal = TRUE''', which will lead to a Student’s t-test being performed instead.\n\n==Paired t-test==""]"|0.8734126983981414|1.0
17|Why is the Delphi method chosen over traditional forecasting methods?|The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.|"['In 1964, a RAND report from Gordon & Helmer brought the method to attention for a wider audience outside the military defense field (4, 5). Subsequently, Delphi became a prominent method in technological forecasting; it was also adapted in management; in fields such as drug policy, education, urban planning; and applied in order to understand economic and social phenomena (2, 4, 5). An important field today is the healthcare sector (7). While during the first decade of its use the Delphi method was mostly about forecasting future scenarios, a second form was developed later that focused on concept & [[Glossary|framework]] development (3).\n\nDelphi was first applied in these non-scientific fields before it reached academia (4). Here, it can be a beneficial method to identify topics, questions, terminologies, constructs or theoretical perspectives for research endeavours (3).', '== Strengths & Challenges ==\nThe literature indicates a variety of benefits that the Delphi method offers. \n* Delphi originally emerged due to a lack of data that would have been necessary for traditional forecasting methods. To this day, such a lack of empirical data or theoretical foundations to approach a problem remains a reason to choose Delphi. Delphi may also be an appropriate choice if the collective subjective judgment by the experts is beneficial to the problem-solving process (2, 4, 5, 6).\n* Delphi can be used as a form of group counseling when other forms, such as face-to-face interactions between multiple stakeholders, are not feasible or even detrimental to the process due to counterproductive group dynamics (4, 5).\n* The value of the Delphi method is that it reveals clearly those ideas that are the reason for disagreements between stakeholders, and those that are consensual (5).', '== Normativity ==\n==== Connectedness / nestedness ====\n* While Delphi is a common forecasting method, backcasting methods (such as [[Visioning & Backcasting|Visioning]]) or [[Scenario Planning]] may also be applied in order to evaluate potential future scenarios without tapping into some of the issues associated with forecasting (see more in the [[Visioning|Visioning]] entry)\n* Delphi, and the conceptual insights gathered during the process, can be a starting point for subsequent research processes.\n* Delphi can be combined with qualitative or quantitative methods beforehand (to gain deeper insights into the problem to be discussed) and afterwards (to gather further data).', 'survey answers, and the results for the 15 variables, the researchers further inferred that ""paying attention to each others\' answers made the forecasts more accurate"" (p.320), and that the participants were well able to assess the accuracy of their own estimates. The researchers calculated many more measures and a comparison to a non-Delphi forecasting round, which you can read more about in the publication. Overall, this example shows that the Delphi method works in that it leads to more accurate results over time, and that the process itself helps individuals better forecast than traditional forecasts would.', 'In their 2014 publication, Kauko & Palmroos present their results from a Delphi process with financial experts in Finland. They held a Delphi session with five individuals from the Bank of Finland, and the Financial Supervisory Authority of Finland, each. Every individual was anonymized with a self-chosen pseudonym so that the researchers could track the development of their responses. \'\'\'The participants were asked questions in a questionnaire that revolved around the close future of domestic financial markets.\'\'\' Specifically, the participants were asked to numerically forecast 15 different variables (e.g. stock market turnover, interest in corporate loans, banks\' foreign net assets etc.) with simple point estimates. These variables were chosen to ""fall within the field of expertise of the respondents, and at the same time be as independent of each other as possible."" (p.316). The participants were provided with information on the past developments of each of these variables.', ""|}\n<br/>__NOTOC__\n<br/>\n<br/>\n'''In short:''' The Delphi Method is an interactive form of data gathering in which expert opinions are summarized and consensus is facilitated."", ""* (5) Gordon, T.J. 2009. ''The Delphi Method.'' Futures Research Methodology V 3.0.\n* (6) Hallowell, M.R. Gambatese, J.A. 2010. ''Qualitative Research: Application of the Delphi method to CEM Research''. Journal of Construction Engineering and Management 136(1). 99-107.\n* (7) Boulkedid et al. 2011. ''Using and Reporting the Delphi Method for Selecting Healthcare Quality Indicators: A Systematic Review.'' PLoS ONE 6(6). 1-9.""]"|0.4428571428423809|1.0
18|What is the main goal of Sustainability Science and what are the challenges it faces?|The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.|"['\'\'\'Transdisciplinary research is of special importance to Sustainability Science and has received immense recognition in this field in the last years.\'\'\' This is because ""[s]ustainability is also inherently transdisciplinary"" (Stock & Burton 2011, p.1091), as it builds on the premise of solving real-world problems which are deeply nestled in ecological, political, economic and social processes and structures and therefore cannot be understood and solved without engaging with these spheres (Kates et al. 2015). Transdisciplinary research is a suitable approach for Sustainability Science: it allows to incorporate the knowledge of relevant stakeholders; it considers the normative dimensions involved in societal endeavors (that is, diverging norms, goals and visions of different societal spheres); and it increases [[Glossary|legitimacy]], ownership and accountability for the jointly developed solutions (Lang et al. 2012; Stock & Burton 2011). The integrative transdisciplinary approach highlights systemic interdependencies, enables a better understanding of', ""approaches developed over the last decades, which illustrated the importance of joined problem framing of science together with society, and the opportunities that arise out of such a mutual learning approach. '''All in all, Sustainability Science has shifted repeatedly in terms its underlying paradigms in order to arrive at its current agenda,''' and is - following [https://www.simplypsychology.org/Kuhn-Paradigm.html Kuhn] - truly emerging out of the recognition of a crisis. This article provides a brief overview on the developments that contribute to the methodological approaches that are prominent within the wide arena of Sustainability Science today. Since this arena is emerging, this overview cannot be conclusive, and is certainly not comprehensive."", ""== Why & When ==\nSustainability Research is a research field that aims at tackling societal problems by developing systemic understandings of these problems and oftentimes collaboratively solving them through [[Glossary|transdisciplinary]] research designs. In this context, the [[Glossary|concept]] of stakeholders is a crucial component. Stakeholders are people that are directly affected by sustainability problems and can contribute to their solution. '''Mapping-out these stakeholders is a common entry-point''' to gaining an understanding of the system and problem at hand, and is further important to gather an overview of the relevant stakeholders in this context.\n\n== Goal(s) ==\n* Gain an overview of relevant stakeholders and their positionality towards a real-world problem\n* Get a methodological starting point for subsequent transdisciplinary research\n* Enable the development of strategies for jointly developing solutions"", '* Hall & Rourke (2014) expand on communicative challenges, claiming that ""[n]ot all of the challenges that threaten TDSS [transdisciplinary sustainability science] are communication challenges, but communication breakdown can exacerbate any of them. Because of its centrality, care must be taken by collaborators to cultivate a healthy communication dynamic; however, given the many perspectives involved in a typical TDSS project, this will not be easy. These projects meet complex problems with complex responses, entailing the need to remain flexible and responsive to participant requirements and the need to modify the approach if new information and values arise."" They highlight communicative challenges especially in the framing of problems (exclusion of important perspectives, different views of the problem) and the conduction of the research (unwillingness to share personal perspectives, failure to recognize or articulate differences in individual assumptions, uncertainties and incomplete or incompatible knowledge, limited cognitive abilities to integrated individual partial knowledge).', ""'''Note:''' This entry focuses especially on Methods of Sustainability Science. For a more general conceptual view on Methods, please refer to the entry on the [[Design Criteria of Methods]].\n\n'''In short:''' In this entry, you will find out more about how we can distinguish and design our methodological approaches in Sustainability Science."", ""The interconnectedness of different forms of knowledge; reflexivity of researchers and other actors; the recognition of [[Agency, Complexity and Emergence|complexity]] in systems; and the necessity of power relations in knowledge production are prominent examples of current challenges that methodologies as of yet have not solved. We are probably still far away from the creation of a solidified canon of knowledge to solve these riddles. One could argue that we will never solve these methodological problems altogether, but that the joined action and interaction while working on these problems will be the actual solution, and that a real solution as with past methods of normal science shall never be reached. This is unclear to date, yet we need to become aware that when we question the ''status quo'' of knowledge production and methodologies, we may not be looking for goals, but more for a path. There are several examples in sustainability science that follow this path, such as [[Transdisciplinarity|transdisciplinarity]] and [[System"", 'Sustainability science shall thus realize that making cases comparable is the key to coherent knowledge, and ultimately a more systematic approach to new knowledge. Statistics will certainly not be the only contribution to this end, but it is quite certain that it will contribute to a holistic understanding.']"|0.3333333333|1.0
19|Why are critical theory and ethics important in modern science?|Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.|"[""'''Critique of the historical development and our status quo'''\nWe have to recognise that modern science is a [[Glossary|system]] that provides a singular and non-holistic worldview, and is widely built on oppression and inequalities. Consequently, the scientific system per se is flawed as its foundations are morally questionable, and often lack the necessary link between the empirical and the ethical consequences of science. Critical theory and ethics are hence the necessary precondition we need to engage with as researchers continuously.\n\n'''Interaction of scientific methods with philosophy and society'''\nScience is challenged: while our scientific knowledge is ever-increasing, this knowledge is often kept in silos, which neither interact with other silos nor with society at large. Scientific disciplines should not only orientate their wider focus and daily interaction more strongly towards society, but also need to reintegrate the humanities in order to enable researchers to consider the ethical conduct and consequences of their research."", 'as well as other current developments of philosophy can be seen as a thriving towards an integrated and holistic philosophy of science, which may ultimately link to an overaching theory of ethics (Parfit). If the empirical and the critical inform us, then both a philosophy of science and ethics may tell us how we may act based on our perceptions of reality.', 'critical theory], which had clearly ramifications towards the methodological canon, and the societal developments during this time. Despite all these ivory towers is science not independent from the Zeitgeist and the changes within society, and science is often both rooted and informed from the past and influence the presence, while also building futures. This is the great privilege society gained from science, that we can now create, embed and interact with a new knowledge production that is ever evolving.<br/>', 'Linking to the historical development of methods, we can thus clearly claim that Critical Theory (and Critical Realism) opened a new domain or mode of thinking, and its impact can be widely felt way beyond the social science and philosophy that it affected directly. However, coming back to bias, the answer to an almost universal rejection of empiricism will [[Big problems for later|not be followed here]]. Instead, we need to come back to the three foundational theories of philosophy, and need to acknowledge that reason, social contract and utilitarianism are the foundation of the first empirical disciplines that are at their core normative (e.g. psychology, social and political science, and economics). Since bias can be partly related to these three theories, and consequentially to specific empirical disciplines, we need to recognise that there is an overarching methodological bias. This methodological bias has a signature rooted in specific design criteria, which are in turn related to specific disciplines. Consequently, this methodological bias is a', 'Out of the growing empiricism of the enlightenment there grew a concern which we came to call Critical Theory. At the heart of critical theory is the focus on critiquing and changing society as a whole, in contrast to only observing or explaining it. Originating in Marx, Critical Theory consists of a clear distancing from previous theories in philosophy - or associated with the social - that try to understand or explain. By embedding society in its historical context (Horkheimer) and by focussing on a continuous and interchanging critique (Benjamin) Critical Theory is a first and bold step towards a more holistic perspective in science. Remembering the Greeks and also some Eastern thinkers, one could say it is the first step back to a holistic thinking. From a methodological perspective, Critical Theory is radical because it seeks to distinguish itself not only from previously existing philosophy, but more importantly from the widely dominating empiricism, and its societal as well as scientific consequences. A Critical Theory should thus be explanatory,', '== Critical Theory and Bias ==', 'will have to acknowledge that much of modern research to date will fall into the traditionally established inductive or deductive category. It is therefore not surprising that [[Different_paths_to_knowledge#Critical_Theory_.26_Bias|critical theory]] and other approaches cast doubt over these rigid criteria, and the knowledge they produce. Nevertheless, they represent an important categorisation for the current state of the art of many areas of science, and therefore deserve critical attention, least because many methods imply either one or the other approach.']"|0.9999999999|1.0
20|What is system thinking?|System thinking is a method of investigation that considers interactions and interdependencies within a system, which could be anything from a business to a population of wasps.|"['\'\'\'The system is at the basis of System Thinking.\'\'\' System Thinking is a form of scientific approach to organizing and understanding \'systems\' as well as solving questions related to them. It can be said that every creation of a (scientific) model of a system is an expression of System Thinking, but there is more to System Thinking than just building and working with system models (1). System Thinking revolves around the notion of \'holistic\' understanding, i.e. the idea that the components of a system can best be understood by also observing adjacent components and attempting to understand their connections. Among diverging definitions of the term, recurring characteristics of System Thinking are the acknowledgement of non-linear interrelationships between system elements, including [[Glossary|feedback loop]]s, that lead to dynamic behavior of the system and make it necessary to examine the whole system instead of only its parts (6). Further, System Thinking assumes that ""(...) all system dynamics are in principle non-linear"" and that ""(...)', ""'''In short:''' Based on the idea of System Thinking, this entry discusses how to properly draw boundaries in systems.\n\n'''[[System Thinking & Causal Loop Diagrams|System thinking]] has emerged during the last decades as one of the most important approaches in sustainability science.''' Originating in the initial system theory, several conceptual frameworks and data analysis methodologies have been developed that aim to generate a better understanding of the interactive dynamics within a typically spatially bound system. While much attention has been hence drawn to a better theory development and subsequent application within a defined setting, less attention has been aimed at a definition of system boundaries."", ""System Thinking can be applied in any discipline to understand underlying structures and develop models of the system that is analysed. However, sustainability science relies heavily on System Thinking since it acknowledges the complex and lagged interrelationships within and in between ecological, economic, social and further systems. A Key Publication in this field was 'Thinking in Systems' by '''Donella Meadows''', who had been co-authoring the landmark 'Limits to Growth' before. Sustainability Science, which attempts to bridge the world how it is with the world how it ought to be, relies on System Thinking to understand how the world is and in which '[[Glossary|leverage points]]' one needs to intervene to bring about change. System Thinking can also be applied outside of scientific research, e.g. to analyze company-internal processes, for marketing purposes etc. (see e.g. 3)\n\n\n== What the method does ==\nBefore explaining System Thinking, it should first be explained what is a 'system'."", ""== Strengths & Challenges ==\nSystem Thinking provides advantages over linear analyses that do not do complex problems justice. \n* System Thinking helps find the real roots of problems instead of applying 'end of pipe' solutions that only fix symptoms, not causes. It also helps not to disimprove things by fixing one element but worsening another one while doing so (1).\n* Human thinking often tends to fall into simple cause-and-effect patterns that make it easier to understand the world. Applying System Thinking and the creation of Causal Loop Diagrams may be seen as solutions to this tendency of the human mind to simplify complex relationships where several variables influence each other. System Thinking makes it easier to get a feeling for how things really interact, as opposed to a very simplified model that neglects too many interactions. However, one should keep in mind that any model is just a (more or less detailed) approximation of the real world, and sometimes, problems are not as complex as they seem."", ""'''Peter Checkland introduced the notion that there are two main types of System Thinking:''' hard and soft. Hard System Thinking (HST) includes the earlier forms of applied System Thinking that could be found in technology management or engineering. It assumes that the analyzed system is objectively real and in itself systemic, that it can be understood and modeled in a reductionist approach and intervened by an external observer to optimize a problematic situation. HST is defined by understanding the world as a system that has a clear structure, a single set of underlying values and norms and a specific goal (9). We could think of a machine as a 'system' in this sense."", 'Soft System Thinking (SST), by comparison, considers a \'system\' an ""(...) epistemological concept which is subjectively constructed by people rather the objective entities in the world"" (Zexian & Xuhui 2010, p.143). SST is defined by a systemic and iterative approach to understanding the world and acknowledges that social systems include diverse sets of worldviews and interests (9). In SST, the observer interacts with the system they observe and cannot optimize it, but improve it through active involvement. In this view, a social organisation could be a \'system\'.\n\n[[File:Causal Loop Diagram - Hard vs Soft.png|450px|thumb|right|\'\'\'Hard System Thinking and Soft System Thinking\'\'\' according to Checkland. Source: Checkland 2000, p.18]]\n\nSystem Thinking (especially HST) finds concrete applications in science through two concepts that it builds upon: \'\'System Analysis\'\' and \'\'System Dynamics\'\' (1).', ""Checkland, P. Systems Thinking. In: Curry, W.L. Galliers, B. 1999. ''Rethinking Managagement Information Systems.'' Oxford University Press. 45-56.\n* Representative for the various contributions Checkland made since the 1970s.\n\n\n== References ==\n\n(1) Haraldsson, H.V. 2004. ''Introduction to System Thinking and Causal Loop Diagrams.'' Reports in ecology and environmental engineering (1). KFS AB, Lund, Sweden.\n\n(2) Team TIP. 2011. ''Guidelines for drawing causal loop diagrams.'' Systems Thinker 22(1). 5-7.\n\n(3) Toole, M.T. 2005. ''A Project Management Causal Loop Diagram.'' ARCOM Conference, London, UK.""]"|0.8333333332916666|1.0
21|What is the main principle of the Feynman Method?|The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.|"[""== What, Why & When ==\n\n''Teaching is the best way to learn.''\n\nThe Feynman Method, named after famous physician Richard Feynman, is a learning technique that builds on the idea that explaining a topic to someone is the best way to learn it. This approach helps us better understand what we are learning, and not just memorize technical terms. This way, we can more easily transfer our new knowledge to unknown situations.\n\n== Goals ==\n* Obtain a profound understanding of any topic.\n* Learn more quickly and with more depth.\n* Become able to explain any given topic to anyone.\n\n== Getting Started ==\nThe Feynman method is a simple, circular process:"", ""# '''Have a look at your notes and try to find more information.''' Read scientific publications, Wikipedia entries or dedicated books; watch documentaries or YouTube videos - have a look at everything that may help you better understand the topic, and fill your knowledge gaps. Pay attention to the technical terms that you used, and find better ways to explain these things without relying on the terms.\n# '''Now explain the topic again.''' Possibly you can speak to the same person as previously. Did they understood you better now? Were you able to explain also those parts that you could not properly explain before? There will likely still be some gaps, or unclear spots in your explanation. This is why the Feynman method is circular: go back to the second step of taking notes, and repeat until you can confidently explain the topic to anyone, and they will understand it. Now you have also understood it. Congratulations!"", 'Im Spannungsfeld zwischen neuen Paradigmen und der bestehenden Methodik kann eine Anpassung bestehender Methoden oder eine Entwicklung gänzlich neuer Methoden erforderlich sein. Während z.B. in den 1720er Jahren im Anschluss an Locke und Bacon die Herangehensweise an die Newtonsche Theorie weitgehend induktiv war, wurden in den folgenden Jahrzehnten Wärme, Elektrizität und Phänomene der Chemie proklamiert, von denen kaum behauptet werden kann, dass sie induktiv abgeleitet sind, da sie als solche nicht beobachtbar sind. In der Folge wurde eine Methodik abgeleitet, die die hypothesenbildende Deduktion ermöglichte, die noch eine ganze Weile vorherrschen und sogar dominieren sollte. Lakatos bot eine Modifikation', 'The Franciscan friar [https://en.wikipedia.org/wiki/William_of_Ockham William of Occam] almost single-handedly came up with one of the most fundamental principles to date in science. He basically concluded, that ""everything should be as simple as possible, but as complex as necessary."" Being a principle, it is suggested that this thought extends to all. While in his time it was rooted in philosophy or more specifically in logic,  [https://science.howstuffworks.com/innovation/scientific-experiments/occams-razor.htm Occam\'s razor] turned out to propel many scientific fields later on, such as physics, biology, theology, mathematics and many more. It is remarkable how this principle purely rooted in theoretical consideration generated the foundation for the scientific method, which would surface centuries later out of it. It also poses as one of the main building blocks of modern statistics as William of Occam came up with the principle of parsimony.', ""[https://www.feynmanlectures.caltech.edu/ The Feynman lectures] provide the most famous textbook volumes on physics. Richard Feynman and his coauthors compiled these books in the 1960, yet to this day, many consider these volumes to be the most concise and understandable introduction to physics. Granted, Feynman's work is brilliant. '''At the same time, however, this fact also means that in the 1960s the majority of the knowledge necessary for an introduction in physics was already available.''' While we may consider that much happened ever since, students can still use these textbooks today. Something similar can be claimed for basic statistics, although it should be noted that physics is a scientific discipline, while statistics is a scientific method. Some disciplines use different methods, and many disciplines use statistics as a method. However, the final word on statistics has not been said, as the differences between [[A matter of probability|probability]] statistics and"", '== Links & Further Reading ==\n* [https://karrierebibel.de/feynman-methode/ Karrierebibel]\n* [https://blog.doist.com/feynman-technique/ ToDo-ist]\n* [https://www.goodwall.io/blog/feynman-technique/ Goodwall]\n* [https://www.youtube.com/watch?v=_f-qkGJBPts Thomas Frank - How to learn with the Feynman Technique] \n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Productivity Tools]]\n[[Category:Personal Skills]]\n[[Category:Team Size 1]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.', ""The general principle of the ANOVA is rooted in [[Experiments and Hypothesis Testing|hypothesis testing]]. An idealized null hypothesis is formulated against which the data is being tested. If the ANOVA gives a significant result, then the null hypothesis is rejected, hence it is statistically unlikely that the data confirms the null hypothesis. As one gets an overall p-value, it can be thus confirmed whether the different groups differ overall. Furthermore, the ANOVA allows for a measure beyond the p-value through the '''sum of squares calculations''' which derive how much is explained by the data, and how large in relation the residual or unexplained information is.""]"|0.9999999999|1.0
22|What is the difference between fixed and random factors in ANOVA designs?|Fixed effects are the focus of the study, while random effects are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.|"['Within [[ANOVA]] designs, the question whether a variable is a [https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html fixed or a random] factor is often difficult to consider. Generally, fixed effects are about what we want to find out, while random effects are about aspects which variance we explicitly want to ignore, or better, get rid of. However, it is our choice and part of our design whether a factor is random or fixed. Within most medical trials the information whether someone smokes or not is a random factor, since it is known that smoking influences much of what these studies might be focusing about. This is of course different if these studies focus explicitly on the effects of smoking. Then smoking would be a fixed factor, and the fact whether someone smokes or not is part of the research. Typically, factors that are part of a block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To', ""==== Random factors ====\nThis brings us to the next factor that can affect our model validity. There are many things we might want to know when creating a statistical model, but there may also be things that we do not want to know. Statistically speaking, these are ''random factors'' or ''random variables''. Regarding these, we explicitly exclude the variance that is created by these parts of the dataset, because we want to minimise their effects. An example would be a medical study that wants to investigate the effect of a certain drug on the recovery rate of some diseased patients. In such studies, the information whether a patient is a smoker or not is often included as a random factor, because smoking negatively affects many diseases and the respective recovery rates. We know that smoking makes many things worse (from a medical standpoint) and this is why such variables are excluded. More on random factors can be found [[ Field_experiments#Fixed_effects_vs._Random_effects | here]]."", 'When designing an ANOVA study, great care needs to be taken to have sufficient samples to allow for a critical interpretation of the results. Subsequently, ANOVA experiments became more complex, combining several independent variables and also allowing to correct for so called random factors, which are elements for which the variance is calculated out of the ANOVA model. This allows for instance to increase the sample size to minimise the effects of the variance in an agricultural experiment which is being conducted on several agricultural fields. In this example, agricultural fields are then included as block factor, which allows to minimise the variance inferred by these replications. Hence, the variance of the agricultural fields is tamed by a higher number of replicates. This led to the ANOVA becoming one of the most relevant methods in statistics, yet recent developments such as the reproducibility crisis in psychology highlight that care needs to be taken to not overplay ones hand. Preregistering hypotheses and more recognition of the', 'block design are random factors, and variables that are constructs relating to our hypothesis are fixed variables. To this end, it is helpful to consult existing studies to differentiate between [https://www.youtube.com/watch?v=Vb0GvznHf8U random and fixed factors]. Current medical trials may consider many variables, and have to take even more random factors into account. Testing the impact of random factors on the raw data is often a first step when looking at initial data, yet this does not help if it is a purely deductive design. In this case, simplified pre-tests are often a first step to make initial attempts to understand the system and also check whether variables - both fixed or random - are feasible and can be utilised in the respective design. Initial pre-tests at such smaller scales are a typical approach in medical research, yet other branches of research reject them as being too unsystematic. Fisher himself championed small sample designs, and we would encourage pre-tests in field experiments', '[https://www.youtube.com/watch?v=Vb0GvznHf8U Fixed vs. Random Effects]: A differentiation\n----\n[[Category:Statistics]]\n[[Category:R examples]]\n\nThe [[Table of Contributors|author]] of this entry is Henrik von Wehrden.', ""[https://web.ma.utexas.edu/users/mks/statmistakes/fixedvsrandom.html Random vs. Fixed Factors]: A differentiation\n\n[https://en.wikipedia.org/wiki/Ronald_Fisher#Rothamsted_Experimental_Station,_1919%E2%80%931933 Field Experiments in Agriculture]: Ronald Fisher's experiment\n\n[https://www.simplypsychology.org/milgram.html Field Experiments in Psychology]: A famous example\n\n[https://www.nature.com/articles/s41599-019-0372-0 Field Experiments in Economics]: An example paper\n\n[https://revisesociology.com/2016/01/17/field-experiments-sociology/ Field Experiments in Sociology]: Some examples\n\n===Videos===\n\n[https://www.youtube.com/watch?v=10ikXret7Lk Types of Experimental Designs]: An introduction"", 'In addition, the original ANOVA builds on balanced designs, which means that all categories are represented by an equal sample size. Extensions have been developed later on in this regard, with the type 3 ANOVA allowing for the testing of unbalanced designs, where sample sizes differ between different categories levels. The Analysis of Variance is implemented into all standard statistical software, such as R and SPSS. However, differences in the calculation may occur when it comes to the calculation of unbalanced designs.']"|0.699999999965|1.0
23|What is the replication crisis and how does it affect modern research?|The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.|"['resistance, demanding new and updated versions to keep track with the bacteria that are likewise constantly evolving. This showcases that while the field experiment led to many positive developments, it also created ripples that are hard to anticipate. There is an overall crisis in complex experiments that is called replication crisis. What is basically meant by that is that some possibilities to replicate the results of studies -often also of landmark papers- failed spectacularly. In other words, a substantial proportion of modern research cannot be reproduced. This crisis affects many arenas in sciences, among them psychology, medicine, and economics. While much can be said about the roots and reasons of this crisis, let us look at it from a statistical standpoint. First of all, at a threshold of p=0.05, a certain arbitrary amount of models can be expected to be significant purely by chance. Second, studies are often flawed through the connection between theory and methodological design, where for instance much of the dull results remains unpublished, and statistical designs can', 'a deeper understanding of the specific context and case. While real world experiments emerged some decades ago already, they are only starting to gain wider recognition. All the while, the reproducibility crisis challenges the classical laboratory and field experiments, as a wider recognition that many results - for instance from psychological studies - cannot be reproduced. All this indicates that while much of our scientific knowledge is derived from experiments, much remains to be known, also about the conduct of experiments themselves.', 'The Analysis of Variance was one of the most relevant contributions of statistics to the developments of the 20th century. By allowing for the systematic testing of hypotheses, not only did a whole line of thinking of the theory of science evolve, but whole disciplines were literally emerging. Lately, frequentist statistics was increasingly critizised for its reliance on p-values. Also, the reproducibility crisis highlights the limitations of ANOVA-based designs, which are often not reproducible. Psychological research faces this challenge for instance by pre-registering studies, indicating their statistical approach before approaching the data, and other branches of science are also attempting to do more justice to the limitations of the knowledge of experiments. In addition, new ways of experimentation of science evolve, introducing a systematic approach to case studies and solution oriented approaches. This may open a more systematic approach to inductive experiments, making documentation a key process in the creation of a canonised knowledge. Scientific experiments were at the forefront of developments that are', 'context, escalating to wrong analysis and even bad science. Establishing standards for open source is a continuous challenge, yet there are good examples out there to highlight that we are on a good way. We need to find reporting schemes that enable reproducibility, which might then enable us detect bias in the future, when more data, better analysis or new concepts might have become available, thereby evolving past results further. Reporting all necessary information on the result of a software or a statistical test or model is one essential step towards this goal. How good is a model fit? How significant is the reuse? How complex is the model? All these values are inevitable to put the results into a broader context of the statistics canon.', 'Yet there are also direct limitations or reasons for concern within science itself. Regressions were initially derived to test for dependence between two continuous variables. In other words, a regression is at its heart a mostly deductive approach. This has been slowly eroded over the last decades, and these days many people conduct regression analysis that do not test for statistical fishing, and end up being just that. If we test long enough for significant relations, there will eventually be one, somewhere in the data. It is however tricky to draw a line, and many of the problems of the reproducibility crisis and other shortcomings of modern sincere can be associated to the blur if not wrong usage of statistical models.', 'There are many facets that could be highlighted under such a provocative heading. Since Larry Laudan, it has become clear that the assumption that developments of science which are initiated by scientists are reasonable, is a myth at best. Take the example of one dominating [[Glossary|paradigm]] in science right now: Publish or perish. This paradigm highlights the current culture dominating most parts of science. If you do not produce a large amount of peer-reviewed publications, your career will not continue. This created quite a demand on statistics as well, and the urge to arrive at significant results that probably created frequent violations of Occam\'s razor (that is, things should be as complex as necessary, but as simple as possible). The reproducibility crisis in psychology is one example of these developments, yet all other disciplines building on statistics struggle to this end, too. Another problem is the fact that with this [[Questioning the status quo in methods|ever-increasing demand for ""scientific', ""there are no replicates of the Easter Islands. This is at a first glance a very specific and singular problem, yet it is often considered to be an important example on how unsustainable behaviour led to a collapse of a while civilisation. Such settings are referred to as [https://www.britannica.com/science/natural-experiment Natural Experiments]. From a certain perspective, our whole planet is a Natural Experiment, and it is also from a statistical perspective a problem that we do not have any replicates, besides other ramifications and unclarity that derives such single case studies, which are however often increasingly relevant on a smaller scale as well. With a rise in qualitative methods both in diversity and abundance, and an urge for understanding even complex systems and cases, there is clearly a demand for the integration of knowledge from Natural Experiments. '''From a statistical point of view, such cases are difficult and challenging due to a lack of being reproducible, yet the knowledge can still be relevant,""]"|0.91666666664375|1.0
24|What is the purpose and process of the flashlight method in group discussions?|The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.|"[""== What, Why & When ==\nThe flashlight method is used during sessions to get an immediate picture and evaluation of where the group members stand in relation to a specific question, the general course of discussion, or how they personally feel at that moment. \n\n== Goals ==\nHave a quick (and maybe fun) interlude to identify:\n<br> ''Is everyone on the same page?''\n<br> ''Are there important issues that have been neglected so far?''\n<br> ''Is there unspoken dissonance?''\n<br> ''Is there an elephant in the room?''\n<br> ''What are we actually talking about?''"", ""===== ''Please note further'' =====\n* The flashlight can be used as a starting round or energizer in between.\n* The team leader should be aware of good timing, usefulness at this point, and the setting for the flashlight. \n* The method is quick and efficient, and allows every participant to voice their own point without interruption. This especially benefits the usually quiet and shy voices to be heard. On the downside, especially the quiet and shy participants can feel uncomfortable being forced to talk to the whole group. Knowing the group dynamics is key to having successful flashlight rounds in small and big groups.  \n* The request to keep ones own statement short and concise may distract people from listening carefully, because everyone is crafting their statements in their heads instead. To avoid that distraction, start by giving the question and let everyone think for 1-2 minutes."", ""* Flashlights in groups with 30+ participants can work well, however, the rounds get very long, and depending on the flashed topic can also get inefficient. Breaking into smaller groups with a subsequent sysnthesis should be considered.\n* To create a relaxed atmosphere try creative questions like: <br> ''What song would you choose to characterize the current state of discussion, and why?'' <br> ..."", '== How to ==\n==== ...do a basic flashlight ====\n* Flashlight rounds can be initiated by the team leader or a team member. \n* Everyone is asked to share their opinion in a short 2-3 sentence statement. \n* During the flashlight round everyone is listening and only questions for clarification are allowed. Arising issues can be discussed after the flashlight round ended.', ""== Rules ==\n* no commenting on each other's statements → comparable to the flashlight method\n* active [[Listening|listening]]\n* you can go clockwise and if somebody does not want to say something, it's also fine\n* check-ins and check-outs are a routine, not a one time thing - try to establish it as the normal process of starting and finishing a meeting\n* this time belongs to personal insights and is not meant to complete tasks or make arrangements\n\n== Potential Pitfalls ==\nIt may happen that - especially in meetings in which the members do not know each other well - the members might not want to share their very personal stories or want to be honest about their current feelings. It can help that you as a moderator start the round and share a private insight if you like. Additionally, you should mention that the members should only share what they feel comfortable with."", ""==== Use Break-Out-Sessions ====\nBreak-Out-Sessions are a great way to dissolve your group into smaller discussion groups, work out a specific question or problem and then come together to discuss in the larger group. Unfortunately, this is, as far as we know, restricted to the software [https://zoom.us/ zoom] at the moment.\n\nHere's some things we find important for using them:\n\n📣 Communicate clearly a) how much time is to be assigned for the break-out-sessions and b) what the result of the break-out should be and how it should be presented.\n\n⏲ Give participants time to organize themselves. They might see each other for the first time.\n\n😈 Suggest roles for the sessions and encourage participants to distribute those among themselves. (''Yes, that's meant to be a [https://en.wikipedia.org/wiki/Devil%27s_advocate Devil's Advocate]'')"", 'All of this can of course also be done on paper or in a notebook - it might just get messy if you\'re not very disciplined.\n\n=== Further remarks ===\n==== Notes on group work ====\nGroup work is great, but can also be largely ineffective. If you gather 5 people who know next to nothing about a topic, there is very little chance you\'ll create insight from nothing just by being together. Try to have a clear idea on what you want out of your group work and assign or rotate roles accordingly. One idea is to, each week, have one person in the group explain a recent topic to everyone else (see also ""Elaboration"" and ""Active Recall""). You can also have sessions within which one person tries to actively recall and elaborate on a topic, and the other(s) ask questions regarding gaps they might be seeing or remark inaccuracies. This combines active recall, elaboration and having an incredible amount of fun together! 🥳']"|0.7499999999625|1.0
25|What types of data can Generalized Linear Models handle and calculate?|Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions.|"[""== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician."", ""unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."", '|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.', ""== Strengths & Challenges ==\n'''Generalized Linear Models allow powerful calculations in a messy and thus often not normally distributed world.''' Many datasets violate the assumption of the normal distribution, and this is where GLMs take over and clearly allow for an analysis of datasets that are often closer to dynamics found in the real world, particularly [[Experiments_and_Hypothesis_Testing#The_history_of_the_field_experiment | outside of designed studies]]. GLMs thus represented a breakthrough in the analysis of datasets that are skewed or imperfect, may it be because of the nature of the data itself, or because of imperfections and flaws in data sampling."", ""'''How do I know?'''\n* Try to understand the data type of your dependent variable and what it is measuring. For example, if your data is the answer to a yes/no (1/0) question, you should apply a GLM with a Binomial distribution. If it is count data (1, 2, 3, 4...), use a Poisson Distribution.\n* Check the entry on [[Data_distribution#Non-normal_distributions|Non-normal distributions]] to learn more.\n\n\n==== Generalised Linear Models ====\n'''You have arrived at a Generalised Linear Model (GLM).''' GLMs are a family of models that are a generalization of ordinary linear regressions. The key R command is <code>glm()</code>.\n\nDepending on the existence of random variables, there is a distinction between Mixed Effect Models, and Generalised Linear Models based on regressions."", '[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]', 'Over the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer [[Causality|causality]] when explaining the']"|0.6999999999766667|1.0
26|What is a heatmap and why is it useful?|A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.|"[""'''Note:''' This entry revolves specifically around Heatmaps. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA heatmap is a graphical representation of data where the individual numerical values are substituted with colored cells. In other words, it is a table with colors in place of numbers. As in the regular table, in the heatmap, each column is a feature and each row is an observation.\n\n==Why use a heatmap?==\nHeatmaps are useful to get an overall understanding of the data. While it can be hard to look at the table of numbers it is much easier to perceive the colors. Thus it can be easily seen which value is larger or smaller in comparison to others and how they are generally distributed."", 'It also means that if our data are not normalized, we can compare each value with any other by color across the whole heatmap. However, if the data are normalized, then the color is assigned based on the relative values in the row or column, and therefore each value can be compared with others only in their corresponding row or column, while the same color in a different row/column will not have the same value behind it or belong to the same bin.', '==R Code==\nTo build the heatmap we will use the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function and \'\'\'mtcars\'\'\' dataset.\nIt is important to note that the <syntaxhighlight lang=""R"" inline>heatmap()</syntaxhighlight> function only takes a numeric matrix of the values as data for plotting. Therefore we need to check if our dataset only includes numbers and then transform our dataset into a matrix, using <syntaxhighlight lang=""R"" inline>as.matrix()</syntaxhighlight> function.\n<syntaxhighlight lang=""R"" line>\ndata(""mtcars"")\nmatcars <- as.matrix(mtcars)\n</syntaxhighlight>\nAlso, for better representation, we are going to rename the columns, giving them their full names. It is not a mandatory step, but it makes our heatmap more comprehensible.', '<syntaxhighlight lang=""R"" line>\nfullcolnames <- c(""Miles per Gallon"", ""Number of Cylinders"",\n                  ""Displacement"", ""Horsepower"", ""Rear Axle Ratio"",\n                  ""Weight"", ""1/4 Mile Time"", ""Engine"", ""Transmission"",\n                  ""Number of Gears"", ""Number of Carburetors"")\n</syntaxhighlight>\n\nNow we are using the transformed dataset (matcars) to create the heatmap. Other used arguments are explained below.\n[[File:Heatmap.png|350px|thumb|right|Fig.1]]\n<syntaxhighlight lang=""R"" line>\n#Fig.1\nheatmap(matcars, Colv = NA, Rowv = NA, \n        scale = ""column"", labCol = fullcolnames, \n        margins = c(11,5))\n</syntaxhighlight>\n\n== How to interpret a heatmap? ==', '==Color assignment and normalization of data==\nThe principle by which the colors in a heatmap are assigned is that all the values of the table are ranked from the highest to lowest and then segregated into bins. Each bin is then assigned a particular color. However, in the case of the small datasets, colors might be assigned based on the values themselves and not on the bins. Usually, for higher value, the color is more intense or darker, and for the smaller is paler or lighter, depending on which color palette is chosen.\n\nIt is important to remember that since each feature in a dataset does not always have the same scale of measurement, usually the normalization (scaling) of data is required. The goal of normalization is to change the values of numeric rows and/or columns in the dataset to a common scale, without distorting differences in the ranges of values.', '==Explanation of used arguments==\n* <syntaxhighlight lang=""R"" inline>Colv = NA</syntaxhighlight> and <syntaxhighlight lang=""R"" inline>Rowv = NA</syntaxhighlight> are used to remove the dendrograms from rows and columns. A dendrogram is a diagram that shows the hierarchical relationship between objects and is added on top of the heatmap by default if the argument is not specified. The main reason for removing it here is that it is a different method of data visualisation which is not mandatory for the heatmap representation and requires a separate article to review it fully.', '== How to interpret a heatmap? ==\n\nIn the default color palette the interpretation is usually the following: the darker the color the higher the responding value, and vice versa. For example, let’s look at the feature <syntaxhighlight lang=""R"" inline>“Number of Carburetors”</syntaxhighlight>. We can see that \'\'\'Maserati Bora\'\'\' has the darkest color, hence it has the largest number of carburetors, followed by \'\'\'Ferrari Dino\'\'\', which has the second-largest number of carburetors. While other models such as \'\'\'Fiat X1-9\'\'\' or \'\'\'Toyota\'\'\' have the lightest colors. It means that they have the lowest numbers of carburetors. This interpretation can be applied to every other column.']"|0.699999999965|1.0
27|How did Alhazen contribute to the development of scientific methods?|Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.|"[""expert on one matter or the other. Of course Alhazen stands here only as one of many that formed the rise of science on '''the Islamic world during medieval times, which can be seen as a cradle of Western science, and also as a continuity from the antics, when in Europe much of the immediate Greek and Roman heritage was lost.'''"", 'Many concrete steps brought us closer to the concrete application of scientific methods, among them - notably - the approach of controlled testing by the Arabian mathematician and astronomer [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]. Emerging from the mathematics of antiquity and combining this with the generally rising investigation of physics, Alhazen was the first to manipulate [[Field_experiments|experimental conditions]] in a systematic sense, thereby paving the way towards the scientific method, which would emerge centuries later. Alhazen is also relevant because he could be considered a polymath, highlighting the rise of more knowledge that actually enabled such characters, but still being too far away from the true formation of the diverse canon of [[Design Criteria of Methods|scientific disciplines]], which would probably have welcomed him as an expert on one matter or the other. Of course Alhazen stands here only as one of many', 'Viele konkrete Schritte brachten uns der konkreten Anwendung wissenschaftlicher Methoden näher, darunter - insbesondere - der Ansatz des kontrollierten Testens durch den arabischen Mathematiker und Astronomen [https://www.britannica.com/biography/Ibn-al-Haytham Alhazen (a.k.a. Ibn al-Haytham]). Aus der Mathematik der Antike hervorgegangen und diese mit der allgemein aufkommenden Erforschung der Physik verbindend, war Alhazen der erste, der [[Field experiments|Versuchsbedingungen]] in einem systematischen Sinne manipulierte und damit den Weg zu der wissenschaftlichen Methode ebnete, die Jahrhunderte später aufkommen sollte. Alhazen ist auch deshalb relevant, weil er als Universalgelehrter betrachtet werden kann, was den', ""It was [https://plato.stanford.edu/entries/francis-bacon/ Francis Bacon] who ultimately moved out of this dominance of thinking and called for the importance of empirical inquiry. '''By observing nature and its phenomena, we are able to derive conclusion based on the particular.''' Bacon is thus often coined to be the father of 'the scientific method', an altogether misleading term, as there are many scientific methods. However, empiricism, meaning the empirical investigation of phenomena and the derivation of results or even rules triggered a scientific revolution, and also a specialization of different scientific branches. While Leonardo da Vinci (1452-1519) had been a polymath less than a century ago, Bacon (1561-1626) triggered an increase in empirical inquiry that led to the deeper formation of scientific disciplines. Many others contributed to this development: [https://plato.stanford.edu/entries/locke/ Locke] claimed that human knowledge builds on experience, and"", 'Hence these systematic experimental approaches aided many fields such as botany, chemistry, zoology, physics and [http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf much more], but what was even more important, these fields created a body of knowledge that kickstarted many fields of research, and even solidified others. The value of systematic experiments, and consequently systematic knowledge created a direct link to practical application of that knowledge. [https://www.youtube.com/watch?v=UdQreBq6MOY The scientific method] -called with the ignorant recognition of no other methods beside systematic experimental hypothesis testing as well as standardisation in engineering- hence became the motor of both the late enlightenment as well as the industrialisation, proving a crucial link between basically enlightenment and modernity.', 'Hence these systematic experimental approaches aided many fields such as botany, chemistry, zoology, physics and [http://www.academia.dk/Blog/wp-content/uploads/KlinLab-Hist/LabHistory1.pdf much more], but what was even more important, these fields created a body of knowledge that kickstarted many fields of research, and even solidified others. The value of systematic experiments, and consequently systematic knowledge created a direct link to practical application of that knowledge. [https://www.youtube.com/watch?v=UdQreBq6MOY The scientific method] -called with the ignorant recognition of no other methods beside systematic experimental hypothesis testing as well as standardisation in engineering- hence became the motor of both the late enlightenment as well as the industrialisation, proving a crucial link between basically enlightenment and modernity.', 'Equal considerations can be made for the early methodological approaches that rose to prominence in the antique, the most notable among them being astrological observations, mathematics, physics, or early medicine. It was often within urban cultures that such early examples of scientific methods flourished, and are still known today. A noteworthy example are the precise astronomical calculations of the Maya, which were often closely linked with their daily conduct, since the movement of the planets played a vital role in their religion. Their writing system shows similarity to the Japanese writing and many other approaches that provide a link to a documentation through paintings and thus deserve a deep interpretation well beyond the content of the symbols itselves. This documentation hence now only shows us how scientific methods emerged, but also allow us to investigate the past with our current methods. To this end, paintings are a form of art that deserves a closer analytical look, and many diverse approaches exist that allow for a close examination of the preserved record. It was the financial surplus in urban settlements']"|0.49999999995|0.5
28|How can multivariate data be graphically represented?|Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.|"[""Multivariate data can be principally shown by three ways of graphical representation: '''ordination plots''', '''cluster diagrams''' or '''network plots'''. Ordination plots may encapsulate such diverse approaches as decorana plots, principal component analysis plots, or results from a non-metric dimensional scaling. Typically, the first two most important axis are shown, and additional information can be added post hoc. While these plots show continuous patterns, cluster dendrogramms show the grouping of data. These plots are often helpful to show hierarchical structures in data. Network plots show diverse interactions between different parts of the data. While these can have underlying statistical analysis embedded, such network plots are often more graphical representations than statistical tests."", '* brand -- Pizza brand (class label)\n* id -- Sample analysed\n* mois -- Amount of water per 100 grams in the sample\n* prot -- Amount of protein per 100 grams in the sample\n* fat -- Amount of fat per 100 grams in the sample\n* ash -- Amount of ash per 100 grams in the sample\n* sodium -- Amount of sodium per 100 grams in the sample\n* carb -- Amount of carbohydrates per 100 grams in the sample\n* cal -- Amount of calories per 100 grams in the sample\n\nHow can you represent this data as concise and understandable as possible? It is impossible to plot all variables as is onto a flat screen/paper. Furthermore, high-dimensional data suffers from what is called the curse of dimensionality.', ""'''Note:''' This entry revolves specifically around Bubble plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n'''In short:''' \nA Bubble plot is a graphical representation of multivariate data table. One can think of it as an XY scatter plot with two additional variables. X and Y variables are numeric, and two additional variables, either continuous or categorical, can be represented by the bubble colour and bubble size. \n\n==Overview==\nThis wiki entry will elaborate what a bubble plot is, how to implement such a plot and how to customize your own bubble plot."", ""Simple '''pie charts''' are not really ideal, as they camouflage the real proportions of the data they show. '''Venn diagrams''' are a simple way to compare 2-4 groups and their overlaps, allowing for multiple hits. Larger co-connections can either be represented by a '''bipartite plot''', if the levels are within two groups, or, if multiple interconnections exist, then a '''structural equation model''' representation is valuable for more deductive approaches, while rather inductive approaches can be shown by '''circular network plots''' (aka [[Chord Diagram]]).\n[[File:Introduction to Statistical Figures - Venn Diagram example.png|200px|thumb|left|'''A Venn Diagram showing the number of articles in a systematic review that revolve around one or more of three topics.''' Source: Partelow et al. 2018. A Sustainability Agenda for Tropical Marine Science.]]"", '\'\'\'Of course, there is more.\'\'\' While the figures introduced above represent a vast share of the visual representations of data that you will encounter, there are different forms that have not yet been touched. \'\'\'We have found the website [https://www.data-to-viz.com/#connectedscatter ""From data to Viz""] to be extremely helpful when choosing appropriate data visualisation.\'\'\' You can select the type of data you have (numeric, categoric, or both), and click through the exemplified figures. There is also R code examples.', '<imagemap>Image:Statistical Figures Overview 27.05.png|1050px|frameless|center|\ncircle 120 201 61 [[Big problems for later|Factor analysis]]\ncircle 312 201 61 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA, TREE SPECIES IN AMERICA as three colors, with joint species in the overlaps]]\ncircle 516 190 67 [[Venn Diagram|Venn Diagram, e.g. variables TREE SPECIES IN EUROPE, TREE SPECIES IN ASIA as two colors, with joint species in the overlaps]]\ncircle 718 178 67 [[Stacked Barplots|Stacked Barplot, e.g. count data of different species (colors) for the variable TREES]]', ""'''In short:''' This entry introduces you to the most relevant forms of [[Glossary|data]] visualisation, and links to dedicated entries on specific visualisation forms with R examples.\n\n== Basic forms of data visualisation ==\n__TOC__\nThe easiest way to represent count information are basically '''barplots'''. They are a bit over simplistic if they contain only one level of information such as three groups and their abundance, and can be more advanced if they contain two levels of information such as in stacked barplots. These can be shown as either absolute numbers or proportions, which may make a dramatic difference for the analysis or interpretation.""]"|0.7708333333140625|1.0
29|What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?|Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.|"['The trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].', ""__NOTOC__\n\n'''In short:''' Machine Learning subsumes methods in which computers are trained to learn rules in order to autonomously analyze data.\n\n== Background ==\n[[File:Machine learning.png|thumb|400px|right|'''SCOPUS hits per year for Machine Learning until 2019.''' Search term: 'Machine Learning' in Title, Abstract, Keywords. Source: own.]]\nTraditionally in the fields of computer science and mathematics, you have some input and some rule in form of a function. You feed the input into the rule and get some output. In this setting, you '''need''' to know the rule exactly in order to create a system that gives you the outputs that you need. This works quite well in many situations where the nature of inputs and/or the nature of the outputs is well understood."", '== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.', '== Normativity ==\nMachine Learning gets criticized for not being as thorough as traditional statistical methods are. However, in their essence, Machine Learning techniques are not that different from statistical methods as both of them are based on rigorous mathematics and computer science. The main difference between the two fields is the fact that most of statistics is based on careful experimental design (including hypothesis setting and testing), the field of Machine Learning does not emphasize this as much as the focus is placed more on modeling the algorithm (find a function <syntaxhighlight lang=""text"" inline>f(x)</syntaxhighlight> that predicts <syntaxhighlight lang=""text"" inline>y</syntaxhighlight>) rather than on the experimental settings and assumptions about data to fit theories [4].', 'However, in situations where the inputs can be \'\'noisy\'\' or the outputs are expected to be different in each case, you cannot hand-craft the ""[[Glossary|rules]]"" that account for \'\'every\'\' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: \'\'\'Machine Learning\'\'\'. The core idea behind Machine Learning is that instead of being required to hand-craft \'\'all\'\' the rules that take inputs and provide outputs in a fairly accurate manner, you can \'\'train\'\' the machine to \'\'learn\'\' the rules based on the inputs and outputs that you provide.', ""What did however improve rapidly over the last decades are [[Machine Learning]] approaches that utilize diverse algorithmic approaches to maximize model fits and thus take a clear aim at predictive modelling. This whole hemisphere of science is partly strongly interacting and building on data originating in the Internet, and many diverse streams of data demand powerful modelling approaches that build on Machine Learning. 'Machine Learning' itself is a composite term that encapsulates a broad and diverse array of approaches, some of which are established since decades ([[Clustering Methods|Clustering]], [[Ordinations]], [[Regression Analysis]]), while other methods are at the frontier of the current development, such as artificial neural networks or decisions trees. Most approaches were already postulated decades ago, but some only gained momentum with the [[History of Methods|computer revolution]]. Since many approaches demand high computational capacity there is a clear rise in the last years, along with the rise of larger computer capacities. [[Bayesian Inference|Bayesian]] approaches are the best example of"", ""However, there is nothing stopping researchers from embedding Machine Learning techniques to their research design. In fact, a lot of methods that are categorized under the umbrella of the term Machine Learning have been used by statisticians and other researchers for a long time; for instance, ''k-means clustering'', ''hierarchical clustering'', various approaches to performing ''regression'', ''principle component analysis'' etc.\n\nBesides this, scientists also question how Machine Learning methods, which are getting more powerful in their predictive abilities, will be governed and used for the benefit (or harm) of the society. Jordan and Mitchell (2015) highlight that the society lacks a set of data privacy related rules that prevent nefarious actors from potentially using the data that exists publicly or that they own can be used with powerful Machine Learning methods for their personal gain at the other' expense [7].""]"|0.5888888888692593|1.0
30|What are some of the challenges faced by machine learning techniques?|Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.|"['== Strengths & Challenges ==\n* Machine Learning techniques perform very well - sometimes better than humans- on variety of tasks (eg. detecting cancer from x-ray images, playing chess, art authentication, etc.)\n* In a variety of situations where outcomes are noisy, Machine Learning models perform better than rule-based models.\n* Even though many methods in the field of Machine Learning have been researched quite extensively, the techniques still suffer from a lack of interpretability and explainability.\n* Reproducibility crisis is a big problem in the field of Machine Learning research as highlighted in [6].\n* Machine Learning approaches have been criticized as being a ""brute force"" approach of solving tasks.\n* Machine Learning techniques only perform well when the dataset size is large. With large data sets, training a ML model takes a large computational resources that can be costly in terms of time and money.', 'The trained models have their foundations in the fields of mathematics and computer science. As computers of various types (from servers, desktops and laptops to smartphones, and sensors integrated in microwaves, robots and even washing machines) have become ubiquitous over the past two decades, the amount of data that could be used to train Machine Learning models have become more accessible and readily available. The advances in the field of computer science have made working with large volumes of data very efficient. As the computational resources have increased exponentially over the past decades, we are now able to train more complex models that are able to perform specific tasks with astonishing results; so much so that it almost seems magical.\n\nThis article presents the different types of Machine Learning tasks and the different Machine Learning approaches in brief. If you are interested in learning more about Machine Learning, you are directed to Russel and Norvig [2] or Mitchell [3].', 'However, in situations where the inputs can be \'\'noisy\'\' or the outputs are expected to be different in each case, you cannot hand-craft the ""[[Glossary|rules]]"" that account for \'\'every\'\' type of inputs or for every type of output imaginable. In such a scenario, another powerful approach can be applied: \'\'\'Machine Learning\'\'\'. The core idea behind Machine Learning is that instead of being required to hand-craft \'\'all\'\' the rules that take inputs and provide outputs in a fairly accurate manner, you can \'\'train\'\' the machine to \'\'learn\'\' the rules based on the inputs and outputs that you provide.', '====Challenges of statistics====', '(6) Beam, A. L., Manrai, A. K., & Ghassemi, M. (2020). Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA, 323(4), 305–306.\n\n(7) Jordan, M. I., & Mitchell, T. M. (2015). Machine Learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260.\n\n(8) Dwork, C. (2006). Differential privacy. In ICALP’06 Proceedings of the 33rd international conference on Automata, Languages and Programming - Volume Part II (pp. 1–12).', ""What did however improve rapidly over the last decades are [[Machine Learning]] approaches that utilize diverse algorithmic approaches to maximize model fits and thus take a clear aim at predictive modelling. This whole hemisphere of science is partly strongly interacting and building on data originating in the Internet, and many diverse streams of data demand powerful modelling approaches that build on Machine Learning. 'Machine Learning' itself is a composite term that encapsulates a broad and diverse array of approaches, some of which are established since decades ([[Clustering Methods|Clustering]], [[Ordinations]], [[Regression Analysis]]), while other methods are at the frontier of the current development, such as artificial neural networks or decisions trees. Most approaches were already postulated decades ago, but some only gained momentum with the [[History of Methods|computer revolution]]. Since many approaches demand high computational capacity there is a clear rise in the last years, along with the rise of larger computer capacities. [[Bayesian Inference|Bayesian]] approaches are the best example of"", ""The ubiquity of Machine Learning empowered technologies have made concerns about individual privacy and social security more relevant. Refer to Dwork [8] to learn about a concept called ''Differential Privacy'' that is closely related to data privacy in data governed world.\n\n\n== Outlook ==\nThe field of Machine Learning is going to continue to flourish in the future as the technologies that harness Machine Learning techniques are becoming more accessible over time. As such, academic research in Machine Learning techniques and their performances continue to be attractive in many areas of research moving forward.\n\nThere is no question that the field of Machine Learning has been able to produce powerful models with great data processing and prediction capabilities. As a result, it has produced powerful tools that governments, private companies, and researches alike use to further advance their objectives. In light of this, Jordan and Mitchell [7] conclude that Machine Learning is likely to be one of the most transformative technologies of the 21st century.""]"|0.8333333332916666|0.75
31|What are the characteristics of scientific methods?|Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.|"[""=== What are scientific methods? ===\nWe define ''Scientific Methods'' as follows:\n* First and foremost, scientific methods produce knowledge. \n* Focussing on the academic perspective, scientific methods can be either '''reproducible''' and '''learnable'''; can be '''documented''' and are '''learnable'''; or are '''reproducible''', can be '''documented''', and are '''learnable'''. \n* From a systematic perspective, methods are approaches that help us '''gather''' data, '''analyse''' data, and/or '''interpret''' it. Most methods refer to either one or two of these steps, and few methods refer to all three steps."", '== What are scientific methods? ==', ""The course '''Scientific methods - Different paths to knowledge''' introduces the learner to the fundamentals of scientific work. It summarizes key developments in science and introduces vital concepts and considerations for academic inquiry. It also engages with thoughts on the future of science in view of the challenges of our time. Each chapter includes some general thoughts on the respective topic as well as relevant Wiki entries.\n\n__TOC__\n<br/>\n=== Definition & History of Methods ===\n'''Epochs of scientific methods'''\nThe initial lecture presents a rough overview of the history of science on a shoestring, focussing both on philosophy as well as - more specifically - philosophy of science. Starting with the ancients we focus on the earliest preconditions that paved the way towards the historical development of scientific methods."", ""'''Based on these considerations, one needs to remember the following criteria when you approach a concrete scientific method:''' \n* [[:Category:Quantitative|Quantitative]] - [[:Category:Qualitative|Qualitative]]\n* [[:Category:Inductive|Inductive]] - [[:Category:Deductive|Deductive]]\n* Spatial scale: [[:Category:Individual|Individual]] - [[:Category:System|System]] - [[:Category:Global|Global]]\n* Temporal scale: [[:Category:Past|Past]] - [[:Category:Present|Present]] - [[:Category:Future|Future]]"", ""'''We need to choose and apply methods depending on the type of knowledge we aim to create, regardless of the disciplinary background or tradition.''' We should aim to become more and more experienced and empowered to use the method that is most ideal for each research purpose and not rely solely on what our discipline has always been doing. In order to achieve this, design criteria of methods can help to create a conceptualization of the nature of methods. In other words: what are the underlying principles that guide the available scientific methods? First, we need to start with the most fundamental question:"", ""'''In short:''' This entry revolves around the history of Time as a concept, and its implications for scientific inquiry. Please refer to the [[Design Criteria of Methods]] entry for a general overview on all design criteria.\n\nTime is one of the greatest misconceptions in terms of methods (and humans in general), and together with space (or spatial scale, or grain) poses one of the key challenges in methodological aspects of science. Here, we give a short introduction to the different aspects of time from a methodological standpoint. Starting with the most relevant general aspects of time, the text will then focus on concrete methodological aspects of time. An outlook on the necessary next steps concerning time in scientific methods concludes the text."", ""* [[History of Methods]]\n* [[History of Methods (German)]]\n\n=== Methods in science ===\nA great diversity of methods exists in science, and no overview that is independent from a disciplinary bias exists to date. One can get closer to this by building on the core design criteria of scientific methods. \n\n'''Quantitative vs. qualitative'''\nThe main differentiation within the methodological canon is often between quantitative and qualitative methods. This difference is often the root cause of the deep entrenchment between different disciplines and subdisciplines. Numbers do count, but they can only tell you so much. There is a clear difference between the knowledge that is created by either qualitative or qualitative methods, hence the question of better or worse is not relevant. Instead it is more important to ask which knowledge would help to create the most necessary knowledge under the given circumstances.""]"|0.6428571428357143|0.6666666666666666
32|What is the main goal of practicing mindfulness?|The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.|"['== Goals ==\nSince the goal of mindfulness is basically having ""no mind"", it is counterintuitive to approach the practice with any clear goal. Pragmatically speaking, one could say that mindfulness practices are known to help people balance feelings of anxiety, stress and unhappiness. Many psychological challenges thus seem to show positive developments due to mindfulness practice. Traditionally, mindfulness is the seventh of the eight parts of the buddhist practice aiming to become free.', 'If you want to establish a mindfulness practice for yourself, you could try out the app Headspace or the aforementioned breathing exercises. Other ideas that you can find online include journaling, doing a body scan, or even Yoga. You could also start by going on a walk by yourself in nature without any technical distractions – just observing what you can see, hear, or smell. Mindfulness is a practice you can implement in everyday activities like doing the dishes, cleaning, or eating a meal as well. The goal here is to stay in the present moment without seeking distractions or judging situations. Another common mindfulness practice is writing a gratitude journal, which can help you to become more aware of the things we can be grateful for in live. This practice has proven to increase the well being of many people, and is a good mindfulness approach for people that do not want to get into a mediation. Yoga, Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that', 'Tai Chi and other physical body exercises can have strong components of mindfulness as well. You will find that a regular mindfulness practice helps you relieve stress and fear, can increase feelings of peace and gratitude, and generally makes you feel more calm and focused. Try it out!', 'During the last decades mindfulness took a strong tooting in the western world, and the commercialisation of the principle of mindfulness led to the development of several approaches and even apps, like Headspace, that can introduce lay people to a regular practice. The Internet contains many resources, yet it should be stressed that such approaches are often far away from the original starting point of mindfulness.', 'Mindfulness has been hyped as yet another self-optimisation tool. However, mindfulness is not an end in itself, but can be seen as a practice of a calm mind. Sweeping the floor is a common metaphor for emptying your mind. Our mind is constantly rambling around – often referred to as the monkey mind –, but there are several steps to recognise, interact with, train and finally calm your monkey mind (for tips on how to quiet the monkey mind, have a look at [https://www.forbes.com/sites/alicegwalton/2017/02/28/8-science-based-tricks-for-quieting-the-monkey-mind/#6596e6a51af6 this article]). Just like sports, mindfulness exercises are a practice where one gets better over time.', 'here, then I would suggest that you should always try to establish a path, and not goals. Being on a way and establishing the associated mindset is the most fundamental change we need. If we keep rushing towards goals, and keep changing these goals like all the time, and never acknowledge when we reach these goals, then all is lost. I much prefer to just be on a path, even if I am not clear in all points where it will lead me. You may write this one down on a teabag.', ""== Getting started ==\nThe easiest form of mindfulness is to sit in an upright position, and to breathe in, and breathe out. Counting your breath and trying to breathe deeply and calmly is the most fundamental mindfulness exercises. As part of the noble eightfold path in Buddhism, mindfulness became a key practice in Eastern monastic cultures ranging across Asia. Zazen – sitting meditation – is a key approach in Zen Buddhism, whereas other schools of Buddhism have different approaches. Common approaches try to explore the origins of our thoughts and emotions, or our interconnectedness with other people.\n\n[[File:Headspace - Mindfulness.png|300px|thumb|left|'''[https://www.headspace.com/de Headspace] is an app that can help you meditate''', which may be a way of practicing Mindfulness for you. Source: Headspace]]""]"|0.9999999999|1.0
33|How is information arranged in a Mindmap?|In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.|"[""'''A Mindmap enables the visual arrangement of various types of information, including tasks, key concepts, important topics, names and more.''' It allows for a quick overview of all relevant information items on a topic at a glance. It helps keeping track of important elements of a topic, and may support communication of information to other people, for example in meetings. A Mindmap can also be a good tool for a [[Glossary|brainstorming]] process, since it does not focus too much on the exact relationships between the visualised elements, but revolves around a more intuitive approach of arranging information."", ""== What, Why & When ==\n'''Mindmapping is a tool for the visual organisation of information''', showing ideas, words, names and more in relation to a central topic and to each other. The focus is on structuring information in a way that provides a good overview of a topic and supports [[Glossary|communication]] and [[Glossary|creativity.]]\n\n== Goals ==\n* Visualise information in an intuitive structure for a good overview of key elements of a topic.\n* Better communicate and structure information for individual and team work.\n\n== Getting started ==\nAlthough this claim is not fully supported by research, the underlying idea of a Mindmap is to mimick the way the brain structures information by creating a map of words, hence the name. Yet indeed, research has indicated that this approach to structuring information is an efficient way of memorizing and understanding information. The Mindmap was created and propagated in the 1970s by Tony Buzan."", ""[[File:Mindmap Example 2.jpg|600px|thumb|right|'''MindMaps can take the form of trees, with the words on the branches, or clusters/bubbles, as in this example.''' They can also be visually improved not only through the usage of colors, but also by varying the thickness and length of ties, and using symbols. Source: [https://www.thetutorteam.com/wp-content/uploads/2019/07/shutterstock_712786150.jpg thetutorteam.com]]]"", '=== Differences to Concept Mapping and Mindmaps ===\n[[Concept Maps]] refer to visual representations of conceptual ideas, as introduced by Novak and colleagues in the 1970s (Trochim 1989). Information (words or short sentences) is put into boxes or circles, which are connected in a network-like structure through arrows. This is in a way comparable to a mindmap. However, [[Mindmap|Mindmaps]] do not serve the purpose of presenting a final, comprehensive overview over a specific focus on a topic, but rather an unstructured, spontaneous collection of general related ideas around one key term. Both processes - Concept Maps and Mindmaps - can be done by a single person or in a group. For example, Brown (2003) shows how group-based creation of a concept map can enhance biology teaching.', ""'''4) Data synthesis & analysis:'''\nAfter the documents were selected, the researcher reads them and extracts information that helps answer the research questions. In the process of synthesizing, a [[Mindmap]] or [[Concept Maps]] may be useful tools which can help organize and understand the concepts and theories used in the documents. The documents may further be arranged in such a map to support the structuring process of the review (1)."", 'Importantly, while Concept Maps are often applied in research, teaching or planning, they should not be confused with the mixed methods approach of [[Group Concept Mapping]]. The latter emerged based on Concept Maps in the 1980s, but is a more structured, multi-step process. A Concept Map further differs from a [[Mindmap]] in that the latter are spontaneous, unstructured visualisations of ideas and information around one central topic, without a hierarchy or linguistic homogeneity, and do not necessarily include labeled information on how conceptual elements relate to each other.', ""The central topic is written into the center of the [[Glossary|visualisation]] (e.g. a whiteboard, with a digital tool, or a large horizontally arranged sheet of paper). '''This central topic can be see as a city center on a city map, and all relevant information items then are arranged around it like different districts of the city.''' The information items should focus on the most important terms and data, and omit any unncessary details. These elements may be connected to the central topic through lines, like streets, or branches, resulting in a web structure. '''Elements may be subordinate to other elements, indicating nestedness of the information.''' Colors, symbols and images may be used to further structure the differences and similaritiess between different areas of the map, and the length thickness of the connections may be varied to indicate the importance of connections.""]"|0.14285714284285714|1.0
34|Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?|Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.|"[""Mixed Effect Models were a continuation of Fisher's introduction of random factors into the Analysis of Variance. Fisher saw the necessity not only to focus on what we want to know in a statistical design, but also what information we likely want to minimize in terms of their impact on the results. Fisher's experiments on agricultural fields focused on taming variance within experiments through the use of replicates, yet he was strongly aware that underlying factors such as different agricultural fields and their unique combinations of environmental factors would inadvertedly impact the results. He thus developed the random factor implementation, and Charles Roy Henderson took this to the next level by creating the necessary calculations to allow for linear unbiased estimates. These approaches allowed for the development of previously unmatched designs in terms of the complexity of hypotheses that could be tested, and also opened the door to the analysis of complex datasets that are beyond the sphere of purely deductively designed datasets. It is thus not surprising that Mixed Effect Models rose to prominence in such diverse disciplines as"", ""While psychology, medicine, agricultural science, biology and later ecology thus thrived in their application of experimental designs and studies, there was also an increasing recognition of information that was creating [[Bias and Critical Thinking|biases]] or otherwise falsely skewed the results. '''The ANOVA hence became amended by additional modifications, ultimately leading to more advanced statistics that were able to focus on diverse statistical effects''', and reduce the influence of skews rooted, for instance, in sampling bias, statistical bias or other flaws. Hence, [[Mixed-Effect Models]] became an advanced next step in the history of statistical models, leading to more complex statistical designs and experiments, taking more and more information into account. In addition, meta-analytical approaches led to the combination and summarising of several case studies into a systematic overview. This was the dawn of a more integrational understanding of different studies that were combined into a [[Meta-Analysis]], taking different contexts of the numerous studies into account as well. In"", ""Während Psychologie, Medizin, Agrarwissenschaft, Biologie und später auch die Ökologie auf diese Weise in der Anwendung von Versuchsplänen und Studien gediehen, gab es auch eine zunehmende Anerkennung von Informationen, die [[Bias and Critical Thinking|Bias]] erzeugten oder die Ergebnisse anderweitig verzerrten. '''Die ANOVA wurde daher durch zusätzliche Modifikationen ergänzt, was schließlich zu fortgeschritteneren Statistiken führte, die in der Lage waren, sich auf verschiedene statistische Effekte zu konzentrieren'', und den Einfluss von Bias zu reduzieren, z.B. in Stichproben, statistischem Bias oder anderen Fehlern. Somit wurden [[Mixed-Effect Models]] zu einem fortgeschrittenen nächsten Schritt in der Geschichte der statistischen Modelle, der zu"", 'Statistician Karl Pearson had a strong focus on openly integrating knowledge into overarching results, and in this spirit he created one of the first systematic Meta-Analyses by integrating results from several comparable studies on typhoid into one of the first Meta-Analyses in 1904 (1, 2). It was more than 35 years later that within psychology the next relevant Meta-Analysis was published: the book-length ""Extrasensory Perception After Sixty Years"", authored by Pratt et al. in 1940, summarizing experimental results from 1882 to 1939 (1). Despite some few studies being compiled over the next decades (most based on clinical research), more sophisticated statistical analysis tools emerging after WW2 ultimately sealed the breakthrough of the method. In the 1970s, Gene V. Glass - together with many other statisticians - developed what he called the \'analysis of the analyses\' (2). By integrating statistical results into a meta-analytical frame,', ""Being baffled by the restrictions of regressions that rely on the normal distribution, John Nelder and Robert Wedderburn developed Generalized Linear Models (GLMs) in the 1960s to encompass different statistical distributions. Just as Ronald Fisher, both worked at the Rothamsted Experimental Station, seemingly a breeding ground not only in agriculture, but also for statisticians willing to push the envelope of statistics. Nelder and Wadderburn extended [https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ Frequentist] statistics beyond the normal distribution, thereby unlocking new spheres of knowledge that rely on distributions such as Poisson and Binomial. '''Nelder's and Wedderburn's work allowed for statistical analyses that were often much closer to real world datasets, and the array of distributions and the robustness and diversity of the approaches unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only"", 'Another very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.', 'Often, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models.']"|0.699999999965|1.0
35|How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?|Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.|"[""Typically, the diverse algorithmic foundations to calculate Mixed Effect Models (such as maximum likelihood and restricted maximum likelihood) allow for more statistical power, which is why Mixed Effect Models often work slightly better compared to standard regressions. The main strength of Mixed Effect Models is however not how they can deal with fixed effects, but that they are able to incorporate random effects as well. '''The combination of these possibilities makes (generalised) Mixed Effect Models the swiss army knife of univariate statistics.''' No statistical model to date is able to analyse a broader range of data, and Mixed Effect Models are the gold standard in deductive designs."", ""== Strengths & Challenges ==\n'''The biggest strength of Mixed Effect Models is how versatile they are.''' There is hardly any part of univariate statistics that can not be done by Mixed Effect Models, or to rephrase it, there is hardly any part of advanced statistics that - even if it can be made by other models - cannot be done ''better'' by Mixed Effect Models. They surpass the Analyis of Variance in terms of statistical power, eclipse Regressions by being better able to consider the complexities of real world datasets, and allow for a planning and understanding of random variance that brings science one step closer to acknowledge that there are things that we want to know, and things we do not want to know."", 'Often, these models serve as a baseline for the whole scheme of analysis, and thus already help researchers to design and plan their whole data campaign. Naturally, then, these models are the heart of the analysis, and depending on the discipline, the interpretation can range from widely established norms (e.g. in medicine) to pushing the envelope in those parts of science where it is still unclear which variance is being tamed, and which cannot be tamed. Basically, all sorts of quantitative data can inform Mixed Effect Models, and software applications such as R, Python and SARS offer broad arrays of analysis solutions, which are still continuously developed. Mixed Effect Models are not easy to learn, and they are often hard to tame. Within such advanced statistical analysis, experience is key, and practice is essential in order to become versatile in the application of (generalised) Mixed Effect Models.', 'Another very important point regarding Mixed Effect Models is that they - probably more than any statistical method - remark the point where experts in one method (say for example interviews) now needed to learn how to conduct interviews as a scientific method, but also needed to learn advanced statistics in the form of Mixed Effect Models. This creates a double burden, and while learning several methods can be good to embrace a more diverse understanding, it is also challenging, and highlights a new development. Today, statistical analysis are increasingly outsourced to experts, and I consider this to be a generally good development. In my own experience, it takes a few thousand hours to become versatile at Mixed Effect Models, and modern science is increasingly built on collaboration.', ""== What the method does ==\nMixed Effect Models are - mechanically speaking - one step further with regards to the combination of [[Regression Analysis|regression analysis]] and Analysis of Variance, as they can combine the strengths of both these approaches: '''Mixed Effects Models are able to incorporate both [[Data formats|categorical and/or continuous]] independent variables'''. Since Mixed Effect Models were further developed into [[Generalized Linear Models|Generalized Linear Mixed Effect Models]], they are also able to incorporate different distributions concerning the dependent variable."", ""Honouring the complexity of the world while still deriving value statements based on statistical analyses has never been more advanced on a broader scale. '''Still, statisticians need to recognize the limitations of real world data, and researchers utilising these need to honour the preconditions and pitfalls of these analyses'''. Current science is in my perception far away from reporting reproducible analyses, meaning that one and the same dataset will be differently analysed by Mixed Effect Model approaches, partly based on experience, partly based on differences between disciplines, and probably also because of many other factors. Mixed Effect Models need to be consolidated and unified, which would make normale science probably better than ever."", '==  Normativity ==\nMixed Effect Models are the gold standard when it comes to reducing complexities into constructs, for better or worse. All variables that go into a Mixed Effect Model are normative choices, and these choices matter deeply. First of all, many people struggle to decide which variables are about fixed variance, and which variables are relevant as random variance. Second, how are these variables constructed - are they continuous or categorical, and if the latter, what is the reasoning behind the category levels? Designing Mixed Effect Modell studies is thus definitely a part of advanced statistics, and this is even harder when it comes to integrating non-designed datasets into a Mixed Effect Model [[Glossary|framework]]. Care and experience are needed to evaluate sample sizes, variance across levels and variables. This brings us to the most crucial point: Model inspection.']"|0.99999999995|1.0
36|Why should stepwise procedures in model reduction be avoided?|Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.|"['The most blunt approach to any form of model reduction of a maximum model is a stepwise procedure. Based on p-values or other criteria such as AIC, a stepwise procedure allow to boil down any given model until only significant or otherwise statistically meaningful variables remain. While you can start from a maximum model that is boiled down which equals a backward selection, and a model starting with one predictor that subsequently adds more and more predictor variables and is named a forward selection, there is even a combination of these two. Stepwise procedures and not smart but brute force approaches that are only based on statistical evaluations, yet not necessarily very good ones. No experience or preconceived knowledge is included, hence such stepwise procedures are nothing but buckshot approaches that boil any given dataset down, and are not prone against many of the errors that may happen along the way. Hence stepwise procedures should be avoided at all costs, or at least be seen as the non-smart brute force tool they are.', ""== '''Starting to engage with model reduction - an initial approach''' =="", ""is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics."", '===Parsimonious models===', ""The diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the"", ""Regarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy."", 'reduction is preferable to reporting full models"", I mean myself and likeminded scientists. This is a deeply normative statement rooted in my experience and the experience of other scientist, and therefore ideally deserves further explanation. Consequently, I might write something to the end of ""by avoiding p-values and building my model reduction on AIC values, the reported statistic model are more robust under the criteria of parsimony"".']"|0.9999999999|1.0
37|What are the methods to identify redundancies in data for model reduction?|The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.|"['=== Redundancy analysis ===', ""== '''Starting to engage with model reduction - an initial approach''' =="", 'The simplest approach to look for redundancies are correlations. Pearson correlation even do not demand a normal distribution, hence these can be thrown onto any given combination of continuous variables and allow for identifying which variables explain fairly similar information. The word ""fairly"" is once more hard to define. All variables that are higher collected than a correlation coefficient of 0.9 are definitely redundant. Anything between 0.7 and 0.9 is suspicious, and should ideally be also excluded, that is one of the two variables. Correlation coefficients below 0.7 may be redundant, yet this danger is clearly lower. A more integrated approach that has the benefit to be graphically appealing are ordinations, with principal component analysis being the main tool for continuous variables. based on the normal distribution, this analysis represents a form of dimension reduction, where the main variances of datasets are reduced to artificial axes or dimensions. The axis are orthogonally related, which means that they are maximally', ""is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the heart of the overall philosophy of model reduction, and it will be up to any given analysis to honour this approach within a given analysis. Within this living document, we try to develop a learning curve to create an analytical framework that can aid an initial analysis of any given dataset within the hemisphere of univariate statistics."", 'Instead, one should always inspect all data initially regarding the statistical distributions and prevalences. Concretely, one should check the datasets for outliers, extremely skewed distributions or larger gaps as well as other potentially problematic representations. All sorts of qualitative data needs to be checked for a sufficient sample size across all factor levels. Equally, missing values need to be either replaced by averages or respective data lines be excluded. There is no rule of thumb on how to reduce a dataset riddled with missing values. Ideally, one should check the whole dataset and filter for redundancies. Redundant variables can be traded off to exclude variables that re redundant and contain more NAs, and keep the ones that have a similar explanatory power but less missing values.', ""The diverse approaches of model reduction, model comparison and model simplification within univariate statistics are more or less stuck between deep dogmas of specific schools of thought and modes of conduct that are closer to alchemy as it lacks transparency, reproducibility and transferable procedures. Methodology therefore actively contributes to the diverse crises in different branches of science that struggle to generate reproducible results, coherent designs or transferable knowledge. The main challenge in the hemisphere of model treatments (including comparison, reduction and simplification) are the diverse approaches available and the almost uncountable control measures and parameters. It would indeed take at least dozens of example to reach some sort of saturation in knowledge to to justice to any given dataset using univariate statistics alone. Still, there are approaches that are staples and that need to be applied under any given circumstances. In addition, the general tendency of any given form of model reduction is clear: Negotiating the point between complexity and simplicity. Occam's razor is thus at the"", ""Regarding vocabulary, we will understand model reduction always as some form of model simplification, making the latter term futile. Model comparison is thus a means to an end, because if you have a deductive approach i.e. clear and testable hypotheses, you compare the different models that represent these hypotheses. Model reduction is thus the ultimate and best term to describe the wider hemisphere that is Occam's razor in practise. We have a somewhat maximum model, which at its worst are all variables and maybe even their respective combinations. This maximum model has several problems. First of all, variables may be redundant, explain partly the same, and fall under the fallacy of multicollinearity. The second problem of a maximum model is that it is very difficult to interpret, because it may -depending on the dataset-contain a lot of variables and associated statistical results. Interpreting such results can be a challenge, and does not exactly help to come to pragmatic information that may inform decision or policy.""]"|0.7555555555303703|0.8
38|How are 'narratives' used in Narrative Research?|'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.|"[""'''As a scientific method, Narrative Research - often just phrased 'narrative' - is a rather recent phenomenon''' (Barrett & Stauffer 2009; Clandinin 2006, see Squire et al. 2014). Narratives have developed towards modes of scientific inquiry in various disciplines in Social Sciences, including the arts, anthropology, cultural studies, psychology, sociology, and educational science (Barrett & Stauffer 2009). This development paralleled an increasing role of qualitative research during the second half of the 20th Century, and built on the understanding of 'narrative' as both a form of story and a form of meaning-making of the human experience. Today, Narrative Research may be used across a wide range of disciplines and is an increasingly applied form in educational research (Moen 2006, Stauffer & Barrett 2009, Webster & Mertova 2007)."", 'Narrative Research is ""(...) the study of stories"" (Polkinghorne 2007, p.471) and thus ""(...) the study of how human beings experience the world, and narrative researchers collect these stories and write narratives of experience."" (Moen 2006, p.56). The distinctive feature of this approach - compared to other methods of qualitative research with individuals, such as [[Open Interview|Interviews]] or [[Ethnography]] - is the focus on narrative elements and their meaning. Researchers may focus on the \'narratology\', i.e. the structure and grammar of a story; the \'narrative content\', i.e. the themes and meanings conveyed through the story; and/or the \'narrative context\', which revolves around the effects of the story (Squire et al. 2014).', '\'\'\'One common approach in Narrative Research is the usage of narratives in form of spoken or written text or other types of media as the data material for analysis.\'\'\' This understanding is comparable to [[Content Analysis]] or [[Hermeneutics]]. A second understanding focuses on the creation of narratives as part of the methodological design, so that the narrative material is not pre-developed but emerges from the inquiry itself (Squire et al. 2014, Jovchelovitch & Bauer 2000). In both approaches, \'narrative\' is the underlying ""frame of reference"" (Moen 2006, p.57) for the research. An example for the latter understanding is the \'Narrative Interview\'.', '|-\n| style=""width: 33%""| [[:Category:Past|Past]] || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || [[:Category:Future|Future]]\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Narrative Research describes qualitative field research based on narrative formats which are analyzed and/or created during the research process.', '* Webster & Mertova (2007, p.4) highlight that research methods that understand narratives as a mere format of data presented by the subject, which can then be analyzed just like other forms of content, neglect an important feature of narratives: Narrative Research ""(...) requires going beyond the use of narrative as rhetorical structure, to an analytic examination of the underlying insights and assumptions that the story illustrates"". Further, ""Narrative inquiry attempts to capture the \'whole story\', whereas other methods tend to communicate understandings of studied subjects or phenomena at certain points, but frequently omit the important \'intervening\' stages"" (ibid, p.3), with the latter being the context and cultural surrounding that is better understood when taking the whole narrative into account (see Moen 2006, p.59).', ""(Clandinin 2006, p.20, citing Clandinin & Connelly 2000). To this end, Clandinin (2006) distinguishes between the re-telling of stories that Interview participants tell the researchers, and the telling of stories that the researchers experience themselves, e.g. in ethnographic studies. The difference is not so much of methodological nature as it is in the purpose and perspective of the research (Barrett & Stauffer 2009). In this understanding, 'Narrative Inquiry' is a reflexive and iterative process, with the researchers entering into a field of experiences, telling their own stories, telling the stories of other participants, and dialogically co-creating joined stories with them (Moen 2006). The created narratives serve as a way of presenting the research experiences, but also as forms of data for the analysis of the joint and conveyed experiences."", ""(4) Webster, L. Mertova, P. 2007. ''Using Narrative Inquiry as a Research Method. An introduction to using critical event narrative analysis in research on learning and teaching.'' Routledge London, New York. \n\n(5) Squire, C., Davis, M., Esin, C., Andrews, M., Harrison, B., Hyden, L-C, and Hyden, M. 2014. ''What is narrative research?'' London: Bloomsbury\n\n(6) Moen, T. 2006. ''Reflections on the Narrative Research Approach.'' International Journal of Quantitative Methods 5(4). 56-69.""]"|0.46666666665111106|1.0
39|What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?|Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.|"['Over the last decades, many types of [[Statistics|statistical]] models emerged that are better suited to deal with such non-linear dynamics. One of the most prominent approaches is surely that of Generalized Additive Models (GAM), which represents a statistical revolution. Much can be said about all the benefits of these models, which in a nutshell are - based on a smooth function - able to compromise predictor variables in a non-linear fashion. Trevor Hastie and Robert Tibshirani (see Key Publications) were responsible for developing these models and matching them with [[Generalised Linear Models]]. By building on more computer-intense approaches, such as penalized restricted likelihood calculation, GAMs are able to outperform linear models if predictors follow a non-linear fashion, which seems trivial in itself. This comes however with a high cost, since the ability of higher model fit comes - at least partly - with the loss of our ability to infer [[Causality|causality]] when explaining the', '|-\n| style=""width: 33%""| \'\'\'[[:Category:Past|Past]]\'\'\' || style=""width: 33%""| \'\'\'[[:Category:Present|Present]]\'\'\' || \'\'\'[[:Category:Future|Future]]\'\'\'\n|}\n<br/>__NOTOC__\n<br/><br/>\n\'\'\'In short:\'\'\' Generalized Linear Models (GLM) are a family of models that are a generalization of [[Regression Analysis|ordinary linear regression]], thereby allowing for different statistical distributions to be implemented.', '[[File:ConceptGLM.png|450px|frameless|left|[[Sustainability Methods:About|Method categorization]] for [[Generalized Linear Models]]]]', ""unlocked new worlds of data for statisticians.''' The insurance business, econometrics and ecology are only a few examples of disciplines that heavily rely on Generalized Linear Models. GLMs paved the road for even more complex models such as additive models and generalised [[Mixed Effect Models|mixed effect models]]. Today, Generalized Linear Models can be considered to be part of the standard repertoire of researchers with advanced knowledge in statistics."", ""== What the method does ==\nGeneralized Linear Models are statistical analyses, yet the dependencies of these models often translate into specific sampling designs that make these statistical approaches already a part of an inherent and often [[Glossary|deductive]] methodological approach. Such advanced designs are among the highest art of quantitative deductive research designs, yet Generalized Linear Models are used to initially check/inspect data within inductive analysis as well. Generalized Linear Models calculate dependent variables that can consist of count data, binary data, and are also able to calculate data that represents proportions. '''Mechanically speaking, Generalized Linear Models are able to calculate relations between continuous variables where the dependent variable deviates from the normal distribution'''. The calculation of GLMs is possible with many common statistical software solutions such as R and SPSS. Generalized Linear Models thus represent a powerful means of calculation that can be seen as a necessary part of the toolbox of an advanced statistician."", 'Another advantage of GLMs is the diversity of evaluative criteria, which may help to understand specific problems within data distributions. For instance, presence/absence variables are often riddled by prevalence, which is the proportion between presence values and absence values. Binomial GLMs are superior in inspecting the effects of a skewed prevelance, allowing for a sound tracking of thresholds within models that can have direct consequences. Examples for this can be found in medicine regarding the identification of precise interventions to save a patients life, where based on a specific threshhold for instance in oxygen in the blood an artificial Oxygen sources needs to be provides to support the breathing of the patient.', 'assumptions by using a [[Generalized_Linear_Models|generalized linear model]]. Nevertheless, one advantage, partial correlation has over linear regression, is that it delivers an estimate for the relationship which is easy to interpret. The linear regression outputs weights that depend on the unit of measurement, making it hard to compare them to other models. As a work-around, one could use standardized beta coefficients. Yet those coefficients are harder to interpret since they do not necessarily have an upper bound. Partial correlation coefficients, on the other hand, are defined to range from -1 to 1.']"|0.9999999999|1.0
40|What are the three conditions under which Poisson Distribution can be used?|Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.|"['Poisson distribution can only be used under three conditions: 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.\n\nFor example, Poisson distribution is used by mobile phone network companies to calculate their performance indicators like efficiency and customer satisfaction ratio. The number of network failures in a week in a locality can be measured given the history of network failures in a particular time duration. This predictability prepares the company with proactive solutions and customers are warned in advance. For more real-life examples of Poisson distribution in practice, visit [https://studiousguy.com/poisson-distribution-examples/ this page].\n\n==3. Calculation and Probability Mass Function (PMF) Graph==\n\n===Probability Mass Function Graph===', '|-\n| Poisson || It provides the probability of an event happening a certain number of times (k) within a given interval of time or space. For example, figuring out the probability of disease occurrence m times in the next month given that it occurs n times in 1 year.\n|-\n| Geometric || It determines the number of independent trials needed to get the first successful outcome. Geometric distribution may be used to conduct a cost-benefit analysis of a certain decision in a business.\n|}', '==2. Poisson Distribution==\nPoisson Distribution is one of the discrete probability distributions along with binomial, hypergeometric, and geometric distributions. The following table differentiates what applies where.\n{| class=""wikitable""\n|-\n! Distribution !! Definition\n|-\n| Binomial || It is used when there are two possible outcomes (success/failure) in a process that are independent of each other in n number of trials. The easiest example is a coin toss whereas a more practical use of binomial distribution is testing a drug, whether the drug cures the disease or not in n number of trials\n|-\n| Hypergeometric || It calculates the number of k successes in n number of trials where the probability of success changes with each passing trial. This kind of distribution applies in Poker when drawing a red card from the deck changes the probability of drawing another red card after it.\n|-', '6. Poisson Limit Theorem states that Poisson distribution may be used as an approximation to the binomial distribution, under certain conditions. When the value of n (number of trials) in a binomial distribution is large and the value of p (probability of success) is very small, the binomial distribution can be approximated by a Poisson distribution i.e., n -> ∞ and λ = np, rate parameter, λ is defined as the number of trials, n, in binomial distribution multiplied by the probability of success, p.', 'The resulting pmf confirms that the closest integer value to lamba i.e., 3 has the most number of years out of 10,000 meaning most years will have 3 duststorms. You can also see that the data is slightly skewed to the right since there is a larger variance to the right than to the left. Looking at the distribution, it looks fairly normally distributed. However, the low lambda does not allow to use the Poisson distribution as an approximation for a normal distribution. Most probably, the large dataset allows us to see it as a normal distribution, since most distributions converge to a normal distribution with increasing sample size.\n==6. References==', '==6. References==\n* Brownlee, Jason: ""A Gentle Introduction to Probability Distributions"", 14.11.2019. Retrieved from: https://machinelearningmastery.com/what-are-probability-distributions/#:~:text=A%20probability%20distribution%20is%20a,properties%20that%20can%20be%20measured, last checked: 21.05.2023\n* Koehrsen, Will: "" The Poisson Process and Poisson Distribution Explained (With Meteors!), 28.10.2022. Retrieved from: https://builtin.com/data-science/poisson-process, last checked: 21.05.2023\n* Papoulis, Athanasios; Pillai, S. Unnikrishna: ""Probability, Random Variables, and Stochastic Processes"" (4th ed.).', ""==== Not normally distributed dependent variable ====\n'''The dependent variable(s) is/are not normally distributed.''' Which kind of distribution does it show, then? For both Binomial and Poisson distributions, your next step is the Generalised Linear Model. However, it is important that you select the proper distribution type in the GLM.\n<imagemap>Image:Statistics Flowchart - Dependent - Distribution type.png|650px|center|\npoly 290 4 154 140 288 269 423 138 [[Data distribution]]\npoly 138 151 2 287 136 416 270 284 271 285 [[An_initial_path_towards_statistical_analysis#Generalised_Linear_Models|Generalised Linear Models]]""]"|0.6428571428357143|1.0
41|How does the Pomodoro technique work?|The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.|"[""== What, Why & When ==\nUsing Pomodoro is generally a good idea when you have to get work done and don't want to lose yourself in the details as well as want to keep external distraction to a minimum. It also works brilliantly when you struggle with starting a task or procrastination in general.\n\n== Goals ==\n* Stay productive\n* Manage time effectively\n* Keep distractions away\n* Stay flexible within your head\n* Avoid losing yourself in details\n\n== Getting started ==\nPomodoro is a method to self-organize, avoid losing yourself, stay productive and energized. \n\n'''Pomodoro is very simple. All you need is work to be done and a timer.'''  \n\nThere are six steps in the technique:"", 'There are six steps in the technique:\n\n# Decide on the task to be done.\n# Set the pomodoro timer (traditionally to \'\'25 minutes = 1 ""Pomodoro""\'\').\n# Work on the task.\n# End work when the timer rings (Optionally: put a checkmark on a piece of paper).\n# If you have fewer than four checkmarks (i.e. done less than 4 Pomodoros) , take a short break (5 minutes), then go back to step 2.\n# After four pomodoros, take a longer break (15–30 minutes), reset your checkmark count to zero, then start again at step 1.\n\n\n== Links & Further reading ==\n\n==== Resources ====', '== Links & Further reading ==\n\n==== Resources ====\n\n* Wikipedia - [https://en.wikipedia.org/wiki/Pomodoro_Technique Pomodoro Technique]\n* [https://lifehacker.com/productivity-101-a-primer-to-the-pomodoro-technique-1598992730 Extensive Description] on Lifehacker\n* [https://www.youtube.com/watch?v=H0k0TQfZGSc Video description] from Thomas Frank\n\n==== Apps ====\n\n* Best Android App: [https://play.google.com/store/apps/details?id=net.phlam.android.clockworktomato&hl=de Clockwork Tomato]\n* Best iPhone App: [https://apps.apple.com/us/app/focus-keeper-time-management/id867374917 Focus Keeper]', ""'''3) Rekombination bestehender Methoden"", ""[https://www.youtube.com/watch?v=XQOnsVSg5VQ YouTube MTTM Animations - The Disney Strategy]\n<br/> A video that (in a nicely animated matter, with dreamy guitar music) explains the method.\n\nYou might also be interested in 'Saving Mr. Banks', a movie starring Tom Hanks that focuses on Walt Disney.\n\n----\n__NOTOC__\n[[Category:Skills_and_Tools]]\n[[Category:Collaborative Tools]]\n[[Category:Team Size 2-10]]\n[[Category:Team Size 11-30]]\n\nThe [[Table_of_Contributors| author]] of this entry is Christopher Franz."", ""'''2) Relokation bestehender Methoden'''"", '== What the method does ==\nGenerally, Iconology works as a three-step model through which different questions are formulated and the according insights are put together subsequently in a complex manner (10). The three steps are: the pre-iconographic step, the step of interpretation (or iconography) and the iconological synthesis (10). Panofsky advised to not consider the steps as absolutely separated, because the verbal articulation of the artwork can only really be usefully read in its complete meaning after the analysis and interpretation (7).']"|0.49999999995|1.0
42|What is the 'curse of dimensionality'?|The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.|"[""=== Curse of dimensionality ===\nThis term was coined by Richard R. Bellman, an American applied mathematician. As the number of features / dimensions increases, the distance among data points grows exponential. Things become really sparse as the instances lie very far away from each other. This makes applying machine learning methods much more difficult, since there is a certain relationship between the number of features and the number of training data. In short, with higher dimensions you need to gather much more data for learning to actually occur, which leaves a lot of room for error. Moreover, higher-dimension spaces have many counter-intuitive properties, and the human mind, as well as most data analysis tools, is used to dealing with only up to three dimensions (like the world we are living in). Thus, data visualization and intepretation become much harder, and computational costs of model training greatly increases. '''Principle Component Analysis helps to alleviate this problem'''."", '=== Non metric (multi)dimensional scaling ===', '* Information loss through reduction of dimensionality (oftentimes acceptable)\n== Key Publications ==\n* Wold, S., Esbensen, K., & Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3), 37-52.', 'in the univariate model.s While this literally cures the problem of redundancy and collinearity, it can make interpretation of the results much harder, as one always has to think around the corner on what the individual axes actually meant, and with which predictor variables they correlated. Still, or maybe exactly because of this, the PCA is one of the most powerful ordination techniques, and has proven its value over time.', '===Parsimonious models===', 'the hardest struggle that you can fin neither in textbooks nor in articles. The empirical reality is that many applications of ordinations violate much of the mathematical assumptions or rules, yet the patterns derived from these analyses are still helpful if not even valid. Mathematicians can choose to live in a world where much of ordination techniques is perfect in every way, yet the datasets the world gives to ordinations are simply not. Instead, we have to acknowledge that multivariate data is almost always messy, contains a high amount of noise, many redundancies, and even data errors. Safety comes in numbers. Ordinations are so powerful exactly because they can channel all these problems through the safety of the size of the data, and thus derive either initial analysis or even results that serve as endpoints. However, there is a difference between initial or final results, and this will be our first starting point here.', '== Complexity ==']"|0.9999999999|1.0
43|Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?|Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.|"['<syntaxhighlight lang=""Python"" line>\ndata.describe()## here you can an overview over the standard descriptive data.\n</syntaxhighlight> \n\n<syntaxhighlight lang=""Python"" line>\ndata.columns #This command shows you the variables you will be able to use. The data type is object, since it contains different data formats.\n</syntaxhighlight>\n\n==Homoscedasticity and Heteroscedasticity==\nBefore conducting a regression analysis, it is important to look at the variance of the residuals. In this case, the term ‘residual’ refers to the difference between observed and predicted data (Dheeraja V.2022).\n\nIf the variance of the residuals is equally distributed, it is called homoscedasticity. Unequal variance in residuals causes heteroscedastic dispersion.', 'Looking a the graphs along the diagonal line, it shows a higher peak among the male students than the female students for the written exam, solved question (quanti) and exercise points (points). Looking at the relationship between exam and points, no clear pattern can be identified. Therefore, it cannot be safely assumed, that a heteroscedastic dispersion is given.\n[[File:Pic 1.png|700px]]\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""group"", diag_kind=""hist"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored in the final written exam, for the exercises and number of solved questions solved among the learning groups A,B and C.\n\n[[File:Pic 2.png|700px]]', 'If heteroscedasticity is the case, the OLS is not the most efficient estimating approach anymore. This does not mean that your results are biased, it only means that another approach can create a linear regression that more adequately models the actual trend in the data. In most cases, you can transform the data in a way that the OLS mechanics function again.\nTo find out if either homoscedasticity or heteroscedasticity is the case, we can look at the data with the help of different visualization approaches.\n\n<syntaxhighlight lang=""Python"" line>\nsns.pairplot(data, hue=""sex"") ## creates a pairplot with a matrix of each variable on the y- and x-axis, but gender, which is colored in orange (hue= ""sex"")\n</syntaxhighlight>\nThe pair plot model shows the overall performance scored of the final written exam, exercise and number of solved questions among males and females (gender group).', '==Ordinary Least Squares Estimator==\nThe following parameters are used:\n\nid = final digit of the student matriculation number \ngroup = name of the learning group \nsex = gender in binary categories (m = male, f = female) \nquanti = number of solved exercises \npoints = total score achieved from the exercises exam = total score achieved from the final written exam (Students must have to participate in the final written exam. If not, they will be considered as fail) \npassed = dummy whether the person has passed the class or not\n\nFirstly, the aim for analyzing the dataset is to figure out the performance scored among the learning groups and gender for the solved questions, exercises and written exams.\n\nSecondly, we want to figure out the correlation between variables and most importantly to figure out heteroscedastic and homoscedastic dispersion, since the OLS is only apt when homoscedasticity is the case.', ""'''How do I know?'''\n* Variance in the data is the measure of dispersion: how much the data spreads around the mean? Use an f-Test to check whether the variances of the two datasets are equal. The key R command for an f-test is <code>var.test()</code>. If the rest returns insignificant results (>0.05), we can assume equal variances. Check the [[Simple_Statistical_Tests#f-test|f-Test]] entry to learn more.\n* If the variances of your two datasets are equal, you can do a Student's t-test. By default, the function <code>t.test()</code> in R assumes that variances differ, which would require a Welch t-test. To do a Student t-test instead, set <code>var.equal = TRUE</code>."", '[[File:Pic 2.png|700px]]\n\nThe histograms among the diagonal line from top left to bottom right show no normal distribution. The histogram for id can be ignored since it does not provide any information about the student records. Looking at the relationship between exam and points, a slight pattern could be identified that would hint to heteroscedastic dispersion.\n\n===Correlations with Heatmap===\nA heatmap shows the correlation between different variables. Along the diagonal line from left to right, there is perfect positive correlation because it is the correlation of one variable against itself. Generally, you can assess the heatmap fully visually, since the darker the square, the stronger the correlation.\n\n<syntaxhighlight lang=""Python"" line>\np=sns.heatmap(data.corr(),\n              annot=True, cmap=\'PuBu\');\n</syntaxhighlight>\n\n[[File:Pic 3.png]]', 'Next, we should test for heteroscedasticity. In our regression model, we have assumed homoscedasticity which means that the variance of the residuals is equally distributed. The residuals are the difference between your observations from your predictions.\nIf this is not the case, you have heteroscedasticity. This is often the case because of outliers or skewness in the distribution of a variable. You can assess this visually by plotting the variance of your residuals against an independent variable. Again here, this only makes sense for continuous variables, which is why we look at GCPA.\n\n<syntaxhighlight lang=""Python"" line>\n# Calculate the residuals\nresiduals = model.resid\n\n# Calculate the squared residuals ( to only have positive values)\nsquared_resid = residuals ** 2\n\n# Group the squared residuals by the values of each independent variable\ngrouped_resid = squared_resid.groupby(x[\'CGPA\'])']"|0.8178571428435119|1.0
44|How did Shell contribute to the advancement of Scenario Planning?|"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."|"['\'\'\'Shortly after, Scenario Planning was heavily furthered through corporate planning, most notably by the oil company Shell.\'\'\' At the time, corporate planning was traditionally ""(...) based on forecasts, which worked reasonably well in the relatively stable 1950s and 1960s. Since the early 1970s, however, forecasting errors have become more frequent and occasionally of dramatic and unprecedented magnitude."" (Wack 1985, p.73). As a response to this and to be prepared for potential market shocks, Shell introduced the ""Unified Planning Machinery"". The idea was to listen to planners\' analysis of the global business environment in 1965 and, by doing so, Shell practically invented Scenario Planning. The system enabled Shell to first look ahead for six years, before expanding their planning horizon until 2000. The scenarios prepared Shell\'s management to deal with the 1973 and 1981 oil crises (1). Shell\'s success popularized the method. By 1982,', ""oil crises (1). Shell's success popularized the method. By 1982, more than 50% of Fortune 500 (= the 500 highest-grossing US companies) had switched to Scenario Planning (2)."", '== Further Information ==\n* Shell works with Scenarios still today. [[On this websitehttps://www.shell.com/energy-and-innovation/the-energy-future/scenarios.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL3NjZW5hcmlvcy5odG1s|On this website]], you find a lot of information about their scenario approach.\n----\n[[Category:Qualitative]]\n[[Category:Quantitative]]\n[[Category:Deductive]]\n[[Category:System]]\n[[Category:Global]]\n[[Category:Future]]\n[[Category:Methods]]\n\nThe [[Table of Contributors|author]] of this entry is Christopher Franz.', ""Today, Scenario Planning remains an important tool for corporate planning in the face of increasing complexity and uncertainty in business environments (5). Adjacent to [[Visioning & Backcasting]], it has also found its way into research. For instance, researchers in [[Glossary|transdisciplinary]] sustainability science gather stakeholders' expertise to think about (un)desirable states of the future and how (not) to get there. This way, companies, non-governmental organizations, cities and even national states can be advised and supported in their planning."", '\'\'\'The use of scenarios as a tool for structured thinking about the future dates back to the Manhattan Project in the early 1940s\'\'\'. The physicists involved in developing the atomic bomb attempted to estimate the consequences of its explosion and employed computer simulations to do so. Subsequently, this approach advanced in three separate strands: computer simulations, game theory, and military planning through, among others, the RAND corporation that also developed the [[Delphi]] Method. Later, during the 1960s, scenarios were ""(...) extensively used for social forecasting, public policy analysis and [[Glossary|decision making]]"" in the US. (Amer et al. 2013, p.24).', '* Scenario Planning is to some extent comparable to the [[Delphi]], where experts share their opinions on the future of an issue and come to a joint prediction. However, Scenario Planning differs in that it attempts to develop several complex scenarios instead of reaching one single (often quantitative) result to a question.', '== Strengths & Challenges ==\n* Scenario Planning allows for any organization that deploys it to be more innovative, flexible and thus better prepared for unforeseen disruptions and changes. In a corporate context, this can reduce costs, provide market benefits and improve internal communication (3, 5).\n* Scenario Planning broadens the structural perspective of an actor to think about the future (5). For example, an oil company may well be able to assess risks in their technical processes of oil exploration and extraction, but only through a more detailed scenario analysis they may be enabled to include economic, political and societal trends into their planning (2).\n* Scenarios are psychologically attractive. They are a way of transforming seemingly disparate data into relatable, coherent narratives. They present uncertainty across scenarios instead of providing probabilistic information for all elements within each individual one. In addition, they reduce the complexity and uncertainty of the future into graspable states (2).']"|0.99999999995|1.0
45|Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?|Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.|"['Romanian-American psychosociologist \'\'\'Jacob Moreno heavily influenced the field of Social Network Analysis in the 1930s\'\'\' with his - and his collaborator Helen Jennings\' - \'sociometry\', which served ""(...) as a way of conceptualizing the structures of small groups produced through friendship patterns and informal interaction."" (Scott 1988, p.110). Their work was sparked by a case of runaways in the Hudson School for Girls in New York. Their assumption was that the girls ran away because of the position they occupated in their social networks (see Diagram).', '\'\'\'One of the originators of Network Analysis was German philosopher and sociologist Georg Simmel\'\'\'. His work around the year 1900 highlighted the importance of social relations when understanding social systems, rather than focusing on individual units. He argued ""against understanding society as a mass of individuals who each react independently to circumstances based on their individual tastes, proclivities, and beliefs and who create new circumstances only by the simple aggregation of their actions. (...) [W]e should focus instead on the emergent consequences of the interaction of individual actions."" (Marin & Wellman 2010, p.6)\n\n[[File:Social Network Analysis History Moreno.png|350px|thumb|left|\'\'\'Moreno\'s original work on Social Networks.\'\'\' Source: Borgatti et al. 2009, p.892]]', 'Moreno\'s and Jennings\' work was subsequently taken up and furthered as the field of \'\'\'\'group dynamics\', which was highly relevant in the US in the 1950s and 1960s.\'\'\' Simultaneously, sociologists and anthropologists further developed the approach in Britain. ""The key element in both the American and the British research was a concern for the structural properties of networks of social relations, and the introduction of concepts to describe these properties."" (Scott 1988, p.111). In the 1970s, Social Network Analysis gained even more traction through the increasing application in fields such as geography, economics and linguistics. Sociologists engaging with Social Network Analysis remained to come from different fields and topical backgrounds after that. Two major research areas today are community studies and interorganisational relations (Scott 1988; Borgatti et al. 2009). However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it', ""* Prell, C. (2012): ''Social network analysis: History, theory and methodology'', London.\n* Reed, M.S., Graves, A., Dandy, N., Posthumus, H., Hubacek, K., Morris, J., Prell, C., Quinn, C.H., Stringer, L.C. 2009. ''Who’s in and why? A typology of stakeholder analysis methods for natural resource management.'' Journal of Environmental Management 90, 1933-1949."", '* \'\'\'Data Collection\'\'\': When the research focus is chosen, the data necessary for Social Network Analysis can be gathered in [[Survey Research|Surveys]] or [[Interviews]], through [[Ethnography|Observation]], [[Content Analysis]] (typically literature-based) or similar forms of data gathering. Surveys and Interviews are most common, with the researchers asking individuals about the existence and strength of connections between themselves and others actors, or within other actors excluding themselves (Marin & Wellman 2010, p.14). There are two common approaches when surveying individuals about their own connections to others: In a \'\'prompted recall\'\' approach, they are asked which people they would think of with regards to a specific topic (e.g. ""To whom would you go for advice at work?"") while they are shown a pre-determined list of potentially relevant individuals. In the \'\'free list\'\' approach, they are asked to recall individuals without seeing a list (Butts', '== What the method does ==\n""Social network analysis is neither a theory nor a methodology. Rather, it is a perspective or a paradigm."" (Marin & Wellman 2010, p.17) It subsumes a broad variety of methodological approaches; the fundamental ideas will be presented hereinafter.', 'However, since Social Network Analysis allows to assess many kinds of complex interaction between entities, it has also come to use in fields such as ecology to identify and analyze trophic networks, in computer science, as well as in epidemiology (Stattner & Vidot 2011, p.8).']"|0.9999999999|1.0
46|What are the limitations of Stacked Area Plots?|Stacked Area Plots are not suitable for studying the evolution of individual data series.|"['==Overview==\n\nA Stacked Area Plot displays quantitative values for multiple data series. The plot comprises of lines with the area below the lines colored (or filled) to represent the quantitative value for each data series. \nThe Stacked Area Plot displays the evolution of a numeric variable for several groups of a dataset. Each group is displayed on top of each other, making it easy to read the evolution of the total.\nA Stacked Area Plot is used to track the total value of the data series and also to understand the breakdown of that total into the different data series. Comparing the heights of each data series allows us to get a general idea of how each subgroup compares to the other in their contributions to the total.\nA data series is a set of data represented as a line in a Stacked Area Plot.\n\n==Best practices==\n\nLimit the number of data series. The more data series presented, the more the color combinations are used.', '\'\'\'Note:\'\'\' This entry revolves specifically around Stacked Area plots. For more general information on quantitative data visualisation, please refer to [[Introduction to statistical figures]].\n\n\'\'\'In short:\'\'\' \nThis entry aims to introduce Stacked Area Plot and its visualization using R’s <syntaxhighlight lang=""R"" inline>ggplot2</syntaxhighlight> package. A Stacked Area Plot is similar to an Area Plot with the difference that it uses multiple data series. And an Area Plot is similar to a Line Plot with the difference that the area under the line is colored.\n\n==Overview==', 'Consider the order of the lines. While the total shape of the plot will be the same regardless of the order of the data series lines, reading the plot can be supported through a good choice of line order.\n\n==Some issues with stacking==\n\nStacked Area Plots must be applied carefully since they have some limitations. They are appropriate to study the evolution of the whole data series and the relative proportions of each data series, but not to study the evolution of each individual data series.\n\nTo have a clearer understanding, let us plot an example of a Stacked Area Plot in R.\n\n==Plotting in R==\n\nR uses the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> to create Stacked Area Plots.\nThe function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> has the following syntax:', '\'\'\'Syntax\'\'\': <syntaxhighlight lang=""R"" inline>ggplot(Data, aes(x=x_variable, y=y_variable, fill=group_variable)) + geom_area()</syntaxhighlight>\n\nParameters:\n\n* Data: This parameter contains whole dataset (with the different data series) which are used in Stacked Area Plot.\n\n* x: This parameter contains numerical value of variable for x axis in Stacked Area Plot.\n\n* y: This parameter contains numerical value of variables for y axis in Stacked Area Plot.\n\n* fill: This parameter contains group column of Data which is mainly used for analyses in Stacked Area Plot.', '<syntaxhighlight lang=""R"" line>\n#Fig.1\nggplot(uspopage, aes(x = Year , y = Thousands, fill = AgeGroup)) +\n  geom_area()\n</syntaxhighlight>\n\nFrom this Stacked Area Plot, we can visualize the evolution of the US population throughout the years, with all the age groups growing steadily with time, especially the population higher than 64 years old.\n\n==Additional==\nAdditionally, we can play with the format of the plot. To our previous example, we will reduce the size of the lines, scale the color of the filling to different tones of “Blues”, and add labels.', 'Now, we will plot the Stacked Area Plot in R. We will need the following R packages:\n[[File:stckarea.png|450px|thumb|right|Fig.1: An example of the stacked area plot.]]\n[[File:stcharea.png|450px|thumb|right|Fig.2: Stacked area plot after customization.]]  \n<syntaxhighlight lang=""R"" line>\nlibrary(tidyverse)  #This package contains the ggplot2 needed to apply the function geom_area()\nlibrary(gcookbook)  #This package contains the dataset for the exercise\n</syntaxhighlight>\n\nPlotting the dataset <syntaxhighlight lang=""R"" inline>""uspopage""</syntaxhighlight> using the function <syntaxhighlight lang=""R"" inline>geom_area()</syntaxhighlight> from the <syntaxhighlight lang=""R"" inline>ggplot2 package</syntaxhighlight>:', 'Stacked bar plots should be used for Comparisons and Proportions but with emphasis on Composition. This composition analysis can be static for a certain moment in time, or dynamic for a determined period of time.\n\nStacked bar Plots are two-dimensional with two axes: one axis shows categories, the other axis shows numerical values. The axis where the categories are indicated does not have a scale (*) to highlight that it refers to discrete (mutually exclusive) groups. The axis with numerical values must have a scale with its corresponding measurements units.\n\n\n===When you should use a stacked bar plot===\nThe main objective of a standard bar chart is to compare numeric values between levels of a categorical variable. One bar is plotted for each level of the categorical variable, each bar’s length indicating numeric value. A stacked bar chart also achieves this objective, but also targets a second goal.']"|0.30952380950833336|1.0
47|What is the purpose of Thought Experiments?|"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."|"[""|}\n<br/>__NOTOC__\n<br/><br/>\n'''In short:''' Thought Experiments are systematic speculations that allow to explore modifications, manipulations or completly new states of the world."", '== What the method does ==\nThought Experiments are the philosophical method that asks the ""What if"" questions in a systematic sense. Thought Experiments are typically designed in a way that should question our assumptions about the world. They are thus typically deeply normative, and can be transformative. Thought Experiments can unleash transformation knowledge in people since such experiments question the status quo of our understanding of the world. The word ""experiment"" is insofar slightly misleading, as the outcome of Thought Experiments is typically open. In other words, there is no right or wrong answer, but instead, the experiments are a form of open discourse. While thus some Thought Experiments may be designed to imply a presumpted answer, many famous Thought Experiments are completely open, and potential answers reflect the underlying norms and moral constructs of people. Hence Thought Experiments are not only normative in their design, but especially in terms of the possible answers of results.', ""All this exemplifies that Thought Experiments are deeply normative, and show a great flexibility in terms of the methodological design setup in space and time. Some of the most famous Thought Experiments (such as the [https://en.wikipedia.org/wiki/Teletransportation_paradox tele-transportation paradox]) are quite unconnected from our realities, yet still they are. This is the great freedom of Thought Experiments, as they help us to understand something basically about ourselves. '''Thought Experiments can be a deeply transformational methods, and can enable us to learn the most about ourselves, our choices, and our decisions.'''"", 'Thought Experimenters. The same holds true for many other people, and just as our norms and values change, the value of specific Thought Experiments can change over time. Thought Experiments are like a reflection, and any reflection can be blurry, partly, bended, or plain wrong - the last case, if we cannot identify our reflection in the mirror of Thought Experiments.', 'So far, we see that there are Thought Experiments that resolve exclusively about a - subjective - human decision, and other types of Thought Experiments that are designed around setting in the physical world. The difference between these two is in the design of the experiment itself, as the first are always focused on the normative decisions or people, while the second focuses on our normative interpretation of anticipation of a design that is without a human influence. This distinction is already helpful, yet another dimension is about time. Many Thought Experiments are independent of time, while others try to reinterpret the past to make assumptions about a future about which we have no experience. Thought Experiments that focus on re-interpretation of the past (""What if the assassination of Franz Ferdinands failed? Would the first World War still have happened?"") look for alternative pathways of history, often to understand the historical context and - more impotantly - the consequences of this context better. Most Thought Experiments are independent of', '- more impotantly - the consequences of this context better. Most Thought Experiments are independent of a longer time scale. These experiments - such as the Trolley experiment - look at a very close future, and are often either very constructed or lack a connection to a specific context. Thought Experiments that focus on the future are often build around utopian or at least currently unthinkable examples that question the status quo, either form an ethical, societal, cultural or any other perspective. Such desirable futures are deeply normative yet can build an important bridge to our current reality through backcasting (""What if there is no more poverty, and how can we achieve this?"").', ""The core strengths of Thought Experiments is to raise normative assumptions of about the world, and about the future. Thought Experiments can thus unleash a transformative potential within individuals, as people question the status quo in their norms and morals. Another strength of Thought Experiments is the possibility to consider different futures, as well as alternatives of the past. Thought Experiments are thus as versatile and flexible as people's actions or decision, and the ''What if'' of Thought Experiments allows us to re-design our world and make deep inquiries into alternative state of the world. This makes Thought Experiments potentially time-saving, and also resource-efficient. If we do not need to test our assumptions in the real world, our work may become more efficient, and we may even be able to test assumptions that would be unethical in the real world. '''Schrödinger's Cat experiment is purely theoretical, and thus not only important for physics, but also better for the cat.''' This latest strength is""]"|0.8333333333055556|1.0
48|What is temporal autocorrelation?|Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.|"['We can see larger electricity usage at the end and the beginning of the time period. However, no useful interpretation can be made. To explain this process, we might have to look at larger time frames or add other information, such as the hours spent at home (and when it is dark), days in home office, temperature (if heating requires electricity), and many other.\n\n===Autocorrelation===\nAutocorrelation measures the degree of similarity between a given time series and a lagged version of itself. High autocorrelation occurs with periodic data, where the past data highly correlates with the current data.', '== Temporal grain and measures of time ==', ""Thus, let us focus on the main obstacle to this bright future that we have faced ever since we began: ''Temporal autocorrelation.'' This principle defines that we humans value everything that occurs in the close past or future to be more relevant than occurrences in the distant future or past. This is even independent of the likelihood whether future events will actually happen. As an example, imagine that you want to have a new computer every few years, and you can pay 5€ to have a 50% chance to get a new computer tomorrow. If you actually needed one, you would surely do that. However, even adjusted for inflation, many people would not pay the same 5€ to have a 50 % chance to win the latest computer in 10 years. What difference does it make? It is the latest tech in any case. '''Temporal discounting is one of the main ways how people act unreasonably'''. This even extends well beyond the point where we are dead"", '== Types of temporal changes ==', 'The autocorrelation largely ranges between -0.2 and 0.2 and is considered to be a weak autocorrelation and can be neglected.\n\n<syntaxhighlight lang=""Python"" line>\nimport matplotlib.pyplot as plt ## imports necessary functions to create a plot\nfrom statsmodels.graphics.tsaplots import plot_acf ## imports functions to calculate the confidence intervals (the so-called autocorrelation function)', 'seconds. Both patterns can be explained by regressions, but the temporal horizon of the analysis makes them almost incomparable. Such effects can be described as phase shifts, and the standard regression is not able to meaningfully combine such changes into one model.', '== Temporal relativity ==']"|0.9999999999|1.0
49|What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?|The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models.|"['* The research project [http://besatz-fisch.de/content/view/34/57/lang,german/ ""Besatzfisch""] is a good example of a long-term transdisciplinary research project that engages with different methodological approaches. This four year project attempted to \'\'\'understand the ecological, social and economic role and effects of stocking fish in natural ecosystems.\'\'\' First, fish was introduced to ecosystems and the subsequent population dynamics were qualitatively & quantitatively measured, much of this jointly with the cooperating anglers (\'\'Cooperation\'\'). Second, anglers were questioned about fish population sizes and their economic implications (\'\'Consultation\'\') before the data was analyzed using monetary modelling. Third, decision-making processes were modelled based on conversations with anglers, and their mental models about fishing were evaluated (\'\'Consultation\'\'). Fourth, participatory workshops were conducted to help anglers optimize their fishing grounds (\'\'Empowerment\'\'). Fifth, social-ecological models', '* Das Forschungsprojekt [http://besatz-fisch.de/content/view/34/57/lang,german/ ""Besatzfisch""] ist ein gutes Beispiel für ein langfristiges transdisziplinäres Forschungsprojekt, das sich mit unterschiedlichen methodischen Ansätzen beschäftigt. In diesem vierjährigen Projekt wurde versucht, \'\'\'die ökologische, soziale und wirtschaftliche Rolle und die Auswirkungen des Besatzfischs in natürlichen Ökosystemen zu verstehen\'\'\'. Zunächst wurden Fische in die Ökosysteme eingeführt und die nachfolgende Populationsdynamik qualitativ und quantitativ gemessen, vieles davon gemeinsam mit den kooperierenden Anglern (""Kooperation""). Zweitens wurden die Angler*innen über die Größe der Fischpopulationen und ihre', ""Zweitens wurden die Angler*innen über die Größe der Fischpopulationen und ihre wirtschaftlichen Auswirkungen befragt (''Konsultation''), bevor die Daten mit Hilfe von monetären Modellen analysiert wurden. Drittens wurden Entscheidungsprozesse auf der Grundlage von Gesprächen mit den Angler*innen modelliert und ihre mentalen Modelle über das Angeln ausgewertet (''Konsultation''). Viertens wurden partizipative Workshops durchgeführt, um die Angler bei der Optimierung ihrer Fischgründe zu unterstützen (''Empowerment''). Fünftens wurden auf der Grundlage der bisherigen empirischen Ergebnisse sozial-ökologische Modelle entwickelt (''Konsultation''). Zuletzt werden die Projektergebnisse in verschiedenen Medien für unterschiedliche Zielgruppen veröffentlicht"", ""anglers optimize their fishing grounds (''Empowerment''). Fifth, social-ecological models were developed based on the previous empirical results. (''Consultation'') Sixth, the project results are published in different forms of media for different target groups (''Information'')."", ""Aus einer bestimmten Perspektive ist unser ganzer Planet ein natürliches Experiment, und es ist auch aus statistischer Sicht ein Problem, für das wir keine Wiederholungen haben, abgesehen von anderen Verzweigungen und Unklarheiten, die sich aus solchen Einzelfallstudien ergeben, die jedoch oft auch in kleinerem Maßstab zunehmend relevant sind. Mit der Zunahme qualitativer Methoden in Vielfalt und Fülle und dem Drang, auch komplexe Systeme und Fälle zu verstehen, besteht eindeutig ein Bedarf an der Integration von Wissen aus natürlichen Experimenten. '''Aus statistischer Sicht sind solche Fälle schwierig und herausfordernd, da sie nicht reproduzierbar sind, aber das Wissen kann dennoch relevant, plausibel und gültig sein.''' Zu diesem Zweck proklamiere"", 'Um Nietzsche zu zitieren: ""Nie gab es eine so neue Morgenröte und einen so klaren Horizont, und ein so offenes Meer.""\n\n\n== Additional Information ==\n* Hanspach et al. 2014. \'\'A holistic approach to studying social-ecological systems and its application to southern Transylvania\'\'. Ecology and Society 19(4): 32.\nZeigt eine Kombination verschiedener Methoden in empirischer Forschung auf, darunter Scenario Planning, GIS, Causal-Loop Diagrams).', 'Empirical: Euliss Jr, N. H., Swanson, G. A., & MacKay, J. (1992). Multiple tube sampler for benthic and pelagic invertebrates in shallow wetlands. The Journal of wildlife management, 186-191.\n\n==References==\n(1) Furse, M. T., Wright, J. F., Armitage, P. D., & Moss, D. (1981). An appraisal of pond-net samples for biological monitoring of lotic macro-invertebrates. Water Research, 15(6), 679-689.\n\n(2) Baird, D. J., & Hajibabaei, M. (2012). Biomonitoring 2.0: a new paradigm in ecosystem assessment made possible by next‐generation DNA sequencing.']"|0.99999999995|1.0
