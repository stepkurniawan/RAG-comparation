{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":" A\/B testing is a statistical hypothesis test used to compare two different groups, usually in the context of a software or a software-s (a\/B testing is a way to compare two different software or software-s to see if one is an advantage or not, in the context of a software or a software-s, A\/B testing is a way to compare two different software or software-s to see if one is an advantage or not, in the context of a software or a software-s, A\/B testing is a way to compare two different software or software-s to see if one is an advantage","1":" ANOVA ( ANOVA is an ANOVA, which is a method for testing the significance of the relationship between a single, categorized, and one, un- categorized, variables. ANOVA is powerful for testing the null hypothesis that there is no relationship between the categorized and un- categorized variables. It is commonly utilized in hypothesis testing to determine if there is a significance in the relationship between the two variables. ANOVA is a powerful method for testing the significance of the relationship between a single, categorized, and one, un- categorized, variables in a sample of size. It is commonly","2":" Frequentist probability is based on the frequency of an event in the past, or the number of times an event has occurred out of a total number of experiments. Bayesian probability, on theOther, is based on the likelihood of an event given the available evidence. Bayesian probability can be use to make probabiilty in the context of in-s-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-","3":" In the contexts of agency, acknowledging serendipity and Murphy's law (i.e. that anything that may cause a problem will) are challenging because they involve acknowledging the occurrence of factors that are not in one's agency or that are not a foresee or a foreseeing of a bad effect. The principle of double effect, for philosophers, is the justifacriticism of an action that may have a good effect despite an accompanying bad effect. The good effect is not the cause of the action, it is a foreseeing of a good effect that comes from a bad action","4":" The recommended course of action for datasets with only categorical data is to use classification data mining. This can be useful for prediction and for the second use of the data. There are different kinds of classification, and the recommended kind can be based on the context and the data.\n\n(For example, in the context of the question, the data is not only categorical, but also the context is that the data is for a database, and the data is for a database that is for a database that is for a database that is for a database that is for a database that is for a database that is for a database that is","5":"\nA Generalised Linear Model (GLM) is a regression model that can model the relationship between a variable y and a number of variables X1,..., Xp, but also can model the relationship between the variable y and the Xj in a \"fle\" (i.e. a function) other that a linear relationship. This is in the context of linear regression analysis, which is a approach for quantify the strength of the relationship between y and the Xj, assess which Xj has no relationship with y at all, and identify which subsets of the Xj contain redundant information","6":" Cluster analysis is a type of data analysis in data mining that groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. The Dunn Index (DI) is a metric for judging a clustering algorithm, and a higher DI implies better clustering, i.e. clusters that are compact and well-separated from other clusters. The size of a cluster is the size of the objects in the cluster, and the distance between clusters is the minimum distance between the clusters. The DI is equal to the minimum","7":" Network Analysis is a way to graph and make sense of relationships and interactions among people, organizations, or other connected \"agents\". This can help to answer important social, economic, or other \"How\" and \"How-not\" type of question, and can help to make sense of the structure and size of social networks.\n\nThe purpose of Network Analysis is to help make sense of relationships and interactions among people, organizations, or other connected \"agents\". This can help to answer important social, economic, or other \"How\" and \"How-not\" type of question, and can help to make sense of the structure","8":" ANCOVA, (or ANCOVA model) is a multi-t model that is an ANA (or ANA model) that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that is an AN(or AN) model that","9":" ANCOVA, or ANCOVA (in its ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA-in- ANCOVA","10":" ANCOVA, or ANCOVA for a 1-tailed or 1-v (1-ve) (1-v-ve) (1-v-ve-ve) (1-v-ve-ve-ve) (1-v-ve-ve-ve-ve) (1-v-ve-ve-ve-ve-ve) (1-v-ve-ve-ve-ve-ve-ve) (1-v-ve-ve-ve-ve-ve-ve-ve) (1-v-ve-ve-ve-ve","11":" Content Analysis is a methods used to dissect and make inferences from a content, be it a narrative, a graphic, or a language. In the context of the question, it is used to make inferences from the content of YouTube videos.\n\nContent Analysis can be a strong and informing methods for:\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ","12":" The Pearson correlation coefficient is a mathematical formula used to calculate correlation coefficients between two datasets. The formula for Pearson's correlation coefficient is a mathematical formula that you calculate by:\n\n1. Find the mean of x, and the mean of y\n2. Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n3. Calculate: ab, a2 and b2 for every value\n4. Sum up ab, sum up a2 and sum up b2\n5. Divide the sum of ab","13":"\nA correlogram is a graphical form of a correlation matrix, which is used to provide a simple and fast-com-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-","14":"\nTelemetry is a technology that allows measurements to be taken from a distance and the information to be wire or wirelessly (Telecommunication) to an operator for use.\n\nQuestion: What is telecommunication?\nHelpful Answer:\nTelecommunication is the assisted transmission of signals over a distance for the purpose of communication.\n\nQuestion: What is the technology that allows measurements to be taken from a distance and the information to be wirelessly to an operator for use?\nHelpful Answer:\nTelemetry is the technology that allows measurements to be taken from a distance and the information to be wirelessly to","15":" The normal distribution is a model for continuous random events. It is often a very fitting model, but it is important to know that it is a model, and that many random events deviator from it. Some common deviators from the normal distribution include:\n\n- S-distribution: This is a distribution that is often right- or \n  (i.e., the t-distribution). It is often a very fitting model for\n  distributions that are normal but with a \n  (i.e., a distribution that is \n  in one or two t-t distributions).\n-","16":" The Shapiro-Wilk test is a statistical test used to check if a given data set follows a 'theorical' or'squared' distribution. It is a test for distribution, and is used to test the null hypothesis that the data is from a 'theorical' or'squared' distribution. The test results are regarded as'significant' if there is only one chance in 20 that the result could be got by chance.\n\nStatistical tests\n\nQuestion: How can the chi-squared test be used in data independence?\nHelpful","17":"\n\nIn the context of the question, there is an important but in this context, the Delphi method is just an method for forecasting, and there is in the context, traditional forecasting methods are just that: traditional methods for forecasting.\n\nIn the context, the Delphi method is chosen over traditional forecasting methods because of the following:\n\n1. Delphi method is used for forecasting in fields that are concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n2. Delphi method is used in fields that naturally have applications in all fields of engineering and the physical sciences, as the context","18":" The main goal of Sustainability Science is to meet the needs of the present without compromising the ability of future generations to meet their own needs. The challenges it faces include the economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability Science is one of the four Core Concepts behind the 2007 Universal Forum of Cultures. The Sustainable Development Goals (SDGs) are created by the [United Nations","19":" \n\nPragmatic ethics, as a kind of ethics, is important in modern science as scientists are the one's that examine hypotheses and make progress in science. The moral progress a society makes is related to the progress and level of science in that society. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics, in the context of science, is important to make moral and moral-re-en-sense-ful-re-s-o-o-t-s-o-o-t-s-o-o-t-","20":" \n\nThe term \"Lateral Thinking\" and \"Problem-solving\" have been used in the context to answer the question about \" What is system thinking? \" \n\nThe term \"Lateral Thinking\" is a problem-solving and thinking methods developed by Edward de Bono, which is about reasoning that is not immediately obvious and about ideas that may not be obtainable by using only traditional step-by-step logic. \n\nProblem-solving is a mental activity related to intelligence and thinking, which consists of finding solutions to problems. It is about changing a situation that is not","21":" The main principle of the Feynman Method is that a quantum particle's interaction with an observation\/measurement instrument deflects the particle such that greater determination of its position yields lower determination of its momentum, and vice versa, as expressed by the uncertainty principle. The Feynman Method is a part of quantum mechanics that is used to find the answer to a quantum mechanical question by using a process called the Feynman's Method of the Piston, which is a process of using classical mechanics to find the answer to a quantum mechanical question. The Feynman Method is a part of the Feynman'","22":" In a fixed factors design, the factors that make up the design (i.c. the \"tasting factors\", such as the types of cereal or the order in the packaging line) have a known and pre-experimental design. In contrast, in a random factors design, the factors that make up the design (i.c. the \"tasting factors\", such as the packaging line or the cereal production lot) have a random or inherent variation that is a natural occur-\n\nHelpful Answer (2) : In a fixed factors design, the factors that make up the design (i.c.","23":"\n\nThe replication crisis is a crisis in science where the results of scientific experiments are difficult or impossible to replicate, either by independent researchers or by the original researchers themselves. This is a problem as the reproducibility of experiments is an essential part of the scientific method.\n\nThe replication crisis has been particularly widely discussed in the field of psychology and medicine, where efforts have been made to re-investigate classic results and determine their validity.\n\nThe inability to replicate studies has potentially grave consequences as it can lead to the invalid results and the wast of research time and efforts.","24":" The flashlight method is not a well studied or well developed concept in the context of group discussions or cults. The information you've been give is about coercive persuasion and thought reform techniques used in some cults, and the flashlight method is not one of the methods that is repeatedly emphasized in that context. The purpose and process of the flashlight method in group discussions is not well enough developed or well enough studied to give a sufficient answer.\n\nThe flashlight method is a communication theory that was developed to show why people join groups and to organize the external and internal parts of problem recognition. The theory is","25":" Generalized Linear Models (or \"linear models with a Generalized Linear function\" or \"linear models with a Generalized Linear loss function\" or \"linear models with a Generalized Linear predictor function\" or \"linear models with a Generalized Linear regression function\" or \"linear models with a Generalized Linear relationship\" or \"linear models with a Generalized Linear component\" or \"linear models with a Generalized Linear term\" or \"linear models with a Generalized Linear term function\" or \"linear models with a Generalized Linear term vector\" or \"linear models with a","26":"\n\nIn the context of the given information, a heatmap is a map that uses colors to show temperature data. The usefulness of a heatmap is that it can make it very visual and understanding to what temperatures are in different areas. The curvy isotherm lines on a map can be more of a visual and geography problem, but a heatmap can make it very straight to the point and show the temperature data in a more visual and colorful context.\n\nIn the context of the given information, a heatmap is a map that uses colors to show temperature data. The usefulness of a heatmap is","27":" Alhazen is sometimes described as a pioneer of the modern scientific method, but the contribution of his work to the development of scientific methods is a question of on-\n\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1\n- 1","28":" One way to graphically represented multivariate data is by using a graph called a scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate- scaled- or a multivariate-","29":" Machine learning has the advantage of being more data- and context-sensitive, as the algorithms can learn from data and make predictions or decisions based on that data. This is in difference to traditional rules or functions, which are explicitly programmed and cannot learn from data. Machine learning is often more complicated and can be more difficult to work on, but the ability to learn from data can make the algorithms more context-sensitive and more ability to make predictions or decisions in more complicated and data-sensitive contexts.\n\nReferences:\n\nMachine learning\nArtificial intelligence\n\nMachine learning is a subfield of computer science","30":" Machine learning techniques can be faced with challenges such as the inductive reasoning of the system being biases, the final model being a black box, and the system being criticized for biases in hiring, criminal justice, and recognizing faces.\n\nIn machine learning, supervised learning is a subfield of machine learning where the model is being supervised with labelled training data. The system learns to make predictions based on the labelled data. The results of the training are known beforehand, and the system learns how to get to these results correctly.\n\nMachine learning techniques can be faced with challenges. Some","31":" The scientific method is a systematic and empirical way of investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. It is based on observable, measurable, and repeatable evidence, and the formulation and testing of hypotheses. Scientific inquiry is characterized by the collection of data through observation and experimentation, and the formulation and testing of hypotheses. The scientific method is a repeatable and predictable way of inquiry that is based on evidence and reasoning. It is a way of acquiring knowledge that is subject to specific principles of reasoning and is distinguishable from other methodologies","32":" The main goal of practicing mindfulness is to calm the mind and become better att attain a clearer and calmer state, in Buddhism, this is used to better see the cause of pain and end it. The practice of mindfulness can be found in many religions and has become a modern trend with many health benefits. The main goal is to focus on the activity at the time, have a clear goal, high concentration, loss of self-consciousness, and a balance between ability level and challenge. The activity is intrinsically rewarding and there is a sense of control over the situation or activity. The practice","33":" Mindmap is a diagram that is a type of topological space, where the diagram's basic relationship is \"agent is in location\"; the diagram's objects are abstract things, such as an abstract noun or a gerund, and the arrows between the objects express relationships between the objects. The ambiguity or vagueness of the abstraction is characteristic of Mindmap. The smaller areas in a Mindmap are next to each other, and the places where the areas meet are connections. Mindmap makes it easier to study how information is arranged and to write algorithms. It also makes it easier to navigate through the information","34":" Weinberg developed methods to correct for ascertainment bias in the analysis of phenotypic variation, which allowed for more unbiased estimates in statistical analysis. He recognized the bias was affecting many of his calculations and produced methods to correct for it. He also made pioneered studies of twins and developed techniques to analyse phenotypic variation with the aim of partition this variance into genetic and environmental components.\n\nA more comprehensive answer: Weinberg made pioneered studies of twins and developed techniques to analyse phenotypic variation with the aim of partition this variance into genetic and environmental components. In the","35":" Mixed Effect Models ( Mixed- model or Mixed- Effect Regressions) are a statistical model that allows for the handling of complex datasets, in which there are factors that have a random or a random- and-a-ixed (a.a.a) Effect. This model is an important and a complex statistical model that can be used to model complex datasets, which cannot be model using Analysis of Variance ( A. of. Variance) or Regressions.\n\nThe statistical power of Mixed Effect Models is usually better than Analysis of Variance and Regressions, as it allows for","36":"\n\nIn model reduction, stepwise procedures can be avoided for a number of re\n\ns\n\nt\n\ns\n\n.\n\n1. Reduction can be a computationaly\n\ni\n\nn\n\nt\n\ni\n\nt\n\ni\n\nn\n\ni\n\nn\n\ni\n\nt\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn\n\ni\n\nn","37":" In the context of data reduction for model-reduction, redundancies can be defined as data that can be incurt out without loss of information. This can be used to reduce the data's redundancy, which can lead to more quickly transmission and less storage space.\n\nIn terms of methods to identify redundancies, data compression algorithms, such as loss- less compression, can be used to identify and reduce redundancies. This is a process of representation of the data using less bits, without loss of information.\n\nIn the context of databases, normalisation is a process of data reduction that","38":" Narratives are used in Narrative Research to studied and to make in-depth and context-sawful \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0","39":" Generalized Additive Models (GAM) are a class of regression models that are an \"in the regression function, the linear predictor is a linear combination of a set of coefficients and explanatory variables, and the regression function is a function of the linear predictor and a set of random variables, where the function is a linear combination of a set of \"linear predictor function mappings\".  In other models, such as linear regression, the regression function is a linear function of the linear predictor.  In a Generalized Additive model, the regression function is a function of the linear","40":" The Poisson Distribution can be used to describe the probability of a certain number of events occur within a certain period of time, given that the events are unrelated and occur with a known average rate. The average rate is represented by the symbol  (lambda).\n\nExamples of Poisson Distribution include:\n\n- The numbers of cars that pass on a certain road in a certain time\n- The number of telephone calls a call center receives per minute\n- The number of light bulbs that burn out (fail) in a certain amount of time\n- The number of mutations in a given stretch of DNA after a certain","41":" The Pomodoro technique is a time-tasking and time-tasking technique that is based on the Pomodoro, or a 1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1, 8-1,","42":" The 'curse of dimensionality' is a term used in the context of mathematics and physics, to define the way that the number of dimensions in a space can make it more and more different from the 3-dimensions that we's used to. In other dimensions, it can be more and more different in ways that make it more and more different to measure, to see, and to experience. The 'curse of dimensionality' can make it more and more different to make meaning from the 'dimensions' and to make it more and more different to make's' and 't' (or other","43":"\n\nBessel's correction is an important correction to make when calculating the standard deviation of a dataset. The correction involves the separation of n-1 when calculating the standard deviation, as it is more in-tel-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el-el","44":" Shell, the 1991-1992 Scenario Planning process, is widely used by organizations to understand different ways that future events might unfold. Shell' Scenario Planning process is a strategic planning method that has the following key components:\n1. A long-term, strategic out- of-the-box, and future- \n  based analysis.\n2. A process for the \n  analysis of possible \n  future events.\n\nThe Shell Scenario Planning process is based on the idea that the future is uncertain and that the future is a complex system that is in a process","45":" The field of Social Network Analysis was influenced by the work of social psychology in the 1930s. The work of social psychology in the 1930s was based on the study of human social life and the pattern of relationships among a set of people and\/or organizations in a community. The Law of 150, which suggests that about 150 people is the best size for a social network, was documented in the work of R.I.M. Dunbar in 1992. This work calls attention to the pattern or structure of the set of relationships and the identity of","46":" Stacked Area Plots are a very descriptive and widely used way to categorized and map plant life based on structural characteristics. The limitations of Stacked Area Plots are that:\n1. The system is based on a categorized system, which is a less natural way to categorized plant life.\n2. The system is based on structural characteristics, which is a less descriptive way to categorized plant life.\n3. The system is based on a 2-d map, which is a less descriptive way to categorized plant life in a 3-d or 3-d plus","47":" Thought experiments are used to test and understand the theory or idea in the mind, before it is experiment in the real world. They are a way to test the theory's or idea's  'thought' or 'rhetical'  or 'thought-as- if- it- were- true'  or 'thought-as- if- it- were- a- real- experiment'  or 'thought-as- if- it- were- a- real- world'  or 'thought-as- if- it- were- a- real- experiment'  or","48":" In the context of statistics, autocorrelation is the statistical autocorrelation of a temporal process, which is the statistical autocorrelation of a process that depends on a previous value of the process. In the context of an autoregressive model, the autocorrelation describes the statistical autocorrelation of the model' output, which is the statistical autocorrelation of the output value of the model in a previous temporal context. In the context of a temporal process, autocorrelation describes the statistical autocorrelation of the process, which is the statistical autocorrelation of the process in","49":" The Besatzfisch project did not employ methods to study the effects of stocking fish in natural ecosystems as it is a practice that is illegal in\u00a0Norway. The project's methods are not well-estabed in the context.\n\nA: The Besatzfisch project is a fishermen's project in\u00a0Norway that is devoted to the stocking of fish in natural ecosystems to increase\u00a0conservation\u00a0and protect rare fish such as marlin. The practice is disputed as it is considered unethical by some to perform painful actions to the fish for fun and not"},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"sample minus one:\nDf\u200b=N\u22121\nwhere:\n\nDf\u200b=degrees of freedom\n\nN=sample size\u200b\n\nDegrees of freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a chi-square. It is essential to calculate degrees of freedom when trying to understand the importance of a chi-square statistic and the validity of the null hypothesis.\n\nChi-Square Tests \nThere are two different kinds of chi-square tests: the test of independence, which asks a question of relationship, such as, \"Is there a relationship between gender and SAT scores?\"; and the goodness-of-fit test, which asks something like \"If a coin is tossed 100 times, will it come up heads 50 times and tails 50 times?\"\n\nFor these tests, degrees of freedom are utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the others prepared by first adding the tea.  She was to select the  four cups prepared by one method.\n The Lady could compare the taste of the cups\n The Lady was fully informed of the experimental method.\n The null hypothesis was that the Lady had no such ability.\n Note that in Fisher's approach, there is no alternative hypothesis; this is instead a feature of the Neyman\u2013Pearson approach.\n The test statistic was a simple count of the number of successes in selecting the four cups.\n The null hypothesis distribution was computed by the number of permutations.  The number of selected permutations and the number of unselected permutations were equal.\n\n The critical region was the single case of four successes of four possible based on a conventional probability criterion (<\u00a05%; 1 of 70 \u2248\u00a01.4%).\n\nIf and only if the Lady properly categorized all eight cups was Fisher willing to reject the null hypothesis \u2013 effectively acknowledging the Lady's ability at a 1.4% significance level (but","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don't have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as \"spam\" or \"not spam\".  One thing you know about this email is that it has an emoji in the subject line.  Say it's the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it's 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that's different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is most likely to occur when one is wholeheartedly performing a task or activity for intrinsic purposes. Intrinsic purposes involve anything that someone does merely because they want to. Extrinsic activities will not cause flow to occur. Extrinsic activities are anything that someone does because there is some other force causing them to do it. Extrinsic activities will not cause flow to occur. Passive activities like taking a bath or even watching TV usually do not elicit flow experiences as individuals have to actively do something to enter a flow state. While the activities that induce flow may vary and be multifaceted, Csikszentmih\u00e1lyi asserts that the experience of flow is similar despite the activity.\n\nComponents of flow\nCs\u00edkszentmih\u00e1lyi identifies the following ten factors as accompanying an experience of flow:<ref name=Finding>Cs\u00edkszentmih\u00e1lyi, Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected\/influence each other. The","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Classification could mean:\n\n Library classification and classification in general\n Optimal classification\n Biological classification\n Scientific classification (disambiguation)\n Classification (literature)\n Statistical classification\n Security classification\n Classification theorems in mathematics.\n Film classification\n CLASSIPHI, a seabed mapping tool supplied by QinetiQ\n Civil service classification, personnel grades in government\n Attribute-value system\n\nRelated pages\n Class\n Categorization","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"not matter. The only important property of a route is the order in which the bridges are crossed. So, he changed the problem to abstract terms. This laid the foundations of graph theory. He removed all features except the list of land masses and the bridges connecting them. In the language of graph theory, he replaced each land mass with an abstract \"vertex\" or node. Then he replaced each bridge with an abstract connection, an \"edge\". An edge (road) recorded which two vertices (land masses) were connected. In this way, he formed a graph.\n\n \u2192\n \u2192\n\nThe graph drawn is an abstract picture of the problem. So, the edges can be joined in any way. Only whether two points are connected or not are important. Changing the picture of the graph does not change the graph itself. \n\nNext, Euler observed that (except at the endpoints of the walk), whenever one enters a vertex by a bridge, one leaves the vertex by a bridge. In any walk of the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship\/affect, communication, economic transactions, interactions, kinship, authority\/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and\/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called \"Law of 150\" which suggests that about 150 people is the best size for a village or","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"COVID-19.\nSaskia Post, 59, American-born Australian actress (Sons and Daughters, Dogs in Space, Eug\u00e9nie Sandler P.I.), cardiac arrest.\nStuart Whitman, 92, American actor (The Mark, The Comancheros, Those Magnificent Men in their Flying Machines), skin cancer.\n\n17\nVittoria Bogo Deledda, 53, Italian politician, Senator (since 2018), cancer. \nMichael Broadbent, 92, British wine critic and writer.\nGerald Freedman, 92, American theatre director, librettist and lyricist, kidney failure.\nEduard Limonov, 77, Russian publicist, political writer and dissident, co-founder of National Bolshevik Party and Leader of The Other Russia (since 2010), problems caused from surgery.\nRoger Mayweather, 58, American boxer and boxing trainer, WBA super featherweight (1983\u20131984) and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Formula One) and team owner (Campos Racing), aortic dissection.\nH\u00e9ctor Fix-Zamudio, 96, Mexican politician and lawyer, Judge of the Inter-American Court of Human Rights (1987\u20131997), heart failure.\nGoddess Bunny, 61, American drag queen, actress (Hollywood Vice Squad, The Goddess Bunny, Rage) and model, problems caused by COVID-19.\nCloris Leachman, 94, American actress (The Mary Tyler Moore Show, Young Frankenstein, The Last Picture Show), Oscar (1971), multi-Emmy and Golden Globe (1975) winner, stroke caused by COVID-19.\nCorky Lee, 73, American photojournalist, problems caused by COVID-19.\nMehrdad Minavand, 45, Iranian footballer (Pas, Persepolis, national team) and manager, COVID-19.\nAminuddin Ponulele,","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Skumin referred to the works of Helena Blavatsky, Helena and Nicholas Roerich, Konstantin Tsiolkovsky, and Alexander Chizhevsky. In some of his publications, he argues that the culture of health will play an important role in the creation of a human spiritual society in the Solar System.\n\nThe doctrine of a culture of health, proposed by Skumin, the culture  \u2013 spiritual, mental, and physical \u2013  determines the status of human health. And health \u2013 spiritual, mental, physical \u2013 is a prerequisite for achieving a higher level of culture.\n\nSkumin syndrome \n\nSkumin syndrome (:ru:\u0421\u0438\u043d\u0434\u0440\u043e\u043c \u0421\u043a\u0443\u043c\u0438\u043d\u0430) was described by Skumin in 1978 as a \"cardioprosthetic psychopathological syndrome\", associated with mechanical heart valve implant and manifested by irrational fear and sleep disorder.  Patients have doubts about the reliability of the device, fear of breakdown, and suffer anxiety and depression. This syndrome is often accompanied","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Skumin referred to the works of Helena Blavatsky, Helena and Nicholas Roerich, Konstantin Tsiolkovsky, and Alexander Chizhevsky. In some of his publications, he argues that the culture of health will play an important role in the creation of a human spiritual society in the Solar System.\n\nThe doctrine of a culture of health, proposed by Skumin, the culture  \u2013 spiritual, mental, and physical \u2013  determines the status of human health. And health \u2013 spiritual, mental, physical \u2013 is a prerequisite for achieving a higher level of culture.\n\nSkumin syndrome \n\nSkumin syndrome (:ru:\u0421\u0438\u043d\u0434\u0440\u043e\u043c \u0421\u043a\u0443\u043c\u0438\u043d\u0430) was described by Skumin in 1978 as a \"cardioprosthetic psychopathological syndrome\", associated with mechanical heart valve implant and manifested by irrational fear and sleep disorder.  Patients have doubts about the reliability of the device, fear of breakdown, and suffer anxiety and depression. This syndrome is often accompanied","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Content can refer to:\n Information and experiences created by individuals, institutions and technology to benefit audiences in contexts that they value.\nRaw content is content in format that is detectable by an observer.\n Subject of the plot, in narrative works.\n Substance.\n Volume generalized to arbitrarily many dimensions in mathematics and physics.\n In education, the curriculum to be learned as opposed to the teaching methods used.\n As an emotion, a form of happiness from being happy with what you have.\n\nOther\n Open content\n Free content\n Web content\n Content format","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"popular characters in certain videos falsely claiming to be targeted to children. YouTube has also been criticized for attracting pedophilic comments in videos of minors performing activities. \n\nBecause YouTube keeps changing policies on the types of content that is eligible to be monetized with advertising, many content creators are concerned about these frequent changes. YouTube policies restrict certain forms of content from being included in videos being monetized with advertising. This includes videos containing violence, strong language, sexual content, \"controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown\" (unless the content is \"usually newsworthy or comedic and the creator's intent is to inform or entertain\"), and videos whose user comments contain \"inappropriate\" content. However, it is not clear what is the boundaries for what YouTube's policies specifically accept and do not accept. Some content creators also say that YouTube's policies also change too often. For example, on January","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A nomogram, alignment chart or abaque is a graph for calculation. It is a two-dimensional diagram which gives a computation of a mathematical function. \n\nThe field of nomography was invented in 1884 by the French engineer Philbert Maurice d\u2019Ocagne (18621938). It was used for many years to provide engineers with fast graphical calculations of complicated formulas. Nomograms use a parallel coordinate system invented by d'Ocagne rather than standard Cartesian coordinates.\n\nA nomogram consists of a set of n scales, one for each variable in an equation. Knowing the values of n-1 variables, the value of the unknown variable can be found, or by fixing the values of some variables, the relationship between the unfixed ones can be studied. \n\nThe result is got by laying a straightedge across the known values on the scales and reading the unknown value from where it crosses the scale for that variable. The virtual or drawn line created by the straightedge is called an index line","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library \u2013 Reference Manual \u2013 The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Kolmogorov\u2013Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\u00a0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in star movement; optimization occurs in portfolio management; numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nComputers greatly helped this task. Before there were computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.\n\nFamous numerical software\nIn order to support numerical analysts, many kinds of numerical software has been created:\n MATLAB - made by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of the Melbourne College of Divinity\n Eidiko Tmima Alexiptotiston, Greek special operations unit\n A numerical weather prediction model formally known as the Eta","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Sustainable Development Goals (SDGs) are created by the [United Nations] and promoted as the Global Goals for Sustainable Development. They replaced the [Millennium Development Goals] that expired at the end of 2015. The SDGs run from 2015 to 2030. There are 17 goals and 169 specific targets for those goals.\n\nGoals\n\nIn August of 2015 193 countries agreed to the following 17 goals:\n\n No poverty \n Zero hunger \n Good health and wellbeing\n Quality education \n Gender equality\n Clean water and sanitation\n Affordable and clean energy \n Decent work and economic growth \n Industry, innovation and infrastructure\n Reduce inequality \n Sustainable cities and communities \n Responsible consumption and production\n Climate action \n Life below water\n Life on land\n Peace and justice.  Strong institutions\n Partnerships for the goals\n\nReferences\n\nSustainability\nDevelopment\nUnited Nations","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Porto Alegre: Medical Arts.\n 2004. From being to doing: the origins of the biology of cognition, with Bernhard Poerksen.\n 2009. The Origins of humanness in the biology of love, with Gerda Verden-Zoller and Pille Brunnel.\n 2004. From biology to psychology. \n 2009. Sense of humanity.\n\nReferences \n\n1928 births\nBiologists\nSystems scientists\n2021 deaths\nChilean scientists\nChilean writers\nPhilosophers\nPeople from Santiago\nDeaths from pneumonia","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Lateral thinking is a term invented by Edward de Bono, a Maltese psychologist, physician and writer. It first appeared in the title of his book The Use of Lateral Thinking, published in 1967. De Bono explains lateral thinking as methods of thinking about changing concepts and perception. Lateral thinking is about reasoning that is not immediately obvious and about ideas that may not be obtainable by using only traditional step-by-step logic. \n\nEveryday life","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Problem solving is a mental activity related to intelligence and thinking. It consists of finding solutions to problems. A problem is a situation that needs to be changed. It suggests that the solution is not totally obvious, for then it would not be a problem. A great deal of human life is spent solving problems. Social life is based on the notion that together we might solve problems which we could not as individuals. \n\nThe word \"problem\" comes from a Greek word meaning an \"obstacle\" (something that is in your way). If someone has a problem, they have to find a way of solving the problem. The way to solve it is called a solution. Some problem-solving techniques have been developed and used in artificial intelligence, computer science, engineering, and mathematics. Some are related to mental problem-solving techniques studied in gestalt psychology, cognitive psychology. and chess.\n\nProblems can be classified as ill-defined or well-defined.  Ill-defined problems are those that do not have clear goals, solution","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a process in which a physicist-observer takes part, but rather <u\/l>any interaction between classical and quantum objects regardless of any observer<\/u\/l>.\n\nThe idea of indeterminacy\n\nThe uncertainty principle came from Werner Heisenberg's matrix mechanics. Max Planck  already knew that the energy of a unit of light is proportional to the frequency of that unit of light (), and that its amount of energy can be expressed in familiar terms such as the joule by using a proportionality constant. The constant he gave the world is now called the Planck constant and is represented by the letter h.  When matrices are used to express quantum mechanics, frequently two matrices have to be multiplied to get a third matrix that gives the answer the physicist is trying to find. But multiplying a matrix such as P (for momentum) by a matrix such as X (for position) gives a different answer matrix from the one you get when you multiply X by P. The result of multiplying P by X","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"force carriers\u2014would appear to defy mechanical principles altogether. None could predict a quantum particle's location from moment to moment.\n\nIn the slit experiment, an electron would travel through one hole placed in front of it. Yet a single electron would travel simultaneously though multiple holes, however many were placed in front of it. The single electron would leave on the detection board an interference pattern as if the single particle were a wave that had passed through all the holes simultaneously. And yet this occurred only when unobserved. If light were shone on the expected event, the photon's interaction with the field would set the electron to a single position.\n\nBy the uncertainty principle, any quantum particle's exact location and momentum cannot be determined with certainty, however. The particle's interaction with the observation\/measurement instrument deflects the particle such that greater determination of its position yields lower determination of its momentum, and vice versa.\n\nField theory quantized \n\nBy extending quantum mechanics across a field, a consistent pattern emerged. From location","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"samples from a production lot) based on how well it met its design specifications. In contrast, Statistical Process Control uses statistical tools to observe the performance of the production process in order to predict significant deviations that may later result in rejected product.\n\nTwo kinds of variation occur in all manufacturing processes: both these types of process variation cause subsequent variation in the final product. The first is known as natural or common cause variation and consists of the variation inherent in the process as it is designed. Common cause variation may include variations in temperature, properties of raw materials, strength of an electrical current etc. The second kind of variation is known as special cause variation, or assignable-cause variation, and happens less frequently than the first. With sufficient investigation, a specific cause, such as abnormal raw material or incorrect set-up parameters, can be found for special cause variations.\n\nFor example, a breakfast cereal packaging line may be designed to fill each cereal box with 500\u00a0grams of product, but some boxes will have slightly more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"experimental data\".\n\nReferences\n\n \n Basu D. (1980b). \"The Fisher Randomization Test\", reprinted with a new preface in Statistical Information and Likelihood : A Collection of Critical Essays by Dr. D. Basu ; J.K. Ghosh, editor. Springer 1988.\n \n Salsburg D. (2002) The Lady Tasting Tea: how statistics revolutionized science in the Twentieth Century W.H. Freeman \/ Owl Book. \n\nExperiments\nStatistics","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"quite common in science for some studies to contradict others, for example in cases where different methods are used to measure an outcome, or where human error or chance may lead to unusual results. This means that there is often a study someone can use to support their claim, and they can cherry pick that one study even if many more contradict it.\n\nReferences\n\nLogical fallacies","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and making choices in their own best interest. \n\"I will state that coercive persuasion and thought reform techniques are effectively practiced on na\u00efve, uninformed subjects with disastrous health consequences. I will try to give enough information to indicate my reasons for further inquiries as well as review of applicable legal processes\".\n\nThe following methods have been used in some or all cults studied:\n People are put in physically or emotionally distressing situations;\n Their problems are reduced to one simple explanation, which is repeatedly emphasized;\n They receive what seems to be unconditional love, acceptance, and attention from a charismatic leader or group;\n They get a new identity based on the group;\n They are subject to entrapment (isolation from friends, relatives and the mainstream culture) and their access to information is severely controlled.\n\nThis view is disputed by some. Society for the Scientific Study of Religion stated in 1990 that there was not sufficient research for a consensus, and that \"one should not automatically equate the techniques involved in the process","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"well developed, it develops even more common. The theory has been used to show why people join groups who really want things to happen (activist groups), to organize the external (real, actual) and internal (perceived) parts (dimensions) of problem recognition, and to learn whether information that is used in information processing (processed information) can create publics. Also, some more research was done on the internal and external dimensions of problem recognition, constraint recognition, and level of involvement (Grunig & Hon, 1988; Grunig, 1997). The research is about whether the ideas are internal or external. The research shows that if the concepts are internal, they can be changed by communication, and that if they are external, then the holdable things that are around the person need to be changed in order for the person's concept of the variables to change (Grunig, 1997, p.\u00a025). But, there is a small amount","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Isotherms are lines drawn around places with the same temperature range on isotherm maps. Each point on this line shows one temperature reading, or the average of many temperature readings. Isotherm maps also have scales that tell the signals or colors for the different temperatures. Isotherm lines are usually curvy and not straight lines.\n\nMeteorology\nThermodynamics\n\nen:Contour line#Temperature and related subjects","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"fields. A simple example is attempting to find the smallest possible difference in the distance of two objects in two-dimensional space (x and y). In this context, the derivative of the function that gives the difference is taken in order to find the minimum. A more complicated example is in Machine Learning, in which the optimization function attempts to find the global minimum of the loss function in order to minimize the difference or loss between the algorithm\u2019s predictions and the actual values. This example is more difficult as Machine learning algorithms often utilize multidimensional data usually in the form of tensors yielding more complicated functions.\n\nRelated software\nToday, there are many tools to support optimization studies:\n MATLAB\n Wolfram Mathematica\n\nReferences\n\nScience\nMathematics","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Meditation tries to get past the \"thinking\" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic Books.  [a popular exposition emphasizing technique]<\/ref>\n\n Clear goals. Expectations and rules are known and goals are attainable and within one's skills and abilities.  Moreover, the challenge level and skill level should both be high.\n Concentrating: a high degree of concentration on a limited field of attention (a person engaged in the activity will have the opportunity to focus and to delve deeply into it).\n A loss of the feeling of self-consciousness.\n Distorted sense of time, one's sense of time is altered.\n Direct and immediate feedback (successes and failures in the course of the activity are apparent, so that behavior can be adjusted as needed).\n A balance between ability level and challenge: the activity is neither too easy nor too difficult.\n A sense of control over the situation or activity.\n The activity is intrinsically rewarding, so there is an effortlessness of action.\n A lack","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund SITTING and the nouns agent and location express the diagram's basic relationship; \"agent is SITTING on location\"; Elsie is an instance of CAT.\n\nAlthough the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter's illustration of that ambiguity, with a progression from abstract to concrete in G\u00f6del, Escher, Bach (1979):\n(1) a publication\n(2) a newspaper\n(3) The San Francisco","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Each of these smaller areas (room, state, number) is next to other small areas (other rooms\/states\/numbers). The places where the areas meet are connections. If we write down on paper a list of spaces, and the connections between them, we have written down a description of a space -- a topological space. All topological spaces have the same properties such as connections, and are made of the same structure (a list of smaller areas). This makes it easier to study how spaces behave. It also makes it easier to write algorithms. For instance, to program a robot to navigate a house, we simply give it a list of rooms, the connections between each room (doors), and an algorithm that can work out which rooms to go through to reach any other room. For more examples of this type of problem, look at Graph theory.\n\nWe can go further by creating subdivisions of subdivisions of space. For instance, a nation divided into states, divided into counties, divided into","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"law \n\nWeinberg developed the principle of genetic equilibrium independently of British mathematician G.H. Hardy. He delivered an exposition of his ideas in a lecture on 13 January 1908, about six months before Hardy's paper was published in English. His lecture was printed later that year in the society's yearbook. \n\nWeinberg's contributions were unrecognized in the English speaking world for more than 35 years. Curt Stern, a German geneticist who emigrated to the United States before World War II, pointed out in a brief paper in Science that Weinberg's exposition was both earlier and more comprehensive than Hardy's.\n\nAscertainment bias \nWeinberg pioneered studies of twins, and developed techniques to analyse phenotypic variation. His aim was to partition this variance into genetic and environmental components. In the process, he recognized that ascertainment bias was affecting many of his calculations, and he produced methods to correct for it. \n\nWeinberg observed that proportions of homozygotes","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size\u2014if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a graphics gallery, a replay function, and the ability to view various endings once they are achieved during a normal game.\n\n\u00a0Manufacturer's description:\n\nIs a hero, to keep with the story made up only doing the animation drives ?love story begins with the fourth series of the port city of snow. The main character, had received the emotions live in apartments next to Sakuragi, loses his love. One day, I told the police that the incident happened to. When leading a secret desire, a mixture of truth and falsehood.\n\n\u00a0Features:\n\n First person perspective.\n 2D graphics\n Cartoon graphics\n Mystery, dating & Anime themes.\n\nPlayStation games\nPlayStation Portable games\n1998 video games\n2005 video games\nVisual novels\nJapan exclusive video games","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"She has studied about them until now and taught about cultural anthropology in Kawamura Gakuen Woman\u2019s University\n\nHer career as a writer started in 1989. She published her first work \u201cSeirei-no-Ki (Spirit Tree)\u201d. From that year she has written many stories until today.\n\nFictional Works\n\nShika-no-Oh series (King of the Deers) \nVan, a man who was the top of the \u201cDokkaku (a kind of army)\u201d , Yuna, a young girl who was saved by Van and Hossal, a doctor, had fought with the mysterious sickness named \u201cMizzal (black-wolf fever)\u201d.\n\nKemono-no-Soja series (The Beast Player) \nErin, an orphan girl met with a sacred young animal and grew with it experiencing various things, meeting with a lot of people and learning about what the creatures are.\n\nKoteki-no-Kanata (Beyond the Fox Whistle)","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, Poisson distribution is a probability distribution. It is named after Sim\u00e9on Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The calculation of the last three quantities is explained in the respective Wiki pages. Then, with the help of formulas given in the previous section, the factors \u03bc and \u03b2 can be calculated. In this way, the CDF of the Gumbel distribution belonging to the data can be determined and the probability of interesting data values can be found.\n\nApplication \n\nIn hydrology, the Gumbel distribution is used to analyze such variables as monthly and annual maximum values of daily rainfall and river discharge volumes, and also to describe droughts.\n \nThe blue picture illustrates an example of fitting the Gumbel distribution to ranked maximum one-day October rainfalls showing also the 90% confidence belt based on the binomial distribution.\n\nReferences \n\nProbability distributions","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"that submerge it by a time in the language no figurativo. Also Manterola  has worked the sculpture, converting each piece in an only piece using different materials so much the iron , steel cut ,copper , iron galvanised , nylon, stainless steel, resin or iron painted.\n\nAnother of the appearances to stand out of Manterola is that it believes in the synergies and in the collaborations with other artists and disciplines. It says that it exists a mutual enrichment in all maridaje. Proof of this are the exercises with artists like Gorka Larra\u00f1aga and Samuel Dougados, the project UNYON beside Ivory Jewellers and, more recently, the collaboration with the chef French H\u00e9l\u00e8ne Darroze, that has 3 stars Michelin, for the one who has designed the servilleteros of his restaurant of Paris and where in each table luce one of his sculptures.\n\nBibliography \n Article of Him Figaro of 18 September","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"authors of the various paintings, will draw the contours of the paintings on the roadway, first with chalk and then with lime, based on the original sketch, also using perforated cartoons (dusting technique).  The placement of the flowers is done by placing the petals, taken from the baskets, within the contours already traced on the street.  On Sunday evening Mass is celebrated in front of the Church of Saint Maria of Cima and, after a solemn Eucharistic celebration in which the bishop of Albano often takes part, the carpet is covered by the religious procession of the celebrants who bring the Blessed Sacrament to the not so distant Collegiate Church of the Holy Trinity.  The carpet is maintained, with replacements of withered petals, until Monday evening, when the destruction of the floral display bychildren (the so-called shoulder ) takes place.\n\nNote \n\n Anna Baldazzi and Renato Torti (edited by).  Genzano and the flower display   : anthology of","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"4D, meaning the common 4 dimensions, is a concept in mathematics. It has been studied by mathematicians and philosophers since the 18th century.  Mathematicians who studied four-dimension space in the 19th century include M\u00f6bius, Schl\u00e4fi, Bernhard Riemann, and Charles Howard Hinton.\n\nIn geometry, the fourth dimension is related to the other three dimensions  of length, width, and depth by imagining another direction through space. Just as the dimension of depth can be added to a square to create a cube, a fourth dimension can be added to a cube to create a tesseract.\n\n4D is also an important idea in physics, developed in the 20th century. In physics, it refers to the idea of time as a fourth dimension, added to the (3D) spatial dimensions.  Albert Einstein developed the idea of spacetime by connecting space and time together. The difference is that spacetime is not a Euclidean space,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the idea to a prospective producer, director, or composer.\n\nScenarios are also used in policy planning, and when trying out strategies against uncertain future developments. Here the key idea is for the scenario to be an overview, a summary, of a projected course of action, events or situations. Scenarios are widely used by organizations of all types to understand different ways that future events might unfold.\n\nTheater\nPlanning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scenario may refer to:\n Scenario, a brief description of an event.\n Screenplay, in movies.\n Scenario (computing), a typical interaction between the user and the system or between two software components.\n Scenario analysis, a process of analysing possible future events by considering alternative possible outcomes. \n Scenario paintball, a variant of the game of paintball.\n Scenario planning, a strategic planning method that some organisations use to make flexible long-term plans.\n Scenario test, a test based on a hypothetical story used to help a person think through a complex problem or system.\n Kingmaker scenario, in a game of three or more players, is an endgame situation where a losing player, him- or herself unable to win, has the capacity to determine which player among others is the winner.\n User scenario, used to communicate an idea for a product or experience involving interactivity.\nScenario (A Tribe Called Quest), a 1992 song by hip-hop group, A Tribe Called Quest.","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship\/affect, communication, economic transactions, interactions, kinship, authority\/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and\/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called \"Law of 150\" which suggests that about 150 people is the best size for a village or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"church or temple is almost always a center of a social network). Often the network has an identity of its own which is quite real, even though it may have no official recognition. Networks may be centered on places, or on families, or on worldwide communities with common interests.\n\nSources \nThe Law of 150 is documented in R.I.M. Dunbar 1992. Neocortex size as a constraint on group size in primates. Journal of Human Evolution. 20, pp.\u00a0469\u2013493.\n\nThe field of study which investigates human social life is social psychology.\n\nRelated pages \nSocial network service\n\nRelationships\nSocial groups","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the base.\n\nFor example, a descriptive system widely used in Australia is based on structural characteristics based on life-form, plus the height and amount of foliage cover of the tallest layer or dominant species.\n\nFor shrubs 2\u20138 m high the following structural forms are categorized:\n dense foliage cover (70\u2013100%) \u2014 closed-scrub\n mid-dense foliage cover (30\u201370%) \u2014 open-scrub\n sparse foliage cover (10\u201330%) \u2014 tall shrubland\n very sparse foliage cover (<10%) \u2014 tall open shrubland\n\nFor shrubs less than 2 m high the following structural forms are categorized:\n dense foliage cover (70\u2013100%) \u2014 closed-heath\n mid-dense foliage cover (30\u201370%) \u2014 heath\n sparse foliage cover (10\u201330%) \u2014 low shrubland\n very sparse foliage cover (<10%) \u2014 low open shrubland\n\nReferences\n\nOther websites\nSelecting Shrubs for Your Home (University of Illinois","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"this area.  This is also where farming takes place.\n Below 1,000 metres are the lowlands.  Here, a larger variety of plants are produced. Aside from plants, villages are also in the lowlands because the temperature is easier for humans and farm animals.\n\nThe Alps is a classic example of what happens when a temperate area at lower altitude gives way to higher land. A rise from sea level into the upper regions causes the temperature to decrease. The effect of mountain chains on winds is to carry warm air belonging to the lower region into an upper zone, where it expands and loses heat, and drops snow or rain.\n\nPlants \nThe typical trees\u2014oak, beech, ash and sycamore maple have a natural height limit: the 'tree line'. Their upper limit matches the change in climate which comes with increasing height. The change from a temperate to a colder climate is also shown true by a change in the wild flowering plant life. This limit","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"fishermen, as well as spin and bait casting fishermen, to increase\u00a0conservation\u00a0and to protect rare fish such as marlin. The practice is however disputed as it by some is considered unethical to perform painful actions to the fish for fun and not for the reason of food production. Because of this, catch-and-release practice is illegal in\u00a0Norway.\n\nCollection of live fish\nFish can also be collected in ways that do not injure them (such as in a\u00a0seine net), for observation and study or for keeping in\u00a0Aquarium. There is a substantial industry devoted to the collection, transport, export and farming of wild and domesticated live fish, usually freshwater or marine tropical fish.\n\nFishing with traps\nFish can also be collected in ways that do not injure them (such as in a\u00a0seine net), for observation and study or for keeping in\u00a0Aquarium. There is a substantial industry devoted to the collection, transport, export and farming of wild and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"most of which are either deposit or\u00a0filter feeders. In this way, the toxins are\u00a0concentrated upward\u00a0within ocean\u00a0food chains.\n\nWhen pesticides are incorporated into the\u00a0marine ecosystem, they quickly become absorbed into marine\u00a0food webs. Once in the food webs, these pesticides can cause mutations, as well as diseases, which can be harmful to humans as well as the entire food web.\n\nToxic metals\u00a0can also be introduced into marine food webs. These can cause a change to tissue matter, biochemistry, behaviour, reproduction, and suppress growth in marine life. Also, many\u00a0animal feeds\u00a0have a high\u00a0fish meal\u00a0or\u00a0fish hydrolysate\u00a0content. In this way, marine toxins can be transferred to land animals, and appear later in meat and dairy products.  \n\nEvery time we wash a car or use fertilizer on our lawns we are polluting the ocean. People often think that water pollution comes from big factories, but most of the","type":"Document"}]}}