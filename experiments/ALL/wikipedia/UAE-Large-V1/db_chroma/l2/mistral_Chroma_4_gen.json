{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":" A\/B testing is a statistical hypothesis test used to compare two different groups, usually in the context of a software or a software-beverage-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-","1":" ANOVA (the ANOVA test) is a statistical method used to compare the mean (average) of two or (s) groups. It is used when we want to know if there is a statistical significance between the groups. ANOVA is a powerful method when we have a lot of data and we want to compare the mean of two or (s) groups. It is also used when we want to compare the mean of two or (s) groups that have different sample size. ANOVA is a powerful method when we want to compare the mean of two or (s) groups that have different variance.","2":"\n\nFrequentist probability is based on the frequency of an event in the past, or the number of times an event has occurred out of a total number of experiments. It is a way to make probabiilty inferences based on the observed results of experiments.\n\nBayesian probability, on the another, is based on the likelihood of an event given some evidence. It is a way to make probabiilty inferences based on the available evidence and prior probabiilty.\n\nIn the example given, Bayesian probability is used to classify an email as \"spam\" or \"not spam\" based","3":" A challenging imposing hierarchy on the order of causes is the one that priori-\n\ntions of causality. A challenging imposing hierarchy on the order of causes is the one that priori-\n\ntions agency and chance. In the contexts of agency, acknowledging serendipity and Murphy's law can be challenging because they suggest that chance and agency are on the same causal hierarchy, and that the outcome of an action is not just a cause of the effect, but also a chance occur-\n\nance.","4":" For datasets with only categorical data, the recommended course of action is to use statistical classification or other data mining and predicting model like Classification tree, Neural network, or Bayesian network. The choice of model will be based on the size and the quality of the data, as each model has different use- and generalization- Cities.\n\nFor example, if the data is about the different kinds of apples and their taste, the recommended model could be a Classification tree, as it can predict the taste based on the size, colour, and shininess of the apple.\n\nFor datasets with a","5":"\nA Generalised Linear Model (GLM) is a statistical model that uses a linear predictor function, but can also include non-linear and non-linear relationship between the predictor variables and the dependent variable. It is a more General version of Linear regression.\n\nIn a Generalised Linear Model, the relationship between the predictor variables and the dependent variable is modeled using a \"linear predictor\" (or \"linear predictor function\" or \"linear predictor variable\" or \"linear predictor term\" or \"linear predictor vector\" or \"linear predictor value\" or \"linear predict","6":" Cluster analysis is a type of data analysis in which objects are group together so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters). It is a common task in data mining. The Dunn Index (DI) is a metric for judging a clustering algorithm, with a higher DI value implies better clustering. The DI is equal to the minimum inter-cluster distance divided by the maximum cluster size.\n\nIn other contexts, analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it","7":" Network Analysis is a field of mathematics and computer technology used to represent, structure, and make in-\n\nQuestion: What is the purpose of Network Analysis?\nHelpful Answer: Network Analysis is a field of mathematics and computer technology used to represent, structure, and make in-\n\nQuestion: What is the purpose of Network Analysis?\nHelpful Answer: Network Analysis is a field of mathematics and computer technology used to represent, structure, and make in-\n\nQuestion: What is the purpose of Network Analysis?\nHelpful Answer: Network Analysis is a field of mathematics and computer technology used to represent","8":" ANCOVA, an\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab\n\nab","9":" ANCOVA, or ANAlouvsiCOVA, is a method for the one- or even the \n\nof a \n\nin the context of a \n\n. It is a \n\nof \n\n, which is a \n\nof \n\n. The key principles and assumptions of ANCOVA include:\n\n1. The \n\nof the \n\n: ANCOVA is based on the \n\nof \n\n, which is the \n\nof \n\n. This \n\nof \n\n, \n\n, and ","10":" ANCOVA, or ANCOVA for a one-at- a- \n\nThe Hippocrates' theory of personality is a theory that was proposed by Hippocrates, an early \n\nThe Hippocrates' theory of personality is a theory that was proposed by Hippocrates, an early \n\nThe Hippocrates' theory of personality is a theory that was proposed by Hippocrates, an early \n\nThe Hippocrates' theory of personality is a theory that was proposed by Hippocrates, an early \n\nThe Hippocrates","11":" Content Analysis is a research and research methods used to study the content of media, including text, imagery, and other forms of media. The strengths of Content Analysis include the following:\n\n1. Fetable and incontroversial: Content Analysis is a widely used research methods that is widely used in many fields, including media and social science research.\n2. Fetable and incontroversial: Content Analysis is a widely used research methods that is widely used in many fields, including media and social science research.\n3. Fetable and incontroversial: Content Analysis is a widely","12":" The three main methods to calculate the correlation coefficient are Pearson's correlation coefficient, Spearman's rank correlation coefficient, and the coefficient of correlation.\n\nPearson's correlation coefficient is a mathematical formula used to calculate correlation coefficients between two datasets. This is the one you'r working on in the context.\n\nStep 1: Find the mean of x, and the mean of y\nStep 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\nStep 3: Cal","13":" A correlogram is a graphical or mathematical way to show the correlation between different variables. It is used to show the relationship between variables and to indicate the strength and direction of the correlation. It is a form of a nomogram, which is a graphical calculation of a mathematical function. The correlogram is a way to calculate and graph the correlation coefficients between different variables in a dataset. It is a way to quickly and graphical show the relationship between variables and to make it possible to make in-sulations and make in-sulations. It is a way to make it possible to make in-sulations and make","14":"\nTelemetry is a technology that allows measurements to be taken from far away, often through telecommunication systems, and transmitted back to an operator for use. This can be used to measure and transmit data from a machine or system, such as the motion of a particle, and is often used in fields such as engineering and space travel. The bandwidth of the telecommunication system used for telemetry is an measure of the amount of data that can be transmitted in a given time period.\n\nExamples of signals that can be measured and transmitted through telemetry systems:\n- Motion of a particle\n-","15":" The normal distribution is a probability distribution that is often used to model continuous data. It is a symmetric, bell-shaped distribution with a mean and a standard deviation. The normal distribution is important because many values in the statistical and data science fields follow this distribution.\n\nThe normal distribution is a model of a random event that is the sum of many identical but random events. The central limit theorem says that if an event is the sum of identical but random events, it will be normally distributed.\n\nThe normal distribution is a continuous probability distribution, meaning that it can assume any value between its mean and its mean with an probability.","16":" The Shapiro-Wilk test is a statistical test used to check the hypothesis that a given set of data follows a normal distribution. It is a test for distribution, and it is named after its two main devi- leol- gant- Shapiro and-Wilk. The test results in a test statistic, and the null hypothesis is that the data follows a normal distribution. The test is used to check the null hypothesis, and if the test statistic is less than the given significance value, the null hypothesis is not be- gared to be true, and the data is said to be","17":"\n\nIn the context of forecasting, the Delphi method is a forecasting method that is chosen over traditional forecasting methods because of its use in obtaining a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often a large and often","18":" The main goal of Sustainability Science is to promote the long- term sustainability of human and environmental health and well-being. The challenges it faces include the need to meet the needs of the present without compromising the ability of future generations to meet their own needs, while at the the\n\nThe main goal of Sustainability Science is to promote the long- term sustainability of human and environmental health and well-being. The challenges it faces include the need to meet the needs of the present without compromising the ability of future generations to meet their own needs, while at the\n\nThe main goal of Sustainability","19":"\n\nCritics such as Richard Lewontin and Stephen Jay Gould, and the Sociobiology Study Group, were important in the controversy that a new field of sociobiology, as it was then called, became the subject of heated controversy in the mid-1970s. The criticism was driven by political events of the day, particularly the war in Vietnam and the Marxist or semi-Marxist views of some scholars. The main concern of the critics seemed to be the idea that a mechanistic paradigm, as it was then called, might make it easier to accept ideas that were not founded","20":"\n\nThe term \"Lateral Thinking\" and \"Problem-solving\" are related concepts. Lateral Thinking is a way of thinking that is not immediately obvious and may not be obtainable by using only traditional step-by-step logic. It is a term invented by Edward de Bono and first appeared in his book \"The Use of Lateral Thinking\" published in 1967.\n\nProblem-solving is a mental activity related to intelligence and thinking. It consists of finding solutions to problems. A problem is a situation that needs to be changed. It suggests that the solution is","21":" The main principle of the Feynman Method is that a quantum particle can be represented as a wave, and the square of the wave's wave-lobed pattern gives the particle's position-in-a-certain-observed-situs-in-a-certain-observed-situs-in-a-certain-observed-situs-in-a-certain-observed-situs-in-a-certain-observed-situs-in-a-certain-observed-situs-in-","22":" In ANOVA designs, fixed factors are factors that are of specific, pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated, or pre-tated,","23":" The replication crisis is a crisis in science where the results of scientific experiments are difficult or impossible to replicate. This has grave consequences as reproducibility is an essential part of the scientific method. The crisis has been particularly widely discussed in the field of psychology and medicine. The inability to replicate studies can lead to contradicting results, and researchers may cherry pick studies that support their claim. The replication crisis has been made better by discussions and efforts to re-investigate classic results. However, it is common for studies to contradict others, and human error or chance may lead to unusual results. The","24":" The flashlight method is not a well-formed or well- studied method in group discussions. It is not a method that is used in group discussions as a common or well- developed technique. The context you have been reading about is about coercive persuasion and thought reform techniques used in cults. The flashlight method is not a method that is used in this context. It is not a method that is used in group discussions at all. It is a method that is used in police questioning, and it is a method that is considered to be un ethical and coercive. It is a method that is used to try","25":" Generalized Linear Models (or \"linear models with a linked function\" or \"linear models with a probability function\" or \"linear models with a probability function and a linear predictor function\" or \"linear models with a probability function and a linear predictor and a linear predictor function that is not linear in the predictor variables\" or \"linear models with a probability function and a linear predictor and a probability function that is not a logistic function\" or \"linear models with a probability function and a linear predictor and a probability function that is not a logistic function or a logistic function with a linear predictor","26":" A heatmap is a visual tool that uses colors to show the temperature range in a given area. It is useful for quickly understanding temperature patterns and can help in weather forecasting, geographic analysis, and other areas where temperature data is important. The curvy isotherm lines on a heatmap make it more useful and accurate in showing temperature differences and patterns, and the color scales help to differentize the temperatures. In mapmaking, heatmap colors can be used to show different features, and the colors can affect the understanding or feel of the map. In meteorology, weather maps are used to show temperature differences and weather","27":" Alhazen is known as the \"father of modern optics\" for his contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, and Muslim theology. He is also known for his work on the scientific method, as he was an early pioneer of this way of in scientific in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in","28":" One way to graphically display multivariate data is by using a 3D graph, which can display more variables in a single graph. However, it can be more complex to create and visual. Vector graphics, which use lines, shapes, and text, can also be used to display multivariate data, particularly in the form of a 3D graph or a 3D chart. These types of graphics can be scaled indefinitely without loss of quality, making it useful for display data with a multivariate relationship.\n\nRelated pages\n2D graphics\n3D graphics","29":" Machine learning has the advantage of being more data-independent and generalizing well to data that is more complicated or of a more variable form. It can also learn from data where designing and programming explicit algorithms is difficult or in the context of supervised learning, can learn to predict the outcome of a dependent variable based on a set of explanatory variables. In machine learning, the system learns the function from the data, as the function is often more complicated and multidimensional, and the system can learn to make predictions or decisions based on this function. This is in difference to traditional rules or functions which are explicitly programmed and","30":" Machine learning techniques can be black box, which can make it hard to understand how the model is working and make it hard to make decisions based on the model. Some machine learning techniques have been criticized for biases in hiring, criminal justice, and recognizing faces. Machine learning techniques may not be the efficient optimization algorithms for specific problems and problem instances. Alternative and complementary optimization algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence. The suitability of machine learning techniques is dependent on the amount of knowledge of the problem.\n\nIn machine learning, super","31":" The scientific method is a systematic and empirical way of investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. It is based on gathering observable, empirical and measurable evidence, and the formulation and testing of hypotheses. The scientific method is characterized by the following features:\n1. A problem or question is formulate\ned.\n\n1. The problem is investiga\n\nted through observation and experimentation.\n\n1. A theory or hypotheses is formulate\ned to explan\n\n1. The theory or hypotheses is teste\n\n1","32":" The main goal of practicing mindfulness is to calm the mind and become better att attainable and within one's skills and abilities. This can end pain and stress, and also has health benefits such as lowers stress, anxiety and depression. The practice can focus on various things such as breath, metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds. In Buddhism, this type of seeing can end pain. In Christianity, meditation is an expression of prayer and engages thought, imagination, emotion, and desire. In both religions, the practice","33":" In a Mindmap, information is arranged in a diagram that illustrates the relationships between different pieces of information. The diagram is made up of a number of areas, each of which can be seen as a space, and the connections between these areas. The areas can be seen as abstract things, and the connections as relationships between these abstract things. The process of creating a Mindmap can be seen as a process of abstraction, where multiple pieces of constituent data are abstracted to a single piece of abstract data based on similarities in the constituent data. This process emphasizes the inherent equality of both constituent and abstract","34":" The concept of degrees of freedom was first noted in the early 1800s by Carl Friedrich Gauss. The modern usage and understanding of the term were expounded upon first by William Sealy Gosset in his article \"The Probable Error of a Mean,\" published in Biometrika in 1908. Gosset did not specifically use the term \"degrees of freedom\" in his article, however he did give an explanation for the concept throughout the course of developing what would eventually be known as Student\u2019s T-distribution. Mixed Effect Models are a more modern statistical method","35":"\n\nIn statistics, Mixed Effect Models (a.k.a. random effect models or random interance models) are a more complex and sophisticated statistical method than Analysis of Variance (a.k.a. one- or two- way Analysis of Variance, or just Analysis of Variance) and Regressions. They are used to analyze complex datasets with multiple factors and interactions between them.\n\nIn terms of statistical power, Mixed Effect Models can be more power than Analysis of Variance and Regressions, as they allow for the handling of random and dependent data, which can be more complex","36":"\n\nIn model reduction, stepwise procedures can be avoided because they can often make the reduction process more inaccurate or inaccurate more often. This is because in a stepwise reduction process, the model is first partitioning into a set of pieces, and then one or more pieces are then manipulating at a given step. This can make it more possible for numerical or algebraic error to be inaccurate in the reduction process, as the manipulating of one or more pieces can make the error in the reduction process more \"cry\" and more \"c\" in the reduction process.\n\nFive-point","37":" In data model reduction, redundancies can be identify using methods such as:\n\n1. P-redundancy: If a set of data can be recast as a linear transformation of another set of data, then the redundant set can be dis-thin the model by elimination of the redundant set.\n2. P-in-Q-redundancy: If a set of data can be recast as a linear transformation of another set of data, and the data in the redundant set is a linear transformation of the data in the set that is not redundant, then","38":" Narratives are used in Narrative Research to better context and make in-sicn the stories, emotions, and cultural contexts of the people or the text that is studied. Narratives are used to make the stories and their contexts 'almost' live, and to make the stories and their contexts 'almost' be a 'went-almost- happened' or 'went-almost- be' in the world. Narratives are used to make the stories and their contexts 'almost' be a 'went-almost- be' in the world,","39":" Generalized Additive Models (GAM) are a class of statistical models that allow for the use of a function other that a linear function as the predictor in a regression model. This is useful when the linear model does not well- model the data. \n\nGAM are an example of a generative model, as they use randomness to model the data. \n\nAn advantage of using a Generalized Additive model is that it can model more \n\nHelpful Answer: Generalized Additive Models (GAM) are a class of statistical models that allow for the use of a function","40":" The Poisson distribution can be used to describe the number of occur-\n\nThe Poisson distribution is a probability distribution that can be used to describe the number of occur-\n\nThe Poisson distribution is a probability distribution that measures the probability that a certain number of events occur within a certain period of time. It is named after Sim\u00e9on Denis Poisson. The three conditions under which the Poisson distribution can be used are:\n\nThe events must be unrelated to each other.\nThe events must occur with a known average rate, represented by the symbol  (lambda).\nThe events must be independent of each","41":" The Pomodoro technique is a time-tasking and time-estimations technique that was first established by an artist, and it is based on the idea of using a technique to work for a specific time, then taking a specific time for a work- or a work- related or a work- not related-  to re-estimations and re-estimations. The technique is based on the idea of using a technique to work for a specific time, then taking a specific time for a work- or a work- related or a work- not related-  to re-estimations and re","42":" The 'curse of dimensionality' is a term used in mathematics and physics to define the way that the number of dimensions in a space can make it more and more different from the 3 dimensions that we's used to. It can make it more and more different to measure, to study, and to make use of. The more dimensions there are, the more ways there are for things to be different, and the more ways there are for things to be the 'out of the usual' or 'an out-an-out-an-an-an-an-an-an-an-an-an","43":"\n\nIn statistical analysis, dispersion is a measure of the variability or the distribution of data around the mean. The dispersion of a dataset is important to consider when testing statistical hypothesis, as it may have an important and, at the same, an inessentical, or a different, \n\nIn simple language, dispersion is the measure of how data is distributed around the mean. If the data is distributed such that the variance is the same for different values of the mean, it is called homoscedastic dispersion. If the variance is different for different values of the mean, it is called heteros","44":" Shell was an organizations that widely used scenario planning as a strategic planning method. They used it to make flexible long-term plans and to understand different ways that future events might unfold. Shell's use of scenario planning was a key contribute to the advancement of this method as a widely used strategic planning method in organizations.\n\nThe game can be used to make level maps using \"Construct Mode\". All of the enemies, options and the terrains and their music that are available in the main game are available. These files are saved as CONSTRUCT.TXT, and can be emailed to friends. They can also be shared","45":" The field of Social Network Analysis was influenced by the work of sociologists in the 1930s, who based their work on the study of human social life and the pattern of relationships among a set of people and\/or organizations in a community. This work called attention to the structure of social relationships and the identity of social networks, which can involve social support, give people a sense of community, and help them protect and help each other. The sociologists of this period, who may have been influenced by the work of social psychologists, also considered the size and structure of personal networks and the idea of a \"Law","46":" Stacked Area Plots are a good way to make it easier to make a time- or categorized- based, but they do have limitations.\n\n1. They can only be used for one categorized or time- based data set at a time.\n2. They do not allow for the representation of interdependencies between different data- or time- based data.\n3. They can be used to make it easier to make a time- or categorized- based, but they do not allow for the representation of the change in the data- or time- based data.\n\nFor example, if you have","47":" Thought experiments are mental experiments that help us understand how the world might be if a specific idea or theory were true. They are a way to test theories and to see what might follow from them. Thought experiments have been used in science and philosophy for many years to help us understand the world around us. They are a way to test theories and to see what might follow from them, even if the theories cannot be tested in the real world.\n\nIn the context of cognitive- Behaviour theory, thought experiments are used to test irrational beliefs and to develop positive thinking patterns. In this way, CBT is scientific because it uses","48":" In the context of time- Series, temporal autocorrelation is the statistical method of using the previous values of a time- Series to forecast the value of the time- Series at a time in the \n\nAn example of this is the autoregressive model, which is a statistical model used to describe a random process. In this model, the output value depends linearly on one of the previous values of the model, plus a random variable. In other \n\nIn the context of Digital Signal Processing, temporal autocorrelation is the method of using the previous values of a time- Series to forecast","49":" The Besatzfisch project, which ran from the late 10s to the late 2010s, collected live fish in ways that did not injure them, such as in a net, for observation and study or for keeping in an\u00a0Aquarium. The project also performed tank and ocean experiments on other animals, such as lobsters, to see if they would eat enough purple sea urchins to help the kelp forests grow back. In places where sea otters have returned, like Juan de Fuca Strait, they ate enough sea urchins that the kelp forests"},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and other vehicles include a crash test. The car is put under severe conditions to see what will make it fail, or deliberately crashed to measure the damage. Other machines can also be crash tested. Crash test dummies can be used instead of humans. They are placed in the car seat to see if a human in the crash would have been injured or killed.\n\nPeople \n\nPeople are tested to see what they have learned. This is often called an assessment or examination. In learning, a test item is a question, or set of questions.\n\nMany people think tests are valuable. They believe tests:\n are a quick and fair way of judging a test taker's performance\n enable predictions about test takers to be made\n allow selection\n improve performance by highlighting areas that need work.\n\nHowever, academic tests are not perfect measures. Tests could only partly measure a student\u2019s memory and maybe their understanding. The test","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test bay is an area used in engineering. It can be a hall, laboratory or even a department. At these places, the produced materials are tested before delivery. To test them, testing guidelines of industry or this firm are used. The test bay is part of the quality management. The protocols written during testing are evidences in case of insurance issues, say.\n\nEngineering\nTests","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"sample minus one:\nDf\u200b=N\u22121\nwhere:\n\nDf\u200b=degrees of freedom\n\nN=sample size\u200b\n\nDegrees of freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a chi-square. It is essential to calculate degrees of freedom when trying to understand the importance of a chi-square statistic and the validity of the null hypothesis.\n\nChi-Square Tests \nThere are two different kinds of chi-square tests: the test of independence, which asks a question of relationship, such as, \"Is there a relationship between gender and SAT scores?\"; and the goodness-of-fit test, which asks something like \"If a coin is tossed 100 times, will it come up heads 50 times and tails 50 times?\"\n\nFor these tests, degrees of freedom are utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the others prepared by first adding the tea.  She was to select the  four cups prepared by one method.\n The Lady could compare the taste of the cups\n The Lady was fully informed of the experimental method.\n The null hypothesis was that the Lady had no such ability.\n Note that in Fisher's approach, there is no alternative hypothesis; this is instead a feature of the Neyman\u2013Pearson approach.\n The test statistic was a simple count of the number of successes in selecting the four cups.\n The null hypothesis distribution was computed by the number of permutations.  The number of selected permutations and the number of unselected permutations were equal.\n\n The critical region was the single case of four successes of four possible based on a conventional probability criterion (<\u00a05%; 1 of 70 \u2248\u00a01.4%).\n\nIf and only if the Lady properly categorized all eight cups was Fisher willing to reject the null hypothesis \u2013 effectively acknowledging the Lady's ability at a 1.4% significance level (but","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don't have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as \"spam\" or \"not spam\".  One thing you know about this email is that it has an emoji in the subject line.  Say it's the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it's 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that's different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayes' theorem is just another way to write that equation.\n\nRelated pages \n\n Bayesian probability\n Bayesian network\n\nReferences \n\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability theory and applications, Bayes' theorem shows the relation between a conditional probability and its reverse form. For example, the probability of a hypothesis given some observed pieces of evidence, and the probability of that evidence given the hypothesis. This theorem is named after Thomas Bayes ( or \"bays\") and is often called Bayes' law or Bayes' rule.\n\nFormula \n\nThe equation used is:\n\nWhere:\n P(A) is the prior probability or marginal probability of A. It is \"prior\" in the sense that it does not take into account any information about\u00a0B.\n P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from (or depends upon) the specified value of\u00a0B.\n P(B|A) is the conditional probability of B given A. It is also called the likelihood.\n P(B) is the prior or marginal probability of B, and acts as a normalizing constant.\nIn many","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is most likely to occur when one is wholeheartedly performing a task or activity for intrinsic purposes. Intrinsic purposes involve anything that someone does merely because they want to. Extrinsic activities will not cause flow to occur. Extrinsic activities are anything that someone does because there is some other force causing them to do it. Extrinsic activities will not cause flow to occur. Passive activities like taking a bath or even watching TV usually do not elicit flow experiences as individuals have to actively do something to enter a flow state. While the activities that induce flow may vary and be multifaceted, Csikszentmih\u00e1lyi asserts that the experience of flow is similar despite the activity.\n\nComponents of flow\nCs\u00edkszentmih\u00e1lyi identifies the following ten factors as accompanying an experience of flow:<ref name=Finding>Cs\u00edkszentmih\u00e1lyi, Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"what they experienced. So, relying on our memories to make decisions does not lead to the highest net pleasure. To experience the net largest amount of pleasure,  people should spend as much time as possible on things that they are unwilling to stop doing.\n\nRestrictions and criticisms \nStudies found that other factors influence the peak-end rule. Firstly, the expectation. A high expectation is compared with the actual experience at the start. If a difference exists, the starting experience will be the most important factor in overall experience evaluation.  The peak-end rule is more likely to be applicable to lower expectation situations. Secondly, one study finds that the effect of peak-end law is small on one day experiences. Moreover, some people are easily affected by the rule but others are not. Thirdly, according to Ariely and Carmon, how we feel at the moment of evaluation also affects the outcome.\n\nReferences","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or those that give a purpose to behaviour  Example: The reason why the artist wanted to make the statue.\n\nAristotle told people of two types of causes: proper (prior) causes and accidental (chance) causes. Both types of causes, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes; so that generic effects assigned to generic causes, particular effects to particular causes, and operating causes to actual effects. It is also essential that ontological causality does not suggest the temporal relation of before and after - between the cause and the effect; that spontaneity (in nature) and chance (in the sphere of moral actions) are among the causes of effects belonging to the efficient causation, and that no incidental, spontaneous, or chance cause can be prior to a proper, real, or underlying cause per se.\n\nAll investigations of causality coming later in history will consist in imposing a favorite hierarchy on the order","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected\/influence each other. The","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Classification could mean:\n\n Library classification and classification in general\n Optimal classification\n Biological classification\n Scientific classification (disambiguation)\n Classification (literature)\n Statistical classification\n Security classification\n Classification theorems in mathematics.\n Film classification\n CLASSIPHI, a seabed mapping tool supplied by QinetiQ\n Civil service classification, personnel grades in government\n Attribute-value system\n\nRelated pages\n Class\n Categorization","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to make something that can say how the different data attributes are connected\/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and\/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster\/tree\/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"not matter. The only important property of a route is the order in which the bridges are crossed. So, he changed the problem to abstract terms. This laid the foundations of graph theory. He removed all features except the list of land masses and the bridges connecting them. In the language of graph theory, he replaced each land mass with an abstract \"vertex\" or node. Then he replaced each bridge with an abstract connection, an \"edge\". An edge (road) recorded which two vertices (land masses) were connected. In this way, he formed a graph.\n\n \u2192\n \u2192\n\nThe graph drawn is an abstract picture of the problem. So, the edges can be joined in any way. Only whether two points are connected or not are important. Changing the picture of the graph does not change the graph itself. \n\nNext, Euler observed that (except at the endpoints of the walk), whenever one enters a vertex by a bridge, one leaves the vertex by a bridge. In any walk of the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship\/affect, communication, economic transactions, interactions, kinship, authority\/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and\/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called \"Law of 150\" which suggests that about 150 people is the best size for a village or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A computer network is a group of two or more computers that are linked together. Networks are usually used to share resources, exchange files or communicate with other users.\n\nA network is a set of nodes connected by communication links.  A node can be a computer, printer, or any other device capable of sending or receiving data from or to the other node in the network.\n\nOther devices are often needed for the network to work correctly.  Examples for such devices include hubs and switches.  Different kinds of network can be connected to each other with a router.  In general, networks that use cables to connect can operate at higher speeds than those using wireless technology.\n\nComputers in a network can be near each other, or far. A Local Area Network (LAN) connects computers which are close together.  Building a LAN is easier than connecting different networks (by a Wide Area Network).  The largest Wide Area Network is the Internet.\n\nComputers can be part of several different networks. Networks can also","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Graph theory is a field of mathematics about graphs. A graph is an abstract representation of: a number of points that are connected by lines. Each point is usually called a vertex (more than one are called vertices), and the lines are called edges. Graphs are a tool for modelling relationships. They are used to find answers to a number of problems.\n\nSome of these questions are:\n What is the best way for a mailman to get to all of the houses in the area in the least amount of time? The points could represent street corners and lines could represent the houses along the street. (see Chinese postman problem)\n A salesman has to visit different customers, but wants to keep the distance traveled as small as possible. The problem is to find a way so they can do it. This problem is known as Travelling Salesman Problem (and often abbreviated TSP). It is among the hardest problems to solve. If a commonly believed conjecture is true (described as P \u2260","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"COVID-19.\nSaskia Post, 59, American-born Australian actress (Sons and Daughters, Dogs in Space, Eug\u00e9nie Sandler P.I.), cardiac arrest.\nStuart Whitman, 92, American actor (The Mark, The Comancheros, Those Magnificent Men in their Flying Machines), skin cancer.\n\n17\nVittoria Bogo Deledda, 53, Italian politician, Senator (since 2018), cancer. \nMichael Broadbent, 92, British wine critic and writer.\nGerald Freedman, 92, American theatre director, librettist and lyricist, kidney failure.\nEduard Limonov, 77, Russian publicist, political writer and dissident, co-founder of National Bolshevik Party and Leader of The Other Russia (since 2010), problems caused from surgery.\nRoger Mayweather, 58, American boxer and boxing trainer, WBA super featherweight (1983\u20131984) and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Formula One) and team owner (Campos Racing), aortic dissection.\nH\u00e9ctor Fix-Zamudio, 96, Mexican politician and lawyer, Judge of the Inter-American Court of Human Rights (1987\u20131997), heart failure.\nGoddess Bunny, 61, American drag queen, actress (Hollywood Vice Squad, The Goddess Bunny, Rage) and model, problems caused by COVID-19.\nCloris Leachman, 94, American actress (The Mary Tyler Moore Show, Young Frankenstein, The Last Picture Show), Oscar (1971), multi-Emmy and Golden Globe (1975) winner, stroke caused by COVID-19.\nCorky Lee, 73, American photojournalist, problems caused by COVID-19.\nMehrdad Minavand, 45, Iranian footballer (Pas, Persepolis, national team) and manager, COVID-19.\nAminuddin Ponulele,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"country singer, Mayor of Macon, Georgia (1967\u20131975).\nEric Weissberg, 80, American folk singer (\"Dueling Banjos\") and guitarist (The Tarriers), problems caused by Alzheimer's disease.\nJ\u00fcrg Zeltner, 52, Swiss banking executive (KBL), brain cancer.\n\n23\nAlberto Arbasino, 90, Italian writer, essayist and politician, Deputy (1983\u20131987).\nMaurice Berger, 63, American cultural historian and art critic, heart failure caused by COVID-19.\nLucia Bos\u00e8, 89, Italian actress (No Peace Under the Olive Tree, Story of a Love Affair, Rome 11:00), Miss Italia (1947), pneumonia caused by COVID-19.\nCarlo Casini, 85, Italian politician, Deputy (1979\u20131994) and MEP (1984\u20131999, 2006\u20132014), problems caused by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Nicolodi, 70, Italian actress (Deep Red, Shock) and screenwriter (Suspiria).\nLouis Nzala Kianza, 74, Congolese Roman Catholic prelate, Bishop of Popokabaka (1996\u20132020).\nHafez Abu Seada, 55, Egyptian politician and human rights activist, Chairman of the Egyptian Organization for Human Rights (since 2004), COVID-19.\nKamen Tchanev, 56, Bulgarian operatic tenor, COVID-19.\nCelestino Vercelli, 74, Italian Tour de France racing cyclist (1971, 1976).\nDadang Wigiarto, 53, Indonesian politician, Regent of Situbondo (since 2010), COVID-19.\n\n27\nKevin Burnham, 63, American sailor, Olympic champion (2004) and silver medalist (1992), problems caused by lung disease.\nMohsen","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Skumin referred to the works of Helena Blavatsky, Helena and Nicholas Roerich, Konstantin Tsiolkovsky, and Alexander Chizhevsky. In some of his publications, he argues that the culture of health will play an important role in the creation of a human spiritual society in the Solar System.\n\nThe doctrine of a culture of health, proposed by Skumin, the culture  \u2013 spiritual, mental, and physical \u2013  determines the status of human health. And health \u2013 spiritual, mental, physical \u2013 is a prerequisite for achieving a higher level of culture.\n\nSkumin syndrome \n\nSkumin syndrome (:ru:\u0421\u0438\u043d\u0434\u0440\u043e\u043c \u0421\u043a\u0443\u043c\u0438\u043d\u0430) was described by Skumin in 1978 as a \"cardioprosthetic psychopathological syndrome\", associated with mechanical heart valve implant and manifested by irrational fear and sleep disorder.  Patients have doubts about the reliability of the device, fear of breakdown, and suffer anxiety and depression. This syndrome is often accompanied","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reliability of the device, fear of breakdown, and suffer anxiety and depression. This syndrome is often accompanied by asthenia.\n\nAlain Carpentier \u2013 a member of the French Academy of Sciences and the head the Department of Cardiovascular Surgery at the H\u00f4pital Europ\u00e9en Georges-Pompidou in Paris \u2013 believed in 2011 that Skumin syndrome develops in a quarter of the patients with an artificial heart valve. It is possible that a similar problem arises in the conduct of operations to implement an artificial heart.\n\nSkumin mind control method \n\nIn 1979, Skumin created a special modification of mind control method for psychological rehabilitation of patients. \n\nThis method is based on autogenic training. Autogenic training is a relaxation technique developed by the psychiatrist Johannes Heinrich Schultz. He emphasized parallels to techniques in yoga and meditation. It is a method for influencing one's autonomic nervous system. The technique involves the daily practice of sessions that last around 15 minutes, usually in the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the greatest number,\" but did not use the word utilitarianism. It was Mill, a follower of Bentham's ideas, who named the idea.\n\nUtilitarianism in Practice \nMany philosophers argue that utilitarianism has important practical implications for how we should live ethically. For instance, according to utilitarianism we should help the poor, prevent animal suffering, and ensure that future generations have good lives.\n\nMaslowism \nMaslowism or Maslow's hierarchy of needs can be divided into social and physical needs. Social needs in Maslowism include physiological, safety, love, esteem, and self-actualization. Maslowist themes like physiology suggests you need food while Maslowist themes like love suggests humans need sex.\n\nRelated pages \nHedonism\n\nReferences \n\nPhilosophical movements and positions\nEthics","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Skumin referred to the works of Helena Blavatsky, Helena and Nicholas Roerich, Konstantin Tsiolkovsky, and Alexander Chizhevsky. In some of his publications, he argues that the culture of health will play an important role in the creation of a human spiritual society in the Solar System.\n\nThe doctrine of a culture of health, proposed by Skumin, the culture  \u2013 spiritual, mental, and physical \u2013  determines the status of human health. And health \u2013 spiritual, mental, physical \u2013 is a prerequisite for achieving a higher level of culture.\n\nSkumin syndrome \n\nSkumin syndrome (:ru:\u0421\u0438\u043d\u0434\u0440\u043e\u043c \u0421\u043a\u0443\u043c\u0438\u043d\u0430) was described by Skumin in 1978 as a \"cardioprosthetic psychopathological syndrome\", associated with mechanical heart valve implant and manifested by irrational fear and sleep disorder.  Patients have doubts about the reliability of the device, fear of breakdown, and suffer anxiety and depression. This syndrome is often accompanied","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"She was invited to tour the United States to recommend and speed up her project. She sailed for the United States in 1921. She collected enough money and equipment for a new laboratory. She then started speaking at meetings to raise more money and became a celebrity. She also supported world peace by serving on the council of the League of Nations.\n\nDeath\nNear the 1920s, Curie and many of her colleagues began to suffer from symptoms of cancer. Curie began to lose her sight. Cataract surgeries to try to bring back her sight did not help. Curie knew that the element (radium) she discovered might have been causing the symptoms, but she did not want to admit it to herself or others. In the early 1930s, Curie\u2019s health started to quickly get worse. Doctors diagnosed her with pernicious anemia. Pernicious anemia is a blood anemia that happens when someone is overly exposed to radiation. The doctors didn\u2019t","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"When one of the fluids was too much or too little, it affected the personality.  Hippocrates associated each of the humors with different elements, and temperaments:\n Blood was associated with air and resulted in sanguine, or hopefulness.\n Black bile, associated with earth, resulted in a temperament that was melancholic and resulted in a person feeling sad.\n Yellow bile was associated with fire and resulted in what he called a choleric temperament, resulting in irritability and aggression.\n Phlegm, associated with water, resulted in a phlegmatic temperament, associated with being apathetic.  \nIf any of the four humors were at irregular levels, Hippocrates believed the person would display the temperaments or show signs of sickness.\n\nDifferent perspectives of personality research\nWhen studying personality, one must consider all the different perspectives in the approach to understand how personality is created.  Some of these perspectives include things like trait theory, the influence of motives, evolutionary theories, and the social learning","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Content can refer to:\n Information and experiences created by individuals, institutions and technology to benefit audiences in contexts that they value.\nRaw content is content in format that is detectable by an observer.\n Subject of the plot, in narrative works.\n Substance.\n Volume generalized to arbitrarily many dimensions in mathematics and physics.\n In education, the curriculum to be learned as opposed to the teaching methods used.\n As an emotion, a form of happiness from being happy with what you have.\n\nOther\n Open content\n Free content\n Web content\n Content format","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"popular characters in certain videos falsely claiming to be targeted to children. YouTube has also been criticized for attracting pedophilic comments in videos of minors performing activities. \n\nBecause YouTube keeps changing policies on the types of content that is eligible to be monetized with advertising, many content creators are concerned about these frequent changes. YouTube policies restrict certain forms of content from being included in videos being monetized with advertising. This includes videos containing violence, strong language, sexual content, \"controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown\" (unless the content is \"usually newsworthy or comedic and the creator's intent is to inform or entertain\"), and videos whose user comments contain \"inappropriate\" content. However, it is not clear what is the boundaries for what YouTube's policies specifically accept and do not accept. Some content creators also say that YouTube's policies also change too often. For example, on January","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"research mainly deals with analyzing and decrypting the digital sphere. With approximately 30 000 citations on Google Scholar, Professor Kaplan was counted amongst the Top 50 Business and Management authors in the world according to John Wiley & Sons. In 2020, Andreas Kaplan ranked in Stanford study of the world's top 2% of scientists.\n\nIn particular his 2010 article \"Users of the world, unite! The challenges and opportunities of social media\" published in Business Horizons is widely cited and known in the field. This seminal article recurrently achieved first place in Science Direct's annual list of the 25 most downloaded publications across all 24 core subject areas covered in Science Direct and thus was downloaded more often than any other of the approximately 13.4 million papers in the collection.\n\nPublications\n \n \n Kaplan, A. and M. Haenlein (2019) Siri, Siri in my Hand, who is the Fairest in the Land? On the Interpretations,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"written and edited to make them understandable to scientists in other fields and any educated person. \n\nIn the first pages of each issue are editorials, news and feature articles on issues of general interest to scientist. Topics include current affairs, science funding, business, scientific ethics and research breakthroughs. There are also sections on books and arts. Because of limits on the length of articles, often the printed text is a summary of the work in question. Details may be put in supplementary material on the journal's website.\n\nNature has spun off a series of more specialised journals which publish longer, more technical articles which would be unsuitable for Nature's wider readership.\n\nReferences \n\nScience and technology magazines\nWeekly magazines\nJournals","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"have 2. Then, it goes up until it is all ranked. You have to do this to both sets of data.\n\nStep two \nNext, we have to find the difference between the two ranks. Then, you multiply the difference by itself, which is called squaring. The difference is called , and the number you get when you square  is called .\n\nStep three \nCount how much data we have. This data has ranks 1 to 5, so we have 5 pieces of data. This number is called .\n\nStep four \nFinally, use everything we have worked out so far in this formula: .\n\n means that we take the total of all the numbers that were in the column . This is because  means total.\n\nSo,  is  which is 4. The formula says multiply it by 6, which is 24.\n\n is  which is 120.\n\nSo, to find out , we simply do .\n\nTherefore, Spearman's rank correlation coefficient is","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A nomogram, alignment chart or abaque is a graph for calculation. It is a two-dimensional diagram which gives a computation of a mathematical function. \n\nThe field of nomography was invented in 1884 by the French engineer Philbert Maurice d\u2019Ocagne (18621938). It was used for many years to provide engineers with fast graphical calculations of complicated formulas. Nomograms use a parallel coordinate system invented by d'Ocagne rather than standard Cartesian coordinates.\n\nA nomogram consists of a set of n scales, one for each variable in an equation. Knowing the values of n-1 variables, the value of the unknown variable can be found, or by fixing the values of some variables, the relationship between the unfixed ones can be studied. \n\nThe result is got by laying a straightedge across the known values on the scales and reading the unknown value from where it crosses the scale for that variable. The virtual or drawn line created by the straightedge is called an index line","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In the fields of communications, signal processing, and in electrical engineering more generally, a signal is any time-varying quantity. \n\nThe concept is broad, and hard to define precisely.  Definitions specific to subfields are common. For example, in information theory, a signal is a codified message, i.e., the sequence of states in a communications channel that encodes a message. In a communications system, a transmitter encodes a message into a signal, which is carried to a receiver by the communications channel.  For example, the words \"Mary had a little lamb\" might be the message spoken into a telephone. The telephone transmitter converts the sounds into an electrical voltage signal. The signal is transmitted to the receiving telephone by wires; and at the receiver it is reconverted into sounds.\n\nExamples of signals \n Motion.  The motion of a particle through some space can be considered to be a signal, or can be represented by a signal.  The domain of a motion signal is","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bandwidth is used to measure electronic and other types of communication. This includes radio, electronics, and other forms of electromagnetic radiation, Bandwidth is the difference between the electronic signal having highest-frequency and the signal having the lowest-frequency.\n\nIn computer networks, bandwidth is often used as a term for the data transfer bit rate.  More easily, the amount of data that is carried or passed from one point to another in a network, in a given time period (usually a second).\n\nFrequency\nMany systems work by means of continuous movements, or oscillations. Each complete \"back and forth\" it makes is called a cycle. The number of cycles every second is its frequency. Frequency is measured in cycles per second, most often called \"Hertz\", or \"Hz\" for short.\n\nSystems have at least one frequency, and usually many different frequencies.  For example, sound waves travel as vibrations.  People can hear sound frequencies low as 20\u00a0Hz, and high as","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library \u2013 Reference Manual \u2013 The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"higher average is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return).\n\nRules for normally distributed numbers\n\nMost math equations for standard deviation assume that the numbers are normally distributed. This means that the numbers are spread out in a certain way on both sides of the average value. The normal distribution is also called a Gaussian distribution because it was discovered by Carl Friedrich Gauss. It is often called the bell curve because the numbers spread out to make the shape of a bell on a graph. \n\nNumbers are not normally distributed if they are grouped on one side or the other side of the average value. Numbers can be spread out and still be normally distributed. The standard deviation tells how widely the numbers are spread out.\n\nRelationship between the average (mean) and standard deviation \nThe average (mean) and the standard deviation of a set of data are usually written together. Then a person can understand what the average number is and how widely other numbers in the group are spread out.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"sample standard deviation  over the normalizing term  (that is, ). In this way, the t-distribution can be used to estimate how likely it is that the true mean lies in any given range.\n\nThe t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero.  The Student's t-distribution is a special case of the generalised hyperbolic distribution.\n\nRelated pages \n\n F-distribution\n\nReferences\n\nProbability distributions","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Kolmogorov\u2013Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability theory and statistics, the chi-square distribution (also chi-squared or \u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with  degrees of freedom is written as . It is a special case of gamma distribution.\n\nChi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\n\nRelated pages \n\n Chi-squared test\n\nReferences\n\nOther websites\nChi-Square Tutorial by Khans Academy\n\nProbability distributions","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\u00a0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in star movement; optimization occurs in portfolio management; numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nComputers greatly helped this task. Before there were computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.\n\nFamous numerical software\nIn order to support numerical analysts, many kinds of numerical software has been created:\n MATLAB - made by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of the Melbourne College of Divinity\n Eidiko Tmima Alexiptotiston, Greek special operations unit\n A numerical weather prediction model formally known as the Eta","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Forecasting is studying and saying what is likely to happen in the future. It is similar to predicting, but usually forecasting is done with scientific methods. Forecasting can be done for many different things, like weather forecasting (predicting the weather) or economy forecasting.  Science cannot know the future for sure, so forecasters try to identify the most probable events, and sometimes they are wrong.\n\nWords","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Demand forecasting is when a business predicts future demand for its products. A business looks at many things when they do demand forecasting. Some of these things are past sales, data from test markets, and statistics. Businesses can also use educated guesses to help predict future demand. Businesses use demand forecasting to help them come up with the amount of demand for their products so they know how much supply to make.\n\nReferences\n\nBusiness\nCommerce\nEconomic theories","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Sustainable Development Goals (SDGs) are created by the [United Nations] and promoted as the Global Goals for Sustainable Development. They replaced the [Millennium Development Goals] that expired at the end of 2015. The SDGs run from 2015 to 2030. There are 17 goals and 169 specific targets for those goals.\n\nGoals\n\nIn August of 2015 193 countries agreed to the following 17 goals:\n\n No poverty \n Zero hunger \n Good health and wellbeing\n Quality education \n Gender equality\n Clean water and sanitation\n Affordable and clean energy \n Decent work and economic growth \n Industry, innovation and infrastructure\n Reduce inequality \n Sustainable cities and communities \n Responsible consumption and production\n Climate action \n Life below water\n Life on land\n Peace and justice.  Strong institutions\n Partnerships for the goals\n\nReferences\n\nSustainability\nDevelopment\nUnited Nations","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainable development''' is a way for people to use resources without the resources running out. It means doing development without damaging or affecting the environment.\nThe term used by the Brundtland Commission defined it as development with sustainability that \"meets the needs of the present and don't compromise the ability of future generations to meet their own needs.\"\n\nEveryone wants a better place to live. Some people want better homes and housing, while other people want better schools, more jobs, better shops, or cleaner and safer streets. Others may want all these things. Whatever the problems in any neighbourhood, they can usually be grouped into three issues. People need:\n a better environment \u2013 that means green spaces, play areas, no litter, nice gardens, decent houses, less noise and pollution. The resources used should renew over generations.\n a better economy \u2013 that means jobs, reasonable prices, cheaper heat and light, no loan sharks\n better social conditions \u2013 that means good leisure facilities, lots of community groups offering sports","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of life\u2019 is under threat: from crime, from pollution, or from living in neighbourhoods where no-one in authority seems to care.\n\nMany areas have programmes to promote \u2018local sustainability\u2019: many are called \u2018Local Agenda 21\u2019 plans, named after the international Agenda 21 action plan for sustainable development agreed at the United Nations Earth Summit held in 1992.\n\"Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs\". This is a definition offered by the famous World Commission on Environment and Development in its report Our Common Future.\n\nRelated pages\n Ecology\n Green economy\n Recycling\n Renewable energy\n Sustainable Development Goals\n Overpopulation\n\nReferences\n\nOther websites \n Appropedia - a Wiki focused on sustainable international development and poverty reduction\n Sustainable Development in Amazonia\n\nSustainable development","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Porto Alegre: Medical Arts.\n 2004. From being to doing: the origins of the biology of cognition, with Bernhard Poerksen.\n 2009. The Origins of humanness in the biology of love, with Gerda Verden-Zoller and Pille Brunnel.\n 2004. From biology to psychology. \n 2009. Sense of humanity.\n\nReferences \n\n1928 births\nBiologists\nSystems scientists\n2021 deaths\nChilean scientists\nChilean writers\nPhilosophers\nPeople from Santiago\nDeaths from pneumonia","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a short section of 30 pages at the end of the book. Yet because of this section, the (apparently) new field of sociobiology became the subject of heated controversy. The criticism was driven by political events of the day.\n\n\"The mid-1970s were years of intense political activity on campuses, much of it initiated by left-wing professors and their students who opposed the war in Vietnam. At Harvard University [Wilson's employer] the war... came under fire from a number of scholars of the Marxist or semi-Marxist persuasion... Marxist philosophy is founded on the premise of the perfectability of human institutions through ideological prescription. Therefore, persons with Marxist views were particularly unreceptive to the notion that an evolved 'human nature' exists\".\n\nCriticism by Richard Lewontin and Stephen Jay Gould, and the Sociobiology Study Group hinted that there was some relationship between these ideas and some of the worst events in history. The main concern of the critics seemed to be the idea that","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"easier to make if people accept a mechanistic paradigm - but it may be harder to say why it does not work, if one believes in these ideas.  For instance, creating diagnostic trees might be easier if one works from experience, not from an idea of how a technology should or must work.\n\nA controversial idea is that mechanistic ideas are just an older idea called scholasticism, with more mathematics.  Both tried to work from what should or must be, instead of what experiment seemed to show.\n\nAnother controversial idea is that scientism, belief in science as if it were a religion or ethical tradition, comes from this paradigm.  Most scientists who are mechanistic do not say they see science as a guide to ethics, but try to keep them separate.\n\nPhysics\nChemistry\nBiology","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Lateral thinking is a term invented by Edward de Bono, a Maltese psychologist, physician and writer. It first appeared in the title of his book The Use of Lateral Thinking, published in 1967. De Bono explains lateral thinking as methods of thinking about changing concepts and perception. Lateral thinking is about reasoning that is not immediately obvious and about ideas that may not be obtainable by using only traditional step-by-step logic. \n\nEveryday life","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Problem solving is a mental activity related to intelligence and thinking. It consists of finding solutions to problems. A problem is a situation that needs to be changed. It suggests that the solution is not totally obvious, for then it would not be a problem. A great deal of human life is spent solving problems. Social life is based on the notion that together we might solve problems which we could not as individuals. \n\nThe word \"problem\" comes from a Greek word meaning an \"obstacle\" (something that is in your way). If someone has a problem, they have to find a way of solving the problem. The way to solve it is called a solution. Some problem-solving techniques have been developed and used in artificial intelligence, computer science, engineering, and mathematics. Some are related to mental problem-solving techniques studied in gestalt psychology, cognitive psychology. and chess.\n\nProblems can be classified as ill-defined or well-defined.  Ill-defined problems are those that do not have clear goals, solution","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Universal reason is something that philosophers think is at the bottom of a thinking system that allows it to understand certain natural things that are generally complex.\n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"I will not walk to the table. If I want an apple, but think that the table across the room is just a picture of a table, then I will not walk to the table.\n\nThe neurological (brain) theory\nOther philosophers say that the want-think theory is not true. They say that it is my brain and my nerves that causes an action. They see the brain as working like a computer that directs small electric packages from some nerves to other nerves and stores some for a while in memory. These packages cannot be named, they are just like computer bits and bytes. These philosophers say that we talk about what we want and what we think because the brain is very complicated and we do not understand it yet.\n\nThese questions form part of the philosophy of mind.\n\nBranches of philosophy","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a process in which a physicist-observer takes part, but rather <u\/l>any interaction between classical and quantum objects regardless of any observer<\/u\/l>.\n\nThe idea of indeterminacy\n\nThe uncertainty principle came from Werner Heisenberg's matrix mechanics. Max Planck  already knew that the energy of a unit of light is proportional to the frequency of that unit of light (), and that its amount of energy can be expressed in familiar terms such as the joule by using a proportionality constant. The constant he gave the world is now called the Planck constant and is represented by the letter h.  When matrices are used to express quantum mechanics, frequently two matrices have to be multiplied to get a third matrix that gives the answer the physicist is trying to find. But multiplying a matrix such as P (for momentum) by a matrix such as X (for position) gives a different answer matrix from the one you get when you multiply X by P. The result of multiplying P by X","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"force carriers\u2014would appear to defy mechanical principles altogether. None could predict a quantum particle's location from moment to moment.\n\nIn the slit experiment, an electron would travel through one hole placed in front of it. Yet a single electron would travel simultaneously though multiple holes, however many were placed in front of it. The single electron would leave on the detection board an interference pattern as if the single particle were a wave that had passed through all the holes simultaneously. And yet this occurred only when unobserved. If light were shone on the expected event, the photon's interaction with the field would set the electron to a single position.\n\nBy the uncertainty principle, any quantum particle's exact location and momentum cannot be determined with certainty, however. The particle's interaction with the observation\/measurement instrument deflects the particle such that greater determination of its position yields lower determination of its momentum, and vice versa.\n\nField theory quantized \n\nBy extending quantum mechanics across a field, a consistent pattern emerged. From location","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"anything changed, but our measuring itself makes a change, and the best we can hope to do is to reduce to a minimum the energy we contribute to the electron by measuring it. That minimum amount of energy has the Planck constant as one of its factors.\n\nUncertainty goes beyond matrix math\n\nHeisenberg's uncertainty principle was found in the earliest equations of the \"new\" quantum physics, and the theory was given by using matrix math. However, the uncertainty principle is a fact about nature, and it shows up in other ways of talking about quantum physics such as the equations made by Erwin Schr\u00f6dinger.\n\nIndeterminacy in nature, not uncertainty of humans\n\nThere have been two very different ways of looking at what Heisenberg discovered: Some people think that things that happen in nature are \"determinate,\" that is, things happen by a definite rule and if we could know everything we need to know we could always say what will happen next. Other people think that things","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"out matrices. Matrices were new and strange, even for mathematicians of that time, but how to do math with them was already clearly known. He and a few others worked everything out in matrix form before Heisenberg came back from his time off, and within a few months the new quantum mechanics in matrix form gave them the basis for another paper.\n\nMax Born saw that when the matrices that represent pq and qp were calculated they would not be equal. Heisenberg had already seen the same thing in terms of his original way of writing things out, and Heisenberg may have guessed what was almost immediately obvious to Born\u2014that the difference between the answer matrices for pq and for qp would always involve two factors that came out of Heisenberg's original math: Planck's constant h and i, which is the square root of negative one.  So the very idea of what Heisenberg preferred to call the \"indeterminacy principle\" (usually known as the uncertainty principle) was","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"samples from a production lot) based on how well it met its design specifications. In contrast, Statistical Process Control uses statistical tools to observe the performance of the production process in order to predict significant deviations that may later result in rejected product.\n\nTwo kinds of variation occur in all manufacturing processes: both these types of process variation cause subsequent variation in the final product. The first is known as natural or common cause variation and consists of the variation inherent in the process as it is designed. Common cause variation may include variations in temperature, properties of raw materials, strength of an electrical current etc. The second kind of variation is known as special cause variation, or assignable-cause variation, and happens less frequently than the first. With sufficient investigation, a specific cause, such as abnormal raw material or incorrect set-up parameters, can be found for special cause variations.\n\nFor example, a breakfast cereal packaging line may be designed to fill each cereal box with 500\u00a0grams of product, but some boxes will have slightly more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"experimental data\".\n\nReferences\n\n \n Basu D. (1980b). \"The Fisher Randomization Test\", reprinted with a new preface in Statistical Information and Likelihood : A Collection of Critical Essays by Dr. D. Basu ; J.K. Ghosh, editor. Springer 1988.\n \n Salsburg D. (2002) The Lady Tasting Tea: how statistics revolutionized science in the Twentieth Century W.H. Freeman \/ Owl Book. \n\nExperiments\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Informatica, 30:3\u201331, 2006.\n\nFurther reading \n\n Bradley, R.A. and Terry, M.E. (1952). Rank analysis of incomplete block designs, I. the method of paired comparisons. Biometrika, 39, 324\u2013345.\n David, H.A. (1988). The Method of Paired Comparisons. New York: Oxford University Press.\n Luce, R.D. (1959). Individual Choice Behaviours: A Theoretical Analysis. New York: J. Wiley.\n Thurstone, L.L. (1927).  A law of comparative judgement. Psychological Review, 34, 278\u2013286.\n Thurstone, L.L. (1929).  The Measurement of Psychological Value.  In T.V. Smith and W.K. Wright (Eds.), Essays in Philosophy by Seventeen Doctors of Philosophy of the \tUniversity of Chicago.  Chicago: Open Court.\n Thurstone, L.L.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"20 intervals contain the true value of the parameter.\n\nPractical example\nA machine fills cups with margarine. It is adjusted so that the content of the cups is 250g of margarine. As the machine cannot fill every cup with exactly 250g, the content added to individual cups shows some variation, and is considered a random variable X.\n\nThis variation is assumed to be normally distributed around the desired average of 250g, with a standard deviation of 2.5g. To determine if the machine is adequately calibrated, a sample of n = 25 cups of margarine is chosen at random, and the cups are weighed. The weights of margarine are X1, ..., X25, a random sample from X.\n\nTo get an impression of the expectation \u03bc, an estimate is needed. The appropriate estimator is the sample mean: \t \n\nThe sample shows actual weights x1, ...,x25, with mean:\n\nIf we take another sample of 25 cups, we","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"quite common in science for some studies to contradict others, for example in cases where different methods are used to measure an outcome, or where human error or chance may lead to unusual results. This means that there is often a study someone can use to support their claim, and they can cherry pick that one study even if many more contradict it.\n\nReferences\n\nLogical fallacies","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"was the same length as the \u201cstandard\u201d one, in the first image. \n\nA control trial showed the task was rather easy. There, the participants made almost no mistakes in matching the correct lines. However, in the main part of the experiment, all except for one participant were confederates. This means that the researcher had told them how to behave during the experiment. They were told to give wrong answers to see how the other participant reacts. The other participant, however, believed that everyone else was also a participant like him. \n\nThe confederates and the participants formed groups of 7 to 9 students. Furthermore, they were asked to match the correct lines 12 times (12 trials). Three-fourths of participants gave the same, wrong answer as the confederates in at least one trial. All in all, participants conformed to the others on an average of 36.8% of the time. These results suggest that more than 3 out of 10","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and making choices in their own best interest. \n\"I will state that coercive persuasion and thought reform techniques are effectively practiced on na\u00efve, uninformed subjects with disastrous health consequences. I will try to give enough information to indicate my reasons for further inquiries as well as review of applicable legal processes\".\n\nThe following methods have been used in some or all cults studied:\n People are put in physically or emotionally distressing situations;\n Their problems are reduced to one simple explanation, which is repeatedly emphasized;\n They receive what seems to be unconditional love, acceptance, and attention from a charismatic leader or group;\n They get a new identity based on the group;\n They are subject to entrapment (isolation from friends, relatives and the mainstream culture) and their access to information is severely controlled.\n\nThis view is disputed by some. Society for the Scientific Study of Religion stated in 1990 that there was not sufficient research for a consensus, and that \"one should not automatically equate the techniques involved in the process","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"well developed, it develops even more common. The theory has been used to show why people join groups who really want things to happen (activist groups), to organize the external (real, actual) and internal (perceived) parts (dimensions) of problem recognition, and to learn whether information that is used in information processing (processed information) can create publics. Also, some more research was done on the internal and external dimensions of problem recognition, constraint recognition, and level of involvement (Grunig & Hon, 1988; Grunig, 1997). The research is about whether the ideas are internal or external. The research shows that if the concepts are internal, they can be changed by communication, and that if they are external, then the holdable things that are around the person need to be changed in order for the person's concept of the variables to change (Grunig, 1997, p.\u00a025). But, there is a small amount","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Tuckman's stages of group development is a model of group development made by Bruce Tuckman in 1965. It has four phases: Forming, Storming, Norming and Performing. \n\nForming is when the members of a team just got together.\nStorming is when they are in conflict and are not agreeing with each other.\nNorming is when they have finished deciding what is \"normal\".\nPerforming is when they are working together efficiently.\n\nTuckman believed that these stages are all necessary and always happen in order for a team:\nto grow\nto face challenges\nto tackle problems\nto find solutions\nto plan work\nto deliver results\n\nThis model has become the basis for later models.\n\nMore reading on this topic can be found here: \n\nPsychology\nPerforming this is a stage of a fully functional group where members see themselves as a group and get involved in the task.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"not in custody. That is one of the few things a police officer may not lie about. During questioning police may use trickery, lying or other forms of deception in order to get a confession from a suspect.\n\nPEACE method \nA fairly new technique is being used in the UK, Denmark, New Zealand and other places. The word PEACE is an acronym for Preparation and Planning, Engage and Explain, Account, Closure and Evaluate. It is considered a more ethical model of police questioning. It is intended to prevent wrongful convictions by eliminating any coercion or deceptive methods. When tested in England and Wales, it resulted in the same percentage of convictions as the older methods produced. Peace is more of a journalistic approach. It assumes a liar will find it increasingly difficult to keep all the lies consistent and will eventually break down and confess.\n\nThird degree \nA historic method of obtaining confessions is the applying of the so-called third degree. It consists of treating a suspect brutally, keeping them awake for long","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in ridge regression. The least squares approach can also be used to fit models that are not linear. As outlined above, the terms \"least squares\" and \"linear model\" are closely linked, but they are not synonyms.\n\nUsage\n\nEconomics\n\nLinear regression is the main analytical tool in economics. For example, it is used to guess consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand and labor supply.\n\nRelated pages \n\n Curve fitting\n Logistic regression\n Ordinary least squares\n\nReferences\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"which uses multiple values for the variable y. This form of logistic regression is known as multinomial logistic regression.\n\nLogistic regression uses the logistic function to find a model that fits with the data points. The function gives an 'S' shaped curve to model the data. The curve is restricted between 0 and 1, so it is easy to apply when y is binary. Logistic regression can then model events better than linear regression, as it shows the probability for y being 1 for a given x value. Logistic regression is used in statistics and machine learning to predict values of an input from previous test data.\n\nBasics \nLogistic regression is an alternative method to use other than the simpler linear regression. Linear regression tries to predict the data by finding a linear \u2013 straight line \u2013 equation to model or predict future data points. Logistic regression does not look at the relationship between the two variables as a straight line. Instead, Logistic regression uses the natural logarithm function to find the relationship between the variables","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Isotherms are lines drawn around places with the same temperature range on isotherm maps. Each point on this line shows one temperature reading, or the average of many temperature readings. Isotherm maps also have scales that tell the signals or colors for the different temperatures. Isotherm lines are usually curvy and not straight lines.\n\nMeteorology\nThermodynamics\n\nen:Contour line#Temperature and related subjects","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A weather map is a tool. It shows facts about the weather quickly.  Weather maps have been used from the mid-19th century, for study and for weather forecasting. Some maps show differences of temperature, and weather fronts. \n\nA station model is a symbolic picture showing the weather at a reporting station. Meteorologists made the station model to put down many weather elements in a small space on weather maps. Maps thickly filled with station-model plots can be hard to read. However, they help meteorologists, pilots, and mariners to see important weather patterns. A computer draws a station model for every place of observation. The station model is mostly used for surface-weather maps. It can also be used to show the weather in the sky, though. A complete station-model map lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\nHistory \n\nPeople first began using weather charts in a modern way in the mid-19th century. They began using","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is the best known example of using thematic maps for analysis of data. His method anticipates the principles of a geographic information system (GIS). He started with an accurate map of a London neighborhood which included streets and water pump locations. Onto this Snow placed a dot for each cholera death. The pattern centered around one particular pump on Broad Street. At Snow\u2019s request, the handle of the pump was removed, and new cholera cases ceased almost at once. Further investigation of the area revealed the Broad Street pump was near a cesspit under the home of the outbreak's first cholera victim.\n\nReferences \n\nCartography","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better known.\n\nHistorical aspects \nElements of scientific method were worked out by some early students of nature.\n \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\" Ptolemy (85165\u00a0AD). This is an early example of what we call Occam's razor.\n Ibn al-Haytham (Alhazen) (965\u20131039), Robert Grosseteste (1175\u20131253) and Roger Bacon (1214\u20131294), all made some progress in developing scientific method.\n Scientists in the 17th century started agreeing that the experimental method is the main way to find the truth. This was done in western Europe by men like Galileo, Kepler, Hooke, Boyle, Halley and Newton. At the same time, the microscope and the telescope were invented (in Holland), and the Royal Society was formed. Instruments,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Book of Optics is a book that was written by Ibn al Haytham (965\u20131040 CE), an Arabic scientist and polymath. It was a seven-volume treatise on optics, physics, mathematics, anatomy and psychology, from 1011 to 1021. It was originally written in Arabic and was later translated into Persian, Latin and Italian within the next several centuries. The book had an important influence on the development of optics and on science in general because it introduced the experimental scientific method. Ibn al-Haytham has been called the \"father of modern optics\". Ibn al-Haytham wrote more than 200 works on a wide range of subjects, of which at least 96 of his scientific works are known.\n\nthe book of optics is also written en part by Ibn al-Haythams nephew at four years old\n\nScience books","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"techniques, which display a relationship between two variables that take either discrete or a continuous ranges of values; examples:\n\nSchematics and other types of diagrams, e.g.,\n\nReferences \n\n \nNon-verbal communication","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Computer graphics are visual representations of data, made with the help of computer. Computer graphics can be a series of images (video or animation) or a single image.\n\nComputer graphics are very useful. Computer-generated imagery is used for movie making, video games, computer program development, photo editing, scientific modeling, design for advertising and more. Some people see computer graphics as art.\n\nAreas of computer graphics \nComputer graphics can be 2D or 3D. They are made differently and used differently. People can use computer programs to make different types of graphics.\n\n2D graphics \n2D computer graphics are usually split into two categories: vector graphics and raster graphics.\n\nVector graphics \n\nVector graphics use lines, shapes and text to create a more complex image. If a vector graphic image is made very big on the monitor, it will still look as good (smooth) as its regular size. This is one of the reasons vector graphics are liked so much. Vector images also take very little computer memory when","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"fields. A simple example is attempting to find the smallest possible difference in the distance of two objects in two-dimensional space (x and y). In this context, the derivative of the function that gives the difference is taken in order to find the minimum. A more complicated example is in Machine Learning, in which the optimization function attempts to find the global minimum of the loss function in order to minimize the difference or loss between the algorithm\u2019s predictions and the actual values. This example is more difficult as Machine learning algorithms often utilize multidimensional data usually in the form of tensors yielding more complicated functions.\n\nRelated software\nToday, there are many tools to support optimization studies:\n MATLAB\n Wolfram Mathematica\n\nReferences\n\nScience\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to make something that can say how the different data attributes are connected\/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence (e.g.: ant colony optimization, particle swarm optimization) and methods based on integer linear programming. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\nHistory \nIn 1950, Alan Turing proposed a \"learning machine\" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.  His 1954 publication was not widely noticed. Starting in 1957,  the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a question or a problem about nature. Some problems are simple, such as \"how many legs do flies have?\" and some are very deep, such as \"why do objects fall to the ground?\"\n Next, scientists investigate the problem. They work at it, and collect facts. Sometimes all it takes is to look carefully.\n Some questions cannot be answered directly. Then scientists suggest ideas, and test them out. They do experiments and collect data.\n Eventually, they find what they think is a good answer to the problem. Then they tell people about it.\n Later, other scientists may agree or not agree. They may suggest another answer. They may do more experiments. Anything in science might be revised if we find out the previous solution was not good enough.\n\nAn example \nA famous example of science in action was the expedition led by Arthur Eddington to Principe Island in Africa in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"refers to a way of pursuing knowledge, not just the knowledge itself. It is mainly about the phenomena of the material world. The Greek works into Western Europe from the 6th to 7th century B.C. revived \"Philosophy\". In the 17th and 18th centuries scientists increasingly sought to formulate knowledge in terms of laws of nature such as Newton's laws of motion. And during the 19th century, the word \"science\" became more and more associated with the scientific method itself. It was seen as a way to study the natural world, including physics, chemistry, geology and biology.\n\nIt was also in the 19th century that the term scientist was created by William Whewell. He meant it tell the difference between those who looked for knowledge on nature from those who looked for other types of knowledge.\n\nThe scientific method is the name given to the methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Meditation tries to get past the \"thinking\" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic Books.  [a popular exposition emphasizing technique]<\/ref>\n\n Clear goals. Expectations and rules are known and goals are attainable and within one's skills and abilities.  Moreover, the challenge level and skill level should both be high.\n Concentrating: a high degree of concentration on a limited field of attention (a person engaged in the activity will have the opportunity to focus and to delve deeply into it).\n A loss of the feeling of self-consciousness.\n Distorted sense of time, one's sense of time is altered.\n Direct and immediate feedback (successes and failures in the course of the activity are apparent, so that behavior can be adjusted as needed).\n A balance between ability level and challenge: the activity is neither too easy nor too difficult.\n A sense of control over the situation or activity.\n The activity is intrinsically rewarding, so there is an effortlessness of action.\n A lack","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the mind can better see the cause of pain. Buddhists believe that this type of seeing can end pain.\n\nBuddhist meditation is not just used for spiritual reasons. Research shows that Buddhist meditation lowers stress, anxiety and depression.\n\nMost types of Buddhist meditation focus on something. The most popular things to focus on include breath,  metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds.\n\nChristian meditation \nChristians sometimes meditate by thinking about small parts of the Bible, or by saying the words of a prayer to themselves over and over. Meditation is an expression of Christian prayer. In the Catechism of the Catholic Church is specified that by means of meditation \"The mind seeks to understand the why and how of the Christian life, in order to adhere and respond to what the Lord is asking\"; also it is pointed out that \"meditation engages thought, imagination, emotion, and desire. This mobilization of faculties is necessary in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"your hearts be troubled and do not be afraid.\" ()\n\nInner peace \n\nInner peace (or peace of mind) refers to a state of being mentally and spiritually at peace, with enough knowledge and understanding to keep oneself strong in the face of stress. Being \"at peace\" is considered by many to be healthy and the opposite of being stressed or anxious. Peace of mind is generally associated with bliss and happiness.\n\nPeace of mind, serenity, and calmness are descriptions of a disposition free from the effects of stress. In some cultures, inner peace is considered a state of consciousness or enlightenment that may be cultivated by various forms of training, such as prayer, meditation, Tai chi chuan or yoga, for example. Many spiritual practices refer to this peace as an experience of knowing oneself.\n\nMovements and activism\n\nPeace movement \n\nA movement that seeks to get ideals such as the ending of a particular war, minimize inter-human violence in a particular place or type of situation, often linked to the goal of","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund SITTING and the nouns agent and location express the diagram's basic relationship; \"agent is SITTING on location\"; Elsie is an instance of CAT.\n\nAlthough the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter's illustration of that ambiguity, with a progression from abstract to concrete in G\u00f6del, Escher, Bach (1979):\n(1) a publication\n(2) a newspaper\n(3) The San Francisco","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Each of these smaller areas (room, state, number) is next to other small areas (other rooms\/states\/numbers). The places where the areas meet are connections. If we write down on paper a list of spaces, and the connections between them, we have written down a description of a space -- a topological space. All topological spaces have the same properties such as connections, and are made of the same structure (a list of smaller areas). This makes it easier to study how spaces behave. It also makes it easier to write algorithms. For instance, to program a robot to navigate a house, we simply give it a list of rooms, the connections between each room (doors), and an algorithm that can work out which rooms to go through to reach any other room. For more examples of this type of problem, look at Graph theory.\n\nWe can go further by creating subdivisions of subdivisions of space. For instance, a nation divided into states, divided into counties, divided into","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"writing space in which traces of authority persist only as local and contingent effects, the social equivalent of the deconstructed author-function. A \"populite\" culture might mark the first step toward realization of Jean-Francois Lyotard's \"game of perfect information\" where all have equal access to the world of data, and where \"[g]iven equal competence (no longer in the acquisition of knowledge, but in its production), what extra performativity depends on in the final analysis is 'imagination,' which allows one either to make a new move or change the rules of the game.\" This is the utopia of information-in-process, the ultimate wetware dream of the clerisy: discourse converted with 100 percent efficiency into capital, the mechanism of that magical process being nomology or rule-making\u2014admittedly a rather specialized form of \"imagination.\"\n\nWorks by Ted Nelson\n Life, Love, College, etc. (1959)\n Computer Lib: You can and must","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"It can blur the distinction between abstract and concrete things.\n\nCompression\n\nAn abstraction can be seen as a process of mapping multiple different pieces of constituent data to a single piece of abstract data based on similarities in the constituent data, for example many different physical cats map to the abstraction \"CAT\". This conceptual scheme emphasizes the inherent equality of both constituent and abstract data, thus avoiding problems arising from the distinction between \"abstract\" and \"concrete\". In this sense the process of abstraction entails the identification of similarities between objects and the process of associating these objects with an abstraction (which is itself an object).\nFor example, picture 1 above illustrates the concrete relationship \"Cat sits on Mat\".\nChains of abstractions can therefore be constructed moving from neural impulses arising from sensory perception to basic abstractions such as color or shape to experiential abstractions such as a specific cat to semantic abstractions such as the \"idea\" of a CAT to classes of objects such as \"mammals\" and even categories","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"law \n\nWeinberg developed the principle of genetic equilibrium independently of British mathematician G.H. Hardy. He delivered an exposition of his ideas in a lecture on 13 January 1908, about six months before Hardy's paper was published in English. His lecture was printed later that year in the society's yearbook. \n\nWeinberg's contributions were unrecognized in the English speaking world for more than 35 years. Curt Stern, a German geneticist who emigrated to the United States before World War II, pointed out in a brief paper in Science that Weinberg's exposition was both earlier and more comprehensive than Hardy's.\n\nAscertainment bias \nWeinberg pioneered studies of twins, and developed techniques to analyse phenotypic variation. His aim was to partition this variance into genetic and environmental components. In the process, he recognized that ascertainment bias was affecting many of his calculations, and he produced methods to correct for it. \n\nWeinberg observed that proportions of homozygotes","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and the length of the month are also ancient.\n\nThe Round-to-even method has served as the ASTM (E-29) standard since 1940. The origin of the terms unbiased rounding and statistician's rounding are fairly self-explanatory. In the 1906 4th edition of Probability and Theory of Errors  Robert Simpson Woodward called this \"the computer's rule\" indicating that it was then in common use by human computers who calculated mathematical tables. Churchill Eisenhart's 1947 paper \"Effects of Rounding or Grouping Data\" (in Selected Techniques of Statistical Analysis, McGrawHill, 1947, Eisenhart, Hastay, and Wallis, editors) indicated that the practice was already \"well established\" in data analysis.\n\nThe origin of the term \"bankers' rounding\" remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and samples within the experiment. For example, when considering students and course choice, a sample size of 30 or 40 students is likely not large enough to generate significant data. Getting the same or similar results from a study using a sample size of 400 or 500 students is more valid.\n\nHistory \nThe earliest and most basic concept of degrees of freedom was noted in the early 1800s, intertwined in the works of mathematician and astronomer Carl Friedrich Gauss. The modern usage and understanding of the term were expounded upon first by William Sealy Gosset, an English statistician, in his article \"The Probable Error of a Mean,\" published in Biometrika in 1908 under a pen name to preserve his anonymity.1\n\nIn his writings, Gosset did not specifically use the term \"degrees of freedom.\" He did, however, give an explanation for the concept throughout the course of developing what would eventually be known as Student\u2019s T-distribution. The actual","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size\u2014if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"observe and analyze the specific behavior of a subject in that setting. The cause and effect of a certain behavior, though, is tough to analyze due to presence of multiple variables in a natural environment. Most of the data collection is based not entirely on cause and effect but mostly on correlation. While field research looks for correlation, the small sample size makes it difficult to establish a causal relationship between two or more variables.\n\nMethods of Field research \nField research is typically conducted in 5 distinctive methods. They are:\n\n Direct Observation\n\nIn this method, the data is collected via an observational method or subjects in a natural environment. In this method, the behavior or outcome of situation is not interfered in any way by the researcher. The advantage of direct observation is that it offers contextual data on people, situations, interactions and the surroundings. This method of field research is widely used in a public setting or environment but not in a private environment as it raises an ethical dilemma.\n\n Participant Observation\n\nIn this method of field","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". Computer Graphics Forum, 22  2,  171\u2013179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". Computers & Graphics, 30  6,  1020\u2013 1026.\n\nSurveys\nL. H. de Figueiredo and J.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Methods\nMany PDEs have appeared from physics. So we can think about difference methods preserving physical properties. These difference methods are known as structure preserving numerical methods. The following list is the examples of them:\n Symplectic integrators\n Discrete gradient method\n Discrete variational derivative method (DVDM)\nSome experts are studying their relation between numerical linear algebra.\n\nOthers\nThe difference mthods in above have high accuracy, but their usage is limited because they depend on the behaviour of the given PDEs. This is why new types of FDM are still studied. For example, the following methods are studied:\n Shortley-Weller approximation\n Swarztrauber-Sweet approximation\n Ascher-Mattheij-Russell difference formula\n\nValidated Numerics for PDEs\n\nNot only approximate solvers, but the study to \"verify the existence of solution by computers\" is also active. This study is needed because numerically obtained solutions could be phantom solutions (fake","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a graphics gallery, a replay function, and the ability to view various endings once they are achieved during a normal game.\n\n\u00a0Manufacturer's description:\n\nIs a hero, to keep with the story made up only doing the animation drives ?love story begins with the fourth series of the port city of snow. The main character, had received the emotions live in apartments next to Sakuragi, loses his love. One day, I told the police that the incident happened to. When leading a secret desire, a mixture of truth and falsehood.\n\n\u00a0Features:\n\n First person perspective.\n 2D graphics\n Cartoon graphics\n Mystery, dating & Anime themes.\n\nPlayStation games\nPlayStation Portable games\n1998 video games\n2005 video games\nVisual novels\nJapan exclusive video games","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"She has studied about them until now and taught about cultural anthropology in Kawamura Gakuen Woman\u2019s University\n\nHer career as a writer started in 1989. She published her first work \u201cSeirei-no-Ki (Spirit Tree)\u201d. From that year she has written many stories until today.\n\nFictional Works\n\nShika-no-Oh series (King of the Deers) \nVan, a man who was the top of the \u201cDokkaku (a kind of army)\u201d , Yuna, a young girl who was saved by Van and Hossal, a doctor, had fought with the mysterious sickness named \u201cMizzal (black-wolf fever)\u201d.\n\nKemono-no-Soja series (The Beast Player) \nErin, an orphan girl met with a sacred young animal and grew with it experiencing various things, meeting with a lot of people and learning about what the creatures are.\n\nKoteki-no-Kanata (Beyond the Fox Whistle)","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"can be viewed. The game also makes the player view some of the files that the game uses to play.\n\nThe game has a lot of foreshadowing that tells the player certain things about the story. If the player decodes the character files, there are extra stories unlocked. At certain times special files appear in the game files. Text documents show up about the next scenes.\n\nPlot\nThe player's character is invited by his childhood friend, Sayori, to join the Literature Club. He meets the other members of the club. They are Natsuki, Yuri, and Monika. The player begins to get involved in the club and getting to know the girls better. Near the end of the first act, Sayori tells the player that she has depression and loves the player. Sayori soon commits suicide by hanging herself. The game shows a lot of glitches and it restarts.\n\nWhen the player begins a new game, the game is very close to the first run, but","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"especially prevalent in Japan, where they made up nearly 70% of the PC game titles released in 2006. Visual novels are distinguished from other game types by their extremely minimal gameplay. Typically the majority of player interaction is limited to clicking to keep the text, graphics and sound moving (most recent games offer \"play\" or \"fast-forward\" toggles that make even this unnecessary). Most visual novels have multiple storylines and many endings; the gameplay mechanic in these cases typically consists of intermittent multiple-choice decision points, where the player selects a direction in which to take the game.\n\nDevelopment \nSimple 2000 Series Vol.122 - The Ningyo Hime Monogatari - Mermaid Prism is a japanese dating sim, this female-targeted love simulation puts players in the role of a high school student that one day gets warped to a separate world, becoming a mermaid along the way! Good thing she was a member of her school's swimming club! The game allows players to move","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"has the disadvantage that the same list of values does not have a well-defined, deterministic median.\n\nMedian and mean \nMedian and mean are different in several ways. Mean is a better measure in many cases, because many of the statistical tests can use mean and standard deviation of two observations to compare them, while the same comparison cannot be performed using the medians.\n\nMedian is more useful when the variance of the values is not important, and we only need a central measure of the values. If the maximum value of a set of numbers changes while the other numbers of this set are kept the same, the mean of this set of numbers changes, but the median does not.\n\nAnother advantage of median is that it can be calculated sooner when we are studying survival data. For example, a researcher can calculate the median survival of patients with a kidney transplant, when half the patients participated in his study die. Calculating the mean survival requires continuing the study, and following all the patients until their death.\n\nExample","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"are often used in these models.\n Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and  query, and as such can easily incorporate almost any other retrieval  model as just a yet another feature.\n\nSecond dimension: the properties of the model \n Models without term-interdependancies treat different terms\/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\n Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, Poisson distribution is a probability distribution. It is named after Sim\u00e9on Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The calculation of the last three quantities is explained in the respective Wiki pages. Then, with the help of formulas given in the previous section, the factors \u03bc and \u03b2 can be calculated. In this way, the CDF of the Gumbel distribution belonging to the data can be determined and the probability of interesting data values can be found.\n\nApplication \n\nIn hydrology, the Gumbel distribution is used to analyze such variables as monthly and annual maximum values of daily rainfall and river discharge volumes, and also to describe droughts.\n \nThe blue picture illustrates an example of fitting the Gumbel distribution to ranked maximum one-day October rainfalls showing also the 90% confidence belt based on the binomial distribution.\n\nReferences \n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson distribution\n\nReferences \n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of all people have green eyes). (n=500, p=0.05)\nIn order to use the binomial distribution, the following must be true about the problem:\n The outcomes are mutually exclusive, that is, there are two possible outcomes which cannot occur simultaneously (for example. in flipping a coin, there are two possible outcomes: heads or tails. It is always one or the other, never both or a mix of outcomes).\n The probability of a success (p) is consistent throughout the problem (for example, a basketball player makes 85% of his free throws. Each time the player attempts a free throw, 85% is assumed to be the likelihood of a made shot).\n The trials are independent of each other (for example, on the second flip of a coin, the first outcome does not impact the chance of the next toss: the chance of tossing a heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"that submerge it by a time in the language no figurativo. Also Manterola  has worked the sculpture, converting each piece in an only piece using different materials so much the iron , steel cut ,copper , iron galvanised , nylon, stainless steel, resin or iron painted.\n\nAnother of the appearances to stand out of Manterola is that it believes in the synergies and in the collaborations with other artists and disciplines. It says that it exists a mutual enrichment in all maridaje. Proof of this are the exercises with artists like Gorka Larra\u00f1aga and Samuel Dougados, the project UNYON beside Ivory Jewellers and, more recently, the collaboration with the chef French H\u00e9l\u00e8ne Darroze, that has 3 stars Michelin, for the one who has designed the servilleteros of his restaurant of Paris and where in each table luce one of his sculptures.\n\nBibliography \n Article of Him Figaro of 18 September","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"authors of the various paintings, will draw the contours of the paintings on the roadway, first with chalk and then with lime, based on the original sketch, also using perforated cartoons (dusting technique).  The placement of the flowers is done by placing the petals, taken from the baskets, within the contours already traced on the street.  On Sunday evening Mass is celebrated in front of the Church of Saint Maria of Cima and, after a solemn Eucharistic celebration in which the bishop of Albano often takes part, the carpet is covered by the religious procession of the celebrants who bring the Blessed Sacrament to the not so distant Collegiate Church of the Holy Trinity.  The carpet is maintained, with replacements of withered petals, until Monday evening, when the destruction of the floral display bychildren (the so-called shoulder ) takes place.\n\nNote \n\n Anna Baldazzi and Renato Torti (edited by).  Genzano and the flower display   : anthology of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"actual nature;\n in an ironic sense, it alludes to the idea that an already established artist would find a market and the consent of a critic for any work he produces, beyond its specific quality;\n the operation of Garau at the same time the artistic value of this work by Piero Manzoni is delicately conceptual art, and therefore accessible to all without restrictions due to either the purchase cost, material possession or physical accessibility, nor due to technical reproducibility. It is therefore, according to Duchamp, a typical \"anesthetic\".\n\nRelated works\n\nFiato d\u2019artista\nThe most famous related work is the Fiato d\u2019artista (Artist\u2019s Breath), involving red, blue or white balloons inflated by Piero Manzoni himself, closed with string and lead, with the name \"Piero Manzoni\" punched into it, then attached to a wooden base with a plaque on it using gesso. The pieces were made in 1960, and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"very natural, because they have been drawn from looking at real people. The clothes of the figures are not arranged to form a beautiful pattern, like the clothes in Cimabue's paintings. They fit the figures and hang in a natural way like real clothes. This more natural way of showing people was started by Pietro Cavallini, but Giotto took the new ideas much further.\n\nIn the paintings around the walls of the Scrovegni Chapel, each scene looks like a shallow stage with actors on it. There are always some buildings or landscape such as a rocky hill, so that the viewer can see where the action is happening. The figures in each scene are carefully arranged so that the viewer can imagine that they are right there, taking part in the action.\n\nThe figures are not just shown with natural bodies, clothing and action. Giotto is a brilliant story-teller, because he shows the emotions of the characters in each painting, in both their faces and their","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"4D, meaning the common 4 dimensions, is a concept in mathematics. It has been studied by mathematicians and philosophers since the 18th century.  Mathematicians who studied four-dimension space in the 19th century include M\u00f6bius, Schl\u00e4fi, Bernhard Riemann, and Charles Howard Hinton.\n\nIn geometry, the fourth dimension is related to the other three dimensions  of length, width, and depth by imagining another direction through space. Just as the dimension of depth can be added to a square to create a cube, a fourth dimension can be added to a cube to create a tesseract.\n\n4D is also an important idea in physics, developed in the 20th century. In physics, it refers to the idea of time as a fourth dimension, added to the (3D) spatial dimensions.  Albert Einstein developed the idea of spacetime by connecting space and time together. The difference is that spacetime is not a Euclidean space,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"objects in everyday life are specified by three dimensions, which are usually called length, width and depth. Mathematicians call this concept Euclidean space.\n\nDimensions can be used to measure position too. The distance to a position from a starting place can be measured in the length, width and height directions. These distances are a measure of the position.\n\nIn some occasions, a fourth (4D) dimension, time, is used to show the position of an event in time and space.\n\nOther Dimensions\nIn modern science, people use other dimensions. Dimensions like temperature and weight can be used to show the position of something in less simple spaces. Scientist study those dimension with dimensional analysis.\n\nMathematicians also use dimensions. In mathematics, dimensions are more general. Dimensions in mathematics might not measure things in the world. The rules for doing arithmetic with dimensions in mathematics might be different than usual arithmetic rules.\n\nDimensions and vectors\nVectors are used to show distances and directions. Vectors are often used in engineering and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In dimensional analysis, a dimensionless quantity (or more precisely, a quantity with the dimensions of 1) is a quantity without any physical units and thus a pure number. Such a number is typically defined as a product or ratio of quantities which do have units, in such a way that all the units cancel out.\n\nExample \n\"out of every 10 apples I gather, 1 is rotten.\" -- the rotten-to-gathered ratio is (1 apple) \/ (10 apples) = 0.1 = 10%, which is a dimensionless quantity.\n\nList of dimensionless quantities \nThere are infinitely many dimensionless quantities and they are often called numbers. Some of those that are used most often have been given names, as in the following list of examples (alphabetical order):\n\nOther websites \n Biographies of 16 scientists with dimensionless numbers of heat and mass transfer named after them \n How Many Fundamental Constants Are There? by John Baez\n Michael Sheppard,","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"...,x25, with mean:\n\nIf we take another sample of 25 cups, we could easily expect to find values like 250.4 or 251.1 grams. A sample mean value of 280 grams, however, would be extremely rare if the mean content of the cups is in fact close to 250g. \n\nThere is a whole interval around the observed value 250.2 of the sample mean within which, if the whole population mean actually takes a value in this range, the observed data would not be considered particularly unusual. Such an interval is called a confidence interval for the parameter \u03bc. \n\nTo calculate such an interval, the endpoints of the interval have to be calculated from the sample, so they are statistics, functions of the sample X1, ..., X25, and hence are random variables themselves.\n\nIn our case, we may determine the endpoints by considering that the sample mean  from a normally distributed sample is also normally distributed, with the same expectation \u03bc, but","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Michigan Algorithm Decoder\n Magnetic anomaly detector, detects minute variations in Earth's magnetic field\n Maritime anomaly detection in Global Maritime Situational Awareness, for avoiding maritime collisions\n Mathematicians of the African Diaspora, website highlighting mathematical contributions of members of the African diaspora\n Methodical Accelerator Design, a CERN scripting language\n Modified Atkins diet, a specific form of ketogenic diet\n Mothers against decapentaplegic, a gene discovered in Drosophila\n MPEG Audio Decoder, audio decompression software\n Multi-conjugate Adaptive optics Demonstrator, an astronomical method\n Multi-wavelength anomalous dispersion, a technique used in X-ray crystallography\n\nStatistics\n Mean absolute deviation, a measure of the variability of quantitative data\n Mean absolute difference, a measure of statistical dispersion\n Median absolute deviation, a statistical measure of variability\n\nTelevision and video\n Mad TV, a 1995\u20132009 American sketch comedy television series inspired by Mad magazine\n Mad (TV series), a","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the idea to a prospective producer, director, or composer.\n\nScenarios are also used in policy planning, and when trying out strategies against uncertain future developments. Here the key idea is for the scenario to be an overview, a summary, of a projected course of action, events or situations. Scenarios are widely used by organizations of all types to understand different ways that future events might unfold.\n\nTheater\nPlanning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scenario may refer to:\n Scenario, a brief description of an event.\n Screenplay, in movies.\n Scenario (computing), a typical interaction between the user and the system or between two software components.\n Scenario analysis, a process of analysing possible future events by considering alternative possible outcomes. \n Scenario paintball, a variant of the game of paintball.\n Scenario planning, a strategic planning method that some organisations use to make flexible long-term plans.\n Scenario test, a test based on a hypothetical story used to help a person think through a complex problem or system.\n Kingmaker scenario, in a game of three or more players, is an endgame situation where a losing player, him- or herself unable to win, has the capacity to determine which player among others is the winner.\n User scenario, used to communicate an idea for a product or experience involving interactivity.\nScenario (A Tribe Called Quest), a 1992 song by hip-hop group, A Tribe Called Quest.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Planning is the way most organizations work to do big projects. It is thinking about what needs to happen, and then making a detailed plan. It may include a schedule.\n\nAll humans plan to some extent: it is a fundamental property of intelligent behaviour. In big organisations and government planning is a main activity. It combines forecasting  developments with scenarios of how to react to them.\n\nForecasting is predicting what the future will look like, whereas planning predicts what the future should look like.\n\nThe \"nuts and bolts\" of planning are the documents, diagrams, and meetings, the objectives and the strategy to be followed. Beyond this, planning has a different meaning depending on the context in which it is used.\n\nThe counterpart to planning is self-organization, when order emerges spontaneously out of seeming chaos.\n\nReferences","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"where they were.\n\nA new object called a Crystal Framer was added. It can be pushed, as can an Emerald Framer, and it blocks enemy movements and shots. However, when Rara, known as Lala outside of Japan, shoots a shot, it is reflected clockwise or counter-clockwise, depending on the type of Crystal Framer, allowing her to put monsters such as Medusas or Don Medusas in eggs.\n\nConstruct Mode\nThe game can be used to make level maps using \"Construct Mode\". All of the enemies, options and the terrains and their music that are available in the main game are available. These files are saved as CONSTRUCT.TXT, and can be emailed to friends. They can also be shared online, which is normal in many Eggerland communities. Eggerland for Windows 95 and Revival! Eggerland haVE more available terrains and music files, but all releases can load the same files. However, Eggerland","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship\/affect, communication, economic transactions, interactions, kinship, authority\/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and\/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called \"Law of 150\" which suggests that about 150 people is the best size for a village or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"church or temple is almost always a center of a social network). Often the network has an identity of its own which is quite real, even though it may have no official recognition. Networks may be centered on places, or on families, or on worldwide communities with common interests.\n\nSources \nThe Law of 150 is documented in R.I.M. Dunbar 1992. Neocortex size as a constraint on group size in primates. Journal of Human Evolution. 20, pp.\u00a0469\u2013493.\n\nThe field of study which investigates human social life is social psychology.\n\nRelated pages \nSocial network service\n\nRelationships\nSocial groups","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Stanley Milgram (August 15, 1933 \u2013 December 20, 1984) was an American social psychologist. He is most famous for his controversial study about obedience to authority figures. In these experiments, Milgram showed that about two out of three people would follow orders of an authority figure to the point of risking the life of, or perhaps even killing, an innocent person.  He got the idea from the Holocaust. He wrote a book about his experiments called Obedience to Authority.\n\nThe idea of six degrees of separation comes from Milgram's 1967 small-world experiment. It has been criticized a lot, but in 2008 Microsoft found that the average chain of contacts between users of its '.NET Messenger Service' was 6.6 people. A study published in the January 2014 volume of Computers in Human Behavior found that the average number of acquaintances separating people in unusual jobs is 3.9, and 3.2 for average Facebook","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the base.\n\nFor example, a descriptive system widely used in Australia is based on structural characteristics based on life-form, plus the height and amount of foliage cover of the tallest layer or dominant species.\n\nFor shrubs 2\u20138 m high the following structural forms are categorized:\n dense foliage cover (70\u2013100%) \u2014 closed-scrub\n mid-dense foliage cover (30\u201370%) \u2014 open-scrub\n sparse foliage cover (10\u201330%) \u2014 tall shrubland\n very sparse foliage cover (<10%) \u2014 tall open shrubland\n\nFor shrubs less than 2 m high the following structural forms are categorized:\n dense foliage cover (70\u2013100%) \u2014 closed-heath\n mid-dense foliage cover (30\u201370%) \u2014 heath\n sparse foliage cover (10\u201330%) \u2014 low shrubland\n very sparse foliage cover (<10%) \u2014 low open shrubland\n\nReferences\n\nOther websites\nSelecting Shrubs for Your Home (University of Illinois","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"this area.  This is also where farming takes place.\n Below 1,000 metres are the lowlands.  Here, a larger variety of plants are produced. Aside from plants, villages are also in the lowlands because the temperature is easier for humans and farm animals.\n\nThe Alps is a classic example of what happens when a temperate area at lower altitude gives way to higher land. A rise from sea level into the upper regions causes the temperature to decrease. The effect of mountain chains on winds is to carry warm air belonging to the lower region into an upper zone, where it expands and loses heat, and drops snow or rain.\n\nPlants \nThe typical trees\u2014oak, beech, ash and sycamore maple have a natural height limit: the 'tree line'. Their upper limit matches the change in climate which comes with increasing height. The change from a temperate to a colder climate is also shown true by a change in the wild flowering plant life. This limit","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"more than one class, then all of those exams must be at a different time. The teacher wants to know if he can schedule all of the exams in the same day so that every student is able to take the exam for each of their classes.\n A farmer wants to take 100 watermelons of different masses to the market. She needs to pack the watermelons into boxes. Each box can only hold 20 kilograms without breaking.  The farmer needs to know if 10 boxes will be enough for her to carry all 100 watermelons to market. (This is trivial, if no more than one watermelon weighs more than 2\u00a0kg then any 10 can be placed in each of the crates, if no more than ten watermelons weighs more than 2\u00a0kg then one of each of them can be placed in each crate, etc., to a fast solution; observation will be the key to any rapid solution such as this or the number set problem).","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"they only shared some of these thoughts with Beck.  For example, a person might have thought to themselves, \u201cThe therapist is being very quiet today; I wonder if he\u2019s mad at me?\u201d and then began to feel anxious as a result.\n\nIn the 1960s, researchers did a number of scientific experiments to study how thoughts affect behaviours and emotions.  This period in the history of psychotherapy is called \u201cthe cognitive revolution,\u201d and is also known as the \u201csecond wave\u201d of CBT.\n\nHow it works \nCBT targets different kinds of maladaptive thinking. The goal is to recognise unhealthy thoughts and develop them into positive thinking patterns. In that sense, CBT is scientific because irrational beliefs are thought of as theories which are tested to see if they are true. CBT is structured in that it uses an ABC format. A represents the activating event which triggers B, your beliefs. This is followed by C, the consequences, which are your actions. Beliefs are composed","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Thought is the operation of the brain in conscious activity. It may or may not be goal-directed, aimed at solving specific problems. It is not the only way brains operate. Behaviour may occur as a result of instinct, and the adaptive unconscious may solve problems without a person being aware.\n\nOther animals can use their brains to solve problems, but there is no way of telling whether they do so consciously. Thought is investigated by four or five academic disciplines, each in its own way. The disciplines include psychology, philosophy, biology, physiology, psychoanalysis and sociology.\n\nPhilosophy \nPhilosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, functions, properties, and consciousness. The mind-body problem, i.e. the relationship of the mind to the body, especially the brain, is a central issue in philosophy of mind.\n\nThe mind-body problem \nThe mind-body problem has to do with the explanation of the relationship that exists between minds, or mental","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Thompson: Digital Signal Processing - Concepts and Applications, Palgrave Macmillan, \nSteven W. Smith: Digital Signal Processing - A Practical Guide for Engineers and Scientists, Newnes, \nPaul A. Lynn, Wolfgang Fuerst: Introductory Digital Signal Processing with Computer Applications, John Wiley & Sons, \nJames D. Broesch: Digital Signal Processing Demystified, Newnes, \nJohn G. Proakis, Dimitris Manolakis: Digital Signal Processing - Principles, Algorithms and Applications, Pearson, \nHari Krishna Garg: Digital Signal Processing Algorithms, CRC Press, \nP. Gaydecki: Foundations Of Digital Signal Processing: Theory, Algorithms And Hardware Design, Institution of Electrical Engineers, \nPaul M. Embree, Damon Danieli: C++ Algorithms for Digital Signal Processing, Prentice Hall, \nAnthony Zaknich: Neural Networks for Intelligent Signal Processing, World Scientific Pub Co Inc, \nVijay Madisetti, Douglas B. Williams:","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"preferable to propagate this error into a preferred direction, or equally into several orthogonal dimensions, such as vertically vs. horizontally for bidimensional images, or into parallel color channels at the same position and\/or timestamp, and depending on other properties of these orthogonal discrete dimensions (according to a perception model). In those cases, several roundoff error accumulators may be used (at least one for each discrete dimension), or a (n-1)-dimension vector (or matrix) of accumulators.\n\nIn some of these cases, the discrete dimensions of the data to sample and round may be treated non orthogonally: for example, when working with colored images, the trichromatic color planes data in each physical dimension (height, width and optionally time) could be remapped using a perceptive color model, so that the roundoff error accumulators will be designed to preserve lightness with a higher probability than hue or saturation, instead of propagating errors into each orthogonal color plane independently; and in","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"fishermen, as well as spin and bait casting fishermen, to increase\u00a0conservation\u00a0and to protect rare fish such as marlin. The practice is however disputed as it by some is considered unethical to perform painful actions to the fish for fun and not for the reason of food production. Because of this, catch-and-release practice is illegal in\u00a0Norway.\n\nCollection of live fish\nFish can also be collected in ways that do not injure them (such as in a\u00a0seine net), for observation and study or for keeping in\u00a0Aquarium. There is a substantial industry devoted to the collection, transport, export and farming of wild and domesticated live fish, usually freshwater or marine tropical fish.\n\nFishing with traps\nFish can also be collected in ways that do not injure them (such as in a\u00a0seine net), for observation and study or for keeping in\u00a0Aquarium. There is a substantial industry devoted to the collection, transport, export and farming of wild and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"most of which are either deposit or\u00a0filter feeders. In this way, the toxins are\u00a0concentrated upward\u00a0within ocean\u00a0food chains.\n\nWhen pesticides are incorporated into the\u00a0marine ecosystem, they quickly become absorbed into marine\u00a0food webs. Once in the food webs, these pesticides can cause mutations, as well as diseases, which can be harmful to humans as well as the entire food web.\n\nToxic metals\u00a0can also be introduced into marine food webs. These can cause a change to tissue matter, biochemistry, behaviour, reproduction, and suppress growth in marine life. Also, many\u00a0animal feeds\u00a0have a high\u00a0fish meal\u00a0or\u00a0fish hydrolysate\u00a0content. In this way, marine toxins can be transferred to land animals, and appear later in meat and dairy products.  \n\nEvery time we wash a car or use fertilizer on our lawns we are polluting the ocean. People often think that water pollution comes from big factories, but most of the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the late 2010s.\n\nRestoration\nScientists performed tank and ocean experiments on other animals, such as lobsters, to see if they would eat enough purple sea urchins to help the kelp forests grow back, but they only helped a little.\n\nIn places where sea otters have returned, like Juan de Fuca Strait, they ate enough sea urchins that the kelp forests grew back.\n\nSome companies hired red abalone divers to collect purple sea urchins and bring them to controlled sea ranches where they could be fed until they were healthy enough to sell for human use.\n\nReferences\n\nBiomes\nEcology\nHabitats","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"possible to farm seafood in a way that has very little impact to the environment. Such operations limit habitat damage, disease, escapes of farmed fish and the use of wild fish as feed.\n\nReferences \n\nAquaculture","type":"Document"}]}}