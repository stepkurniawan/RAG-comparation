{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":" A\/B testing is a statistical hypothesis test used to compare two different groups, usually in the context of a software or a\n\nProducts, software, and people are tested for quality and usability. Tests are used to see if the expected result is true, or false, or if the software system can do what it should. A t-test is a statistical hypothesis test used to compare the mean (average) of a measurement of two groups. The advantage of A\/B testing is that it can help to compare the performance of two different groups or two different products, and make an\n\nThe test taker'","1":" ANOVA is a statistical method used to compare the mean (average) of two or (us times) groups. It is powerful when the groups have equal or unknown variance. ANOVA is used to determine if there is a statistical significance between the groups. It is also used to determine if there is a statistical significance between the groups and a theoretical value. ANOVA is also used to determine if there is a statistical significance between the groups and a single group. ANOVA is also used to determine if there is a statistical significance between the groups and a single group and a theoretical value. ANOVA is also used","2":" F frequentist probability is the probability of an observation, in the context of a probability-dynamically balanced position, either player is as likely as the other to emerge from complications with an advantage. F frequentist probability is the probability of an observ-ation, in the context of a probability-dynamically balanced position, either player is as likely as the other to emerge from complications with an advantage. F frequentist probability is the probability of an observ-ation, in the context of a probability-dynamically balanced position, either player is as likely as the other to emerge from complications with an advantage.","3":" In the contexts of agency, acknowledging serendipity and Murphy's law can be challenging because they go against the idea of agency as the idea that individuals have the will and the action to make the thing happen. The principle of double effect, for philosophers, is a principle that justifies taking an action that may have a bad effect, as a good effect may be the cause of the bad effect. The good thing comes from the bad action.\n\nCriticism of the principle of double effect:\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending","4":" The recommended course of action for datasets with only categorical data is to use classification data mining. This is because classification is the only data mining algorithm that can be used to make a prediction based on categorical data. Some of the classification data mining sub-disiples are: Library classification, Optimal classification, Biological classification, Scientific classification, Classification (literature) and Security classification.","5":"\nA Generalised Linear Model (GLM) is a statistical model that can be used to model the relationship between a dependent variable and one or more explanatory variables using a straight line, but also has the additional advantage of the dependent variable's values not following a linear relationship with the explanatory variables. This is because a Generalised Linear Model uses a linear predictor function, but the relationship between the predictor function and the dependent variable is model-ized using a non- linear function called a \"GLM function\" or \"Learning function\" which can be a non- linear function such as a \"L","6":" Cluster analysis is a type of data analysis in data mining that groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. The Dunn Index (DI) is a metric for judging a clustering algorithm, and a higher DI implies better clustering, i.e. clusters are compact and well-separated from other clusters. The size of a cluster and distance between clusters can be define in many ways. The DI is equal to the minimum inter-cluster distance divided by the maximum cluster size.","7":" Network Analysis is a field of mathematics and computer technology used to find answers to a number of problems, such as the best way for a mailman to get to all of the houses in the least amount of time, or the best way for a salesman to visit different customers with the least amount of distance traveled. It is used to find the best way to connect different nodes ( computers, devices, or other points) in a network, and is often used in graph theory. Network Analysis is used to find the best way to connect different networks, and to find the best way to connect different devices in a network. It is also","8":" ANCOVA, or ANAlys is of C Ontants andVAriti ons, is a popularly-sutive for the alys of the means of more-thn 2-n or more-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-","9":" ANCOVA, or ANalses of CCOVA (or ANalses of C ontingual and C ont controlVA) is a general-to-complexity, or a within- and-acorind-to-acorind, or a within- and-acorind-to-acorind-to-acorind, or a within- and-acorind-to-acorind-to-acorind-to-acorind, or a within- and-acorind-to-acorind-to-acorind-to","10":" ANCOVA, or ANA-COVA (or ANCOVA for the ANA-VA, or ANCOVA for the AN-VA, or ANCOVA for the AN-VA-COVA, or ANCOVA for the AN-VA-COVA, or ANCOVA for the AN-VA-COVA-COVA, or ANCOVA for the AN-VA-COVA-COVA-COVA, or ANCOVA for the AN-VA-COVA-COVA-COVA-COVA, or ANCOVA for the AN-VA-COVA-","11":" Content analysis is a technique for the \"analytical and, at the\n\nThe strengths of Content Analysis are its:\n\n- Useful for understanding the context and the genre of a discourse.\n- Useful for understanding the communication of a discourse.\n- Useful for understanding the communication of a discourse in a given context.\n- Useful for understanding the communication of a discourse in a given genre.\n- Useful for understanding the communication of a discourse in a given context and genre.\n- Useful for understanding the communication of a discourse in a given context, genre, and","12":" The Pearson correlation coefficient is the best known correlation coefficient. The formula for this is:\n\nr \n\nr is the correlation coefficient,\n\nsum of ( ( x- x mean) \u00d7 ( y- y mean)\n\nsum of ( ( x- x mean)2) \u00d7 sum of ( ( y- y mean)2)\n\nr is a value between 1 and 1, and it is positive or negative, and it is a value that is used to describe the strength of the correlation between two datasets.\n\nThere are other correlation coefficients, such as the coefficient of simple correlation,","13":" The purpose of a correlogram is not a known or used industry or data storage or light- related or holography- related or interferometer- related or correlation- related or Atacama Large Millimeter Array- related or related to the Michelson interferometer or related to the Pearson product-moment correlation coefficients or related to interference properties of light waves or related to correlation in statistics and probability theory or related to correlation in the context of a scatter graph or related to correlation in the context of strong or weak correlation.\n\nThere is a correlogram in the context of statistics and probability theory, and it","14":"\nTelemetry is a technology that allows measurements to be taken from a distance and transmitted to a receiver for processing. The receiver can be a computer, a telephone, or any system that can transmit and receiving information. The measurements can be taken from a machine, a particle, or any system that can be represented by a signal. The signal is transmitted over a communications channel, such as a telephone wire or wireless internet. The receiver can give commands to the machine or system, and the machine or system can report back with the measurements it takes.\n\nIn the context of the information in the context, telemetry is a technology","15":" The central limit theorem says that if an event is the sum of identical but random events, it will be normally distributed. If the events are not identical, the distribution will not be normal.\n\nQuestion: What is the standard deviation of a normal distribution?\nHelpful Answer: The standard deviation of a normal distribution is a scale or spread of the distribution. It is a number that tells how widely the numbers in the distribution are spread out.\n\nQuestion: What is the normal distribution?\nHelpful Answer: The normal distribution is a probability distribution that is very important in many fields of science. It is a continuous","16":" The Shapiro-Wilk test is a statistical test used to check the hypothesis that a given sample is from a continuous, null distribution. It is a test for distribution, and it is used to test the null hypothesis that the data is from a given distribution, such as the standard distribution. The test results are regarded as'significant' if there is only one chance in 50 that the result could be got by chance. It is a test for homogeneity, as it tests that all samples taken have the same probability distribution, or are from the same set of values. The Shapiro-W","17":" In the context of the question, the Delphi method is not chosen over traditional forecasting methods in the pieces of context. There is no just-the-ans-to-the- question- context that shows the Delphi method is used and traditional forecasting methods are not.\n\nQuestion: Why did Selena's fans in Dallas, Texas, buy Dreaming of You in hours before stores were open?\nHelpful Answer: Selena fans in Dallas, Texas, began lining up to buy Dreaming of You hours before stores were open because of the album's record-selling debut and the first album by","18":" The main goal of Sustainability Science is to develop interdisciplinary foundations for all science, and to develop sustainable systems in nature, in society and in science itself. The challenges it faces include the interdisciplinary nature of the field, the interdisciplinary nature of the systems it studies, and the interdisciplinary nature of the society it is a Part of.\n\nNotes and References\n\nSustainability Science is a field of science that studies the principles of sustainability in nature, in society and in science itself.\n\nThe main goal of Sustainability Science is to develop inter","19":"\n\nUnderstanding ethics and critical theory is important in modern science for a least a \nt least a \nth \n\n- Ethics is important in science to help scientists make moral and fair choices in the way they \n\n- Ethics can help scientists decide what is good or bad, and how to act on that, in the context of scientific \n\n- Ethics can help scientists to test the fairness of a situation, such as how people should treat each other, and \n\n- Ethics can help scientists to examine the hypotheses they believe to be true and replace them with better","20":" System thinking is a framework for organizing and inter- connect- \n\nRelated:\n\nSystems theory is a framework for organizing and inter- connect- \n\nSystems thinking is a framework for organizing and inter- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is a systems- \n\nSystem thinking is","21":" The main principle of the Feynman Method is that if you can't make it simple, you don't know it very well. Feynman diagrams are a simple and diagram-tic (sic) means of representa-tig (sic) the probability amplitudes for elementary particles to collide and bounce off one another. The lines in a Feynman diagram represent the probability amplitude for a particle to go from one place to another, and the meeting points for the lines represent the particles that happen to be at the same point in space at the same time. The meeting points can also be","22":" In ANOVA designs, fixed factors are variables that are manipulate by the experimenter, and random factors are variables that are not manipulate by the experimenter, and are usually not of much control to the experimenter. For example, if you are interested in how the color of a plant affects growth, color is a fixed effect, as you can choose the color of the plant. If you are interested in how the growth of a plant is effected by the temperature of the air, temperature is a random effect, as you can't control the temperature, it is something that is already there and is fixed in the experiment","23":"\n\nThe replication crisis refers to a crisis in science where the results of scientific experiments are difficult or impossible to replicate, either by independent researchers or by the original researchers themselves. This is a problem as the reproducibility of experiments is an essential part of the scientific method.\n\nThe replication crisis has been particularly widely discussed in the field of psychology and medicine, where efforts have been made to re-investigate classic results and determine both the validity of the results and, if invalid, the reasons for the failure of replication.\n\nThe inability to replicate studies has potentially grave consequences as it","24":" The flashlight method is not a group process or a method used in group discussions. It is a portable light used to see in the\n\nThe answer is: The flashlight method is not a group process or a method used in group discussions. It is a portable light used to see in the\n\nI's a portable light used to see in the\n\nIts purpose is to provide a beam of light which helps to see in the\n\nIts process is usually the unscrewing of the flashlight head to switch it on, or the push of a side switch on larger flashlights.\n\nI","25":" Generalized Linear Models (or \"linear models with a linked function\" or \"linear models with a linear predictor and a linked function\" or \"linear models with a linear predictor and a function of the data in the model term\" or \"linear models with a linear predictor and a function of the data in the model term and a linked function\" or \"linear models with a linear predictor and a function of the data in the model term and a function of the data in the model term and a linked function\" or \"linear models with a linear predictor and a function of the data in the model term and","26":" A heatmap is a visual tool used to show data where different colors or temperatures are used to show patterns or differences. In the context of the question, a heatmap is a map that uses colors to show temperature data. It is useful for quickly understanding temperature patterns and differences in a given geography. In meteorology, heat maps are used to show temperature data from weather maps and can help meteorologists, pilots, and mariners to see important weather patterns.\n\nIn the context of the question, a heatmap is a map that uses colors to show temperature data. It is useful for quickly understanding temperature","27":" Alhazen was a pioneer of modern optics and is sometimes described as a pioneer of the modern scientific method. He laid the foundations for telescopic astronomy and made contributions to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, and Muslim theology. He is known for his work on the motion of a body, and maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. Alhazen lived mainly in Cairo, Egypt, and made contributions to the principles of optics. He is sometimes","28":" One way to graphically display multivariate data is by using a \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more \n\nA multivariate data is data that is described by more ","29":" Machine learning has the advantage of being more data- and context-sensitive, as the algorithms can learn from the data and make predictions or decisions based on that data. This is in difference to traditional rules or functions, which are explicitly programmed and cannot learn or make decisions based on data. Machine learning is often more complicated and can be more difficult to work with, but the ability to learn from data can make the algorithms more general and more context-sensitive, which can be an advantage in many computer science and mathematics contexts.\n\nIn the context of optimization, machine learning algorithms can learn the optimization function from the data","30":" Machine learning techniques can be black box, which can make it hard to understand how the model is working and what attributes are being connected or how. Some machine learning techniques, like Neural network, can be hard to understand and can be criticized for biases in hiring, criminal justice, and recognizing faces.\n\nIn machine learning, supervised learning is a task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Machine learning techniques can be black box, which can make it hard to understand how the model is","31":" The scientific method is a systematic and empirical way of investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. It is based on observable, empirical, and measurable evidence, and the formulation and testing of hypotheses. Scientific methods are repeatable, and theories that encompass wider domains of inquiry may help form new hypotheses or place groups of hypotheses into context. The scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses. Scientists identify a question or a problem about nature, investigate the problem, propose","32":" The main goal of practicing mindfulness is to calm and focus the mind, and to be able to respond to the stress and pain of life in a clearer and healthful (or healthful-er) \n\n(or healthful) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-er) \n\n(or healthful-","33":"\n\nA Mindmap is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the Mindmap, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.\n\nThe Wikiatlas is a type of Mindmap that is arranged in a distinctive format, with maps displayed down the left-side of the page, and explanatory text presented next to the maps. This","34":" Weinberg developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models, and he invented linear regression analysis as a starting point for this work.\n\nA more comprehensive answer: Weinberg was a pioneered statistical analysis in genetics, and his work on the analysis of real data was a springboard for the development of new statistical methods. He began to pay particular attention to the labour involved in computations and developed practical methods. In 1925, he published the first book on statistical methods for research workers, which went into many editions and translations in later years. In","35":" Mixed Effect Models (including random and random- Effect Models) are a more complex statistical tool than Analysis of Variance (including one- and two- ways Analysis of Variance) and Regression. This is both in terms of the calculations required and the assumptions made.\n\nThe statistical power of a model is a measure of the probability of being to make a \"w\" (a \"w\" is a statistical term for a \"w\"-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-","36":"\n\nIn model reduction, stepwise procedures can be avoided to make the process more computational and algebraic in the following two possible justi-\n\n1. Computational complexity:\n\nIn model reduction, the computational complexity of the stepwise procedures can be a problem, as it can make the process more in-\n\n2. Laxity of error:\n\nIn model reduction, the stepwise procedures can make the error analysis more problem-\n\n1. Computational complexity:\n\nIn model reduction, the computational complexity of the stepwise procedures can be a problem, as it can make the","37":" In data model reduction, redundancies can be identify using methods such as:\n1. P-redundancy: If a set of data can be recreated from another set of data, it is called P-redundant. This can be used to identify and reduce redundancies in data.\n\n1. P-redundancy can be used to identify redundant data in a data set. It can be defined as a set of data that can be recreated from another set of data.\n\n1. P-redundancy can be used to identify redundant data in a data set","38":" Narratives are used in Narrative Research as the data that is to be study. Narratology is the study of structure in narratives. Narratives are the events that make up a story or book, and the way they connect to make up the plot. Narrative research is a method of research that involves the collection and analysis of narratives. This can be through qualitative interviews, case study or text analysis. The study of narratives can be used to understand the behavior of research subjects, and to collect relational data.\n\nReferences\n\nNarrative research\nNarratology","39":" Generalized Additive Models (GAM) are a class of regression models that allow for the linear predictor function to incorporate terms that are modeled as functions of other variables, in a way that can model term interdependencies. This is in between linear models, where the predictor function is a linear combination of coefficients and explanatory variables, and models with immanent term interdependencies, where the interdependencies between terms are derived from the data.\n\nGAM' advantages are that they can model term interdependencies, and they can also model the linear and the term-interdependency terms in","40":" Poisson Distribution can be used to:\n1. \n2. \n3. \n\nHelpful Answer: Poisson Distribution can be used to:\n1. \n2. \n3. \n\n1. to \n2. to \n3. to \n\n1. to \n2. to \n3. to \n\n1. to \n2. to \n3. to \n\n1. to \n2. to \n3. to \n\n1. to \n2. to \n3. to ","41":" The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work and\n\nHelpful Answer: The Pomodoro technique is a work","42":" The 'curse of dimensionality' is a term used in mathematics and physics to define the way that the number of dimensions in a space can make it more and more different from the common 3 dimensions that we's used to. It can make it more and more different to measure, to see, and to experience. In a vector space, the dimension of a space is equal to the cardinality of a basis of that space, or the number of unique directions that the space actually has. In a 4D space, there are four unique directions, and it can be more and more different from the common ","43":"\n\nThe dispersion, or variability, of a dataset is an important statistical property that measures how the data is distribution. There are different kinds of dispersion: homoscedastic and heteroscedastic.\n\nA dataset is said to have homoscedastic dispersion if the variability is the similar at all the different data-taked or measurement-taked. This is also known as homoscedastic error or homoscedastic variance.\n\nA dataset is said to have heteroscedastic dispersion if the variability is different at different data-taked or measurement-taked.","44":" Shell, a large and successful corporation, was a key business-sinking of Herman Kahn's ideas on scenario thinking. In the 1960s and 1970s, Shell's Planning and business strategy was based on Kahn's ideas of scenario thinking. This helped to make scenario thinking a key business strategy, and Shell was a foreraner in the use of scenario thinking to develop business strategy.\n\nKahn's ideas on scenario thinking were based on the need to think through the consequences of various scenarios, and to develop responses that were proportionate to the threat.","45":" The field of Social Network Analysis was influenced by sociologists and psychologists during the years immediately following World War II. The greatest period of collaboration between sociologists and psychologists in this field was during this period. There is no documented influence on Social Network Analysis in the 1930s by a sociologists or psychologists that is based on the study of social networks as a pattern or structure of relationships. The Law of 150, which suggests that about 150 people is the best size for a village or community, was documented in 1992 by R.I.M. Dunbar.","46":" Stacked Area Plots are a types of area or area-density-magnitude-magnitude-rank-magnitude-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank-rank","47":" Thought experiments are mental experiments used to test the thought of a theory or idea, and to show what might happen if a theory or idea were true. They are a way to understand the world around us, and have been used in science and philosophy for many years. They cannot tell us if a theory is true, but they can help us understand the world and test the thought of a theory.\n\nThe purpose of thought experiments is to test the thought of a theory or idea, and to show what might happen if that theory or idea were true. They are a way to understand the world around us, and have been used in science","48":" In the context of time-frequency representation, autocorrelation is a method to describe the statistical autocorrelation of a time-frequency signal. It is a method to describe the statistical autocorrelation of a time-frequency signal, which is a representation of a signal in the time-frequency-frequency (or time-frequency-frequency-frequency) representation. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a random variable. In the context of time-frequency representation, autocorrelation is a method to describe the statistical autocor","49":"\n\nThe Besatzfisch project did not employ a model of nature that is based on economic valuation of ecosystem services. Its focus was on the ecological effects of stocking fish in natural ecosystems.\n\nTo study these effects, the project used a research methods that includes:\n\n1. Scientists stocking fish in natural ecosystems and following their population size, health, and interactions.\n\n   a. Scientists used different stocking methods and fish size to study the impact of stocking on the ecosystem.\n\n   b. Scientists also used different fish, like rainbow and Arctic fish"},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and other vehicles include a crash test. The car is put under severe conditions to see what will make it fail, or deliberately crashed to measure the damage. Other machines can also be crash tested. Crash test dummies can be used instead of humans. They are placed in the car seat to see if a human in the crash would have been injured or killed.\n\nPeople \n\nPeople are tested to see what they have learned. This is often called an assessment or examination. In learning, a test item is a question, or set of questions.\n\nMany people think tests are valuable. They believe tests:\n are a quick and fair way of judging a test taker's performance\n enable predictions about test takers to be made\n allow selection\n improve performance by highlighting areas that need work.\n\nHowever, academic tests are not perfect measures. Tests could only partly measure a student\u2019s memory and maybe their understanding. The test","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the others prepared by first adding the tea.  She was to select the  four cups prepared by one method.\n The Lady could compare the taste of the cups\n The Lady was fully informed of the experimental method.\n The null hypothesis was that the Lady had no such ability.\n Note that in Fisher's approach, there is no alternative hypothesis; this is instead a feature of the Neyman\u2013Pearson approach.\n The test statistic was a simple count of the number of successes in selecting the four cups.\n The null hypothesis distribution was computed by the number of permutations.  The number of selected permutations and the number of unselected permutations were equal.\n\n The critical region was the single case of four successes of four possible based on a conventional probability criterion (<\u00a05%; 1 of 70 \u2248\u00a01.4%).\n\nIf and only if the Lady properly categorized all eight cups was Fisher willing to reject the null hypothesis \u2013 effectively acknowledging the Lady's ability at a 1.4% significance level (but","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"sample minus one:\nDf\u200b=N\u22121\nwhere:\n\nDf\u200b=degrees of freedom\n\nN=sample size\u200b\n\nDegrees of freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a chi-square. It is essential to calculate degrees of freedom when trying to understand the importance of a chi-square statistic and the validity of the null hypothesis.\n\nChi-Square Tests \nThere are two different kinds of chi-square tests: the test of independence, which asks a question of relationship, such as, \"Is there a relationship between gender and SAT scores?\"; and the goodness-of-fit test, which asks something like \"If a coin is tossed 100 times, will it come up heads 50 times and tails 50 times?\"\n\nFor these tests, degrees of freedom are utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"38.3 || 76.1|| 4.9 || 1.8|| 0.6|| 0.2 || 11.7\n|-\n|\n| 65|| 1 || 26.6|| 47.8|| 35.1|| 61.4 || 4.0|| 3.4|| 1.1|| 0.3|| 7.0\n|-\n|\n| 70 || 66 || 20.7 || 62.7 || 100.0 || 48.0 || 7.0 || 2.3 || 0.5 || 1.6 || 5.4\n|-\n|\n| 46|| 13|| 16.7|| 54.8|| 0.0 || 53.0 || 5.6|| 0.7|| 0.4|| 1.1 ||","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to a few openings, rather than being an opening prescription for Black in general\".p227 Evans wrote that after one of his games against Fischer, \"Fischer confided his 'secret' to me: unlike other masters, he sought to win with the black pieces from the start. The revelation that Black has dynamic chances and need not be satisfied with mere equality was the turning point in his career, he said\".p91 Watson surmised that Kasparov, when playing black, bypasses the question of whether White has an opening advantage \"by thinking in terms of the concrete nature of the dynamic imbalance on the board, and seeking to seize the initiative whenever possible\".p231 Watson observes that \"energetic opening play by Black may ... lead to a position so complex and unclear that to speak of equality is meaningless. Sometimes we say 'dynamically balanced' instead of 'equal' to express the view that either player is as likely as the other to emerge from complications with an advantage.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"4.7 || 1.3|| 0.7|| 0.2 || 9.0\n|-\n|\n| 24|| 3 || 32.0|| 47.6|| 38.5|| 56.1 || 4.4|| 3.8|| 1.2|| 0.4|| 8.9\n|-\n|\n| 22 || 22 || 16.6 || 62.3 || 0.0 || 35.7 || 5.7 || 1.4 || 0.6 || 1.6 || 4.6\n|-\n|\n| 23|| 1|| 8.8|| 53.6|| 0.0 || 43.2 || 2.7|| 0.3|| 0.0|| 0.3 || 4.0\n|-\n|\n| 24|| 7 ||","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Murphy's law is an idea, and an adage or epigram, that says: Anything that can go wrong will go wrong.\n\nPhrases","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is most likely to occur when one is wholeheartedly performing a task or activity for intrinsic purposes. Intrinsic purposes involve anything that someone does merely because they want to. Extrinsic activities will not cause flow to occur. Extrinsic activities are anything that someone does because there is some other force causing them to do it. Extrinsic activities will not cause flow to occur. Passive activities like taking a bath or even watching TV usually do not elicit flow experiences as individuals have to actively do something to enter a flow state. While the activities that induce flow may vary and be multifaceted, Csikszentmih\u00e1lyi asserts that the experience of flow is similar despite the activity.\n\nComponents of flow\nCs\u00edkszentmih\u00e1lyi identifies the following ten factors as accompanying an experience of flow:<ref name=Finding>Cs\u00edkszentmih\u00e1lyi, Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected\/influence each other. The","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Classification could mean:\n\n Library classification and classification in general\n Optimal classification\n Biological classification\n Scientific classification (disambiguation)\n Classification (literature)\n Statistical classification\n Security classification\n Classification theorems in mathematics.\n Film classification\n CLASSIPHI, a seabed mapping tool supplied by QinetiQ\n Civil service classification, personnel grades in government\n Attribute-value system\n\nRelated pages\n Class\n Categorization","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and\/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster\/tree\/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and\/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster\/tree\/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A computer network is a group of two or more computers that are linked together. Networks are usually used to share resources, exchange files or communicate with other users.\n\nA network is a set of nodes connected by communication links.  A node can be a computer, printer, or any other device capable of sending or receiving data from or to the other node in the network.\n\nOther devices are often needed for the network to work correctly.  Examples for such devices include hubs and switches.  Different kinds of network can be connected to each other with a router.  In general, networks that use cables to connect can operate at higher speeds than those using wireless technology.\n\nComputers in a network can be near each other, or far. A Local Area Network (LAN) connects computers which are close together.  Building a LAN is easier than connecting different networks (by a Wide Area Network).  The largest Wide Area Network is the Internet.\n\nComputers can be part of several different networks. Networks can also","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Graph theory is a field of mathematics about graphs. A graph is an abstract representation of: a number of points that are connected by lines. Each point is usually called a vertex (more than one are called vertices), and the lines are called edges. Graphs are a tool for modelling relationships. They are used to find answers to a number of problems.\n\nSome of these questions are:\n What is the best way for a mailman to get to all of the houses in the area in the least amount of time? The points could represent street corners and lines could represent the houses along the street. (see Chinese postman problem)\n A salesman has to visit different customers, but wants to keep the distance traveled as small as possible. The problem is to find a way so they can do it. This problem is known as Travelling Salesman Problem (and often abbreviated TSP). It is among the hardest problems to solve. If a commonly believed conjecture is true (described as P \u2260","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A Guide to Network Topology\n Research network topology \n Types of topology \n Logical Topology Example\n 8 Common Network Topologies & How to Use\n\nTendaishe Sigauke, (2007: 46) Explaining networking terms\n\nComputer networking","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"20, it began a slow weakening trend after passing over cooler waters. Steering winds ahead of an approaching mid-level trough caused Beryl to speed up northeastward, and early on July 21 the center of the storm crossed the island of Nantucket. The convection diminished as it moved through progressively colder waters, and shortly after 1200\u00a0UTC on July 21 Beryl became an extratropical cyclone a short distance east of Cape Cod. Hours later, it made landfall in southwestern Nova Scotia, and on July 22 the extratropical remnants of Beryl merged with an approaching cold front over Newfoundland.\n\nPreparations\nForecasters originally predicted Beryl to brush the Carolinas; as such, a tropical storm watch was issued for the eastern coast of North Carolina from Cape Lookout northward to Currituck Beach Lighthouse. When a more northeastward track became apparent, a tropical storm watch was issued about 33\u00a0hours before moving ashore from Woods Hole","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"century author Yagut al-Hamawi writes: \u201cNashawa is a city in Azerbaijan, or, as some say, located in Arran. It is popularly known as among the people Nakhchivan or Nakhch\u0131van  The Arabic word \"Nashava\" means \"the place where the sun rises\", \"the place where the light shines\". We also know from historical and mythological sources that the belief of the people living in this land in the sun, fire and light dates back to ancient times. Today, this belief manifests itself on March 21, that is, \"Nuhruz\", the day when Noah\u2019s followers came ashore (dated as Novruz).  Because these lands are ancient Turkic lands, and the people living here have preserved their past beliefs, traditions and national affiliations to this day.\nThe city was named Naqshi-Jahan in the 14th century Azerbaijani historian Muhammad ibn Hindushah","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"that the system was becoming more well organized. Later that day, the PAGASA started monitoring the system as a tropical depression and named it Mina. Late on August 22, the system became more well organized prompting the JTWC to initiate advisories on the system, designating it with 14W. On August 23, the JMA upgraded 14W to a tropical storm, naming it Nanmadol. Overnight, the system continued to intensify and early on August 24, the JMA upgraded Nanmadol to a severe tropical storm. Later that day, convective banding improved and Nanmadol developed an eye-like feature. As a result, Nanmadol continued to intensify rapidly and became a typhoon, by midnight, that day. Nanmadol continued to drift northeast and made landfall over Gonzaga, Cagayan, Philippines with strong winds of over . Nanmadol weakened significantly after interacting with land and early on August 28, the","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to abandon c the mechanistic views of Descartes.  Critiquing Descartes' reductionistic view that everything can be studied in parts to understand the whole, he allows his readers to take an objective and fresh mind, encouraging them to see the world through 'complexity theory'.\n\nCapra is aiming for a theory of living systems, a theoretical framework for ecology. This theory has its roots in several scientific fields from the first half of the twentieth century \u2014 organismic biology, gestalt psychology, ecology, general systems theory, and cybernetics.\n\nBibliography \nThe Tao of Physics (1975)\nGreen Politics with (Charlene Spretnak)\nThe Turning Point (1982)\nUncommon Wisdom (1988)\nBelonging to the Universe: Explorations on the Frontiers of Science and Spirituality (1991) (coauthored with David Steindl-Rast and Thomas Matus)\nThe Web of Life (1997)\nThe Hidden","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"1932. Walter Cannon thought these were the features of homeostasis:\nConstancy in an open system, such as our bodies, is done by mechanisms which maintain this constancy. Cannon based this on insights into the ways by which steady states such as glucose concentrations, body temperature and acid-base balance were regulated.\nTo keep a steady-state condition, any change automatically meets with factors that resist change. An increase in blood salt results in thirst as the body attempts to dilute the concentration of salt in the extracellular fluid.\nThe regulating system doing homeostasis has a number of cooperating mechanisms which act simultaneously or successively. Blood sugar is regulated by insulin, glucagons, and other hormones that control its release from the liver or its uptake by the tissues.\n\nExamples of homeostasis:\n The operation of a thermostat\n The regulation of water and minerals in the body \n The regulation of body temperature: mammals and birds have complicated systems which keep their body temperature within close limits.\n\nIn mammals,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"then tried to cure the people using psychiatric medicines. All eight were allowed to leave within 7 to 52 days, but only when they had said that they accepted the illness they had been diagnosed with. Later in the study, a research and teaching hospital challenged Rosenhan to run a similar experiment with its own diagnoses and rules for letting people in to the hospital. Psychiatric staff were warned that at least one pseudo-patient might be sent to their hospital. At least one staff member thought that 83 out of the 193 new patients were actors. In fact, Rosenhan sent no actors. The study showed that the ways diagnosis was done was very inaccurate in telling people with mental disorders and people without mental disorders apart. The paper made a lot of controversy. Some people are not sure of how valid and credible the study was, but say that psychiatric diagnoses need to be improved.\n\nProfessor Rosenhan worked with the Stanford University Department of Psychology. Later, he became professor emeritus at Stanford","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or 'incorrect inferences about external reality'. Some religious or spiritual beliefs by their nature may not be falsifiable, and hence cannot be described as false or incorrect, no matter whether the person holding these beliefs was diagnosed as delusional or not.\n\nA doctor or psychiatrist may say that a delusion is false, because it seems to be unlikely or bizarre.Psychiatrists rarely have the time or resources to check the validity of a person\u2019s claims; this means that some true beliefs are erroneously classified as delusional. This is known as the Martha Mitchell effect, after the wife of the attorney general who alleged that illegal activity was taking place in the White House. At the time her claims were thought to be signs of mental illness. Only after the Watergate scandal  was she proved right, and sane.\n\nTreating delusions is difficult, but in some cases antypsychotics can be used. If the delusions pose a risk to the health of the patient (if","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to its members\n that I will lead my life and practice my art with virtue and honor\n that into whatsoever home I shall enter it shall be for the good of the sick and the well by the utmost of my power and that I will hold myself aloof from wrong and from corruption and from the tempting of others to vice\n that I will exercise my art solely for the benefit of my patients, the relief of suffering, the prevention of disease and promotion of health, and I will give no drug and perform no act for an immoral purpose\n that in the treatment of the sick, I will consider their well-being to be of a greater importance than their ability to compensate my services\n that what I may see or hear in the course of treatment or even outside the treatment in regard to the lives of persons which is not fitting to be spoken, I will keep inviolably secret\n that I will commit myself to a lifetime of continued learning of the art and science of medicine\n these things I do","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"chat?), influences discourse. Discourse analysis is also interested in the genre (topic) of the discourse. \n\nDiscourse analysis is studied not only in linguistics, but also in sociology, anthropology, psychology, communication studies and translation studies.\n\nOther websites \n Daniel L. Everett, statement concerning James Loriot, p.\u00a09 \n The Discourse Attributes Analysis Program and Measures of the Referential Process .\n Linguistic Society of America: Discourse Analysis, by Deborah Tannen \n Strategies for analysing a case study\n\nLinguistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Critical analysis is to examine something that someone has said. It means to study the individual parts of the work.\n\nReferences\n\nThought\nLogic","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"create a virtual picture of the recorded object, even if that picture can only be seen from a specific angle.\n\nApplications\n\nMeasurement \n\nIndustries use holograms to measure things. In the car industry, cars are measured using holography so engineers can see bulges and vibration characteristics. Phase-shift holography is one kind of holography used to make cars.\n\nThe first step in making a hologram is to examine the ground level state of the object, then overload the object through heat or mechanical pressure. Covering the original hologram and the modified hologram can produce interference fringes. By measuring the interference fringes, engineers learn how big the deformation or other problem is. Engineers can measure tiny terminal expansions or vibrations in mechanical systems. This needs two reference waves.\n\nData storage \n\nThere are holographic storage machines for analog pictures and digital data. Digital information will be affiliated by a two-dimensional bit-pattern.\n\nLight","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Interferometers are for measuring the interference properties of light waves. One of their purposes is to measure things precisely, for example in the Michelson interferometer.\n\nRelated pages \n Atacama Large Millimeter Array\n\nOptical devices","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In the fields of communications, signal processing, and in electrical engineering more generally, a signal is any time-varying quantity. \n\nThe concept is broad, and hard to define precisely.  Definitions specific to subfields are common. For example, in information theory, a signal is a codified message, i.e., the sequence of states in a communications channel that encodes a message. In a communications system, a transmitter encodes a message into a signal, which is carried to a receiver by the communications channel.  For example, the words \"Mary had a little lamb\" might be the message spoken into a telephone. The telephone transmitter converts the sounds into an electrical voltage signal. The signal is transmitted to the receiving telephone by wires; and at the receiver it is reconverted into sounds.\n\nExamples of signals \n Motion.  The motion of a particle through some space can be considered to be a signal, or can be represented by a signal.  The domain of a motion signal is","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library \u2013 Reference Manual \u2013 The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"higher average is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return).\n\nRules for normally distributed numbers\n\nMost math equations for standard deviation assume that the numbers are normally distributed. This means that the numbers are spread out in a certain way on both sides of the average value. The normal distribution is also called a Gaussian distribution because it was discovered by Carl Friedrich Gauss. It is often called the bell curve because the numbers spread out to make the shape of a bell on a graph. \n\nNumbers are not normally distributed if they are grouped on one side or the other side of the average value. Numbers can be spread out and still be normally distributed. The standard deviation tells how widely the numbers are spread out.\n\nRelationship between the average (mean) and standard deviation \nThe average (mean) and the standard deviation of a set of data are usually written together. Then a person can understand what the average number is and how widely other numbers in the group are spread out.","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Kolmogorov\u2013Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics, a frequency distribution is a list of the values that a variable takes in a sample. It is usually a list, ordered by quantity. It will show the number of times each value appears. For example, if 100 people rate a five-point Likert scale assessing their agreement with a statement on a scale on which 1 denotes strong agreement and 5 strong disagreement, the frequency distribution of their responses might look like:\n\nThis simple table has two drawbacks. When a variable can take continuous values instead of discrete values or when the number of possible values is too large, the table construction is difficult, if it is not impossible. A slightly different scheme based on the range of values is used in such cases. For example, if we consider the heights of the students in a class, the frequency table might look like below.\n\nApplications \nManaging and operating on frequency tabulated data is much simpler than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and was the best-selling debut by a female artist. Selena fans in Dallas, Texas, began lining up to buy the album hours before stores were open. Dreaming of You sold 331,000 copies in its first week. It became the biggest first-week seller by a female artist. It also became the first album by a Hispanic artist to debut at number one. EMI believed that Dreaming of You sold more than 400,000 copies because SoundScan did not track copies sold in small stores.\n\nDreaming of You also debuted at number one on the Billboard Top Latin Albums and Billboard Latin Pop Albums charts. It remained on the charts for nearly two-hundred weeks. In Austin, stores were sold out of Dreaming of You within minutes on the day it was released. Throughout the entire city, Austin stores only allowed two copies per costumer. Fans in Corpus Christi began celebrating the release of Dreaming of You the day before the album was released, at music stores across the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Revue shows and western operas and musicals, which hold auditions for each new production.\n\nIn his history of Takarazuka, Kobayashi used a curiously domestic metaphor in describing his dream: \u201cIt would be impossible to train men in that way for our productions. In cooking too, perhaps only men can produce something truly excellent, but it is women who rustle up something quick for the family. There are currently between 400 and 500 Takarazuka students and we could not have so many if we included men. There would be non-stop fighting and arguments. There is no room for men in Takarazuka, and it has an atmosphere that could not be created without limiting it to women only.\u201d\n\nKobayashi\u2019s Many Pies \nAside from theatrical activities, Kobayashi oversaw a number of creative entrepreneurial projects centered on the railway business that targeted ordinary consumers. He expanded the focus of his business from simply taking passengers from one point to another","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the short program scoring 63.50 points and the free skate with 133.70 points. So she set a world record for the free skate score under the ISU Judging System. She also did a flip jump-toe loop put together, a loop jump, a lutz jump-toe loop jump, an axel jump-toe loop jump, and a triple lutz jump.\n\nShe was allowed to the 2007\u20132008 Grand Prix of Figure Skating Final in Turin, Italy. She won the short program with 64.62 points. With a score of 196.83 points, Kim won her second Grand Prix Final.\n\nKim did not attend the 2007\u20132008 South Korean Nationals and also did not go to the 2008 Four Continents Figure Skating Championships because of a hip injury. Even though she still had back pain, she went to the 2008 World Figure Skating Championships in Gothenburg, Sweden. She was","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Sustainable Development Goals (SDGs) are created by the [United Nations] and promoted as the Global Goals for Sustainable Development. They replaced the [Millennium Development Goals] that expired at the end of 2015. The SDGs run from 2015 to 2030. There are 17 goals and 169 specific targets for those goals.\n\nGoals\n\nIn August of 2015 193 countries agreed to the following 17 goals:\n\n No poverty \n Zero hunger \n Good health and wellbeing\n Quality education \n Gender equality\n Clean water and sanitation\n Affordable and clean energy \n Decent work and economic growth \n Industry, innovation and infrastructure\n Reduce inequality \n Sustainable cities and communities \n Responsible consumption and production\n Climate action \n Life below water\n Life on land\n Peace and justice.  Strong institutions\n Partnerships for the goals\n\nReferences\n\nSustainability\nDevelopment\nUnited Nations","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"ethics should use the scientific method to study \nthings that people think are good or bad. Their work can be used\nto test the fairness of a situation, such as how people should treat\neach other. An example of this kind of thinking is the categorical imperative.\nMany countries have laws based on this idea of fairness.\n\nWhat is ethics used for? \nUnderstanding ethics can help people decide what to do when they have choices. Many philosophers think that doing anything or making any choice is a part of ethics.\n\nEthics is part of other fields of study in many ways. Here are some ways:\n\n Ethics is part of the study of religion. In religion, people often learn what is good or bad from what they believe about God (or gods). Some important ideas about what is good or bad have come from religion. See Ten Commandments.\n Some theories of economics say ethics has to do with money. Money is a big part of most people's lives. Thinking about morality can be important in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the most complex thing about humans is how they relate to each other in society.  The most general social science is ethics, or economics, depending on your point of view.\n\nHowever, life science also deals with humans as objects of study (notably in medicine), while hard science such as chemistry deals with humans as the observer who does the study - sets the scale at which observation can happen, incurs observer effects - as studied in philosophy of science.\n\nOften sciences have different names based on whether they study humans or not.  For instance economics is the study of how humans make a living, while ecology is the study of how non-humans make a living.  Medicine is normally restricted to humans while veterinary medicine refers to the same techniques made on other species.  One relies more on ethics and less on economics when dealing with humans, at least according to any ethical tradition.\n\nThe living, social, and sensory observing aspects of humans are probably all part of the human sciences to the degree they","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems theory is the study of the nature of systems in nature, society, and science.  More specifically, systems theory is a framework to analyze or describe any group of things which work together to produce some result. This can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. \n\nSystems theory as a technical and general academic area of study. It was founded by Ludwig von Bertalanffy and others in the 1950s.\n\nRelated pages\n Autopoiesis\n Chaos theory\n Fritjof Capra","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A system  is a group of related things that work together as a whole. These things can be real or imaginary. Systems can be man-made things like a car engine or natural things like a star system. Systems can also be concepts made by people to organize ideas. \n\nA subsystem is a system that is part of some larger system. For example, in computer networking, a disk subsystem is a part of a computer system.\n\nDefinition  \nA system is a group of things that connect and form some kind of coherent whole.\n\nOne of the founders of physiology, Claude Bernard, took a big step when he noticed that the internal systems of the body tend to keep things the same even if they are disturbed. He called the functions which keep system stable as homeostasis. This led towards the ideas of error-correction, feedback and regulation. These are all typical of systems which have some kind of goal and can adjust their behaviour so as to correct errors.\n\nExample \nThe Solar System is an example","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"point, that means that the particle was either created or destroyed, depending on the direction in time that the particle came in from.\n\nAll the lines and vertices have an amplitude. When you multiply the probability amplitude for the lines, the amplitude for the particles to go from wherever they start to wherever they meet, and to the next meeting point, and so on, and also multiply by the amplitude for each meeting point, you get a number that tells you the total amplitude for the particles to do what the diagram says they do. If you add up all these probability amplitudes over all the possible meeting points, and over all the starting and ending points with an appropriate weight, you get the total probability amplitude for a collision in a particle accelerator, which tells you the total probability of these particles to bounce off one another in any particular direction.\n\nFeynman diagrams are named after Richard Feynman, who won the Nobel Prize in Physics. His diagrams are very simple in the case of quantum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A Feynman diagram is a diagram that shows what happens when elementary particles collide.\n\nFeynman diagrams are used in quantum mechanics. A Feynman diagram has lines in different shapes\u2014straight, dotted, and squiggly\u2014which meet up at points called vertices. The vertices are where the lines begin and end. The points in Feynman diagrams where the lines meet represent two or more particles that happen to be at the same point in space at the same time. The lines in a Feynman diagram represent the probability amplitude for a particle to go from one place to another.\n\nIn Feynman diagrams, the particles are allowed to go both forward and backward in time. When a particle is going backward in time, it is called an antiparticle. The meeting points for the lines can also be interpreted forward or backwards in time, so that if a particle disappears into a meeting point, that means that the particle was either created or destroyed, depending on the direction in time that","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Richard Feynman (11 May 1918 \u2013 15 February 1988) was an American physicist of Jewish descent. He was born in Far Rockaway, Queens, New York City. He was part of the Manhattan Project team that made the atomic bomb. Feynman won the Nobel Prize in Physics 1965. He was one of the first people to study quantum physics. Feynman added significantly to a branch of science called quantum electrodynamics and invented the Feynman diagram. He died of liposarcoma in Los Angeles, California.\n\nHis sister Joan Feynman (born 1927) is a retired astrophysicist.\n\nSelected books \nFeynman wrote some best-selling autobiographies, and his lecture notes became popular with physics students and qualified professionals.\n\nFeynman, Richard P. (1985). Ralph Leighton, ed. Surely You're Joking, Mr. Feynman!: adventures of a curious character.","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"experimental data\".\n\nReferences\n\n \n Basu D. (1980b). \"The Fisher Randomization Test\", reprinted with a new preface in Statistical Information and Likelihood : A Collection of Critical Essays by Dr. D. Basu ; J.K. Ghosh, editor. Springer 1988.\n \n Salsburg D. (2002) The Lady Tasting Tea: how statistics revolutionized science in the Twentieth Century W.H. Freeman \/ Owl Book. \n\nExperiments\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In an experiment, the variables used can be classed as either dependent or independent variables. The dependent variable is the possible outcome of the experiment; the effect. It depends on what happens to other variables in the experiment. The dependent variable is basically the part that you are changing. For example, if you want to know how much light a plant needs to grow, the amount of growth is the dependent. If you wanted to see if a plant would grow better in hot or cold areas your independent variable would be the temperature of the air.\n\nThe independent variable is the variable that you have control over, what you can choose and manipulate. It is usually what you think will affect the dependent variable. In some cases, you may not be able to manipulate the independent variable. It may be something that is already there and is fixed, something you would like to evaluate with respect to how it affects something else, the independent variable like color, kind, time. As an example, you are interested in how","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Informatica, 30:3\u201331, 2006.\n\nFurther reading \n\n Bradley, R.A. and Terry, M.E. (1952). Rank analysis of incomplete block designs, I. the method of paired comparisons. Biometrika, 39, 324\u2013345.\n David, H.A. (1988). The Method of Paired Comparisons. New York: Oxford University Press.\n Luce, R.D. (1959). Individual Choice Behaviours: A Theoretical Analysis. New York: J. Wiley.\n Thurstone, L.L. (1927).  A law of comparative judgement. Psychological Review, 34, 278\u2013286.\n Thurstone, L.L. (1929).  The Measurement of Psychological Value.  In T.V. Smith and W.K. Wright (Eds.), Essays in Philosophy by Seventeen Doctors of Philosophy of the \tUniversity of Chicago.  Chicago: Open Court.\n Thurstone, L.L.","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"quite common in science for some studies to contradict others, for example in cases where different methods are used to measure an outcome, or where human error or chance may lead to unusual results. This means that there is often a study someone can use to support their claim, and they can cherry pick that one study even if many more contradict it.\n\nReferences\n\nLogical fallacies","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A flashlight (in North American English) or torch (in most Commonwealth countries) is a small, portable spotlight.\nIts function is a beam of light which helps to see. It usually requires batteries.\n\nThe light is made by a small light bulb.  In the 20th century, it was usually an incandescent bulb.  Nowadays most use LEDs, which work slightly differently.  In 1896, the first dry cell battery was invented. Unlike previous batteries, it used a paste electrolyte instead of a liquid. This was the first battery suitable for portable electrical devices, as it did not spill or break easily and worked in any orientation.  Sometimes the electricity comes from a small generator instead of a battery.\n\nThe torch can also refer to a flammable stick that is lit on fire to provide light and (or) heat.\n\nOrigin \n\nHundreds of years ago, people used candles for all indoor light. This was expensive, and once the candle was gone, you would have to","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the flashlight is turned on by slightly unscrewing the flashlight head. The larger flashlights have a push-button switch on the side of the aluminum tube.\n\nLight_sources","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"less of an idea of where it is right now. \n\nScientists had already learned why certain substances give off characteristic colors of light when they are heated or otherwise excited. Heisenberg was trying to explain why these colors each have a characteristic brightness. It would not have been good enough if he and the other scientists had just said, \"Well, that's just the way it is.\" They were sure that there had to be a good reason for these differences, and for the fact that the ratios among the bright line strengths were always the same for each sample of an element.\n\nHe had no idea that he was going to stumble over a hidden secret of nature when he set off to discover the explanation for the intensities of the colored lines characteristic of each of the elements. The study of quantum mechanics had already shown why hydrogen has four bright lines in the part of the spectrum that humans can see. It must have seemed that the next thing to learn would simply be how to calculate their brightness. Hydrogen","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in ridge regression. The least squares approach can also be used to fit models that are not linear. As outlined above, the terms \"least squares\" and \"linear model\" are closely linked, but they are not synonyms.\n\nUsage\n\nEconomics\n\nLinear regression is the main analytical tool in economics. For example, it is used to guess consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand and labor supply.\n\nRelated pages \n\n Curve fitting\n Logistic regression\n Ordinary least squares\n\nReferences\n\nStatistics","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Isotherms are lines drawn around places with the same temperature range on isotherm maps. Each point on this line shows one temperature reading, or the average of many temperature readings. Isotherm maps also have scales that tell the signals or colors for the different temperatures. Isotherm lines are usually curvy and not straight lines.\n\nMeteorology\nThermodynamics\n\nen:Contour line#Temperature and related subjects","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A weather map is a tool. It shows facts about the weather quickly.  Weather maps have been used from the mid-19th century, for study and for weather forecasting. Some maps show differences of temperature, and weather fronts. \n\nA station model is a symbolic picture showing the weather at a reporting station. Meteorologists made the station model to put down many weather elements in a small space on weather maps. Maps thickly filled with station-model plots can be hard to read. However, they help meteorologists, pilots, and mariners to see important weather patterns. A computer draws a station model for every place of observation. The station model is mostly used for surface-weather maps. It can also be used to show the weather in the sky, though. A complete station-model map lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\nHistory \n\nPeople first began using weather charts in a modern way in the mid-19th century. They began using","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better known.\n\nHistorical aspects \nElements of scientific method were worked out by some early students of nature.\n \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\" Ptolemy (85165\u00a0AD). This is an early example of what we call Occam's razor.\n Ibn al-Haytham (Alhazen) (965\u20131039), Robert Grosseteste (1175\u20131253) and Roger Bacon (1214\u20131294), all made some progress in developing scientific method.\n Scientists in the 17th century started agreeing that the experimental method is the main way to find the truth. This was done in western Europe by men like Galileo, Kepler, Hooke, Boyle, Halley and Newton. At the same time, the microscope and the telescope were invented (in Holland), and the Royal Society was formed. Instruments,","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"techniques, which display a relationship between two variables that take either discrete or a continuous ranges of values; examples:\n\nSchematics and other types of diagrams, e.g.,\n\nReferences \n\n \nNon-verbal communication","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"fields. A simple example is attempting to find the smallest possible difference in the distance of two objects in two-dimensional space (x and y). In this context, the derivative of the function that gives the difference is taken in order to find the minimum. A more complicated example is in Machine Learning, in which the optimization function attempts to find the global minimum of the loss function in order to minimize the difference or loss between the algorithm\u2019s predictions and the actual values. This example is more difficult as Machine learning algorithms often utilize multidimensional data usually in the form of tensors yielding more complicated functions.\n\nRelated software\nToday, there are many tools to support optimization studies:\n MATLAB\n Wolfram Mathematica\n\nReferences\n\nScience\nMathematics","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to make something that can say how the different data attributes are connected\/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a question or a problem about nature. Some problems are simple, such as \"how many legs do flies have?\" and some are very deep, such as \"why do objects fall to the ground?\"\n Next, scientists investigate the problem. They work at it, and collect facts. Sometimes all it takes is to look carefully.\n Some questions cannot be answered directly. Then scientists suggest ideas, and test them out. They do experiments and collect data.\n Eventually, they find what they think is a good answer to the problem. Then they tell people about it.\n Later, other scientists may agree or not agree. They may suggest another answer. They may do more experiments. Anything in science might be revised if we find out the previous solution was not good enough.\n\nAn example \nA famous example of science in action was the expedition led by Arthur Eddington to Principe Island in Africa in","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Meditation tries to get past the \"thinking\" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the mind can better see the cause of pain. Buddhists believe that this type of seeing can end pain.\n\nBuddhist meditation is not just used for spiritual reasons. Research shows that Buddhist meditation lowers stress, anxiety and depression.\n\nMost types of Buddhist meditation focus on something. The most popular things to focus on include breath,  metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds.\n\nChristian meditation \nChristians sometimes meditate by thinking about small parts of the Bible, or by saying the words of a prayer to themselves over and over. Meditation is an expression of Christian prayer. In the Catechism of the Catholic Church is specified that by means of meditation \"The mind seeks to understand the why and how of the Christian life, in order to adhere and respond to what the Lord is asking\"; also it is pointed out that \"meditation engages thought, imagination, emotion, and desire. This mobilization of faculties is necessary in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Inner peace\u00a0(or\u00a0peace of mind) happens when a person is able to make their mind calm despite stressful things that may be happening around them.\n\nPeace\nMeditation","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A mind map is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the mind map, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.\n\nOther websites \n\nMindmapping for offices, Website \n\nDiagrams\nLearning\n\nfi:K\u00e4sitekartta","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"style=\"font-size:87%; margin:7px 0 0 6px;\"  cellspacing=0 cellpadding=0\n|\u00a0 \n|\u00a0 \n|-\n|\u00a0 England map\n|\u00a0\u00a0\n|\u00a0 Italy map\n|}\n\nAtlas\u00a0pages\u00a0are map groupings\nThe Wikiatlas is based on some simple ideas. All maps are grouped, in the manner of a World atlas, into pages stored with the name \"Atlas of...\" for each continent, such as:\n\nSimilar page titles \"Atlas of...\" cover each nation or region, such as:\n\nSome of the U.S. states also have Wikiatlas pages, such as:\n\nSimilarly, hundreds of other Wikiatlas pages are named as \"Atlas of...\" for over 200 nations and over 94 other regions of the World.\n\nWikiatlas page format\nMany pages in the Wikiatlas are arranged in a distinctive format: with maps displayed down the left-side of the page, and explanatory text presented","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Each of these smaller areas (room, state, number) is next to other small areas (other rooms\/states\/numbers). The places where the areas meet are connections. If we write down on paper a list of spaces, and the connections between them, we have written down a description of a space -- a topological space. All topological spaces have the same properties such as connections, and are made of the same structure (a list of smaller areas). This makes it easier to study how spaces behave. It also makes it easier to write algorithms. For instance, to program a robot to navigate a house, we simply give it a list of rooms, the connections between each room (doors), and an algorithm that can work out which rooms to go through to reach any other room. For more examples of this type of problem, look at Graph theory.\n\nWe can go further by creating subdivisions of subdivisions of space. For instance, a nation divided into states, divided into counties, divided into","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"law \n\nWeinberg developed the principle of genetic equilibrium independently of British mathematician G.H. Hardy. He delivered an exposition of his ideas in a lecture on 13 January 1908, about six months before Hardy's paper was published in English. His lecture was printed later that year in the society's yearbook. \n\nWeinberg's contributions were unrecognized in the English speaking world for more than 35 years. Curt Stern, a German geneticist who emigrated to the United States before World War II, pointed out in a brief paper in Science that Weinberg's exposition was both earlier and more comprehensive than Hardy's.\n\nAscertainment bias \nWeinberg pioneered studies of twins, and developed techniques to analyse phenotypic variation. His aim was to partition this variance into genetic and environmental components. In the process, he recognized that ascertainment bias was affecting many of his calculations, and he produced methods to correct for it. \n\nWeinberg observed that proportions of homozygotes","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"will tend to lie closer to the centre, the mean, of the distribution. He quantified this trend, and in doing so invented linear regression analysis. This is the starting point for much of modern statistical modelling. Since then, the term \"regression\" has taken on different meanings, and it may be used by modern statisticians to describe phenomena of sampling bias which have little to do with Galton's original observations in the field of genetics.\n\nGalton's explanation for the regression phenomenon he observed is now known to be incorrect. He stated: \u201cA child inherits partly from his parents, partly from his ancestors. Speaking generally, the further his genealogy goes back, the more numerous and varied will his ancestry become, until they cease to differ from any equally numerous sample taken at haphazard from the race at large.\u201d This is incorrect, since a child receives its genetic makeup exclusively from its parents. There is no generation-skipping in genetic material: any genetic material from earlier ancestors than the","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Regression analysis is a field of statistics. It is a tool to show the relationship between the inputs and the outputs of a system. There are different ways to do this. Better curve fitting usually needs more complex calculations. \n\nData modeling can be used without knowing about the underlying processes that have generated the data; in this case the model is an empirical model. Moreover, in modelling, knowledge of the probability distribution of the errors is not required. Regression analysis requires assumptions to be made regarding probability distribution of the errors. Statistical tests are made on the basis of these assumptions. In regression analysis the term \"model\" embraces both the function used to model the data and the assumptions concerning probability distributions.\n\nRegression can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. These uses of regression rely heavily on the underlying assumptions being satisfied. Regression analysis has been criticized as being misused for these purposes in many cases where the appropriate assumptions cannot be verified to hold.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size\u2014if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". Computer Graphics Forum, 22  2,  171\u2013179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". Computers & Graphics, 30  6,  1020\u2013 1026.\n\nSurveys\nL. H. de Figueiredo and J.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A narrative is a literary term for the events that make up a story. It is the way the events connect and make up the plot of a story or book.\n\nA narrative reports connected events, presented to the person reading or listener in a sequence of written or spoken words.\n\nA common term for narrative is plot. The study on structure in narratives is called narratology.\n\nRelated pages\nNarrative poetry\n\nReferences\n\nFiction\nWriting\nNarratology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Narratology is the study of structure in narratives. The theory of narrative or narratology was developed in the 1960s. Narratology is based on the idea of a common literary language. Narratives are found and told through oral and written language. Narratology has helped to make it easier to understand the how and why of narrative.\n\nReferences\n\nOther websites\nhttp:\/\/wikis.sub.uni-hamburg.de\/lhn\/index.php\/Main_Page  - The Living Handbook of Narratology\nhttp:\/\/www.units.miamioh.edu\/technologyandhumanities\/narratology.htm  - Notes on Narratology\nhttp:\/\/www.nou-la.org\/ling\/1975a-theonarreme.pdf - THBORIE DES NARRGMES ET ALGORITHMES NARRATIFS\n\nNarratology\nWriting","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"their day-to-day behavior.\n\n Qualitative Interviews\n\nQualitative interviews are close-ended questions that are asked directly to the research subjects. The qualitative interviews could be either informal and conversational, semi-structured, standardized and open-ended or a mix of all the above three. This provides a wealth of data to the researcher that they can sort through. This also helps collect relational data. This method of field research can use a mix of one-on-one interviews, focus groups and text analysis.\n\n Case Study\n\nA case study research is an in-depth analysis of a person, situation or event. This method may look difficult to operate, however, it is one of the simplest ways of conducting research as it involves a deep dive and thorough understanding the data collection methods and inferring the data.\n\nReferences\n\nBiology\nResearch methods","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"are often used in these models.\n Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and  query, and as such can easily incorporate almost any other retrieval  model as just a yet another feature.\n\nSecond dimension: the properties of the model \n Models without term-interdependancies treat different terms\/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\n Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"0.3 || 4.0\n|-\n|\n| 24|| 7 || 21.4|| 48.8|| 0.0 || 86.5 || 3.2|| 3.3|| 0.5|| 0.2|| 8.2\n|-\n|\n| 24|| 0 || 8.4|| 39.0|| 41.9 || 77.4|| 2.0|| 0.5 || 0.1 || 0.3|| 5.6\n|-\n|\n| 23|| 0 || 11.0|| 58.0|| 39.3 || 76.2 || 1.2 || 0.7|| 0.5|| 0.0|| 5.6\n|-\n|\n| 14|| 0 || 7.9|| 45.0|| 33.3 ||","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"38.3 || 76.1|| 4.9 || 1.8|| 0.6|| 0.2 || 11.7\n|-\n|\n| 65|| 1 || 26.6|| 47.8|| 35.1|| 61.4 || 4.0|| 3.4|| 1.1|| 0.3|| 7.0\n|-\n|\n| 70 || 66 || 20.7 || 62.7 || 100.0 || 48.0 || 7.0 || 2.3 || 0.5 || 1.6 || 5.4\n|-\n|\n| 46|| 13|| 16.7|| 54.8|| 0.0 || 53.0 || 5.6|| 0.7|| 0.4|| 1.1 ||","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"4.7 || 1.3|| 0.7|| 0.2 || 9.0\n|-\n|\n| 24|| 3 || 32.0|| 47.6|| 38.5|| 56.1 || 4.4|| 3.8|| 1.2|| 0.4|| 8.9\n|-\n|\n| 22 || 22 || 16.6 || 62.3 || 0.0 || 35.7 || 5.7 || 1.4 || 0.6 || 1.6 || 4.6\n|-\n|\n| 23|| 1|| 8.8|| 53.6|| 0.0 || 43.2 || 2.7|| 0.3|| 0.0|| 0.3 || 4.0\n|-\n|\n| 24|| 7 ||","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Similar to a finger puppet, but  larger. The puppeteer uses his or her fingers and hand to work the puppet. Punch and Judy are famous glove puppets.\n\nPop-up puppets: A cone with a rod through and a puppet inside. When the rod is pushed up, the puppet appears.\n\nPaddle Puppet: The puppet is on the end of a paddle. When the puppeteer steps on the paddles back end the other end lifts up, making it seem that the puppet moves by itself.\n\nJumping Jack: A puppet where the arms and legs of the puppet are joined to a string. When the string is pulled down, the arms and legs go up.\n\nRod Puppet and Bunraku: The puppet is worked with rods joined to the puppets arms and legs, while another puppeteer moves the head and sometimes the mouth. Bunraku is a special type of Japanese rod puppetry.\n\nShadow Puppet: Another very simple puppet. A cut out figure on a rod is held","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vibrato is a series of very quick changes up and down to the pitch of a note. It is like a deliberate, controlled wobble. The word \u201cvibrato\u201d is related to the word \u201cvibrate\u201d.\n\nVibrato is used to make a melody sound more beautiful by making it more expressive. Singers use vibrato, especially when singing dramatic solo music such as opera. In Italy a singer\u2019s vibrato is sometimes called \u201ctremolo\u201d.\n\nViolinists and other players of string instruments produce vibrato by moving the finger backwards and forwards on the fingerboard. Vibrato has to be learned carefully, as the hand needs to be very relaxed. Many beginners who start to use vibrato will try to vibrate too fast and this makes an unpleasant sound. Vibrato can be produced on wind instruments by small changes of breath control.\n\nMusicians sometimes disagree about how much vibrato was used in earlier times. People who play music from the Renaissance or Baroque on period","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a quick change in dynamics, molto cresc. and molto dim. are often used (molto means a lot). For slow changes poco a poco cresc. and poco a poco dim. are used (poco a poco means little by little, i.e. very gradually.)\n\nMusical notation","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"4D, meaning the common 4 dimensions, is a concept in mathematics. It has been studied by mathematicians and philosophers since the 18th century.  Mathematicians who studied four-dimension space in the 19th century include M\u00f6bius, Schl\u00e4fi, Bernhard Riemann, and Charles Howard Hinton.\n\nIn geometry, the fourth dimension is related to the other three dimensions  of length, width, and depth by imagining another direction through space. Just as the dimension of depth can be added to a square to create a cube, a fourth dimension can be added to a cube to create a tesseract.\n\n4D is also an important idea in physics, developed in the 20th century. In physics, it refers to the idea of time as a fourth dimension, added to the (3D) spatial dimensions.  Albert Einstein developed the idea of spacetime by connecting space and time together. The difference is that spacetime is not a Euclidean space,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In dimensional analysis, a dimensionless quantity (or more precisely, a quantity with the dimensions of 1) is a quantity without any physical units and thus a pure number. Such a number is typically defined as a product or ratio of quantities which do have units, in such a way that all the units cancel out.\n\nExample \n\"out of every 10 apples I gather, 1 is rotten.\" -- the rotten-to-gathered ratio is (1 apple) \/ (10 apples) = 0.1 = 10%, which is a dimensionless quantity.\n\nList of dimensionless quantities \nThere are infinitely many dimensionless quantities and they are often called numbers. Some of those that are used most often have been given names, as in the following list of examples (alphabetical order):\n\nOther websites \n Biographies of 16 scientists with dimensionless numbers of heat and mass transfer named after them \n How Many Fundamental Constants Are There? by John Baez\n Michael Sheppard,","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of a piece of metal, or the speed of light. If we weigh an object with sensitive equipment we will get minutely different results. No system of measurement is ever perfect. We get a series of estimates, each one being a measurement. These are samples, with a certain degree of error. Statistics is designed to describe error, and carry out analysis on this kind of data.\n\nThere are different kinds of samples:\nA complete sample includes all the elements that have a given property. \nAn unbiased or representative sample is produced by taking a complete sample and selecting elements from it, in a process that does not depend on the properties of the elements.\nThe way the sampling is obtained, along with the sample size, will have an impact on how the data is viewed.\n\nStratified sampling \nIf a population has obvious sub-populations, then each of the sub-populations needs to be sampled. This is called stratified sampling. Stratified sampling is also known as stratified random sample.","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a framework for speculation on the next thirty-three years. MacMillan. . With Anthony Wiener.\n1968 Can we win in Viet Nam?. Praeger. Kahn with four other authors: Gastil, Raymond D.; Pfaff, William; Stillman, Edmund; Armbruster, Frank E. \n1970. The Emerging Japanese Superstate: challenge and response. Prentice Hall. \n1971. The Japanese challenge: The success and failure of economic success. Morrow; Andre Deutsch. \n1972. Things to come: thinking about the seventies and eighties. Macmillan. . With B. Bruce-Briggs.\n1973. Herman Kahnsciousness: the megaton ideas of the one-man think tank. New American Library. Selected and edited by Jerome Agel. \n1974. The future of the corporation. Mason & Lipscomb. \n1976. The next 200 Years: a scenario for America and the world. Morrow.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Herman Kahn (February 15, 1922 \u2013 July 7, 1983) was one of the preeminent futurists of the second half of the twentieth century. His theories helped to develop the nuclear strategy of the United States.\n\nCareer \nKahn, a man of \"captivating personality and large intellectual gifts\", worked for the RAND Corporation, Santa Monica, California, as a military strategist (19481958). He was a founder of the Hudson Institute think tank. This gives ideas and consultations for the U.S. government and military chiefs, and for business clients.\n\nHe was known for analyzing the likely consequences of nuclear war and recommending ways to improve survivability. His key idea was that the threat of nuclear war could be controlled by the use of carefully graded deterrence. In order to get his ideas across he ran two-day seminars which included role-playing in various scenarios. His ideas, when published in On Thermonuclear War, caused a sensation.\n\n\"At the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"scenarios. His ideas, when published in On Thermonuclear War, caused a sensation.\n\n\"At the minimum, an adequate deterrent for the United States must ... persuade [the Soviets] that, no matter how skillful or ingenious they were, an attack on the United States would lead to a very high risk if not certainty of large-scale destruction to Soviet civil society and military forces\".\n\nHowever, responses must be proportionate, because if one threatened all-out war as a response for some rather modest misbehaviour, then the threat was simply not believable, and would not work. The need to think things through in detail was the topic of his second book, Thinking about the Unthinkable.\n\nKahn was less successful later in his career when he turned his attention to general economics and politics. His ideas on Japan seemed good at the time, but now seem to be not quite right. He predicted that Japan would become the world's third superstate and a military superpower. This he based on Japan's","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship\/affect, communication, economic transactions, interactions, kinship, authority\/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and\/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called \"Law of 150\" which suggests that about 150 people is the best size for a village or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"church or temple is almost always a center of a social network). Often the network has an identity of its own which is quite real, even though it may have no official recognition. Networks may be centered on places, or on families, or on worldwide communities with common interests.\n\nSources \nThe Law of 150 is documented in R.I.M. Dunbar 1992. Neocortex size as a constraint on group size in primates. Journal of Human Evolution. 20, pp.\u00a0469\u2013493.\n\nThe field of study which investigates human social life is social psychology.\n\nRelated pages \nSocial network service\n\nRelationships\nSocial groups","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"those has a letter code from A to Z. (The letter I is not used.) The letters start with A in the northwest corner and go to Z in the southeast corner. These squares are outlined in light grey on the map. The ones that have land have letters shown.\n\nThe system can also be used to give the location of a smaller area inside a lettered square. This is done by giving the distance east and north from the southwest corner of the square. For example, NH0325 (or NH 03 25) means a one-kilometre square whose southwest corner is 3\u00a0km east and 25\u00a0km north from the southwest corner of square NH. The system can also specify areas smaller than one kilometre. For example, the grid reference of the 100-metre square containing the summit of Ben Nevis is .\n\nNotes\n\nReferences \n Standing, Peter (2006) OV0000 a unique grid square at Beast Cliff, Geograph Project,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"area_rank              = \n| area_magnitude         = \n| area_km2               = \n| area_sq_mi             = \n| percent_water          = \n| population_estimate    = 2,000,000\n| population_estimate_year = 2014\n| population_estimate_rank = \n| population_census      = \n| population_census_year = \n| population_density_km2 = \n| population_density_sq_mi = \n| population_density_rank = \n| GDP_PPP_year           = \n| GDP_PPP                = \n| GDP_PPP_rank","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in a location.  Users can specify a place via latitude and longitude, postal code or address.  Then GIS can provide information about the people in that area including: income, population and education levels.  It can also tell about physical things such as soil types, vegetation, water sources, manmade structures like buildings and roads.  \n\nAcronyms\nBranches of geography","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Thought is the operation of the brain in conscious activity. It may or may not be goal-directed, aimed at solving specific problems. It is not the only way brains operate. Behaviour may occur as a result of instinct, and the adaptive unconscious may solve problems without a person being aware.\n\nOther animals can use their brains to solve problems, but there is no way of telling whether they do so consciously. Thought is investigated by four or five academic disciplines, each in its own way. The disciplines include psychology, philosophy, biology, physiology, psychoanalysis and sociology.\n\nPhilosophy \nPhilosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, functions, properties, and consciousness. The mind-body problem, i.e. the relationship of the mind to the body, especially the brain, is a central issue in philosophy of mind.\n\nThe mind-body problem \nThe mind-body problem has to do with the explanation of the relationship that exists between minds, or mental","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The wavelet transform is a time-frequency representation of a signal. For example, we use it for noise reduction, feature extraction or signal compression.\n\nWavelet transform of continuous signal is defined as\n ,\nwhere\n  is so called mother wavelet,\n  denotes wavelet dilation,\n  denotes time shift of wavelet and\n  symbol denotes complex conjugate.\n\nIn case of  and , where ,  and  and  are integer constants, the wavelet transform is called discrete wavelet transform (of continuous signal).\n\nIn case of  and , where , the discrete wavelet transform is called dyadic. It is defined as\n ,\nwhere\n  is frequency scale,\n  is time scale and\n  is constant which depends on mother wavelet.\n\nIt is possible to rewrite dyadic discrete wavelet transform as\n,\nwhere  is impulse characteristic of continuous filter which is identical to  for given .\n\nAnalogously, dyadic wavelet transform with discrete time (of discrete signal) is defined","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ecology is the branch of biology that  studies the biota (living things), the environment, and their interactions. It comes from the Greek oikos = house; logos = study. \n\nEcology is the study of ecosystems. Ecosystems describe the web or network of relations among organisms at different scales of organization. Since ecology refers to any form of biodiversity, ecologists research everything from tiny bacteria in nutrient recycling to the effects of tropical rain forests on the Earth's atmosphere.  Scientists who study these interactions are called ecologists. \n\nTerrestrial ecoregion and climate change research are two areas where ecologists now focus.\n\nThere are many practical applications of ecology in conservation biology, wetland management, natural resource management (agriculture, forestry, fisheries), city planning (urban ecology), community health, economics, and applied science. It provides a framework for understanding and researching human social interaction.\n\nPopulation ecology \n\nPopulation ecology measures the size of a population: all the living things from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"this view which is often part of religion. It puts ethical limits on the actions of scientists, for instance it argues against animal experiments or genetically modified food. It usually is seen as opposed to science, not part of it. Its models of nature are not usually accepted by most scientists, but they matter in politics.\n\nPreservation \nTo preserve nature, ecology movement activists now cooperate in a global power network. It includes not just parties in politics but also NGOs like Greenpeace, Earth First or World Wide Fund for Nature.\n\nEffects on maps \nOne model of nature they have agreed on is a map that shows ecoregions, which are the natural borders of ecology maps. According to this map, there are 867 regions divided into 8 ecozones - plus others which are in the ocean.\n\nEffects on language \nMost anthropologists agree that aboriginal languages have a small model of local nature in them. For instance, there will be more words to describe snow in an Arctic language, and more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ecosystem valuation is a widely used tool to find out the impact of human activities on an environmental system. It is done by giving an economic value to an ecosystem or its ecosystem services.\n\nValue of ecosystem services\nThe simplest form of ecosystem valuation is that one looks at an ecosystem as if its ecological yield had the same value as it would be on commodity markets. That means one looks at the value of water, wood, fish or game, that is produced in that ecosystem.  Thus, a price can be put on the natural capital of an ecosystem based on the price of natural resources it yields each year.  \n\nMore complex arguments in ecosystem valuation regard nature's services and the assignment of values in a service economy to all that nature does \"for humans\".  Studies compiled by Robert Costanza in the 1990s argued strongly that even just considering the most basic seventeen of these services, the combined value of the ecosystems of the earth was worth more (US$33T) each year than","type":"Document"}]}}