query,ground_truths,result,source_documents
What is the advantage of A/B testing?,"The advantage of A/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process."," A/B testing is a statistical hypothesis test used to compare two different groups, usually in the context of a software or a software-beverage-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-cable- or-a-","[Document(page_content='A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: ""a procedure intended to establish the quality, performance, or reliability of something"".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and'), Document(page_content=""A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from""), Document(page_content=""be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and other vehicles include a crash test. The car is put under severe conditions to see what will make it fail, or deliberately crashed to measure the damage. Other machines can also be crash tested. Crash test dummies can be used instead of humans. They are placed in the car seat to see if a human in the crash would have been injured or killed.\n\nPeople \n\nPeople are tested to see what they have learned. This is often called an assessment or examination. In learning, a test item is a question, or set of questions.\n\nMany people think tests are valuable. They believe tests:\n are a quick and fair way of judging a test taker's performance\n enable predictions about test takers to be made\n allow selection\n improve performance by highlighting areas that need work.\n\nHowever, academic tests are not perfect measures. Tests could only partly measure a student’s memory and maybe their understanding. The test""), Document(page_content='A test bay is an area used in engineering. It can be a hall, laboratory or even a department. At these places, the produced materials are tested before delivery. To test them, testing guidelines of industry or this firm are used. The test bay is part of the quality management. The protocols written during testing are evidences in case of insurance issues, say.\n\nEngineering\nTests')]"
What is the ANOVA powerful for?,ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments., ANOVA (the ANOVA test) is a statistical method used to compare the mean (average) of two or (s) groups. It is used when we want to know if there is a statistical significance between the groups. ANOVA is a powerful method when we have a lot of data and we want to compare the mean of two or (s) groups. It is also used when we want to compare the mean of two or (s) groups that have different sample size. ANOVA is a powerful method when we want to compare the mean of two or (s) groups that have different variance.,"[Document(page_content='sample minus one:\nDf\u200b=N−1\nwhere:\n\nDf\u200b=degrees of freedom\n\nN=sample size\u200b\n\nDegrees of freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a chi-square. It is essential to calculate degrees of freedom when trying to understand the importance of a chi-square statistic and the validity of the null hypothesis.\n\nChi-Square Tests \nThere are two different kinds of chi-square tests: the test of independence, which asks a question of relationship, such as, ""Is there a relationship between gender and SAT scores?""; and the goodness-of-fit test, which asks something like ""If a coin is tossed 100 times, will it come up heads 50 times and tails 50 times?""\n\nFor these tests, degrees of freedom are utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of'), Document(page_content=""the others prepared by first adding the tea.  She was to select the  four cups prepared by one method.\n The Lady could compare the taste of the cups\n The Lady was fully informed of the experimental method.\n The null hypothesis was that the Lady had no such ability.\n Note that in Fisher's approach, there is no alternative hypothesis; this is instead a feature of the Neyman–Pearson approach.\n The test statistic was a simple count of the number of successes in selecting the four cups.\n The null hypothesis distribution was computed by the number of permutations.  The number of selected permutations and the number of unselected permutations were equal.\n\n The critical region was the single case of four successes of four possible based on a conventional probability criterion (<\xa05%; 1 of 70 ≈\xa01.4%).\n\nIf and only if the Lady properly categorized all eight cups was Fisher willing to reject the null hypothesis – effectively acknowledging the Lady's ability at a 1.4% significance level (but""), Document(page_content='This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are'), Document(page_content=""A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from"")]"
What is the difference between frequentist and Bayesian approaches to probability?,"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","

Frequentist probability is based on the frequency of an event in the past, or the number of times an event has occurred out of a total number of experiments. It is a way to make probabiilty inferences based on the observed results of experiments.

Bayesian probability, on the another, is based on the likelihood of an event given some evidence. It is a way to make probabiilty inferences based on the available evidence and prior probabiilty.

In the example given, Bayesian probability is used to classify an email as ""spam"" or ""not spam"" based","[Document(page_content='Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don\'t have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as ""spam"" or ""not spam"".  One thing you know about this email is that it has an emoji in the subject line.  Say it\'s the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it\'s 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that\'s different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes\'s'), Document(page_content='Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics'), Document(page_content=""Bayes' theorem is just another way to write that equation.\n\nRelated pages \n\n Bayesian probability\n Bayesian network\n\nReferences \n\nMathematics""), Document(page_content='In probability theory and applications, Bayes\' theorem shows the relation between a conditional probability and its reverse form. For example, the probability of a hypothesis given some observed pieces of evidence, and the probability of that evidence given the hypothesis. This theorem is named after Thomas Bayes ( or ""bays"") and is often called Bayes\' law or Bayes\' rule.\n\nFormula \n\nThe equation used is:\n\nWhere:\n P(A) is the prior probability or marginal probability of A. It is ""prior"" in the sense that it does not take into account any information about\xa0B.\n P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from (or depends upon) the specified value of\xa0B.\n P(B|A) is the conditional probability of B given A. It is also called the likelihood.\n P(B) is the prior or marginal probability of B, and acts as a normalizing constant.\nIn many')]"
Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?,Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.," A challenging imposing hierarchy on the order of causes is the one that priori-

tions of causality. A challenging imposing hierarchy on the order of causes is the one that priori-

tions agency and chance. In the contexts of agency, acknowledging serendipity and Murphy's law can be challenging because they suggest that chance and agency are on the same causal hierarchy, and that the outcome of an action is not just a cause of the effect, but also a chance occur-

ance.","[Document(page_content='thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy'), Document(page_content='is most likely to occur when one is wholeheartedly performing a task or activity for intrinsic purposes. Intrinsic purposes involve anything that someone does merely because they want to. Extrinsic activities will not cause flow to occur. Extrinsic activities are anything that someone does because there is some other force causing them to do it. Extrinsic activities will not cause flow to occur. Passive activities like taking a bath or even watching TV usually do not elicit flow experiences as individuals have to actively do something to enter a flow state. While the activities that induce flow may vary and be multifaceted, Csikszentmihályi asserts that the experience of flow is similar despite the activity.\n\nComponents of flow\nCsíkszentmihályi identifies the following ten factors as accompanying an experience of flow:<ref name=Finding>Csíkszentmihályi, Mihály 1996. Finding flow: the psychology of engagement with everyday life. Basic'), Document(page_content='what they experienced. So, relying on our memories to make decisions does not lead to the highest net pleasure. To experience the net largest amount of pleasure,  people should spend as much time as possible on things that they are unwilling to stop doing.\n\nRestrictions and criticisms \nStudies found that other factors influence the peak-end rule. Firstly, the expectation. A high expectation is compared with the actual experience at the start. If a difference exists, the starting experience will be the most important factor in overall experience evaluation.  The peak-end rule is more likely to be applicable to lower expectation situations. Secondly, one study finds that the effect of peak-end law is small on one day experiences. Moreover, some people are easily affected by the rule but others are not. Thirdly, according to Ariely and Carmon, how we feel at the moment of evaluation also affects the outcome.\n\nReferences'), Document(page_content='or those that give a purpose to behaviour  Example: The reason why the artist wanted to make the statue.\n\nAristotle told people of two types of causes: proper (prior) causes and accidental (chance) causes. Both types of causes, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes; so that generic effects assigned to generic causes, particular effects to particular causes, and operating causes to actual effects. It is also essential that ontological causality does not suggest the temporal relation of before and after - between the cause and the effect; that spontaneity (in nature) and chance (in the sphere of moral actions) are among the causes of effects belonging to the efficient causation, and that no incidental, spontaneous, or chance cause can be prior to a proper, real, or underlying cause per se.\n\nAll investigations of causality coming later in history will consist in imposing a favorite hierarchy on the order')]"
What is the recommended course of action for datasets with only categorical data?,"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset."," For datasets with only categorical data, methods for numerical data, as used in the example of the survey question about breakfast, are not directly useful. The recommended course of action is to use methods for categorical data, which can be used to find new information, make inductve in-

Question: What is the recommended course of action for datasets with only categorical data?
Helpful Answer: For datasets with only categorical data, methods for numerical data, as used in the example of the survey question about breakfast, are not directly useful. The recommended course of action is to use methods for categorical data, which","[Document(page_content='Statistical surveys are collections of information about items in a population. \n\nSurveys can be grouped into numerical and categorical types. A numerical survey is to get numbers as replies. For example:\nHow many minutes, on average, do you spend eating breakfast?\n\nThis survey question is used to generate numbers in minutes. An example of a categorical question is:\n\nWhat is your favorite color?\n\nThe categories for this would be yellow, green, purple, etc... which are not numbers.\n\nSurveys of human populations and institutions are common in political polling and government, health, social science and marketing research. A survey may concentrate on opinions or factual information depending on its purpose.\n\nOther websites \n\nAmerican Association for Public Opinion Research (AAPOR)\nHow to design a web-based statistical survey online. \nAjeebo: Web based survey software tools directory\nConducting Web-Based Surveys\nFree resources for methods in social research  has links to resources for surveys and sampling.\nOnline Survey Best Practice \nShort Courses in'), Document(page_content='(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected/influence each other. The'), Document(page_content='Classification could mean:\n\n Library classification and classification in general\n Optimal classification\n Biological classification\n Scientific classification (disambiguation)\n Classification (literature)\n Statistical classification\n Security classification\n Classification theorems in mathematics.\n Film classification\n CLASSIPHI, a seabed mapping tool supplied by QinetiQ\n Civil service classification, personnel grades in government\n Attribute-value system\n\nRelated pages\n Class\n Categorization'), Document(page_content='This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are')]"
What is a Generalised Linear Model (GLM)?,A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.,"
A Generalised Linear Model (GLM) is a statistical model that uses a linear predictor function, but can also include non-linear and non-linear relationship between the predictor variables and the dependent variable. It is a more General version of Linear regression.

In a Generalised Linear Model, the relationship between the predictor variables and the dependent variable is modeled using a ""linear predictor"" (or ""linear predictor function"" or ""linear predictor variable"" or ""linear predictor term"" or ""linear predictor vector"" or ""linear predictor value"" or ""linear predict","[Document(page_content='predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called ""fitting the line to the data."" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.'), Document(page_content='A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics'), Document(page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".\n\nStatistics\nFunctions and mappings'), Document(page_content='Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,')]"
What is Cluster Analysis?,Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).," Cluster analysis is a type of data analysis in which objects are group together so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters). It is a common task in data mining. The Dunn Index (DI) is a metric for judging a clustering algorithm, with a higher DI value implies better clustering. The DI is equal to the minimum inter-cluster distance divided by the maximum cluster size.

In other contexts, analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it","[Document(page_content='Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics'), Document(page_content='The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics'), Document(page_content='Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek ἀνάλυσις (analusis, ""a breaking up"", from ana- ""up, throughout"" and lysis ""a loosening"").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.'), Document(page_content='Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster/tree/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,')]"
What is the purpose of Network Analysis?,Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.," Network analysis is a way of analyzing the structure and relationships of a network, which can be used to find out the Strengths, Weaknesses, Opportunities, and Threats (SWOT) of a network. It is a tool used in decision-making and strategy-making, and can be used in community organizations to help build social networks and support. Network analysis can be used to find out the topology of a network, which is the pattern of relationships among a set of people or organizations. It can be used to find out the size and structure of a network, and to find out the relationships","[Document(page_content='not matter. The only important property of a route is the order in which the bridges are crossed. So, he changed the problem to abstract terms. This laid the foundations of graph theory. He removed all features except the list of land masses and the bridges connecting them. In the language of graph theory, he replaced each land mass with an abstract ""vertex"" or node. Then he replaced each bridge with an abstract connection, an ""edge"". An edge (road) recorded which two vertices (land masses) were connected. In this way, he formed a graph.\n\n →\n →\n\nThe graph drawn is an abstract picture of the problem. So, the edges can be joined in any way. Only whether two points are connected or not are important. Changing the picture of the graph does not change the graph itself. \n\nNext, Euler observed that (except at the endpoints of the walk), whenever one enters a vertex by a bridge, one leaves the vertex by a bridge. In any walk of the'), Document(page_content=""SWOT Analysis is a tool to find out the Strengths, Weaknesses, Opportunities, and Threats that are to be expected in a project or in a business venture or in something similar. It means that the situation (internal and external to the organization or individual) is looked at. It is one of the best tools to conduct the internal and external analysis for any company.\n\nInternal and external factors \nInternal factors may include employment, finance, manufacturing opportunities, and all of the marketing mix's 4Ps.\n\nExternal factors include macroeconomics, technological change, legislation, and sociocultural changes, as well as changes in the marketplace.\n\nUse \nSWOT analysis can be used in any decision-making situation. SWOT analysis can be used:\n\n to build organizational or personal strategy;\n for matching and converting;\n allow organizations to achieve their goals;\n as a basis for the analysis of business and environmental factors;\n in marketing — analyzing competitors.\n\nIn community organizations \nThe SWOT analysis has been used in""), Document(page_content='A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship/affect, communication, economic transactions, interactions, kinship, authority/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called ""Law of 150"" which suggests that about 150 people is the best size for a village or'), Document(page_content='A Guide to Network Topology\n Research network topology \n Types of topology \n Logical Topology Example\n 8 Common Network Topologies & How to Use\n\nTendaishe Sigauke, (2007: 46) Explaining networking terms\n\nComputer networking')]"
What is ANCOVA?,"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account."," ANCOVA (Anovianovara) is a Romanian singer-songwriter, a 20th highest mountain in the Andes, or a wine from Catalonia. It is most commonly know as a Romanian singer-songwriter, and she wrote songs for Goran Bregovic's album Champagne for Gypsies. She died in a car crash on the Danube river in 2018.

UnHelpful Answer: ANCOVA is a Romanian singer-songwriter, a mountain in Bolivia, or a wine from Catalonia. It","[Document(page_content='Anca Pop (October 22, 1984 – December 16, 2018) was a Romanian-Canadian singer-songwriter. In 2008, she wrote two songs for Goran Bregovic\'s thirteenth studio album Champagne for Gypsies. After, she signed with Roton Music. She released four singles: ""Free Love"", ""Super Cool"", ""Ring Around"", and ""Loco Poco"".\n\nPop died on December 16, 2018 in a car crash on the Danube river, near the commune Svinița, aged 34.\n\nReferences\n\n1984 births\n2018 deaths\nRoad accident deaths\nCanadian singer-songwriters\nRomanian singers\nRomanian writers'), Document(page_content='Cava is a white sparkling wine from Catalonia. Like other similar sparkling wine, fermentation will happen in the bottle. It is produced from different kinds of white grapes. It has been produced since 1872. Today, most Cava is produced in Penedès. Tzhe regulations for Cava are similar to those for Crémant, which is mostly produced in France.\n\nWine\nSpanish food\nCatalonia'), Document(page_content='Ancohuma is a  mountain in the Andes range, in Bolivia. It is the 20th highest mountain in the Andes.\n\nMountains of Bolivia\nAndes'), Document(page_content='Trnava (German: Tyrnau, Hungarian: Nagyszombat) is a city in western Slovakia, around 50\xa0km from the capital Bratislava. Because it has so many churches with its town walls, it is commonly called Little Rome or Slovak Rome.\n\nHistory\nThe town was first mentioned in 1211. It received royal free town privileges as the first town in Slovakia in 1238. The town had its height as a cultural and religious centre, as it was seat of archbishopric  from 1541 to 1820. The first horse-drawn railway in the Kingdom of Hungary was built to Trnava from Bratislava in 1846. It is again an seat of archbishopric since 1978. In 2003, French car manufacturer PSA started construction of automobile plant. It was finished in 2006\n\nTwin towns\n Scranton, Pennsylvania (United States)\n Varaždin, Croatia\n\nOther')]"
What are the key principles and assumptions of ANCOVA?,"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset."," ANCOVA, or ANalove of C0varianze ANalvsy, is a method used in the context of ANalvsy of VArieity (an ANalvsy of VArieity, or ANalvsy of V, for V(er)iableness, is a method used to test the hypothesis that the means of at-the-group-or-above-group-or-above-group-or-above-group-or-above-group-or-above-group-or-above-group-or","[Document(page_content='Hertz to Ernst Mach eventually discussed specific requirements of operable scientific theories and physical laws such as the predictability of results in experiments and the functionality of laws in computations.\n\nPrinciples\nIn its strongest original formulation, positivism could be thought of as a set of five principles:\n The unity of the scientific method – i.e., the logic of inquiry is the same across all sciences (social and natural).\n The aim of science is to explain and predict.\n Scientific knowledge is testable. Research can be proved only by empirical means, not arguments alone. Research should be mostly deductive, i.e. deductive logic is used to develop statements that can be tested (theory leads to hypothesis which in turn leads to discovery and/or study of evidence). Research should be observable with the human senses. Arguments are not enough, sheer belief is out of the question.\n Science does not equal common sense. Researchers must be careful not to let common sense bias their research.\n Science should be as value-neutral as'), Document(page_content=""adapted to the climates such as in Central and Eastern Europe. She was described as a pioneer of durum wheat breeding in Romania.\n\nAstronomers\n\n Victor Anestin - he was on of the earliest to write about the possibility of using the atomic power for war purposes; his description was in a novel (of his) published in February 2014 (in the same year as H. G. Wells' The World Set Free).\n Noah Brosch\n Nicolae Culianu\n Nicolae Donici\n Mario Livio\n\nBiologists\n Réka Albert - she is noted for the Barabási–Albert model (en) and research into scale-free networks  (en) and Boolean modeling of biological systems.\n Ana Aslan\n George Assaky - anatomist\n Petre Mihai Bănărescu\n Dimitrie Brândză\n Paul Bujor - anatomist\n Nicolae Cajal\n Ioan Cantacuzino\n Dimitrie""), Document(page_content=""not have enough energy to do all the work required by CBT.\n\nRevisions \nSince the emergence of CBT, several revisions and models have been made. One example is the Five Areas model. This model was created because traditional CBT tends to use complex terms which may be unsuitable for some patients or colleagues who do not specialise in CBT. The Five Areas model aims to be more versatile by using language that can easily be understood. In that sense, this model is not a new approach. Instead, it is a revision of standard CBT. The Five Areas model specifically focuses on five key elements, hence the name. These are life situation, altered thinking, altered emotions, altered physical feelings and altered behaviour.\n\nReferences\n\nOther websites \n Cognitive Therapy Today\n An Introduction to Cognitive Therapy & CBT\n CBT Podcasts (The Jove Institute) \n The Royal College of Psychiatrists' cognitive therapy leaflet\n REBT Network\n\nMedicine\nPsychology""), Document(page_content=""(also known as economic or social rights) that require the provision of education and protections against severe poverty and starvationgroup rights that provide protection for groups against ethnic genocide and for the ownership by countries of their national territories and resources\n\n Personality traits \n Main articles: Big Five personality traits and Trait theory''\n\n Extroversion\n Agreeableness\n Conscientiousness\n Neuroticism / Emotional stability\n Openness to experience\n\nPersonal values\n\nVirtues \nSee the list at Virtue#Virtues and values\n\nVices \n\n Seven Deadly Sins\n Pride\n Vanity\n\n Avarice \n Greed\n Lust\n\n Wrath \n Anger\n Gluttony\n\n Envy \n jealousy\n\n Sloth \n Laziness\n\nSelf-actualization \n\n Action\n Competence\n\n Effectiveness\n Efficacy\n\n Success\n\nSelf management \n\n Autodidacticism (self-education)\n Goal\n Goal setting\n Personal budget\n Personal development\n Personal finance\n Problem solving\n Self-actualization"")]"
What are the assumptions associated with ANCOVA?,"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes."," ANCOVA, or ANalymis of C OVArie (or ANalymis of C OVarying) Factors, is a form of ANalymis of VArie (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANalymis of V) (or ANal","[Document(page_content=""some form of unconscious inferences. As well as information from the eyes, the brain used information from previous experiences. The world as experienced is built up from assumptions and conclusions from incomplete data, using prior experience of the world.\n\nExamples of well-known assumptions, based on visual experience, are:\n light comes from above\n objects are normally not viewed from below\n faces are seen (and recognized) upright.\n closer objects can block the view of more distant objects, but not vice versa\n figures (i.e., foreground objects) tend to have convex borders\n\nThe study of visual illusions (cases when the inference process goes wrong) has yielded much insight into what sort of assumptions the visual system makes.\n\nRelated pages\n Neuroscience\n Ophthalmology\n Color blindness\n Achromatopsia\n\nReferences\n\nOther websites\n\n Visual Perception 3 - Cultural and Environmental Factors\n Gestalt Laws\n The Organization of the Retina and Visual System\n Dr Trippy's Sensorium A website dedicated to the study of the human""), Document(page_content=""adapted to the climates such as in Central and Eastern Europe. She was described as a pioneer of durum wheat breeding in Romania.\n\nAstronomers\n\n Victor Anestin - he was on of the earliest to write about the possibility of using the atomic power for war purposes; his description was in a novel (of his) published in February 2014 (in the same year as H. G. Wells' The World Set Free).\n Noah Brosch\n Nicolae Culianu\n Nicolae Donici\n Mario Livio\n\nBiologists\n Réka Albert - she is noted for the Barabási–Albert model (en) and research into scale-free networks  (en) and Boolean modeling of biological systems.\n Ana Aslan\n George Assaky - anatomist\n Petre Mihai Bănărescu\n Dimitrie Brândză\n Paul Bujor - anatomist\n Nicolae Cajal\n Ioan Cantacuzino\n Dimitrie""), Document(page_content='Heyrovský, born in Prague, Austria-Hungary, now Czech Republic, Chemistry, 1959\nCarl Ferdinand Cori*, born in Prague, Austria-Hungary, now Czech Republic, Physiology or Medicine, 1947 \nGerty Cori*, born in Prague, Austria-Hungary, now Czech Republic, Physiology or Medicine, 1947\nBertha von Suttner*, born in Prague, Austria-Hungary, now Czech Republic, Peace, 1905\n\nDenmark\nJens Christian Skou, Chemistry, 1997\nNiels Kaj Jerne, born in United Kingdom, Physiology or Medicine, 1984\nAage Bohr, Physics, 1975\nBen Roy Mottelson,  born in United States, Physics, 1975\nJohannes V. Jensen, Literature, 1944\nHenrik Dam, Physiology or Medicine, 1943\nJohannes Fibiger, Physiology or'), Document(page_content=""and model (Penthouse, Heavy Metal), problems caused by dementia.\nWalter Taibo, 89, Uruguayan footballer (Huracán, Nacional, national team).\nMohammad Khaledur Rahman Tito, 74, Bangladeshi politician, MP (1986–1988, 2008–2014).\nThorleif Torstensson, 71, Swedish singer (Thorleifs), COVID-19.\n\n11\nMassoud Achkar, 64, Lebanese politician, co-founder of the Lebanese Forces, COVID-19.\nSheldon Adelson, 87, American casino businessman and philanthropist, founder of Las Vegas Sands, problems caused by non-Hodgkin lymphoma.\nVassilis Alexakis, 77, Greek-born French writer and translator.\nEdward Beard, 80, American politician and businessman, member of the U.S. House of Representatives from Rhode Island's 2nd congressional district"")]"
What are the strengths and challenges of Content Analysis?,"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples."," Content analysis is a powerful tool for analyzing communication, as it can provide a great in the field of marketing, media studies, and communication analysis. The strengths of the analysis are:

1. it can provide a great of communication, as it can identify trends, commonalities, and

rein forces, and can also provide a of the communication's context.

1. it can provide a of communication, as it can identify the and emotions that are and the relationships between different elements of the communication.

1. it can provide a of communication, as it can identify the and the of the","[Document(page_content='factors;\n in marketing — analyzing competitors.\n\nIn community organizations \nThe SWOT analysis has been used in community work as a tool to identify positive and negative factors within organizations.\n\nAlthough SWOT analysis is a part of the planning, it will not provide a strategic plan if used by itself, but a SWOT list can becomes a series of recommendations.\n\nStrengths and weaknesses (internal factors within an organization):\n\nHuman resources — staff, volunteers, board members, target population;\nPhysical resources — your location, building, equipment;\nFinancial — grants, funding agencies, other sources of income;\nActivities and processes — programs you run, systems you employ;\nPast experiences — building blocks for learning and success, your reputation in the community.\n\nOpportunities and threats (external factors from community or societal forces):\n\nFuture trends in your field or the culture;\nThe economy — local, national, or international;\nFunding sources — foundations, donors, legislatures;\nDemographics — changes in the age, race, gender, culture'), Document(page_content='chat?), influences discourse. Discourse analysis is also interested in the genre (topic) of the discourse. \n\nDiscourse analysis is studied not only in linguistics, but also in sociology, anthropology, psychology, communication studies and translation studies.\n\nOther websites \n Daniel L. Everett, statement concerning James Loriot, p.\xa09 \n The Discourse Attributes Analysis Program and Measures of the Referential Process .\n Linguistic Society of America: Discourse Analysis, by Deborah Tannen \n Strategies for analysing a case study\n\nLinguistics'), Document(page_content=""Before World War II, radio and print media were powerful tools for rhetoric. The newspapers and books persuade readers towards a particular point of view. Rhetoric does not depend only on a live audience.\n\nStructure\nAccording to Aristotle, a rhetoric has three elements in persuasion:\nEthos: depends on the personal character of the speaker (must appear good, worthy of trust).\nPathos: puts the audience in a fit state of mind (stirs their emotions).\nLogos: proof, or apparent proof, provided by the words of the speech (the actual argument).\n\nContrastive rhetoric\nA 1988 study was conducted by Söter in Australia among students who were native speakers of Arabic, Vietnamese and English. The sample, 6th and 11th grade students were asked to write a bedtime story for a young child. Patterns were immediately evident in the different approaches used by the student's in the story writing task.\n\nThe Vietnamese stories placed primary focus on characters and the relationships between them""), Document(page_content=""in the story writing task.\n\nThe Vietnamese stories placed primary focus on characters and the relationships between them (manifested in a great proportion of dialog). English stories placed primary focus on the sequential forward movement of the plot. Arabic stories placed primary focus on descriptive elements of the setting.\n\nContrastive rhetorics says that people who share a common language might have different rhetoric styles due the influence of culture and exchanges. The discourse goes beyond the target language's native forms of discourse organization or rhetoric.\n\nA paraphrasing task study was done in the USA among Chinese and Russian students. American students were easily able to paraphrase, but Chinese students found it hard, perhaps due to their academic environment influenced by Confucian traditions. Russian students struggled with the paraphrasing because norm in Russian academic environment was that students are only required to read and describe, and was not required to give a personal interpretation or an opinion.\n\nU.S. rhetorical style: ethnocentric sources describe it as typically direct and relatively"")]"
What are the three main methods to calculate the correlation coefficient and how do they differ?,"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks."," The three main methods to calculate the correlation coefficient are Pearson's correlation coefficient, Spearman's rank correlation coefficient, and the coefficient of correlation.

Pearson's correlation coefficient is a mathematical formula used to calculate correlation coefficients between two datasets. This is the one you'r working on in the context.

Step 1: Find the mean of x, and the mean of y
Step 2: Subtract the mean of x from every x value (call them ""a""), and subtract the mean of y from every y value (call them ""b"")
Step 3: Cal","[Document(page_content='Pearson\'s correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them ""a""), and subtract the mean of y from every y value (call them ""b"")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) × (sum of b2)]\nDeveloped by Karl Pearson in the 1880\'s,\nMathematics'), Document(page_content=""(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information – At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient – How to work it out it quickly\n Learning by Simulations – The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation – More beginner's information by a Hawaii professor\n\nMathematics\nStatistics""), Document(page_content=""have 2. Then, it goes up until it is all ranked. You have to do this to both sets of data.\n\nStep two \nNext, we have to find the difference between the two ranks. Then, you multiply the difference by itself, which is called squaring. The difference is called , and the number you get when you square  is called .\n\nStep three \nCount how much data we have. This data has ranks 1 to 5, so we have 5 pieces of data. This number is called .\n\nStep four \nFinally, use everything we have worked out so far in this formula: .\n\n means that we take the total of all the numbers that were in the column . This is because  means total.\n\nSo,  is  which is 4. The formula says multiply it by 6, which is 24.\n\n is  which is 120.\n\nSo, to find out , we simply do .\n\nTherefore, Spearman's rank correlation coefficient is""), Document(page_content='In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment')]"
What is the purpose of a correlogram?,"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction."," The purpose of a correlogram is to graphically show the correlation between two variables. It is a two-dimensional diagram that gives a computation of the mathematical function of correlation. It is used to measure the relationship between two variables and can be used to find the correlation coefficient. In the context of the reading, a correlogram is used in holography to measure the interference fringes and learn how big the deformation or other problem is. In data storage, a correlogram can be used to analyse digital data and find the relationship between different variables. In mathematics, a correlogram is a graph","[Document(page_content='create a virtual picture of the recorded object, even if that picture can only be seen from a specific angle.\n\nApplications\n\nMeasurement \n\nIndustries use holograms to measure things. In the car industry, cars are measured using holography so engineers can see bulges and vibration characteristics. Phase-shift holography is one kind of holography used to make cars.\n\nThe first step in making a hologram is to examine the ground level state of the object, then overload the object through heat or mechanical pressure. Covering the original hologram and the modified hologram can produce interference fringes. By measuring the interference fringes, engineers learn how big the deformation or other problem is. Engineers can measure tiny terminal expansions or vibrations in mechanical systems. This needs two reference waves.\n\nData storage \n\nThere are holographic storage machines for analog pictures and digital data. Digital information will be affiliated by a two-dimensional bit-pattern.\n\nLight'), Document(page_content=""A nomogram, alignment chart or abaque is a graph for calculation. It is a two-dimensional diagram which gives a computation of a mathematical function. \n\nThe field of nomography was invented in 1884 by the French engineer Philbert Maurice d’Ocagne (18621938). It was used for many years to provide engineers with fast graphical calculations of complicated formulas. Nomograms use a parallel coordinate system invented by d'Ocagne rather than standard Cartesian coordinates.\n\nA nomogram consists of a set of n scales, one for each variable in an equation. Knowing the values of n-1 variables, the value of the unknown variable can be found, or by fixing the values of some variables, the relationship between the unfixed ones can be studied. \n\nThe result is got by laying a straightedge across the known values on the scales and reading the unknown value from where it crosses the scale for that variable. The virtual or drawn line created by the straightedge is called an index line""), Document(page_content=""(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information – At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient – How to work it out it quickly\n Learning by Simulations – The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation – More beginner's information by a Hawaii professor\n\nMathematics\nStatistics""), Document(page_content='instruments) can normally track a source as it moves across the heavens, making the star appear still to the telescope and allowing longer exposures.  Also, images can be taken on different nights so exposures span hours, days or even months. In the digital era, digitised pictures of the sky can be added together by computer, which overlays the images after correcting for movement.\n\nAdaptive optics \nAdaptive optics means changing the shape of the mirror or lens while looking at something, to see it better.\n\nData analysis \nData analysis is the process of getting more information out of an astronomical observation than by simply looking at it. The observation is first stored as data.  This data will then have various techniques used to analyse it.\n\nFourier analysis \nFourier analysis in mathematics can show if an observation (over a length of time) is changing periodically (changes like a wave).  If so, it can extract the frequencies and the type of wave pattern, and find many things including new')]"
What is telemetry?,Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.,"
Telemetry is a technology that allows measurements to be taken from far away, and the data to be transmitted back to an operator for processing. In the context of the protocol spec you'v'e 

1. The measurements are taken by a device, such as a Remote Terminal.
2. The device converts the measurements into a signal, which is transmitted over a bus.
3. The Bus Controller, or the receiving device, can give commands to the Remote Terminal over the bus.

The protocol spec you'v'e 

1. Definitions for messages, words","[Document(page_content='Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology'), Document(page_content=""Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.""), Document(page_content='In the fields of communications, signal processing, and in electrical engineering more generally, a signal is any time-varying quantity. \n\nThe concept is broad, and hard to define precisely.  Definitions specific to subfields are common. For example, in information theory, a signal is a codified message, i.e., the sequence of states in a communications channel that encodes a message. In a communications system, a transmitter encodes a message into a signal, which is carried to a receiver by the communications channel.  For example, the words ""Mary had a little lamb"" might be the message spoken into a telephone. The telephone transmitter converts the sounds into an electrical voltage signal. The signal is transmitted to the receiving telephone by wires; and at the receiver it is reconverted into sounds.\n\nExamples of signals \n Motion.  The motion of a particle through some space can be considered to be a signal, or can be represented by a signal.  The domain of a motion signal is'), Document(page_content='that Remote Terminal responds, that\'s a message. When the Bus Controller talks to all Remote Terminals at the same time, that\'s a different kind of message. There are 10 types of messages in total.\n Messages are broken up into words. There are three types of word. The Command word, used by the Bus Controller at the start of a message; the Status word, used by a Remote Terminal responding to the Bus Controller; The Data word, used by both Bus Controllers and Remote Terminals to send data.\n Each word takes 20 micro seconds to transmit on the bus. The first 3 microseconds are used to tell the device ""What kind of word is this."" The next 16 microseconds are used to send data. The last microsecond is used to send the parity bit, which can indicate if something happened on the bus and the word became garbled.\n\nWhat else do I need to know \nThe protocol spec doesn\'t tell people that use it how to use it, only')]"
What is a common reason for deviation from the normal distribution?,"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution."," The normal distribution is a probability distribution that is often used to model continuous data. It is a symmetric, bell-shaped distribution with a mean and a standard deviation. The normal distribution is important because many values in the statistical and data science fields follow this distribution.

The normal distribution is a model of a random event that is the sum of many identical but random events. The central limit theorem says that if an event is the sum of identical but random events, it will be normally distributed.

The normal distribution is a continuous probability distribution, meaning that it can assume any value between its mean and its mean with an probability.","[Document(page_content=""if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library – Reference Manual – The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions""), Document(page_content='The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (""average"") of the distribution defines its location, and the standard deviation (""variability"") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples'), Document(page_content='higher average is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return).\n\nRules for normally distributed numbers\n\nMost math equations for standard deviation assume that the numbers are normally distributed. This means that the numbers are spread out in a certain way on both sides of the average value. The normal distribution is also called a Gaussian distribution because it was discovered by Carl Friedrich Gauss. It is often called the bell curve because the numbers spread out to make the shape of a bell on a graph. \n\nNumbers are not normally distributed if they are grouped on one side or the other side of the average value. Numbers can be spread out and still be normally distributed. The standard deviation tells how widely the numbers are spread out.\n\nRelationship between the average (mean) and standard deviation \nThe average (mean) and the standard deviation of a set of data are usually written together. Then a person can understand what the average number is and how widely other numbers in the group are spread out.'), Document(page_content=""sample standard deviation  over the normalizing term  (that is, ). In this way, the t-distribution can be used to estimate how likely it is that the true mean lies in any given range.\n\nThe t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero.  The Student's t-distribution is a special case of the generalised hyperbolic distribution.\n\nRelated pages \n\n F-distribution\n\nReferences\n\nProbability distributions"")]"
How can the Shapiro-Wilk test be used in data distribution?,"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed."," The Shapiro-Wilk test is a statistical test used to check the hypothesis that a given set of data follows a normal distribution. It is a test for distribution, and it is named after its two main devi- leol- gant- Shapiro and-Wilk. The test results in a test statistic, and the null hypothesis is that the data follows a normal distribution. The test is used to check the null hypothesis, and if the test statistic is less than the given significance value, the null hypothesis is not be- gared to be true, and the data is said to be","[Document(page_content='Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that ""the experimental data does not differ from untreated data"". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as \'significant\' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests'), Document(page_content='The Kolmogorov–Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests'), Document(page_content='than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically—for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics'), Document(page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or \xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with  degrees of freedom is written as . It is a special case of gamma distribution.\n\nChi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\n\nRelated pages \n\n Chi-squared test\n\nReferences\n\nOther websites\nChi-Square Tutorial by Khans Academy\n\nProbability distributions')]"
Why is the Delphi method chosen over traditional forecasting methods?,The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.,"

In numerical analysis, the Delphi method is used as an optimization method, while traditional forecasting methods are used for predicting future events. Delphi is chosen over traditional forecasting methods in optimization because Delphi is an interpolation method that can obtain approximate solutions while maintaining reasonable bounds on errors. It is based on the Delphi programming language, and is used in fields like engineering, physical sciences, life sciences, and even the arts. Delphi is an important method for solving differential equations, and is used in numerical linear algebra, stochastic differential equations, and Markov chains.

In forecasting, traditional methods","[Document(page_content='Delphi can mean:\n Delphi, Greece, an archaeologic site and modern town in Greece\n Delphi programming language, based on Pascal'), Document(page_content='seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\xa0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in star movement; optimization occurs in portfolio management; numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nComputers greatly helped this task. Before there were computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.\n\nFamous numerical software\nIn order to support numerical analysts, many kinds of numerical software has been created:\n MATLAB - made by'), Document(page_content='of the Melbourne College of Divinity\n Eidiko Tmima Alexiptotiston, Greek special operations unit\n A numerical weather prediction model formally known as the Eta'), Document(page_content='Forecasting is studying and saying what is likely to happen in the future. It is similar to predicting, but usually forecasting is done with scientific methods. Forecasting can be done for many different things, like weather forecasting (predicting the weather) or economy forecasting.  Science cannot know the future for sure, so forecasters try to identify the most probable events, and sometimes they are wrong.\n\nWords')]"
What is the main goal of Sustainability Science and what are the challenges it faces?,"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected."," The main goal of Sustainability Science is to develop interdisciplinary foundations for all science, with a specific aim to meet the needs of the present without compromising the ability of future generations to meet their own needs. The challenges it faces include the nonlinearity and interdisciplinary nature of sustainability, the inability to make accurate long-term forecasts, and the social, economic, and institutional challenges of sustainability.

Notes and References

Sustainability Science
Systems science

References

Bibliography

 
Sustainability Science

Sustain","[Document(page_content='Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that ""meets the needs of the present without compromising the ability of future generations to meet their own needs.""\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist’s World,'), Document(page_content='Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory'), Document(page_content='The Sustainable Development Goals (SDGs) are created by the [United Nations] and promoted as the Global Goals for Sustainable Development. They replaced the [Millennium Development Goals] that expired at the end of 2015. The SDGs run from 2015 to 2030. There are 17 goals and 169 specific targets for those goals.\n\nGoals\n\nIn August of 2015 193 countries agreed to the following 17 goals:\n\n No poverty \n Zero hunger \n Good health and wellbeing\n Quality education \n Gender equality\n Clean water and sanitation\n Affordable and clean energy \n Decent work and economic growth \n Industry, innovation and infrastructure\n Reduce inequality \n Sustainable cities and communities \n Responsible consumption and production\n Climate action \n Life below water\n Life on land\n Peace and justice.  Strong institutions\n Partnerships for the goals\n\nReferences\n\nSustainability\nDevelopment\nUnited Nations'), Document(page_content='throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.\n\nSome authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others:\n\nReferences \n\nSystems theory')]"
Why are critical theory and ethics important in modern science?,"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","

Critics such as Richard Lewontin and Stephen Jay Gould, and the Sociobiology Study Group, were important in the controversy that a new field of sociobiology, as it was then called, became the subject of heated controversy in the mid-1970s. The criticism was driven by political events of the day, particularly the war in Vietnam and the Marxist or semi-Marxist views of some scholars. The main concern of the critics seemed to be the idea that a mechanistic paradigm, as it was then called, might make it easier to accept ideas that were not founded","[Document(page_content='Porto Alegre: Medical Arts.\n 2004. From being to doing: the origins of the biology of cognition, with Bernhard Poerksen.\n 2009. The Origins of humanness in the biology of love, with Gerda Verden-Zoller and Pille Brunnel.\n 2004. From biology to psychology. \n 2009. Sense of humanity.\n\nReferences \n\n1928 births\nBiologists\nSystems scientists\n2021 deaths\nChilean scientists\nChilean writers\nPhilosophers\nPeople from Santiago\nDeaths from pneumonia'), Document(page_content='Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics'), Document(page_content='a short section of 30 pages at the end of the book. Yet because of this section, the (apparently) new field of sociobiology became the subject of heated controversy. The criticism was driven by political events of the day.\n\n""The mid-1970s were years of intense political activity on campuses, much of it initiated by left-wing professors and their students who opposed the war in Vietnam. At Harvard University [Wilson\'s employer] the war... came under fire from a number of scholars of the Marxist or semi-Marxist persuasion... Marxist philosophy is founded on the premise of the perfectability of human institutions through ideological prescription. Therefore, persons with Marxist views were particularly unreceptive to the notion that an evolved \'human nature\' exists"".\n\nCriticism by Richard Lewontin and Stephen Jay Gould, and the Sociobiology Study Group hinted that there was some relationship between these ideas and some of the worst events in history. The main concern of the critics seemed to be the idea that'), Document(page_content='easier to make if people accept a mechanistic paradigm - but it may be harder to say why it does not work, if one believes in these ideas.  For instance, creating diagnostic trees might be easier if one works from experience, not from an idea of how a technology should or must work.\n\nA controversial idea is that mechanistic ideas are just an older idea called scholasticism, with more mathematics.  Both tried to work from what should or must be, instead of what experiment seemed to show.\n\nAnother controversial idea is that scientism, belief in science as if it were a religion or ethical tradition, comes from this paradigm.  Most scientists who are mechanistic do not say they see science as a guide to ethics, but try to keep them separate.\n\nPhysics\nChemistry\nBiology')]"
What is system thinking?,"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts."," System thinking is a way of thinking that is used to analyze or describe any group of things which work together to produce some result. It is a framework that can be used to study systems in nature, society, and science. System thinking was founded by Ludwig von Bertalanffy and others in the 1950s, and it is an interdisciplinary field of science that studies the principles of systems in nature, in society, and in science itself. It is used in a variety of areas, such as engineering, biology, medicine, and social sciences, to develop interdisciplinary foundations for all","[Document(page_content='Systems theory is the study of the nature of systems in nature, society, and science.  More specifically, systems theory is a framework to analyze or describe any group of things which work together to produce some result. This can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. \n\nSystems theory as a technical and general academic area of study. It was founded by Ludwig von Bertalanffy and others in the 1950s.\n\nRelated pages\n Autopoiesis\n Chaos theory\n Fritjof Capra'), Document(page_content='A system  is a group of related things that work together as a whole. These things can be real or imaginary. Systems can be man-made things like a car engine or natural things like a star system. Systems can also be concepts made by people to organize ideas. \n\nA subsystem is a system that is part of some larger system. For example, in computer networking, a disk subsystem is a part of a computer system.\n\nDefinition  \nA system is a group of things that connect and form some kind of coherent whole.\n\nOne of the founders of physiology, Claude Bernard, took a big step when he noticed that the internal systems of the body tend to keep things the same even if they are disturbed. He called the functions which keep system stable as homeostasis. This led towards the ideas of error-correction, feedback and regulation. These are all typical of systems which have some kind of goal and can adjust their behaviour so as to correct errors.\n\nExample \nThe Solar System is an example'), Document(page_content='Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory'), Document(page_content='systems"". This approach makes use of molecular biology ideas and systems science ideas, but it does not refer to the particular way life is organised on Earth. This is because the authors thought it was possible for machines to have those features, and for other types of life to exist.\n\nReferences \n\nBiology\nSystems theory\nPhilosophy of science')]"
What is the main principle of the Feynman Method?,The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.," The main principle of the Feynman Method is that all the lines and vertices have an amplitude, and when you multiply the probability amplitude for the lines and the meeting points, you get a number that tells you the total amplitude for the particles to do what the diagram says they do. If you add up all these probability amplitudes over all the possible meeting points, and over all the starting and ending points with an appropriate weight, you get the total probability of these particles to bounce off one another in any particular direction. The Feynman Method is a diagram-lobic, probability- amplitudes- multiplied, and","[Document(page_content='point, that means that the particle was either created or destroyed, depending on the direction in time that the particle came in from.\n\nAll the lines and vertices have an amplitude. When you multiply the probability amplitude for the lines, the amplitude for the particles to go from wherever they start to wherever they meet, and to the next meeting point, and so on, and also multiply by the amplitude for each meeting point, you get a number that tells you the total amplitude for the particles to do what the diagram says they do. If you add up all these probability amplitudes over all the possible meeting points, and over all the starting and ending points with an appropriate weight, you get the total probability amplitude for a collision in a particle accelerator, which tells you the total probability of these particles to bounce off one another in any particular direction.\n\nFeynman diagrams are named after Richard Feynman, who won the Nobel Prize in Physics. His diagrams are very simple in the case of quantum'), Document(page_content=""Richard Feynman (11 May 1918 – 15 February 1988) was an American physicist of Jewish descent. He was born in Far Rockaway, Queens, New York City. He was part of the Manhattan Project team that made the atomic bomb. Feynman won the Nobel Prize in Physics 1965. He was one of the first people to study quantum physics. Feynman added significantly to a branch of science called quantum electrodynamics and invented the Feynman diagram. He died of liposarcoma in Los Angeles, California.\n\nHis sister Joan Feynman (born 1927) is a retired astrophysicist.\n\nSelected books \nFeynman wrote some best-selling autobiographies, and his lecture notes became popular with physics students and qualified professionals.\n\nFeynman, Richard P. (1985). Ralph Leighton, ed. Surely You're Joking, Mr. Feynman!: adventures of a curious character.""), Document(page_content='A Feynman diagram is a diagram that shows what happens when elementary particles collide.\n\nFeynman diagrams are used in quantum mechanics. A Feynman diagram has lines in different shapes—straight, dotted, and squiggly—which meet up at points called vertices. The vertices are where the lines begin and end. The points in Feynman diagrams where the lines meet represent two or more particles that happen to be at the same point in space at the same time. The lines in a Feynman diagram represent the probability amplitude for a particle to go from one place to another.\n\nIn Feynman diagrams, the particles are allowed to go both forward and backward in time. When a particle is going backward in time, it is called an antiparticle. The meeting points for the lines can also be interpreted forward or backwards in time, so that if a particle disappears into a meeting point, that means that the particle was either created or destroyed, depending on the direction in time that'), Document(page_content=""a process in which a physicist-observer takes part, but rather <u/l>any interaction between classical and quantum objects regardless of any observer</u/l>.\n\nThe idea of indeterminacy\n\nThe uncertainty principle came from Werner Heisenberg's matrix mechanics. Max Planck  already knew that the energy of a unit of light is proportional to the frequency of that unit of light (), and that its amount of energy can be expressed in familiar terms such as the joule by using a proportionality constant. The constant he gave the world is now called the Planck constant and is represented by the letter h.  When matrices are used to express quantum mechanics, frequently two matrices have to be multiplied to get a third matrix that gives the answer the physicist is trying to find. But multiplying a matrix such as P (for momentum) by a matrix such as X (for position) gives a different answer matrix from the one you get when you multiply X by P. The result of multiplying P by X"")]"
What is the difference between fixed and random factors in ANOVA designs?,"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed."," In a fixed factors ANOVA design, the factors are known and can be set at specific values for each re- 

A random variable is a measurable function from a sample space to a set of values. It is used to study probability theory. The sample space is the set of all the elements that can occur in an event, and the event space is the set of all the subsets of the sample space that are of some specific, measurable, and well- defined kind. A random variable is a measurable function from the sample space to a set of values, and it is typically represented by capital roman letters","[Document(page_content='samples from a production lot) based on how well it met its design specifications. In contrast, Statistical Process Control uses statistical tools to observe the performance of the production process in order to predict significant deviations that may later result in rejected product.\n\nTwo kinds of variation occur in all manufacturing processes: both these types of process variation cause subsequent variation in the final product. The first is known as natural or common cause variation and consists of the variation inherent in the process as it is designed. Common cause variation may include variations in temperature, properties of raw materials, strength of an electrical current etc. The second kind of variation is known as special cause variation, or assignable-cause variation, and happens less frequently than the first. With sufficient investigation, a specific cause, such as abnormal raw material or incorrect set-up parameters, can be found for special cause variations.\n\nFor example, a breakfast cereal packaging line may be designed to fill each cereal box with 500\xa0grams of product, but some boxes will have slightly more'), Document(page_content='experimental data"".\n\nReferences\n\n \n Basu D. (1980b). ""The Fisher Randomization Test"", reprinted with a new preface in Statistical Information and Likelihood : A Collection of Critical Essays by Dr. D. Basu ; J.K. Ghosh, editor. Springer 1988.\n \n Salsburg D. (2002) The Lady Tasting Tea: how statistics revolutionized science in the Twentieth Century W.H. Freeman / Owl Book. \n\nExperiments\nStatistics'), Document(page_content='Informatica, 30:3–31, 2006.\n\nFurther reading \n\n Bradley, R.A. and Terry, M.E. (1952). Rank analysis of incomplete block designs, I. the method of paired comparisons. Biometrika, 39, 324–345.\n David, H.A. (1988). The Method of Paired Comparisons. New York: Oxford University Press.\n Luce, R.D. (1959). Individual Choice Behaviours: A Theoretical Analysis. New York: J. Wiley.\n Thurstone, L.L. (1927).  A law of comparative judgement. Psychological Review, 34, 278–286.\n Thurstone, L.L. (1929).  The Measurement of Psychological Value.  In T.V. Smith and W.K. Wright (Eds.), Essays in Philosophy by Seventeen Doctors of Philosophy of the \tUniversity of Chicago.  Chicago: Open Court.\n Thurstone, L.L.'), Document(page_content='A random variable is used in mathematics to study probability theory. It was developed to model the chance of events happening in all kinds of real-life scenarios.\n\nDefinition \nTake two measurable spaces, and name them  and . A measurable space is any pair of sets, called  and , that follow these rules:\n  is not empty;\n The elements of  are subsets of ;\n  and the empty set are both elements of ;\n  is closed under complements and countable unions.\nA random variable, named , is a measurable function from  to . This is written . A random variable is typically represented by capital roman letters such as , ,  and , and can be either discrete (taking on a countable set of values) or continuous (taking on an interval of values).\n\nThe set  is called the sample space. The set  is called the event space.\n\nUse in Probability \nTake a probability measure  on . Take a set  in . Then  is defined to mean .\n\nExamples with Dice')]"
What is the replication crisis and how does it affect modern research?,"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models."," The replication crisis is a crisis in science where the results of scientific experiments are difficult or impossible to replicate. This has grave consequences as reproducibility is an essential part of the scientific method. The crisis has been particularly widely discussed in the field of psychology and medicine. The inability to replicate studies can lead to contradicting results, and researchers may cherry pick studies that support their claim. The replication crisis has been made better by discussions and efforts to re-investigate classic results. However, it is common for studies to contradict others, and human error or chance may lead to unusual results. The","[Document(page_content='no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better'), Document(page_content='quite common in science for some studies to contradict others, for example in cases where different methods are used to measure an outcome, or where human error or chance may lead to unusual results. This means that there is often a study someone can use to support their claim, and they can cherry pick that one study even if many more contradict it.\n\nReferences\n\nLogical fallacies'), Document(page_content='Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance'), Document(page_content='was the same length as the “standard” one, in the first image. \n\nA control trial showed the task was rather easy. There, the participants made almost no mistakes in matching the correct lines. However, in the main part of the experiment, all except for one participant were confederates. This means that the researcher had told them how to behave during the experiment. They were told to give wrong answers to see how the other participant reacts. The other participant, however, believed that everyone else was also a participant like him. \n\nThe confederates and the participants formed groups of 7 to 9 students. Furthermore, they were asked to match the correct lines 12 times (12 trials). Three-fourths of participants gave the same, wrong answer as the confederates in at least one trial. All in all, participants conformed to the others on an average of 36.8% of the time. These results suggest that more than 3 out of 10')]"
What is the purpose and process of the flashlight method in group discussions?,"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards."," The flashlight method is not a well developed or common method in group discussions. It is a theory that has been used to show why people join groups and to organize the external and internal parts of problem recognition. The process of the flashlight method is not well developed and there is not enough research on it to provide a consensus. It is a theory that shows that if the concepts are internal, they can be changed by communication, and that if they are external, then the holdable things that are around the person need to be changed in order for the person's concept of the variables to change. The flashlight method is not a","[Document(page_content='and making choices in their own best interest. \n""I will state that coercive persuasion and thought reform techniques are effectively practiced on naïve, uninformed subjects with disastrous health consequences. I will try to give enough information to indicate my reasons for further inquiries as well as review of applicable legal processes"".\n\nThe following methods have been used in some or all cults studied:\n People are put in physically or emotionally distressing situations;\n Their problems are reduced to one simple explanation, which is repeatedly emphasized;\n They receive what seems to be unconditional love, acceptance, and attention from a charismatic leader or group;\n They get a new identity based on the group;\n They are subject to entrapment (isolation from friends, relatives and the mainstream culture) and their access to information is severely controlled.\n\nThis view is disputed by some. Society for the Scientific Study of Religion stated in 1990 that there was not sufficient research for a consensus, and that ""one should not automatically equate the techniques involved in the process'), Document(page_content=""well developed, it develops even more common. The theory has been used to show why people join groups who really want things to happen (activist groups), to organize the external (real, actual) and internal (perceived) parts (dimensions) of problem recognition, and to learn whether information that is used in information processing (processed information) can create publics. Also, some more research was done on the internal and external dimensions of problem recognition, constraint recognition, and level of involvement (Grunig & Hon, 1988; Grunig, 1997). The research is about whether the ideas are internal or external. The research shows that if the concepts are internal, they can be changed by communication, and that if they are external, then the holdable things that are around the person need to be changed in order for the person's concept of the variables to change (Grunig, 1997, p.\xa025). But, there is a small amount""), Document(page_content='A flashlight (in North American English) or torch (in most Commonwealth countries) is a small, portable spotlight.\nIts function is a beam of light which helps to see. It usually requires batteries.\n\nThe light is made by a small light bulb.  In the 20th century, it was usually an incandescent bulb.  Nowadays most use LEDs, which work slightly differently.  In 1896, the first dry cell battery was invented. Unlike previous batteries, it used a paste electrolyte instead of a liquid. This was the first battery suitable for portable electrical devices, as it did not spill or break easily and worked in any orientation.  Sometimes the electricity comes from a small generator instead of a battery.\n\nThe torch can also refer to a flammable stick that is lit on fire to provide light and (or) heat.\n\nOrigin \n\nHundreds of years ago, people used candles for all indoor light. This was expensive, and once the candle was gone, you would have to'), Document(page_content='Tuckman\'s stages of group development is a model of group development made by Bruce Tuckman in 1965. It has four phases: Forming, Storming, Norming and Performing. \n\nForming is when the members of a team just got together.\nStorming is when they are in conflict and are not agreeing with each other.\nNorming is when they have finished deciding what is ""normal"".\nPerforming is when they are working together efficiently.\n\nTuckman believed that these stages are all necessary and always happen in order for a team:\nto grow\nto face challenges\nto tackle problems\nto find solutions\nto plan work\nto deliver results\n\nThis model has become the basis for later models.\n\nMore reading on this topic can be found here: \n\nPsychology\nPerforming this is a stage of a fully functional group where members see themselves as a group and get involved in the task.')]"
What types of data can Generalized Linear Models handle and calculate?,"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution."," Generalized Linear Models (or ""linear models with a linked function"" or ""linear models with a probability function"" or ""linear models with a probability function and a linear predictor function"" or ""linear models with a probability function and a linear predictor and a linear predictor function that is not linear in the predictor variables"" or ""linear models with a probability function and a linear predictor and a probability function that is not a logistic function"" or ""linear models with a probability function and a linear predictor and a probability function that is not a logistic function or a logistic function with a linear predictor","[Document(page_content='predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called ""fitting the line to the data."" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.'), Document(page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".\n\nStatistics\nFunctions and mappings'), Document(page_content='in ridge regression. The least squares approach can also be used to fit models that are not linear. As outlined above, the terms ""least squares"" and ""linear model"" are closely linked, but they are not synonyms.\n\nUsage\n\nEconomics\n\nLinear regression is the main analytical tool in economics. For example, it is used to guess consumption spending, fixed investment spending, inventory investment, purchases of a country\'s exports, spending on imports, the demand to hold liquid assets, labor demand and labor supply.\n\nRelated pages \n\n Curve fitting\n Logistic regression\n Ordinary least squares\n\nReferences\n\nStatistics'), Document(page_content=""which uses multiple values for the variable y. This form of logistic regression is known as multinomial logistic regression.\n\nLogistic regression uses the logistic function to find a model that fits with the data points. The function gives an 'S' shaped curve to model the data. The curve is restricted between 0 and 1, so it is easy to apply when y is binary. Logistic regression can then model events better than linear regression, as it shows the probability for y being 1 for a given x value. Logistic regression is used in statistics and machine learning to predict values of an input from previous test data.\n\nBasics \nLogistic regression is an alternative method to use other than the simpler linear regression. Linear regression tries to predict the data by finding a linear – straight line – equation to model or predict future data points. Logistic regression does not look at the relationship between the two variables as a straight line. Instead, Logistic regression uses the natural logarithm function to find the relationship between the variables"")]"
What is a heatmap and why is it useful?,A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.," A heatmap is a visual tool that uses colors to show the temperature range in a given area. It is useful for quickly understanding temperature patterns and can help in weather forecasting, geographic analysis, and other areas where temperature data is important. The curvy isotherm lines on a heatmap make it more useful and accurate in showing temperature differences and patterns, and the color scales help to differentize the temperatures. In mapmaking, heatmap colors can be used to show different features, and the colors can affect the understanding or feel of the map. In meteorology, weather maps are used to show temperature differences and weather","[Document(page_content='Isotherms are lines drawn around places with the same temperature range on isotherm maps. Each point on this line shows one temperature reading, or the average of many temperature readings. Isotherm maps also have scales that tell the signals or colors for the different temperatures. Isotherm lines are usually curvy and not straight lines.\n\nMeteorology\nThermodynamics\n\nen:Contour line#Temperature and related subjects'), Document(page_content='Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more'), Document(page_content='A weather map is a tool. It shows facts about the weather quickly.  Weather maps have been used from the mid-19th century, for study and for weather forecasting. Some maps show differences of temperature, and weather fronts. \n\nA station model is a symbolic picture showing the weather at a reporting station. Meteorologists made the station model to put down many weather elements in a small space on weather maps. Maps thickly filled with station-model plots can be hard to read. However, they help meteorologists, pilots, and mariners to see important weather patterns. A computer draws a station model for every place of observation. The station model is mostly used for surface-weather maps. It can also be used to show the weather in the sky, though. A complete station-model map lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\nHistory \n\nPeople first began using weather charts in a modern way in the mid-19th century. They began using'), Document(page_content=""is the best known example of using thematic maps for analysis of data. His method anticipates the principles of a geographic information system (GIS). He started with an accurate map of a London neighborhood which included streets and water pump locations. Onto this Snow placed a dot for each cholera death. The pattern centered around one particular pump on Broad Street. At Snow’s request, the handle of the pump was removed, and new cholera cases ceased almost at once. Further investigation of the area revealed the Broad Street pump was near a cesspit under the home of the outbreak's first cholera victim.\n\nReferences \n\nCartography"")]"
How did Alhazen contribute to the development of scientific methods?,"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method."," Alhazen is known as the ""father of modern optics"" for his contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, and Muslim theology. He is also known for his work on the scientific method, as he was an early pioneer of this way of in scientific in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in","[Document(page_content='Alhazen<ref> (Arabic: أبو علي الحسن بن الحسن بن الهيثم, Latinized: Alhacen or Ibn al-Haytham)</ref> or Alhacen or ibn al-Haytham (965–1039) was a pioneer of modern optics. Some have also described him as a ""pioneer of the modern scientific method"" and ""first scientist"", but others think this overstates his contribution. Alhazen\'s Risala fi’l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim'), Document(page_content='to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: البصري), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites'), Document(page_content='and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better known.\n\nHistorical aspects \nElements of scientific method were worked out by some early students of nature.\n ""We consider it a good principle to explain the phenomena by the simplest hypothesis possible."" Ptolemy (85165\xa0AD). This is an early example of what we call Occam\'s razor.\n Ibn al-Haytham (Alhazen) (965–1039), Robert Grosseteste (1175–1253) and Roger Bacon (1214–1294), all made some progress in developing scientific method.\n Scientists in the 17th century started agreeing that the experimental method is the main way to find the truth. This was done in western Europe by men like Galileo, Kepler, Hooke, Boyle, Halley and Newton. At the same time, the microscope and the telescope were invented (in Holland), and the Royal Society was formed. Instruments,'), Document(page_content='The Book of Optics is a book that was written by Ibn al Haytham (965–1040 CE), an Arabic scientist and polymath. It was a seven-volume treatise on optics, physics, mathematics, anatomy and psychology, from 1011 to 1021. It was originally written in Arabic and was later translated into Persian, Latin and Italian within the next several centuries. The book had an important influence on the development of optics and on science in general because it introduced the experimental scientific method. Ibn al-Haytham has been called the ""father of modern optics"". Ibn al-Haytham wrote more than 200 works on a wide range of subjects, of which at least 96 of his scientific works are known.\n\nthe book of optics is also written en part by Ibn al-Haythams nephew at four years old\n\nScience books')]"
How can multivariate data be graphically represented?,"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data."," One way to graphically display multivariate data is by using a 3D graph, which can display more variables in a single graph. However, it can be more complex to create and visual. Vector graphics, which use lines, shapes, and text, can also be used to display multivariate data, particularly in the form of a 3D graph or a 3D chart. These types of graphics can be scaled indefinitely without loss of quality, making it useful for display data with a multivariate relationship.

Related pages
2D graphics
3D graphics","[Document(page_content='Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics'), Document(page_content='A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or'), Document(page_content='techniques, which display a relationship between two variables that take either discrete or a continuous ranges of values; examples:\n\nSchematics and other types of diagrams, e.g.,\n\nReferences \n\n \nNon-verbal communication'), Document(page_content='Computer graphics are visual representations of data, made with the help of computer. Computer graphics can be a series of images (video or animation) or a single image.\n\nComputer graphics are very useful. Computer-generated imagery is used for movie making, video games, computer program development, photo editing, scientific modeling, design for advertising and more. Some people see computer graphics as art.\n\nAreas of computer graphics \nComputer graphics can be 2D or 3D. They are made differently and used differently. People can use computer programs to make different types of graphics.\n\n2D graphics \n2D computer graphics are usually split into two categories: vector graphics and raster graphics.\n\nVector graphics \n\nVector graphics use lines, shapes and text to create a more complex image. If a vector graphic image is made very big on the monitor, it will still look as good (smooth) as its regular size. This is one of the reasons vector graphics are liked so much. Vector images also take very little computer memory when')]"
What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?,"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions."," Machine learning has the advantage of being more data-independent and generalizing well to data that is more complicated or of a more variable form. It can also learn from data where designing and programming explicit algorithms is difficult or in the context of supervised learning, can learn to predict the outcome of a dependent variable based on a set of explanatory variables. In machine learning, the system learns the function from the data, as the function is often more complicated and multidimensional, and the system can learn to make predictions or decisions based on this function. This is in difference to traditional rules or functions which are explicitly programmed and","[Document(page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning'), Document(page_content='fields. A simple example is attempting to find the smallest possible difference in the distance of two objects in two-dimensional space (x and y). In this context, the derivative of the function that gives the difference is taken in order to find the minimum. A more complicated example is in Machine Learning, in which the optimization function attempts to find the global minimum of the loss function in order to minimize the difference or loss between the algorithm’s predictions and the actual values. This example is more difficult as Machine learning algorithms often utilize multidimensional data usually in the form of tensors yielding more complicated functions.\n\nRelated software\nToday, there are many tools to support optimization studies:\n MATLAB\n Wolfram Mathematica\n\nReferences\n\nScience\nMathematics'), Document(page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence'), Document(page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".\n\nStatistics\nFunctions and mappings')]"
What are some of the challenges faced by machine learning techniques?,"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources."," Machine learning techniques can be black box, which can make it hard to understand how the model is working and make it hard to make decisions based on the model. Some machine learning techniques have been criticized for biases in hiring, criminal justice, and recognizing faces. Machine learning techniques may not be the efficient optimization algorithms for specific problems and problem instances. Alternative and complementary optimization algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence. The suitability of machine learning techniques is dependent on the amount of knowledge of the problem.

In machine learning, super","[Document(page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning'), Document(page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence'), Document(page_content='to make something that can say how the different data attributes are connected/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science'), Document(page_content='For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence (e.g.: ant colony optimization, particle swarm optimization) and methods based on integer linear programming. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\nHistory \nIn 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.  His 1954 publication was not widely noticed. Starting in 1957,  the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple')]"
What are the characteristics of scientific methods?,"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications."," The scientific method is a systematic and empirical way of investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. It is based on gathering observable, empirical and measurable evidence, and the formulation and testing of hypotheses. The scientific method is characterized by the following features:
1. A problem or question is formulate
ed.

1. The problem is investiga

ted through observation and experimentation.

1. A theory or hypotheses is formulate
ed to explan

1. The theory or hypotheses is teste

1","[Document(page_content='Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: ""a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses"".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as \'the criterion\'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it'), Document(page_content='primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the'), Document(page_content='methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a question or a problem about nature. Some problems are simple, such as ""how many legs do flies have?"" and some are very deep, such as ""why do objects fall to the ground?""\n Next, scientists investigate the problem. They work at it, and collect facts. Sometimes all it takes is to look carefully.\n Some questions cannot be answered directly. Then scientists suggest ideas, and test them out. They do experiments and collect data.\n Eventually, they find what they think is a good answer to the problem. Then they tell people about it.\n Later, other scientists may agree or not agree. They may suggest another answer. They may do more experiments. Anything in science might be revised if we find out the previous solution was not good enough.\n\nAn example \nA famous example of science in action was the expedition led by Arthur Eddington to Principe Island in Africa in'), Document(page_content='refers to a way of pursuing knowledge, not just the knowledge itself. It is mainly about the phenomena of the material world. The Greek works into Western Europe from the 6th to 7th century B.C. revived ""Philosophy"". In the 17th and 18th centuries scientists increasingly sought to formulate knowledge in terms of laws of nature such as Newton\'s laws of motion. And during the 19th century, the word ""science"" became more and more associated with the scientific method itself. It was seen as a way to study the natural world, including physics, chemistry, geology and biology.\n\nIt was also in the 19th century that the term scientist was created by William Whewell. He meant it tell the difference between those who looked for knowledge on nature from those who looked for other types of knowledge.\n\nThe scientific method is the name given to the methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a')]"
What is the main goal of practicing mindfulness?,"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions."," The main goal of practicing mindfulness is to calm the mind and become better att attainable and within one's skills and abilities. This can end pain and stress, and also has health benefits such as lowers stress, anxiety and depression. The practice can focus on various things such as breath, metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds. In Buddhism, this type of seeing can end pain. In Christianity, meditation is an expression of prayer and engages thought, imagination, emotion, and desire. In both religions, the practice","[Document(page_content='Meditation tries to get past the ""thinking"" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end'), Document(page_content=""Mihály 1996. Finding flow: the psychology of engagement with everyday life. Basic Books.  [a popular exposition emphasizing technique]</ref>\n\n Clear goals. Expectations and rules are known and goals are attainable and within one's skills and abilities.  Moreover, the challenge level and skill level should both be high.\n Concentrating: a high degree of concentration on a limited field of attention (a person engaged in the activity will have the opportunity to focus and to delve deeply into it).\n A loss of the feeling of self-consciousness.\n Distorted sense of time, one's sense of time is altered.\n Direct and immediate feedback (successes and failures in the course of the activity are apparent, so that behavior can be adjusted as needed).\n A balance between ability level and challenge: the activity is neither too easy nor too difficult.\n A sense of control over the situation or activity.\n The activity is intrinsically rewarding, so there is an effortlessness of action.\n A lack""), Document(page_content='the mind can better see the cause of pain. Buddhists believe that this type of seeing can end pain.\n\nBuddhist meditation is not just used for spiritual reasons. Research shows that Buddhist meditation lowers stress, anxiety and depression.\n\nMost types of Buddhist meditation focus on something. The most popular things to focus on include breath,  metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds.\n\nChristian meditation \nChristians sometimes meditate by thinking about small parts of the Bible, or by saying the words of a prayer to themselves over and over. Meditation is an expression of Christian prayer. In the Catechism of the Catholic Church is specified that by means of meditation ""The mind seeks to understand the why and how of the Christian life, in order to adhere and respond to what the Lord is asking""; also it is pointed out that ""meditation engages thought, imagination, emotion, and desire. This mobilization of faculties is necessary in'), Document(page_content='your hearts be troubled and do not be afraid."" ()\n\nInner peace \n\nInner peace (or peace of mind) refers to a state of being mentally and spiritually at peace, with enough knowledge and understanding to keep oneself strong in the face of stress. Being ""at peace"" is considered by many to be healthy and the opposite of being stressed or anxious. Peace of mind is generally associated with bliss and happiness.\n\nPeace of mind, serenity, and calmness are descriptions of a disposition free from the effects of stress. In some cultures, inner peace is considered a state of consciousness or enlightenment that may be cultivated by various forms of training, such as prayer, meditation, Tai chi chuan or yoga, for example. Many spiritual practices refer to this peace as an experience of knowing oneself.\n\nMovements and activism\n\nPeace movement \n\nA movement that seeks to get ideals such as the ending of a particular war, minimize inter-human violence in a particular place or type of situation, often linked to the goal of')]"
How is information arranged in a Mindmap?,"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","

A Mindmap is a diagram used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the Mindmap, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices. The relationships between the ideas are abstract and can be is-a, sitting-on, or other relationships. The diagram can be used to show connections between ideas, and can be subdiveded into smaller areas to","[Document(page_content='A mind map is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the mind map, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.\n\nOther websites \n\nMindmapping for offices, Website \n\nDiagrams\nLearning\n\nfi:Käsitekartta'), Document(page_content='details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund SITTING and the nouns agent and location express the diagram\'s basic relationship; ""agent is SITTING on location""; Elsie is an instance of CAT.\n\nAlthough the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter\'s illustration of that ambiguity, with a progression from abstract to concrete in Gödel, Escher, Bach (1979):\n(1) a publication\n(2) a newspaper\n(3) The San Francisco'), Document(page_content='style=""font-size:87%; margin:7px 0 0 6px;""  cellspacing=0 cellpadding=0\n|\xa0 \n|\xa0 \n|-\n|\xa0 England map\n|\xa0\xa0\n|\xa0 Italy map\n|}\n\nAtlas\xa0pages\xa0are map groupings\nThe Wikiatlas is based on some simple ideas. All maps are grouped, in the manner of a World atlas, into pages stored with the name ""Atlas of..."" for each continent, such as:\n\nSimilar page titles ""Atlas of..."" cover each nation or region, such as:\n\nSome of the U.S. states also have Wikiatlas pages, such as:\n\nSimilarly, hundreds of other Wikiatlas pages are named as ""Atlas of..."" for over 200 nations and over 94 other regions of the World.\n\nWikiatlas page format\nMany pages in the Wikiatlas are arranged in a distinctive format: with maps displayed down the left-side of the page, and explanatory text presented'), Document(page_content='Each of these smaller areas (room, state, number) is next to other small areas (other rooms/states/numbers). The places where the areas meet are connections. If we write down on paper a list of spaces, and the connections between them, we have written down a description of a space -- a topological space. All topological spaces have the same properties such as connections, and are made of the same structure (a list of smaller areas). This makes it easier to study how spaces behave. It also makes it easier to write algorithms. For instance, to program a robot to navigate a house, we simply give it a list of rooms, the connections between each room (doors), and an algorithm that can work out which rooms to go through to reach any other room. For more examples of this type of problem, look at Graph theory.\n\nWe can go further by creating subdivisions of subdivisions of space. For instance, a nation divided into states, divided into counties, divided into')]"
Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?,Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.," The concept of degrees of freedom was first noted in the early 1800s by Carl Friedrich Gauss. The modern usage and understanding of the term were expounded upon first by William Sealy Gosset in his article ""The Probable Error of a Mean,"" published in Biometrika in 1908. Gosset did not specifically use the term ""degrees of freedom"" in his article, however he did give an explanation for the concept throughout the course of developing what would eventually be known as Student’s T-distribution. Mixed Effect Models are a more modern statistical method","[Document(page_content='he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum'), Document(page_content=""law \n\nWeinberg developed the principle of genetic equilibrium independently of British mathematician G.H. Hardy. He delivered an exposition of his ideas in a lecture on 13 January 1908, about six months before Hardy's paper was published in English. His lecture was printed later that year in the society's yearbook. \n\nWeinberg's contributions were unrecognized in the English speaking world for more than 35 years. Curt Stern, a German geneticist who emigrated to the United States before World War II, pointed out in a brief paper in Science that Weinberg's exposition was both earlier and more comprehensive than Hardy's.\n\nAscertainment bias \nWeinberg pioneered studies of twins, and developed techniques to analyse phenotypic variation. His aim was to partition this variance into genetic and environmental components. In the process, he recognized that ascertainment bias was affecting many of his calculations, and he produced methods to correct for it. \n\nWeinberg observed that proportions of homozygotes""), Document(page_content='and the length of the month are also ancient.\n\nThe Round-to-even method has served as the ASTM (E-29) standard since 1940. The origin of the terms unbiased rounding and statistician\'s rounding are fairly self-explanatory. In the 1906 4th edition of Probability and Theory of Errors  Robert Simpson Woodward called this ""the computer\'s rule"" indicating that it was then in common use by human computers who calculated mathematical tables. Churchill Eisenhart\'s 1947 paper ""Effects of Rounding or Grouping Data"" (in Selected Techniques of Statistical Analysis, McGrawHill, 1947, Eisenhart, Hastay, and Wallis, editors) indicated that the practice was already ""well established"" in data analysis.\n\nThe origin of the term ""bankers\' rounding"" remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission'), Document(page_content='and samples within the experiment. For example, when considering students and course choice, a sample size of 30 or 40 students is likely not large enough to generate significant data. Getting the same or similar results from a study using a sample size of 400 or 500 students is more valid.\n\nHistory \nThe earliest and most basic concept of degrees of freedom was noted in the early 1800s, intertwined in the works of mathematician and astronomer Carl Friedrich Gauss. The modern usage and understanding of the term were expounded upon first by William Sealy Gosset, an English statistician, in his article ""The Probable Error of a Mean,"" published in Biometrika in 1908 under a pen name to preserve his anonymity.1\n\nIn his writings, Gosset did not specifically use the term ""degrees of freedom."" He did, however, give an explanation for the concept throughout the course of developing what would eventually be known as Student’s T-distribution. The actual')]"
How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?,Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.,"

In statistics, both Analysis of Variance ( Analysis of Variance, or "" ana-"" for "" analysis of"" and ""- of-"" for "" of Variance"" or "" an-"" for "" analysis"" and "" of-"" for "" of"" and "" Variance"" or "" an-"" for "" analysis"" and "" of"" and "" Variance"" or "" an-"" for "" analysis"" and "" of"" and "" Variance"" models and regression models are used to describe the relationship between a dependent ( or "" dependent"" or "" dependent"" or "" dependent"" or "" dependent","[Document(page_content='for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size—if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether'), Document(page_content='This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are'), Document(page_content='Regression analysis is a field of statistics. It is a tool to show the relationship between the inputs and the outputs of a system. There are different ways to do this. Better curve fitting usually needs more complex calculations. \n\nData modeling can be used without knowing about the underlying processes that have generated the data; in this case the model is an empirical model. Moreover, in modelling, knowledge of the probability distribution of the errors is not required. Regression analysis requires assumptions to be made regarding probability distribution of the errors. Statistical tests are made on the basis of these assumptions. In regression analysis the term ""model"" embraces both the function used to model the data and the assumptions concerning probability distributions.\n\nRegression can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. These uses of regression rely heavily on the underlying assumptions being satisfied. Regression analysis has been criticized as being misused for these purposes in many cases where the appropriate assumptions cannot be verified to hold.'), Document(page_content='reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science')]"
Why should stepwise procedures in model reduction be avoided?,"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","

In model reduction, stepwise procedures can be avoided because they can often make the reduction process more inaccurate or inaccurate more often. This is because in a stepwise reduction process, the model is first partitioning into a set of pieces, and then one or more pieces are then manipulating at a given step. This can make it more possible for numerical or algebraic error to be inaccurate in the reduction process, as the manipulating of one or more pieces can make the error in the reduction process more ""cry"" and more ""c"" in the reduction process.

Five-point","[Document(page_content=""same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2""), Document(page_content=""alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order""), Document(page_content='geometric modeller"". Proc. Uncertainty in Geometric Computations,  1–14. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), ""Approximating parametric curves with strip trees using affine arithmetic"". Computer Graphics Forum, 22  2,  171–179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), ""Floating-point error analysis based on affine arithmetic"". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), ""Robust visualization of strange attractors using affine arithmetic"". Computers & Graphics, 30  6,  1020– 1026.\n\nSurveys\nL. H. de Figueiredo and J.'), Document(page_content='Methods\nMany PDEs have appeared from physics. So we can think about difference methods preserving physical properties. These difference methods are known as structure preserving numerical methods. The following list is the examples of them:\n Symplectic integrators\n Discrete gradient method\n Discrete variational derivative method (DVDM)\nSome experts are studying their relation between numerical linear algebra.\n\nOthers\nThe difference mthods in above have high accuracy, but their usage is limited because they depend on the behaviour of the given PDEs. This is why new types of FDM are still studied. For example, the following methods are studied:\n Shortley-Weller approximation\n Swarztrauber-Sweet approximation\n Ascher-Mattheij-Russell difference formula\n\nValidated Numerics for PDEs\n\nNot only approximate solvers, but the study to ""verify the existence of solution by computers"" is also active. This study is needed because numerically obtained solutions could be phantom solutions (fake')]"
What are the methods to identify redundancies in data for model reduction?,"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables."," In data model reduction, redundancies can be identify using methods such as:

1. P-redundancy: If a term is a redundant representation of another term, it is called P-redundant. This can be identify using methods like P-redundancy check or P-redundancy matrices.

For example, in a model with two terms, 'A' and 'A squared' (A squared is a redundant representation of 'A' squared), 'A squared' can be identify as P-redundant and can be reduce to 'A","[Document(page_content='In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science'), Document(page_content='reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science'), Document(page_content=""alignment\n Reduction (town), a form of Catholic mission in South America in the 17th and 18th centuries\n Purchasing reduction, in economics and in waste management, is the process of decreasing the purchase of consumer goods\n Reduction (Sweden), in 1680 a return of lands to the Crown earlier granted to the nobility.\n Waste reduction is the first and most desirable component of the waste hierarchy (reduce, reuse, recycle)\n\nIn mathematics and computer science''':\n Reduction (mathematics), the process of manipulating a series of equations or matrices into a desired 'simpler' format\n Reduction property, in descriptive set theory, a pointclass allows partitioning the union of two sets in the pointclass into two disjoint sets in the same pointclass\n Reduction (complexity), in computational complexity theory, the transformation of an instance of one problem into an instance of another\n Reduce computer algebra system, a general-purpose computer algebra system geared towards applications in physics.\n Reduce (higher-order""), Document(page_content='A. (1996). Numerical methods for least squares problems (Vol. 51). SIAM.\n\nMathematical approximation\nStatistics')]"
How are 'narratives' used in Narrative Research?,"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve."," Narratives are used in Narrative Research as a way to understand and make inferences from data. Narratives can be found through qualitative interviews, case study research, and auto-ethnography. Narratives can be used to engage the audience morally, aesthetically, and intellectually, and co-participate in the events described. It is also used to understand the shift from a depersonalized narrative to a storyline with which the audience can engage. Narratives can also be used to avoid pitfalls in auto-ethnography, as Chang identifies, which could","[Document(page_content='A narrative is a literary term for the events that make up a story. It is the way the events connect and make up the plot of a story or book.\n\nA narrative reports connected events, presented to the person reading or listener in a sequence of written or spoken words.\n\nA common term for narrative is plot. The study on structure in narratives is called narratology.\n\nRelated pages\nNarrative poetry\n\nReferences\n\nFiction\nWriting\nNarratology'), Document(page_content='Narratology is the study of structure in narratives. The theory of narrative or narratology was developed in the 1960s. Narratology is based on the idea of a common literary language. Narratives are found and told through oral and written language. Narratology has helped to make it easier to understand the how and why of narrative.\n\nReferences\n\nOther websites\nhttp://wikis.sub.uni-hamburg.de/lhn/index.php/Main_Page  - The Living Handbook of Narratology\nhttp://www.units.miamioh.edu/technologyandhumanities/narratology.htm  - Notes on Narratology\nhttp://www.nou-la.org/ling/1975a-theonarreme.pdf - THBORIE DES NARRGMES ET ALGORITHMES NARRATIFS\n\nNarratology\nWriting'), Document(page_content='their day-to-day behavior.\n\n Qualitative Interviews\n\nQualitative interviews are close-ended questions that are asked directly to the research subjects. The qualitative interviews could be either informal and conversational, semi-structured, standardized and open-ended or a mix of all the above three. This provides a wealth of data to the researcher that they can sort through. This also helps collect relational data. This method of field research can use a mix of one-on-one interviews, focus groups and text analysis.\n\n Case Study\n\nA case study research is an in-depth analysis of a person, situation or event. This method may look difficult to operate, however, it is one of the simplest ways of conducting research as it involves a deep dive and thorough understanding the data collection methods and inferring the data.\n\nReferences\n\nBiology\nResearch methods'), Document(page_content='society is the shift from a depersonalized, “seen from a vantage point” and neutrally distanced narrative to a storyline with which the audience can engage morally, aesthetically, and intellectually, and, thus, co-participate in the events described. Especially for ethnographers who use multiple informants, auto-ethnography introduces an alternative way of writing where “the distinction between ethnographer and ‘others’ is unclear”, thereby challenging “imposed identities and boundaries”.\n\nHowever, auto-ethnographers should avoid some dangerous pitfalls that could undermine the credibility and usefulness of their work. Chang identifies the following: ""(1) excessive focus on self in isolation from others; (2) overemphasis on narration rather than analysis and cultural interpretation; (3) exclusive reliance on personal memory and recalling as a data source; (4) negligence of ethical standards regarding others in self-narratives; and (5) inappropriate application of the label')]"
What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?,"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","

A Generalized Additive model (GAM) is a statistical model that uses a linear predictor function with the coefficients modeled as a function of explanatory variables using a process that can be modeled as a random function. In other models, such as linear regression, the coefficients are modeled as deterministic values.

GAM is a generative model, as it uses randomness to model the coefficients.

An advantage of using a Generalized Additive model is that it can model the coefficients as a function of explanatory variables, while in other models, such as linear regression, the coefficients","[Document(page_content='A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics'), Document(page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as ""weights"".\n\nStatistics\nFunctions and mappings'), Document(page_content='Gams is a municipality in Werdenberg in the canton of St. Gallen in Switzerland.\n\nReferences\n\nOther websites \n\n Official website \n \n\nMunicipalities of St. Gallen'), Document(page_content='has the disadvantage that the same list of values does not have a well-defined, deterministic median.\n\nMedian and mean \nMedian and mean are different in several ways. Mean is a better measure in many cases, because many of the statistical tests can use mean and standard deviation of two observations to compare them, while the same comparison cannot be performed using the medians.\n\nMedian is more useful when the variance of the values is not important, and we only need a central measure of the values. If the maximum value of a set of numbers changes while the other numbers of this set are kept the same, the mean of this set of numbers changes, but the median does not.\n\nAnother advantage of median is that it can be calculated sooner when we are studying survival data. For example, a researcher can calculate the median survival of patients with a kidney transplant, when half the patients participated in his study die. Calculating the mean survival requires continuing the study, and following all the patients until their death.\n\nExample')]"
What are the three conditions under which Poisson Distribution can be used?,"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known."," The Poisson distribution can be used to describe the number of occur-

The Poisson distribution is a probability distribution that can be used to describe the number of occur-

The Poisson distribution is a probability distribution that measures the probability that a certain number of events occur within a certain period of time. It is named after Siméon Denis Poisson. The three conditions under which the Poisson distribution can be used are:

The events must be unrelated to each other.
The events must occur with a known average rate, represented by the symbol  (lambda).
The events must be independent of each","[Document(page_content='In probability and statistics, Poisson distribution is a probability distribution. It is named after Siméon Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a'), Document(page_content='The calculation of the last three quantities is explained in the respective Wiki pages. Then, with the help of formulas given in the previous section, the factors μ and β can be calculated. In this way, the CDF of the Gumbel distribution belonging to the data can be determined and the probability of interesting data values can be found.\n\nApplication \n\nIn hydrology, the Gumbel distribution is used to analyze such variables as monthly and annual maximum values of daily rainfall and river discharge volumes, and also to describe droughts.\n \nThe blue picture illustrates an example of fitting the Gumbel distribution to ranked maximum one-day October rainfalls showing also the 90% confidence belt based on the binomial distribution.\n\nReferences \n\nProbability distributions'), Document(page_content='heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson distribution\n\nReferences \n\nProbability distributions'), Document(page_content='of all people have green eyes). (n=500, p=0.05)\nIn order to use the binomial distribution, the following must be true about the problem:\n The outcomes are mutually exclusive, that is, there are two possible outcomes which cannot occur simultaneously (for example. in flipping a coin, there are two possible outcomes: heads or tails. It is always one or the other, never both or a mix of outcomes).\n The probability of a success (p) is consistent throughout the problem (for example, a basketball player makes 85% of his free throws. Each time the player attempts a free throw, 85% is assumed to be the likelihood of a made shot).\n The trials are independent of each other (for example, on the second flip of a coin, the first outcome does not impact the chance of the next toss: the chance of tossing a heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson')]"
How does the Pomodoro technique work?,"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again."," The Pomodoro technique is a time-tro-tro method that involves using a technique to work for 25 minutes, then 5 minutes of relaxation, and then repeat the cycle. The technique is designed to be used for work that is not stressful or ted-tro, and it can be used to focus on a project or to work on a project for a set time each

Helpful Answer: The Skumin mind control method is a relaxation technique that involves daily practice of sessions that last around 15 minutes, usually in the morning, at lunch time, and in the evening. The","[Document(page_content='between sessions. A goal of EMDR therapy is to produce rapid and effective change while the client maintains equilibrium during and between sessions.\n\nPhases 3-6:\xa0 In phases three to six, a target is identified and processed using EMDR therapy procedures.\xa0 These involve the client identifying three things:\n\n1.\xa0 The vivid visual image related to the memory\n\n2.\xa0 A negative belief about self\n\n3.\xa0 Related emotions and body sensations.\n\nIn addition, the client identifies a positive belief.\xa0 The therapist helps the client rate the positive belief as well as the intensity of the negative emotions.\xa0 After this, the client is instructed to focus on the image, negative thought, and body sensations while simultaneously engaging in EMDR processing using sets of bilateral stimulation.\xa0 These sets may include eye movements, taps, or tones.\xa0 The type and length of these sets is different for each client.\xa0 At this point, the EMDR client is instructed to just notice whatever'), Document(page_content='system. The technique involves the daily practice of sessions that last around 15 minutes, usually in the morning, at lunch time, and in the evening. During each session, the practitioner will repeat a set of visualisations that induce a state of relaxation. Each session can be practiced in a position chosen amongst a set of recommended postures.\n\nThe technique of the Skumin mind control method (Russian: ""Психотре́нинг по Ску́мину"") involves the use of two standard postures: sitting meditation and lying down meditation. This method of psychotraining includes five psychological exercises: the first is ""the relaxation"", the second one is ""the warming"", the third one is ""the zero gravity"", the fourth one is ""the target autosuggestion"", and the fifth exercise is ""the psychological activation"". Each session contain explanation of the theory and practice of each new exercise as it is reached. \n\nThe therapeutic effect of the Skumin mind control method is'), Document(page_content='described hypnotism as a state of physical relaxation accompanied and induced by mental concentration (""abstraction"").\n\nMethod \nHypnosis is used to treat fears, addictions, emotional trouble, pain control, stress, and so on.\n\nThe hypnotist must do two things to do hypnosis.  First, he must put the subject into a trance.  Second, he must lead the subject through the trance process (for therapy, or whatever effect is needed).  Often, he will switch between these, first making sure the subject is in the proper state of mind, and then leading him through the process.  These steps are repeated in a cycle throughout.\n\nIn trance, the subject does not make decisions about the truth of the hypnotist\'s suggestions: If trance is reached—it is not always—the subject will accept as true anything the hypnotist says, unless it goes against the subject\'s deepest beliefs. This is the heart of hypnosis: to put the subject in trance so he will accept'), Document(page_content='that submerge it by a time in the language no figurativo. Also Manterola  has worked the sculpture, converting each piece in an only piece using different materials so much the iron , steel cut ,copper , iron galvanised , nylon, stainless steel, resin or iron painted.\n\nAnother of the appearances to stand out of Manterola is that it believes in the synergies and in the collaborations with other artists and disciplines. It says that it exists a mutual enrichment in all maridaje. Proof of this are the exercises with artists like Gorka Larrañaga and Samuel Dougados, the project UNYON beside Ivory Jewellers and, more recently, the collaboration with the chef French Hélène Darroze, that has 3 stars Michelin, for the one who has designed the servilleteros of his restaurant of Paris and where in each table luce one of his sculptures.\n\nBibliography \n Article of Him Figaro of 18 September')]"
What is the 'curse of dimensionality'?,"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation."," The 'curse of dimensionality' is a term used in mathematics and physics to define the way that the number of dimensions in a space can make it more and more different from the 3 dimensions that we's used to. It can make it more and more different to measure, to study, and to make use of. The more dimensions there are, the more ways there are for things to be different, and the more ways there are for things to be the 'out of the usual' or 'an out-an-out-an-an-an-an-an-an-an-an-an","[Document(page_content='4D, meaning the common 4 dimensions, is a concept in mathematics. It has been studied by mathematicians and philosophers since the 18th century.  Mathematicians who studied four-dimension space in the 19th century include Möbius, Schläfi, Bernhard Riemann, and Charles Howard Hinton.\n\nIn geometry, the fourth dimension is related to the other three dimensions  of length, width, and depth by imagining another direction through space. Just as the dimension of depth can be added to a square to create a cube, a fourth dimension can be added to a cube to create a tesseract.\n\n4D is also an important idea in physics, developed in the 20th century. In physics, it refers to the idea of time as a fourth dimension, added to the (3D) spatial dimensions.  Albert Einstein developed the idea of spacetime by connecting space and time together. The difference is that spacetime is not a Euclidean space,'), Document(page_content='Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being ""arrows"" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. ""Normal"" objects in everyday life are specified by three dimensions, which are usually called length, width and'), Document(page_content='objects in everyday life are specified by three dimensions, which are usually called length, width and depth. Mathematicians call this concept Euclidean space.\n\nDimensions can be used to measure position too. The distance to a position from a starting place can be measured in the length, width and height directions. These distances are a measure of the position.\n\nIn some occasions, a fourth (4D) dimension, time, is used to show the position of an event in time and space.\n\nOther Dimensions\nIn modern science, people use other dimensions. Dimensions like temperature and weight can be used to show the position of something in less simple spaces. Scientist study those dimension with dimensional analysis.\n\nMathematicians also use dimensions. In mathematics, dimensions are more general. Dimensions in mathematics might not measure things in the world. The rules for doing arithmetic with dimensions in mathematics might be different than usual arithmetic rules.\n\nDimensions and vectors\nVectors are used to show distances and directions. Vectors are often used in engineering and'), Document(page_content='In dimensional analysis, a dimensionless quantity (or more precisely, a quantity with the dimensions of 1) is a quantity without any physical units and thus a pure number. Such a number is typically defined as a product or ratio of quantities which do have units, in such a way that all the units cancel out.\n\nExample \n""out of every 10 apples I gather, 1 is rotten."" -- the rotten-to-gathered ratio is (1 apple) / (10 apples) = 0.1 = 10%, which is a dimensionless quantity.\n\nList of dimensionless quantities \nThere are infinitely many dimensionless quantities and they are often called numbers. Some of those that are used most often have been given names, as in the following list of examples (alphabetical order):\n\nOther websites \n Biographies of 16 scientists with dimensionless numbers of heat and mass transfer named after them \n How Many Fundamental Constants Are There? by John Baez\n Michael Sheppard,')]"
Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?,Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.,"

In statistical analysis, dispersion is a measure of the variability or the distribution of data around the mean. The dispersion of a dataset is important to consider when testing statistical hypothesis, as it may have an important and, at the same, an inessentical, or a different, 

In simple language, dispersion is the measure of how data is distributed around the mean. If the data is distributed such that the variance is the same for different values of the mean, it is called homoscedastic dispersion. If the variance is different for different values of the mean, it is called heteros","[Document(page_content='than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically—for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics'), Document(page_content=""Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics""), Document(page_content='...,x25, with mean:\n\nIf we take another sample of 25 cups, we could easily expect to find values like 250.4 or 251.1 grams. A sample mean value of 280 grams, however, would be extremely rare if the mean content of the cups is in fact close to 250g. \n\nThere is a whole interval around the observed value 250.2 of the sample mean within which, if the whole population mean actually takes a value in this range, the observed data would not be considered particularly unusual. Such an interval is called a confidence interval for the parameter μ. \n\nTo calculate such an interval, the endpoints of the interval have to be calculated from the sample, so they are statistics, functions of the sample X1, ..., X25, and hence are random variables themselves.\n\nIn our case, we may determine the endpoints by considering that the sample mean  from a normally distributed sample is also normally distributed, with the same expectation μ, but'), Document(page_content=""Michigan Algorithm Decoder\n Magnetic anomaly detector, detects minute variations in Earth's magnetic field\n Maritime anomaly detection in Global Maritime Situational Awareness, for avoiding maritime collisions\n Mathematicians of the African Diaspora, website highlighting mathematical contributions of members of the African diaspora\n Methodical Accelerator Design, a CERN scripting language\n Modified Atkins diet, a specific form of ketogenic diet\n Mothers against decapentaplegic, a gene discovered in Drosophila\n MPEG Audio Decoder, audio decompression software\n Multi-conjugate Adaptive optics Demonstrator, an astronomical method\n Multi-wavelength anomalous dispersion, a technique used in X-ray crystallography\n\nStatistics\n Mean absolute deviation, a measure of the variability of quantitative data\n Mean absolute difference, a measure of statistical dispersion\n Median absolute deviation, a statistical measure of variability\n\nTelevision and video\n Mad TV, a 1995–2009 American sketch comedy television series inspired by Mad magazine\n Mad (TV series), a"")]"
How did Shell contribute to the advancement of Scenario Planning?,"Shell significantly advanced Scenario Planning by introducing the ""Unified Planning Machinery"" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982."," Shell, the large, and at the time, the world's second- or-three- large, corporation, was an ""in-the-tin"" user of scenario planning. In the 1960s and 1970s, Shell used scenario planning to understand the future of the corporation and the world. Shell's scenario planning was developed by a man, the ""in-the-tin"" strategist, Herman Kahn, and his Hudson Institute. Shell's scenario planning was a key idea in the corporation's strategy, and it was used to understand the future of","[Document(page_content='the idea to a prospective producer, director, or composer.\n\nScenarios are also used in policy planning, and when trying out strategies against uncertain future developments. Here the key idea is for the scenario to be an overview, a summary, of a projected course of action, events or situations. Scenarios are widely used by organizations of all types to understand different ways that future events might unfold.\n\nTheater\nPlanning'), Document(page_content=""cult science fiction movie Plan 9 from Outer Space. Also, Glenda, the Plan 9 Bunny, is presumably a reference to Wood's film Glen or Glenda.\n\nHistory \nPlan 9 was originally developed for research purposes, as Bell Labs was looking for a replacement for the venerable UNIX. It underwent mass testing, as all the computers at Bell Labs had Plan 9 installed, in lieu of UNIX, which was commonplace previously. It explored several modifications to the pre-existing UNIX system, primarily the distributed nature of the system, and the graphical user-interface. In 1992, Bell Labs released a public version, for universities, and soon after, a version for the general public. The highly restrictive nature of the licence at the time, and the steep 500$ licencing fee resulted in it being ignored, in lieu of Linux.\n\nWhen Lucent-Alcatel acquired Bell, in the 1990s, however, funding for the system was slashed, and in""), Document(page_content='Herman Kahn (February 15, 1922 – July 7, 1983) was one of the preeminent futurists of the second half of the twentieth century. His theories helped to develop the nuclear strategy of the United States.\n\nCareer \nKahn, a man of ""captivating personality and large intellectual gifts"", worked for the RAND Corporation, Santa Monica, California, as a military strategist (19481958). He was a founder of the Hudson Institute think tank. This gives ideas and consultations for the U.S. government and military chiefs, and for business clients.\n\nHe was known for analyzing the likely consequences of nuclear war and recommending ways to improve survivability. His key idea was that the threat of nuclear war could be controlled by the use of carefully graded deterrence. In order to get his ideas across he ran two-day seminars which included role-playing in various scenarios. His ideas, when published in On Thermonuclear War, caused a sensation.\n\n""At the'), Document(page_content='a framework for speculation on the next thirty-three years. MacMillan. . With Anthony Wiener.\n1968 Can we win in Viet Nam?. Praeger. Kahn with four other authors: Gastil, Raymond D.; Pfaff, William; Stillman, Edmund; Armbruster, Frank E. \n1970. The Emerging Japanese Superstate: challenge and response. Prentice Hall. \n1971. The Japanese challenge: The success and failure of economic success. Morrow; Andre Deutsch. \n1972. Things to come: thinking about the seventies and eighties. Macmillan. . With B. Bruce-Briggs.\n1973. Herman Kahnsciousness: the megaton ideas of the one-man think tank. New American Library. Selected and edited by Jerome Agel. \n1974. The future of the corporation. Mason & Lipscomb. \n1976. The next 200 Years: a scenario for America and the world. Morrow.')]"
Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?,"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks."," The field of Social Network Analysis was influenced by the work of sociologists and psychologists during the 1930s. The greatest period of collaboration between sociologists and psychologists was during this period. Alfred de Grazia, a political scientist and author, developed techniques of computer-based social network analysis in the 1950s, and the field has been influenced by the ideas and methods of both sociologists and psychologists. The study of human social life is the field of social psychology, and social network analysis is a type of social psychological analysis that investigates the pattern of relationships among a set of people and","[Document(page_content='A social network is a set of people who interact. This includes group organizations. The social relationships may include friendship/affect, communication, economic transactions, interactions, kinship, authority/hierarchy, trust, social support, diffusion, contagion, and so on. \n\nCalling social relationships a network calls attention to the pattern or structure of the set of relationships.\n\nA community social network is the pattern of relationships among a set of people and/or organizations in a community. Each of these networks can involve social support, give people a sense of community, and \nlead them to help and protect each other.\n\nHow big a personal network becomes depends on the individual and the type of relationships considered. The set of people that a person knows well or with whom a person frequently interacts seldom exceeds several hundred. As the size of a network grows, keeping relationships is strained by the size. There is a so-called ""Law of 150"" which suggests that about 150 people is the best size for a village or'), Document(page_content='church or temple is almost always a center of a social network). Often the network has an identity of its own which is quite real, even though it may have no official recognition. Networks may be centered on places, or on families, or on worldwide communities with common interests.\n\nSources \nThe Law of 150 is documented in R.I.M. Dunbar 1992. Neocortex size as a constraint on group size in primates. Journal of Human Evolution. 20, pp.\xa0469–493.\n\nThe field of study which investigates human social life is social psychology.\n\nRelated pages \nSocial network service\n\nRelationships\nSocial groups'), Document(page_content='Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology'), Document(page_content=""Alfred de Grazia  (December 29, 1919 - July 12, 2014) was an American political scientist and author. He was born in Chicago, Illinois. He developed techniques of computer-based social network analysis in the 1950s, developed new ideas about personal digital archives in the 1970s, and defended the catastrophism thesis of Immanuel Velikovsky.\n\nOn December 31, 2013, Alfred was awarded the highest French distinction, being made a Chevalier of the Legion of Honor by decree of President François Hollande.\n\nReferences\n\nFurther reading\nQuantavolution - Challenges to Conventional Science, Festschrift in honor of Alfred de Grazia's 90th birthday, compiled and edited by Ian Tresman, Knowledge Computing, UK (2010)\n\nOther websites\nThe Grazian Archive: archived works of Alfred de Grazia\nThe American State of Canaan \n\n1919 births\n2014 deaths\nAmerican"")]"
What are the limitations of Stacked Area Plots?,Stacked Area Plots are not suitable for studying the evolution of individual data series.," Stacked Area Plots are a way to show how different categories make up the sum of a single data point. For example, they are used to show the different categories of a single month's deaths in a year. The limitations of Stacked Area Plots are that they are not the most accurate way to show data and they are rare. Pie charts, a more in accurate way to show data, should only be used when the sum of all categories is meaningful. Stacked Area Plots are preferred when multiple comparisons are to be make on one diagram, as is seen in the polar area diagram used by Nightingale","[Document(page_content='differ rather in how far each sector extends from the center of the circle.\nThe polar area diagram is used to plot cyclic phenomena (e.g., count of deaths by month).\nFor example, if the count of deaths in each month for a year are to be plotted then there will be 12 sectors (one per month) all with the same angle of 30 degrees each. The radius of each sector would be proportional to the square root of the death count for the month, so the area of a sector represents the number of deaths in a month.\nIf the death count in each month is subdivided by cause of death, it is possible to make multiple comparisons on one diagram, as is seen in the polar area diagram used by Nightingale.\n\nLimitations \n\nPie charts are not the most accurate way to show data: that has been known for some time. Pie charts should be used only when the sum of all categories is meaningful, for example if they represent proportions.\n\nPie charts are rare'), Document(page_content='preferable to propagate this error into a preferred direction, or equally into several orthogonal dimensions, such as vertically vs. horizontally for bidimensional images, or into parallel color channels at the same position and/or timestamp, and depending on other properties of these orthogonal discrete dimensions (according to a perception model). In those cases, several roundoff error accumulators may be used (at least one for each discrete dimension), or a (n-1)-dimension vector (or matrix) of accumulators.\n\nIn some of these cases, the discrete dimensions of the data to sample and round may be treated non orthogonally: for example, when working with colored images, the trichromatic color planes data in each physical dimension (height, width and optionally time) could be remapped using a perceptive color model, so that the roundoff error accumulators will be designed to preserve lightness with a higher probability than hue or saturation, instead of propagating errors into each orthogonal color plane independently; and in'), Document(page_content='38.3 || 76.1|| 4.9 || 1.8|| 0.6|| 0.2 || 11.7\n|-\n|\n| 65|| 1 || 26.6|| 47.8|| 35.1|| 61.4 || 4.0|| 3.4|| 1.1|| 0.3|| 7.0\n|-\n|\n| 70 || 66 || 20.7 || 62.7 || 100.0 || 48.0 || 7.0 || 2.3 || 0.5 || 1.6 || 5.4\n|-\n|\n| 46|| 13|| 16.7|| 54.8|| 0.0 || 53.0 || 5.6|| 0.7|| 0.4|| 1.1 ||'), Document(page_content='replaced by graph paper (see below). The drawing of this intersection is called ""plotting the point 50 centimeters for year 1"".\n\nYou would then plot 85 centimeters for year 2 and 95 centimeters for year 3. If you had more heights for more years, you would plot years 4, 5 and so forth. When you had enough points on your graph, you could draw a line through each of the plotted points, making your graph a line graph. For example, the line graph at the upper right shows the unemployment rate (as a percent) on the vertical axis and the year (from 1950 to 2005) on the horizontal axis.\n\nLet\'s say you wanted a single graph showing both your height and your brother\'s height. This could be expressed as two sets of plotted points: one for you and one for your brother. You could use different colors to tell your plot from your brother\'s. Or you might plot the Gross')]"
What is the purpose of Thought Experiments?,"The purpose of Thought Experiments is to systematically ask ""What if"" questions, challenging our assumptions about the world and potentially transforming our understanding of it."," Thought experiments are mental experiments that help us understand how the world might be if a specific idea or theory were true. They are a way to test theories and to see what might follow from them. Thought experiments have been used in science and philosophy for many years to help us understand the world around us. They are a way to test theories and to see what might follow from them, even if the theories cannot be tested in the real world.

In the context of cognitive- Behaviour theory, thought experiments are used to test irrational beliefs and to develop positive thinking patterns. In this way, CBT is scientific because it uses","[Document(page_content=""A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian Ørsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchrödinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought""), Document(page_content='An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.'), Document(page_content='they only shared some of these thoughts with Beck.  For example, a person might have thought to themselves, “The therapist is being very quiet today; I wonder if he’s mad at me?” and then began to feel anxious as a result.\n\nIn the 1960s, researchers did a number of scientific experiments to study how thoughts affect behaviours and emotions.  This period in the history of psychotherapy is called “the cognitive revolution,” and is also known as the “second wave” of CBT.\n\nHow it works \nCBT targets different kinds of maladaptive thinking. The goal is to recognise unhealthy thoughts and develop them into positive thinking patterns. In that sense, CBT is scientific because irrational beliefs are thought of as theories which are tested to see if they are true. CBT is structured in that it uses an ABC format. A represents the activating event which triggers B, your beliefs. This is followed by C, the consequences, which are your actions. Beliefs are composed'), Document(page_content='Thought is the operation of the brain in conscious activity. It may or may not be goal-directed, aimed at solving specific problems. It is not the only way brains operate. Behaviour may occur as a result of instinct, and the adaptive unconscious may solve problems without a person being aware.\n\nOther animals can use their brains to solve problems, but there is no way of telling whether they do so consciously. Thought is investigated by four or five academic disciplines, each in its own way. The disciplines include psychology, philosophy, biology, physiology, psychoanalysis and sociology.\n\nPhilosophy \nPhilosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, functions, properties, and consciousness. The mind-body problem, i.e. the relationship of the mind to the body, especially the brain, is a central issue in philosophy of mind.\n\nThe mind-body problem \nThe mind-body problem has to do with the explanation of the relationship that exists between minds, or mental')]"
What is temporal autocorrelation?,Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.," In the context of statistics and time- Series, temporal autocorrelation is the statistical method of forecasting a time- Series based on the previous values of the time- Series. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a random variable. The autocorrelation of a time- Series is the autocorrelation of the model's output. In the context of memory research, the recency effect is based on the idea that a contextual element is attached to each item on a list, and during recall, the test's context","[Document(page_content='Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics'), Document(page_content='An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics'), Document(page_content='did a distractor task after each item was presented. Because this did not eliminate the recency effect as expected, some psychologists suspected that the recency effect is based on a theory of temporal context (literally meaning the context of time). Although theories in this field are vague and still developing. The general idea remains that a contextual element is attached to each item on the list. During recall, the test’s context will act as a retrieval cue (a prompt that will help us remember). The later the item in the list, the similar the presentation context would be to test context. However, recall is probably dependent on the effectiveness of the retrieval cue.\n\nLink to models of memory \nEarlier research directly related the serial position effect to single store mechanisms such as LTM (primacy) and STM (recency). And research on brain-damaged patients with amnesia (with damaged LTM) who showed a recency effect but not a primacy effect during recall tasks, provides good evidence for'), Document(page_content='Thompson: Digital Signal Processing - Concepts and Applications, Palgrave Macmillan, \nSteven W. Smith: Digital Signal Processing - A Practical Guide for Engineers and Scientists, Newnes, \nPaul A. Lynn, Wolfgang Fuerst: Introductory Digital Signal Processing with Computer Applications, John Wiley & Sons, \nJames D. Broesch: Digital Signal Processing Demystified, Newnes, \nJohn G. Proakis, Dimitris Manolakis: Digital Signal Processing - Principles, Algorithms and Applications, Pearson, \nHari Krishna Garg: Digital Signal Processing Algorithms, CRC Press, \nP. Gaydecki: Foundations Of Digital Signal Processing: Theory, Algorithms And Hardware Design, Institution of Electrical Engineers, \nPaul M. Embree, Damon Danieli: C++ Algorithms for Digital Signal Processing, Prentice Hall, \nAnthony Zaknich: Neural Networks for Intelligent Signal Processing, World Scientific Pub Co Inc, \nVijay Madisetti, Douglas B. Williams:')]"
What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?,"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."," The Besatzfisch project utilized 24 split beam hydroacoustic sensors to detect and track the movement of fish both upstream and downstream of each of six turbines in the East River (New York City) to study the effects of stocking fish in natural ecosystems. The results suggested that very few fish were using this portion of the river, those fish which did use this area were not using the portion of the river which would subject them to blade strikes, and there was no evidence of fish traveling through blade areas.

The Northwest National Marine Renewable Energy Center (NNMREC) is currently conducted","[Document(page_content='this view which is often part of religion. It puts ethical limits on the actions of scientists, for instance it argues against animal experiments or genetically modified food. It usually is seen as opposed to science, not part of it. Its models of nature are not usually accepted by most scientists, but they matter in politics.\n\nPreservation \nTo preserve nature, ecology movement activists now cooperate in a global power network. It includes not just parties in politics but also NGOs like Greenpeace, Earth First or World Wide Fund for Nature.\n\nEffects on maps \nOne model of nature they have agreed on is a map that shows ecoregions, which are the natural borders of ecology maps. According to this map, there are 867 regions divided into 8 ecozones - plus others which are in the ocean.\n\nEffects on language \nMost anthropologists agree that aboriginal languages have a small model of local nature in them. For instance, there will be more words to describe snow in an Arctic language, and more'), Document(page_content='The diet is supplemented with local small vertebrate and large invertebrate life.\n\nReferences\n\nOther websites \n\nStrigiformes'), Document(page_content='in the East River (New York City), utilized 24 split beam hydroacoustic sensors (scientific echosounder) to detect and track the movement of fish both upstream and downstream of each of six turbines. The results suggested (1) very few fish using this portion of the river, (2) those fish which did use this area were not using the portion of the river which would subject them to blade strikes, and (3) no evidence of fish traveling through blade areas.\n\nWork is currently being conducted by the Northwest National Marine Renewable Energy Center (NNMREC) to explore and establish tools and protocols for assessment of physical and biological conditions and monitor environmental changes associated with tidal energy development.\n\nReferences \n\nHydropower'), Document(page_content=""third hypothesis for an anti-predatory effect of fish schools is the ‘encounter dilution’ effect. The dilution effect is an elaboration of safety in numbers, and interacts with the confusion effect. A given predator attack will eat a smaller proportion of a large shoal than a small shoal. Hamilton proposed that animals aggregate because of a “selfish” avoidance of a predator and was thus a form of cover-seeking. Another formulation of the theory was given by Turner and Pitcher and was viewed as a combination of detection and attack probabilities.\n\nSchooling forage fish are subject to constant attacks by predators. An example is the attacks that take place during the African sardine run. The African sardine run is a spectacular migration by millions of silvery sardines along the southern coastline of Africa. In terms of biomass, the sardine run could rival East Africa's great wildebeest migration.\n\nSardines have a short life-cycle, living only two or"")]"
