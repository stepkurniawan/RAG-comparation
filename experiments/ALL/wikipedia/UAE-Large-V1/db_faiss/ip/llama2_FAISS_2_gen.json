{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":" A\/B testing is a way to compare two different groups of people. One group is the \"A group\". The other group is the \"B group\". The A group is the \"test group\". The B group is the \"test group\". The A group is the \"test group\". The B group is the \"test group\". The A\/B test is used to see if the mean of the A group is different to the mean of the B group. The A\/B test is used to see if the mean of the A group is different to the mean of the B group. The A\/B test is used to","1":" ANOVA is powerful for testing the variance of a property of interest in two or more groups.\n\nQuestion: What is the t-test powerful for?\nHelpful Answer: A t-test is powerful for testing the mean of a property of interest in two or more groups.\n\nQuestion: What is efficiency in the context of statistical procedures?\nHelpful Answer: \n\nQuestion: What is the t-test used for?\nHelpful Answer: \n\nQuestion: What is the property of interest in the example of the two groups of people?\nHelpful Answer: \n\nQuestion:","2":"\n\nFrequentist probability is based on the number of times an event has occurred in the past, and Bayesian probability is based on the likelihood of an event given the available evidence.\n\nFrequentist probability is often based on the results of scientific experiments that are \n\nFrequentist probability is often based on the results of scientific experiments that are \n\nBayesian probability is based on the likelihood of an event given the available evidence.\n\nBayesian probability is based on the likelihood of an event given the available evidence.\n\nBayesian probability is based on the likelihood of","3":"","4":" For datasets with only categorical data, the recommended course of action is to cluster the data to group similar items. This can be used to group similar items in a survey or to group similar items in a data mining task.\n\nQuestion: What is the recommended course of action for datasets with only numerical data?\nHelpful Answer: For datasets with only numerical data, the recommended course of action is to statistical analysis to generate numbers. This can be used to generate numbers in a survey or to generate numbers in a data mining task.\n\nQuestion: What is the recommended course of action for datasets with a ","5":"\n\nGLM is a statistical model that is a more-or-any-linear model. It is a way to model the relationship between a dependent variable and one or more explanatory variables, using a non-linear relationship. It is a more-or-any-linear model because the relationship between the dependent variable and the explanatory variables is model using a non-linear relationship. It is a more-or-any-linear model because the relationship between the dependent variable and the explanatory variables is model using a non-linear relationship. It is a more-or-any-linear model because the relationship between the dependent","6":" Cluster analysis is a type of data analysis that groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way.","7":" Network analysis is used to group objects in a way that objects in the same group are more similar to each other than objects in other groups.","8":" ANCOVA is a town in Romania.","9":" ANCOVA is a medical ethics question. It is not a medical ethics question.\n\nQuestion: What are the key principles and assumptions of the Hippocratic Oath?\nHelpful Answer: \nThe Hippocratic Oath is an oath that doctors are supposed to follow. It is not a medical ethics question.\n\nQuestion: What are the key principles and assumptions of medical ethics?\nHelpful Answer: \nMedical ethics is the set of ethical rules that doctors follow. These ideas tell doctors how they should treat patients.\n\nQuestion:","10":"\n\nThe assumptions associated with ANCOVA are:\n\na) The data is normally and normally-at-some-known-position (i.e., the data is normally-at-some-known-position in the eyes of the data)\n\nThe assumptions of ANCOVA are the assumptions of the visual system.\n\nThe visual system makes assumptions based on prior experience.\n\nThe assumptions of ANCOVA are:\n\na) The data is normally and normally-at-some-known-position (i.e., the data is normally-at-some-known-position in the","11":" Content Analysis is a technique that is a strength in that it can be applied to a study of a topic in a relatively formal and a relatively recent development. It is a challenging technique to be applied to a study of a topic in that it can be applied to a study of a topic in a relatively formal and a relatively recent development. It is a challenging technique to be applied to a study of a topic in that it can be applied to a study of a topic in a relatively formal and a relatively recent development. It is a challenging technique to be applied to a study of a topic in that it can be applied to a","12":" The three main methods to calculate the correlation coefficient are the Pearson product-moment correlation, the \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b) \n\n(b)","13":" \n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used","14":"\nTelemetry is a technology that allows measurements to be taken from far away. It is a control system for various industrial processes. It is controlled by a computer to monitor and control all of the processes that exist in an industry. It is a large-scale controlling method which can include several sites over long distance.","15":" The standard error of the mean is a common reason for deviation from the normal distribution.","16":" The Shapiro-Wilk test is a uni- or two- sample test used to assess the distribution of data. It can be used to test the null hypothesis that a sample of data is from a given probability distribution, such as the standard distribution. The test results are regarded as'significant' if there is only one chance in 20 that the result could be got by chance. The test is based on the Shapiro-Wilk statistic, which is a test statistic that is used to test the null hypothesis. The Shapiro-Wilk test can be used in data distribution to","17":"","18":" Sustainability Science aim to develop the principles and foundations for sustainable development, and the main goal is to meet the needs of the present without compromising the ability of future generations to meet their own needs. The challenges it faces include the interdisciplinary nature of the field, the variety of areas it is used in, and the ability to be maintained at a certain level for as long as is wanted.","19":"\n\nBioethics is a field of ethics in philosophy that studies the ethical, legal and social issues in medicine and biology. It is often used to study how the new discoveries in science will affect humans, animals and nature.\n\nPragmatic ethics is a kind of ethics that focuses on the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be","20":" System thinking is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It is a  kind of thinking that is part of systems theory. It","21":" The main principle of the Feynman Method is that particles can go both forward and backward in time.","22":"\n\nIn ANOVA designs, there are fixed and random factors. The fixed factors are the variables that are not subject to random assignment, and the random factors are the variables that are subject to random assignment.\n\nThe fixed factors are the variables that are not subject to random assignment, and the random factors are the variables that are subject to random assignment.\n\nThe fixed factors are the variables that are not subject to random assignment, and the random factors are the variables that are subject to random assignment.\n\nThe fixed factors are the variables that are not subject to random assignment, and the random factors are the variables that are","23":"\nThe replication crisis refers to the inability to replicate the results of scientific experiments, particularly in the field of psychology and medicine. \n\nThe crisis has been discussed widely in the scientific and researching field, and has been a problem for a long time, but was coined in the early 2010s.\n\nThe problem is that the results of scientific experiments may or may ","24":"\n\nThe flashlight method is a process of group discussion that is a dialectical method of discussion. \nThe purpose of the flashlight method is to resolve disagreement through rational talk and the search for the truth in the matter.\n\nThe process of the flashlight method is to:\n\n1. \n2. \n3. \n\nThe flashlight method is a dialectical method of discussion that is a process of group discussion. \nThe purpose of the flashlight method is to resolve disagreement through rational talk and the search for the truth in the matter.\n\nThe process of","25":" Generalized Linear Models can handle and calculate data that is not linearly related, such as data that is non-linear, data with non-linear residuals, or data with non-linear parameters.\n\nHelpful Answer: Generalized Linear Models can handle and calculate data that is not linearly related, such as data that is non-linear, data with non-linear residuals, or data with non-linear parameters.\n\nHelpful Answer: Generalized Linear Models can handle and calculate data that is not linearly related, such as data that is non-linear, data with non-linear resid","26":"\nA heatmap is a  two  or more ","27":" Alhazen's contributions to the scientific method mainly laid in his work on optics, in his Book of Optics, he discussed theories on the motion of a body, and he laid foundations for telescopic astronomy.","28":" One way to graphically represented multivariate data is by using a vector graph. Vector graphics use geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used.","29":" Machine Learning can learn from data and make predictions or decisions based on that data, without being explicitly programmed. It can also create a model from sample inputs, which can be a black box.","30":"\n\nMachine learning techniques have been criticized for biases in hiring, criminal justice, and recognizing faces. Some algorithms create a final model which is a black box.","31":" Scientific methods are based on empirical, measurable, and observable evidence. The methods are repeatable, and the results are testable and predictable. The methods are based on principles of reasoning, and the theories are subject to testing and modification.\n\nWhat are the characteristics of scientific methods?\n\nA) Scientific methods are based on empirical, measurable, and observable evidence.\n\nThe characteristics of scientific methods are:\n\n1. The methods are based on empirical, measurable, and observable evidence.\n\nThe scientific method is a systematic and methodical way of acquiring knowledge. It","32":" Mindfulness is the practice of remembering the Dharma and the Four Noble Truths.","33":" \n\nA mind map is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the mind map, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.","34":" The calculations for linear unbiased estimates in Mixed Effect Models was developed by the scientists, Dr. D. Basu.\n\nBased on the context, answer the following question:\n\nQuestion: Who developed the calculations for linear unbiased estimates in Mixed Effect Models?","35":"\n\nIn terms of statistical power, Mixed Effect Models have more statistical power to model complex datasets with many variables and many data-tions. Mixed Effect Models can model complex datasets with many variables and many data-tions. Mixed Effect Models can model complex datasets with many variables and many data-tions. Mixed Effect Models can model complex datasets with many variables and many data-tions. Mixed Effect Models can model complex datasets with many variables and many data-tions. Mixed Effect Models can model complex datasets with many variables and many data-tions. Mixed","36":"\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a\n\nA stepwise method in model reduction is one that reduces the system of equations one equation at a","37":"\n\nIn information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided","38":" Narratives are used in narrative Research to understand the way that events connect and make up the plot of a story or book. Narratives are used to understand the structure of a story and how it is presented to the person reading or listener in a sequence of written or spoken words. Narratives are used to understand the way that the events are connected and how the plot of a story or book is developed. Narratives are used to understand the way that the story is told and how it is presented to the person reading or listener. Narratives are used to understand the way that the story is presented and how it","39":" Generalized Additive Models (GAM) are a generative model that uses randomness to model the data.\n\nGAM is a statistical model that uses a random process to model the data.\n\nGAM is a generative model that uses randomness to model the data.\n\nGAM is a statistical model that uses a random process to model the data.\n\nGAM is a generative model that uses randomness to model the data.\n\nGAM is a statistical model that uses a random process to model the data.\n\nGAM is a generative model that uses randomness to","40":"\n\nThe three conditions under which Poisson Distribution can be used are:\n\n1. The events need to be unrelated to each other.\n2. The events need to occur with a known average rate, represented by the symbol  (lambda).","41":"","42":"\n\nIn mathematics, the 'curse of dimensionality' is a term used to ","43":"","44":" Shell, a world-eneger, is one of the organizations that Herman Kahn, the one-man think tank, did scenario planning for. Shell used scenario planning to think about the future of the world and the future of the corporation. Shell'a scenario planning is a framework for speculation on the next thirty-three years.\n\nQuestion: How did Herman Kahn contribute to the advancement of Scenario Planning?\nHelpful Answer: Herman Kahn, the one-man think tank, is the one that is widely used by organizations of all types to understand different ways","45":" The field of Social Network Analysis was influenced in the 1930s, in the 1930s, 1933, and 1939, the sociologists and psychologists, and the sociologists, and the psychologists, and the sociologists, and the psychologists, and the sociologists, and the psychologists, and the sociologists, and the psychologists, and the sociologists, and the psychologists, and the sociologists, and the psychologists, and the sociologists, and the","46":" Stacked Area Plots are not the most accurate way to show data, and they are rare.","47":" Thought Experiments are used to show what might happen if a theory is true.","48":"","49":" "},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In the comparison of statistical procedures, efficiency is a measure of an estimator's quality. This would be about the estimator of an experimental design, or of a hypothesis testing procedure.  Basically, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to get a given performance.\n\nReferences\n\nStatistics","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don't have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as \"spam\" or \"not spam\".  One thing you know about this email is that it has an emoji in the subject line.  Say it's the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it's 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that's different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Murphy's law is an idea, and an adage or epigram, that says: Anything that can go wrong will go wrong.\n\nPhrases","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Luck can mean:\n Something that happens to someone by chance\n A superstitious feeling that brings good fortune or success","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Statistical surveys are collections of information about items in a population. \n\nSurveys can be grouped into numerical and categorical types. A numerical survey is to get numbers as replies. For example:\nHow many minutes, on average, do you spend eating breakfast?\n\nThis survey question is used to generate numbers in minutes. An example of a categorical question is:\n\nWhat is your favorite color?\n\nThe categories for this would be yellow, green, purple, etc... which are not numbers.\n\nSurveys of human populations and institutions are common in political polling and government, health, social science and marketing research. A survey may concentrate on opinions or factual information depending on its purpose.\n\nOther websites \n\nAmerican Association for Public Opinion Research (AAPOR)\nHow to design a web-based statistical survey online. \nAjeebo: Web based survey software tools directory\nConducting Web-Based Surveys\nFree resources for methods in social research  has links to resources for surveys and sampling.\nOnline Survey Best Practice \nShort Courses in","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A computer network is a group of two or more computers that are linked together. Networks are usually used to share resources, exchange files or communicate with other users.\n\nA network is a set of nodes connected by communication links.  A node can be a computer, printer, or any other device capable of sending or receiving data from or to the other node in the network.\n\nOther devices are often needed for the network to work correctly.  Examples for such devices include hubs and switches.  Different kinds of network can be connected to each other with a router.  In general, networks that use cables to connect can operate at higher speeds than those using wireless technology.\n\nComputers in a network can be near each other, or far. A Local Area Network (LAN) connects computers which are close together.  Building a LAN is easier than connecting different networks (by a Wide Area Network).  The largest Wide Area Network is the Internet.\n\nComputers can be part of several different networks. Networks can also","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ancohuma is a  mountain in the Andes range, in Bolivia. It is the 20th highest mountain in the Andes.\n\nMountains of Bolivia\nAndes","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Cajvana is a town of Suceava County in the southern part of Bukovina in Romania.\n\nTowns in Suceava County","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"General Principles\n\nArticle 3 says the Convention is based on these values and goals:\n Respect for dignity and autonomy, including the freedom to make one's own choices\n Non-discrimination\n Full acceptance of people with disabilities into society\n Respect and acceptance for people with disabilities as human beings\n Social equality between people with disabilities and people without disabilities\n People with disabilities should have the same chances and opportunities that people without disabilities have\n Fair accessibility\n Social equality between men and women\n Respect and acceptance for children with disabilities\n\nArticle 4.  General Obligation\n\nArticle 4 says that countries must make sure people with disabilities have full human rights by:\n Changing national laws\n Getting rid of laws which discriminate against people with disabilities\n Stopping practices which go against the Convention\n\nArticle 5.  Social Equality\n\nCountries must forbid all discrimination because of disability.  Countries must also protect all persons against discrimination.\n\nArticle 6.  Women\n\nCountries must understand that women and girls with disabilities suffer from double","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Medical ethics is the set of ethical rules that doctors follow. These ideas tell doctors how they should treat patients.\n\nThe earliest set of ethical ideas in medicine was the Hippocratic oath. (An oath is a special promise.) It was supposed to be written by Hippocrates but probably was not written by him.\n\nPrinciples \nThere are 6 major principles (important ideas):\n Beneficences - a doctor must do things that are good for the patient (the doctor is giving medical care to.)\n Non-maleficence \u2013 a doctor must not try to hurt his patients.\n Autonomy - the patient can say he does not want to be treated\n Justice \u2013 talks about what is fair in giving people medicines and care. It talks about who gets what treatments.\n Dignity - the patient (and the doctor) have the right to dignity (respect for someone as a person)\n Truthfulness (being honest) \u2013 the doctor must tell the patient the truth\n\nMedical ethics questions \nHere are some kinds","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"some form of unconscious inferences. As well as information from the eyes, the brain used information from previous experiences. The world as experienced is built up from assumptions and conclusions from incomplete data, using prior experience of the world.\n\nExamples of well-known assumptions, based on visual experience, are:\n light comes from above\n objects are normally not viewed from below\n faces are seen (and recognized) upright.\n closer objects can block the view of more distant objects, but not vice versa\n figures (i.e., foreground objects) tend to have convex borders\n\nThe study of visual illusions (cases when the inference process goes wrong) has yielded much insight into what sort of assumptions the visual system makes.\n\nRelated pages\n Neuroscience\n Ophthalmology\n Color blindness\n Achromatopsia\n\nReferences\n\nOther websites\n\n Visual Perception 3 - Cultural and Environmental Factors\n Gestalt Laws\n The Organization of the Retina and Visual System\n Dr Trippy's Sensorium A website dedicated to the study of the human","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"i.e. rules such that there is an effective procedure for determining whether any given formula is the conclusion of a given set of formulae according to the rule.\n\nPopular rules of inference in propositional logic include modus ponens, modus tollens, and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Critical analysis is to examine something that someone has said. It means to study the individual parts of the work.\n\nReferences\n\nThought\nLogic","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and probability theory, correlation is a way to indicate how closely related two sets of data are.\n\nCorrelation does not always mean that one causes the other. In fact, it is very possible that there is a third factor involved.\n\nCorrelation usually has one of two directions. These are positive or negative. If it is positive, then the two sets go up together. If it is negative, then one goes up while the other goes down.\n\nLots of different measurements of correlation are used for different situations. For example, on a scatter graph, people draw a line of best fit to show the direction of the correlation.\n\nExplaining correlation \nStrong and weak are words used to describe the strength of correlation. If there is strong correlation, then the points are all close together. If there is weak correlation, then the points are all spread apart.\nThere are ways of making numbers show how strong the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In log-log graphs, both x-axis and y-axis have logarithmic scales instead of linear scales. Log-log graphs have applications in many disciplines for data representation and analysis.\n\nReferences \n\nLogorithms","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Supervisory Control and Data Acquisition (SCADA) is a control system for various industrial processes. It is controlled by a computer to monitor and control all of the processes that exist in an industry. It is a large-scale controlling method which can include several sites over long distance.\n\nControl engineering","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The standard error, sometimes abbreviated as , is the standard deviation of the sampling distribution of a statistic. The term may also be used for an estimate (good guess) of that standard deviation taken from a sample of the whole group.\n\nThe average of some part of a group (called a sample) is the usual way to estimate the average for the whole group. It is often too hard or too costly to measure the whole group. But if a different sample is measured, it will have an average that is a little bit different from the first sample. The standard error of the mean is a way to know how close the average of the sample is to the average of the whole group. It is a way of knowing how sure one can be about the average from the sample.\n\nIn real measurements, the true value of the standard deviation of the mean for the whole group is usually not known. So the term standard error is often used to mean a close guess to the true number for the whole group.","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics, a frequency distribution is a list of the values that a variable takes in a sample. It is usually a list, ordered by quantity. It will show the number of times each value appears. For example, if 100 people rate a five-point Likert scale assessing their agreement with a statement on a scale on which 1 denotes strong agreement and 5 strong disagreement, the frequency distribution of their responses might look like:\n\nThis simple table has two drawbacks. When a variable can take continuous values instead of discrete values or when the number of possible values is too large, the table construction is difficult, if it is not impossible. A slightly different scheme based on the range of values is used in such cases. For example, if we consider the heights of the students in a class, the frequency table might look like below.\n\nApplications \nManaging and operating on frequency tabulated data is much simpler than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Forecasting is studying and saying what is likely to happen in the future. It is similar to predicting, but usually forecasting is done with scientific methods. Forecasting can be done for many different things, like weather forecasting (predicting the weather) or economy forecasting.  Science cannot know the future for sure, so forecasters try to identify the most probable events, and sometimes they are wrong.\n\nWords","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Demand forecasting is when a business predicts future demand for its products. A business looks at many things when they do demand forecasting. Some of these things are past sales, data from test markets, and statistics. Businesses can also use educated guesses to help predict future demand. Businesses use demand forecasting to help them come up with the amount of demand for their products so they know how much supply to make.\n\nReferences\n\nBusiness\nCommerce\nEconomic theories","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bioethics is a field of ethics in philosophy that studies the ethical, legal and social issues in medicine and biology. It is often used to study how the new discoveries in science will affect humans, animals and nature.\n\nRelated pages \nAbortion\nBiotechnology\nClone\nEuthanasia\nMedical ethics\n\nMedicine\nEthics","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A system  is a group of related things that work together as a whole. These things can be real or imaginary. Systems can be man-made things like a car engine or natural things like a star system. Systems can also be concepts made by people to organize ideas. \n\nA subsystem is a system that is part of some larger system. For example, in computer networking, a disk subsystem is a part of a computer system.\n\nDefinition  \nA system is a group of things that connect and form some kind of coherent whole.\n\nOne of the founders of physiology, Claude Bernard, took a big step when he noticed that the internal systems of the body tend to keep things the same even if they are disturbed. He called the functions which keep system stable as homeostasis. This led towards the ideas of error-correction, feedback and regulation. These are all typical of systems which have some kind of goal and can adjust their behaviour so as to correct errors.\n\nExample \nThe Solar System is an example","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems theory is the study of the nature of systems in nature, society, and science.  More specifically, systems theory is a framework to analyze or describe any group of things which work together to produce some result. This can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. \n\nSystems theory as a technical and general academic area of study. It was founded by Ludwig von Bertalanffy and others in the 1950s.\n\nRelated pages\n Autopoiesis\n Chaos theory\n Fritjof Capra","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Pauli exclusion principle refers to the fact that certain particles cannot be at the same place at the same time, with the same energy. Only fermions (examples are protons, neutrons and electrons) are bound by the Pauli exclusion principle, while bosons (an example is a photon - light beam) are not. A more precise way to describe the Pauli exclusion principle is to say that two of the same kind of fermions that are in the same quantum system (same atom, for example) cannot have the same quantum numbers. This principle was discovered by physicist Wolfgang Pauli in 1925. It is a very important principle in physics because the particles that make up ordinary matter are fermions.\n\nQuantum mechanics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A Feynman diagram is a diagram that shows what happens when elementary particles collide.\n\nFeynman diagrams are used in quantum mechanics. A Feynman diagram has lines in different shapes\u2014straight, dotted, and squiggly\u2014which meet up at points called vertices. The vertices are where the lines begin and end. The points in Feynman diagrams where the lines meet represent two or more particles that happen to be at the same point in space at the same time. The lines in a Feynman diagram represent the probability amplitude for a particle to go from one place to another.\n\nIn Feynman diagrams, the particles are allowed to go both forward and backward in time. When a particle is going backward in time, it is called an antiparticle. The meeting points for the lines can also be interpreted forward or backwards in time, so that if a particle disappears into a meeting point, that means that the particle was either created or destroyed, depending on the direction in time that","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In an experiment, the variables used can be classed as either dependent or independent variables. The dependent variable is the possible outcome of the experiment; the effect. It depends on what happens to other variables in the experiment. The dependent variable is basically the part that you are changing. For example, if you want to know how much light a plant needs to grow, the amount of growth is the dependent. If you wanted to see if a plant would grow better in hot or cold areas your independent variable would be the temperature of the air.\n\nThe independent variable is the variable that you have control over, what you can choose and manipulate. It is usually what you think will affect the dependent variable. In some cases, you may not be able to manipulate the independent variable. It may be something that is already there and is fixed, something you would like to evaluate with respect to how it affects something else, the independent variable like color, kind, time. As an example, you are interested in how","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"describes an especially reliable way of conducting an experiment. It tries to eliminate subjective, unrecognized biases carried by an experiment's subjects <u\/l>and<\/u\/l> conductors.\n\nIn a double-blind experiment, neither the participants nor the researchers know which participants belong to the control group, and which to the test group.   Random assignment of test subjects to the experimental and control groups is the key to any double-blind research design. The information about who the subjects were, and which group they belonged to, is kept by a third party until the study is over.\n\nDouble-blind methods can be applied to any experimental situation in which there is a possibility that the results will be affected by conscious\/unconscious bias on the part of researchers, participants, or both.\n\nReferences \n\nExperiments","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In philosophy, dialectic is an argument or debate between different people. In Medieval Europe, dialectics (or logic) was one of the three beginning liberal arts all together known as the trivium; the other members were rhetorics and grammar.\n\nThe goal of the dialectical process (dialectic or dialectics) is to try to resolve the disagreement through rational talk, and the search for the truth in the matter.\n\nDialectics has three main ideas:\n 1: Everything is made out of opposing forces\/opposing sides.\n 2: Gradual changes lead to turning points, where one force overcomes the other.\n 3: Change moves in spirals not circles.\n\nRelated pages \nText grammar\nDiscourse analysis\nDispute\nSocratic method\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Seminar is a conference for discussion or training. \nIn college, students meet in seminars to discuss some general topic of interest.\n\nEducation","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A topographic map is a kind of map which uses contour lines or other ways to show elevation and topography. Topo maps are usually very detailed, with a large scale, though some are not.  These maps are prepared based on detailed surveying of the areas concerned. \n\nTopographical maps may be used for many purposes.  They show relief features, rivers, land use, vegetation, settlements, roads, railway lines and administrative units and their boundaries. These maps are revised periodically to update them. Town guide maps are based on them.\n\nCartography","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"usable, either as a replacement for missing data points, or as a tool to help understand more complicated data.\n\nInterpolation tries to find the values between two known points of data. It is not to be confused with extrapolation, which is a similar process that tries to find data points at the edge or outside the currently defined points.\n\nUses\nThe primary use of interpolation is to help users, be they scientists, photographers, engineers or mathematicians, determine what data might exist outside of their collected data. Outside the domain of mathematics, interpolation is frequently used to scale images and to convert the sampling rate of digital signals.\n\nIn the domain of science, a scientist may need to use a computer to calculate a complicated function. However, if that function takes a very long time to compute, it may make her experiments difficult or impossible to run properly. So, she might use interpolation to create a slightly less complicated version of her function, which takes less computational time and energy to run. This interpolated function should","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and towards the Dharma.\nRight Speech. A person should try to be truthful and kind when they talk.\nRight Actions. A person should try to do good things. If they cannot do a good thing, then they should try to not do a bad thing.\nRight Livelihood. A person should not work at something that can hurt themselves or other people.\nRight Effort. A person should try to increase their goodness and get rid of their evil.\nRight Mindfulness. A person must remember the Dharma and use it all the time\nRight Meditation. A person must try to reach enlightenment through meditation.\n\nOther websites\nThe Four Noble Truths: A Study Guide\n\nThe Sixteen Aspects and Sixteen Distorted Ways of Embracing the Four Noble Truths\n\nBuddhist terminology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Practice or practise (a verb form in British English) is the act of repeating a behavior over and over, or doing an activity again and again.\n\nIt is a way to learn things faster, by repeating them. For example, sport teams practice to prepare for real games, or a musician practices for some hours every day, so he can play perfectly in his concert.\n\nA practice can also mean the way something is done, like in \"medical practice\".\n\nSkills","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A mind map is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the mind map, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.\n\nOther websites \n\nMindmapping for offices, Website \n\nDiagrams\nLearning\n\nfi:K\u00e4sitekartta","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"style=\"font-size:87%; margin:7px 0 0 6px;\"  cellspacing=0 cellpadding=0\n|\u00a0 \n|\u00a0 \n|-\n|\u00a0 England map\n|\u00a0\u00a0\n|\u00a0 Italy map\n|}\n\nAtlas\u00a0pages\u00a0are map groupings\nThe Wikiatlas is based on some simple ideas. All maps are grouped, in the manner of a World atlas, into pages stored with the name \"Atlas of...\" for each continent, such as:\n\nSimilar page titles \"Atlas of...\" cover each nation or region, such as:\n\nSome of the U.S. states also have Wikiatlas pages, such as:\n\nSimilarly, hundreds of other Wikiatlas pages are named as \"Atlas of...\" for over 200 nations and over 94 other regions of the World.\n\nWikiatlas page format\nMany pages in the Wikiatlas are arranged in a distinctive format: with maps displayed down the left-side of the page, and explanatory text presented","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"experimental data\".\n\nReferences\n\n \n Basu D. (1980b). \"The Fisher Randomization Test\", reprinted with a new preface in Statistical Information and Likelihood : A Collection of Critical Essays by Dr. D. Basu ; J.K. Ghosh, editor. Springer 1988.\n \n Salsburg D. (2002) The Lady Tasting Tea: how statistics revolutionized science in the Twentieth Century W.H. Freeman \/ Owl Book. \n\nExperiments\nStatistics","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Regression analysis is a field of statistics. It is a tool to show the relationship between the inputs and the outputs of a system. There are different ways to do this. Better curve fitting usually needs more complex calculations. \n\nData modeling can be used without knowing about the underlying processes that have generated the data; in this case the model is an empirical model. Moreover, in modelling, knowledge of the probability distribution of the errors is not required. Regression analysis requires assumptions to be made regarding probability distribution of the errors. Statistical tests are made on the basis of these assumptions. In regression analysis the term \"model\" embraces both the function used to model the data and the assumptions concerning probability distributions.\n\nRegression can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. These uses of regression rely heavily on the underlying assumptions being satisfied. Regression analysis has been criticized as being misused for these purposes in many cases where the appropriate assumptions cannot be verified to hold.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In an experiment, the variables used can be classed as either dependent or independent variables. The dependent variable is the possible outcome of the experiment; the effect. It depends on what happens to other variables in the experiment. The dependent variable is basically the part that you are changing. For example, if you want to know how much light a plant needs to grow, the amount of growth is the dependent. If you wanted to see if a plant would grow better in hot or cold areas your independent variable would be the temperature of the air.\n\nThe independent variable is the variable that you have control over, what you can choose and manipulate. It is usually what you think will affect the dependent variable. In some cases, you may not be able to manipulate the independent variable. It may be something that is already there and is fixed, something you would like to evaluate with respect to how it affects something else, the independent variable like color, kind, time. As an example, you are interested in how","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A. (1996). Numerical methods for least squares problems (Vol. 51). SIAM.\n\nMathematical approximation\nStatistics","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\n\nIn addition, each model has parameters, which influence its performance. Some of the models make a number of assumption about the data; they were developed for a very special purpose. Using such a model for a different purpose may not yield good results.\n\nComputer science","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A narrative is a literary term for the events that make up a story. It is the way the events connect and make up the plot of a story or book.\n\nA narrative reports connected events, presented to the person reading or listener in a sequence of written or spoken words.\n\nA common term for narrative is plot. The study on structure in narratives is called narratology.\n\nRelated pages\nNarrative poetry\n\nReferences\n\nFiction\nWriting\nNarratology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Narratology is the study of structure in narratives. The theory of narrative or narratology was developed in the 1960s. Narratology is based on the idea of a common literary language. Narratives are found and told through oral and written language. Narratology has helped to make it easier to understand the how and why of narrative.\n\nReferences\n\nOther websites\nhttp:\/\/wikis.sub.uni-hamburg.de\/lhn\/index.php\/Main_Page  - The Living Handbook of Narratology\nhttp:\/\/www.units.miamioh.edu\/technologyandhumanities\/narratology.htm  - Notes on Narratology\nhttp:\/\/www.nou-la.org\/ling\/1975a-theonarreme.pdf - THBORIE DES NARRGMES ET ALGORITHMES NARRATIFS\n\nNarratology\nWriting","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"has the disadvantage that the same list of values does not have a well-defined, deterministic median.\n\nMedian and mean \nMedian and mean are different in several ways. Mean is a better measure in many cases, because many of the statistical tests can use mean and standard deviation of two observations to compare them, while the same comparison cannot be performed using the medians.\n\nMedian is more useful when the variance of the values is not important, and we only need a central measure of the values. If the maximum value of a set of numbers changes while the other numbers of this set are kept the same, the mean of this set of numbers changes, but the median does not.\n\nAnother advantage of median is that it can be calculated sooner when we are studying survival data. For example, a researcher can calculate the median survival of patients with a kidney transplant, when half the patients participated in his study die. Calculating the mean survival requires continuing the study, and following all the patients until their death.\n\nExample","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, Poisson distribution is a probability distribution. It is named after Sim\u00e9on Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, the binomial distribution is a probability distribution which models the probabilities of having a certain number of successes among n identical trials (each having p as the probability of success). It is also written as . The variables n and p are thus the two parameters of a binomial distribution.\n\nThe binomial distribution has discrete values. It counts the number of successes in yes\/no-type experiments. Each of these experiment, also called Bernoulli trial, either results in success or failure. Examples of binomial distribution include:\n Tossing a coin 10 times, and counting the number of face-ups. (n=10, p=1\/2)\n Rolling a dice 10 times, and counting the number of sixes. (n=10, p=1\/6)\n Counting the number of green-eyed people among 500 randomly chosen people (assuming that 5% of all people have green eyes). (n=500, p=0.05)\nIn order","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A technique may refer to:\n\n Technology, the use of the knowledge of tools and crafts\n Skill, a learned ability to do something","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In mathematics, the Fourier inversion theorem says that for many types of functions it is possible to recover a function from its Fourier transform. Intuitively it may be viewed as the statement that if we know all frequency and phase information about a wave then we may reconstruct the original wave precisely.\n\nMathematical theorems","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"subspace methods: principles and analysis. OUP Oxford.\n\nFields of mathematics\nTechnology\nLinear algebra\nComputer science\nComputing","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Statistical inference  is the statiscal process of drawing conclusions from data. \n\nExamples would be the randomness of the data, observational errors,  sampling variation, and other issues.\n\nScope\nFor the most part, statistical inference makes statements about populations, using data drawn from the population of interest by some form of random sampling. The result is some kind of statistical proposition, such as:\n an estimate; i.e., a particular value that best approximates some parameter of interest\n a confidence interval. That is an interval from a dataset such that, under repeated sampling, the interval would contain the true parameter value with the probability at the stated confidence level\n a credible interval; i.e., a set of values containing, for example, 95% of samples would include the true value of the parameter. \n rejection of a hypothesis\n clustering or classifying data points into groups\n\nReferences \n\nStatistics","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the idea to a prospective producer, director, or composer.\n\nScenarios are also used in policy planning, and when trying out strategies against uncertain future developments. Here the key idea is for the scenario to be an overview, a summary, of a projected course of action, events or situations. Scenarios are widely used by organizations of all types to understand different ways that future events might unfold.\n\nTheater\nPlanning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a framework for speculation on the next thirty-three years. MacMillan. . With Anthony Wiener.\n1968 Can we win in Viet Nam?. Praeger. Kahn with four other authors: Gastil, Raymond D.; Pfaff, William; Stillman, Edmund; Armbruster, Frank E. \n1970. The Emerging Japanese Superstate: challenge and response. Prentice Hall. \n1971. The Japanese challenge: The success and failure of economic success. Morrow; Andre Deutsch. \n1972. Things to come: thinking about the seventies and eighties. Macmillan. . With B. Bruce-Briggs.\n1973. Herman Kahnsciousness: the megaton ideas of the one-man think tank. New American Library. Selected and edited by Jerome Agel. \n1974. The future of the corporation. Mason & Lipscomb. \n1976. The next 200 Years: a scenario for America and the world. Morrow.","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is as follows: at any party with at least six people, there are three people who are either (a) mutual acquaintances (each one knows the other two) or (b) mutual strangers (each one does not know either of the other two). \n\nRamsey theory is now a complete branch of mathematics.\n\nNotes \n\nGraph  theory","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"differ rather in how far each sector extends from the center of the circle.\nThe polar area diagram is used to plot cyclic phenomena (e.g., count of deaths by month).\nFor example, if the count of deaths in each month for a year are to be plotted then there will be 12 sectors (one per month) all with the same angle of 30 degrees each. The radius of each sector would be proportional to the square root of the death count for the month, so the area of a sector represents the number of deaths in a month.\nIf the death count in each month is subdivided by cause of death, it is possible to make multiple comparisons on one diagram, as is seen in the polar area diagram used by Nightingale.\n\nLimitations \n\nPie charts are not the most accurate way to show data: that has been known for some time. Pie charts should be used only when the sum of all categories is meaningful, for example if they represent proportions.\n\nPie charts are rare","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In log-log graphs, both x-axis and y-axis have logarithmic scales instead of linear scales. Log-log graphs have applications in many disciplines for data representation and analysis.\n\nReferences \n\nLogorithms","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A fishery is a place for raising fish or harvesting fish. There are all kinds of fisheries in the world, like salmon fisheries. Fisheries might take fish from the wild or breed them.\n\nFish","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"possible to farm seafood in a way that has very little impact to the environment. Such operations limit habitat damage, disease, escapes of farmed fish and the use of wild fish as feed.\n\nReferences \n\nAquaculture","type":"Document"}]}}