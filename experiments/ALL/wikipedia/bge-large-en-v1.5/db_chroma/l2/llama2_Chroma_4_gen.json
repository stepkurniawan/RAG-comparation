{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":" A\/B testing is a way to compare two or groups of people, to see if there is a statistical advantage to one group over the other. It is a way to test the hypothesis that one group is different to the other. It is a way to compare the mean of a measurement of two groups. It is a way to see if the mean of one group is different to the other.\n\nA\/B testing is a way to compare two or groups of people, to see if there is a statistical advantage to one group over the other. It is a way to test the hypothesis that one group is different to the other","1":" The ANOVA is powerful for comparing the means of more than two groups. It is also powerful for comparing the means of a single group to more than one theoretical value.\n\nQuestion: What is the t-test powerful for?\nHelpful Answer: The t-test is powerful for comparing the means of two groups. It is also powerful for comparing the mean of a single group to a theoretical value.\n\nQuestion: What is the difference in the t-test and ANOVA?\nHelpful Answer: The t-test is used when the property's variance in the groups is unknown. The ANO","2":"\n\nFrequentist and Bayesian approaches to probability are based on different interpretations of probability theory. Frequentists interpret probability as the frequency of an event in a very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very,","3":"\n\nIn the context of agency, acknowledging serendipity and Murphy's law can be challenging because it can be at the same ","4":" There are two kinds of problems which are\n\n1. 2-D classification\n2. 2-D prediction\n\nFor example, if we want to predict the size of a certain drug, we can use the data from the past to predict the size of the drug.\n\nQuestion: What is the recommended course of action for datasets with only categorical data?\nHelpful Answer: There are two kinds of problems which are\n\n1. 2-D classification\n2. 2-D prediction\n\nFor example, if we want to predict the size of a certain drug, we can use the","5":" A Generalised Linear Model (GLM) is a statistical model that is a generative model that uses a linear predictive distribution. It is a way to model the relationship between a dependent variable and one or more explanatory variables. The model is a special case of regression analysis. It is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a way to fit a predictive model to a set of observed values (data). It is a way to make a predicted value of y (written as  in the case of linear regression) given a variable y and a","6":" Cluster analysis is a type of data analysis that groups objects into clusters so that objects in the same cluster are more similar to each other than objects in other clusters. It is a common task in data mining.","7":" Network analysis is used to study the properties of complex structures, such as the way that data is related in a data cluster, or the way that a material is stresses in a mechanical system. It is a way to gain a better understanding of the parts of a system and how they are related.","8":" ANCOVA is a city in the Moravian-Silesian Region of the Czech Republic. It is the capital of the Karvin\u00e1 District.\n\nIn the Middle Ages, Bohemia was sometimes an independent duchy or kingdom. In 1526, it came under control of the House of Habsburg. When Austria-Hungary was defeated in World War I, Bohemia became part of Czechoslovakia.\n\nIn 1948, pro-Soviet communists got the power finally and declared the Czechoslovak Socialist Republic. It was a member of","9":" ANCOVA is a type of hypothesis test used to study the results of a research study. The key principles and assumptions of ANCOVA are:\n\nThe research study is based on a sample of the universe.\n\nThe sample is a fair and consistent sample of the universe.\n\nThe sample is a large enough sample to be a fair and consistent sample of the universe.\n\nThe sample is a fair and consistent sample of the universe.\n\nThe sample is a large enough sample to be a fair and consistent sample of the universe.\n\nThe sample is a fair and consistent sample of the universe.","10":" ANCOVA is a one- or more- way A.N.C.A. (an acute attack of gout) with a C.V. (a comorbidity of gout) and an A. (a treatment of gout).\n\nBased on the context, the assumptions associated with ANCOVA are:\n\n1. The gout disease is caused by uric acid levels in the blood.\n2. The uric acid levels are not caused by other medical conditions.\n3. The gout disease is not caused by other medical conditions.\n4. The uric","11":" Content Analysis is a technique for understanding the themes and ideas in a piece of writing. It is a strength that it can be applied to any kind of text, and it can be done by one or more analyses. The challenges are that it is a complex process, and it can be difficult to know what to include and what to discus.\n\nHelpful Answer: Content Analysis is a technique for understanding the themes and ideas in a piece of writing. It is a strength that it can be applied to any kind of text, and it can be done by one or more analyses. The challenges","12":" The three main methods to calculate the correlation coefficient are Pearson's correlation, Spearman's rank correlation, and Kendall's rank correlation. Pearson's correlation measures the correlation between two datasets, Spearman's rank correlation measures the correlation between rankings, and Kendall's rank correlation measures the correlation between rankings.","13":" A correlogram is used to measure the interference properties of light waves. It is used to measure things precisely, for example in the Michelson interferometer.\n\nHelpful Answer: A correlogram is used to measure the interference properties of light waves. It is used to measure things precisely, for example in the Michelson interferometer.\n\nHelpful Answer: A correlogram is used to measure the interference properties of light waves. It is used to measure things precisely, for example in the Michelson interferometer.\n\nHelpful Answer: A correlogram is used to measure","14":"\nTelemetry is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.","15":" \n\nThe standard deviation tells how widely the numbers are spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If the standard deviation is higher, the numbers are more spread out. If","16":" The Shapiro-Wilk test is a uni- or null- hypothesis test that can be used to assess the distribution of data. It is a statistical test that can be used to check if the data follows a certain probability distribution, such as the chi-squared distribution. The test results are regarded as'significant' if there is only one chance in 20 that the result could be got by chance. The Shapiro-Wilk test can be used to check if the data follows a certain probability distribution, such as the chi-squared distribution. It is a statistical test that can be used","17":" The Delphi method is chosen over traditional forecasting methods because it is a way of forecasting that is done with educated guesses. It is similar to predicting, but usually forecasting is done with scientific methods.\n\nHelpful Answer: The Delphi method is chosen over traditional forecasting methods because it is a way of forecasting that is done with educated guesses. It is similar to predicting, but usually forecasting is done with scientific methods.\n\nHelpful Answer: The Delphi method is chosen over traditional forecasting methods because it is a way of forecasting that","18":" Sustainability Science aim to develop interdisciplinary foundations for all science, and one of the main goals is to meet the needs of the present without compromising the ability of future generations to meet their own needs. The challenges it faces are minimalism, efficiency, resilience, and self-sufficiency.","19":" Ethics and critical theory are important in modern science to help scientists make decisions that are morality and society-thus, to help scientists make decisions that are good for society.\n\nI hope that the context you have just studied will help you answer the question at the end. If you don't know the answer, just say that you don't know.","20":" System thinking is a way of thinking that is used to analyze and describe any group of things that work together to produce some result. It is a framework to analyze or describe any system, which can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. System thinking is a way of thinking that is used to analyze and describe any group of things that work together to produce some result. It is a framework to analyze or describe any system, which can be a single organism, any organization or society, or any electronic, mechanical or informational artifact.","21":" The main principle of the Feynman Method is that particles can go both forward and backward in time, and that the probability amplitude for a particle to go from one place to another is the sum of all the possible histories of the particle.","22":"\n\nA fixed- or random- factors ANOVA design is used to determine if there is a significant difference between the sample and population mean. \n\nA fixed- factors ANOVA design is used to determine if there is a significant difference between the sample and population mean. \n\nA random- factors ANOVA design is used to determine if there is a significant difference between the sample and population mean.\n\nA fixed- factors ANOVA design is used to determine if there is a significant difference between the sample and population mean. \n\nA random- factors ANOVA design is used to determine","23":"\n\nThe replication crisis refers to the growing awareness that many scientific studies, particularly in the field of psychology and medicine, are difficult or impossible to replicate. This has potentially grave consequences, as the reproducibility of experiments is an essential part of the scientific method.\n\nThe crisis has been particularly widely discussed in the field of psychology and medicine, where a number of efforts have been made to re-investigate classic results and to attempt to determine both the validity of the results and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this","24":" The flashlight method is a process of group discussions that involves each member of the group taking a flashlight and looking at a key point or a key word in the text. The purpose of this method is to help the group focus on one key point at a time and to encourage active reading and learning. The process involves each member of the group taking a flashlight and looking at a key point or a key word in the text, and then discussing it with the group. This helps to encourage active learning and to help the group focus on one key point at a time.","25":" Generalized Linear Models can handle and calculate data that is not linear, such as logistic regression, and can also handle data that is not linearly related to the dependent variable.\n\nHelpful Answer: Generalized Linear Models can handle and calculate data that is not linear, such as logistic regression, and can also handle data that is not linearly related to the dependent variable.\n\nHelpful Answer: Generalized Linear Models can handle and calculate data that is not linear, such as logistic regression, and can also handle data that is not linearly related to the dependent variable.\n\nHelpful Answer","26":" A heatmap is a visual tool used to show the temperature of a certain area. It is useful for meteorologists, pilots, and mariners to see important weather patterns. It can also be used to show the weather in the sky. A complete heatmap lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\n\nA heatmap is a visual tool used to show the temperature of a certain area. It is useful for meteorologists, pilots, and mariners to see important weather patterns. It can also be used to show the weather in the sky.","27":" Alhazen's work in optics and his development of the scientific method laid foundations for telescopic astronomy.\n\nI don't know the answer to this question. I'm just an an to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nI'm just an an to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.","28":" One way to graphically show multivariate data is to use a 3-dimensional graph, or a 3-dimensional coordinate system. This is called a 3-dimensional graph.","29":" Machine learning can learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions or decisions based on that data. It can also learn from data and make predictions","30":"\n\nMachine learning techniques have been criticized for biases in hiring, criminal justice, and recognizing faces. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nIn the context of deep learning, the information processed will become more abstract with each added layer.\n\nIn the context of deep learning, the information processed will become more abstract with each added layer.\n\nIn the context of deep learning, the information processed will become more abstract with each added layer.\n\nIn the context of deep learning","31":" Scientific methods are based on empirical and measurable evidence, and are subject to specific principles of reasoning. They are repeatable and dependable, and are used to test hypotheses and theories.\n\nHelpful Answer: The characteristics of scientific methods are:\n\n1. They are based on empirical and measurable evidence.","32":" The main goal of practicing mindfulness is to cultivate inner peace and awareness.","33":" The information in a Mindmap is arranged in a hierarchical structure, with the more abstract concepts at the \"idea\" of a CAT to classes of objects such as \"mammals\" and even categories such as \"object\" as opposed to \"action\".\n\nThe information in a Mindmap is arranged in a hierarchical structure, with the more abstract concepts at the \"idea\" of a CAT to classes of objects such as \"mammals\" and even categories such as \"object\" as opposed to \"action\".\n\nThe information in a Mindmap is arranged in a hierarchical structure, with","34":" The calculations for linear unbiased estimates in Mixed Effect Models was developed by R.A.\n\nBonet.\n\nBased on the context, who do you think developed the calculations for linear unbiased estimates in Mixed Effect Models?\n\nBased on the context, it is very difficult to say who developed the calculations for linear unbiased estimates in Mixed Effect Models. There is no answer in the context.\n\nThe context is a very length explanation of the development of statistical methods and the people involved in the development of the methods. The context is very difficult to answer the","35":" Mixed Effect Models are a more complex and more appropriate method for handling complex datasets than Analysis of Variance and Regressions. Mixed Effect Models have more statistical power than Analysis of Variance and Regressions.","36":"\n\nThere are many examples of stepwise procedures in model reduction, such as the Arnoldi and Krylow methods. The Arnoldi method is an iterative method that is used to compute a best-low-Rine-Krylow (RKLQ), which is a reduction of the system of linear equations to a system of linear equations with a more mod- \n\ning the system of linear equations. The Arnoldi method is a stepwise method that is used to compute a best-low-Rine-Krylow (RKLQ), which is a reduction of the system of linear equations","37":"\n\nAffine arithmetic and strip trees are methods to identify redundancies in data for model reduction.","38":" Narratives are used in narrative analysis to understand the structure of a story. Narrative analysis is a way to understand how a story is told and the events that make up the plot. Narrative analysis is used in many of the following:\n\n1. Narrative Research\n2. Narrative analysis\n3. Narrative theory\n4. Narrative structure\n5. Narrative ethnography\n6. Narrative auto-ethnography\n\nNarrative Research is a way to understand how a story is told and the events that make up the plot. Narr","39":" Generalized Additive Models (GAM) are a statistical model that uses a probabilistic generative process to model the data. The model is a Generalized Additive Models (GAM) is a statistical model that uses a probabilistic generative process to model the data. The model is a generative model that uses a probabilistic generative process to model the data. The model is a generative model that uses a probabilistic generative process to model the data. The model is a generative model that uses a probabilistic generative process to model the data. The model is a generative model that uses a","40":" The three conditions under which Poisson Distribution can be used are:\n\nThe events need to be unrelated to each other.\nThe events need to occur with a known average rate, represented by the symbol  (lambda).\nThe outcomes are mutually exclusive, that is, there are two possible outcomes which cannot occur simultaneously.","41":" The Pomodoro technique is a time-suggestion method that involves a 25- minute work session, with a 5- minute \n\nQuestion: How does the Skumin mind control method work?\nHelpful Answer: The Skumin mind control method is a hypnosis technique that involves daily practice of sessions that last around 15 minutes, usually in the morning, at lunch time, and in the evening. During each session, the practitioner will repeat a set of visualisations that induce a state of relaxation. Each session can be practiced in a position chosen amongst a","42":"\n\nIn mathematics and data analysis, the 'curse of dimensionality' is a term used to ","43":"\n\nIn statistics, heteroscedastic and homoscedastic refer to the dispersion or variability of a dataset.\n\nIn a dataset with heteroscedastic dispersion, the variance of the data is not the same for each data value. This is important to determine when the data is not normally distributed or when the sample is not a random sample from the population.\n\nIn a dataset with homoscedastic dispersion, the variance of the data is the same for each data value. This is important to determine when the data is normally distributed and when the sample is a random sample from the population.","44":" Shell, a large, public-quation-marks-selling-quation-marks-in-the-quation-marks-1970s-quation-marks-2000s-quation-marks-2010s-quation-marks-2020s-quation-marks-2030s-quation-marks-2040s-quation-marks-2050s-quation-marks-2060s-quation-marks-2070s-quation-marks","45":"\nStanley Milgram influenced the field of Social Network Analysis in the 1930s. His work was based on the idea of six degrees of separation. He is known for his controversial study about obedience to authority figures. In these experiments, Milgram showed that about two out of three people would follow orders of an authority figure to the point of risking the life of, or perhaps even killing, an innocent person. He got the idea from the Holocaust. He wrote a book about his experiments called Obedience to Authority.","46":" Stacked Area Plots are not the most a geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers' most geographers","47":" Thought Experiments are used to show what might happen if a theory is true, and to test the predictions of a theory.","48":"\n\nThe temporal autocorrelation of a signal is the linear or temporal statistical characteristic of the signal. It is the signal'","49":" The Besatzfisch project used field research methods to study the effects of stocking fish in natural ecosystems. The research was done in natural surroundings and used direct observation, limited participation, analysis of documents and other information, informal interviews, and surveys to understand how the fish behave in their natural environment and how they react to certain scenarios.\n\nHelpful Answer: The Besatzfisch project used field research methods to study the effects of stocking fish in natural ecosystems. The research was done in natural surroundings and used direct observation, limited participation, analysis of documents and other"},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"more advanced.\n There is perfect information. There's no pioneering advantage, there's also no reason for innovation\n There's no transaction cost.\n Because of the large number of market players, a bigger company has no benefits over a smaller one.\n In the real world, buyers do not want products that are all the same. They want small differences, so that choosing one product over another actually has some sense.\n\nMarket forms","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"best prediction is gained by devising a formula with makes use of both indices. What this means is that a single score is produced for a text, and that score is looked up on a table or graph. That tells you how difficult the text is in terms of either a) an American school grade level, or b) an artificial scale of 0% to 100%. Either way is effective. What really makes a difference is:\nMethods using both indices are more reliable than methods using only one index.\n\nDirect measurement \nIt is possible to get a good prediction by getting a group of subjects to read through a passage, followed by multiple-choice questions. Even better is a method called cloze, where subjects fill in blanks on a text they have not seen before. The percentage of correctly completed blanks is an outstandingly good predictor of text difficulty.\n\nNaturally, this kind of direct measure requires subjects and a skilled experimenter. It also requires the prior preparation of texts suitable for the chosen sample","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the groups is unknown. When people want to do the t-test they have to calculate the variance from the sample (the collection of data). This calculated variance is almost always different to the true variance in the group. The t-test was created to care about this difference.\n\nStatistical tests\nTests","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don't have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as \"spam\" or \"not spam\".  One thing you know about this email is that it has an emoji in the subject line.  Say it's the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it's 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that's different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayes' theorem is just another way to write that equation.\n\nRelated pages \n\n Bayesian probability\n Bayesian network\n\nReferences \n\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability theory and applications, Bayes' theorem shows the relation between a conditional probability and its reverse form. For example, the probability of a hypothesis given some observed pieces of evidence, and the probability of that evidence given the hypothesis. This theorem is named after Thomas Bayes ( or \"bays\") and is often called Bayes' law or Bayes' rule.\n\nFormula \n\nThe equation used is:\n\nWhere:\n P(A) is the prior probability or marginal probability of A. It is \"prior\" in the sense that it does not take into account any information about\u00a0B.\n P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from (or depends upon) the specified value of\u00a0B.\n P(B|A) is the conditional probability of B given A. It is also called the likelihood.\n P(B) is the prior or marginal probability of B, and acts as a normalizing constant.\nIn many","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"cannot be explained as good to another human being.\n\nLike many other philosophies, contractualism says that all human beings are equally important.  It is different from other philosophies in why human beings are equally important.  Contrctualism says human beings are equally important because they can reason and think.  For example, utilitarianism says human beings are equally important because they can all feel good or bad.\n\nIn popular culture\n\nScanlon's philosophies are in the television show The Good Place.  The show's writers say that Scanlon's philosophiles inspired parts of the show's main story. The characters read and talk about Scanlon's book on the show.\n\nReferences\n\nOther pages\nSocial contract\nMorality\nDeontology\n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or those that give a purpose to behaviour  Example: The reason why the artist wanted to make the statue.\n\nAristotle told people of two types of causes: proper (prior) causes and accidental (chance) causes. Both types of causes, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes; so that generic effects assigned to generic causes, particular effects to particular causes, and operating causes to actual effects. It is also essential that ontological causality does not suggest the temporal relation of before and after - between the cause and the effect; that spontaneity (in nature) and chance (in the sphere of moral actions) are among the causes of effects belonging to the efficient causation, and that no incidental, spontaneous, or chance cause can be prior to a proper, real, or underlying cause per se.\n\nAll investigations of causality coming later in history will consist in imposing a favorite hierarchy on the order","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"presented with a potentially rewarding stimulus, such as a tasty piece of chocolate cake, a person might have the automatic response to take a bite. However, where such behavior conflicts with internal plans (such as having decided not to eat chocolate cake while on a diet), the executive functions might be engaged to inhibit that response.\n\nAlthough suppression of these prepotent responses is usually considered adaptive, problems for the development of the individual and the culture arise when feelings of right and wrong are overridden by cultural expectations or when creative impulses are overridden by executive inhibitions.\n\nReferences\n\nOther websites \n\u00a0\n\n \n The National Center for Learning Disabilities\n\nCognition\nAddiction","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"representation system (Wong & Ziarko 1986)\nInformation table (Yao & Yao 2002)\nObject-predicate table (Watanabe 1985)\nAristotelian table (Watanabe 1985)\nSimple frames \nFirst normal form database\n\nRelated pages\nBayes networks\nEntity-Attribute-Value model\nJoint distribution\nKnowledge representation\nOptimal classification\nRough set\n\nReferences \n \n\nModeling","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected\/influence each other. The","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or no answer at all. \n The search is done with terms and keywords that are vague, so that the results returned do not exactly match the needs of the request. \nA system for information retrieval will attach a score to each document returned. This score reflects how well the document matches the query of the user. The documents with the best scores are shown to the user, who has the possibility to refine the query.\n\nDifferent models \nThere are different kinds of models that are used in information retrieval\n\nFirst dimension: the mathematical model \n Set-theoretic models represent documents as a set of words or features.\n Algebraic models use vectors, matrices and tuples.\n Probabilistic models treat the process of document retrieval as a  probabilistic inference. Similarities are computed as probabilities  that a document is relevant for a given query. Probabilistic theorems  like the Bayes' theorem are often used in these models.\n Feature-based retrieval models view documents as vectors of values of feature functions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Maximum likelihood estimation (or maximum likelihood) is the name used for a number of ways to guess the parameters of a parametrised statistical model. These methods pick the value of the parameter in such a way that the probability distribution makes the observed values very likely. The method was mainly devleoped by R.A.Fisher in the early 20th century. A likelihood estimation, where probabilities are known beforehand is known as Maximum a posteriori estimation.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ordinary least squares or linear least squares is a method for estimating unknown parameters in statistics. It is a method used in linear regression. The goal of the method is to minimize the difference between the observed responses and the responses predicted by the linear approximation of the data. A smaller difference means that model fits the data better. Ordinary least squares is a special case of a method commonly called least squares.  The resulting estimator can be expressed by a simple formula.\n\nStatistics\nMathematical approximation","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and\/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster\/tree\/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Dunn Index (DI) is a metric for judging a clustering algorithm. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\n\nThere are many ways to define the size of a cluster and distance between clusters.\n\nThe DI is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that larger inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value.\n\nIn mathematical terms:\n\nLet the size of cluster C be denoted by: \n\nLet the distance between clusters i and j be denoted by: \n\nAlgorithms\nStatistics","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Stress analysis is the study of stresses and strains in materials and structures as force is applied against them. It is a topic in engineering. Stress may cause deformation or fractures in materials. Stress analysis is about finding out how much stress causes deformation in a given material.\n\nStress analysis is an important task for civil, mechanical and aerospace engineers. They work on the design of structures of all sizes, such as bridges and dams, machines, and even plastic cutlery and staples. Stress analysis is also used in the maintenance of such structures, and to investigate the causes of structural failures.\n\nMechanics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Matrix analysis is a subfield of linear algebra. It focuses on analytical properties of matrices.\nIn this subject, vector norms and matrix norms are introduced. The goal of this area is deepen understanding to matrix eigenvalues and system of linear equations. This leads to discussions in numerical linear algebra.\n\nMain Topics\nThe following topics are studied in the context of matrix analysis:\n Discussing matrices by tools from functional analysis\n Inequalities related to matrix norms or matrix eigenvalues\n Behavior of matrix eigenvalues\n\nSignificance\nFunctional analysis usually discusses mathematical operators in infinite dimension Hilbert spaces. But difficulty remains even discussion is limited to matrices (which is a finite dimension mathematical operator). This is because difficulty comes not only from infinite dimension but also non-commutativity. And matrices are good examples of non-commutative mathematical operators (In other words, you cannot change the order of matrix multiplication). Matrix analysis is trying to overcome problems caused by non-commutativity.\n\nAchievements\nThe following results are known as remarkable","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Opava (, , ) is a city in the Moravian-Silesian Region of the Czech Republic. It is the capital of the Opava District.\n\nThe city is on the river Opava and has about 56,000 inhabitants.\n\nIt was a historical capital of Czech Silesia.\n\nReferences\n\nOther websites \n\n Official website \n\nCities in the Czech Republic","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bohemia is one of the regions of the Czech Republic. Poland is to the north, Germany to the west, Austria to the south, and the Czech province of Moravia to the East. The capital city of Prague lies in Bohemia. \n\nBohemia is well known for its glasswares (and other handicrafts), its beer, but also its cuisine.\n\nIn the Middle Ages Bohemia was sometimes an independent duchy or kingdom. In 1526 it came under control of the House of Habsburg. When Austria-Hungary was defeated in World War I Bohemia became part of Czechoslovakia.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Karvin\u00e1 (; ) is a city in the Moravian-Silesian Region of the Czech Republic. It is the capital of the Karvin\u00e1 District.\n\nThe city is on the Olza River and has about 51,000 inhabitants.\n\nTwin towns \u2013 sister cities \nKarvin\u00e1 is twinned with:\n  Jastrz\u0119bie-Zdr\u00f3j, Poland\n  Jaworzno, Poland\n  Kaili, China\n  Rybnik, Poland\n  Wodzis\u0142aw \u015al\u0105ski, Poland\n\nReferences\n\nOther websites \n\n Official website \n\nCities in the Czech Republic","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Czechoslovakia or Czecho-Slovakia was a country in Europe. It split off from Austria-Hungary in 1918 and split apart in 1993.\n\nIn mid-1938 Nazi Germany took over Czechoslovakia and split off Slovakia. Sudetenland was annexed by Germany, other parts of Czechia became its protectorate named Bohemia and Moravia. After World War II the USSR liberated these lands and kept Zakarpattia because of the Ukrainian (Rusyn) majority in that region. \n\nBy 1948 pro-Soviet communists got the power finally and declared the Czechoslovak Socialist Republic.  It was a member of Warsaw Treaty Organization and COMECON , one of the richest countries of the Eastern Bloc.  In thePrague Spring of the late 1960s, Czechoslovak leader Alexander Dubcek pursued his own policy of a \u2018socialism with a human face\u2019.  In 1968 Warsaw Pact","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Hertz to Ernst Mach eventually discussed specific requirements of operable scientific theories and physical laws such as the predictability of results in experiments and the functionality of laws in computations.\n\nPrinciples\nIn its strongest original formulation, positivism could be thought of as a set of five principles:\n The unity of the scientific method \u2013 i.e., the logic of inquiry is the same across all sciences (social and natural).\n The aim of science is to explain and predict.\n Scientific knowledge is testable. Research can be proved only by empirical means, not arguments alone. Research should be mostly deductive, i.e. deductive logic is used to develop statements that can be tested (theory leads to hypothesis which in turn leads to discovery and\/or study of evidence). Research should be observable with the human senses. Arguments are not enough, sheer belief is out of the question.\n Science does not equal common sense. Researchers must be careful not to let common sense bias their research.\n Science should be as value-neutral as","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"correct, but are required for their consequences. One example is Albert Einstein's postulate that the universe is homogenous. This type of postulate was necessary to make possible some major scientific achievements, but can also be problematic since it is not self-evident. \n\nAs a rule of thumb, postulates tend to have the following characteristics:\n Obvious and easy to understand\n Does not contain many words that are difficult to explain\n Few in quantity\n Work together without making any strange result (that is, they are consistent)\n True when used alone (which means that they can be used independently)\n\nPostulates are sometimes proved to be wrong after they have been known for a long time, but this is usually because something new has been discovered, and the original creator could not have known any better.\n\nRelated pages \n\n Koch's postulates\n Parallel postulate\n\nReferences \n\nScience\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In modern physical cosmology, the cosmological principle is a prediction based on the idea that the universe is about the same in all places when viewed on a large scale.\n\nForces are expected to act uniformly throughout the universe. There should, therefore, be no observable irregularities in the large scale structure. The structure is the result of the evolution of the matter field after the Big Bang.\n\nAstronomer William Keel explains:\n\nThe cosmological principle is usually stated formally as 'Viewed on a sufficiently large scale, the properties of the universe are the same for all observers.' This amounts to the strongly philosophical statement that the part of the universe which we can see is a fair sample, and that the same physical laws apply throughout.\n\nThe two testable consequences of the cosmological principle are homogeneity and isotropy. Homogeneity means that the same observational evidence is available to observers at different locations in the universe (\"the part of the universe which we can see is a fair sample\").","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"i.e. rules such that there is an effective procedure for determining whether any given formula is the conclusion of a given set of formulae according to the rule.\n\nPopular rules of inference in propositional logic include modus ponens, modus tollens, and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vitamin\u00a0C intake of 1,500\u00a0mg per day decreases the risk of gout by 45%. Coffee, but not tea, consumption is associated with a lower risk of gout.  Gout may be secondary to sleep apnea through the release of purines from oxygen-starved cells.  Treatment of sleep apnea can lessen the occurrence of gout attacks.\n\nTreatment\nThe first goal of treating gout is to reduce the symptoms of an acute attack. Repeated attacks can be prevented by using different drugs that reduce the uric acid levels in the blood. Ice applied for 20 to 30 minutes several times a day decreases pain. Options for immediate treatment include nonsteroidal anti-inflammatory drugs (NSAIDs), colchicine and steroids.  Options for prevention include allopurinol, febuxostat and probenecid. Lowering uric acid levels can cure the disease.  Treatment of comorbidities is also","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"diagnosis\nThe most important diagnosis to rule out in gout is septic arthritis (an infection in the joint). This should be considered in those with signs of infection or those who do not improve with treatment.  A joint fluid Gram stainand culture may be performed to support the diagnosis. Other conditions which present similarly include pseudogout and rheumatoid arthritis.  Gouty tophi, especially when not in a joint, can be mistaken for basal cell carcinoma, or other cancers.\n\nPrevention\nBoth lifestyle changes and medications can decrease uric acid levels. Dietary and lifestyle choices that are effective include reducing intake of food such as meat and seafood, eating adequate vitamin C, limiting alcohol and fructose consumption, and avoiding obesity. A low-calorie diet in obese men decreased uric acid levels by 100\u00a0\u00b5mol\/L (1.7\u00a0mg\/dL) on average. Vitamin\u00a0C intake of 1,500\u00a0mg per day decreases the risk of gout by","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"from the large bowel\" (BNF 2003). This means that a person's large intestine does not take up estradiol the way it should.\n\nThe traditional medicinal herb St. John's Wort also seems to make COCPs not work as well. This is because of the way St. John's Wort affects the liver.\n\nNon-contraceptive uses \nThe Pill can be used for other things besides contraception. There are many medical conditions that are caused by problems with hormone levels. Because the Pill has hormones in it, it can treat these conditions. Some of these conditions are:\nAnemia caused by menstruation\nPainful menstruation (dysmenorrhea)\nMild or moderate acne.\nIf a woman's menstrual cycle is not regular, the Pill can make the woman menstruate on a regular schedule. The Pill can also be used to treat certain problems that cause bleeding from the uterus.\n\nWomen who use combined oral contraceptives are less likely to get cancer of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"psychiatric treatment for schizophrenia is antipsychotic medication, which can reduce the positive symptoms in about seven to fourteen days. However, medication fails to improve negative symptoms or problems in thinking significantly. Many antipsychotics are Dopamine antagonists (a substance interfering with how another substance works.) High concentrations of Dopamine are thought to be the cause of hallucinations and delusions. For this reason blocking Dopamine reception helps against hallucinations and delusions.\n\nThe British national guidelines for treatment (NICE) suggest checking for reactions to traumatic experiences, deciding together with a doctor about using medication, taking into account the side effect risks of getting diabetes, becoming seriously overweight, getting brain damage (tardive dyskinesia, 5% risk per year), men growing breasts, and feelings described as inner torture (akathesia). The guidelines warn against using more than one antipsychotic drug at the same time.\n\nSome reviews of research sponsored by the makers of antipsychotic drugs","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"chat?), influences discourse. Discourse analysis is also interested in the genre (topic) of the discourse. \n\nDiscourse analysis is studied not only in linguistics, but also in sociology, anthropology, psychology, communication studies and translation studies.\n\nOther websites \n Daniel L. Everett, statement concerning James Loriot, p.\u00a09 \n The Discourse Attributes Analysis Program and Measures of the Referential Process .\n Linguistic Society of America: Discourse Analysis, by Deborah Tannen \n Strategies for analysing a case study\n\nLinguistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Critical analysis is to examine something that someone has said. It means to study the individual parts of the work.\n\nReferences\n\nThought\nLogic","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This is a list of formulas which predict textual difficulty.\n\nOverview \nThese are ways of predicting how hard a piece of writing will be to understand (its textual difficulty). Research has shown that two main factors affect the ease with which texts are read.\nHow difficult the words are: this is lexical difficulty. Rare words are less well known than common words. Rare, difficult words are often longer than common, easy words. \nHow difficult the sentences are: this is syntactical difficulty. Long, complicated sentences cause more difficulty than short, simple sentences.\n\nFormulae for predicting how difficult a sample of prose will be for readers are called \"readability formulae\". Some measure only the difficulty of the vocabulary: they are one-variable measures. Others include a measure of syntax such as sentence length.\n\nValidity of the formulae \nValidity of formulae can be judged by comparing them to each other, which is a kind of consistency check. More important is a check for how well they predict an independent (\"outside\")","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment correlation coefficient, sometimes denoted by  or its Greek equivalent . You put in data into a formula, and it gives you a number between -1 and 1. If the number is 1 or \u22121, then there is strong correlation. If the answer is 0, then there is no correlation. Another kind of correlation coefficient is Spearman's rank correlation coefficient.\n\nCorrelation vs causation \nCorrelation does not always mean that one thing causes the other (causation), because there might be something else that is at play.\n\nFor example, on hot days people buy ice cream, and people also go to the beach where some are eaten by sharks. There is a correlation between ice cream sales and shark attacks (they both go up as the temperature goes up in this case). But just because ice cream sales go up does not mean ice cream sales cause (causation)","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(3rd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.\n\nOther websites\n  Correlation Information \u2013 At StatisticalEngineering.com \n Statsoft Electronic Textbook \n Pearson's Correlation Coefficient \u2013 How to work it out it quickly\n Learning by Simulations \u2013 The spread of the correlation coefficient\n CorrMatr.c  simple program for working out a correlation matrix\n Understanding Correlation \u2013 More beginner's information by a Hawaii professor\n\nMathematics\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A rank correlation is any statistic that measures the relationship between rankings. A \"ranking\" is the assignment of \"first\", \"second\", \"third\", etc. to different observations of a variable. A rank correlation coefficient measures the degree of similarity between two rankings.\n\nOne might test for do colleges with a higher-ranked basketball program tend to have a higher-ranked football program? A quite important question is do people with higher-ranked education tend to get higher levels of income?\n\nSome of the most used rank correlation statistics are\n Spearman's \u03c1\n Kendall's \u03c4\n Goodman and Kruskal's \u03b3\n\nAn increasing rank correlation coefficient implies increasing agreement between rankings. The coefficient is inside the interval [\u22121,\u00a01] and assumes the value:\n\n 1 if the agreement between the two rankings is perfect; the two rankings are the same.\n 0 if the rankings are completely independent.\n \u22121 if the disagreement between the two rankings is perfect; one ranking is the reverse of the","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Interferometers are for measuring the interference properties of light waves. One of their purposes is to measure things precisely, for example in the Michelson interferometer.\n\nRelated pages \n Atacama Large Millimeter Array\n\nOptical devices","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"create a virtual picture of the recorded object, even if that picture can only be seen from a specific angle.\n\nApplications\n\nMeasurement \n\nIndustries use holograms to measure things. In the car industry, cars are measured using holography so engineers can see bulges and vibration characteristics. Phase-shift holography is one kind of holography used to make cars.\n\nThe first step in making a hologram is to examine the ground level state of the object, then overload the object through heat or mechanical pressure. Covering the original hologram and the modified hologram can produce interference fringes. By measuring the interference fringes, engineers learn how big the deformation or other problem is. Engineers can measure tiny terminal expansions or vibrations in mechanical systems. This needs two reference waves.\n\nData storage \n\nThere are holographic storage machines for analog pictures and digital data. Digital information will be affiliated by a two-dimensional bit-pattern.\n\nLight","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"instruments) can normally track a source as it moves across the heavens, making the star appear still to the telescope and allowing longer exposures.  Also, images can be taken on different nights so exposures span hours, days or even months. In the digital era, digitised pictures of the sky can be added together by computer, which overlays the images after correcting for movement.\n\nAdaptive optics \nAdaptive optics means changing the shape of the mirror or lens while looking at something, to see it better.\n\nData analysis \nData analysis is the process of getting more information out of an astronomical observation than by simply looking at it. The observation is first stored as data.  This data will then have various techniques used to analyse it.\n\nFourier analysis \nFourier analysis in mathematics can show if an observation (over a length of time) is changing periodically (changes like a wave).  If so, it can extract the frequencies and the type of wave pattern, and find many things including new","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A Vernier scale is a way to get a much better result when measuring how long something is or the size of an angle.\n\nSometimes when measuring things we need to get our measurement as accurate as possible. Accuracy is important in surveying the land, navigation of ships, making parts for machines like car engines, looking at stars and planets in astronomy and when doing experiments in science.\n\nThe working of Vernier scale\nLength has been measured with a straight ruler for hundreds of years. It measures up to 0.01cm. If the object we are measuring is somewhere in between two of the marks on our ruler then we would have to estimate where between the two marks to take the measurement. A Vernier scale on our ruler does this last fine piece of measuring between the two marks for us. \n\nSo , for example if the measurement with our eye said it was between 1.5 and 1.6 centimeters, we would look at the Vernier and we would be able to","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Metadata (also Meta data) is data about data, that is information about other information. This is often useful. Libraries usually group books by subject and keep a catalog of the books. In that context the metadata in the catalog include various pieces of information including the author and the subject it is classified under. The index and table of contents of a book also present metadata.\n\nDigital cameras allow people to take pictures. Here the data is the picture taken, and the metadata is information about the picture taken, like when (the date and time), photographic exposure, focal length and other technical details about how the camera made the picture. Some cameras, especially smartphones, have GPS and can include the location in metadata.\n\nComputer science\nData science\nPhotography","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bandwidth is used to measure electronic and other types of communication. This includes radio, electronics, and other forms of electromagnetic radiation, Bandwidth is the difference between the electronic signal having highest-frequency and the signal having the lowest-frequency.\n\nIn computer networks, bandwidth is often used as a term for the data transfer bit rate.  More easily, the amount of data that is carried or passed from one point to another in a network, in a given time period (usually a second).\n\nFrequency\nMany systems work by means of continuous movements, or oscillations. Each complete \"back and forth\" it makes is called a cycle. The number of cycles every second is its frequency. Frequency is measured in cycles per second, most often called \"Hertz\", or \"Hz\" for short.\n\nSystems have at least one frequency, and usually many different frequencies.  For example, sound waves travel as vibrations.  People can hear sound frequencies low as 20\u00a0Hz, and high as","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library \u2013 Reference Manual \u2013 The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"though with longer \"tails\". \n\nDue to this, estimating the mean value may not converge to any single value with more data (law of large numbers) unlike a normal distribution; due to a higher chance of getting extreme values (the tails of a frequency plot).\n\nRelated pages \n\n Student's t-distribution\n\nReferences\n\nPhysics\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"higher average is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return).\n\nRules for normally distributed numbers\n\nMost math equations for standard deviation assume that the numbers are normally distributed. This means that the numbers are spread out in a certain way on both sides of the average value. The normal distribution is also called a Gaussian distribution because it was discovered by Carl Friedrich Gauss. It is often called the bell curve because the numbers spread out to make the shape of a bell on a graph. \n\nNumbers are not normally distributed if they are grouped on one side or the other side of the average value. Numbers can be spread out and still be normally distributed. The standard deviation tells how widely the numbers are spread out.\n\nRelationship between the average (mean) and standard deviation \nThe average (mean) and the standard deviation of a set of data are usually written together. Then a person can understand what the average number is and how widely other numbers in the group are spread out.","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Kolmogorov\u2013Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics, a frequency distribution is a list of the values that a variable takes in a sample. It is usually a list, ordered by quantity. It will show the number of times each value appears. For example, if 100 people rate a five-point Likert scale assessing their agreement with a statement on a scale on which 1 denotes strong agreement and 5 strong disagreement, the frequency distribution of their responses might look like:\n\nThis simple table has two drawbacks. When a variable can take continuous values instead of discrete values or when the number of possible values is too large, the table construction is difficult, if it is not impossible. A slightly different scheme based on the range of values is used in such cases. For example, if we consider the heights of the students in a class, the frequency table might look like below.\n\nApplications \nManaging and operating on frequency tabulated data is much simpler than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability theory and statistics, the chi-square distribution (also chi-squared or \u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with  degrees of freedom is written as . It is a special case of gamma distribution.\n\nChi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\n\nRelated pages \n\n Chi-squared test\n\nReferences\n\nOther websites\nChi-Square Tutorial by Khans Academy\n\nProbability distributions","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Demand forecasting is when a business predicts future demand for its products. A business looks at many things when they do demand forecasting. Some of these things are past sales, data from test markets, and statistics. Businesses can also use educated guesses to help predict future demand. Businesses use demand forecasting to help them come up with the amount of demand for their products so they know how much supply to make.\n\nReferences\n\nBusiness\nCommerce\nEconomic theories","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Forecasting is studying and saying what is likely to happen in the future. It is similar to predicting, but usually forecasting is done with scientific methods. Forecasting can be done for many different things, like weather forecasting (predicting the weather) or economy forecasting.  Science cannot know the future for sure, so forecasters try to identify the most probable events, and sometimes they are wrong.\n\nWords","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Numerical weather prediction is the way weather forecasts are made.  This is done using computer models of the atmosphere. Such models describe the current weather conditions, and how they change over time using equations. Using the current weather conditions, the equations can be solved, or approximated to tell what the weather will be like in the near future. The relevant physical parameters, such as pressure, temperature, the direction and the speed of the wind are taken to be functions of time. These are modelled with a system of partial differential equations. This is a dynamic system that is solved numerically. Most of these equations are implemented using FORTRAN. The equations are approximated. Since the number of calculations is huge, supercomputers usually do them, to finish before it's too late.\n\nBasic idea\nThe atmosphere is modelled as a fluid. The basic idea of numerical weather prediction is to sample the state of the fluid at a given time. The equations of fluid dynamics and thermodynamics can then be","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\u00a0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in star movement; optimization occurs in portfolio management; numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nComputers greatly helped this task. Before there were computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.\n\nFamous numerical software\nIn order to support numerical analysts, many kinds of numerical software has been created:\n MATLAB - made by","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Sustainable Development Goals (SDGs) are created by the [United Nations] and promoted as the Global Goals for Sustainable Development. They replaced the [Millennium Development Goals] that expired at the end of 2015. The SDGs run from 2015 to 2030. There are 17 goals and 169 specific targets for those goals.\n\nGoals\n\nIn August of 2015 193 countries agreed to the following 17 goals:\n\n No poverty \n Zero hunger \n Good health and wellbeing\n Quality education \n Gender equality\n Clean water and sanitation\n Affordable and clean energy \n Decent work and economic growth \n Industry, innovation and infrastructure\n Reduce inequality \n Sustainable cities and communities \n Responsible consumption and production\n Climate action \n Life below water\n Life on land\n Peace and justice.  Strong institutions\n Partnerships for the goals\n\nReferences\n\nSustainability\nDevelopment\nUnited Nations","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainable energy is energy that is created in an environmentally friendly way.\n\nIt involves both energy efficiency and renewable energy. Both resources help to stabilize and reduce carbon dioxide emissions. \n\nEfficient energy use allows energy demand to slow so that rising clean energy supplies can make big cuts in fossil fuel burning.  If energy use grows too fast, renewable energy development will chase a receding target. Likewise, unless clean energy supplies come online rapidly, slowing demand growth will only begin to reduce total emissions; reducing the carbon content of energy sources is also needed.\n \nAny serious vision of a sustainable energy economy thus requires major commitments to both efficiency and renewables.\n\nReferences\n\nEnergy\nSustainable development","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bioethics is a field of ethics in philosophy that studies the ethical, legal and social issues in medicine and biology. It is often used to study how the new discoveries in science will affect humans, animals and nature.\n\nRelated pages \nAbortion\nBiotechnology\nClone\nEuthanasia\nMedical ethics\n\nMedicine\nEthics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ethics is the study of good and bad behaviour. It is one of the main parts of philosophy. Ethics tries to answer questions like:\n\n What actions are good? What actions are evil?\n How can we tell the difference?\n Are good and evil the same?\n How should we make hard decisions that might help or hurt other people?\n How do our actions affect others?\n\nIdeas about ethics \nWhen discussing ethics, the philosophy is generally separated into:\n thinking about morality,\n the involvement of science, \n the freedom of people to decide for themselves how to act within their own beliefs.\n\nMorality is what someone thinks or feels is good or bad.\nThere are many different moralities, but they share some things.\nFor example, most people think that murder (killing somebody) is wrong.(compare Exodus 20:13)\nSome philosophers have hope to find more things that moralities share.\nThey think that ethics should use the scientific method to study \nthings that people think are good or bad. Their work","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the most complex thing about humans is how they relate to each other in society.  The most general social science is ethics, or economics, depending on your point of view.\n\nHowever, life science also deals with humans as objects of study (notably in medicine), while hard science such as chemistry deals with humans as the observer who does the study - sets the scale at which observation can happen, incurs observer effects - as studied in philosophy of science.\n\nOften sciences have different names based on whether they study humans or not.  For instance economics is the study of how humans make a living, while ecology is the study of how non-humans make a living.  Medicine is normally restricted to humans while veterinary medicine refers to the same techniques made on other species.  One relies more on ethics and less on economics when dealing with humans, at least according to any ethical tradition.\n\nThe living, social, and sensory observing aspects of humans are probably all part of the human sciences to the degree they","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pragmatic ethics is a kind of ethics that focuses  on  the development in society: People such as John Dewey believe that the moral progress a society makes is related to the progress and level in science of that society. Scientists look at hypotheses and examine them; they can then act in such a way that they believe these hypotheses to be true. When science advances, future scientists can replace a hypothesis with a better one. \n\nEthics","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems theory is the study of the nature of systems in nature, society, and science.  More specifically, systems theory is a framework to analyze or describe any group of things which work together to produce some result. This can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. \n\nSystems theory as a technical and general academic area of study. It was founded by Ludwig von Bertalanffy and others in the 1950s.\n\nRelated pages\n Autopoiesis\n Chaos theory\n Fritjof Capra","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A system  is a group of related things that work together as a whole. These things can be real or imaginary. Systems can be man-made things like a car engine or natural things like a star system. Systems can also be concepts made by people to organize ideas. \n\nA subsystem is a system that is part of some larger system. For example, in computer networking, a disk subsystem is a part of a computer system.\n\nDefinition  \nA system is a group of things that connect and form some kind of coherent whole.\n\nOne of the founders of physiology, Claude Bernard, took a big step when he noticed that the internal systems of the body tend to keep things the same even if they are disturbed. He called the functions which keep system stable as homeostasis. This led towards the ideas of error-correction, feedback and regulation. These are all typical of systems which have some kind of goal and can adjust their behaviour so as to correct errors.\n\nExample \nThe Solar System is an example","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"systems\". This approach makes use of molecular biology ideas and systems science ideas, but it does not refer to the particular way life is organised on Earth. This is because the authors thought it was possible for machines to have those features, and for other types of life to exist.\n\nReferences \n\nBiology\nSystems theory\nPhilosophy of science","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Pauli exclusion principle refers to the fact that certain particles cannot be at the same place at the same time, with the same energy. Only fermions (examples are protons, neutrons and electrons) are bound by the Pauli exclusion principle, while bosons (an example is a photon - light beam) are not. A more precise way to describe the Pauli exclusion principle is to say that two of the same kind of fermions that are in the same quantum system (same atom, for example) cannot have the same quantum numbers. This principle was discovered by physicist Wolfgang Pauli in 1925. It is a very important principle in physics because the particles that make up ordinary matter are fermions.\n\nQuantum mechanics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"point, that means that the particle was either created or destroyed, depending on the direction in time that the particle came in from.\n\nAll the lines and vertices have an amplitude. When you multiply the probability amplitude for the lines, the amplitude for the particles to go from wherever they start to wherever they meet, and to the next meeting point, and so on, and also multiply by the amplitude for each meeting point, you get a number that tells you the total amplitude for the particles to do what the diagram says they do. If you add up all these probability amplitudes over all the possible meeting points, and over all the starting and ending points with an appropriate weight, you get the total probability amplitude for a collision in a particle accelerator, which tells you the total probability of these particles to bounce off one another in any particular direction.\n\nFeynman diagrams are named after Richard Feynman, who won the Nobel Prize in Physics. His diagrams are very simple in the case of quantum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A Feynman diagram is a diagram that shows what happens when elementary particles collide.\n\nFeynman diagrams are used in quantum mechanics. A Feynman diagram has lines in different shapes\u2014straight, dotted, and squiggly\u2014which meet up at points called vertices. The vertices are where the lines begin and end. The points in Feynman diagrams where the lines meet represent two or more particles that happen to be at the same point in space at the same time. The lines in a Feynman diagram represent the probability amplitude for a particle to go from one place to another.\n\nIn Feynman diagrams, the particles are allowed to go both forward and backward in time. When a particle is going backward in time, it is called an antiparticle. The meeting points for the lines can also be interpreted forward or backwards in time, so that if a particle disappears into a meeting point, that means that the particle was either created or destroyed, depending on the direction in time that","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The idea of many universes is called the many-worlds interpretation.\n\nInflationary models are also discussed in this chapter, and so is the idea of a theory that unifies quantum mechanics and gravity.\n\nEach particle has many histories. This idea is known as Feynman's theory of sum over histories. A theory that unifies quantum mechanics and gravity should have Feynman's theory in it. To find the chance that a particle will pass through a point, the waves of each particle needs to be added up. These waves happen in imaginary time. Imaginary numbers, when multiplied by themselves, make a negative number. For example, 2i X 2i = -4.\n\nOther editions \n 1988 \u2014 The first edition is published. This edition had an introduction by Carl Sagan.\n 1990 - Similar to the 1996 but with an introduction by Carl Sagan, uncolored pictures, and it was printed in paperback\n 1996 \u2014 An","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"samples from a production lot) based on how well it met its design specifications. In contrast, Statistical Process Control uses statistical tools to observe the performance of the production process in order to predict significant deviations that may later result in rejected product.\n\nTwo kinds of variation occur in all manufacturing processes: both these types of process variation cause subsequent variation in the final product. The first is known as natural or common cause variation and consists of the variation inherent in the process as it is designed. Common cause variation may include variations in temperature, properties of raw materials, strength of an electrical current etc. The second kind of variation is known as special cause variation, or assignable-cause variation, and happens less frequently than the first. With sufficient investigation, a specific cause, such as abnormal raw material or incorrect set-up parameters, can be found for special cause variations.\n\nFor example, a breakfast cereal packaging line may be designed to fill each cereal box with 500\u00a0grams of product, but some boxes will have slightly more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"20 intervals contain the true value of the parameter.\n\nPractical example\nA machine fills cups with margarine. It is adjusted so that the content of the cups is 250g of margarine. As the machine cannot fill every cup with exactly 250g, the content added to individual cups shows some variation, and is considered a random variable X.\n\nThis variation is assumed to be normally distributed around the desired average of 250g, with a standard deviation of 2.5g. To determine if the machine is adequately calibrated, a sample of n = 25 cups of margarine is chosen at random, and the cups are weighed. The weights of margarine are X1, ..., X25, a random sample from X.\n\nTo get an impression of the expectation \u03bc, an estimate is needed. The appropriate estimator is the sample mean: \t \n\nThe sample shows actual weights x1, ...,x25, with mean:\n\nIf we take another sample of 25 cups, we","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"chosen at random is 1.80\u00a0m tall, the \"(statistical) error\" is 0.05\u00a0m (5\u00a0cm); if he is 1.70 tall, the error is \u22125\u00a0cm.\n\nA residual (or fitting error), on the other hand, is an observable estimate of the unobservable statistical error.  The simplest case involves a random sample of n men whose heights are measured.  The sample mean is used as an estimate of the population mean.  Then we have:\n\nThe difference between the height of each man in the sample and the unobservable population mean is a statistical error, and\nThe difference between the height of each man in the sample and the observable sample mean is a residual.\n\nThe sum of the residuals within a random sample must be zero. The residuals are therefore not independent. The sum of the statistical errors within a random sample need not be zero; the statistical errors are independent random variables if the individuals are chosen from the population","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Statistical errors and residuals occur because measurement is never exact.\n\nIt is not possible to do an exact measurement, but it is possible to say how accurate a measurement is. One can measure the same thing again and again, and collect all the data together. This allows us to do statistics on the data. What is meant by errors and residuals is the difference between the observed or measured value and the real value, which is unknown. \n\nIf there is only one random variable, the difference between statistical errors and residuals is the difference between the mean of the population against the mean of the (observed) sample. In that case the residual is the difference between what the probability distribution says, and what was actually measured.\n\nSuppose there is an experiment to measure the height of 21-year-old men from a certain area. The mean of the distribution is 1.75\u00a0m. If one man chosen at random is 1.80\u00a0m tall, the \"(statistical) error\" is","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"then the study is available for people to read without paying a fee.  It is not behind a paywall.\n\nPLOS ONE can publish 20,000 scientific papers in one year.  That means sometimes it publishes bad papers by accident.  In 2016, PLOS ONE published many retractions, which means it said publicly that some of its papers were bad, said how they were bad, and said it did not believe in them.\n\nOther websites\n\nPLOS One\n\nReferences \n\nScience and technology magazines\nJournals","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"quite common in science for some studies to contradict others, for example in cases where different methods are used to measure an outcome, or where human error or chance may lead to unusual results. This means that there is often a study someone can use to support their claim, and they can cherry pick that one study even if many more contradict it.\n\nReferences\n\nLogical fallacies","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and making choices in their own best interest. \n\"I will state that coercive persuasion and thought reform techniques are effectively practiced on na\u00efve, uninformed subjects with disastrous health consequences. I will try to give enough information to indicate my reasons for further inquiries as well as review of applicable legal processes\".\n\nThe following methods have been used in some or all cults studied:\n People are put in physically or emotionally distressing situations;\n Their problems are reduced to one simple explanation, which is repeatedly emphasized;\n They receive what seems to be unconditional love, acceptance, and attention from a charismatic leader or group;\n They get a new identity based on the group;\n They are subject to entrapment (isolation from friends, relatives and the mainstream culture) and their access to information is severely controlled.\n\nThis view is disputed by some. Society for the Scientific Study of Religion stated in 1990 that there was not sufficient research for a consensus, and that \"one should not automatically equate the techniques involved in the process","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"have learned out loud. Reciting helps the learning process much better than reading alone. Review the information a few hours or even days later. This keeps it fresh in your mind.\n\n Skimming and Scanning. Skim reading is a technique to gain the most from reading something in the least amount of time. It is looking at chapter headings, bullet lists of key points in sidebars and key words in sentences.The first sentence of a chapter often is a abstract of the chapter. Scanning involves moving your finger down the page as you read. The object is to try to absorb at least 50% of the text. It is then compared to what was skimmed.\n\n Study Groups. Colleges and universities encourage students to form study groups. A study group can divide tasks and each member concentrate on one segment. Students who teach or share what they know with others learn more. A study group uses active learning, a very effective way to learn.\n\n Taking effective notes. Note taking skills learned in high school","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"not in custody. That is one of the few things a police officer may not lie about. During questioning police may use trickery, lying or other forms of deception in order to get a confession from a suspect.\n\nPEACE method \nA fairly new technique is being used in the UK, Denmark, New Zealand and other places. The word PEACE is an acronym for Preparation and Planning, Engage and Explain, Account, Closure and Evaluate. It is considered a more ethical model of police questioning. It is intended to prevent wrongful convictions by eliminating any coercion or deceptive methods. When tested in England and Wales, it resulted in the same percentage of convictions as the older methods produced. Peace is more of a journalistic approach. It assumes a liar will find it increasingly difficult to keep all the lies consistent and will eventually break down and confess.\n\nThird degree \nA historic method of obtaining confessions is the applying of the so-called third degree. It consists of treating a suspect brutally, keeping them awake for long","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for each client.\u00a0 At this point, the EMDR client is instructed to just notice whatever spontaneously happens.\n\nAfter each set of stimulation, the clinician instructs the client to let his\/her mind go blank and to notice whatever thought, feeling, image, memory, or sensation comes to mind.\u00a0 Depending upon the client\u2019s report, the clinician will choose the next focus of attention.\u00a0 These repeated sets with directed focused attention occur numerous times throughout the session.\u00a0 If the client becomes distressed or has difficulty in progressing, the therapist follows established procedures to help the client get back on track.\n\nWhen the client reports no distress related to the targeted memory, (s)he is asked to think of the preferred positive belief that was identified at the beginning of the session.\u00a0 At this time, the client may adjust the positive belief if necessary, and then focus on it during the next set of distressing events.\n\nPhase 7:\u00a0 In phase seven, closure, the therapist asks the","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in ridge regression. The least squares approach can also be used to fit models that are not linear. As outlined above, the terms \"least squares\" and \"linear model\" are closely linked, but they are not synonyms.\n\nUsage\n\nEconomics\n\nLinear regression is the main analytical tool in economics. For example, it is used to guess consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand and labor supply.\n\nRelated pages \n\n Curve fitting\n Logistic regression\n Ordinary least squares\n\nReferences\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.  Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis.  In many of these models, the coefficients are referred to as \"weights\".\n\nStatistics\nFunctions and mappings","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A weather map is a tool. It shows facts about the weather quickly.  Weather maps have been used from the mid-19th century, for study and for weather forecasting. Some maps show differences of temperature, and weather fronts. \n\nA station model is a symbolic picture showing the weather at a reporting station. Meteorologists made the station model to put down many weather elements in a small space on weather maps. Maps thickly filled with station-model plots can be hard to read. However, they help meteorologists, pilots, and mariners to see important weather patterns. A computer draws a station model for every place of observation. The station model is mostly used for surface-weather maps. It can also be used to show the weather in the sky, though. A complete station-model map lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\nHistory \n\nPeople first began using weather charts in a modern way in the mid-19th century. They began using","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Isotherms are lines drawn around places with the same temperature range on isotherm maps. Each point on this line shows one temperature reading, or the average of many temperature readings. Isotherm maps also have scales that tell the signals or colors for the different temperatures. Isotherm lines are usually curvy and not straight lines.\n\nMeteorology\nThermodynamics\n\nen:Contour line#Temperature and related subjects","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"is the best known example of using thematic maps for analysis of data. His method anticipates the principles of a geographic information system (GIS). He started with an accurate map of a London neighborhood which included streets and water pump locations. Onto this Snow placed a dot for each cholera death. The pattern centered around one particular pump on Broad Street. At Snow\u2019s request, the handle of the pump was removed, and new cholera cases ceased almost at once. Further investigation of the area revealed the Broad Street pump was near a cesspit under the home of the outbreak's first cholera victim.\n\nReferences \n\nCartography","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Besides philosophy and medicine, Avicenna's works includes writings on astronomy, alchemy, geography and geology, psychology, Islamic theology, logic, mathematics, physics and poetry.\n\nLater life \nFrom 1015 to 1022, Avicenna was a high official and doctor to the ruler of Hamedan in western Iran. After the ruler of Hamedan died Avicenna was put in prison. He was released four months later when Hamadan was captured by Al\u0101 al-Dawla, the ruler of Isfahan. Al\u0101 al-Dawla only captured Hamadan for a short period of time.  Avicenna escaped, disguised as a dervish, to Isfahan to work for Al\u0101 al-Dawla as a doctor.  In 1030, the Ghaznavids attacked Isfahan and some of Avicenna's work was lost and possibly stolen. He died during an attack on the city of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ibn Khaldun (; , ; 27 May 1332 \u2013 17 March 1406) was an influential Arab thinker of the 14th century. His family were from Andalusia. Khaldun served the governments of the day in many ways. He was sometimes in prison. He lived in Marrakesh in Morocco for a time, and in Granada. Then he moved to Cairo, where he was a judge.\n\nThe most famous book Khaldun wrote is the Kit\u0101b al-\u02bbIbar (Book of Lessons), a history of the world. The first part, Muqaddimah (Introduction) is often used alone. This book is often credited as inventing sociology. He also wrote his autobiography.\n\nIbn Khaldun lived a life in search of stability and influence. He came from a family of scholars and politicians and he intended to live up to both expectations. He would succeed in the field of scholarship much more so than in","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A diagram is usually a two-dimensional display which communicates using visual relationships. It is a simplified and structured visual representation of concepts, ideas, constructions, relations, statistical data, anatomy etc. It may be used for all aspects of human activities to explain or illustrate a topic.\n\nDiscussion \n\n visual information device: Like the term \"illustration\" the diagram is used as a collective term standing for the whole class of technical genres, including graphs, technical drawings and tables.\n specific kind of visual display: This is the genre that shows qualitative data with shapes that are connected by lines, arrows, or other visual links.\n\nIn science the term is used in both ways. For example, Anderson (1997) stated more generally: \"diagrams are pictorial, yet abstract, representations of information, and maps, line graphs, bar charts, engineering blueprints, and architects' sketches are all examples of diagrams, whereas photographs and video are not\". On the other hand, Lowe (1993) defined diagrams as","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or diagrams that are not graphs.\n\n\n\nHow to draw a simple graph \n\nLet's say you wanted to make a graph showing your height as you were growing up. You might show your height in centimeters on the vertical axis and your age in years on the horizontal axis.\n\nFor example, say you were 60 centimeters tall at age 1, 85 centimeters tall at age 2 and 95 centimeters tall at age 3. You would draw an imaginary straight line passing through the 50 centimeters height mark on the vertical axis. Then you would draw a second imaginary line passing through the age of 1 year on the horizontal axis. At the place where the two imaginary lines meet (called their intersection), you would then draw a dot. Really, the imaginary lines are usually replaced by graph paper (see below). The drawing of this intersection is called \"plotting the point","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Data mining is a term from computer science. Sometimes it is also called knowledge discovery in databases (KDD). Data mining is about finding new information in a lot of data. The information obtained from data mining is hopefully both new and useful. \n\nIn many cases, data is stored so it can be used later. The data is saved with a goal. For example, a store wants to save what has been bought. They want to do this to know how much they should buy themselves, to have enough to sell later. Saving this information, makes a lot of data. The data is usually saved in a database. The reason why data is saved is called the first use. \n\nLater, the same data can also be used to get other information that was not needed for the first use. The store might want to know now what kind of things people buy together when they buy at the store. (Many people who buy pasta also buy mushrooms for example.) That kind of information is in the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected\/influence each other. The","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to make something that can say how the different data attributes are connected\/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Deep learning (also called  deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer.\n\nCertain tasks, such as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer.\n\nDeep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.\n\nReferences \n\nArtificial intelligence","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"methods used by scientists to find knowledge. The main features of the scientific method are:\n Scientists identify a question or a problem about nature. Some problems are simple, such as \"how many legs do flies have?\" and some are very deep, such as \"why do objects fall to the ground?\"\n Next, scientists investigate the problem. They work at it, and collect facts. Sometimes all it takes is to look carefully.\n Some questions cannot be answered directly. Then scientists suggest ideas, and test them out. They do experiments and collect data.\n Eventually, they find what they think is a good answer to the problem. Then they tell people about it.\n Later, other scientists may agree or not agree. They may suggest another answer. They may do more experiments. Anything in science might be revised if we find out the previous solution was not good enough.\n\nAn example \nA famous example of science in action was the expedition led by Arthur Eddington to Principe Island in Africa in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"pages \nPhilosophy of science\nScientific method\n\nScience\nPhilosophy of science\n\nfr:Th\u00e9orie#Sciences\npt:Teoria#Teoria Cient\u00edfica","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Meditation tries to get past the \"thinking\" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Inner peace\u00a0(or\u00a0peace of mind) happens when a person is able to make their mind calm despite stressful things that may be happening around them.\n\nPeace\nMeditation","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"your hearts be troubled and do not be afraid.\" ()\n\nInner peace \n\nInner peace (or peace of mind) refers to a state of being mentally and spiritually at peace, with enough knowledge and understanding to keep oneself strong in the face of stress. Being \"at peace\" is considered by many to be healthy and the opposite of being stressed or anxious. Peace of mind is generally associated with bliss and happiness.\n\nPeace of mind, serenity, and calmness are descriptions of a disposition free from the effects of stress. In some cultures, inner peace is considered a state of consciousness or enlightenment that may be cultivated by various forms of training, such as prayer, meditation, Tai chi chuan or yoga, for example. Many spiritual practices refer to this peace as an experience of knowing oneself.\n\nMovements and activism\n\nPeace movement \n\nA movement that seeks to get ideals such as the ending of a particular war, minimize inter-human violence in a particular place or type of situation, often linked to the goal of","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the mind can better see the cause of pain. Buddhists believe that this type of seeing can end pain.\n\nBuddhist meditation is not just used for spiritual reasons. Research shows that Buddhist meditation lowers stress, anxiety and depression.\n\nMost types of Buddhist meditation focus on something. The most popular things to focus on include breath,  metta or Loving-Kindness towards all, other recollections, situational mindfulness and religious images and sounds.\n\nChristian meditation \nChristians sometimes meditate by thinking about small parts of the Bible, or by saying the words of a prayer to themselves over and over. Meditation is an expression of Christian prayer. In the Catechism of the Catholic Church is specified that by means of meditation \"The mind seeks to understand the why and how of the Christian life, in order to adhere and respond to what the Lord is asking\"; also it is pointed out that \"meditation engages thought, imagination, emotion, and desire. This mobilization of faculties is necessary in","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund SITTING and the nouns agent and location express the diagram's basic relationship; \"agent is SITTING on location\"; Elsie is an instance of CAT.\n\nAlthough the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter's illustration of that ambiguity, with a progression from abstract to concrete in G\u00f6del, Escher, Bach (1979):\n(1) a publication\n(2) a newspaper\n(3) The San Francisco","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Each of these smaller areas (room, state, number) is next to other small areas (other rooms\/states\/numbers). The places where the areas meet are connections. If we write down on paper a list of spaces, and the connections between them, we have written down a description of a space -- a topological space. All topological spaces have the same properties such as connections, and are made of the same structure (a list of smaller areas). This makes it easier to study how spaces behave. It also makes it easier to write algorithms. For instance, to program a robot to navigate a house, we simply give it a list of rooms, the connections between each room (doors), and an algorithm that can work out which rooms to go through to reach any other room. For more examples of this type of problem, look at Graph theory.\n\nWe can go further by creating subdivisions of subdivisions of space. For instance, a nation divided into states, divided into counties, divided into","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"\"idea\" of a CAT to classes of objects such as \"mammals\" and even categories such as \"object\" as opposed to \"action\".\nFor example, graph 1 above expresses the abstraction \"agent sits on location\".\n\nThis conceptual scheme entails no specific hierarchical taxonomy (such as the one mentioned involving cats and mammals), only a progressive exclusion of detail.\n\nThe neurology of abstraction\nA recent meta-analysis suggests that the verbal system has greater engagement for abstract concepts when the perceptual system is more engaged for processing of concrete concepts. This is because abstract concepts cause greater brain activity in the inferior frontal gyrus and middle temporal gyrus, compared to concrete concepts when concrete concepts cause greater activity in the posterior cingulate, precuneus, fusiform gyrus, and parahippocampal gyrus.\n\nOther research into the human brain suggests that the left and right hemispheres, of the brain, differ in their handling of abstraction. For example, one meta-analysis reviewing human","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"computer was used to move them around in a virtual reality town.\n\nThe discovery of place cells in the 1970s led to a theory that the hippocampus might act as a cognitive map a neural representation of the layout of the environment. The \"cognitive map hypothesis\" has been further advanced by recent discoveries of direction cells in several parts of the rodent brain which are strongly connected to the hippocampus.\n\nEvolution \n\nThe hippocampus has a generally similar appearance across the mammals from monotremes such as the echidna to primates such as humans. The hippocampal size to body-size ratio increases: it is about twice as large for primates as for the echidna. It does not, however, increase at anywhere close to the rate of the neocortex to body-size ratio. Therefore, the hippocampus takes up a larger fraction of the cortex in rodents than in primates.\n\nOther vertebrates have areas which may be homologous to the hippocampus of","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and the length of the month are also ancient.\n\nThe Round-to-even method has served as the ASTM (E-29) standard since 1940. The origin of the terms unbiased rounding and statistician's rounding are fairly self-explanatory. In the 1906 4th edition of Probability and Theory of Errors  Robert Simpson Woodward called this \"the computer's rule\" indicating that it was then in common use by human computers who calculated mathematical tables. Churchill Eisenhart's 1947 paper \"Effects of Rounding or Grouping Data\" (in Selected Techniques of Statistical Analysis, McGrawHill, 1947, Eisenhart, Hastay, and Wallis, editors) indicated that the practice was already \"well established\" in data analysis.\n\nThe origin of the term \"bankers' rounding\" remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"will tend to lie closer to the centre, the mean, of the distribution. He quantified this trend, and in doing so invented linear regression analysis. This is the starting point for much of modern statistical modelling. Since then, the term \"regression\" has taken on different meanings, and it may be used by modern statisticians to describe phenomena of sampling bias which have little to do with Galton's original observations in the field of genetics.\n\nGalton's explanation for the regression phenomenon he observed is now known to be incorrect. He stated: \u201cA child inherits partly from his parents, partly from his ancestors. Speaking generally, the further his genealogy goes back, the more numerous and varied will his ancestry become, until they cease to differ from any equally numerous sample taken at haphazard from the race at large.\u201d This is incorrect, since a child receives its genetic makeup exclusively from its parents. There is no generation-skipping in genetic material: any genetic material from earlier ancestors than the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods.\n\nHe began to pay particular attention to the labour involved in the necessary computations, and developed practical methods. In 1925, his first book was published: Statistical methods for research workers. This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines. In 1935, this was followed by The design of experiments, which also became a standard.\n\nHis work on the theory of population genetics made him one of the three great figures of that field, together with Sewall Wright and J.B.S. Haldane. He was one of the founders of the neo-Darwinian modern evolutionary synthesis. In addition to founding modern quantitative genetics with his 1918 paper, he was the first to use diffusion equations to attempt to calculate the distribution of gene frequencies among populations.\n\nHe pioneered the estimation of genetic linkage and gene frequencies by maximum","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Hurwicz had in 1950, and is thought of as very important for economics. The Hurwicz Criterion is also called \"under uncertainty\". Hurwicz had the idea of putting Abraham Walds work together with work done by Pierre-Simon Laplace in 1812. The Hurwicz Criterion tells people to think very carefully of what is good and what is bad before making decisions. Different types of the Hurwicz Criterion have been thought of. Leonard Jimmie Savage made some changes to the first Hurwiczs criterion in 1954. The four peoples work \u2013 Laplace, Wald, Hurwicz and Savage \u2013 that the Hurwicz Criterion has been based on was studied, corrected and used for over fifty years by many different people including John Milnor, G. L. S. Shackle, Daniel Ellsberg, R. Duncan Luce and Howard Raiffa, but some people say the work was started by Jacob Bernoulli.\n\nThe","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Regression analysis is a field of statistics. It is a tool to show the relationship between the inputs and the outputs of a system. There are different ways to do this. Better curve fitting usually needs more complex calculations. \n\nData modeling can be used without knowing about the underlying processes that have generated the data; in this case the model is an empirical model. Moreover, in modelling, knowledge of the probability distribution of the errors is not required. Regression analysis requires assumptions to be made regarding probability distribution of the errors. Statistical tests are made on the basis of these assumptions. In regression analysis the term \"model\" embraces both the function used to model the data and the assumptions concerning probability distributions.\n\nRegression can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. These uses of regression rely heavily on the underlying assumptions being satisfied. Regression analysis has been criticized as being misused for these purposes in many cases where the appropriate assumptions cannot be verified to hold.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Informatica, 30:3\u201331, 2006.\n\nFurther reading \n\n Bradley, R.A. and Terry, M.E. (1952). Rank analysis of incomplete block designs, I. the method of paired comparisons. Biometrika, 39, 324\u2013345.\n David, H.A. (1988). The Method of Paired Comparisons. New York: Oxford University Press.\n Luce, R.D. (1959). Individual Choice Behaviours: A Theoretical Analysis. New York: J. Wiley.\n Thurstone, L.L. (1927).  A law of comparative judgement. Psychological Review, 34, 278\u2013286.\n Thurstone, L.L. (1929).  The Measurement of Psychological Value.  In T.V. Smith and W.K. Wright (Eds.), Essays in Philosophy by Seventeen Doctors of Philosophy of the \tUniversity of Chicago.  Chicago: Open Court.\n Thurstone, L.L.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size\u2014if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". Computer Graphics Forum, 22  2,  171\u2013179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". Computers & Graphics, 30  6,  1020\u2013 1026.\n\nSurveys\nL. H. de Figueiredo and J.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Numerical Hamiltonian Problems, Applied Mathematics and Mathematical Computation 7, Chapman & Hall, London, 1994.\n Arnold, D. N., Bochev, P. B., Lehoucq, R. B., Nicolaides, R. A. and Shashkov, M. (eds.), Compatible Spatial Discretizations, in The IMA Volumes in Mathematics and Its Applications, Springer, New York, 2006.\n Budd, C. and Piggott, M. D., Geometric integration and its applications, in Handbook of Numerical Analysis, XI, North\u2010Holland, Amsterdam, 2003, 35\u2010139.\n Christiansen, S. H., Munthe\u2010Kaas, H. Z. and Owren, B., Topics in structure\u2010preserving discretization, Acta Numerica, 20 (2011), 1\u2010119.\n Shashkov, M., Conservative Finite\u2010Difference Methods on General Grids,","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"\"Polynomial evaluation using affine arithmetic for curve drawing\". Proc. of Eurographics UK 2000 Conference,  49\u201356. .\n N. Femia and G. Spagnuolo (2000), \"True worst-case circuit tolerance analysis using genetic algorithm and affine arithmetic \u2014 Part I\". IEEE Transactions on Circuits and Systems, 47  9,  1285\u20131296.\n R. Martin, H. Shou, I. Voiculescu, and G. Wang (2001), \"A comparison of Bernstein hull and affine arithmetic methods for algebraic curve drawing\". Proc. Uncertainty in Geometric Computations,  143\u2013154. Kluwer Academic Publishers, .\n A. Bowyer, R. Martin, H. Shou, and I. Voiculescu (2001), \"Affine intervals in a CSG geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"same in both cases; it is then possible to write another equation, which replaces the two equations and reduces the number of equations by one.\nGaussian elimination\nQR decomposition\nCholesky decomposition\nCramer's rule\nExamples for iterative methods are:\nRelaxation, including the Gauss-Seidel and Jacobi methods\nMultigrid method\nKrylow method\n\nThere are examples such as geodesy where there many more measurements than unknowns. Such a system is almost always overdetermined and has no exact solution. Each measurement is usually inaccurate and includes some amount of error. Since the measurements are not exact, it is not possible to obtain an exact solution to the system of linear equations; methods such as Least squares can be used to compute a solution that best fits the overdetermined system. This least squares solution can often be used as a stand-in for an exact solution.\n\nSolving a system of linear equations has a complexity of at most O (n3). At least n2","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A. (1996). Numerical methods for least squares problems (Vol. 51). SIAM.\n\nMathematical approximation\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"subspace methods: principles and analysis. OUP Oxford.\n\nFields of mathematics\nTechnology\nLinear algebra\nComputer science\nComputing","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". Computer Graphics Forum, 22  2,  171\u2013179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". Computers & Graphics, 30  6,  1020\u2013 1026.\n\nSurveys\nL. H. de Figueiredo and J.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"\"Polynomial evaluation using affine arithmetic for curve drawing\". Proc. of Eurographics UK 2000 Conference,  49\u201356. .\n N. Femia and G. Spagnuolo (2000), \"True worst-case circuit tolerance analysis using genetic algorithm and affine arithmetic \u2014 Part I\". IEEE Transactions on Circuits and Systems, 47  9,  1285\u20131296.\n R. Martin, H. Shou, I. Voiculescu, and G. Wang (2001), \"A comparison of Bernstein hull and affine arithmetic methods for algebraic curve drawing\". Proc. Uncertainty in Geometric Computations,  143\u2013154. Kluwer Academic Publishers, .\n A. Bowyer, R. Martin, H. Shou, and I. Voiculescu (2001), \"Affine intervals in a CSG geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314.","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A narrative is a literary term for the events that make up a story. It is the way the events connect and make up the plot of a story or book.\n\nA narrative reports connected events, presented to the person reading or listener in a sequence of written or spoken words.\n\nA common term for narrative is plot. The study on structure in narratives is called narratology.\n\nRelated pages\nNarrative poetry\n\nReferences\n\nFiction\nWriting\nNarratology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Narratology is the study of structure in narratives. The theory of narrative or narratology was developed in the 1960s. Narratology is based on the idea of a common literary language. Narratives are found and told through oral and written language. Narratology has helped to make it easier to understand the how and why of narrative.\n\nReferences\n\nOther websites\nhttp:\/\/wikis.sub.uni-hamburg.de\/lhn\/index.php\/Main_Page  - The Living Handbook of Narratology\nhttp:\/\/www.units.miamioh.edu\/technologyandhumanities\/narratology.htm  - Notes on Narratology\nhttp:\/\/www.nou-la.org\/ling\/1975a-theonarreme.pdf - THBORIE DES NARRGMES ET ALGORITHMES NARRATIFS\n\nNarratology\nWriting","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A first-person narrative is a narrative mode in which a story or work that is narrated (told) by one character at a time, speaking from their point of view only. First-person narrative may be told by only one storyteller, or many.\n\nThis type of narrative can use such words like me, myself and I.\n\nLiterary terms","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"society is the shift from a depersonalized, \u201cseen from a vantage point\u201d and neutrally distanced narrative to a storyline with which the audience can engage morally, aesthetically, and intellectually, and, thus, co-participate in the events described. Especially for ethnographers who use multiple informants, auto-ethnography introduces an alternative way of writing where \u201cthe distinction between ethnographer and \u2018others\u2019 is unclear\u201d, thereby challenging \u201cimposed identities and boundaries\u201d.\n\nHowever, auto-ethnographers should avoid some dangerous pitfalls that could undermine the credibility and usefulness of their work. Chang identifies the following: \"(1) excessive focus on self in isolation from others; (2) overemphasis on narration rather than analysis and cultural interpretation; (3) exclusive reliance on personal memory and recalling as a data source; (4) negligence of ethical standards regarding others in self-narratives; and (5) inappropriate application of the label","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Maximum likelihood estimation (or maximum likelihood) is the name used for a number of ways to guess the parameters of a parametrised statistical model. These methods pick the value of the parameter in such a way that the probability distribution makes the observed values very likely. The method was mainly devleoped by R.A.Fisher in the early 20th century. A likelihood estimation, where probabilities are known beforehand is known as Maximum a posteriori estimation.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"are often used in these models.\n Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and  query, and as such can easily incorporate almost any other retrieval  model as just a yet another feature.\n\nSecond dimension: the properties of the model \n Models without term-interdependancies treat different terms\/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\n Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\n Models with","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"1026.\n\nSurveys\nL. H. de Figueiredo and J. Stolfi (2004) \"Affine arithmetic: concepts and applications.\" Numerical Algorithms 37 (1\u20134), 147\u2013158.\n J. L. D. Comba and J. Stolfi (1993), \"Affine arithmetic and its applications to computer graphics\". Proc. SIBGRAPI'93 \u2014 VI Simp\u00f3sio Brasileiro de Computa\u00e7\u00e3o Gr\u00e1fica e Processamento de Imagens (Recife, BR),  9\u201318.\n Nedialkov, N. S., Kreinovich, V., & Starks, S. A. (2004). Interval arithmetic, affine arithmetic, Taylor series methods: why, what next?. Numerical Algorithms, 37(1-4), 325-336.\n\nTechnology\nComputer science","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, Poisson distribution is a probability distribution. It is named after Sim\u00e9on Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of all people have green eyes). (n=500, p=0.05)\nIn order to use the binomial distribution, the following must be true about the problem:\n The outcomes are mutually exclusive, that is, there are two possible outcomes which cannot occur simultaneously (for example. in flipping a coin, there are two possible outcomes: heads or tails. It is always one or the other, never both or a mix of outcomes).\n The probability of a success (p) is consistent throughout the problem (for example, a basketball player makes 85% of his free throws. Each time the player attempts a free throw, 85% is assumed to be the likelihood of a made shot).\n The trials are independent of each other (for example, on the second flip of a coin, the first outcome does not impact the chance of the next toss: the chance of tossing a heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson distribution\n\nReferences \n\nProbability distributions","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The calculation of the last three quantities is explained in the respective Wiki pages. Then, with the help of formulas given in the previous section, the factors \u03bc and \u03b2 can be calculated. In this way, the CDF of the Gumbel distribution belonging to the data can be determined and the probability of interesting data values can be found.\n\nApplication \n\nIn hydrology, the Gumbel distribution is used to analyze such variables as monthly and annual maximum values of daily rainfall and river discharge volumes, and also to describe droughts.\n \nThe blue picture illustrates an example of fitting the Gumbel distribution to ranked maximum one-day October rainfalls showing also the 90% confidence belt based on the binomial distribution.\n\nReferences \n\nProbability distributions","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"between sessions. A goal of EMDR therapy is to produce rapid and effective change while the client maintains equilibrium during and between sessions.\n\nPhases 3-6:\u00a0 In phases three to six, a target is identified and processed using EMDR therapy procedures.\u00a0 These involve the client identifying three things:\n\n1.\u00a0 The vivid visual image related to the memory\n\n2.\u00a0 A negative belief about self\n\n3.\u00a0 Related emotions and body sensations.\n\nIn addition, the client identifies a positive belief.\u00a0 The therapist helps the client rate the positive belief as well as the intensity of the negative emotions.\u00a0 After this, the client is instructed to focus on the image, negative thought, and body sensations while simultaneously engaging in EMDR processing using sets of bilateral stimulation.\u00a0 These sets may include eye movements, taps, or tones.\u00a0 The type and length of these sets is different for each client.\u00a0 At this point, the EMDR client is instructed to just notice whatever","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"feet are exposed, which facilitates the application of medication for foot mites, etc. Clapping hands or giving the chicken a gentle shove will awaken it.\n\nOne can also hypnotize a chicken by mimicking how it sleeps  with its head under its wing. In this method, hold the bird firmly, placing its head under its wing, then, gently rock the chicken back and forth and set it very carefully on the ground. It should stay in the same position for about 30 seconds. H.B. Gibson, in his book Hypnosis: its nature and therapeutic uses, says the record period for a chicken remaining in hypnosis is 3 hours 47 minutes.\n\nTrout tickling \nTrout tickling is the art of rubbing the underbelly of a trout using fingers. If done properly, the trout will go into a trance-like state after a minute or so, and can then easily be thrown onto the nearest bit of dry land.\n\nTonic immobility as a scientific","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"system. The technique involves the daily practice of sessions that last around 15 minutes, usually in the morning, at lunch time, and in the evening. During each session, the practitioner will repeat a set of visualisations that induce a state of relaxation. Each session can be practiced in a position chosen amongst a set of recommended postures.\n\nThe technique of the Skumin mind control method (Russian: \"\u041f\u0441\u0438\u0445\u043e\u0442\u0440\u0435\u0301\u043d\u0438\u043d\u0433 \u043f\u043e \u0421\u043a\u0443\u0301\u043c\u0438\u043d\u0443\") involves the use of two standard postures: sitting meditation and lying down meditation. This method of psychotraining includes five psychological exercises: the first is \"the relaxation\", the second one is \"the warming\", the third one is \"the zero gravity\", the fourth one is \"the target autosuggestion\", and the fifth exercise is \"the psychological activation\". Each session contain explanation of the theory and practice of each new exercise as it is reached. \n\nThe therapeutic effect of the Skumin mind control method is","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"or put into a trance, by holding its head down against the ground, and drawing a line along the ground with a stick or a finger, starting at the beak and extending straight outward in front of the chicken. If the chicken is hypnotized in this manner, it will remain immobile for somewhere between 15 seconds and 30 minutes, continuing to stare at the line. One theory is that the trance is caused by fear, which is probably a defensive mechanism intended to feign death, albeit rather poorly.\n\nThe first known written reference for this method came in 1646, in Mirabile Experimentum de Imaginatione Gallinae by Athanasius Kircher in Rome.\n\nMethods \nAnother hypnosis technique is to hold the chicken face up with its back on the ground, and then run a finger downwards from the chicken's wattles to just above its vent. The chicken's feet are exposed, which facilitates the application of medication for foot mites, etc. Clapping hands","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"subspace methods: principles and analysis. OUP Oxford.\n\nFields of mathematics\nTechnology\nLinear algebra\nComputer science\nComputing","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"preferable to propagate this error into a preferred direction, or equally into several orthogonal dimensions, such as vertically vs. horizontally for bidimensional images, or into parallel color channels at the same position and\/or timestamp, and depending on other properties of these orthogonal discrete dimensions (according to a perception model). In those cases, several roundoff error accumulators may be used (at least one for each discrete dimension), or a (n-1)-dimension vector (or matrix) of accumulators.\n\nIn some of these cases, the discrete dimensions of the data to sample and round may be treated non orthogonally: for example, when working with colored images, the trichromatic color planes data in each physical dimension (height, width and optionally time) could be remapped using a perceptive color model, so that the roundoff error accumulators will be designed to preserve lightness with a higher probability than hue or saturation, instead of propagating errors into each orthogonal color plane independently; and in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"respectively by  and , are subspaces as well.\n\nRelated pages \n\n Parallel projection\n\nReferences \n\nLinear algebra\n\nru:\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e#\u041f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"...,x25, with mean:\n\nIf we take another sample of 25 cups, we could easily expect to find values like 250.4 or 251.1 grams. A sample mean value of 280 grams, however, would be extremely rare if the mean content of the cups is in fact close to 250g. \n\nThere is a whole interval around the observed value 250.2 of the sample mean within which, if the whole population mean actually takes a value in this range, the observed data would not be considered particularly unusual. Such an interval is called a confidence interval for the parameter \u03bc. \n\nTo calculate such an interval, the endpoints of the interval have to be calculated from the sample, so they are statistics, functions of the sample X1, ..., X25, and hence are random variables themselves.\n\nIn our case, we may determine the endpoints by considering that the sample mean  from a normally distributed sample is also normally distributed, with the same expectation \u03bc, but","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"for example, the standard deviation of a group of repeated measurements helps scientists know how sure they are of the average number. When deciding whether measurements from an experiment agree with a prediction, the standard deviation of those measurements is very important. If the average number from the experiments is too far away from the predicted number (with the distance measured in standard deviations), then the theory being tested may not be right. For more information, see prediction interval.\n\nApplication examples\nUnderstanding the standard deviation of a set of values allows us to know how large a difference from the \"average\" (mean) is expected.\n\nWeather\nAs a simple example, consider the average daily high temperatures for two cities, one inland and one near the ocean. It is helpful to understand that the range of daily high temperatures for cities near the ocean is smaller than for cities inland. These two cities may each have the same average daily high temperature. However, the standard deviation of the daily high temperature for the coastal city will be less than that","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a framework for speculation on the next thirty-three years. MacMillan. . With Anthony Wiener.\n1968 Can we win in Viet Nam?. Praeger. Kahn with four other authors: Gastil, Raymond D.; Pfaff, William; Stillman, Edmund; Armbruster, Frank E. \n1970. The Emerging Japanese Superstate: challenge and response. Prentice Hall. \n1971. The Japanese challenge: The success and failure of economic success. Morrow; Andre Deutsch. \n1972. Things to come: thinking about the seventies and eighties. Macmillan. . With B. Bruce-Briggs.\n1973. Herman Kahnsciousness: the megaton ideas of the one-man think tank. New American Library. Selected and edited by Jerome Agel. \n1974. The future of the corporation. Mason & Lipscomb. \n1976. The next 200 Years: a scenario for America and the world. Morrow.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Herman Kahn (February 15, 1922 \u2013 July 7, 1983) was one of the preeminent futurists of the second half of the twentieth century. His theories helped to develop the nuclear strategy of the United States.\n\nCareer \nKahn, a man of \"captivating personality and large intellectual gifts\", worked for the RAND Corporation, Santa Monica, California, as a military strategist (19481958). He was a founder of the Hudson Institute think tank. This gives ideas and consultations for the U.S. government and military chiefs, and for business clients.\n\nHe was known for analyzing the likely consequences of nuclear war and recommending ways to improve survivability. His key idea was that the threat of nuclear war could be controlled by the use of carefully graded deterrence. In order to get his ideas across he ran two-day seminars which included role-playing in various scenarios. His ideas, when published in On Thermonuclear War, caused a sensation.\n\n\"At the","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"scenarios. His ideas, when published in On Thermonuclear War, caused a sensation.\n\n\"At the minimum, an adequate deterrent for the United States must ... persuade [the Soviets] that, no matter how skillful or ingenious they were, an attack on the United States would lead to a very high risk if not certainty of large-scale destruction to Soviet civil society and military forces\".\n\nHowever, responses must be proportionate, because if one threatened all-out war as a response for some rather modest misbehaviour, then the threat was simply not believable, and would not work. The need to think things through in detail was the topic of his second book, Thinking about the Unthinkable.\n\nKahn was less successful later in his career when he turned his attention to general economics and politics. His ideas on Japan seemed good at the time, but now seem to be not quite right. He predicted that Japan would become the world's third superstate and a military superpower. This he based on Japan's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the hard sciences like maths, physics and economics. It was and still is funded mainly by the American government.\n\nThe cold war period raised the question of how the American government might handle nuclear weapons in bargaining and negotiating with the Soviet Union. This was the speciality of Herman Kahn, a RAND staff member who set up his own think tank in 1961, called the Hudson Institute. His publications on how to think about these issues, and those of Thomas Schelling, were landmarks in the post-war history of negotiating strategy.\n\nModern period \nMore recently, think tanks have specialised into particular areas of interest, such as foreign policy. Many have become advocates for political interests, doing research which can be used by public relations and lobbying groups.\n\nReferences\n\nRelating pages \nLobbying","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Stanley Milgram (August 15, 1933 \u2013 December 20, 1984) was an American social psychologist. He is most famous for his controversial study about obedience to authority figures. In these experiments, Milgram showed that about two out of three people would follow orders of an authority figure to the point of risking the life of, or perhaps even killing, an innocent person.  He got the idea from the Holocaust. He wrote a book about his experiments called Obedience to Authority.\n\nThe idea of six degrees of separation comes from Milgram's 1967 small-world experiment. It has been criticized a lot, but in 2008 Microsoft found that the average chain of contacts between users of its '.NET Messenger Service' was 6.6 people. A study published in the January 2014 volume of Computers in Human Behavior found that the average number of acquaintances separating people in unusual jobs is 3.9, and 3.2 for average Facebook","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Marcel Mauss (May 10, 1872 - February 10, 1950) was a French anthropologist and sociologist. With a strong background in sociology, he began using ethnography to look at how different cultures build relationships. He was also influenced by his nephew, another famous anthropologist, \u00c9mile Durkheim. Mauss is known for his work on gifts and exchange, magic, sacrifice, the body, and comparing cultures. Mauss\u2019s most famous work is The Gift. The Gift is about the ways gifts and exchanges build relationships. Mauss is also known for influencing structural anthropologists such as Claude L\u00e9vi-Strauss.\n\nBackground and education \nMarcel Mauss was born in \u00c9pinal, France in 1872. His parents were Gerson Mauss and Rosine (Durkheim) Mauss. He was raised in a non-observant Jewish family.\n\nIn the early 1890s, Mauss went to the University","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and rules.\n\nSociologists often use statistics to count and measure patterns in how people act or behave. Sociologists also interview people or hold group discussions to find out why people behave in certain ways. Some sociologists combine different research methods.\n\nHistory of sociology \n\nSocial analysis has been done since the time of Plato. Sociology became accepted as a type of science in the early 1800s. European cities were changing as many people moved into cities and began working in factories. Sociologists tried to understand how people interacted and how groups interacted.\n\nThe word \"sociology\" was invented by French thinker Emmanuel-Joseph Siey\u00e8s in 1780. Early thinkers who wrote about sociology included Auguste Comte and Max Weber.\n\nSociology was taught in a university for the first time at the University of Kansas in 1890. The first European department of sociology was founded in 1895 at the University of Bordeaux by \u00c9mile Durkheim. The first sociology department to","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"(Burgundy) department and to the west by the arrondissement of Nevers.\n\nIt is the westernmost of the arrondissements of the department, with an area of , the second largest in area of the department. It has 25,717 inhabitants and a population density of  inhabitants\/km\u00b2.\n\nComposition\n\nCantons\nAfter the reorganisation of the cantons in France, cantons are not subdivisions of the arrondissements so they could have communes that belong to different arrondissements.\n\nThere are only three cantons in the arrondissement of Ch\u00e2teau-Chinon (Ville): they are:\n Ch\u00e2teau-Chinon (partly) \n Gu\u00e9rigny (partly)\n Luzy\n\nCommunes\nThe arrondissement of Ch\u00e2teau-Chinon (Ville) has 80 communes; they are (with their INSEE codes):\n\n Achun (58001)","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"region has an area of . It bordered to the south with Spain and Andorra; to the east is the Mediterranean Sea (the Gulf of Lion). It also bordered four French regions: Midi-Pyr\u00e9n\u00e9es to the west, Auvergne and Rh\u00f4ne-Alpes to the north, and Provence-Alpes-C\u00f4te d'Azur (PACA) to the northeast.\n\nThe highest point in Languedoc-Roussillon is Pic Carlit () in the Pyr\u00e9n\u00e9es-Orientales department; it is  high.\n\nDepartments\nThe Languedoc-Roussillon region is formed by five departments:\n\nDemographics\nThe Languedoc-Roussillon region has a population, in 2012, of 2,700,266, for a population density of  inhabitants\/km2.\n\nThe 10 most important cities in the region are:\n\nGallery\n\nReferences\n\nOther websites\n\n Regional Council of Languedoc-Roussillon","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Mongolia does not share a border with\u00a0Kazakhstan, but its westernmost point is only 38\u00a0kilometers (24\u00a0mi) away from Kazakhstan.\n\nThe geography of Mongolia is varied, with the\u00a0Gobi Desert\u00a0to the south and with cold and mountainous regions to the north and west. Most of Mongolia consists of\u00a0steppes, with forested areas comprising 11.2% of the total land area. \u00a0The highest point in Mongolia is the\u00a0Kh\u00fciten Peak\u00a0in the\u00a0Tavan bogd\u00a0massif, at a height of 4,374\u00a0m (14,350\u00a0ft).\n\nProvinces \nMongolia is divided into 21 provinces called aimags.  The aimags are divided into 329 districts called sums.\n\n Arkhangai\n Bayan-\u00d6lgii\n Bayankhongor\n Bulgan\n Darkhan-Uul\n Dornod\n Dornogovi\n Dundgovi","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Belgium has three main geographical regions. The coastal plain is in the north-west. The central plateau are part of the Anglo-Belgian Basin. The Ardennes uplands are in the south-east. The Paris Basin reaches a small fourth area at Belgium's southernmost tip, Belgian Lorraine.\n\nThe coastal plain is mostly sand dunes and polders. Further inland is a smooth, slowly rising landscape. There are fertile valleys. The hills have many forests. The plateaus of the Ardennes are more rough and rocky. They have caves and small, narrow valleys. Signal de Botrange is the country's highest point at 694 metres (2,277\u00a0ft).\n\nProvinces\nBelgium is divided into three Regions.  Flanders and Wallonia are divided into provinces. The third Region, Brussels is not part of any province.\n\nMilitary\nThe Belgian Armed Forces have about 46,000 active troops. In 2009 the yearly defence budget was $6 billion. There are","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"on their theory they make some predictions. They then carry out an experiment or collect other types of information that will tell them whether their predictions were right or wrong. \n\nSome types of experiments cannot be done on people because the process would be too long, expensive, dangerous, unfair, or otherwise unethical. There are also other ways psychologists study the mind and behavior scientifically, and test their theories. Psychologists might wait for some events to happen on their own; they might look at patterns among existing groups of people in natural environments; or they might do experiments on animals (which can be simpler and more ethical to study).\n\nPsychology shares other things with natural sciences, as well. For example, a good psychological theory may be possible to prove wrong. Just like in any natural science, a group of psychologists can never be completely sure that their theory is the right one. If a theory can be proved wrong, but experiments do not prove it wrong, then it is more likely that the theory is accurate.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Thought is the operation of the brain in conscious activity. It may or may not be goal-directed, aimed at solving specific problems. It is not the only way brains operate. Behaviour may occur as a result of instinct, and the adaptive unconscious may solve problems without a person being aware.\n\nOther animals can use their brains to solve problems, but there is no way of telling whether they do so consciously. Thought is investigated by four or five academic disciplines, each in its own way. The disciplines include psychology, philosophy, biology, physiology, psychoanalysis and sociology.\n\nPhilosophy \nPhilosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, functions, properties, and consciousness. The mind-body problem, i.e. the relationship of the mind to the body, especially the brain, is a central issue in philosophy of mind.\n\nThe mind-body problem \nThe mind-body problem has to do with the explanation of the relationship that exists between minds, or mental","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The wavelet transform is a time-frequency representation of a signal. For example, we use it for noise reduction, feature extraction or signal compression.\n\nWavelet transform of continuous signal is defined as\n ,\nwhere\n  is so called mother wavelet,\n  denotes wavelet dilation,\n  denotes time shift of wavelet and\n  symbol denotes complex conjugate.\n\nIn case of  and , where ,  and  and  are integer constants, the wavelet transform is called discrete wavelet transform (of continuous signal).\n\nIn case of  and , where , the discrete wavelet transform is called dyadic. It is defined as\n ,\nwhere\n  is frequency scale,\n  is time scale and\n  is constant which depends on mother wavelet.\n\nIt is possible to rewrite dyadic discrete wavelet transform as\n,\nwhere  is impulse characteristic of continuous filter which is identical to  for given .\n\nAnalogously, dyadic wavelet transform with discrete time (of discrete signal) is defined","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":".\n\nAnalogously, dyadic wavelet transform with discrete time (of discrete signal) is defined as\n.\n\nCalculus","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ecological genetics is the study of genetics and evolution in natural populations.  \n\nThis contrasts with classical genetics, which works mostly on crosses between laboratory strains, and DNA sequence analysis, which studies genes at the molecular level.  \n\nResearch in ecological genetics is on traits  related to fitness, which affect an organism's survival and reproduction. Examples might be: flowering time, drought tolerance, polymorphism, mimicry, defence against predators. \n\nResearch usually involve a mixture of field and laboratory studies. Samples of natural populations may be taken back to the laboratory for their genetic variation to be analysed. Changes in the populations at different times and places will be noted, and the pattern of mortality in these populations will be studied. Research is often done on insects and other organisms that have short generation times.\n\nHistory \nAlthough work on natural populations had been done previously, it is acknowledged that the field was founded by the English biologist E.B. Ford (1901\u20131988) in the early 20th","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ecology is the branch of biology that  studies the biota (living things), the environment, and their interactions. It comes from the Greek oikos = house; logos = study. \n\nEcology is the study of ecosystems. Ecosystems describe the web or network of relations among organisms at different scales of organization. Since ecology refers to any form of biodiversity, ecologists research everything from tiny bacteria in nutrient recycling to the effects of tropical rain forests on the Earth's atmosphere.  Scientists who study these interactions are called ecologists. \n\nTerrestrial ecoregion and climate change research are two areas where ecologists now focus.\n\nThere are many practical applications of ecology in conservation biology, wetland management, natural resource management (agriculture, forestry, fisheries), city planning (urban ecology), community health, economics, and applied science. It provides a framework for understanding and researching human social interaction.\n\nPopulation ecology \n\nPopulation ecology measures the size of a population: all the living things from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World, Chelsea Green Publishing., White River Junction, VT\n Bartlett, A. 1998. \"Reflections on Sustainability, Population Growth, and the Environment\u2014Revisited\"  revised version (January 1998) of paper first published in Population & Environment, Vol. 16, No. 1, September 1994, pp.\u00a05\u201335.\n Benyus, J. 1997. Biomimicry: Innovations Inspired by Nature, William Morrow, New York\n Brown, M.T. and Ulgiati, S 1999. Emergy Evaluation of Natural Capital and Biosphere Services, AMBIO, Vol.28, No.6, Sept. 1999.\n Brundtland, G.H. (ed.), (1987),  Our common future: The World Commission on Environment and Development, Oxford, Oxford University Press.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Field research is research done in natural surroundings. It is used in biology to contrast with research done in the laboratory.\n\nField research is defined as a qualitative method of data collection that aims to observe, interact and understand people while they are in a natural environment. For example, nature conservationists observe behavior of animals in their natural surroundings and the way they react to certain scenarios. In the same way, social scientists conducting field research may conduct interviews or observe people from a distance to understand how they behave in a social environment and how they react to situations around them.\n\nField research encompasses a diverse range of social research methods including direct observation, limited participation, analysis of documents and other information, informal interviews, surveys etc. Although field research is generally characterized as qualitative research, it often involves multiple aspects of quantitative research in it.\n\nField research typically begins in a specific setting although the end objective of the study is to observe and analyze the specific behavior of a subject in that setting. The cause and effect of a certain","type":"Document"}]}}