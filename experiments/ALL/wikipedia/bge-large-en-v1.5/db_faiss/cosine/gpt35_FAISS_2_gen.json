{"query":{"0":"What is the advantage of A\/B testing?","1":"What is the ANOVA powerful for?","2":"What is the difference between frequentist and Bayesian approaches to probability?","3":"Why is acknowledging serendipity and Murphy's law challenging in the contexts of agency?","4":"What is the recommended course of action for datasets with only categorical data?","5":"What is a Generalised Linear Model (GLM)?","6":"What is Cluster Analysis?","7":"What is the purpose of Network Analysis?","8":"What is ANCOVA?","9":"What are the key principles and assumptions of ANCOVA?","10":"What are the assumptions associated with ANCOVA?","11":"What are the strengths and challenges of Content Analysis?","12":"What are the three main methods to calculate the correlation coefficient and how do they differ?","13":"What is the purpose of a correlogram?","14":"What is telemetry?","15":"What is a common reason for deviation from the normal distribution?","16":"How can the Shapiro-Wilk test be used in data distribution?","17":"Why is the Delphi method chosen over traditional forecasting methods?","18":"What is the main goal of Sustainability Science and what are the challenges it faces?","19":"Why are critical theory and ethics important in modern science?","20":"What is system thinking?","21":"What is the main principle of the Feynman Method?","22":"What is the difference between fixed and random factors in ANOVA designs?","23":"What is the replication crisis and how does it affect modern research?","24":"What is the purpose and process of the flashlight method in group discussions?","25":"What types of data can Generalized Linear Models handle and calculate?","26":"What is a heatmap and why is it useful?","27":"How did Alhazen contribute to the development of scientific methods?","28":"How can multivariate data be graphically represented?","29":"What is the advantage of using Machine Learning over traditional rules or functions in computer science and mathematics?","30":"What are some of the challenges faced by machine learning techniques?","31":"What are the characteristics of scientific methods?","32":"What is the main goal of practicing mindfulness?","33":"How is information arranged in a Mindmap?","34":"Who developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models?","35":"How do Mixed Effect Models compare to Analysis of Variance and Regressions in terms of statistical power and handling complex datasets?","36":"Why should stepwise procedures in model reduction be avoided?","37":"What are the methods to identify redundancies in data for model reduction?","38":"How are 'narratives' used in Narrative Research?","39":"What are Generalized Additive Models (GAM) and what are their advantages and disadvantages?","40":"What are the three conditions under which Poisson Distribution can be used?","41":"How does the Pomodoro technique work?","42":"What is the 'curse of dimensionality'?","43":"Why is it important to determine heteroscedastic and homoscedastic dispersion in the dataset?","44":"How did Shell contribute to the advancement of Scenario Planning?","45":"Who influenced the field of Social Network Analysis in the 1930s and what was their work based on?","46":"What are the limitations of Stacked Area Plots?","47":"What is the purpose of Thought Experiments?","48":"What is temporal autocorrelation?","49":"What methods did the Besatzfisch project employ to study the effects of stocking fish in natural ecosystems?"},"ground_truths":{"0":"The advantage of A\/B testing is its ability to establish causal relationships with a high degree of probability, which can transform decision making from an intuitive process to a scientific, evidence-based process.","1":"ANOVA is powerful for reducing variance in field experiments or to account for variance in repeated measures of experiments.","2":"Thomas Bayes introduced a different approach to probability that relied on small or imperfect samples for statistical inference. Frequentist and Bayesian statistics represent opposite ends of the spectrum, with frequentist methods requiring specific conditions like sample size and a normal distribution, while Bayesian methods work with existing data.","3":"Acknowledging serendipity and Murphy's law is challenging in the contexts of agency because lucky or unlucky actions that were not anticipated by the agents are not included in the definition of agency.","4":"For datasets containing only categorical data, users are advised to conduct a Chi Square Test. This test is used to determine whether there is a statistically significant relationship between two categorical variables in the dataset.","5":"A Generalised Linear Model (GLM) is a versatile family of models that extends ordinary linear regressions and is used to model relationships between variables.","6":"Cluster Analysis is a approach of grouping data points based on similarity to create a structure. It can be supervised (Classification) or unsupervised (Clustering).","7":"Network Analysis is conducted to understand connections and distances between data points by arranging data in a network structure.","8":"ANCOVA (analysis of covariance) is a statistical test that compares the means of two or more groups, while treating the covariance as a noise into account.","9":"ANCOVA compares group means while controlling for covariate influence, uses hypothesis testing, and considers Sum of Squares. Assumptions from linear regression and ANOVA should be met, which is normal distribution of the dataset.","10":"ANCOVA assumptions include linearity, homogeneity of variances, normal distribution of residuals, and optionally, homogeneity of slopes.","11":"Strengths of Content Analysis include its ability to counteract biases and allow researchers to apply their own social-scientific constructs. Challenges include potential biases in the sampling process, development of the coding scheme, and interpretation of data, as well as the inability to generalize theories and hypotheses beyond the data in qualitative analysis of smaller samples.","12":"The three main methods to calculate the correlation coefficient are Pearson's, Spearman's rank, and Kendall's rank. Pearson's is the most popular and is sensitive to linear relationships with continuous data. Spearman's and Kendall's are non-parametric methods based on ranks, sensitive to non-linear relationships, and measure the monotonic association. Spearman's calculates the rank order of the variables' values, while Kendall's computes the degree of similarity between two sets of ranks.","13":"A correlogram is used to visualize correlation coefficients for multiple variables, allowing for quick determination of relationships, their strength, and direction.","14":"Telemetry is a method used in wildlife ecology that uses radio signals to gather information about an animal.","15":"A common reason for deviation from the normal distribution is human actions, which have caused changes in patterns such as weight distribution.","16":"The Shapiro-Wilk test can be used to check for normal distribution in data. If the test results are insignificant (p-value > 0.05), it can be assumed that the data is normally distributed.","17":"The Delphi method is chosen over traditional forecasting methods due to a lack of empirical data or theoretical foundations to approach a problem. It's also chosen when the collective judgment of experts is beneficial to problem-solving.","18":"The main goal of Sustainability Science is to develop practical and contexts-sensitive solutions to existent problems through cooperative research with societal actors. The challenges it faces include the need for more work to solve problems and create solutions, the importance of how solutions and knowledge are created, the necessity for society and science to work together, and the challenge of building an educational system that is reflexive and interconnected.","19":"Critical theory and ethics are important in modern science because it is flawed with a singular worldview, built on oppression and inequalities, and often lacks the necessary link between empirical and ethical consequences.","20":"System thinking is a method of investigation that considers interactions and interdependencies within a system, instead of breaking it into parts.","21":"The main principle of the Feynman Method is that explaining a topic to someone is the best way to learn it.","22":"Fixed effects (or fixed factors) are the focus of the study, while random effects (or random factors) are aspects we want to ignore. In medical trials, whether someone smokes is usually a random factor, unless the study is specifically about smoking. Factors in a block design are typically random, while variables related to our hypothesis are fixed.","23":"The replication crisis refers to the inability to reproduce a substantial proportion of modern research, affecting fields like psychology, medicine, and economics. This is due to statistical issues such as the arbitrary significance threshold of p=0.05, flaws in the connection between theory and methodological design, and the increasing complexity of statistical models.","24":"The flashlight method is used to get an immediate understanding of where group members stand on a specific question or topic, or how they feel at a particular moment. It is initiated by a team leader or member, and involves everyone sharing a short statement of their opinion. Only questions for clarification are allowed during the round, and any arising issues are discussed afterwards.","25":"Generalized Linear Models can handle and calculate dependent variables that can be count data, binary data, or proportions. It also can calculate continuous variables that deviates from the normal distribution.","26":"A heatmap is a graphical representation of data where numerical values are replaced with colors. It is useful for understanding data as it allows for easy comparison of values and their distribution.","27":"Alhazen contributed to the development of scientific methods by being the first to systematically manipulate experimental conditions, paving the way for the scientific method.","28":"Multivariate data can be graphically represented through ordination plots, cluster diagrams, and network plots. Ordination plots can include various approaches like decorana plots, principal component analysis plots, or results from non-metric dimensional scaling. Cluster diagrams show the grouping of data and are useful for displaying hierarchical structures. Network plots illustrate interactions between different parts of the data.","29":"Machine Learning can handle scenarios where inputs are noisy or outputs vary, which is not feasible with traditional rules or functions.","30":"Some of the challenges faced by machine learning techniques include a lack of interpretability and explainability, a reproducibility crisis, and the need for large datasets and significant computational resources.","31":"Scientific methods are reproducible, learnable, and documentable. They help in gathering, analyzing, and interpreting data. They can be differentiated into different schools of thinking and have finer differentiations or specifications.","32":"The main goal of practicing mindfulness is to clear the mind and focus on the present moment, free from normative assumptions.","33":"In a Mindmap, the central topic is placed in the center of the visualization, with all relevant information arranged around it. The information should focus on key terms and data, omitting unnecessary details. Elements can be connected to the central topic through lines or branches, creating a web structure. Colors, symbols, and images can be used to further structure the map, and the thickness of the connections can vary to indicate importance.","34":"Charles Roy Henderson developed the calculations that allowed for linear unbiased estimates in Mixed Effect Models.","35":"Mixed Effect Models surpass Analysis of Variance in terms of statistical power and eclipse Regressions by being better able to handle the complexities of real world datasets.","36":"Stepwise procedures in model reduction should be avoided because they are not smart but brute force approaches based on statistical evaluations, and they do not include any experience or preconceived knowledge. They are not prone against many of the errors that may happen along the way.","37":"The methods to identify redundancies in data for model reduction are through correlations, specifically Pearson correlation, and ordinations, with principal component analysis being the main tool for continuous variables.","38":"'Narratives' in Narrative Research are used as a form of communication that people apply to make sense of their life experiences. They are not just representations of events, but a way of making sense of the world, linking events in meaning. They reflect the perspectives of the storyteller and their social contexts, and are subject to change over time as new events occur and perspectives evolve.","39":"Generalized Additive Models (GAM) are statistical models developed by Trevor Hastie and Robert Tibshirani to handle non-linear dynamics. These models can compromise predictor variables in a non-linear fashion and outperform linear models when predictors follow a non-linear pattern. However, this comes at the cost of potentially losing the ability to infer causality when explaining the modeled patterns.","40":"Poisson Distribution can be used when 1. Data is counts of events i.e., they are non-negative integers. 2. The events are random and occur independently of each other. 3. The mean number of events occurring in a specific time frame is constant and known.","41":"The Pomodoro technique works by deciding on a task, setting a timer for 25 minutes for example, working on the task until the timer rings, taking a short break if fewer than four intervals have been completed, and taking a longer break after four intervals, then resetting the count and starting again.","42":"The 'curse of dimensionality' refers to the challenges of dealing with high-dimensional data in machine learning, including sparsity of data points, increased difficulty in learning, and complications in data visualization and interpretation.","43":"Determining heteroscedastic and homoscedastic dispersion is important because the ordinary least squares estimator (OLS) is only suitable when homoscedasticity is present.","44":"Shell significantly advanced Scenario Planning by introducing the \"Unified Planning Machinery\" in response to increasing forecasting errors. This system allowed them to anticipate future events and manage the 1973 and 1981 oil crises. Shell's success with this method led to its widespread adoption, with over half of the Fortune 500 companies using Scenario Planning by 1982.","45":"Romanian-American psychosociologist Jacob Moreno and his collaborator Helen Jennings heavily influenced the field of Social Network Analysis in the 1930s with their 'sociometry'. Their work was based on a case of runaways in the Hudson School for Girls in New York, assuming that the girls ran away because of their position in their social networks.","46":"Stacked Area Plots are not suitable for studying the evolution of individual data series.","47":"The purpose of Thought Experiments is to systematically ask \"What if\" questions, challenging our assumptions about the world and potentially transforming our understanding of it.","48":"Temporal autocorrelation is a principle that states humans value events in the near past or future more than those in the distant past or future.","49":"The Besatzfisch project employed a variety of methods including measuring fish population dynamics, questioning anglers about economic implications, modeling decision-making processes, conducting participatory workshops, and developing social-ecological models."},"result":{"0":"A\/B testing, also known as split testing, is a method used to compare two versions of a webpage, app, or other digital asset to determine which one performs better. The advantage of A\/B testing is that it allows you to make data-driven decisions based on real user behavior rather than assumptions or opinions. By testing different variations, you can understand what resonates best with your audience, leading to improved conversion rates, user engagement, and overall performance of your digital assets.","1":"ANOVA (Analysis of Variance) is a statistical test that is powerful for comparing the means of three or more groups. It is commonly used when there are multiple groups to compare, rather than just two. ANOVA helps determine if there are statistically significant differences between the means of the groups being compared. It is useful in situations where you want to test the impact of multiple factors or treatments on a dependent variable.","2":"The main difference between frequentist and Bayesian approaches to probability lies in how they interpret and calculate probabilities. \n\nFrequentist probability relies on the frequency of events occurring in repeated experiments to determine probabilities. It focuses on the long-run frequency of events happening.\n\nOn the other hand, Bayesian probability calculates probabilities based on available evidence and prior knowledge. It updates the probability of an event occurring as new evidence is obtained.\n\nIn summary, frequentist probability is based on observed frequencies, while Bayesian probability incorporates prior knowledge and updates probabilities based on new evidence.","3":"Acknowledging serendipity and Murphy's Law can be challenging in the context of agency because they involve recognizing the role of chance and unintended consequences in our actions. Agency typically emphasizes intentional actions and the ability to make choices to achieve specific goals. Serendipity, which refers to finding something valuable or pleasant when not looking for it, and Murphy's Law, which suggests that anything that can go wrong will go wrong, introduce elements of unpredictability and lack of control that may challenge the notion of agency as solely driven by intentional actions and outcomes.","4":"When dealing with datasets that only contain categorical data, the recommended course of action is to use appropriate statistical methods for analyzing categorical data. This can include techniques such as frequency tables, chi-square tests, logistic regression, and correspondence analysis. It is important to choose the right statistical method based on the specific research question and the nature of the categorical variables in the dataset. If you need further guidance on analyzing datasets with only categorical data, consulting with a statistician or researcher experienced in categorical data analysis would be beneficial.","5":"A Generalized Linear Model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. GLMs are used when the assumptions of normality, homoscedasticity, and independence of errors are not met. They extend the linear model framework to include non-normal error distributions and link functions. GLMs are widely used in various fields such as statistics, machine learning, and data analysis to model relationships between variables when the data does not meet the assumptions of ordinary linear regression.","6":"Cluster analysis, also known as clustering, is a type of data analysis where objects are grouped together based on similarities in the data. The goal is to create clusters or groups where objects within the same cluster are more similar to each other than to those in other clusters. This technique is commonly used in data mining to discover patterns or relationships within a dataset.","7":"I don't know.","8":"ANCOVA stands for Analysis of Covariance. It is a statistical technique that blends analysis of variance (ANOVA) and regression analysis. ANCOVA is used to compare the means of dependent variables across different groups, while controlling for the effects of continuous variables that are not of primary interest but can influence the results.","9":"I don't have information on ANCOVA (Analysis of Covariance) in the context provided. ANCOVA is a statistical technique used to compare group means while controlling for the effects of one or more continuous variables. If you have any other questions or need further clarification, feel free to ask.","10":"I don't have information on ANCOVA assumptions in the context provided. ANCOVA (Analysis of Covariance) is a statistical technique used to compare group means while controlling for the effects of continuous variables. The assumptions typically associated with ANCOVA include homogeneity of regression slopes, homogeneity of variances, normality of residuals, and independence of observations.","11":"Strengths of Content Analysis:\n1. It provides a systematic and objective way to analyze large amounts of data.\n2. It can uncover patterns, trends, and relationships within the content being analyzed.\n3. Content analysis can be used to study both qualitative and quantitative data.\n4. It allows for the examination of both textual and visual content.\n5. It can be used to analyze content from various sources such as documents, interviews, social media, etc.\n\nChallenges of Content Analysis:\n1. The interpretation of content can be subjective, depending on the researcher's perspective.\n2. It can be time-consuming, especially when analyzing large datasets.","12":"The three main methods to calculate the correlation coefficient are Pearson's correlation coefficient, Spearman's rank correlation coefficient, and Kendall's tau coefficient. \n\n1. Pearson's correlation coefficient: This method measures the linear relationship between two continuous variables. It calculates the strength and direction of the relationship between the two variables. The value ranges from -1 to 1, where 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n\n2. Spearman's rank correlation coefficient: This method assesses the monotonic relationship between two continuous or ordinal variables. It does not require the data","13":"A correlogram is a visual representation of a correlation matrix. It is used to show the correlation coefficients between variables in a dataset. It helps in identifying patterns and relationships between variables.","14":"Telemetry, also known as telematics, is a technology that allows measurements to be taken from far away. It involves sending commands to a machine remotely and receiving back measurements or data.","15":"A common reason for deviation from the normal distribution is the presence of outliers or extreme values in the data. Outliers can significantly impact the mean and standard deviation of the dataset, causing the distribution to deviate from the typical bell-shaped curve of a normal distribution. Other factors that can lead to deviation include skewed data, non-linear relationships, or the presence of multiple subpopulations within the dataset.","16":"The Shapiro-Wilk test is a statistical test used to determine whether a data sample comes from a normally distributed population. It tests the null hypothesis that a sample comes from a normally distributed population. If the p-value from the test is less than a chosen significance level, the null hypothesis is rejected, indicating that the data is not normally distributed. This test is commonly used in statistics to assess the normality of data before applying certain statistical techniques that assume normality.","17":"The Delphi method is chosen over traditional forecasting methods in situations where there is a high degree of uncertainty or when there is limited data available. The Delphi method involves gathering opinions and insights from a panel of experts through a series of questionnaires or rounds of feedback. This approach allows for a more collaborative and iterative process, which can be beneficial when dealing with complex or ambiguous situations. Additionally, the Delphi method helps to reduce bias and groupthink by allowing experts to provide input anonymously.","18":"The main goal of Sustainability Science is to develop and implement solutions that ensure the well-being of current and future generations while maintaining the health of the planet. It aims to address complex environmental, social, and economic challenges by integrating knowledge from various disciplines.\n\nSome of the challenges faced by Sustainability Science include balancing economic growth with environmental protection, addressing social inequalities, changing unsustainable consumption patterns, dealing with climate change, and ensuring global cooperation and governance for sustainable development. Additionally, there are challenges related to data availability, interdisciplinary collaboration, and policy implementation.","19":"Critical theory and ethics are important in modern science because they help to address the ethical, legal, and social implications of scientific advancements. Critical theory allows for a critical examination of the underlying assumptions, power dynamics, and social structures that influence scientific research and its applications. This helps to ensure that scientific knowledge is produced and used in a responsible and ethical manner.\n\nEthics, on the other hand, provides a framework for evaluating the moral implications of scientific discoveries and technological advancements. It helps to guide scientists, policymakers, and society as a whole in making decisions that consider the well-being of individuals, communities, and the environment. By incorporating critical theory and","20":"Systems thinking is a way of understanding how things influence one another within a whole. It is a holistic approach that focuses on the relationships between the parts of a system rather than the parts themselves. Systems thinking involves looking at the big picture, understanding the interconnections, feedback loops, and patterns within a system to better comprehend its behavior and outcomes. It helps in identifying the underlying structures and dynamics that shape complex systems.","21":"The main principle of the Feynman Method is to simplify complex concepts by breaking them down into simpler parts and explaining them in a way that anyone can understand. This method involves teaching a concept to someone else as if they were a beginner, which helps to identify gaps in your own understanding.","22":"In ANOVA (Analysis of Variance) designs, fixed factors are variables that are controlled or predetermined by the researcher. They represent specific levels of interest that the researcher wants to study. On the other hand, random factors are variables that are not controlled by the researcher and are considered to be a random sample from a larger population. Random factors introduce variability into the experiment that is beyond the control of the researcher. The distinction between fixed and random factors is important in ANOVA as it helps in determining the appropriate statistical model to use for analyzing the data.","23":"The replication crisis refers to a situation in science where the results of experiments are difficult or impossible to replicate by independent researchers or even by the original researchers themselves. This crisis has potentially grave consequences as reproducibility is a fundamental aspect of the scientific method. It has been particularly discussed in fields like psychology and medicine. Efforts have been made to re-investigate classic results to determine their validity and reasons for replication failures. The crisis has led to increased awareness and discussions about the importance of replicating studies in modern research.","24":"I'm not aware of a specific method called the \"flashlight method\" in group discussions. It's possible that it may refer to a technique or approach used in certain contexts or by specific groups, but without more information, I can't provide a detailed explanation. If you can provide more context or details, I may be able to help further.","25":"Generalized Linear Models (GLMs) are a class of models that can handle a wide variety of data types and distributions. They are particularly useful for modeling non-normally distributed data and can handle continuous, binary, count, and categorical data. GLMs can calculate parameters that relate the mean of the response variable to a linear combination of predictor variables through a link function.","26":"A heatmap is a graphical representation of data where values are depicted using colors. It is useful because it allows for the visualization of complex data sets, making it easier to identify patterns, trends, and variations within the data. Heatmaps are commonly used in various fields such as data analysis, statistics, and geography to represent data in a visually appealing and easy-to-understand way.","27":"Alhazen, also known as Ibn al-Haytham, is often described as a pioneer of the modern scientific method. His contributions to the development of scientific methods include his emphasis on experimentation, observation, and evidence-based reasoning. Alhazen's work in optics, particularly his \"Book of Optics,\" laid the foundation for the scientific method by promoting systematic observation, experimentation, and the testing of hypotheses. He emphasized the importance of empirical evidence and the need for verification through experimentation, which are key principles of the scientific method.","28":"Multivariate data can be graphically represented using techniques such as scatter plots, bubble charts, parallel coordinate plots, heat maps, and 3D plots. These methods allow for the visualization of relationships between multiple variables in a single graph.","29":"One advantage of using Machine Learning over traditional rules or functions in computer science and mathematics is that Machine Learning allows computers to learn and make predictions without being explicitly programmed. This means that instead of manually designing and programming explicit algorithms, Machine Learning algorithms can learn from data and adapt to new situations. This flexibility and ability to learn from data make Machine Learning particularly useful in situations where traditional rule-based programming may be challenging or impractical.","30":"Some challenges faced by machine learning techniques include:\n\n1. **Bias and Fairness**: Machine learning models can inherit biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n\n2. **Interpretability**: Some machine learning models, like deep learning models, can be complex and act as a black box, making it difficult to understand how they arrive at a decision.\n\n3. **Overfitting**: Models can perform well on training data but poorly on new, unseen data if they have overfit the training data.\n\n4. **Data Quality**: Machine learning models heavily rely on the quality of the data they are trained","31":"The characteristics of scientific methods include systematic observation, measurement, and experimentation, as well as the formulation, testing, and modification of hypotheses. Scientific methods rely on gathering observable, empirical, and measurable evidence, and they are based on specific principles of reasoning. Additionally, scientific methods involve the collection of data through observation and experimentation, the proposal of hypotheses to explain phenomena, and the design of experimental studies to test these hypotheses. The ability to repeat these steps is crucial for predicting future results reliably. Theories in scientific methods often bind multiple hypotheses together in a coherent structure, which can lead to the formation of new hypotheses or provide context for existing ones.","32":"The main goal of practicing mindfulness is to train attention and awareness to achieve a clearer and calmer state of mind. It aims to help individuals get past the \"thinking\" mind and reach a deeper state of relaxation or awareness. Mindfulness can help in reducing stress, improving focus, and promoting overall well-being.","33":"Information in a Mindmap is arranged around a central word or idea, which is the main topic. Subtopics, ideas, tasks, or other related information are then linked to the central topic using lines or branches. This hierarchical structure helps to visually organize and connect different pieces of information in a clear and structured way.","34":"I don't know.","35":"Mixed Effect Models, Analysis of Variance (ANOVA), and Regression analysis are all statistical methods used in data analysis, but they have different strengths and weaknesses when it comes to handling complex datasets and statistical power.\n\nMixed Effect Models are more flexible than ANOVA and Regression because they can account for both fixed effects (like in ANOVA) and random effects (like in Regression). This makes them particularly useful for analyzing complex datasets with hierarchical or nested structures, such as repeated measures or longitudinal data. Mixed Effect Models can provide more accurate estimates and better account for the variability in the data, leading to increased statistical power compared to ANOVA","36":"Stepwise procedures in model reduction should be avoided because they can lead to the loss of important information and introduce errors in the reduced model. These errors can accumulate and affect the accuracy and reliability of the model. It is generally recommended to use more robust and comprehensive methods for model reduction to ensure that the reduced model retains the essential characteristics of the original model.","37":"To identify redundancies in data for model reduction, some common methods include:\n\n1. **Principal Component Analysis (PCA)**: PCA is a statistical technique used to reduce the dimensionality of the data by finding the principal components that explain the most variance in the data. Redundant information can be identified by looking at the components with low variance.\n\n2. **Correlation Analysis**: Analyzing the correlation between different variables in the dataset can help identify redundancies. Highly correlated variables may contain redundant information.\n\n3. **Feature Selection**: Using techniques like Recursive Feature Elimination (RFE) or Lasso Regression can help identify and eliminate redundant features","38":"In Narrative Research, narratives are used as the primary data source. Researchers collect and analyze stories, personal accounts, or experiences shared by individuals to understand a particular phenomenon or topic. These narratives are then analyzed to identify themes, patterns, and meanings within them. Researchers may also use narratological concepts and theories to analyze the structure and elements of the narratives collected. Overall, narratives in Narrative Research provide rich, qualitative data that offer insights into the lived experiences of individuals.","39":"Generalized Additive Models (GAM) are a type of statistical model that extends the generalized linear model (GLM) by allowing for non-linear relationships between the dependent variable and the independent variables. GAMs use smooth functions to model these non-linear relationships, making them more flexible than traditional linear models.\n\nAdvantages of GAMs:\n1. Flexibility: GAMs can capture complex non-linear relationships between variables without the need for specifying the exact form of the relationship.\n2. Interpretability: GAMs allow for easy interpretation of the effects of individual variables on the outcome.\n3. Robustness: GAMs are robust to outliers and","40":"The three conditions under which Poisson Distribution can be used are:\n1. The events need to be unrelated to each other.\n2. The events need to occur with a known average rate, represented by the symbol \u03bb (lambda).\n3. The probability of more than one event occurring in an infinitesimally small interval is negligible.","41":"I don't know.","42":"The \"curse of dimensionality\" refers to the challenges and issues that arise when working with high-dimensional data. As the number of dimensions in a dataset increases, the amount of data required to generalize accurately grows exponentially. This can lead to problems such as increased computational complexity, overfitting, and difficulty in visualizing the data. The curse of dimensionality is a common issue in fields like machine learning, data mining, and statistics.","43":"It is important to determine heteroscedastic and homoscedastic dispersion in a dataset because it affects the reliability of statistical analyses and conclusions drawn from the data. \n\nHomoscedasticity refers to the situation where the variance of the errors or residuals in a regression model is constant across all levels of the independent variable. In contrast, heteroscedasticity occurs when the variance of the errors is not constant across levels of the independent variable. \n\nIf heteroscedasticity is present in the data, it can lead to biased estimates of the coefficients in regression analysis, affecting the accuracy and validity of statistical inferences. Detecting","44":"Shell is credited with advancing the practice of Scenario Planning in the business world. In the 1970s, Shell began using Scenario Planning as a strategic tool to navigate uncertain futures and make better decisions. They developed a process that involved creating multiple plausible scenarios of the future based on different variables and factors. This allowed Shell to anticipate and prepare for various potential outcomes, rather than relying on a single forecast. The success of Shell's use of Scenario Planning inspired other organizations to adopt this approach in their strategic planning processes.","45":"I don't know who specifically influenced the field of Social Network Analysis in the 1930s.","46":"I don't know.","47":"The purpose of thought experiments is to explore hypothetical situations and scenarios in order to understand what might happen if certain conditions were true. Thought experiments help people think through complex ideas and concepts, allowing them to consider the implications and consequences of different scenarios without the need for physical experimentation.","48":"Temporal autocorrelation, also known as serial correlation, is a statistical term that refers to the correlation between observations at different time points in a time series data. In simpler terms, it is the correlation between a variable and a lagged version of itself. This autocorrelation is important in time series analysis as it can affect the accuracy of statistical models and forecasts.","49":"I don't have information on the specific methods used in the Besatzfisch project."},"source_documents":{"0":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A test is a way of checking something to see if it is true, or false, or if it is edible or not. If something can be tested, or finishes the tests correctly, it is testable. The Concise Oxford English Dictionary defines a test as: \"a procedure intended to establish the quality, performance, or reliability of something\".\n\nA test is different from an experiment: Before a test is done, there is an expected result. The test is performed, to show this result. In an experiment, the outcome is open. Very often, tests are performed as part of an experiment.\n\nProducts \nProducts are usually tested for quality, so customers will get good products.\n\nIn software engineering, a test is used to see if the software system can do what it should. Software is tested before it is released. Alpha testing is where software developers check the software for bugs. Software can also be checked for quality and usability. Beta testing is done by groups of users.\n\nTests of cars and","type":"Document"}],"1":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A t-test is a statistical hypothesis test. People use it when they want to compare a mean (average) of a measurement from one group A to some theoretical, expected value. People also use it when they want to compare the mean (average) of a measurement of two groups A and B. They want to decide if the mean in group A is different to the theoretical value or to the mean in group B.\n\nExample\nFor example, pretend there are two groups of people. One group exercises a lot and the other doesn't. Do the people who exercise tend to live longer than those who don't? Then the property of interest is the average life time. Is the average life time of people who exercise different to the average life time of people who don't? A t-test can help answer this question.\n\nWhen this is used\nThe t-test is used when the property's variance in the groups is unknown. When people want to do the t-test they have to calculate the variance from","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"}],"2":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bayesian probability figures out the likelihood that something will happen based on available evidence. This is different from frequency probability which determines the likelihood something will happen based on how often it occurred in the past.\n\nYou might use Bayesian probability if you don't have information on how often the event happened in the past.\n\nExample\nAs an example, say you want to classify an email as \"spam\" or \"not spam\".  One thing you know about this email is that it has an emoji in the subject line.  Say it's the year 2017, and 80% of the emails you got with emoji in them were spam.  So you can look at an email with emoji in the subject and say it's 80% likely to be spam.\n\nBut if only 1% of your emails were spam and 80% of the emojis were spam, that's different than if half your emails are spam and 80% of emoji emails were spam.\n\nThen you can use Bayes's","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Frequency probability or Frequentism is one of the interpretations of probability theory. Repeating a scientific experiment very often gives a number of results. It is then possible, to count the number of times that a given event happened and compare it to the total number of experiments.\n\nThis interpretation of probabiilty was very important for statistics. People who use this interpretation are often called Frequentists. Well-known frequentists include  Richard von Mises, Egon Pearson, Jerzy Neyman, R. A. Fisher and John Venn.\n\nOther interpretations of probability are Bayesian probability and Axiomatic probability theory\n\nMathematics","type":"Document"}],"3":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"thing happen.  The good thing comes from the bad action.\n\nCriticism\n\nSome philosophers say that foreseeing a bad effect (knowing it will happen) and intending a bad effect (wanting and meaning it to happen) are not different enough for the principle of double effect to be real.  Philosophers have used the trolley problem to study the principle of double effect.\n\nOther pages\n\nTrolley problem\nAbsolutism\nConsequentialism\n\nReferences \n\nPhilosophy","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"We can be (and are) concerned also about many other things. Also, real-world problems are problems because there is no obvious way to solve them. Increases in crop production in the 20th century were all fuelled by developments in basic science, not by social movements.\n\nReferences\n\nFurther reading \n Gorayska B. & R. O. Lindsay (1993). The Roots of Relevance. Journal of Pragmatics 19, 301\u2013323. Los Alamitos: IEEE Computer Society Press.\n Hj\u00f8rland, Birger (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.\n Lindsay, R. & Gorayska, B. (2002) Relevance, Goals and Cognitive Technology. International Journal of Cognitive Technology, 1, (2), 187\u2013232\n Sperber, D. & D. Wilson","type":"Document"}],"4":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Statistical surveys are collections of information about items in a population. \n\nSurveys can be grouped into numerical and categorical types. A numerical survey is to get numbers as replies. For example:\nHow many minutes, on average, do you spend eating breakfast?\n\nThis survey question is used to generate numbers in minutes. An example of a categorical question is:\n\nWhat is your favorite color?\n\nThe categories for this would be yellow, green, purple, etc... which are not numbers.\n\nSurveys of human populations and institutions are common in political polling and government, health, social science and marketing research. A survey may concentrate on opinions or factual information depending on its purpose.\n\nOther websites \n\nAmerican Association for Public Opinion Research (AAPOR)\nHow to design a web-based statistical survey online. \nAjeebo: Web based survey software tools directory\nConducting Web-Based Surveys\nFree resources for methods in social research  has links to resources for surveys and sampling.\nOnline Survey Best Practice \nShort Courses in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"object (OVS and OSV) are extremely few, each around 1-0% percent each.\n\nReferences\n\nGrammar","type":"Document"}],"5":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The General Weighted Average (GWA) is the average of grades in all subjects taken, whether passed or failed.  It is the result of combining the performance rating based on the screening criteria or subject. It serves as the indicator of a student's academic performance in a given semester or school year.\n\nRelated pages\n Academic grading in Philippines\n\nReferences\n\nEducation in the Philippines","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Linear regression is a way to explain the relationship between a dependent variable and one or more explanatory variables using a straight line. It is a special case of regression analysis.\n\nLinear regression was the first type of regression analysis to be studied rigorously. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters.  Another advantage of linear regression is that the statistical properties of the resulting estimators are easier to determine.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression can be used to fit a predictive model to a set of observed values (data). This is useful, if the goal is prediction, forecasting or reduction. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a predicted value of y (written as ).\n Given a variable y and a number of variables X1,","type":"Document"}],"6":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Random forest is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and\/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\n\nSteps of the algorithm \nThis is how the clustering takes place.\n\n Of the entire set of data a subset is taken (training set).\n The algorithm clusters the data in groups and subgroups. If you would draw lines between the data points in a subgroup, and lines that connect subgroups into group etc. the structure would look somewhat like a tree. This is called a decision tree.\n At each split or node in this cluster\/tree\/dendrogram variables are chosen at random by the program to judge whether datapoints have a close relationship or not. \n The program makes multiple trees a.k.a. a forest. Each tree is different because for each split in a tree,","type":"Document"}],"7":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"SWOT Analysis is a tool to find out the Strengths, Weaknesses, Opportunities, and Threats that are to be expected in a project or in a business venture or in something similar. It means that the situation (internal and external to the organization or individual) is looked at. It is one of the best tools to conduct the internal and external analysis for any company.\n\nInternal and external factors \nInternal factors may include employment, finance, manufacturing opportunities, and all of the marketing mix's 4Ps.\n\nExternal factors include macroeconomics, technological change, legislation, and sociocultural changes, as well as changes in the marketplace.\n\nUse \nSWOT analysis can be used in any decision-making situation. SWOT analysis can be used:\n\n to build organizational or personal strategy;\n for matching and converting;\n allow organizations to achieve their goals;\n as a basis for the analysis of business and environmental factors;\n in marketing \u2014 analyzing competitors.\n\nIn community organizations \nThe SWOT analysis has been used in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Analysis is the process of breaking a complex topic or substance into smaller parts to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analusis, \"a breaking up\", from ana- \"up, throughout\" and lysis \"a loosening\").\n\nIn this context, Analysis is the opposite of synthesis, which is to bring ideas together.\n\nThe following concepts are closely related to this basic idea:\n Mathematical analysis is the name given to any branch of mathematics that looks at what functions are, how they behave, and what things can be done with them.\n Analytical chemistry looks at the qualities of substances, and their composition.\n\nSome definitions \nThe process of breaking up a concept, proposition, or fact into its simple or ultimate constituents. Cambridge Dictionary of Philosophy.","type":"Document"}],"8":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ancohuma is a  mountain in the Andes range, in Bolivia. It is the 20th highest mountain in the Andes.\n\nMountains of Bolivia\nAndes","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Armenian National Committee of America (ANCA) is the largest and most influential Armenian American advocacy organization in the United States. Working in coordination with a network of offices, chapters, and supporters throughout the country and affiliated organizations around the world, the ANCA advances the concerns of the Armenian American community on a broad range of issues.\n\nBeyond the Central offices of ANCA are in Washington DC, Boston and Los Angeles, ANCA has more than fifty local chapters and thousands of activists, it is cooperating with a large web of regional Armenian National Committees (or Armenian Cause\/Hay Tad Offices) in Armenia, Russia, France, the Middle East, Canada and Australia.\n\nThe executive director of ANCA is Aram Hamparian.\n\nOther websites\n Armenian National Committee of America website.\n ANC of Central California Official site \n An interview with ANCA executive director Aram Hamparian \n\nArmenian diaspora\nPolitical organizations based in the United States","type":"Document"}],"9":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"General Principles\n\nArticle 3 says the Convention is based on these values and goals:\n Respect for dignity and autonomy, including the freedom to make one's own choices\n Non-discrimination\n Full acceptance of people with disabilities into society\n Respect and acceptance for people with disabilities as human beings\n Social equality between people with disabilities and people without disabilities\n People with disabilities should have the same chances and opportunities that people without disabilities have\n Fair accessibility\n Social equality between men and women\n Respect and acceptance for children with disabilities\n\nArticle 4.  General Obligation\n\nArticle 4 says that countries must make sure people with disabilities have full human rights by:\n Changing national laws\n Getting rid of laws which discriminate against people with disabilities\n Stopping practices which go against the Convention\n\nArticle 5.  Social Equality\n\nCountries must forbid all discrimination because of disability.  Countries must also protect all persons against discrimination.\n\nArticle 6.  Women\n\nCountries must understand that women and girls with disabilities suffer from double","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"some form of unconscious inferences. As well as information from the eyes, the brain used information from previous experiences. The world as experienced is built up from assumptions and conclusions from incomplete data, using prior experience of the world.\n\nExamples of well-known assumptions, based on visual experience, are:\n light comes from above\n objects are normally not viewed from below\n faces are seen (and recognized) upright.\n closer objects can block the view of more distant objects, but not vice versa\n figures (i.e., foreground objects) tend to have convex borders\n\nThe study of visual illusions (cases when the inference process goes wrong) has yielded much insight into what sort of assumptions the visual system makes.\n\nRelated pages\n Neuroscience\n Ophthalmology\n Color blindness\n Achromatopsia\n\nReferences\n\nOther websites\n\n Visual Perception 3 - Cultural and Environmental Factors\n Gestalt Laws\n The Organization of the Retina and Visual System\n Dr Trippy's Sensorium A website dedicated to the study of the human","type":"Document"}],"10":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"some form of unconscious inferences. As well as information from the eyes, the brain used information from previous experiences. The world as experienced is built up from assumptions and conclusions from incomplete data, using prior experience of the world.\n\nExamples of well-known assumptions, based on visual experience, are:\n light comes from above\n objects are normally not viewed from below\n faces are seen (and recognized) upright.\n closer objects can block the view of more distant objects, but not vice versa\n figures (i.e., foreground objects) tend to have convex borders\n\nThe study of visual illusions (cases when the inference process goes wrong) has yielded much insight into what sort of assumptions the visual system makes.\n\nRelated pages\n Neuroscience\n Ophthalmology\n Color blindness\n Achromatopsia\n\nReferences\n\nOther websites\n\n Visual Perception 3 - Cultural and Environmental Factors\n Gestalt Laws\n The Organization of the Retina and Visual System\n Dr Trippy's Sensorium A website dedicated to the study of the human","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and Ambassador to Italy (2009\u20132013), COVID-19.\nMartin Khor, 68, Malaysian journalist (The Star) and economist, Executive Director of the South Centre (2009\u20132018), cancer.\nG\u00e9rard Mannoni, 92, French sculptor, COVID-19.\nEllis Marsalis Jr., 85, American jazz pianist, suspected COVID-19.\nR\u00fcdiger Nehberg, 84, German activist and survival expert.\nDirceu Pinto, 39, Brazilian Paralympic boccia player (2008, 2012, 2016), arrhythmia.\nBucky Pizzarelli, 94, American jazz guitarist, COVID-19.\nHarold Rubin, 87, South African-born Israeli jazz clarinetist.\nGerardo Ruiz Esparza, 70, Mexican politician, Secretary of Communications and Transportation (2012\u20132018), stroke.\nAdam Schlesinger, 52,","type":"Document"}],"11":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"factors;\n in marketing \u2014 analyzing competitors.\n\nIn community organizations \nThe SWOT analysis has been used in community work as a tool to identify positive and negative factors within organizations.\n\nAlthough SWOT analysis is a part of the planning, it will not provide a strategic plan if used by itself, but a SWOT list can becomes a series of recommendations.\n\nStrengths and weaknesses (internal factors within an organization):\n\nHuman resources \u2014 staff, volunteers, board members, target population;\nPhysical resources \u2014 your location, building, equipment;\nFinancial \u2014 grants, funding agencies, other sources of income;\nActivities and processes \u2014 programs you run, systems you employ;\nPast experiences \u2014 building blocks for learning and success, your reputation in the community.\n\nOpportunities and threats (external factors from community or societal forces):\n\nFuture trends in your field or the culture;\nThe economy \u2014 local, national, or international;\nFunding sources \u2014 foundations, donors, legislatures;\nDemographics \u2014 changes in the age, race, gender, culture","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in the story writing task.\n\nThe Vietnamese stories placed primary focus on characters and the relationships between them (manifested in a great proportion of dialog). English stories placed primary focus on the sequential forward movement of the plot. Arabic stories placed primary focus on descriptive elements of the setting.\n\nContrastive rhetorics says that people who share a common language might have different rhetoric styles due the influence of culture and exchanges. The discourse goes beyond the target language's native forms of discourse organization or rhetoric.\n\nA paraphrasing task study was done in the USA among Chinese and Russian students. American students were easily able to paraphrase, but Chinese students found it hard, perhaps due to their academic environment influenced by Confucian traditions. Russian students struggled with the paraphrasing because norm in Russian academic environment was that students are only required to read and describe, and was not required to give a personal interpretation or an opinion.\n\nU.S. rhetorical style: ethnocentric sources describe it as typically direct and relatively","type":"Document"}],"12":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the correlation is. These measurements are called correlation coefficients. The best known is the Pearson product-moment correlation coefficient, sometimes denoted by  or its Greek equivalent . You put in data into a formula, and it gives you a number between -1 and 1. If the number is 1 or \u22121, then there is strong correlation. If the answer is 0, then there is no correlation. Another kind of correlation coefficient is Spearman's rank correlation coefficient.\n\nCorrelation vs causation \nCorrelation does not always mean that one thing causes the other (causation), because there might be something else that is at play.\n\nFor example, on hot days people buy ice cream, and people also go to the beach where some are eaten by sharks. There is a correlation between ice cream sales and shark attacks (they both go up as the temperature goes up in this case). But just because ice cream sales go up does not mean ice cream sales cause (causation)","type":"Document"}],"13":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Pearson's correlation is a mathematical formula used to calculate correlation coefficients between two datasets. Most computer programs have a command to calculate this such as CORREL(dataset A: dataset B). You would calculate this your self by...\n\n Step 1: Find the mean of x, and the mean of y\n Step 2: Subtract the mean of x from every x value (call them \"a\"), and subtract the mean of y from every y value (call them \"b\")\n Step 3: Calculate: ab, a2 and b2 for every value\n Step 4: Sum up ab, sum up a2 and sum up b2\n Step 5: Divide the sum of ab by the square root of [(sum of a2) \u00d7 (sum of b2)]\nDeveloped by Karl Pearson in the 1880's,\nMathematics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Interferometers are for measuring the interference properties of light waves. One of their purposes is to measure things precisely, for example in the Michelson interferometer.\n\nRelated pages \n Atacama Large Millimeter Array\n\nOptical devices","type":"Document"}],"14":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telemetry (also known as telematics) is a technology that allows measurements to be taken from far away. Usually this means that an operator can give commands to a machine over a telephone wire, or wireless internet from far away, and the computer can report back with the measurements it takes.\n\nMeasurement\nTechnology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Telecommunication (from two words, tele meaning 'from far distances' and communication meaning to share information) is the assisted transmission of signals over a distance for the purpose of communication. In earlier times, this may have involved the use of smoke signals, drums, semaphore, flags, or a mirror to flash sunlight. Starting with the telegraph, telecommunication typically involves the use of electronic transmitters such as the telephone, television, radio, optical fiber and computer.","type":"Document"}],"15":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss. The normal distribution is a continuous probability distribution that is very important in many fields of science. \n\nNormal distributions are a family of distributions of the same general form. These distributions differ in their location and scale parameters: the mean (\"average\") of the distribution defines its location, and the standard deviation (\"variability\") defines the scale. These two parameters are represented by the symbols  and , respectively.\n\nThe standard normal distribution (also known as the Z distribution) is the normal distribution with a mean of zero and a standard deviation of one (the green curves in the plots to the right). It is often called the bell curve, because the graph of its probability density looks like a bell.\n\nMany values follow a normal distribution. This is because of the central limit theorem, which says that if an event is the sum of identical but random events, it will be normally distributed. Some examples","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"if an event is the sum of identical but random events, it will be normally distributed. Some examples include:\nHeight\nTest scores\nMeasurement errors\nLight intensity (so-called Gaussian beams, as in laser light)\nIntelligence is probably normally distributed. There is a problem with accurately defining or measuring it, though.\nInsurance companies use normal distributions to model certain average cases.\n\nRelated pages \n Frequency distribution\n Least squares\n Student's t-distribution\n\nReferences\n\nOther websites \n\nCumulative Area Under the Standard Normal Curve Calculator  from Daniel Soper's Free Statistics Calculators website. Computes the cumulative area under the normal curve (i.e., the cumulative probability), given a z-score.\nInteractive Distribution Modeler (incl. Normal Distribution).\nGNU Scientific Library \u2013 Reference Manual \u2013 The Gaussian Distribution\nNormal Distribution Table\nDownload free two-way normal distribution calculator\nDownload free normal distribution fitting software\n\nProbability distributions","type":"Document"}],"16":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Chi-squared test (or  test) is a statistical hypothesis test. It usually tests the hypothesis that \"the experimental data does not differ from untreated data\". That is a null hypothesis. The distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.\n\nThe test results are regarded as 'significant' if there is only one chance in 20 that the result could be got by chance.\n\nGroups\nThere are three main groups of tests:\nTests for distribution check that the values follow a given probability distribution.\nTests for independence check that the values are independent; if this is the case, no value can be left out without losing information.\nTests for homogeneity: These check that all samples taken have the same probability distribution, or are from the same set of values.\n\nStatistical tests","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Kolmogorov\u2013Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.\n\nStatistical tests","type":"Document"}],"17":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Demand forecasting is when a business predicts future demand for its products. A business looks at many things when they do demand forecasting. Some of these things are past sales, data from test markets, and statistics. Businesses can also use educated guesses to help predict future demand. Businesses use demand forecasting to help them come up with the amount of demand for their products so they know how much supply to make.\n\nReferences\n\nBusiness\nCommerce\nEconomic theories","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Forecasting is studying and saying what is likely to happen in the future. It is similar to predicting, but usually forecasting is done with scientific methods. Forecasting can be done for many different things, like weather forecasting (predicting the weather) or economy forecasting.  Science cannot know the future for sure, so forecasters try to identify the most probable events, and sometimes they are wrong.\n\nWords","type":"Document"}],"18":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems science is the interdisciplinary field of science, that studies the principles of systems in nature, in society and in science itself.\n\nTypes of systems science are systems theory, cybernetics and chaos theory, and all kinds of similar sciences.\n\nThe aim of systems science is to develop interdisciplinary foundations for all science. This foundation is used in a variety of areas, such as engineering, biology, medicine and social sciences.\n\nRelated pages \n Chaos theory\n Complex systems\n Complexity\n Control theory\n Cybernetics\n Systems engineering\n Systems theory\n\nReferences \n\n \nSystems theory","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Sustainability means that a process or state can be maintained at a certain level for as long as is wanted.\n\nOne definition of sustainability is the one created by the Brundtland Commission, led by the former Norwegian Prime Minister Gro Harlem Brundtland. The Commission defined sustainable development as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs.\"\n\nSustainability relates to the connection of economic, social, institutional and environmental aspects of human society, as well as the non-human environment. Some overarching principles of sustainability include minimalism, efficiency, resilience and self-sufficiency. Sustainability is one of the four Core Concepts behind the 2007 Universal Forum of Cultures.\n\nRelated pages\n\n Environmentalism\nSecond law of thermodynamics\n Simple living\n\nNotes and References\n\nFootnotes\n\nReferences\n\nBibliography\n \n AtKisson, A. 1999. Believing Cassandra, An Optimist looks at a Pessimist\u2019s World,","type":"Document"}],"19":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bioethics is a field of ethics in philosophy that studies the ethical, legal and social issues in medicine and biology. It is often used to study how the new discoveries in science will affect humans, animals and nature.\n\nRelated pages \nAbortion\nBiotechnology\nClone\nEuthanasia\nMedical ethics\n\nMedicine\nEthics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ethics is the study of good and bad behaviour. It is one of the main parts of philosophy. Ethics tries to answer questions like:\n\n What actions are good? What actions are evil?\n How can we tell the difference?\n Are good and evil the same?\n How should we make hard decisions that might help or hurt other people?\n How do our actions affect others?\n\nIdeas about ethics \nWhen discussing ethics, the philosophy is generally separated into:\n thinking about morality,\n the involvement of science, \n the freedom of people to decide for themselves how to act within their own beliefs.\n\nMorality is what someone thinks or feels is good or bad.\nThere are many different moralities, but they share some things.\nFor example, most people think that murder (killing somebody) is wrong.(compare Exodus 20:13)\nSome philosophers have hope to find more things that moralities share.\nThey think that ethics should use the scientific method to study \nthings that people think are good or bad. Their work","type":"Document"}],"20":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Systems theory is the study of the nature of systems in nature, society, and science.  More specifically, systems theory is a framework to analyze or describe any group of things which work together to produce some result. This can be a single organism, any organization or society, or any electronic, mechanical or informational artifact. \n\nSystems theory as a technical and general academic area of study. It was founded by Ludwig von Bertalanffy and others in the 1950s.\n\nRelated pages\n Autopoiesis\n Chaos theory\n Fritjof Capra","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A system  is a group of related things that work together as a whole. These things can be real or imaginary. Systems can be man-made things like a car engine or natural things like a star system. Systems can also be concepts made by people to organize ideas. \n\nA subsystem is a system that is part of some larger system. For example, in computer networking, a disk subsystem is a part of a computer system.\n\nDefinition  \nA system is a group of things that connect and form some kind of coherent whole.\n\nOne of the founders of physiology, Claude Bernard, took a big step when he noticed that the internal systems of the body tend to keep things the same even if they are disturbed. He called the functions which keep system stable as homeostasis. This led towards the ideas of error-correction, feedback and regulation. These are all typical of systems which have some kind of goal and can adjust their behaviour so as to correct errors.\n\nExample \nThe Solar System is an example","type":"Document"}],"21":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"The Pauli exclusion principle refers to the fact that certain particles cannot be at the same place at the same time, with the same energy. Only fermions (examples are protons, neutrons and electrons) are bound by the Pauli exclusion principle, while bosons (an example is a photon - light beam) are not. A more precise way to describe the Pauli exclusion principle is to say that two of the same kind of fermions that are in the same quantum system (same atom, for example) cannot have the same quantum numbers. This principle was discovered by physicist Wolfgang Pauli in 1925. It is a very important principle in physics because the particles that make up ordinary matter are fermions.\n\nQuantum mechanics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"point, that means that the particle was either created or destroyed, depending on the direction in time that the particle came in from.\n\nAll the lines and vertices have an amplitude. When you multiply the probability amplitude for the lines, the amplitude for the particles to go from wherever they start to wherever they meet, and to the next meeting point, and so on, and also multiply by the amplitude for each meeting point, you get a number that tells you the total amplitude for the particles to do what the diagram says they do. If you add up all these probability amplitudes over all the possible meeting points, and over all the starting and ending points with an appropriate weight, you get the total probability amplitude for a collision in a particle accelerator, which tells you the total probability of these particles to bounce off one another in any particular direction.\n\nFeynman diagrams are named after Richard Feynman, who won the Nobel Prize in Physics. His diagrams are very simple in the case of quantum","type":"Document"}],"22":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"samples from a production lot) based on how well it met its design specifications. In contrast, Statistical Process Control uses statistical tools to observe the performance of the production process in order to predict significant deviations that may later result in rejected product.\n\nTwo kinds of variation occur in all manufacturing processes: both these types of process variation cause subsequent variation in the final product. The first is known as natural or common cause variation and consists of the variation inherent in the process as it is designed. Common cause variation may include variations in temperature, properties of raw materials, strength of an electrical current etc. The second kind of variation is known as special cause variation, or assignable-cause variation, and happens less frequently than the first. With sufficient investigation, a specific cause, such as abnormal raw material or incorrect set-up parameters, can be found for special cause variations.\n\nFor example, a breakfast cereal packaging line may be designed to fill each cereal box with 500\u00a0grams of product, but some boxes will have slightly more","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Informatica, 30:3\u201331, 2006.\n\nFurther reading \n\n Bradley, R.A. and Terry, M.E. (1952). Rank analysis of incomplete block designs, I. the method of paired comparisons. Biometrika, 39, 324\u2013345.\n David, H.A. (1988). The Method of Paired Comparisons. New York: Oxford University Press.\n Luce, R.D. (1959). Individual Choice Behaviours: A Theoretical Analysis. New York: J. Wiley.\n Thurstone, L.L. (1927).  A law of comparative judgement. Psychological Review, 34, 278\u2013286.\n Thurstone, L.L. (1929).  The Measurement of Psychological Value.  In T.V. Smith and W.K. Wright (Eds.), Essays in Philosophy by Seventeen Doctors of Philosophy of the \tUniversity of Chicago.  Chicago: Open Court.\n Thurstone, L.L.","type":"Document"}],"23":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"no reason to think that this is true at this time, but we might want to note it as another possible answer.\n\nReplication crisis \nThe replication crisis (or replicability crisis) refers to a crisis in science. Very often the result of a scientific experiment is difficult or impossible to replicate later, either by independent researchers or by the original researchers themselves. While the crisis has long-standing roots, the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nSince the reproducibility of experiments is an essential part of the scientific method, the inability to replicate studies has potentially grave consequences.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the validity of the results, and, if invalid, the reasons for the failure of replication.\n\nRecent discussions have made this problem better","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Replication may refer to:\n\nIn science:\n Replication (scientific method), one of the main principles of the scientific method\n Replication (statistics), the repetition of a test or complete experiment\n Self-replication, the process in which something (a cell, virus, program) makes a copy of itself\n DNA replication, the process of copying a double-stranded DNA molecule\n Semiconservative replication, mechanism of DNA replication\n Replication (metallography), the use of thin plastic films to duplicate the microstructure of a component\n\nIn computing:\n Replication (computing), the use of redundant resources to improve reliability or performance","type":"Document"}],"24":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A flashlight (in North American English) or torch (in most Commonwealth countries) is a small, portable spotlight.\nIts function is a beam of light which helps to see. It usually requires batteries.\n\nThe light is made by a small light bulb.  In the 20th century, it was usually an incandescent bulb.  Nowadays most use LEDs, which work slightly differently.  In 1896, the first dry cell battery was invented. Unlike previous batteries, it used a paste electrolyte instead of a liquid. This was the first battery suitable for portable electrical devices, as it did not spill or break easily and worked in any orientation.  Sometimes the electricity comes from a small generator instead of a battery.\n\nThe torch can also refer to a flammable stick that is lit on fire to provide light and (or) heat.\n\nOrigin \n\nHundreds of years ago, people used candles for all indoor light. This was expensive, and once the candle was gone, you would have to","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the flashlight is turned on by slightly unscrewing the flashlight head. The larger flashlights have a push-button switch on the side of the aluminum tube.\n\nLight_sources","type":"Document"}],"25":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in ridge regression. The least squares approach can also be used to fit models that are not linear. As outlined above, the terms \"least squares\" and \"linear model\" are closely linked, but they are not synonyms.\n\nUsage\n\nEconomics\n\nLinear regression is the main analytical tool in economics. For example, it is used to guess consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand and labor supply.\n\nRelated pages \n\n Curve fitting\n Logistic regression\n Ordinary least squares\n\nReferences\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"predicted value of y (written as ).\n Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj has no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\n\nLinear regression models try to make the vertical distance between the line and the data points (that is, the residuals) as small as possible. This is called \"fitting the line to the data.\" Often, linear regression models try to minimize the sum of the squares of the residuals (least squares), but other ways of fitting exist. They include minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or minimizing a penalized version of the least squares loss function as in ridge regression. The least squares approach can also be used to fit models that are not linear.","type":"Document"}],"26":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A weather map is a tool. It shows facts about the weather quickly.  Weather maps have been used from the mid-19th century, for study and for weather forecasting. Some maps show differences of temperature, and weather fronts. \n\nA station model is a symbolic picture showing the weather at a reporting station. Meteorologists made the station model to put down many weather elements in a small space on weather maps. Maps thickly filled with station-model plots can be hard to read. However, they help meteorologists, pilots, and mariners to see important weather patterns. A computer draws a station model for every place of observation. The station model is mostly used for surface-weather maps. It can also be used to show the weather in the sky, though. A complete station-model map lets people study patterns in air pressure, temperature, wind, cloud cover, and precipitation.\n\nHistory \n\nPeople first began using weather charts in a modern way in the mid-19th century. They began using","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Map coloring is a term used for two different concepts: In geography and mapmaking it is used to say that colors are assigned to certain areas on a map. Examples of this are coloring that show the countries or divisions of a country, but also to visualize other data, for example the altitude. The other use is in mathematics: There it is used to describe the problem of finding the minimal number of colors needed to color a given map.\n\nIn mapmaking \nColor is very useful to show different features on a map. Typical uses of color include showing different countries, different temperatures, or different kinds of roads.\n\nDisplaying the information in different colors can affect the understanding or feel of the map. In many cultures, certain colors have certain meanings. For example, red can mean danger, green can mean nature, and blue can mean water, which can be confused with the sea.\n\nMapmakers may also use colors that are related to what they are mapping. For example, when mapping where it rains more","type":"Document"}],"27":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Alhazen<ref> (Arabic: \u0623\u0628\u0648 \u0639\u0644\u064a \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u062d\u0633\u0646 \u0628\u0646 \u0627\u0644\u0647\u064a\u062b\u0645, Latinized: Alhacen or Ibn al-Haytham)<\/ref> or Alhacen or ibn al-Haytham (965\u20131039) was a pioneer of modern optics. Some have also described him as a \"pioneer of the modern scientific method\" and \"first scientist\", but others think this overstates his contribution. Alhazen's Risala fi\u2019l-makan (Treatise on Place) discussed theories on the motion of a body. He maintained that a body moves perpetually unless an external force stops it or changes its direction of motion. He laid foundations for telescopic astronomy.\n\nHe was an Arab Muslim polymath who made contributions to the principles of optics, as well as to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"to anatomy, engineering, mathematics, medicine, ophthalmology, philosophy, physics, psychology, Muslim theology, visual perception. He is sometimes called al-Basri (Arabic: \u0627\u0644\u0628\u0635\u0631\u064a), after his birthplace in the city of Basra in Iraq (Mesopotamia).\n\nAlhazen lived mainly in Cairo, Egypt, dying there at age 74. Over-confident about practical application of his mathematical knowledge, he thought he could regulate the floods of the Nile. When he was ordered by Al-Hakim bi-Amr Allah, the sixth ruler of the Fatimid caliphate, to carry out this operation, he realized he could not do it, and retired from engineering. Fearing for his life, he pretended to be mad, and was placed under house arrest. For the rest of his life he devoted himself entirely to his scientific work.\n\nRelated pages\n Islamic Golden Age\n Book of Optics\n Scientific method\n\n References \n\n Other websites","type":"Document"}],"28":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A graph is a picture designed to express words, particularly the connection between two or more quantities. You can see a graph on the right.\n\nA simple graph usually shows the relationship between two numbers or measurements in the form of a grid. If this is a rectangular graph using Cartesian coordinate system, the two measurements will be arranged into two different lines at right angle to one another. One of these lines will be going up (the vertical axis). The other one will be going right (the horizontal axis). These lines (or axes, the plural of axis) meet at their ends in the lower left corner of the graph.\n\nBoth of these axes have tick marks along their lengths. You can think of each axis as a ruler drawn on paper. So each measurement is indicated by the length of the associated tick mark along the particular axis.\n\nA graph is a kind of chart or diagram. However, a chart or a diagram may not relate one quantity to other quantities. Flowcharts and tree diagrams are charts or","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Vector graphics (also called graphical modeling, or object-oriented graphics) is a type of computer graphics. Vector graphics uses geometrical objects, like points, lines, curves, and polygons to model the image. Mathematics can be used to describe the graphics. Most often vectors and matrices are used. The first major use of vector graphics was in the Semi-Automatic Ground Environment air defense system.\n\nThe other way to model computer graphics is to use raster graphics. Raster graphics model images as a collection of pixels. Unlike raster images, vector-based images can be scaled indefinitely without loss of quality. Vector graphics are most often used for diagrams, and other things that can be described using simple shapes. Photographs are most often raster images.\n\nRelated pages\nRaster graphics\n\nComputer graphics\nVector graphics","type":"Document"}],"29":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"}],"30":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence","type":"Document"}],"31":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Scientific method refers to ways to investigate phenomena, get new knowledge, correct errors and mistakes, and test theories.\n\nThe Oxford English Dictionary says that scientific method is: \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".\n\nA scientist gathers empirical and measurable evidence, and uses sound reasoning. New knowledge often needs adjusting, or fitting into, previous knowledge.\n\nCriterion \nWhat distinguishes a scientific method of inquiry is a question known as 'the criterion'. It is an answer to the question: is there a way to tell whether a concept or theory is science, as opposed to some other kind of knowledge or belief? There have been many ideas as to how it should be expressed. Logical positivists thought a theory was scientific if it could be verified; but Karl Popper thought this was a mistake. He thought a theory was not scientific unless there was some way it","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"primarily attributed to the accuracy and objectivity (i.e. repeatability) of observation of the reality that science explores.\n\nThe role of observation in the scientific method \n\nScientific method refers to techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning. A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.\n\nAlthough procedures vary from one field of inquiry to another, identifiable features distinguish scientific inquiry from other methodologies of knowledge. Scientific researchers propose hypotheses as explanations of phenomena, and design experimental studies to test these hypotheses. These steps must be repeatable in order to dependably predict any future results. Theories that encompass wider domains of inquiry may bind many hypotheses together in a coherent structure. This in turn may help form new hypotheses or place groups of hypotheses into context.\n\nAmong other facets shared by the","type":"Document"}],"32":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Meditation tries to get past the \"thinking\" mind and aims to go into a deeper state of relaxation or awareness.\n\nMeditation is a practice where an individual trains attention and awareness to get to a clearer and calmer state. Scholars have found meditation difficult to define. The practices vary both between traditions and within them.\n\nIt is a common practice in many religions including Buddhism, Christianity (sometimes), Taoism, Hinduism (where Yoga is important)\nand other religions. Meditation has now become a modern trend, showing many health benefits.\nThe initial origin of meditation is from the Vedic times of India.\n\nBuddhist meditation \n\nIn Buddhism, three things are very important: being a good person, making the mind stronger, and understanding (Insight or Wisdom) about why people are in pain (Dukkha). For Buddhists, meditation is used to calm the mind so that the mind can better see the cause of pain. Buddhists believe that this type of seeing can end","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Mih\u00e1ly 1996. Finding flow: the psychology of engagement with everyday life. Basic Books.  [a popular exposition emphasizing technique]<\/ref>\n\n Clear goals. Expectations and rules are known and goals are attainable and within one's skills and abilities.  Moreover, the challenge level and skill level should both be high.\n Concentrating: a high degree of concentration on a limited field of attention (a person engaged in the activity will have the opportunity to focus and to delve deeply into it).\n A loss of the feeling of self-consciousness.\n Distorted sense of time, one's sense of time is altered.\n Direct and immediate feedback (successes and failures in the course of the activity are apparent, so that behavior can be adjusted as needed).\n A balance between ability level and challenge: the activity is neither too easy nor too difficult.\n A sense of control over the situation or activity.\n The activity is intrinsically rewarding, so there is an effortlessness of action.\n A lack","type":"Document"}],"33":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A mind map is a drawing used to show words, ideas, tasks or other things. Everything is written around a word or idea in the middle. This word or idea is the main topic of the mind map, and the things linked to it are related to the main idea. It is used to store and order ideas, and is very useful for study, ordering things, solving problems, and making choices.\n\nOther websites \n\nMindmapping for offices, Website \n\nDiagrams\nLearning\n\nfi:K\u00e4sitekartta","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"style=\"font-size:87%; margin:7px 0 0 6px;\"  cellspacing=0 cellpadding=0\n|\u00a0 \n|\u00a0 \n|-\n|\u00a0 England map\n|\u00a0\u00a0\n|\u00a0 Italy map\n|}\n\nAtlas\u00a0pages\u00a0are map groupings\nThe Wikiatlas is based on some simple ideas. All maps are grouped, in the manner of a World atlas, into pages stored with the name \"Atlas of...\" for each continent, such as:\n\nSimilar page titles \"Atlas of...\" cover each nation or region, such as:\n\nSome of the U.S. states also have Wikiatlas pages, such as:\n\nSimilarly, hundreds of other Wikiatlas pages are named as \"Atlas of...\" for over 200 nations and over 94 other regions of the World.\n\nWikiatlas page format\nMany pages in the Wikiatlas are arranged in a distinctive format: with maps displayed down the left-side of the page, and explanatory text presented","type":"Document"}],"34":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"and the length of the month are also ancient.\n\nThe Round-to-even method has served as the ASTM (E-29) standard since 1940. The origin of the terms unbiased rounding and statistician's rounding are fairly self-explanatory. In the 1906 4th edition of Probability and Theory of Errors  Robert Simpson Woodward called this \"the computer's rule\" indicating that it was then in common use by human computers who calculated mathematical tables. Churchill Eisenhart's 1947 paper \"Effects of Rounding or Grouping Data\" (in Selected Techniques of Statistical Analysis, McGrawHill, 1947, Eisenhart, Hastay, and Wallis, editors) indicated that the practice was already \"well established\" in data analysis.\n\nThe origin of the term \"bankers' rounding\" remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"will tend to lie closer to the centre, the mean, of the distribution. He quantified this trend, and in doing so invented linear regression analysis. This is the starting point for much of modern statistical modelling. Since then, the term \"regression\" has taken on different meanings, and it may be used by modern statisticians to describe phenomena of sampling bias which have little to do with Galton's original observations in the field of genetics.\n\nGalton's explanation for the regression phenomenon he observed is now known to be incorrect. He stated: \u201cA child inherits partly from his parents, partly from his ancestors. Speaking generally, the further his genealogy goes back, the more numerous and varied will his ancestry become, until they cease to differ from any equally numerous sample taken at haphazard from the race at large.\u201d This is incorrect, since a child receives its genetic makeup exclusively from its parents. There is no generation-skipping in genetic material: any genetic material from earlier ancestors than the","type":"Document"}],"35":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Regression analysis is a field of statistics. It is a tool to show the relationship between the inputs and the outputs of a system. There are different ways to do this. Better curve fitting usually needs more complex calculations. \n\nData modeling can be used without knowing about the underlying processes that have generated the data; in this case the model is an empirical model. Moreover, in modelling, knowledge of the probability distribution of the errors is not required. Regression analysis requires assumptions to be made regarding probability distribution of the errors. Statistical tests are made on the basis of these assumptions. In regression analysis the term \"model\" embraces both the function used to model the data and the assumptions concerning probability distributions.\n\nRegression can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. These uses of regression rely heavily on the underlying assumptions being satisfied. Regression analysis has been criticized as being misused for these purposes in many cases where the appropriate assumptions cannot be verified to hold.","type":"Document"}],"36":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"geometric modeller\". Proc. Uncertainty in Geometric Computations,  1\u201314. Kluwer Academic Publishers, .\n L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". Computer Graphics Forum, 22  2,  171\u2013179.\n C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing.\n A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". Computers & Graphics, 30  6,  1020\u2013 1026.\n\nSurveys\nL. H. de Figueiredo and J.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Numerical Hamiltonian Problems, Applied Mathematics and Mathematical Computation 7, Chapman & Hall, London, 1994.\n Arnold, D. N., Bochev, P. B., Lehoucq, R. B., Nicolaides, R. A. and Shashkov, M. (eds.), Compatible Spatial Discretizations, in The IMA Volumes in Mathematics and Its Applications, Springer, New York, 2006.\n Budd, C. and Piggott, M. D., Geometric integration and its applications, in Handbook of Numerical Analysis, XI, North\u2010Holland, Amsterdam, 2003, 35\u2010139.\n Christiansen, S. H., Munthe\u2010Kaas, H. Z. and Owren, B., Topics in structure\u2010preserving discretization, Acta Numerica, 20 (2011), 1\u2010119.\n Shashkov, M., Conservative Finite\u2010Difference Methods on General Grids,","type":"Document"}],"37":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In information theory, redundancy means that a message is encoded and transmitted using more bits that are necessary to encode the message. If a piece of information is redundant, it can be left out, without loss of information.  Redudant information such as checksums can be used to detect and correct errors in transmission or storage.\n\nOperations like data compression reduce redundancy. This can be good, as the data can be sent more quickly and take less space.  It can also be bad, if an error can no longer be corrected automatically.\n\nWhen using databases, redundancies must be avoided, as they can lead to inconsistencies. In this case, the process is called normalisation. \n\nComputer science","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A. (1996). Numerical methods for least squares problems (Vol. 51). SIAM.\n\nMathematical approximation\nStatistics","type":"Document"}],"38":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A narrative is a literary term for the events that make up a story. It is the way the events connect and make up the plot of a story or book.\n\nA narrative reports connected events, presented to the person reading or listener in a sequence of written or spoken words.\n\nA common term for narrative is plot. The study on structure in narratives is called narratology.\n\nRelated pages\nNarrative poetry\n\nReferences\n\nFiction\nWriting\nNarratology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Narratology is the study of structure in narratives. The theory of narrative or narratology was developed in the 1960s. Narratology is based on the idea of a common literary language. Narratives are found and told through oral and written language. Narratology has helped to make it easier to understand the how and why of narrative.\n\nReferences\n\nOther websites\nhttp:\/\/wikis.sub.uni-hamburg.de\/lhn\/index.php\/Main_Page  - The Living Handbook of Narratology\nhttp:\/\/www.units.miamioh.edu\/technologyandhumanities\/narratology.htm  - Notes on Narratology\nhttp:\/\/www.nou-la.org\/ling\/1975a-theonarreme.pdf - THBORIE DES NARRGMES ET ALGORITHMES NARRATIFS\n\nNarratology\nWriting","type":"Document"}],"39":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A generative model is a process for creating data that uses randomness.\n\nArtificial intelligence\nLearning\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Maximum likelihood estimation (or maximum likelihood) is the name used for a number of ways to guess the parameters of a parametrised statistical model. These methods pick the value of the parameter in such a way that the probability distribution makes the observed values very likely. The method was mainly devleoped by R.A.Fisher in the early 20th century. A likelihood estimation, where probabilities are known beforehand is known as Maximum a posteriori estimation.\n\nStatistics","type":"Document"}],"40":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"In probability and statistics, Poisson distribution is a probability distribution. It is named after Sim\u00e9on Denis Poisson. It measures the probability that a certain number of events occur within a certain period of time. The events need to be unrelated to each other. They also need to occur with a known average rate, represented by the symbol  (lambda). \n\nMore specifically, if a random variable  follows Poisson distribution with rate , then the probability of the different values of  can be described as follows: \n\n    for  \n\nExamples of Poisson distribution include:\n The numbers of cars that pass on a certain road in a certain time\n The number of telephone calls a call center receives per minute\n The number of light bulbs that burn out (fail) in a certain amount of time\n The number of mutations in a given stretch of DNA after a certain amount of radiation\n The number of errors that occur in a system\n The number of Property & Casualty insurance claims experienced in a","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"of all people have green eyes). (n=500, p=0.05)\nIn order to use the binomial distribution, the following must be true about the problem:\n The outcomes are mutually exclusive, that is, there are two possible outcomes which cannot occur simultaneously (for example. in flipping a coin, there are two possible outcomes: heads or tails. It is always one or the other, never both or a mix of outcomes).\n The probability of a success (p) is consistent throughout the problem (for example, a basketball player makes 85% of his free throws. Each time the player attempts a free throw, 85% is assumed to be the likelihood of a made shot).\n The trials are independent of each other (for example, on the second flip of a coin, the first outcome does not impact the chance of the next toss: the chance of tossing a heads (or tails) is still 50%).\n\nRelated pages \n\n Bernoulli distribution\n Poisson","type":"Document"}],"41":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"hand, and shifting.\n\nThe violinist has to learn to put the fingers in exactly the right place so that the music is \u201cin tune\u201d. This is called intonation. The musician will also learn vibrato. This changes the intonation of each note slightly by making it a little bit sharper (higher), then a little bit flatter (lower), producing a kind of wobble. This is important in many styles of music to create mood.\n\nBesides plucking (pizzicato), there are many special effects. Some of them are glissando, portamento, and harmonics. There is also double stopping, chords, and scordatura tuning.\n\nThe violin can be played either standing or sitting down. When playing solo music the violinist normally stands. When playing in chamber music or in orchestras the violinist sits, but this was not always the case. When sitting, the violinist may have to turn his or her right leg in so that it does not get in","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"between sessions. A goal of EMDR therapy is to produce rapid and effective change while the client maintains equilibrium during and between sessions.\n\nPhases 3-6:\u00a0 In phases three to six, a target is identified and processed using EMDR therapy procedures.\u00a0 These involve the client identifying three things:\n\n1.\u00a0 The vivid visual image related to the memory\n\n2.\u00a0 A negative belief about self\n\n3.\u00a0 Related emotions and body sensations.\n\nIn addition, the client identifies a positive belief.\u00a0 The therapist helps the client rate the positive belief as well as the intensity of the negative emotions.\u00a0 After this, the client is instructed to focus on the image, negative thought, and body sensations while simultaneously engaging in EMDR processing using sets of bilateral stimulation.\u00a0 These sets may include eye movements, taps, or tones.\u00a0 The type and length of these sets is different for each client.\u00a0 At this point, the EMDR client is instructed to just notice whatever","type":"Document"}],"42":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"subspace methods: principles and analysis. OUP Oxford.\n\nFields of mathematics\nTechnology\nLinear algebra\nComputer science\nComputing","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Dimensions are the way we see, measure and experience our world, by using up and down, right to left, back to front, hot and cold, how heavy and how long, as well as more advanced concepts from mathematics and physics. One way to define a dimension is to look at the degrees of freedom, or the way an object can move in a specific space. There are different concepts or ways where the term dimension is used, and there are also different definitions. There is no definition that can satisfy all concepts. \n\nIn a vector space  (with vectors being \"arrows\" with directions), the dimension of , also written as , is equal to the cardinality (or number of vectors) of a basis of  (a set which indicates how many unique directions  actually has). It is also equal to the number of the largest group of straight line directions of that space. \"Normal\" objects in everyday life are specified by three dimensions, which are usually called length, width and","type":"Document"}],"43":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.\n\nStatistical hypothesis testing is based on the assessment of differences and similarities between frequency distributions. This assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance.\n\nA frequency distribution is said to be skewed when its mean and median are different. The kurtosis of a frequency distribution is the concentration of scores at the mean, or how peaked the distribution appears if depicted graphically\u2014for example, in a histogram. If the distribution is more peaked than the normal distribution it is said to be leptokurtic; if less peaked it is said to be platykurtic.\n\nFrequency distributions are also used in frequency analysis to crack codes and refer to the relative frequency of letters in different languages.\n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Bessel's correction has high importance in calculating standard deviation. As per Bessel's correction, we should consider n-1 separation while calculating standard deviation of sampled data.\n\nStatistics","type":"Document"}],"44":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"a framework for speculation on the next thirty-three years. MacMillan. . With Anthony Wiener.\n1968 Can we win in Viet Nam?. Praeger. Kahn with four other authors: Gastil, Raymond D.; Pfaff, William; Stillman, Edmund; Armbruster, Frank E. \n1970. The Emerging Japanese Superstate: challenge and response. Prentice Hall. \n1971. The Japanese challenge: The success and failure of economic success. Morrow; Andre Deutsch. \n1972. Things to come: thinking about the seventies and eighties. Macmillan. . With B. Bruce-Briggs.\n1973. Herman Kahnsciousness: the megaton ideas of the one-man think tank. New American Library. Selected and edited by Jerome Agel. \n1974. The future of the corporation. Mason & Lipscomb. \n1976. The next 200 Years: a scenario for America and the world. Morrow.","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"the idea to a prospective producer, director, or composer.\n\nScenarios are also used in policy planning, and when trying out strategies against uncertain future developments. Here the key idea is for the scenario to be an overview, a summary, of a projected course of action, events or situations. Scenarios are widely used by organizations of all types to understand different ways that future events might unfold.\n\nTheater\nPlanning","type":"Document"}],"45":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Social psychology is the study in psychology of how people and groups interact. Researchers in this field are often either psychologists or sociologists. All social psychologists use both the individual and the group as their unit of analysis.\n\nDespite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional groups. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.\n\nReferences\n\nRelating pages\nCognitive psychology\nErich Fromm\nSociology\n\nBranches of psychology","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Stanley Milgram (August 15, 1933 \u2013 December 20, 1984) was an American social psychologist. He is most famous for his controversial study about obedience to authority figures. In these experiments, Milgram showed that about two out of three people would follow orders of an authority figure to the point of risking the life of, or perhaps even killing, an innocent person.  He got the idea from the Holocaust. He wrote a book about his experiments called Obedience to Authority.\n\nThe idea of six degrees of separation comes from Milgram's 1967 small-world experiment. It has been criticized a lot, but in 2008 Microsoft found that the average chain of contacts between users of its '.NET Messenger Service' was 6.6 people. A study published in the January 2014 volume of Computers in Human Behavior found that the average number of acquaintances separating people in unusual jobs is 3.9, and 3.2 for average Facebook","type":"Document"}],"46":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"differ rather in how far each sector extends from the center of the circle.\nThe polar area diagram is used to plot cyclic phenomena (e.g., count of deaths by month).\nFor example, if the count of deaths in each month for a year are to be plotted then there will be 12 sectors (one per month) all with the same angle of 30 degrees each. The radius of each sector would be proportional to the square root of the death count for the month, so the area of a sector represents the number of deaths in a month.\nIf the death count in each month is subdivided by cause of death, it is possible to make multiple comparisons on one diagram, as is seen in the polar area diagram used by Nightingale.\n\nLimitations \n\nPie charts are not the most accurate way to show data: that has been known for some time. Pie charts should be used only when the sum of all categories is meaningful, for example if they represent proportions.\n\nPie charts are rare","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"1026.\n\nSurveys\nL. H. de Figueiredo and J. Stolfi (2004) \"Affine arithmetic: concepts and applications.\" Numerical Algorithms 37 (1\u20134), 147\u2013158.\n J. L. D. Comba and J. Stolfi (1993), \"Affine arithmetic and its applications to computer graphics\". Proc. SIBGRAPI'93 \u2014 VI Simp\u00f3sio Brasileiro de Computa\u00e7\u00e3o Gr\u00e1fica e Processamento de Imagens (Recife, BR),  9\u201318.\n Nedialkov, N. S., Kreinovich, V., & Starks, S. A. (2004). Interval arithmetic, affine arithmetic, Taylor series methods: why, what next?. Numerical Algorithms, 37(1-4), 325-336.\n\nTechnology\nComputer science","type":"Document"}],"47":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"A thought experiment is an experiment that takes place in people's minds instead of in a laboratory or in the real world. In a real-life experiment, people can see and measure changes, but thought experiments only show special ways of thinking.  Anyone can do a thought experiment.\n\nThe usual goal of a thought experiment is to show what might happen: if this were true, what would follow from it?\n\nThe history of science has many thought experiments in it. Hans Christian \u00d8rsted was the first to use the German term  (means 'thought experiment') in about 1812. \n\nPosing hypothetical questions had been done for long time (by scientists and philosophers). However, people had no way of talking about it.\n\nFamous thought experiments\nTrolley problem\nSchr\u00f6dinger's cat\n\nReferences\n\nRelated pages \nParadox\nZeno's paradoxes\n\nScience\nPhilosophy of science\nThought","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An experiment is a test of an idea or a method. It is often used by scientists and engineers. An experiment is used to see how well the idea matches the real world. Experiments have been used for many years to help people understand the world around them. Experiments are part of scientific method. Many experiments are controlled experiments or even blind experiments. Many are done in a laboratory. But thought experiments are done in mind.\n\nExperiments can tell us if a theory is false, or if something does not work. They cannot tell us if a theory is true. When Einstein said that gravity could affect light, it took a few years before astronomers could test it. General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.<ref>Shapiro S.S. et al 2004.","type":"Document"}],"48":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"An autoregressive model is a kind of model, which is mainly used in statistics. Like all statistics models, the idea is to describe a random process. In an autoregressive model, the output value depends linearly on one of the previous values of the model, plus a ransom variable, which describes that there is some randomness in the outcome. \n\nStatistics","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Time Series is a statistical method used to forecast revenues, hotel occupancy, interest rates etc.\n\nStatistics","type":"Document"}],"49":[{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"in the East River (New York City), utilized 24 split beam hydroacoustic sensors (scientific echosounder) to detect and track the movement of fish both upstream and downstream of each of six turbines. The results suggested (1) very few fish using this portion of the river, (2) those fish which did use this area were not using the portion of the river which would subject them to blade strikes, and (3) no evidence of fish traveling through blade areas.\n\nWork is currently being conducted by the Northwest National Marine Renewable Energy Center (NNMREC) to explore and establish tools and protocols for assessment of physical and biological conditions and monitor environmental changes associated with tidal energy development.\n\nReferences \n\nHydropower","type":"Document"},{"lc_attributes":{},"lc_secrets":{},"metadata":{},"page_content":"Ecological genetics is the study of genetics and evolution in natural populations.  \n\nThis contrasts with classical genetics, which works mostly on crosses between laboratory strains, and DNA sequence analysis, which studies genes at the molecular level.  \n\nResearch in ecological genetics is on traits  related to fitness, which affect an organism's survival and reproduction. Examples might be: flowering time, drought tolerance, polymorphism, mimicry, defence against predators. \n\nResearch usually involve a mixture of field and laboratory studies. Samples of natural populations may be taken back to the laboratory for their genetic variation to be analysed. Changes in the populations at different times and places will be noted, and the pattern of mortality in these populations will be studied. Research is often done on insects and other organisms that have short generation times.\n\nHistory \nAlthough work on natural populations had been done previously, it is acknowledged that the field was founded by the English biologist E.B. Ford (1901\u20131988) in the early 20th","type":"Document"}]}}